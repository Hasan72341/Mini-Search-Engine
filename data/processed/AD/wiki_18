{"id": "31726", "revid": "525927", "url": "https://en.wikipedia.org/wiki?curid=31726", "title": "Politics of the United Kingdom", "text": "Political system of the United Kingdom\n&lt;templatestyles src=\"Hlist/styles.css\" /&gt;\nThe United Kingdom is a constitutional monarchy which, by legislation and convention, operates as a unitary parliamentary democracy. A hereditary monarch, currently King Charles III, serves as head of state while the prime minister of the United Kingdom, currently Sir Keir Starmer since 2024, serves as the head of the elected government.\nUnder the United Kingdom's parliamentary system, executive power is exercised by His Majesty's Government, whose prime minister is formally appointed by the king to act in his name. The king must appoint a member of parliament that can command the confidence of the House of Commons, usually the leader of the majority party or apparent majority party, though the king may choose to appoint an alternative if they say that they cannot expect the confidence of the House. Having taken office, the prime minister can then appoint all other ministers from parliament. \nThe Parliament has two houses: the House of Commons and the House of Lords. The Crown in Parliament is the UK's supreme legislative body, with unlimited powers of legislation subject only to convention. Normally bills passed by both Houses become law when presented for Royal Assent. However, there is provision in the Parliament Acts by which the democratically elected House of Commons could exceptionally obtain Royal Assent to a bill which the House of Lords has repeatedly failed or refused to pass. However, any use of this Parliament Acts procedure could provoke a constitutional crisis. \nParliament has devolved some legislative powers to the parliaments of Scotland and Wales and the assembly of Northern Ireland. Many other limited powers are granted by statute to the Privy Council, H.M.Ministers or other authorities, to make delegated legislation on particular subjects.\nThe British political system is a multiple-party system and was according to the V-Dem Democracy Indices 2023 the 22nd most electorally democratic in the world. From the 1920s to date, the two dominant parties have been the Conservative Party and the Labour Party. Before the Labour Party rose in British politics, the Liberal Party was the other major political party, along with the Conservatives. While coalition and minority governments have been an occasional feature of parliamentary politics, the first-past-the-post electoral system used for general elections tends to maintain the dominance of these two parties, though each has in the past century relied upon a third party, such as the Liberal Democrats, to deliver a working majority in Parliament. A Conservative\u2013Liberal Democrat coalition government held office from 2010 until 2015, the first coalition since 1945. The coalition ended following parliamentary elections on 7 May 2015, in which the Conservative Party won an outright majority of seats, 330 of the 650 seats in the House of Commons, while their coalition partners lost all but eight seats.\nWith the partition of Ireland, Northern Ireland received home rule in 1920, though civil unrest meant direct rule was restored in 1972. Support for nationalist parties in Scotland and Wales led to proposals for devolution in the 1970s, though only in the 1990s did devolution happen. Today, Scotland, Wales and Northern Ireland each possess a parliament/assembly and a government, with devolution in Northern Ireland being conditional on participation in certain all-Ireland institutions. The British government remains responsible for non-devolved matters and, in the case of Northern Ireland, co-operates with the government of the Republic of Ireland. Devolution of executive and legislative powers may have contributed to increased support for independence in the constituent parts of the United Kingdom. The principal Scottish pro-independence party, the Scottish National Party, became a minority government in 2007 and then went on to win an overall majority of MSPs at the 2011 Scottish Parliament elections which formed the current Scottish Government administration. In a 2014 referendum on independence 44.7% of voters voted for independence versus 55.3% against. In Northern Ireland, Irish nationalist parties such as Sinn F\u00e9in advocate Irish reunification. In Wales, Welsh nationalist parties such as Plaid Cymru support Welsh independence.\nThe constitution of the United Kingdom is uncodified, being made up of constitutional conventions, statutes and other elements. This system of government, known as the Westminster system, has been adopted by other countries, especially those that were formerly parts of the British Empire. \nThe United Kingdom is also responsible for several other territories, which fall into two categories: the Crown Dependencies, in the immediate vicinity of the UK, are strictly-speaking subject to the British Crown (i.e., the Monarch) but not part of the United Kingdom (though \"de facto\" British territory), and British Overseas Territories, as British colonies were re-designated in 1983, which are part of the sovereign territory of the United Kingdom, in which different aspects of internal governance have been delegated to local governments, with each territory having its own first minister, (though the titles differ, such as in the case of the Chief Minister of Gibraltar). They remain subject to the Parliament of the United Kingdom (which refers only to Great Britain and Northern Ireland, governed directly by the British Government, and not via local subsidiary governments or officers.\nHistory.\nOverviews by period.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nThe Crown.\nThe British monarch, currently King Charles III, is the head of state of the United Kingdom. Though he takes little direct part in government, the Crown remains the fount in which ultimate power over the executive government, the judiciary, the legislature and the established Church of England formally lies. These powers are known as the royal prerogative and cover a vast amount of things, such as the issue or withdrawal of passports, the appointment or dismissal of the prime minister or even the declaration of war. The powers are delegated from the Crown primarily to the prime minister, who may freely appoint privy councillors, junior ministers and other officers and servants of the Crown, (civil, military, diplomatic, and others such as the secret services) to exercise them. All powers are subject to the Rule of Law, so that the legality of their exercise may always be judicially reviewed and quashed by the High Court, and their exercise is supervised by Parliament when it is sitting. \nThe exercise of most powers by His Majesty's Government generally does not require the consent of Parliament; but certain statutory powers are subject to positive or negative resolutions of Parliament, notably the powers to make delegated legislation by Statutory Instruments, and to make certain Rules and Orders.\nThe head of His Majesty's Government, the prime minister, has weekly meetings to consult the sovereign, when they may express their feelings, warn, or advise the prime minister in the government's work.\nAccording to the uncodified constitution of the United Kingdom, the monarch has the following powers:\nDomestic powers\nForeign powers\nExecutive.\nExecutive power in the United Kingdom is exercised by the Sovereign, King Charles III, via His Majesty's Government and the devolved national authorities \u2013 the Scottish Government, the Welsh Government and the Northern Ireland Executive; and by up to three more layers of elected local authorities, often County Councils, District Councils, and Parish Councils. For example, the Corporation of The City of London, which administers only about one square mile of the capital historically enjoys some exceptional local powers, to the exclusion of all other local authorities below Parliament.\nHis Majesty's Government.\nThe monarch appoints a Prime Minister as the head of His Majesty's Government in the United Kingdom, guided by the strict convention that the Prime Minister should be the member of the House of Commons most likely to be able to form a Government with the support of that House. In practice, this means that the leader of any political party with a majority of seats in the House of Commons is chosen to be the prime minister. If no party has an absolute majority, the leader of the largest party is given the first opportunity to form a coalition. The Prime Minister then selects other Ministers who make up the Government and act as political heads of various Government Departments. About twenty of the most senior government ministers make up the Cabinet and approximately 100 ministers in total comprise the government. In accordance with constitutional convention, all ministers within the government are either Members of Parliament or peers in the House of Lords.\nAs in some other parliamentary systems of government (especially those based upon the Westminster system), the executive (called \"the government\") is drawn from and is answerable to Parliament \u2013 a successful vote of no confidence will force the government either to resign or to seek a parliamentary dissolution and a general election. In practice, members of parliament of all major parties are strictly controlled by whips who try to ensure they vote according to party policy. If the government has a large majority, then they are very unlikely to lose enough votes to be unable to pass legislation.\nThe Prime Minister and the Cabinet.\nThe prime minister, currently Sir Keir Starmer, is the most senior minister in the Cabinet. Their tenure begins when they are appointed by the monarch. The prime minister is responsible for chairing Cabinet meetings, selecting Cabinet ministers (and all other positions in His Majesty's government), and formulating government policy. The prime minister being the \"de facto\" leader of the UK, exercises executive functions that are nominally vested in the sovereign (by way of the Royal Prerogatives). Historically, the British monarch was the sole source of executive powers in the government. However, following the lead of the Hanoverian monarchs, an arrangement of a \"prime minister\" chairing and leading the Cabinet began to emerge. Over time, this arrangement became the effective executive branch of government, as it assumed the day-to-day functioning of the British government away from the sovereign.\nTheoretically, the prime minister is \"primus inter pares\" (i.e., Latin for \"first among equals\") among their Cabinet colleagues. While the prime minister is the senior Cabinet minister, they are theoretically bound to make executive decisions in a collective fashion with the other Cabinet ministers. The Cabinet, along with the PM, consists of secretaries of state from the various government departments, the Lord High Chancellor of Great Britain, the Lord Privy Seal, the Lord President of the Council, the president of the Board of Trade, the Chancellor of the Duchy of Lancaster and Ministers without portfolio. Cabinet meetings are typically held weekly, while Parliament is in session.\nGovernment departments and the Civil Service.\nThe Government of the United Kingdom contains a number of ministries known mainly, though not exclusively as departments, e.g., Department for Education. These are politically led by a Government Minister who is often a Secretary of State and member of the Cabinet. The minister may also be supported by a number of junior ministers. In practice, several government departments and ministers have responsibilities that cover England alone, with devolved bodies having responsibility for Scotland, Wales and Northern Ireland, (for example \u2013 the Department of Health), or responsibilities that mainly focus on England (such as the Department for Education).\nImplementation of the Minister's decisions is carried out by a permanent politically neutral organisation known as the Civil Service. Its constitutional role is to support the Government of the day regardless of which political party is in power. Unlike some other democracies, senior civil servants remain in post upon a change of Government. Administrative management of the department is led by a head civil servant known in most Departments as a Permanent secretary. The majority of the civil service staff in fact work in executive agencies, which are separate operational organisations reporting to Departments of State.\n\"Whitehall\" is often used as a metonym for the central core of the Civil Service. This is because most Government Departments have headquarters in and around the former Royal Palace Whitehall.\nHeads of governments.\nHM Government of the United Kingdom is the central government of the United Kingdom. Based in London, it is headed by the prime minister. As England does not have its own separate government, it is governed directly by the prime minister and UK Government. The Scottish Government is the devolved government of Scotland and is based in the capital city of Scotland Edinburgh. The Scottish Government is headed by the first minister and is supported by their deputy first minister. The Northern Ireland Executive, based in Belfast, is headed by both the first minister and deputy first minister of Northern Ireland under the terms of the Good Friday Agreement (1998), whilst the first minister of Wales serves as the leader of the Welsh Government, located in the Welsh capital, Cardiff. \nDevolved national governments.\nScottish Government.\nThe Scottish Government is responsible for all issues that are not explicitly reserved to the United Kingdom Parliament at Westminster, by the Scotland Act; including NHS Scotland, education, justice, rural affairs, agriculture, forestry and fisheries, some aspects of the benefits system, elections to the Scottish Parliament and local government, some aspects of the energy network, the environment, the Scottish Fire and Rescue Service, some aspects of equality legislation, housing, planning, sport, culture, tourism, the Crown Estate, onshore oil and gas licensing, some aspects of taxation including Scottish council tax and transport. \nIn the 2024\u201325 financial year, its annual budget was almost \u00a360 billion. The government is led by the first minister, assisted by the deputy first minister alongside various Ministers with individual portfolios and remits. The Scottish Parliament nominates a Member to be appointed as First Minister by the King. The first minister then appoints their Ministers (known as Cabinet Secretaries) and junior Ministers, subject to approval by the Parliament. The first minister, the Ministers (but not junior ministers), the Lord Advocate and Solicitor General are the Members of the Scottish Government and attend meetings of the Scottish cabinet, as set out in the Scotland Act 1998. They are collectively known as \"the Scottish Ministers\".\nThe Scottish Government has been described as \"one of the world's most powerful devolved administrations\".\nWelsh Government.\nThe Welsh Government and Senedd have more limited powers than those devolved to Scotland, although following the passing of the Government of Wales Act 2006 and the 2011 Welsh devolution referendum, the Senedd can now legislate in some areas through an Act of Senedd Cymru. The current First Minister of Wales, or \"Prif Weinidog Cymru\" in Welsh, is Eluned Morgan of Welsh Labour.\nNorthern Ireland Executive.\nAssembly have powers closer to those already devolved to Scotland. The Northern Ireland Executive is led by a diarchy, most recently First Minister Michelle O'Neill (Sinn F\u00e9in) and deputy First Minister Emma-Little Pengelly (DUP)\nLegislatures.\nThe British Parliament is the supreme legislative body in the United Kingdom (i.e., there is parliamentary sovereignty), and government is drawn from and answerable to it. Parliament is bicameral, consisting of the House of Commons and the House of Lords. There are also devolved Scottish and Welsh parliaments and a devolved assembly in Northern Ireland, with varying degrees of legislative authority.\nBritish Parliament.\nHouse of Commons.\nThe four countries of the United Kingdom are divided into parliamentary constituencies of broadly equal population by the four Boundary Commissions. Each constituency elects a Member of Parliament (MP) to the House of Commons at general elections and, if required, at by-elections. As of the 2010 general election there are 650 constituencies (there were 646 before that year's general election). At the 2017 general election, of the 650 MPs, all but one \u2013 Sylvia Hermon \u2013 were elected as representatives of a political party. However, as of the 2019 general election, there are currently 11 independent MPs, who have either chosen to leave their political party or have had the whip withdrawn.\nIn modern times, all prime ministers and leaders of the opposition have been drawn from the Commons, not the Lords. Alec Douglas-Home resigned from his peerages days after becoming prime minister in 1963, and the last prime minister before him from the Lords left in 1902 (the Marquess of Salisbury).\nOne party usually has a majority in parliament, because of the use of the First Past the Post electoral system, which has been conducive in creating the current Two-party system. The monarch normally asks a person commissioned to form a government simply whether it can \"survive\" in the House of Commons, something which majority governments are expected to be able to do. In exceptional circumstances the monarch asks someone to 'form a government' \"with a parliamentary minority\" which in the event of no party having a majority requires the formation of a coalition government or 'confidence and supply' arrangement. This option is only ever taken at a time of national emergency, such as war-time. It was given in 1916 to Bonar Law, and when he declined, to David Lloyd George and in 1940 to Winston Churchill. A government is not formed by a vote of the House of Commons, it is a commission from the monarch. The House of Commons gets its first chance to indicate confidence in the new government when it votes on the Speech from the throne (the legislative programme proposed by the new government).\nHouse of Lords.\nThe House of Lords was previously a largely hereditary aristocratic chamber, although including life peers, and Lords Spiritual. It is currently midway through extensive reforms, the most recent of these being enacted in the House of Lords Act 1999, in which it sought to reduce the number of hereditary peers within the Lords to remove their automatic right to sit and vote. Although, through negotiation, 92 peers remain temporarily in the Lords. However, in September 2024, Starmer's Labour government introduced the House of Lords (Hereditary Peers) Bill. The bill strives to remove all individuals who hold legislative positions, resulting from birthright, to improve democratic representation within the legislative system - this is expected to become law by the end of 2025 (or early 2026). The house consists of two very different types of member, the Lords Temporal and Lords Spiritual. Lords Temporal include appointed members (life peers with no hereditary right for their descendants to sit in the house) and ninety-two remaining hereditary peers, elected from among, and by, the holders of titles which previously gave a seat in the House of Lords. The Lords Spiritual represent the established Church of England and number twenty-six: the Five Ancient Sees (Canterbury, York, London, Winchester and Durham), and the 21 next-most senior bishops. Secular organisations such as Humanists UK oppose bishops sitting in the House of Lords. The movement to end the Church of England's status as the official state religion of the United Kingdom is known as disestablishmentarianism. Alternatives include a secular state in which the state purports to be officially neutral in matters of religion.\nThe House of Lords currently acts to review legislation initiated by the House of Commons, with the power to propose amendments, and can exercise a suspensive veto. This allows it to delay legislation if it does not approve it for twelve months. However, the use of vetoes is limited by convention and by the operation of the Parliament Acts 1911 and 1949: the Lords may not veto the \"money bills\" or major manifesto promises (see Salisbury convention). Persistent use of the veto can also be overturned by the Commons, under a provision of the Parliament Act 1911. Often governments will accept changes in legislation in order to avoid both the time delay, and the negative publicity of being seen to clash with the Lords. However the Lords still retain a full veto in acts which would extend the life of parliament beyond the 5-year term limit introduced by the Parliament Act 1911.\nThe Constitutional Reform Act 2005 outlined plans for a Supreme Court of the United Kingdom to replace the role of the Law Lords.\nThe Supreme Court of the United Kingdom replaced the House of Lords as the final court of appeal on civil cases within the United Kingdom on 1 October 2009.\nDevolved national parliaments.\nThough the British parliament remains the sovereign parliament, Scotland and Wales have devolved parliaments and Northern Ireland has an assembly. Each can have its powers broadened, narrowed or changed by an act of the UK Parliament. Both the Scottish Parliament and the Welsh Senedd gained legislative power over some forms of taxation between 2012 and 2016. Their power over economic issues is significantly constrained by an act of parliament passed in 2020. \nThe UK is a unitary state with a devolved system of government. This contrasts with a federal system, in which sub-parliaments or state parliaments and assemblies have a clearly defined constitutional \"right\" to exist and a \"right\" to exercise certain constitutionally guaranteed and defined functions and cannot be unilaterally abolished by acts of the central parliament. All three devolved parliaments are elected by proportional representation: the additional-member system is used in Scotland and Wales, and single transferable vote is used in Northern Ireland. England, therefore, is the only country in the UK not to have its own devolved parliament. However, senior politicians of all main parties have voiced concerns in regard to the West Lothian question, which is raised where certain policies for England are set by MPs from all four constituent nations whereas similar policies for Scotland or Wales might be decided in the devolved assemblies by legislators from those countries alone. Alternative proposals for English regional government have stalled, following a poorly received referendum on devolved government for the North East of England, which had hitherto been considered the region most in favour of the idea, with the exception of Cornwall, where there is widespread support for a Cornish Assembly, including all five Cornish MPs. England is therefore governed according to the balance of parties across the whole of the United Kingdom.\nThe government has no plans to establish an English parliament or assembly although several pressure groups are calling for one. One of their main arguments is that MPs (and thus voters) from different parts of the UK have inconsistent powers. Currently an MP from Scotland can vote on legislation which affects only England but MPs from England (or indeed Scotland) cannot vote on matters devolved to the Scottish parliament. Indeed, the former prime minister Gordon Brown, who was an MP for a Scottish constituency until the 2015 general election, introduced some laws that only affect England and not his own constituency. This anomaly is known as the West Lothian question.\nThe policy of the British Government in England was to establish elected regional assemblies with no legislative powers. The London Assembly was the first of these, established in 2000, following a referendum in 1998, but further plans were abandoned following rejection of a proposal for an elected assembly in North East England in a referendum in 2004. Unelected regional assemblies remain in place in eight regions of England.\nScottish Parliament.\nThe Scottish Parliament is the national, unicameral legislature of Scotland, located in the Holyrood area of the capital Edinburgh. The Parliament, informally referred to as \"Holyrood\" (cf. \"Westminster\"), is a democratically elected body comprising 129 members who are known as Members of the Scottish Parliament, or MSPs. Members are elected for four-year terms under the mixed-member proportional representation system. As a result, 73 MSPs represent individual geographical constituencies elected by the plurality (\"first past the post\") system, with a further 56 returned from eight additional member regions, each electing seven MSPs.\nThe current Scottish Parliament was established by the Scotland Act 1998 and its first meeting as a devolved legislature was on 12 May 1999. The parliament has the power to pass laws and has limited tax-varying capability. Another of its roles is to hold the Scottish Government to account. The \"devolved matters\" over which it has responsibility include education, health, agriculture, and justice. A degree of domestic authority, and all foreign policy, remains with the British Parliament in Westminster. The public take part in Parliament in a way that is not the case at Westminster through Cross-Party Groups on policy topics which the interested public join and attend meetings of alongside Members of the Scottish Parliament (MSPs).\nThe resurgence in Celtic language and identity, as well as 'regional' politics and development, has contributed to forces pulling against the unity of the state. This was clearly demonstrated when \u2013 although some argue it was influenced by general public disillusionment with Labour \u2013 the Scottish National Party (SNP) became the largest party in the Scottish Parliament by one seat.\nAlex Salmond (leader of SNP between 2004 and 2014) made history becoming the first First Minister of Scotland from a party other than Labour following the 2007 Scottish Parliament election. The SNP governed as a minority government following this election. Scottish nationalism which advocates for Scotland regaining its independence has experienced a dramatic rise in popularity in recent years, with a pivotal moment coming at the 2011 Scottish Parliament election where the SNP capitalised on the collapse of the Liberal Democrat support to improve on their 2007 performance to win the first ever outright majority at Holyrood (despite the voting system being specifically designed to prevent majorities), with Labour remaining the largest opposition party.\nThis election result prompted the leader of the three main opposition parties to resign. Iain Gray was succeeded as Scottish Labour leader by Johann Lamont, Scottish Conservative and Unionist leader, Annabel Goldie was replaced by Ruth Davidson, and Tavish Scott, leader of the Scottish Liberal Democrats was replaced by Willie Rennie.\nA major SNP manifesto pledge was to hold a referendum on Scottish Independence, which was duly granted by the British Government and held on 18 September 2014. When the nationalists came to power in 2011, opinion polls placed support for independence at around 31%, but in 2014, 45% voted to leave the union. In the wake of the referendum defeat, membership of the SNP surged to over 100,000, overtaking the Liberal Democrats as the third largest political party in the UK by membership, and in the general election of May 2015 the SNP swept the board and took 56 of the 59 Westminster constituencies in Scotland (far surpassing their previous best of 11 seats in the late 1970s) and winning more than 50% of the Scottish vote.\nSalmond resigned as first minister and leader of the SNP following the country's rejection of independence in September 2014, and was succeeded in both roles by his deputy first minister and deputy leader of the SNP, Nicola Sturgeon. Additionally, following the independence referendum, Lamont stood down as Scottish Labour leader and Jim Murphy was elected to replace her. Murphy remained leader until the general election in 2015 in which he lost his seat in Westminster. After the defeat, he resigned his position MSP Kezia Dugdale became leader of the party. In 2017, Dugdale unexpectedly resigned and was replaced as Scottish Labour leader by Richard Leonard. He held the post until resigning in January 2021, with Anas Sarwar replacing him the following month.\nSenedd.\nThe Senedd (formerly the National Assembly for Wales) is the devolved legislature of Wales with power to make legislation and vary taxes. The Parliament comprises 60 members, who are known as Members of the Senedd, or MSs (). Members are elected for four-year terms under an additional members system, where 40 MSs represent geographical constituencies elected by the plurality system, and 20 MSs from five electoral regions using the d'Hondt method of proportional representation.\nThe Welsh Parliament was created by the Government of Wales Act 1998, which followed a referendum in 1997. On its creation, most of the powers of the Welsh Office and Secretary of State for Wales were transferred to it. The Senedd had no powers to initiate primary legislation until limited law-making powers were gained through the Government of Wales Act 2006. Its primary law-making powers were enhanced following a Yes vote in the referendum on 3 March 2011, making it possible for it to legislate without having to consult the British parliament, nor the Secretary of State for Wales in the 20 areas that are devolved.\nNorthern Ireland Assembly.\nThe government of Northern Ireland was established as a result of the 1998 Good Friday Agreement. This created the Northern Ireland Assembly. The Assembly is a unicameral body consisting of 90 members elected under the single transferable vote form of proportional representation. The Assembly is based on the principle of power-sharing, in order to ensure that both communities in Northern Ireland, unionist and nationalist, participate in governing the region. It has power to legislate in a wide range of areas and to elect the Northern Ireland Executive (cabinet). It sits at Parliament Buildings at Stormont in Belfast.\nThe Assembly has authority to legislate in a field of competences known as \"transferred matters\". These matters are not explicitly enumerated in the Northern Ireland Act 1998 but instead include any competence not explicitly retained by the Parliament at Westminster. Powers reserved by Westminster are divided into \"excepted matters\", which it retains indefinitely, and \"reserved matters\", which may be transferred to the competence of the Northern Ireland Assembly at a future date. Health, criminal law and education are \"transferred\" while royal relations are all \"excepted\".\nWhile the Assembly was in suspension, due to issues involving the main parties and the Provisional Irish Republican Army (IRA), its legislative powers were exercised by the UK government, which effectively had power to legislate by decree. Laws that would normally be within the competence of the Assembly were passed by the UK government in the form of Orders-in-Council rather than legislative acts.\nThere has been a significant decrease in violence over the last twenty years, though the situation remains tense, with the more hard-line parties such as Sinn F\u00e9in and the Democratic Unionist Party now holding the most parliamentary seats (see Demographics and politics of Northern Ireland).\nJudiciary.\nThe United Kingdom does not have a single legal system due to it being created by the political union of previously independent countries with the terms of the Treaty of Union guaranteeing the continued existence of Scotland's separate legal system. Today the UK has three distinct systems of law: English law, Northern Ireland law and Scots law. Recent constitutional changes saw a new Supreme Court of the United Kingdom come into being in October 2009 that took on the appeal functions of the Appellate Committee of the House of Lords. The Judicial Committee of the Privy Council, comprising the same members as the Supreme Court, is the highest court of appeal for several independent Commonwealth countries, the UK overseas territories, and the British crown dependencies.\nEngland, Wales and Northern Ireland.\nBoth English law, which applies in England and Wales, and Northern Ireland law are based on common-law principles. The essence of common-law is that law is made by judges sitting in courts, applying their common sense and knowledge of legal precedent (\"stare decisis\") to the facts before them. The Courts of England and Wales are headed by the Senior Courts of England and Wales, consisting of the Court of Appeal, the High Court of Justice (for civil cases) and the Crown Court (for criminal cases). The Supreme Court of the United Kingdom is the highest court in the land for both criminal and civil cases in England, Wales, and Northern Ireland and any decision it makes is binding on every other court in the hierarchy.\nScotland.\nScots law, a hybrid system based on both common-law and civil-law principles, applies in Scotland. The chief courts are the Court of Session, for civil cases, and the High Court of Justiciary, for criminal cases. The Supreme Court of the United Kingdom serves as the highest court of appeal for civil cases under Scots law. Sheriff courts deal with most civil and criminal cases including conducting criminal trials with a jury, known that as Sheriff solemn Court, or with a Sheriff and no jury, known as (Sheriff summary Court). The Sheriff courts provide a local court service with 49 Sheriff courts organised across six Sheriffdoms.\nElectoral systems.\nVarious electoral systems are used in the UK:\nThe use of the first-past-the-post to elect members of Parliament is unusual among European nations. The use of the system means that when three or more candidates receive a significant share of the vote, MPs are often elected from individual constituencies with a plurality (receiving more votes than any other candidate), but not an absolute majority (50 percent plus one vote).\nElections and political parties in the United Kingdom are affected by Duverger's law, the political science principle that states that plurality voting systems, such as first-past-the-post, tend to lead to the development of two-party systems. The UK, like several other states, has sometimes been called a \"two-and-a-half party system\" because parliamentary politics is dominated by the Labour Party and Conservative Party, while the Liberal Democrats used to hold a significant number of seats (but still substantially less than Labour and the Conservatives), and several small parties (some of them regional or nationalist) trailed far behind in the number of seats, although this changed in the 2015 general election.\nIn the last few general elections, voter mandates for Westminster in the 30\u201340% ranges have been swung into 60% parliamentary majorities. No single party has won a majority of the popular vote since the Third National Government of Stanley Baldwin in 1935. On two occasions since World War II \u2013 1951 and February 1974 \u2013 a party that came in second in the popular vote came out with the largest number of seats.\nElectoral reform for parliamentary elections has been proposed many times. Calls for reforms to the first-past-the-post electoral system comes from the significant disproportionate representation that emerges with elections, diminishing the idea of fair elections, also resulting in candidates winning a constituency vote without the majority vote. Leaving smaller parties without representation in Parliament. The plurality rule (first-past-the-post) voting system states that candidates do not require the majority vote (of 50% + 1), but only more votes than any other candidate which gives leading parties the advantage while heavily discriminating against smaller parties who may rely on dispersed national support. The Jenkins Commission report in October 1998 suggested implementing the Alternative Vote Top-up (also called alternative vote plus or AV+) in parliamentary elections. Under this proposal, most MPs would be directly elected from constituencies by the alternative vote, with a number of additional members elected from \"top-up lists.\" However, no action was taken by the Labour government at the time. There are several groups in the UK campaigning for electoral reform, including the Electoral Reform Society, Make Votes Count Coalition, and Fairshare. The boundary commission for England has also suggested in its 2023 boundary review that constituency lines should be redrawn to allow constituencies to have a similar number of residents.\nThe 2010 general election resulted in a hung parliament (no single party was able to command a majority in the House of Commons). This was only the second general election since World War II to return a hung parliament, the first being the February 1974 election. The Conservatives gained the most seats (ending 13 years of Labour government) and the largest percentage of the popular vote but fell 20 seats short of a majority.\nThe Conservatives and Liberal Democrats entered into a new coalition government, headed by David Cameron. Under the terms of the coalition agreement, the government committed itself to holding a referendum in May 2011 on whether to change parliamentary elections from first-past-the-post to AV. Electoral reform was a major priority for the Liberal Democrats, who favour proportional representation but were able to negotiate only a referendum on AV (the alternative vote system is not a form of proportional representation) with the Conservatives. The coalition partners campaigned on opposite sides, with the Liberal Democrats supporting AV and the Conservatives opposing it. The referendum resulted in the Conservatives' favour, and the first-past-the-post system was maintained.\nPolitical parties.\nSince the 1920s the two main political parties in the UK, in terms of the number of seats in the House of Commons, are the Conservative and Unionist Party and the Labour Party. The Scottish National Party has the second largest party membership, but a smaller number of MPs as it only fields candidates for constituencies in Scotland.\nThe modern day Conservative Party was founded in 1834 and is an outgrowth of the Tory movement or party, which began in 1678. Today it is still colloquially referred to as the Tory Party and members/supporters are referred to as \"Tories\". The Liberal Democrats (or \"Lib Dems\") were founded in 1988 by an amalgamation of the Liberal Party and the Social Democratic Party (SDP), a right-wing Labour breakaway movement formed in 1981. The Liberals and SDP had contested elections together as the SDP\u2013Liberal Alliance for seven years previously. The modern Liberal Party had been founded in 1859 as an outgrowth of the Whig movement or party (which began at the same time as the Tory Party and was its historical rival) as well as the Radical and Peelite tendencies.\nThe Liberal Party was one of the two dominant parties (along with the Conservatives) from its founding until the 1920s, when it rapidly declined in popularity, and was supplanted on the left by the Labour Party, which was founded in 1900 and formed its first minority government in 1924. Since that time, the Labour and Conservative parties have been dominant, with the Liberals (later Liberal Democrats) being the third-largest party until 2015, when they lost 49 of their 57 seats. They lost 1 seat in the 2019 general election, but in the 2024 general election gained 64 seats and are, once again, the third-largest party in the House of Commons, with a total of 72 seats.\nFrom the 2015 general election until the 2024 general election, the Scottish National Party was the third-largest party. They gained 56 seats in 2015. Founded in 1934, the SNP advocates Scottish independence and has had continuous representation in Parliament since 1967. The SNP currently leads a minority government in the Scottish Parliament, and after the 2019 general election had 48 MPs in the House of Commons. This number was significantly reduced to just 9 MPs in the 2024 general election, making the SNP the fourth-largest party. \nReform UK, a right-wing party, currently has five MPs, four of which were elected in the 2024 general election. It was formerly known as the Brexit Party and is a supplantation of the now extra parliamentary United Kingdom Independence Party (UKIP). Whilst formerly being a minor party, it has significantly grown in recent years, and are a contender to win the most seats in the next general election.\nThere are several minor political parties in the UK:\nAfter two years of being a minority government, the Conservatives gained a majority in the general election in 2019, but lost this majority to the Labour Party in the 2024 general election.\nConservatives (Tories).\nThe Conservative Party won the largest number of seats at the 2015 general election, returning 330 MPs, enough for an overall majority, and went on to form the first Conservative majority government since the 1992 general election. The Conservatives won only 318 seats at the 2017 general election, but went on to form a confidence and supply deal with the Democratic Unionist Party (DUP) who got 10 seats in the House of Commons, allowing the Conservative Party to remain in government. The Conservatives won a majority government in 2019, taking 365 seats and forming the first majority government since 2015\u201317. The party won 121 seats at the 2024 general election, making it the second-largest group in the House of Commons.\nThe Conservative Party can trace its origin back to 1662, with the Court Party and the Country Party being formed in the aftermath of the English Civil War. The Court Party soon became known as the Tories, a name that has stuck despite the official name being 'Conservative'. The term \"Tory\" originates from the Exclusion Crisis of 1678\u20131681 \u2013 the Whigs were those who supported the exclusion of the Roman Catholic Duke of York from the thrones of England, Ireland and Scotland, and the Tories were those who opposed it. Generally, the Tories were associated with lesser gentry and the Church of England, while Whigs were more associated with trade, money, larger land holders (or \"land magnates\"), expansion and tolerance of Catholicism. The Rochdale Radicals were a group of more extreme reformists who were also heavily involved in the cooperative movement. They sought to bring about a more equal society, and are considered by modern standards to be left-wing.\nAfter becoming associated with repression of popular discontent in the years after 1815, the Tories underwent a fundamental transformation under the influence of Robert Peel, himself an industrialist rather than a landowner, who in his 1834 \"Tamworth Manifesto\" outlined a new \"Conservative\" philosophy of reforming ills while conserving the good.\nThough Peel's supporters subsequently split from their colleagues over the issue of free trade in 1846, ultimately joining the Whigs and the Radicals to form what would become the Liberal Party, Peel's version of the party's underlying outlook was retained by the remaining Tories, who adopted his label of Conservative as the official name of their party.\nThe Conservatives were in government for eighteen years between 1979 and 1997, under the leadership of the first-ever female prime minister, Margaret Thatcher, and former chancellor of the exchequer John Major (1990\u201397). Their landslide defeat at the 1997 general election saw the Conservative Party lose over half their seats gained in 1992, and saw the party re-align with public perceptions of them. The Conservatives lost all their seats in both Scotland and Wales, and was their worst defeat since 1906.\nIn 2008, the Conservative Party formed a pact with the Ulster Unionist Party (UUP) to select joint candidates for European and House of Commons elections; this angered the DUP as by splitting the Unionist vote, republican parties will be elected in some areas.\nAfter thirteen years in opposition, the Conservatives returned to power as part of a coalition agreement with the Liberal Democrats in 2010, going on to form a majority government in 2015. David Cameron resigned as prime minister in July 2016, which resulted in the appointment of the country's second female prime minister, Theresa May. The Conservative Party is the only party in the history of the United Kingdom to have been governed by a female prime minister. In 2019, Boris Johnson was appointed prime minister after May stepped down during Brexit negotiations. At one point during 2019 his party had a parliamentary minority for a short period after he ejected a large number of party members, of which some were subsequently allowed to return for the 2019 General election. Following the election the Tories returned with a majority government under Johnson.\nHistorically, the party has been the mainland party most pre-occupied by British unionism, as attested to by the party's full name, the Conservative and Unionist Party. This resulted in the merger between the Conservatives and Joseph Chamberlain's Liberal Unionist Party, composed of former Liberals who opposed Irish home rule. The unionist tendency is still in evidence today, manifesting sometimes as a scepticism or opposition to devolution, firm support for the continued existence of the United Kingdom in the face of movements advocating independence from the UK, and a historic link with the cultural unionism of Northern Ireland.\nLabour.\nThe Labour Party won the largest number of seats in the House of Commons at the 2024 general election, with 411 seats overall. The Party won the second-largest number of seats at the 2019 general election, with 202 seats, 60 seats less than 2017.\nThe history of the Labour Party goes back to 1900, when a Labour Representation Committee was established and changed its name to \"The Labour Party\" in 1906. After 1918, this led to the demise of the Liberal Party as the main reformist force in British politics. The existence of the Labour Party on the left-wing of British politics led to a slow waning of energy from the Liberal Party, which has consequently assumed third place in national politics. After performing poorly at the general elections of 1922, 1923 and 1924, the Liberal Party was superseded by the Labour Party as being the party of the left.\nFollowing two brief spells in minority governments in 1924 and 1929\u20131931, the party was part of the Churchill war ministry during World War II. When the war ended the Labour Party won a landslide victory at the 1945 \"khaki election\"; winning a majority for the first time ever. Throughout the rest of the twentieth century, Labour governments alternated with Conservative governments. The Labour Party suffered the \"wilderness years\" of 1951\u20131964 (three consecutive general election defeats) and 1979\u20131997 (four consecutive general election defeats). During this second period, Margaret Thatcher, who became Leader of the Conservative Party in 1975, made a fundamental change to Conservative policies, turning the Conservative Party into an economically liberal party. At the 1979 general election, she defeated James Callaghan's Labour government following the Winter of Discontent. For all of the 1980s and most of the 1990s, Conservative governments under Thatcher and her successor John Major pursued policies of privatisation, anti-trade-unionism, and, for a time, monetarism, now known collectively as Thatcherism.\nThe Labour Party elected left-winger Michael Foot as their leader in 1980, and he responded to dissatisfaction within the Labour Party by pursuing a number of radical policies developed by its grassroots members. In 1981, several centrist and right-leaning Labour MPs formed a breakaway group called the Social Democratic Party (SDP), a move which split Labour and is widely believed to have made the Labour Party unelectable for a decade. The SDP formed an alliance with the Liberal Party which contested the 1983 and 1987 general elections as a pro-European, centrist alternative to Labour and the Conservatives. Following some initial success, the SDP did not prosper (partly due to its unfavourable distribution of votes by the First-Past-the-Post electoral system), and was accused by some of splitting the Labour vote. The SDP eventually merged with the Liberal Party to form the Liberal Democrats in 1988.\nThe Labour Party was defeated in a landslide at the 1983 general election, and Michael Foot was replaced shortly thereafter by Neil Kinnock as party leader. Kinnock progressively expelled members of Militant, a left-wing group which practised entryism, and moderated many of the party's policies. Despite these changes, as well as electoral gains and also due to Kinnock's negative media image, Labour was defeated at the 1987 and 1992 \ngeneral elections, and he was succeeded by Shadow Chancellor of the Exchequer, John Smith. Shadow Home Secretary Tony Blair became Leader of the Labour Party following Smith's sudden death from a heart attack in 1994. He continued to move the Labour Party towards the \"centre\" by loosening links with the unions and continuing many of Thatcher's neoliberal policies. This, coupled with the professionalising of the party machine's approach to the media, helped Labour win a historic landslide at the 1997 general election, after eighteen consecutive years of Conservative rule. Some observers say the Labour Party had by then morphed from a democratic socialist party to a social democratic party, a process which delivered three general election victories but alienated some of its core base; leading to the formation of the Socialist Labour Party. A subset of Labour MPs stand as joint Labour and Co-operative candidates due to a long-standing electoral alliance between the Labour Party and the Co-operative Party \u2013 the political arm of the British co-operative movement. At the 2019 general election, 26 were elected.\nFollowing Tony Blair's election as leader of Labour, the part was reformed under the \"New Labour\" branding and won the 1997 election with an overall landslide victory. Under \"New Labour\", the Human Rights Act and National Minimum Wage Act were passed in 1998.\nLiberal Democrats.\nThe Liberal Democrats won the third largest number of seats at the 2024 general election, returning 72 MPs. \nThe Liberal Democrats were founded in 1988 by an amalgamation of the Liberal Party with the Social Democratic Party, but can trace their origin back to the Whigs and the Rochdale Radicals who evolved into the Liberal Party. The term 'Liberal Party' was first used officially in 1868, though it had been in use colloquially for decades beforehand. The Liberal Party formed a government in 1868 and then alternated with the Conservative Party as the party of government throughout the late-nineteenth century and early-twentieth century.\nThe Liberal Democrats are a party with policies on constitutional and political reforms, including changing the voting system for general elections (2011 United Kingdom Alternative Vote referendum), abolishing the House of Lords and replacing it with a 300-member elected Senate, introducing fixed five-year Parliaments, and introducing a National Register of Lobbyists. They also support what they see as greater fairness and social mobility. In the coalition government, the party promoted legislation introducing a pupil premium \u2013 funding for schools directed at the poorest students to give them an equal chance in life. They also supported same-sex marriage and increasing the income tax threshold to \u00a310,000, a pre-election manifesto commitment.\nIn the 2010 election, David Cameron formed a coalition government with Nick Clegg. After the 2015 elections, the Conservative government continued with a single party rather than a coalition.\nSome coalition government reforms that were proposed were for fixed term parliaments. This piece of legislation consisted of setting a five-year interval between general elections. Another piece of coalition reform that was enacted was the Scottish independence referendum. The result overall was remain.\nScottish National Party.\nThe Scottish National Party won the third-largest number of seats in the House of Commons at the 2015 general election, winning 56 MPs from the 59 constituencies in Scotland having won 50% of the popular vote. This was an increase of 50 MPs on the result achieved in 2010. At the 2017 general election, the SNP won 35 seats, a net loss of 21 seats. At the 2019 general election, the SNP won 48 seats, a net gain of 13 seats. At the 2024 general election, the SNP won 9 seats, a net loss of 38 seats.\nThe SNP has enjoyed parliamentary representation continuously since 1967. Following the 2007 Scottish parliamentary elections, the SNP emerged as the largest party with 47 MSPs and formed a minority government with Alex Salmond as First Minister. After the 2011 Scottish parliamentary election, the SNP won enough seats to form a majority government, the first time this had ever happened since devolution was established in 1999. It won 64 of 129 seats in the Scottish Parliament in the 2021 Scottish Parliament election and currently runs a minority government in Scotland.\nMembers of the Scottish National Party and Plaid Cymru work together as a single parliamentary group following a formal pact signed in 1986. This group currently has 13 MPs.\nNorthern Ireland parties.\nThe Democratic Unionist Party (DUP) had 5 MPs elected at the 2024 general election. Founded in 1971 by Ian Paisley, it has grown to become the larger of the two main unionist political parties in Northern Ireland. Sinn F\u00e9in MPs had 7 MPs elected at the 2019 election, but Sinn F\u00e9in MPs traditionally abstain from the House of Commons and refuse to take their seats in what they view as a \"foreign\" parliament. The unionist parties Ulster Unionist Party, Traditional Unionist Voice, crosscommunity Alliance Party and nationalist party SDLP also have Commons representation.\nPlaid Cymru.\nPlaid Cymru has enjoyed parliamentary representation continuously since 1974 and had 4 MPs elected at the 2019 general election, though one was suspended. Following the 2007 Welsh Assembly elections, they joined Labour as the junior partner in a coalition government, but have fallen down to the third-largest party in the Assembly after the 2011 Assembly elections, and have become an opposition party.\nOther parliamentary parties.\nThe Green Party of England and Wales had a single MP, Caroline Lucas, from 2010 until the 2024 UK general election (the party previously had an MP in 1992; Cynog Dafis, Ceredigion, who was elected on a joint Plaid Cymru/Green Party ticket). In the 2024 UK general election the Greens won four seats. It also has three seats on the London Assembly and over 800 local councillors as of May 2024.\nThe Brexit Party was founded in January 2019, with leader Nigel Farage (former retired UKIP leader). It initially had 14 MEPs, all of whom had been elected as members of UKIP. In the 2019 European Parliament election in the United Kingdom, it returned 29 MEPs. The MEPs were elected representatives of the party until 11pm on 31 January 2020 when the UK left the European Union and the position of British MEPs was subsequently abolished. It was reconstituted into the Reform Party. Reform won 5 seats in the 2024 UK general election. \nThere are usually a small number of independent politicians in parliament with no party allegiance. In modern times, this has usually occurred when a sitting member leaves their party, and some such MPs have been re-elected as independents. Between 1950 and 2023, only two new members were elected as independents without having ever stood for a major party:\nIn the 2024 UK general election six independents were elected.\nNon-Parliamentary political parties.\nOther political parties exist, but struggle to return MPs to Parliament.\nThe UK Independence Party (UKIP) had one MP and 24 seats in the European Parliament as well as a number of local councillors. UKIP also had a MLA in the Northern Ireland Assembly. UKIP had become an emerging alternative party among some voters, gaining the third-largest share of the vote in the 2015 general election and the largest share of the vote of any party (27%) in the 2014 European elections. In 2014 UKIP gained its first ever MP following the defection and re-election of Douglas Carswell in the 2014 Clacton by-election. They campaign mainly on issues such as reducing immigration and EU withdrawal. They no longer have any MPs.\nThe Respect party, a left-wing group that came out of the anti-war movement had a single MP, George Galloway from 2005 to 2010, and again between 2012 and 2015.\nChange UK was a political party formed and disbanded in 2019. It had five MPs, four of whom were elected as Labour MPs, and one as Conservative MPs.\nFollowing the 2021 Scottish Parliament election the Scottish Greens have 8 MSPs in the Scottish Parliament and are the junior partner in the SNP/Green coalition. They also 35 local councillors.\nThe Green Party in Northern Ireland has previously had MLAs in the Northern Ireland Assembly. They currently have 8 local councillors.\nThe Scottish Socialist Party (SSP) won its first seat in the Scottish Parliament in the 1999 Scottish Parliament election. In the 2003 Scottish Parliament election the party increased their number of seats to 6. The party built up its support through opposing the war in Iraq and fighting for policies such as free school meals and an end to prescription charges. In the 2007 Scottish Parliament election it lost all of its MSPs but remains politically active and continues to contest elections.\nThe British National Party (BNP) became the official opposition in the 2006 Barking and Dagenham Council election, won a seat in the 2008 London Assembly election, two seats in the 2009 European elections, and received the fifth-highest share of votes in the 2010 general election. At their peak they had 58 local councillors. However, the early 2010s saw the BNP's support collapse and became fractured, resulting in them losing all elected representation by 2018.\nThe British Democratic Party (BDP) was founded in 2013 by Andrew Brons, one of the British National Party's two MEPs. In 2022, following the collapse of the BNP, a plethora of prominent ex-BNP members rapidly began coalescing around the British Democrats. It is currently the only far-right UK political party with any elected representation.\nThe Aspire Party has 24 out of the 45 seats in the Tower Hamlets council.\nThe Women's Equality Party (WEP) was founded in 2015. The party gained its first elected representation in the 2019 United Kingdom local elections, winning one local councillor seat on Congleton Town Council. The party has no other elected representation at any other level of governance.\nThe Libertarian Party was founded in 2008 and has contested several local elections and parliamentary constituencies. It has no elected representatives at any level of governance.\nThe English Democrats was founded in 2002 and advocates England having its own parliament. The party's candidate was elected mayor of Doncaster in 2009, before resigning from the party in February 2013.\nOther parties include: the Socialist Labour Party (UK), the Socialist Party of Great Britain, the Communist Party of Britain, the Socialist Party (England and Wales), the Socialist Workers Party, the Liberal Party, Mebyon Kernow (a Cornish nationalist party) in Cornwall, the Yorkshire Party in Yorkshire, and the National Health Action Party. The Pirate Party UK existed from 2009 to 2020, before being relaunched in 2023.\nSeveral local parties contest only within a specific area, a single county, borough or district. Examples include the Better Bedford Independent Party, which was one of the dominant parties in Bedford Borough Council and led by Bedford's former mayor, Frank Branston. The most notable local party is Health Concern, which controlled a single seat in the British Parliament from 2001 to 2010.\nThe Jury Team, launched in March 2009 and described as a \"non-party party\", is an umbrella organisation seeking to increase the number of independent MPs.\nThe Official Monster Raving Loony Party (OMRLP) was founded in 1983. The OMRLP are distinguished by having a deliberately bizarre manifesto, which contains things that seem to be impossible or too absurd to implement \u2013 usually to highlight what they see as real-life absurdities. It is effectively regarded as a satirical political party.\n2015 to 2019.\nAfter winning the largest number of seats and votes in the 2015 general election, the Conservatives under David Cameron, remained ahead of the Labour Party, led by Jeremy Corbyn since September 2015. The SNP maintained its position in Scotland, the party was just short of an overall majority at the Scottish parliamentary elections in May 2016.\nHowever, a turbulent referendum on the United Kingdom's membership of the European Union, called for by David Cameron, led to his resignation, the appointment of a new prime minister Theresa May, and divided opinion on Europe amongst the party.\nIn addition, the EU referendum campaign plunged the Labour Party into crisis and resulted in a motion of no confidence in the party leader Jeremy Corbyn being passed by the party's MPs in a 172\u201340 vote, which followed a significant number of resignations from the Shadow Cabinet. This led to a leadership election which began with Angela Eagle, the former Shadow First Secretary of State and Shadow Secretary of State for Business, Innovation and Skills who eight days later withdrew from the leadership race, to support Owen Smith, the former Shadow Secretary of State for Work and Pensions. This was won by Jeremy Corbyn with an increased majority.\nFollowing the vote to leave the European Union, Nigel Farage offered his own resignation as leader, something he had campaigned for since 1992. A leadership contest also took place in the Green Party, which led to the joint election on 2 September 2016 of Jonathan Bartley and Caroline Lucas as co-leaders, who took over the role in a job-share arrangement. Lucas, was previously leader until 2010 and is the party's only MP. Strategic cross-party alliances have been initiated, including a \"progressive alliance\" and a \"Patriotic Alliance\", as proposed by UKIP donor Arron Banks.\nIn 2017, the prime minister, Theresa May, called a general election. She hoped to increase the conservative majority to diffuse party opposition to her deal to leave the EU. In the election, the conservatives lost seats and the Labour party, under Jeremy Corbyn, gained 30 seats. This led to a minority conservative government supported by the DUP.\nIn July 2019, Boris Johnson won the leadership of the conservative party following the resignation of May. He became the prime minister by default.\nIn August 2019, Prime Minister Boris Johnson requested the monarch, Queen Elizabeth II, to prorogue the British parliament. Although this measure is common for incoming governments to allow time to prepare the Queen's speech, the move caused great controversy as it was announced to last 23 days instead of the usual 4 or 5 days. It would end the current session of the Parliament that had been running for 2 years and prevent further parliamentary debate. The government stated that it was nothing to do with Brexit and that there would still be \"ample time\" for debate before Brexit happens. Opponents believed that parliament had been suspended to force through a no-deal Brexit and prevent parliament from being able to thwart the government's plan. Others argued that it facilitated the Brexit negotiations by forcing the EU to modify the current proposed deal. The move is unprecedented in British politics and caused debate in the media, an attempt to stop it in the Scottish Court of Session, an attempt by ex-prime minister John Major and others to stop it in the English High Court and in the High Court in Northern Ireland. It was reported by many media sources that the move takes the UK one more step towards a full dictatorship from its current status of 'elective dictatorship'. The legality of the suspension of parliament was tested in courts in England and Scotland. The case was appealed to the Supreme Court of the United Kingdom. On 24 September, it ruled unanimously that the prorogation was both justiciable and unlawful. The prorogation was quashed and deemed \"null and of no [legal] effect\". Parliament resumed the next day.\nOn the return of parliament the government lost its when Conservative MP Phillip Lee crossed the floor of the house to join the Liberal Democrats. This meant that the combined votes of the Conservative and DUP MPs amounted to one less than the combined votes of opposition parties. The government of Boris Johnson then lost a vote, 301 to 328, giving control of the agenda of the house to the MPs, removing the control the government had over the introduction of new laws. The 21 Conservative MPs who voted against their own government had the whip removed by Number 10, removing them from the party. This included long-standing members of the party. Johnson called for a general election and following a few attempts succeeded in getting a vote approving an election through parliament.\nCurrent political landscape.\nIn the December 2019 general election, the Conservative Party, led by Boris Johnson, won a large overall majority. Jeremy Corbyn resigned as leader of the Labour Party. Jo Swinson resigned as Lib Dem leader after losing her own seat.\nOn 20 December 2019, the Brexit withdrawal agreement was passed. The UK left the EU on 31 January 2020 at 11\u00a0p.m. GMT and entered a transition period, set to finish on 31 December 2020.\nIn January 2020, the Labour Party began the process of electing a new leader. On 4 April 2020, Keir Starmer was elected leader of the Labour Party with 56.2% of the vote in the first round.\nIn October 2020, Corbyn was suspended from the Labour Party over his comments about antisemitism. According to \"The Washington Post\":\nCorbyn's ouster from a party he led in the last two national elections, in 2019 and 2017, was a stunning rebuke and mark him now as a near-outcast, at least temporarily. The suspension also shines light on a long-running feud within Europe's largest democratic socialist party over its very soul, as hard-left \"Corbynistas\" pushing for radical change duke it out with a more moderate wing more ideologically aligned with Tony Blair, the centrist former Labour prime minister.\nThe present dispute within the Labour party is likely causing the leftist political coalition to further fragment since the catastrophic result in 2019. Polling generally indicates that \"at present\" (August 2021) Labour has lost significant portions of its vote share to the Green party and the Liberal Democrats. At Labour Conference 2021, several showdowns between the left and right of the party are expected to take place. This includes but is not limited to: a motion to give members power to approve or reject decisions over the Labour whip within the PLP, a potential rejection of the pro-Starmer interim General Secretary David Evans by unions and members alike, a debate over PR and a significant debate over the loss of members and their subscription fees since Corbyn's expulsion which has left the party in a dire state regarding its activist and financial bases.\nThe SNP and the Scottish Greens won the right to form a Scottish coalition government in May 2021. The precise arrangement is loose and allows the Greens freedom to criticise official SNP policy on key areas of disagreement. However, it provides FM Nicola Sturgeon with a mandate to call for a new independence referendum after the failed one in 2014. Proponents of a new referendum particularly cite Brexit as changing the political situation, thus leading Scots to be more pro-independence than in 2014. As an issue, Scottish independence is known to cross-cut across party lines, with many Scottish Labour voters in particular being favourable to the prospect of independence.\nIn 2022, the Democracy Index rated the United Kingdom as a \"full democracy\" ranking 18th worldwide with an overall score of 8.28 out of a maximum of 10. The V-Dem Democracy Indices ranked United Kingdom 22nd worldwide on electoral democracy.\nMembership.\nAll political parties have membership schemes that allow members of the public to actively influence the policy and direction of the party to varying degrees, though particularly at a local level. Membership of British political parties is around 1% of the British electorate, which is lower than in all European countries except for Poland and Latvia. Overall membership to a political party has been in decline since the 1950s. In 1951, the Conservative Party had 2.2 million members, and a year later in 1952 the Labour Party reached their peak of 1 million members (of an electorate of around 34 million).\nThe table below details the membership numbers of political parties that have more than 5,000 members.\nNo data could be collected for the four parties of Northern Ireland: the DUP, UUP, SDLP, and Sinn F\u00e9in. However, in January 1997, it was estimated that the UUP had 10,000 \u2013 12,000 members, and the DUP had 5,000 members.\nIn December 2020, the UK Independence Party had 3,888 members.\nLocal government.\nThe UK is divided into a complex system of local governance.\nFormer European Union membership.\nThe United Kingdom first joined the then European Communities in January 1973 by the then Conservative Prime Minister Edward Heath, and remained a member of the European Union (EU) that it evolved into; British citizens, and other EU citizens resident in the UK, between 1979 and 2019 elected members to represent them in the European Parliament in Brussels and Strasbourg.\nThe UK's membership in the Union has been a major topic of debate over the years and has been objected to over questions of sovereignty, and in recent years there have been divisions in both major parties over whether the UK should form greater ties within the EU, or reduce the EU's supranational powers. Opponents of greater European integration are known as \"Eurosceptics\", while supporters are known as \"Europhiles\". Division over Europe is prevalent in both major parties, although the Conservative Party is seen as most divided over the issue, both whilst in Government up to 1997 and after 2010, and between those dates as the opposition. However, the Labour Party is also divided, with conflicting views over British adoption of the euro whilst in Government (1997\u20132010).\nBritish nationalists have long campaigned against European integration. The strong showing of the eurosceptic UK Independence Party (UKIP) since the 2004 European Parliament elections has shifted the debate over UK relations with the EU.\nIn March 2008, Parliament decided to not hold a referendum on the ratification of the Lisbon Treaty, signed in December 2007. This was despite the Labour government promising in 2004 to hold a referendum on the previously proposed Constitution for Europe.\nOn 23 June 2016, the United Kingdom voted to leave the European Union in a referendum. After the referendum, it was debated as to how and when the UK should leave the EU. On 11 July 2016, the Cabinet Office Minister, John Penrose failed to deliver a final answer on whether it would be at the disposal of the prime minister and one of the Secretaries of State, through the royal prerogative, or of Parliament, through primary legislation.\nIn October 2016 the Conservative prime minister, Theresa May, announced that Article 50 would be invoked by \"the first quarter of 2017\". On 24 January 2017 the Supreme Court ruled in the Miller case by a majority that the process could not be initiated without an authorising act of parliament, but unanimously ruled against the Scottish government's claim in respect of devolution that they had a direct say in the decision to trigger Article 50. Consequently, the European Union (Notification of Withdrawal) Act 2017 empowering the prime minister to invoke Article 50 was passed and enacted by royal assent in March 2017.\nInvocation of Article 50 by the United Kingdom government occurred on 29 March 2017, when Sir Tim Barrow, the Permanent Representative of the United Kingdom to the European Union, formally delivered by hand a letter signed by Prime Minister Theresa May to Donald Tusk, the president of the European Council in Brussels. The letter also contained the United Kingdom's intention to withdraw from the European Atomic Energy Community (EAEC or Euratom). This meant that the UK would cease to be a member of the EU on 30 March 2019, unless an extension to negotiations was agreed upon by the UK and EU.\nThe leaving date was subsequently revised by agreement with the EU to be 31 October 2019. This led to a change of prime minister who promised to leave the EU on this date either with a revised deal or with no-deal.\nThe UK withdrew from the EU at 23.00 GMT on 31 January 2020, beginning a transition period that was set to end on 31 December 2020. During the 11-month transition period, the UK and EU negotiated their future relationship which resulted in the EU\u2013UK Trade and Cooperation Agreement which was agreed on 24 December 2020 just days before the end of the transition period. The UK ended the transition period which ended the incorporation of European Union law into UK law, and ended its membership of the EU Customs Union, and the European single market at 23:00 GMT on 31 December 2020.\nInternational organisation participation.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nCoalitions and Alliances.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "31727", "revid": "50971855", "url": "https://en.wikipedia.org/wiki?curid=31727", "title": "Economy of the United Kingdom", "text": "The United Kingdom has a highly developed social market economy. From 2017 to 2025 it has been the sixth-largest national economy in the world measured by nominal gross domestic product (GDP), tenth-largest by purchasing power parity (PPP), and about 21st by nominal GDP per capita,&lt;ref name=\"pwc.com/gx-projections\"&gt;&lt;/ref&gt; constituting 3.38% of world GDP and 2.13% by purchasing power parity (PPP).\nThe United Kingdom has one of the most globalised economies and comprises England, Scotland, Wales and Northern Ireland. In 2022, the United Kingdom was the fifth-largest exporter of goods and services in the world and the fourth-largest importer. It also had the fourth-largest outward foreign direct investment, and the fifteenth-largest inward foreign direct investment. In 2022, the United Kingdom's trade with the European Union accounted for 42% of the country's exports and 48% of its total imports. The United Kingdom has a highly efficient and strong social security system, which comprises roughly 24.5% of GDP.\nThe service sector dominates, contributing 82% of GDP; the financial services industry is particularly important, and London is the second-largest financial centre in the world. Edinburgh was ranked 17th in the world, and 6th in Europe for its financial services industry in 2021. The United Kingdom's technology sector is valued at US$1 trillion, third behind the United States and China. The aerospace industry in the United Kingdom is the second-largest national aerospace industry. Its pharmaceutical industry, the tenth-largest in the world, plays an important role in the economy. Of the world's 500 largest companies, 17 are headquartered in the UK. The economy is boosted by North Sea oil and gas production; its reserves were estimated at 2.5\u00a0billion barrels in 2021, although it has been a net importer of oil since 2005. There are significant regional variations in prosperity, with South East England and North East Scotland being the richest areas per capita. The size of London's economy makes it the wealthiest city by GDP per capita in Europe. In 2022, the UK spent around 2.8% of GDP on research and development.\nIn the 18th century, Britain was the first nation to industrialise. During the 19th\u00a0century, through its expansive colonial empire and technological superiority, Britain had a preeminent role in the global economy, accounting for 9.1% of the world's GDP in 1870. The Second Industrial Revolution was also taking place rapidly in the United States and the German Empire; this presented an increasing economic challenge for the UK, leading into the 20th century. The cost of fighting both the First and Second World Wars further weakened the UK's relative position. Despite a relative decline in its global dominance, in the 21st century the UK retains the ability to project significant power and influence around the world. During the Great Recession of 2008, the UK economy suffered a significant decline, followed by a period of weak growth and stagnation.\nGovernment involvement is primarily exercised by His Majesty's Treasury, headed by the Chancellor of the Exchequer, and the Department for Business and Trade. Since 1979, management of the economy has followed a broadly laissez-faire approach. The Bank of England is the UK's central bank, and since 1997 its Monetary Policy Committee has been responsible for setting interest rates, quantitative easing, and forward guidance.\nHistory.\n1945 to 1979.\nThe Second World War net loss to UK national wealth amounted to 18.6% (\u00a34.59 billion) of the nation's pre-war wealth (\u00a324.68 billion) at 1938 prices. After the war, a new Labour government fully nationalised the Bank of England, civil aviation, telephone networks, railways, gas, electricity, and the coal, iron and steel industries, affecting 2.3 million workers. Post-war, the United Kingdom enjoyed a long period without a major recession; there was a rapid growth in prosperity in the 1950s and 1960s, with unemployment staying low and not exceeding 3.5% until the early 1970s. The annual rate of growth between 1960 and 1973 averaged 2.9% although this figure was far behind France, West Germany and Italy.\nGradual deindustrialisation meant the closure of operations in mining, heavy industry, and manufacturing, resulting in the loss of highly paid working-class jobs. The UK's share of global manufacturing output had risen from 9.5% in 1830, during the Industrial Revolution, to 22.9% in the 1870s. It fell to 13.6% by 1913, 10.7% by 1938, and 4.9% by 1973. Overseas competition, lack of innovation, trade unionism, the welfare state, loss of the British Empire, and cultural attitudes have all been put forward as explanations. It reached crisis point in the 1970s against the backdrop of a worldwide energy crisis, high inflation, and a dramatic influx of low-cost manufactured goods from Asia.\nDuring the 1973 oil crisis, which saw oil prices quadruple, the 1973\u20131974 stock market crash, and the Secondary banking crisis of 1973\u20131975, the British economy fell into the 1973\u20131975 recession and the government of Edward Heath was ousted by the Labour Party under Harold Wilson, which had previously governed from 1964 to 1970. Wilson formed a minority government in March 1974 after the general election on 28 February ended in a hung parliament. Wilson secured a three-seat overall majority in a second election in October that year. The UK recorded weaker growth than many other European nations in the 1970s; even after the recession, the economy was blighted by rising unemployment and double-digit inflation, which exceeded 20% more than once and was rarely below 10% after 1973.\nDuring the 1976 sterling crisis, the UK was forced to apply for a loan of \u00a32.3\u00a0billion from the International Monetary Fund. Denis Healey, then Chancellor of the Exchequer, was required to implement public spending cuts and other economic reforms in order to secure the loan, and for a while the British economy improved, with growth of 4.3% in early 1979. Following the discovery of large North Sea oil reserves, the UK became a net exporter of oil by the end of the 1970s, which contributed to a massive appreciation of the pound, making exports in general more expensive and imports cheaper. Oil prices doubled between 1979 and 1980, further reducing manufacturing profitability. Following the Winter of Discontent, when the UK was hit by numerous public sector strikes, the government of James Callaghan lost a vote of no confidence in March 1979. This triggered the general election on 3 May 1979 which resulted in Margaret Thatcher's Conservative Party forming a new government. In retrospect, the 1970s is considered to have been a \"lost decade\" for the UK economy.\n1979 to 1997.\nA new period of neo-liberal economics began with this election. During the 1980s, many state-owned industries and utilities were privatised, taxes cut, trade union reforms passed and markets deregulated. GDP fell by 5.9% initially, but growth subsequently returned and rose to an annual rate of 5% at its peak in 1988, one of the highest rates of any country in Europe.\nThatcher's modernisation of the economy was far from trouble-free; her battle with inflation, which in 1980 had risen to 21.9%, resulted in a substantial increase in unemployment from 5.3% in 1979 to over 10.4% by the start of 1982, peaking at nearly 11.9% in 1984 \u2013 a level not seen in Britain since the Great Depression. The rise in unemployment coincided with the early 1980s global recession, after which UK GDP did not reach its pre-recession rate until 1983. In spite of this, Thatcher was re-elected in June 1983 with a landslide majority. Inflation had fallen to 3.7%, while interest rates were relatively high at 9.56%. The increase in unemployment was largely due to the government's economic policy which resulted in the closure of outdated factories and coal pits. Manufacturing in England and Wales declined from around 38% of jobs in 1961 to around 22% in 1981. This trend continued for most of the 1980s, with newer industries and the service sector enjoying significant growth. Many jobs were also lost as manufacturing became more efficient and fewer people were required to work in the sector. Unemployment had fallen below 3\u00a0million by the time of Thatcher's third successive election victory in June 1987; and by the end of 1989 it was down to 1.6\u00a0million.\nBritain's economy slid into another global recession in late 1990; it shrank by a total of 6% from peak to trough, and unemployment increased from around 6.9% in spring 1990 to nearly 10.7% by the end of 1993. However, inflation dropped from 10.9% in 1990 to 1.3% three years later. The subsequent economic recovery was extremely strong, and unlike after the early 1980s recession, the recovery saw a rapid and substantial fall in unemployment, which was down to 7.2% by 1997, although the popularity of the Conservative government had failed to improve with the economic upturn. The government won a fourth successive election in 1992 under John Major, who had succeeded Thatcher in November 1990, but soon afterwards came Black Wednesday, which damaged the Conservative government's reputation for economic competence, and from that stage onwards, the Labour Party was ascendant in the opinion polls, particularly in the immediate aftermath of Tony Blair's election as party leader in July 1994 following the sudden death of his predecessor John Smith.\nDespite two recessions, wages grew consistently by around 2% per year in real terms from 1980 until 1997, and continued to grow until 2008.\n1997 to 2009.\nIn May 1997, Labour, led by Tony Blair, won the general election following 18 years of Conservative government. The Labour Government inherited a strong economy with low inflation, falling unemployment, and a current account surplus. Blair ran on a platform of New Labour which was characterised largely by the continuation of neo-liberal economic policies, but also supporting a strong welfare state. In Britain it was largely viewed as a combination of socialist and capitalist policies, being dubbed 'Third Way'. Four days after the election, Gordon Brown, the new Chancellor of the Exchequer, gave the Bank of England the freedom to control monetary policy, which until then had been directed by the government.\nDuring Blair's 10 years in office there were 40 successive quarters of economic growth, lasting until the second quarter of 2008. GDP growth, which had briefly reached 4% per year in the early 1990s, gently declining thereafter, was relatively anaemic compared to prior decades, such as the 6.5% per year peak in the early 1970s, although growth was smoother and more consistent. Annual growth rates averaged 2.68% between 1992 and 2007, with the finance sector accounting for a greater part than previously. The period saw one of the highest GDP growth rates of any developed economy and the strongest of any European nation. At the same time, household debt rose from \u00a3420\u00a0billion in 1994 to \u00a31\u00a0trillion in 2004 and \u00a31.46\u00a0trillion in 2008 \u2013 more than the entire GDP of the UK. Total government, business, household and financial sector debt reached 469% of GDP in 2008. For historical comparison, total UK debt was 286% of GDP in 1947, after the Second World War, and 110% in 1980.\nThis extended period of growth ended in Q2 of 2008 when the United Kingdom entered the Great Recession brought about by the 2008 financial crisis. The UK was particularly vulnerable to the crisis because its financial sector was the most highly leveraged of any major economy. Beginning with the collapse of Northern Rock, which was taken into public ownership in February 2008, other banks had to be partly nationalised. The Royal Bank of Scotland Group, at its peak the fifth-largest bank in the world by market capitalisation, was effectively nationalised in October 2008. By mid-2009, HM Treasury had a 70.33% controlling shareholding in RBS, and a 43% shareholding, through the UK Financial Investments Limited, in Lloyds Banking Group. The Great Recession, as it came to be known, saw unemployment rise from just over 1.6\u00a0million in January 2008 to nearly 2.5\u00a0million by October 2009.\nIn August 2008 the IMF warned that the country's outlook had worsened due to a twin shock: financial turmoil and rising commodity prices. Both developments harmed the UK more than most developed countries, as it obtained revenue from exporting financial services while running deficits in goods and commodities, including food. In 2007, the UK had the world's third largest current account deficit, due mainly to a large deficit in manufactured goods. In May 2008, the IMF advised the UK government to broaden the scope of fiscal policy to promote external balance. The UK's output per hour worked was on a par with the average for the \"old\" EU-15 countries.\n2009 to 2020.\nIn March 2009, the Bank of England (BoE) cut interest rates to a then-historic low of 0.5% and began quantitative easing (QE) to boost lending and shore up the economy. The UK exited the Great Recession in Q4 of 2009 having experienced six consecutive quarters of negative growth, shrinking by 6.03% from peak to trough, making it the longest recession since records began and the deepest since World War\u00a0II. Support for Labour slumped during the recession, and the general election of 2010 resulted in a coalition government being formed by the Conservatives and the Liberal Democrats.\nIn 2011, household, financial, and business debts stood at 420% of GDP in the UK. As the world's most indebted country, spending and investment were held back after the recession, creating economic malaise. However, it was recognised that government borrowing, which rose from 52% to 76% of GDP, had helped to avoid a 1930s-style depression. Within three years of the general election, government cuts aimed at reducing the budget deficit had led to public sector job losses well into six figures, but the private sector enjoyed strong jobs growth.\nThe ten years following the Great Recession were characterised by extremes. In 2015, employment was at its highest since records began, and GDP growth had become the fastest in the Group of Seven (G7) and Europe, but workforce productivity was the worst since the 1820s, with any growth attributed to a fall in working hours. Output per hour worked was 18% below the average for the rest of the G7. Real wage growth was the worst since the 1860s, and the Governor of the Bank of England described it as a lost decade. Wages fell by 10% in real terms in the eight years to 2016, whilst they grew across the OECD by an average of 6.7%. For 2015 as a whole, the current account deficit rose to a record high of 5.2% of GDP (\u00a396.2bn), the highest in the developed world. In Q4 2015, it exceeded 7%, a level not witnessed during peacetime since records began in 1772. The UK relied on foreign investors to plug the shortfall in its balance of payments. Homes had become less affordable, a problem exacerbated by QE, without which house prices would have fallen by 22%, according to the BoE's own analysis. The Great Recession had a long term effect on UK's growth; GDP growth slowed from an annual average of 3.0% between 1993 and 2007 to 1.5% between 2009 and 2023, while labour productivity growth slowed from an annual average of 1.9% between 1993 and 2008 to 0.4% between 2008 and 2023.\nA rise in unsecured household debt added to questions over the sustainability of the economic recovery in 2016. The BoE insisted there was no cause for alarm, despite having said two years earlier that the recovery was \"neither balanced nor sustainable\". Following the UK's 2016 decision to leave the European Union, the BoE cut interest rates to a new historic low of 0.25% for just over a year. It also increased the amount of QE since the start of the Great Recession to \u00a3435bn. By Q4 2018 net borrowing in the UK was the highest in the OECD at 5% of GDP. Households had been in deficit for an unprecedented nine quarters in a row. Since the Great Recession, the country was no longer making a profit on its foreign investments.\n2020 to present.\nIn March 2020, in response to the COVID-19 pandemic, a temporary ban was imposed on non-essential business and travel in the UK. The BoE cut the interest rate to 0.1%. Economic growth had been weak before the crisis, with zero growth in Q4 2019. By May, 23% of the British workforce was furloughed (temporarily laid off). Government schemes were launched to help affected workers. In the first half of 2020, GDP shrank by 22.6%, the deepest recession in UK history and worse than any other G7 or European country. During 2020 the BoE purchased \u00a3450 billion of government bonds, taking the amount of quantitative easing since the start of the Great Recession to \u00a3895 billion. Overall, GDP shrank by 9.9% in 2020, making it the worst contraction since the Great Frost paralysed the economy in 1709.\nIn 2021 consumer price inflation (CPI) began rising sharply due to higher energy and transport costs. With annual inflation approaching 11%, the BoE gradually increased the base rate to 2.25% during the first nine months of 2022. The UK was not alone: global inflation rates were the highest in 40 years owing to the pandemic and Russia's invasion of Ukraine. The UK had the highest domestic electricity prices and amongst the highest gas prices in Europe, contributing to a cost of living crisis. In February 2022 the BoE began quantitative tightening (a reversal of QE) by not renewing mature government bonds and in November started offloading bonds to private investors, signalling the end to an era of easy borrowing. In October 2022 year-on-year CPI peaked at 11.1%, the worst for 41 years.\nIn August 2023, the ONS announced that nominal GDP had surpassed its pre-COVID-19 size in the final quarter of 2021. GDP per capita rose by 0.7% a year on average from 2007 to 2024, in contrast with 2.5% during the global credit bubble from 1990 to 2007. As of early 2024, average wages in the UK adjusted by inflation were roughly the same as in 2008. As of July 2025, the unemployment rate was 4.7%, up from 3.8% in February 2024. In 2024, there were 11 million economically inactive people (those aged 16 to 64 who are not in the labour force) in the UK.\nGovernment spending and economic management.\nGovernment involvement in the economy is primarily exercised by HM Treasury, headed by the Chancellor of the Exchequer. In recent years, the UK economy has been managed in accordance with principles of market liberalisation and low taxation and regulation. Since 1997, the Bank of England's Monetary Policy Committee, headed by the Governor of the Bank of England, has been responsible for setting interest rates at the level necessary to achieve the overall inflation target for the economy that is set by the Chancellor each year. The Scottish Government, subject to the approval of the Scottish Parliament, has the power to vary the basic rate of income tax payable in Scotland by plus or minus 3 pence in the pound, though this power has not yet been exercised.\nIn the 20-year period from 1986/87 to 2006/07 government spending in the UK averaged around 40% of GDP. In July 2007, the UK had government debt at 35.5% of GDP. As a result of the 2008 financial crisis and the Great Recession, government spending increased to a historically high level of 48% of GDP in 2009\u201310, partly as a result of the cost of a series of bank bailouts. In terms of net government debt as a percentage of GDP, at the end of June 2014 public sector net debt excluding financial sector interventions was \u00a31.305\u00a0trillion, equivalent to 77.3% of GDP. For the financial year of 2013\u20132014 public sector net borrowing was \u00a393.7\u00a0billion. This was \u00a313.0\u00a0billion higher than in the financial year of 2012\u20132013.\nTaxation in the United Kingdom may involve payments to at least two different levels of government: local government and central government (HM Revenue &amp; Customs). Local government is financed by grants from central government funds, business rates, council tax, and, increasingly, fees and charges such as those from on-street parking. Central government revenues are mainly from income tax, national insurance contributions, value added tax, corporation tax and fuel duty.\nSectors.\nThe UK's Office for National Statistics' Blue Book divides the UK economy into 10 broad categories, to list their contribution to the UK economy in terms of Gross value added and employment income (as measured by employee compensation). These are\nAgriculture.\nAgriculture in the UK is intensive, highly mechanised, and efficient by European standards. The country produces around 65% of its food needs. The self-sufficiency level was just under 50% in the 1950s, peaking at 80% in the 1980s, before declining to its present level at the turn of the 21st century.\nAgriculture added gross value of \u00a312.18 billion to the economy in 2018, and around 467,000 people were employed in agriculture, hunting, forestry and fishing. It contributes around 0.5% of the UK's national GDP. Around two-thirds of production by value is devoted to livestock, and one-third to arable crops. The agri-food sector as a whole (agriculture and food manufacturing, wholesale, catering, and retail) was worth \u00a3120 billion and accounts for 4 million jobs in the UK.\nConstruction.\nThe construction industry of the United Kingdom employed around 2.3 million people and contributed gross value of \u00a3123.2 billion to the economy in 2019.\nOne of the largest construction projects in the UK in recent years was Crossrail, costing an estimated \u00a319 billion. It was the largest construction project in Europe. Opened in 2022, it is a new railway line running east to west through London and into the surrounding area, with a branch to Heathrow Airport. The main feature of the project is construction of 42\u00a0km (26\u00a0mi) of new tunnels connecting stations in central London. High Speed 2 between London and the West Midlands is one of Europe's largest infrastructure projects. Crossrail 2 is a proposed rail route in the South East of England.\nProduction industries.\nElectricity, gas and water.\nThis sector added gross value of \u00a351.4\u00a0billion to the economy in 2018. The United Kingdom is expected to launch the building of new nuclear reactors to replace existing generators and to boost the UK's energy reserves.\nManufacturing.\nIn the 1970s, manufacturing accounted for 25 per cent of the economy. Total employment in manufacturing fell from 7.1 million in 1979 to 4.5 million in 1992 and only 2.7 million in 2016, when it accounted for 10% of the economy.\nIn 2023, the manufacturing industry was worth \u00a3451.6 billion or $588 billion to the UK economy, according to the office of national statistics.\nManufacturing has increased in 36 of the last 50 years and was twice in 2007 what it was in 1958.\nIn 2011, the UK manufacturing sector generated approximately \u00a3140.5\u00a0billion in gross value added and employed around 2.6\u00a0million people. Of the approximately \u00a316\u00a0billion invested in R&amp;D by UK businesses in 2008, approximately \u00a312\u00a0billion was by manufacturing businesses. In 2008, the UK was the sixth-largest manufacturer in the world measured by value of output.\nIn 2008, around 180,000 people in the UK were directly employed in the UK automotive manufacturing sector. In that year the sector had a turnover of \u00a352.5\u00a0billion, generated \u00a326.6\u00a0billion of exports and produced around 1.45\u00a0million passenger vehicles and 203,000 commercial vehicles. The UK is a major centre for engine manufacturing, and in 2008 around 3.16\u00a0million engines were produced in the country.\nThe aerospace industry of the UK is the second-largest aerospace industry in the world (after the United States) and the largest in Europe. The industry employs around 113,000 people directly and around 276,000 indirectly and has an annual turnover of around \u00a320\u00a0billion. British companies with a major presence in the industry include BAE Systems and Rolls-Royce (the world's second-largest aircraft engine maker). European aerospace companies active in the UK include Airbus, whose commercial aircraft, space, helicopter and defence divisions employ over 13,500 people across more than 25 UK sites.\nThe pharmaceutical industry employs around 67,000 people in the UK and in 2007 contributed \u00a38.4\u00a0billion to the UK's GDP and invested a total of \u00a33.9\u00a0billion in research and development. In 2007 exports of pharmaceutical products from the UK totalled \u00a314.6\u00a0billion, creating a trade surplus in pharmaceutical products of \u00a34.3\u00a0billion. The UK is home to GlaxoSmithKline and AstraZeneca, respectively the world's third- and seventh-largest pharmaceutical companies.\nMining, quarrying and hydrocarbons.\n \nThe Blue Book 2013 reports that this sector added gross value of \u00a331.4\u00a0billion to the UK economy in 2011. In 2007, the UK had a total energy output of 9.5 quadrillion Btus (10 exajoules), of which the composition was oil (38%), natural gas (36%), coal (13%), nuclear (11%) and other renewables (2%). In 2009, the UK produced 1.5\u00a0million barrels per day (bbl/d) of oil and consumed 1.7\u00a0million bbl/d. Production is now in decline and the UK has been a net importer of oil since 2005. As of 2010 the UK has around 3.1\u00a0billion barrels of proven crude oil reserves, the largest of any EU member state.\nIn 2009, the UK was the 13th largest producer of natural gas in the world and the largest producer in the EU. Production is now in decline and the UK has been a net importer of natural gas since 2004. In 2009 the UK produced 19.7\u00a0million tons of coal and consumed 60.2\u00a0million tons. In 2005 it had proven recoverable coal reserves of 171\u00a0million tons. It has been estimated that identified onshore areas have the potential to produce between 7\u00a0billion tonnes and 16\u00a0billion tonnes of coal through underground coal gasification (UCG). Based on current UK coal consumption, these volumes represent reserves that could last the UK between 200 and 400 years.\nThe UK is home to a number of large energy companies, including two of the six oil and gas \"supermajors\" \u2013 BP and Shell plc. The UK is also rich in a number of natural resources, including coal, tin, limestone, iron ore, salt, clay, chalk, gypsum, lead and silica.\nService industries.\nThe service sector is the dominant sector of the UK economy, and it accounted for 82% of GDP in 2023.\nCreative industries.\nThe creative industries accounted for 7% of gross value added (GVA) in 2005 and grew at an average of 6% per annum between 1997 and 2005. Key areas include London and the North West of England, which are the two largest creative industry clusters in Europe. According to the British Fashion Council, the fashion industry's contribution to the UK economy in 2014 is \u00a326\u00a0billion, up from \u00a321\u00a0billion in 2009. The UK is home to the world's largest advertising company, WPP.\nEducation, health and social work.\nAccording to The Blue Book 2013 the education sector added a gross value of \u00a384.6\u00a0billion in 2011 whilst human health and social work activities added \u00a3104.0\u00a0billion in 2011.\nIn the UK the majority of the healthcare sector consists of the state funded and operated National Health Service (NHS), which accounts for over 80% of all healthcare spending in the UK and has a workforce of around 1.7\u00a0million, making it the largest employer in Europe, and putting it amongst the largest employers in the world. The NHS operates independently in each of the four constituent countries of the UK. The NHS in England is by far the largest of the four parts and had a turnover of \u00a392.5\u00a0billion in 2008.\nIn 2007/08 higher education institutions in the UK had a total income of \u00a323\u00a0billion and employed a total of 169,995 staff. In 2007/08 there were 2,306,000 higher education students in the UK (1,922,180 in England, 210,180 in Scotland, 125,540 in Wales and 48,200 in Northern Ireland).\nFinancial and business services.\nThe UK financial services industry added gross value of \u00a3116.4\u00a0billion to the UK economy in 2011. The UK's exports of financial and business services make a significant positive contribution towards the country's balance of payments.\nLondon is a major centre for international business and commerce and is one of the three \"command centres\" of the global economy (alongside New York City and Tokyo).\nThere are over 500 banks with offices in London, and it is the leading international centre for banking, insurance, Eurobonds, transactions in foreign currencies and energy futures. London's financial services industry is primarily based in the City of London and Canary Wharf. The City houses the London Stock Exchange, the London Metal Exchange, Lloyd's of London, and the Bank of England. Canary Wharf began development in the 1980s and is now home to major financial institutions such as Barclays Bank, Citigroup and HSBC, as well as the UK Financial Services Authority. London is also a major centre for other business and professional services, and four of the six largest law firms in the world are headquartered there.\nSeveral other major UK cities have large financial sectors and related services. Edinburgh has one of the largest financial centres in Europe and is home to the headquarters of Lloyds Banking Group, NatWest Group and Standard Life. Leeds is the UK's largest centre for business and financial services outside London, and the largest centre for legal services in the UK after London.\nAccording to a series of research papers and reports published in the mid-2010s, Britain's financial firms provide sophisticated methods to launder billions of pounds annually, including money from the proceeds of corruption around the world as well as the world's drug trade, thus making the city a global hub for illicit finance. According to a Deutsche Bank study published in March 2015, Britain was attracting circa one billion pounds of capital inflows a month not recorded by official statistics, up to 40 per cent probably originating from Russia, which implies misreporting by financial institutions, sophisticated tax avoidance, and the UK's \"safe-haven\" reputation.\nHotels and restaurants.\nThe Blue Book 2013 reports that this industry added gross value of \u00a336.6\u00a0billion to the UK economy in 2011. InterContinental Hotels Group (IHG), headquartered in Denham, Buckinghamshire, is currently the world's largest hotelier, owning and operating hotel brands such as InterContinental, Holiday Inn and Crowne Plaza.\nInformal.\nA study in 2014 found that sex work and associated services added over \u00a35 billion to the economy each year.\nPublic administration and defence.\nThe Blue Book 2013 reports that this sector added gross value of \u00a370.4\u00a0billion to the UK economy in 2011.\nReal estate and renting activities.\nNotable real estate companies in the United Kingdom include British Land, Landsec and the Peel Group. The UK property market boomed for the seven years up to 2008, and in some areas property trebled in value over that period. The increase in property prices had a number of causes: low interest rates, credit growth, economic growth, rapid growth in buy-to-let property investment, foreign property investment in London and planning restrictions on the supply of new housing.\nIn England and Wales between 1997 and 2016, average house prices increased by 259%, while earnings increased by 68%. An average home cost 3.6 times annual earnings in 1997 compared to 7.6 in 2016. Rent has nearly doubled as a share of GDP since 1985, and is now larger than the manufacturing sector. In 2014, rent and imputed rent \u2013 an estimate of how much home-owners would pay if they rented their home \u2013 accounted for 12.3% of GDP.\nTourism.\nWith over 40 million visits in 2019, inbound tourism contributed \u00a328.5 billion to the British economy, although just over half of that money was spent in London, which was the third most visited city in the world (21.7 million), behind second-placed Bangkok and first-placed Hong Kong.\nThe UK's 10 most significant inbound tourism markets in 2023:\nEffects of the COVID-19 pandemic.\nThe travel restrictions and lockdowns necessitated by the pandemic negatively affected the entire hospitality/tourism section in 2020 with a 76% reduction in \"inbound tourism\" to the UK that year according to VisitBritain. The January 2021 forecast for the year indicated an estimate that visits from other nations would be up \"21% on 2020 but only 29% of the 2019 level\". Some increase was expected during 2021, slowly at first; the tourism authority concluded that the number of visits was not expected to come \"even close to normal levels\".\nThe same VisitBritain report also discussed the effects of the pandemic on domestic travel within the UK in 2020, citing a significant reduction in spending, for an estimated decline of 62% over the previous year. As of January 2021, the forecast for the year suggested that spending would increase by 79% over the previous year and that \"the value of spending will be back to 84% of 2019 levels\" by the end of 2021.\nSome of the \"COVID-19 restrictions\" on domestic travel were to be loosened on 12 April 2021 and the UK planned to begin relaxing some restrictions on travel from other nations in mid May. The latter plan became less certain as of 8 April 2021 when sources in the European Union stated on that a \"third wave of the pandemic [was sweeping] the continent\"; the B117 variant was of particular concern. Two days earlier, PM Boris Johnson had made it clear that \"We don't want to see the virus being reimported into this country from abroad\". All travel restrictions ended on 18 March 2022.\nTransport, storage and communication.\nThe transport and storage industry added a gross value of \u00a359.2\u00a0billion to the UK economy in 2011 and the telecommunication industry added a gross value of \u00a325.1\u00a0billion in the same year.\nThe UK has a total road network of with of major roads, including of motorway. The railway infrastructure, in Great Britain, is owned by Network Rail which has of railway lines, of which is open for traffic. There are a further of track in Northern Ireland, owned and operated by Northern Ireland Railways.\nThe government is to spend \u00a356\u00a0billion on a new high-speed railway line, HS2, with the first phase from London to Birmingham costing \u00a327\u00a0billion. Crossrail (later branded the Elizabeth line), which was completed and officially opened in 2022, is Europe's largest infrastructure project with a \u00a315 billion projected cost.\nNational Highways is the government-owned company responsible for trunk roads and motorways in England apart from the privately owned and operated M6 Toll.\nIn the year from February 2017 to January 2018, UK airports handled a total of 284.8\u00a0million passengers. In that period the three largest airports were London Heathrow Airport (78.0\u00a0million passengers), Gatwick Airport (45.6\u00a0million passengers) and Manchester Airport (27.8\u00a0million passengers). Heathrow, located west of the capital, has the most international passenger traffic of any airport in the world. It is the hub for the UK flag carrier British Airways and Virgin Atlantic. London's six commercial airports form the world's largest city airport system measured by passenger traffic with 171\u00a0million passengers in 2017.\nWholesale and retail trade.\nThis sector includes the motor trade, auto repairs, personal and household goods industries. The Blue Book 2013 reports that this sector added gross value of \u00a3151.8\u00a0billion to the UK economy in 2011.\nAs of 2016, high-street retail spending accounted for about 33% of consumer spending and 20% of GDP. Because 75% of goods bought in the United Kingdom are made overseas, the sector only accounts for 5.7% of gross value added to the British economy. Online sales account for 22% of retail spending in the UK, third highest in the world after China and South Korea, and double that of the United States.\nThe UK grocery market is dominated by four companies: Tesco (26.9% market share), Sainsbury's (14.8%), Asda (14.3%) and Morrisons (8.8%) in March 2023, these supermarkets are known as the \"Big Four\". However discount supermarkets such as Aldi and Lidl have grown in popularity, with Aldi's market share now worth 9.9%.\nLondon is a major retail centre and in 2010 had the highest non-food retail sales of any city in the world, with a total spend of around \u00a364.2\u00a0billion. Outside of London, Manchester and Birmingham are also major retail destinations, the UK is also home to many large out of town shopping centres like Meadowhall, away from the main high streets in town and city centres. Whilst the big international names dominate most towns and cities have streets or areas with many often quirky independent businesses. The UK-based Tesco is the fourth-largest retailer in Europe measured by turnover (after Swartz, Aldi, and Carrefour in 2019).\nCurrency.\n \nLondon is the world capital for foreign exchange trading, with a global market share of 43.1% in 2019 of the daily $6.6 trillion global turnover. The highest daily volume, counted in trillions of US dollars, is reached when New York enters the trade.\nSterling is the currency of the UK, with its main unit, the pound, represented by the symbol \"\u00a3'. The Bank of England is the central bank, responsible for issuing currency. Banks in Scotland and Northern Ireland retain the right to issue their own notes, subject to retaining enough Bank of England notes in reserve to cover the issue. Sterling is also used as a reserve currency by other governments and institutions, and is the third-largest after the US dollar and the euro.\nThe UK chose not to join the euro at the currency's launch. The government of former Prime Minister Tony Blair had pledged to hold a referendum to decide on membership should \"five economic tests\" be met. Until relatively recently there had been debate over whether or not the UK should abolish its currency and adopt the euro. In 2007 the Prime Minister, Gordon Brown, pledged to hold a public referendum based on certain tests he set as Chancellor of the Exchequer. When assessing the tests, Brown concluded that while the decision was close, the United Kingdom should not yet join the euro. He ruled out membership for the foreseeable future, saying that the decision not to join had been right for the UK and for Europe. In particular, he cited fluctuations in house prices as a barrier to immediate entry. Public opinion polls have shown that a majority of Britons have been opposed to joining the single currency for some considerable time, and this position has hardened further in the last few years. In 2005, more than half (55%) of the UK were against adopting the currency, while 30% were in favour. The possibility of joining the euro has become a non-issue since the referendum decision to withdraw from the European Union in 2016 and subsequent withdrawal in 2020.\nExchange rates.\nAverage for each year, in US dollars and euros per pound; and inversely: \u00a3 per US$ and \u20ac. (Synthetic Euro XEU before 1999). These averages conceal wide intra-year spreads. The coefficient of variation gives an indication of this. It also shows the extent to which the pound tracks the euro or the dollar. Note the effect of Black Wednesday in late 1992 by comparing the averages for 1992 and for 1993.\n&lt;templatestyles src=\"Col-begin/styles.css\"/&gt;\n For consistency and comparison purposes, coefficient of variation is measured on both the \"per \u00a3\" ratios, although it is conventional to show the forex rates as dollars per \u00a3 and \u00a3 per euro.\nEconomic variation within the United Kingdom.\nGross domestic product (2022).\nWithin the United Kingdom, England has the largest constituent economy measured by gross domestic product (GDP) according to the figures provided by the Office for National Statistics (ONS) for the year 2022, whilst Northern Ireland has the smallest, which is also in line with their respective population sizes. England also has the highest level of GDP per capita within the UK, whilst Wales has the lowest.\nBy English region.\nEngland constitutes the vast majority of the total UK population (84.3%) and an even larger proportion of the GDP (86.25%). Therefore a large part of the regional variation in the UK economy occurs within England itself. London, which is the capital of both the UK and England, has the largest regional economy in England as well as the highest GDP per capita, whilst North East England has both the smallest economy out of the regions as measured by total nominal GDP, as well as the lowest GDP per capita in England.\nBelow is a list of the Regions of England by GDP and GDP per capita, also provided by the ONS for the year 2022. \nGross value added (2020).\nExcluding the effects of North Sea oil and gas (which is classified in official statistics as extra-regio), England has the highest gross value added (GVA) and Wales the lowest of the UK's countries.\nBy English region.\nWithin England, GVA per capita is highest in London. The following table shows the GVA per capita of the nine statistical regions of England.\nTrade.\nThe trade deficit (goods and services) narrowed \u00a30.2 billion to \u00a37.9 billion in the three months to November 2018 as both goods and services exports each increased \u00a30.1 billion more than their respective imports.\nExcluding erratic commodities (mainly aircraft) the total trade deficit widened \u00a31.2 billion to \u00a39.5 billion in the three months to November 2018.\nLarge increases in export prices of oil and aircraft drove the narrowing of the total trade deficit; removing the effect of inflation, the total trade deficit widened \u00a30.3 billion to \u00a36.5 billion in the three months to November 2018.\nThe trade in goods deficit widened \u00a30.8 billion with EU countries and narrowed \u00a30.9 billion with non-EU countries in the three months to November 2018, due mainly to increases in imports from EU countries and exports to non-EU countries.\nThe total trade deficit widened \u00a34.1 billion in the 12 months to November 2018 due mainly to a \u00a34.4 billion narrowing in the trade in services surplus.\nFollowing the withdrawal of the United Kingdom from the European Union, the negotiation of a trade deal between the UK and the European Union including her 27 member states might have the same status than third countries for statistics related to imports and exports with the UK:\nUK economy received \u00a31 billion to boost through innovative trade digitalisation act in July 2023.\nTrade deals being negotiated\n Other Trade Deals \nInvestment.\nIn 2013 the UK was the leading country in Europe for inward foreign direct investment (FDI) with $26.51bn. This gave it a 19.31% market share in Europe. In contrast, the UK was second in Europe for outward FDI, with $42.59bn, giving a 17.24% share of the European market.\nIn October 2017, the ONS revised the UK's balance of payments, changing the net international investment position from a surplus of \u00a3469bn to a deficit of \u00a322bn. Deeper analysis of outward investment revealed that much of what was thought to be foreign debt securities owned by British companies were actually loans to British citizens. Inward investment also dropped, from a surplus of \u00a3120bn in the first half of 2016 to a deficit of \u00a325bn in the same period of 2017. The UK had been relying on a surplus of inward investment to make up for its long-term current account deficit. In April 2021, Lord Grimstone established the UK Investment Council to enhance UK inward investment and inform the trade policy of the UK by providing a forum for global investors to offer high-level advice to the government.\nAccording to the Office for National Statistics, the UK is the biggest investor in America, and the second biggest in China.\nMergers and acquisitions.\nSince 1985 103,430 deals with UK participation have been announced. There have been three major waves of increased M&amp;A activity (2000, 2007 and 2017; see graph \"M&amp;A in the UK\"). 1999 however, was the year with the highest cumulated value of deals (\u00a3490 bil, which is about 50% more than the current peak of 2017). The Finance industry and Energy &amp; Power made up most of the value from 2000 until 2018 (both about 15%).\nHere is a list of the top 10 deals including UK companies. The Vodafone \u2013 Mannesmann deal is still the biggest deal in global history.\nEuropean Union membership.\nThe proportion of the country's exports going to the EU has fallen from 54 per cent to 47 per cent over the past decade. The total value of exports however, has increased in the same period from \u00a3130 billion (\u20ac160 billion) to \u00a3240 billion (\u20ac275 billion).\nIn June 2016 the UK voted to leave the EU in a national referendum on its membership of the EU. After the activation of Article 50 of the Lisbon Treaty, the UK had been set to leave on Friday 29 March 2019. However the leave date was extended to Friday 12 April 2019 and then extended again to Thursday 31 October 2019, and then extended again until Friday 31 January 2020 with the ability to exit earlier. The future relationship between the UK and EU was under negotiation until the end of October 2019. UK economic growth slowed during 2019, with uncertainty over Brexit and a world economic slowdown blamed.\nThe UK left the EU in January 2020. On 16 July 2020, the government of UK affirmed that businesses across the United Kingdom, after the transition period ends, will continue to enjoy internal trade and jobs would remain protected against uncertain environment . From 1 January 2021, the powers which were previously exercised at an EU level in at least 70 policy areas were to directly transfer to the devolved administrations in Edinburgh, Cardiff and Belfast for the first time.\nPoverty.\nThe United Kingdom is a developed country with social welfare infrastructure, thus discussions surrounding poverty tend to use a relatively high minimum income compared to developing countries. Eurostat figures show that the numbers of Britons at risk of poverty has fallen to 15.9% in 2014, down from 17.1% in 2010 and 19% in 2005 (after social transfers were taken into account).\nPoverty is countered in United Kingdom with the welfare state.\nThe poverty line in the UK is commonly defined as being 60% of the median household income. In 2007\u20132008, this was calculated to be \u00a3115 per week for single adults with no dependent children; \u00a3199 per week for couples with no dependent children; \u00a3195 per week for single adults with two dependent children under 14; and \u00a3279 per week for couples with two dependent children under 14. In 2007\u20132008, 13.5\u00a0million people, or 22% of the population, lived below this line. This is a higher level of relative poverty than all but four EU members. In the same year, 4.0\u00a0million children, 31% of the total, lived in households below the poverty line, after housing costs were taken into account. This is a decrease of 400,000 children since 1998\u20131999.\nThe rate of child poverty in the United Kingdom reached 31% (4.45 million children) in 2025 and is projected to continue rising.\nData.\nThe following table shows the main economic indicators in 1980\u20132021 (with IMF staff estimates in 2022\u20132027). Inflation below 5% is in green.\n&lt;templatestyles src=\"Template:Static row numbers/styles.css\" /&gt;&lt;templatestyles src=\"template:sticky header/styles.css\"/&gt;\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "31728", "revid": "35313544", "url": "https://en.wikipedia.org/wiki?curid=31728", "title": "Telecommunications in the United Kingdom", "text": " \nTelecommunications in the United Kingdom have evolved from the early days of the telegraph to modern fibre broadband and high-speed 5G networks.\nHistory.\nNational Telephone Company (NTC) was a British telephone company from 1881 until 1911, which brought together smaller local companies in the early years of the telephone. Under the Telephone Transfer Act 1911 it was taken over by the General Post Office (GPO) in 1912.\nThe telephone service in the United Kingdom was originally provided by private companies and local city councils, but by 1912\u201313 all except the telephone service of Kingston upon Hull, Yorkshire and Guernsey had been bought out by the General Post Office. Post Office Telephones also operated telephone services in Jersey and the Isle of Man until 1969 when the islands took over responsibility for their own postal and telephone services. Post Office Telephones was reorganised in 1980\u201381 as \"British Telecommunications\" (\"British Telecom\", or \"BT\"), and was the first nationalised industry to be privatised by the Conservative government. The civil telecoms monopoly ended when Mercury Communications arrived in 1983.\nBroadcasting of radio and television was a duopoly of the BBC and Independent Broadcasting Authority (IBA): these two organisations controlled all broadcast services, and directly owned and operated the broadcast transmitter sites. Mobile phone and Internet services did not then exist. Broadcast transmitters, which belonged to the BBC and IBA, were privatised during the 1990s and now belong to Babcock International and Arqiva. \nBritish Rail Telecommunications was created in 1992 by British Rail (BR). It was the largest private telecoms network in Britain, consisting of 17,000 route kilometres of fibre optic and copper cable which connected every major city and town in the country and provided links to continental Europe through the Channel Tunnel. BR also operated its own national trunked radio network providing dedicated train-to-shore mobile communications, and in the early 1980s BR helped establish Mercury Communications', now C&amp;WC, core infrastructure by laying a resilient 'figure-of-eight' fibre optic network alongside Britain's railway lines, spanning London, Bristol, Birmingham, Leeds and Manchester.\nRegulation of communications has changed many times during the same period, and most of the bodies have been merged into Ofcom, the independent regulator and competition authority for the UK communications industries.\nInfrastructure.\nDomestic trunk infrastructure.\nAll communications trunks are now digital. Most are carried via national optical fibre networks. There are several companies with national fibre networks, including BT, Level 3 Communications, Virgin Media, Cable &amp; Wireless, Easynet and Thus. Microwave links are used up to the 155\u00a0Mbit/s level, but are seldom cost-effective at higher bit rates.\nInternational trunks.\nThe UK is a focal point for many of the world's submarine communications cables, which are now mostly digital optical fibre cables.\nBroadcast transmission.\nArqiva provide services for content contribution, coding and multiplexing, distribution to the transmitter sites as well as maintaining the national transmitter network itself for television.\nArqiva's radio network transmits 380 analogue and 300 digital radio stations across the UK via 1,450 radio transmitter sites. They operate the two commercial national multiplexes \u2013 Digital One and Sound Digital \u2013 and provide transmission services to the BBC for their national DAB (Digital Audio Broadcasting) multiplex as well as spectrum planning expertise for small-scale DAB license applicants. They also provide managed transmission services (MTS) and network access (NA) services for both analogue and digital channels from over 1,450 sites across the UK and they also provide contribution, coding and multiplexing and distribution for national and local DAB multiplexes.\nServices.\nTelevision and radio broadcasting.\nRadio.\nIn 1998, there were 663 radio broadcast stations: 219 on AM, 431 on FM and 3 on shortwave. There were 84.5 million radio receiver sets (1997). Today there are around 600 licensed radio stations in the UK. \nTelevision.\nCurrently, the United Kingdom has a collection of free-to-air, free-to-view and subscription services over a variety of distribution media, through which there are hundreds of channels for consumers as well as on-demand content. \nSince 24 October 2012, all television broadcasts in the United Kingdom have been in a digital format, following the end of analogue transmissions in Northern Ireland. Digital content is delivered via terrestrial, satellite and cable, as well as over IP. \nInternet.\nThe country code top-level domain for United Kingdom web pages is codice_1. Nominet UK is the codice_1. Network Information Centre and second-level domains must be used.\nAt the end of 2004, 52% of households (12.6 million) were reported to have access to the internet (Source: Office for National Statistics Omnibus Survey). broadband connections accounted for 50.7% of all internet connections in July 2005, with one broadband connection being created every ten seconds. Broadband connections grew by nearly 80% in 2004. In 1999, there were 364 Internet service providers (ISPs). Public libraries also provide access to the internet, sometimes for a fee.\nIn 2017, 90% of households were reported to have access to an internet connection. This percentage shows an increase in internet access from 80% in 2012 and 61% in 2007.\nMobile telephony.\nMobile phone networks.\nFirst generation networks.\nBoth companies ran ETACS analogue mobile phone networks.\n2G.\n2G is being phased out and replaced with 4G and 5G. The four mobile network operators have agreed to switch off their 2G networks by 2033.\n3G.\nThe four 2G companies all won 3G licences in a competitive auction, as did a new entrant known as Hutchison 3G, which branded its network as 3. \n3G networks were rolled-out during the early 2000s. They made it possible to access the internet through a mobile phone for the first time.\nMobile network operators are in the process of switching off their 3G networks. EE, Vodafone and Three have completed their switch-offs, with O2 expected to follow in 2025, starting with the City of Durham in April.\n4G.\n4G/Long-term evolution (LTE) services are extensive. EE launched their 4G network in October 2012, using part of their existing 1800\u00a0MHz spectrum. O2 launched its 4G network on 29 August 2013, initially in London, Leeds and Bradford with a further 13 cities added by the end of 2013. Vodafone commenced its 4G services on 29 August 2013, initially in London with 12 more cities added by the end of 2013. Three commenced 4G services in London, Birmingham, Manchester, Reading, Wolverhampton and the Black country in December 2013 albeit with a limited number of subscribers to evaluate its implementation. Full rollout to remaining subscribers commenced on 5 February 2014 on a phased basis via a silent SIM update. As a condition of acquiring part of EE's 1800MHz spectrum for 4G use, Three were unable to use it until October 2013.\n5G.\n5G is currently being rolled-out by mobile network operators. The first commercial networks went live in major UK cities in 2019.\nEE was the first to launch their 5G network, initially in London, Cardiff, Edinburgh, Belfast, Birmingham and Manchester on 30 May 2019, followed by Vodafone in Birmingham, Bristol, Cardiff, Glasgow, Manchester, Liverpool and London on 3 July 2019. Three launched their 5G service on 19 August 2019, initially for broadband customers in London. O2 was the last network to launch a 5G network; the rollout of which began in October 2019, starting with Belfast, Cardiff, Edinburgh, London, Slough and Leeds.\nNumbers.\nWhen mobile cellular networks were first rolled out, there were various numbers beginning 03 through 09 in use, these being interspersed between the various existing geographic area codes. As part of the Big Number Change, all mobile (as well as pager and personal) numbers were brought together under the 07 range. The table below shows the initial ranges of numbers that were allocated as part of the new 07 range, which began on 30 September 1999.\nMobile phone services.\nThere are four mobile network operators in the United Kingdom - O2, EE, Vodafone, and Three.\nThe number of active mobile subscriptions (excluding M2M) was 89.6 million at the end of Q2 2024, up 2.1 million (2.4%) from the year before.\nIn 2011 there were 82 million subscriptions in the UK. There were 76 million in 2008 and 55 million in January 2005.\nAll of the mobile network operators sell mobile phone services directly. In addition, there are a large number of mobile virtual network operators (MVNOs). Examples include Tesco Mobile, spusu, Lebara, and SMARTY. \nFixed telephony.\nLandlines.\nBT is still the main provider of fixed telephone lines and it has a universal service obligation, although companies can contract Openreach to install a phone line on their behalf, rather than telling the customer to get BT to install it, then transfer over.\nSky is the second biggest player in the residential telephone line market. Other companies provide fixed telephone services such as Virgin Media, Vodafone and EE.\nIn Q2 2024, the total number of fixed voice lines (including PSTN, ISDN and VoIP) was 25.4 million, a fall of 2.6% compared to the previous year. Total fixed-originated call volumes decreased by 1.22 billion minutes (21.5%) to 4.46 billion minutes.\nThe switched telephone network (both PSTN and ISDN) is due to be turned off on 31 January 2027, after customers are moved to voice over IP services.\nNumbering.\nThere is a set numbering plan for phone numbers within the United Kingdom, which is regulated by the Office of Communications (Ofcom), which replaced the Office of Telecommunications (Oftel) in 2003. Each number consists of an area code \u2013 one for each of the large towns and cities and their surroundings \u2013 and a subscriber number \u2013 the individual number.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "31729", "revid": "13017487", "url": "https://en.wikipedia.org/wiki?curid=31729", "title": "Transport in the United Kingdom", "text": "Transport in the United Kingdom is facilitated by road, rail, air and water networks. Some aspects of transport are a devolved matter, with each of the countries of the United Kingdom having separate systems under separate governments.\nA radial road network totals of main roads, of motorways and of paved roads. The National Rail network of 10,072routemiles (16,116km) in Great Britain and 189routemiles (303km) in Northern Ireland carries over 18,000 passenger and 1,000 freight trains daily. Urban rail networks exist in all cities and towns with dense bus and light rail networks. There are many regional and international airports, with Heathrow Airport in London being one of the busiest in the world and busiest in Europe. UK ports handled 486 million tons of goods in 2019.\nTransport trends.\nSince 1952 (the earliest date for which comparable figures are available), the United Kingdom saw a growth of car use, which increased its modal share, while the use of buses declined, and railway use has grown. \nHowever, since the 1990s, rail has started increasing its modal share at the expense of cars, increasing from 5% to 10% of passenger-kilometres travelled. This coincided with the privatisation of British Rail. In 1952, 27% of distance travelled was by car or taxi; with 42% being by bus or coach and 18% by rail. A further 11% was by bicycle and 3% by motorcycle. The distance travelled by air was negligible.\nPassenger transport continues to grow strongly. Figures from the Department for Transport show in 2018 people made 4.8billion local bus passenger journeys, 58% of all public transport journeys. There were 1.8billion rail passenger journeys in the United Kingdom. Light rail and tram travel also continued to grow, to the highest level (0.3million journeys) since comparable records began in 1983. In 2018/19, there was \u00a318.1bn of public expenditure on railways, an increase of 12% (\u00a31.9bn). The average amount of time people wait at a stop or station for public transport in London and Manchester is 10 minutes.\nFreight transport has undergone similar changes, increasing in volume and shifting from railways onto the road. In 1953 89billion tonne kilometres of goods were moved, with rail accounting for 42%, road 36% and water 22%. By 2010 the volume of freight moved had more than doubled to 222billion tonne kilometres, of which 9% was moved by rail, 19% by water, 5% by pipeline and 68% by road. Despite the growth in tonne kilometres, the environmental external costs of trucks and lorries in the UK have reportedly decreased. Between 1990 and 2000, there has been a move to heavier goods vehicles due to major changes in the haulage industry including a shift in sales to larger articulated vehicles. A larger than average fleet turnover has ensured a swift introduction of new and cleaner vehicles in the UK.\nThe adoption of plug-in electric vehicles is widely supported by the British government through the plug-in car and van grants schemes and other incentives. About 745,000 light-duty plug-in electric vehicles had been registered in the UK up until December 2021, consisting of 395,000 all-electric vehicles and 350,000 plug-in hybrids. In 2019, the UK had the second largest European stock of light-duty plug-in vehicles in use after Norway.\nGreenhouse gas emissions.\nA critical issue for the transport sector is its contribution to climate change emissions. Transport became the largest sector of greenhouse gas emissions in 2016. Since 1990 carbon dioxide emissions from transport in the UK have reduced by just 4% compared with an economy-wide reduction of 43%. Emissions from surface transport accounted for 22% of carbon dioxide emissions in the UK in 2019 with cars being responsible for over half of that. The Climate Change Committee has suggested that transport will need to cut its emissions to zero by a mix of demand reduction, the adoption of more efficient combustion engine vehicles, changing to non-car based modes and electrification of the fleet.\nAir transport.\nThere are 471 airports and airfields in the UK. There are also 11 heliports. Heathrow Airport is the largest airport by traffic volume in the country and is owned by Heathrow Airport Holdings. Gatwick Airport is the second largest airport and is owned by Global Infrastructure Partners. The third largest is Manchester Airport, which is run by Manchester Airport Group, which also owns various other airports.\nOther major airports include Stansted Airport in Essex and Luton Airport in Bedfordshire, both about north of London, Birmingham Airport, Newcastle Airport, Liverpool Airport, and Bristol Airport.\nOutside England, Cardiff, Edinburgh and Belfast, are the busiest airports serving Wales, Scotland and Northern Ireland respectively.\nThe largest airline in the United Kingdom by passenger traffic is easyJet, whereas British Airways is largest by fleet size and international destinations. Others include Jet2, TUI Airways and Virgin Atlantic.\nRail transport.\nThe rail network in the United Kingdom consists of two independent parts, that of Northern Ireland and that of Great Britain. Since 1994, the latter has been connected to mainland Europe via the Channel Tunnel. The network of Northern Ireland is connected to that of the Republic of Ireland. The National Rail network of in Great Britain and 189routemiles (303routekm) in Northern Ireland carries 1.7billion passengers and 110milliontonnes of freight annually.\nUrban rail networks are also well developed in London and several other cities. There were once over of rail network in the UK. The UK was ranked eighth among national European rail systems in the 2017 European Railway Performance Index assessing intensity of use, quality of service and safety.\nGreat Britain.\nThe rail network in Great Britain is the oldest such network in the world. The system consists of five high-speed main lines (the West Coast, East Coast, Midland, Great Western and Great Eastern), which radiate from London and other major cities to the rest of the country, augmented by regional rail lines and dense commuter networks within cities and other high-speed lines. High Speed 1 is operationally separate from the rest of the network, and is built to the same standard as the TGV system in France.\nThe world's first passenger railway running on steam was the Stockton and Darlington Railway, opened on 27 September 1825. Just under five years later the world's first intercity railway was the Liverpool and Manchester Railway, designed by George Stephenson and opened by the Prime Minister, the Duke of Wellington on 15 September 1830. The network grew rapidly as a patchwork of literally hundreds of separate companies during the Victorian era, which eventually was consolidated into just four by 1922, as the boom in railways ended and they began to lose money.\nEventually, the entire system came under state control in 1948, under the British Transport Commission's Railway Executive. After 1962 it came under the control of the British Railways Board; then British Railways (later British Rail), and the network was reduced to less than half of its original size by the infamous Beeching cuts of the 1960s when many unprofitable branch lines were closed. Several stations and lines have since been reopened in England and Wales.\nIn 1994 and 1995, British Rail was split into infrastructure, maintenance, rolling stock, passenger and freight companies, which were privatised from 1996 to 1997. The privatisation has delivered very mixed results, with healthy passenger growth, mass refurbishment of infrastructure, investment in new rolling stock, and safety improvements being offset by concerns over network capacity and the overall cost to the taxpayer, which has increased due to growth in passenger numbers. While the price of anytime and off-peak tickets has increased, the price of Advance tickets has dramatically decreased in real terms: the average Advance ticket in 1995 cost \u00a39.14 (in 2014 prices) compared to \u00a35.17 in 2014.\nIn Great Britain, the infrastructure (track, stations, depots and signalling chiefly) is owned and maintained by Network Rail, a body of the Department for Transport. Passenger services are operated by mostly public train-operating companies (TOCs), with private franchises awarded by the Department for Transport (in England), Transport Scotland, and Transport for Wales. Examples include Avanti West Coast, Great Western Railway and East Midlands Railway. Some other passenger TOCs make use of open-access contracts or concessionary contracts for their operations, such as Hull Trains or Merseyrail, respectively. Freight trains are operated by freight operating companies, such as DB Cargo UK, which are commercial operations unsupported by the government. Most train operating companies do not own the locomotives and coaches that they use to operate passenger services. Instead, they are required to lease these from the three rolling stock companies (ROSCOs), with train maintenance carried out by companies such as Bombardier and Alstom.\nRail passenger revenue in 2018/19 increased in real terms year-on-year. In 2018/19, there was \u00a318.1bn of public expenditure on railways, an increase of 12%. There were 1.8billion rail passenger journeys in England. Light rail and tram travel also continued to grow, to the highest level (0.3million journeys) since comparable records began in 1983.\nIn Great Britain there are of gauge track, reduced from a historic peak of over . Of this, is electrified and is double or multiple tracks. The maximum scheduled speed on the regular network has historically been around on the InterCity lines. On High Speed 1, trains are now able to reach the speeds of French TGVs. High Speed 2, under construction, is a wide high-speed line connecting London with Birmingham Curzon Street. The Network North programme consists of hundreds of transport projects mostly in Northern England and Midlands, including new high-speed lines linking up major cities and railway improvements. To cope with increasing passenger numbers, there is a large ongoing programme of upgrades to the network, including Thameslink, Crossrail, electrification of lines, in-cab signalling, new inter-city trains and high-speed lines. Great British Railways is a planned state-owned public body that will oversee rail transport in Great Britain. The Office of Rail and Road is responsible for the economic and safety regulation of the UK's railways.\nNorthern Ireland.\nIn Northern Ireland, Northern Ireland Railways (NIR) both owns the infrastructure and operates passenger rail services. The Northern Ireland rail network is one of the few networks in Europe that carry no freight. It is publicly owned. NIR was united in 1996 with Northern Ireland's two publicly owned bus operators \u2013 Ulsterbus, Foyle Metro and Metro (formally Citybus) \u2013 under the brand Translink. In Northern Ireland there is of track at gauge. of it is double track.\nInternational rail services.\nEurostar operates trains via the Channel Tunnel to France, Belgium and The Netherlands. The Enterprise which is a joint venture Northern Ireland Railways and Iarnr\u00f3d \u00c9ireann operates the cross border link between Northern Ireland and the Republic of Ireland.\nRapid transit.\nThree cities in the United Kingdom have rapid transit systems. The most well known is the London Underground (commonly known as the Tube), the oldest rapid transit system in the world which opened 1863.\nAnother system also in London is the separate Docklands Light Railway. Although this is more of an elevated light metro system due to its lower passenger capacities; further, it is integrated with the Underground in many ways. Outside London, there is the Glasgow Subway which is the third oldest rapid transit system in the world. One other system, the Tyne &amp; Wear Metro (opened 1980), serves Newcastle, Gateshead, Sunderland, North Tyneside and South Tyneside, and has many similarities to a rapid transit system including underground stations, but is sometimes considered to be light rail. The Liverpool Overhead Railway (opened 1893) was one of the first metros in the world but was dismantled 1956\u20131958 after years of neglect because nobody was willing or able to provide the funds for maintenance and repairs.\nUrban rail.\nUrban commuter rail networks are focused on many of the country's major cities:\nThey consist of several railway lines connecting city centre stations of major cities to suburbs and surrounding towns. Train services and ticketing are fully integrated with the national rail network and are not considered separate. In London, a route for Crossrail 2 has been safeguarded.\nTrams and light rail.\nTram systems were popular in the United Kingdom in the late 19th and early 20th century. However, with the rise of the car they began to be widely dismantled in the 1950s. By 1962 only the Blackpool tramway and the Glasgow Corporation Tramways remained; the final Glasgow service was withdrawn on 4 September 1962. Recent years have seen a revival the United Kingdom, as in other countries, of trams together with light rail systems.\nSince the 1990s, a second generation of tram networks have been built and have started operating in Manchester in 1992, Sheffield in 1994, the West Midlands in 1999, South London in 2000, Nottingham in 2004 and Edinburgh in 2014, whilst the original trams in Blackpool were upgraded to second generation vehicles in 2012.\nFour light rapid transit lines are opening in the Welsh Capital of Cardiff as part of the current South Wales Metro plan Phase 1 in 2023, which will reach as far out of the capital as Hirwaun, a town from Cardiff Bay, as well as three new lines planned to open by 2026.\nRoad transport.\nThe road network in Great Britain, in 2006, consisted of of trunk roads (including of motorway), of principal roads (including of motorway), of \"B\" and \"C\" roads, and of unclassified roads (mainly local streets and access roads) \u2013 totalling .\nRoad is the most popular method of transport in the United Kingdom, carrying over 90% of motorised passenger travel and 65% of domestic freight. The major motorways and trunk roads, many of which are dual carriageway, form the trunk network which links all cities and major towns. These carry about one third of the nation's traffic, and occupy about 0.16% of its land area.\nThe motorway system, which was constructed from the 1950s onwards. National Highways (a UK government-owned company) is responsible for maintaining motorways and trunk roads in England. Other English roads are maintained by local authorities. In Scotland and Wales roads are the responsibility of Transport Scotland, an executive agency of the Scottish Government, and the North and Mid Wales Trunk Road Agent and South Wales Trunk Road Agent on behalf of the Welsh Government respectively. Northern Ireland's roads are overseen by the Department for Infrastructure Roads (DfI Roads). In London, Transport for London is responsible for all trunk roads and other major roads, which are part of the Transport for London Road Network.\nToll roads are rare in the United Kingdom, though there are a number of toll bridges. Road traffic congestion has been identified as a key concern for the future prosperity of the United Kingdom, and policies and measures are being investigated and developed by the government to reduce congestion. In 2003, the United Kingdom's first toll motorway, the M6 Toll, opened in the West Midlands area to relieve the congested M6 motorway. Rod Eddington, in his 2006 report \"Transport's role in sustaining the UK's productivity and competitiveness\", recommended that the congestion problem should be tackled with a \"sophisticated policy mix\" of congestion-targeted road pricing and improving the capacity and performance of the transport network through infrastructure investment and better use of the existing network. Congestion charging systems do operate in the cities of London and Durham and on the Dartford Crossing.\nDriving is on the left. The usual maximum speed limit for cars and motorcycles is on motorways and dual carriageways. On 29 April 2015, the UK Supreme Court ruled that the government must take immediate action to cut air pollution, following a case brought by environmental lawyers at ClientEarth.\nCycle infrastructure.\nThe National Cycle Network, created by the charity Sustrans, is the UK's major network of signed routes for cycling. It uses dedicated bike paths as well as roads with minimal traffic, and covers , passing within of half of all homes. Other cycling routes such as The National Byway, the Sea to Sea Cycle Route and local cycleways can be found across the country.\nSegregated cycle paths are being installed in some cities in the UK such as London, Glasgow, Manchester, Bristol, Cardiff. In London, Transport for London has installed Cycleways.\nRoad passenger transport.\nBuses.\nBus transport is widespread and local bus services cover the whole country. Since deregulation the majority (80% by the late 1990s) of these local bus companies have been taken over by one of the \"Big Five\" private transport companies: Arriva, FirstGroup, Go-Ahead Group, Mobico Group and Stagecoach Group. In Northern Ireland, coach, bus (and rail) services remain state-owned and are provided by Translink. Cities and regions such as Manchester and Nottingham have publicity owned bus networks and other transport.\nCoaches.\nCoaches provide long-distance links throughout the UK: in England and Wales the majority of coach services are provided by National Express. Other companies such as Flixbus and Megabus run no-frills coach services in competition with National Express, the latter's services in Scotland are operated in co-operation with Scottish Citylink. BlaBlaBus also operate to France and the Low Countries from London.\nRoad freight transport.\nIn 2014, there were around 285,000 HGV drivers in the UK and in 2013 the trucking industry moved 1.6billion tonnes of goods, generating \u00a322.9billion in revenue.\nWater transport.\nDue to the United Kingdom's island location, before the Channel Tunnel the only way to enter or leave the country (apart from air travel) was on water, except at the border between Northern Ireland and the Republic of Ireland.\nPorts and harbours.\nAbout 95% of freight enters the United Kingdom by sea (75% by value). UK ports handled 486 million tons of goods in 2019.\nAs of 2022[ [update]], the five major ports that handled the most freight traffic were:\nThere are many other ports and harbours around the United Kingdom, including:\nAberdeen, Barrow, Barry, Belfast, Boston, Bristol, Cairnryan, Cardiff, Dover, Edinburgh/Leith, Falmouth, Felixstowe, Fishguard, Glasgow, Gloucester, Grangemouth, Grimsby, Harwich, Heysham, Holyhead, Hull, Kirkwall, Larne, Liverpool, Londonderry, Manchester, Oban, Pembroke Dock, Peterhead, Plymouth, Poole, Port Talbot, Portishead, Portsmouth, Scapa Flow, Stornoway, Stranraer, Sullom Voe, Swansea, Tees (Middlesbrough), Tyne (Newcastle).\nMerchant marine.\nFor long periods of recent history, Britain had the largest registered merchant fleet in the world, but it has slipped down the rankings partly due to the use of flags of convenience. There are 429 ships of \u00a0gross tonnage\u00a0(GT) or over, making a total of \u00a0GT (\u00a0tonnes deadweight (DWT)). These are split into the following types: bulk carrier 18, cargo ship 55, chemical tanker 48, container ship 134, liquefied gas 11, passenger ship 12, passenger/cargo ship 64, petroleum tanker 40, refrigerated cargo ship 19, roll-on/roll-off 25, vehicle carrier 3.\nFerries.\nFerries, both passenger only and passengers and vehicles, operate within the United Kingdom across rivers and stretches of water. In east London the Woolwich Ferry links the North and South Circular Roads. Gosport and Portsmouth are linked by the Gosport Ferry; Southampton and Isle of Wight are linked by ferry and fast Catamaran ferries; North Shields and South Shields on Tyneside are linked by the Shields Ferry; and the Mersey has the Mersey Ferry.\nIn Scotland, Caledonian MacBrayne provides passenger and roll-on/roll-off ferry services in the Firth of Clyde, to various islands of the Inner and Outer Hebrides from Oban and other ports. Orkney Ferries provides services within the Orkney Isles; and NorthLink Ferries provides services from the Scottish mainland to Orkney and Shetland, mainly from Aberdeen although other ports are also used. Ferries operate to Northern Ireland from Stranraer and Cairnryan to Larne and Belfast.\nHolyhead, Pembroke Dock and Fishguard are the principal ports for ferries between Wales and Ireland. Heysham and Liverpool/Birkenhead have ferry services to the Isle of Man.\nPassenger ferries operate internationally to nearby countries such as France, the Republic of Ireland, Belgium, the Netherlands, and Spain. Ferries usually originate from one of the following:\nMore services from Ramsgate, Newhaven, Southampton, and Lymington operate to France, Belgium and the Isle of Wight.\nWaterbuses operate on rivers in some of the country's largest cities such as London (London River Services and Thames Clippers), Cardiff (Cardiff Waterbus) and Bristol (Bristol Ferry Boat).\nOther shipping.\nCruise ships depart from the United Kingdom for destinations worldwide, many heading for ports around the Mediterranean and Caribbean.The Cunard Line still offer a scheduled transatlantic service between Southampton and New York City with . The Solent is a world centre for yachting and home to largest number of private yachts in the world.\nInland waterways.\nMajor canal building began in the United Kingdom after the onset of the Industrial Revolution in the 18th century. A large canal network was built and it became the primary method of transporting goods throughout the country; however, by the 1830s with the development of the railways, the canal network began to go into decline. There are currently of waterways in the United Kingdom and the primary use is recreational. is used for commerce and leisure.\nEducation and professional development.\nThe United Kingdom also has a well-developed network of organisations offering education and professional development in the transport and logistics sectors. A number of universities offer degree programmes in transport, usually covering transport planning, engineering of transport infrastructure, and management of transport and logistics services. The Institute for Transport Studies at the University of Leeds is one such organisation.\nPupils in England and Wales can study transport and logistics in apprenticeship studies at further education and sixth form colleges. Professional development for those working in the transport and logistics sectors is provided by a number of Professional Institutes representing specific sectors. These include:\nThrough these professional bodies, transport planners and engineers can train for a number of professional qualifications, including:\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "31730", "revid": "50300883", "url": "https://en.wikipedia.org/wiki?curid=31730", "title": "British Armed Forces", "text": "Combined military forces of the United Kingdom\nThe British Armed Forces are the unified military forces responsible for the defence of the United Kingdom, its Overseas Territories and the Crown Dependencies. They also promote the UK's wider interests, support international peacekeeping efforts and provide humanitarian aid. The force is also known as His Majesty's Armed Forces due to the British monarch's status as commander-in-chief of the Armed Forces.\nSince the formation of the united Kingdom of Great Britain in 1707 (later succeeded by the United Kingdom of Great Britain and Ireland, and finally by the United Kingdom of Great Britain and Northern Ireland), the British Armed Forces have seen action in most major wars involving the world's great powers, including the Seven Years' War, the Napoleonic Wars, the Crimean War, the First World War and the Second World War. Britain's victories in most of these wars allowed it to influence world events and establish itself as one of the world's leading military and economic powers. The British Armed Forces consist of: the Royal Navy, a blue-water navy with a fleet of 62 commissioned and active ships, together with the Royal Marines, a highly specialised amphibious light infantry force; the British Army, the UK's land warfare branch; and the Royal Air Force, a technologically sophisticated air force with a diverse operational fleet consisting of both fixed-wing and rotary aircraft. The British Armed Forces include standing forces, Regular Reserve, Volunteer Reserves and Sponsored Reserves.\nKing Charles III, sovereign of the United Kingdom, is the commander-in-chief and is styled as Head of the Armed Forces, with officers and personnel swearing allegiance to him. Long-standing constitutional convention, however, has vested \"de facto\" executive authority, by the exercise of royal prerogative, in the Prime Minister and the secretary of state for defence. The Prime Minister (acting with the Cabinet) makes the key decisions on the use of the armed forces. The UK Parliament approves the continued existence of the British Army by passing an Armed Forces Act at least once every five years, as required by the Bill of Rights 1689. Only a \"standing army\" requires reapproval by Parliament; the Royal Navy, Royal Air Force and the Royal Marines and any other forces are not included in the requirement. The armed forces are managed by the Defence Council.\nThe United Kingdom is one of five recognised nuclear powers, a permanent member on the United Nations Security Council, a founding and leading member of NATO and party to the AUKUS security pact and the Five Power Defence Arrangements. Overseas garrisons and training facilities are maintained at Ascension Island, Bahrain, Belize, Bermuda, British Indian Ocean Territory, Brunei, Canada, Cyprus, the Falkland Islands, Germany, Gibraltar, Kenya, Montserrat, Nepal, Qatar, Singapore and the United States. The British Armed Forces provided military training to approximately 140 countries in 2024\u201325.\nHistory.\nOrganisation.\nWith the Acts of Union 1707, the armed forces of England and Scotland were merged into the armed forces of the Kingdom of Great Britain.\nThere were originally several naval and several military regular and reserve \"forces\", although most of these were consolidated into the Royal Navy or the British Army during the 19th and 20th Centuries (the Royal Naval Air Service and the Royal Flying Corps of the British Army, by contrast, were separated from their parent forces in 1918 and amalgamated to form a new force, the Royal Air Force, which would have complete responsibility for naval, military and strategic aviation until the Second World War).\nNaval forces included the Royal Navy, the Waterguard, later renamed the HM Coastguard, and Sea Fencibles and \"River Fencibles\" formed as and when required for the duration of emergencies. The Merchant Navy and offshore fishing boat crews were also important manpower reserves to the armed naval forces. Any seaman was liable to impressment, with many so conscripted especially during the two decades of conflict from the French Revolution until the end of the Napoleonic Wars, and from 1835 registered on the \"Register of Seamen\" to identify them as a potential resource, and many of their seamen would serve part time in the Royal Navy Reserve, created under the Naval Reserve Act 1859, and Royal Naval Volunteer Reserve, created in 1903.\nThe British military (those parts of the British Armed Forces tasked with land warfare, as opposed to the naval forces) historically was divided into a number of military forces, of which the British Army (also referred to historically as the 'Regular Army' and the 'Regular Force') was only one. The oldest of these organisations was the Militia Force (also referred to as the \"Constitutional Force\"), which (in the Kingdom of England) was originally the main military defensive force (there otherwise were originally only royal bodyguards, including the Yeomen Warders and the Yeomen of the Guard, with armies raised only temporarily for expeditions overseas), made up of civilians embodied for annual training or emergencies, and had used various schemes of compulsory service during different periods of its long existence.\nThe Militia was originally an all infantry force, organised at the city or county level, and members were not required to serve outside of their recruitment area, although the area within which militia units in Britain could be posted was increased to anywhere in the Britain during the 18th century, and Militia coastal artillery, field artillery, and engineers units were introduced from the 1850s. The Yeomanry was a mounted force that could be mobilised in times of war or emergency. Volunteer Force units were also frequently raised during wartime, which did not rely on compulsory service and hence attracted recruits keen to avoid the Militia. These were seen as a useful way to add to military strength economically during wartime, but otherwise as a drain on the Militia and so were not normally maintained in peacetime, although in Bermuda prominent propertied men were still appointed \"Captains of Forts\", taking charge of maintaining and commanding fortified coastal artillery batteries and manned by volunteers (reinforced in wartime by embodied militiamen), defending the colony's coast from the 17th century to the 19th century (when all of the batteries were taken over by the regular Royal Artillery). The militia system was extended to a number of English (subsequently \"British\") colonies, beginning with Virginia and Bermuda. In some colonies, \"Troops of Horse\" or other mounted units similar to the Yeomanry were also created. The militia and volunteer units of a colony were generally considered to be separate forces from the \"Home\" Militia Force and Volunteer Force in the United Kingdom, and from the militia forces and volunteer forces of other colonies. Where a colony had more than one militia or volunteer unit, they would be grouped as a militia or volunteer force for that colony, such as the Jamaica Volunteer Defence Force, which comprised the St. Andrew Rifle Corps, or Kingston Infantry Volunteers, the Jamaica Corps of Scouts, and the Jamaica Reserve Regiment, but not the Jamaica Militia Artillery. In smaller colonies with a single militia or volunteer unit, that single unit would still be considered to be listed within a force, or in some case might be named a force rather than a regiment or corps, such as is the case for the Falkland Islands Defence Force and the Royal Montserrat Defence Force. The militia, yeomanry and volunteer forces collectively were known as the \"reserve forces\", \"auxiliary forces\", or \"local forces\". Officers of these forces could not sit on courts martial of regular forces personnel. The Mutiny Act did not apply to members of the Reserve Forces.\nThe other regular military force that existed alongside the British Army was the Board of Ordnance, which included the \"Ordnance Military Corps\" (made up of the Royal Artillery, Royal Engineers, and the Royal Sappers and Miners), as well as the originally-civilian Commissariat Stores and transport departments, as well as barracks departments, ordnance factories and various other functions supporting the various naval and military forces. The English Army, subsequently the British Army once Scottish regiments were moved onto its establishment following the Union of the Kingdoms of Scotland and England, was originally a separate force from these, but absorbed the Ordnance Military Corps and various previously civilian departments after the Board of Ordnance was abolished in 1855. The \"Reserve Forces\" (which referred to the Home Yeomanry, Militia and Volunteer Forces before the 1859 creation of the British Army \"Regular Reserve\" by Secretary of State for War Sidney Herbert, and re-organised under the Reserve Force Act 1867) were increasingly integrated with the British Army through a succession of reforms over the last two decades of the 19th century (in 1871, command of the Auxiliary Forces in the British Isles was taken from the Lords-Lieutenant of counties and transferred to the War Office, though colonial governors retained control of their militia and volunteer forces, and by the end of the century, at the latest, any unit wholly or partly funded from Army funds was considered part of the British Army) and the early years of the 20th century, whereby the Reserve Forces units mostly lost their own identities and became numbered Territorial Force sub-units of regular British Army corps or regiments (the Home Militia had followed this path, with the Militia Infantry units becoming numbered battalions of British Army regiments, and the Militia Artillery integrating within Royal Artillery territorial divisions in 1882 and 1889, and becoming parts of the Royal Field Artillery or Royal Garrison Artillery in 1902 (though retaining their traditional corps names), but was not merged into the Territorial Force when it was created in 1908 (by the merger of the Yeomanry and Volunteer Force). The Militia was instead renamed the \"Special Reserve\", and was permanently suspended after the First World War (although a handful of Militia units survived in the United Kingdom, its colonies, and the Crown Dependencies). Unlike the Home, Imperial Fortress and Crown Dependency Militia and Volunteer units and forces that continued to exist after the First World War, although parts of the British military, most were not considered parts of the British Army unless they received Army funds, as was the case for the Bermuda Militia Artillery and the Bermuda Volunteer Rifle Corps, which was generally only the case for those in the Channel Islands or the Imperial Fortress colonies (Nova Scotia, before Canadian Confederation, Bermuda, Gibraltar, and Malta). Today, the British Army is the only Home British military force (unless the Army Cadet Force and the Combined Cadet Force are considered), including both the regular army and the forces it absorbed, though British military units organised on Territorial lines remain in British Overseas Territories that are still not considered formally part of the British Army, with only the Royal Gibraltar Regiment and the Royal Bermuda Regiment (an amalgam of the old Bermuda Militia Artillery and Bermuda Volunteer Rifle Corps) appearing on the British Army order of precedence and in the Army List.\nConfusingly, and similarly to the dual meaning of the word Corps in the British Army. As an example, the 1st Battalion of the King's Royal Rifle Corps was in 1914 part of the 6th Brigade that was part of the 2nd Infantry Division, which was itself part of 1st Army Corps), the British Army sometimes also used the term expeditionary force or field force to describe a body made up of British Army units, most notably the British Expeditionary Force, or of a mixture of British Army, Indian Army, or Imperial auxiliary units, such as the Malakand Field Force (this is similarly to the naval use of the term task force). In this usage, \"force\" is used to describe a self-reliant body able to act without external support, at least within the parameters of the task or objective for which it is employed.\nBritish Empire.\nDuring the later half of the 17th century, and in particular, throughout the 18th century, British foreign policy sought to contain the expansion of rival European powers through military, diplomatic and commercial means, especially of its chief competitors Spain, the Netherlands, and France. This saw Britain engage in a number of intense conflicts over colonial possessions and world trade, including a long string of Anglo-Spanish and Anglo-Dutch wars, as well as a series of \"world wars\" with France, such as; the Seven Years' War (1756\u20131763), the French Revolutionary Wars (1792\u20131802) and the Napoleonic Wars (1803\u20131815). During the Napoleonic wars, the Royal Navy victory at Trafalgar (1805) under the command of Horatio Nelson (aboard HMS \"Victory\") marked the culmination of British maritime supremacy, and left the Navy in a position of uncontested hegemony at sea. By 1815 and the conclusion of the Napoleonic Wars, Britain had risen to become the world's dominant great power and the British Empire subsequently presided over a period of relative peace, known as Pax Britannica.\nWith Britain's old rivals no-longer a threat, the 19th century saw the emergence of a new rival, the Russian Empire, and a strategic competition in what became known as The Great Game for supremacy in Central Asia. Britain feared that Russian expansionism in the region would eventually threaten the Empire in India. In response, Britain undertook a number of pre-emptive actions against perceived Russian ambitions, including the First Anglo-Afghan War (1839\u20131842), the Second Anglo-Afghan War (1878\u20131880) and the British expedition to Tibet (1903\u20131904). During this period, Britain also sought to maintain the balance of power in Europe, particularly against Russian expansionism, who at the expense of the waning Ottoman Empire had ambitions to \"carve up the European part of Turkey\". This ultimately led to British involvement in the Crimean War (1854\u20131856) against the Russian Empire.\nFirst World War.\nThe beginning of the 20th century served to reduce tensions between Britain and the Russian Empire, partly due to the emergence of a unified German Empire. The era brought about an Anglo-German naval arms race, which encouraged significant advancements in maritime technology, including Dreadnoughts, torpedoes, submarines), and, in 1906, Britain determined that its only likely naval enemy was Germany. The accumulated tensions in European relations finally broke out into the hostilities of the First World War (1914\u20131918), in what is recognised today, as the most devastating war in British military history, with nearly 800,000 men killed and over 2\u00a0million wounded. Allied victory resulted in the defeat of the Central Powers, the end of the German Empire, the Treaty of Versailles and the establishment of the League of Nations.\nSecond World War.\nGermany was defeated in the First World War, but by 1933 fascism had given rise to Nazi Germany, which under the leadership of Adolf Hitler re-militarised in defiance of the Treaty of Versailles. Once again tensions accumulated in European relations, and following Germany's invasion of Poland in September 1939, the Second World War began (1939\u20131945). The conflict was the most widespread in British history, with British Empire and Commonwealth troops engaged in military campaigns in Europe, North Africa, the Middle East, and the Far East. Approximately 390,000 British Empire and Commonwealth troops died. Allied victory resulted in the defeat of the Axis powers and the establishment of the United Nations, replacing the League of nations.\nCold War.\nPost\u2013Second World War economic and political decline, as well as changing attitudes in British society and government, were reflected by the armed forces' contracting global role, and later epitomised by its political defeat during the Suez Crisis (1956). Reflecting Britain's new role in the world and the escalation of the Cold War (1947\u20131991), the country became a founding member of the NATO military alliance in 1949. Defence Reviews, such as those in 1957 and 1966, announced significant reductions in conventional forces, the pursuement of a doctrine based on nuclear deterrence, and a permanent military withdrawal east of Suez. By the mid-1970s, the armed forces had reconfigured to focus on the responsibilities allocated to them by NATO. The British Army of the Rhine and RAF Germany consequently represented the largest and most important overseas commitments that the armed forces had during this period, while the Royal Navy developed an anti-submarine warfare specialisation, with a particular focus on countering Soviet submarines in the Eastern Atlantic and North Sea.\nWhile NATO obligations took increased prominence, Britain nonetheless found itself engaged in a number of low-intensity conflicts, including a spate of insurgencies against colonial occupation. However the Dhofar Rebellion (1962\u20131976) and The Troubles (1969\u20131998) emerged as the primary operational concerns of the armed forces. Perhaps the most important conflict during the Cold War, at least in the context of British defence policy, was the Falklands War (1982).\nSince the end of the Cold War, an increasingly international role for the armed forces has been pursued, with re-structuring to deliver a greater focus on expeditionary warfare and power projection. This entailed the armed forces often constituting a major component in peacekeeping and humanitarian missions under the auspices of the United Nations, NATO, and other multinational operations, including: peacekeeping responsibilities in the Balkans and Cyprus, the 2000 intervention in Sierra Leone and participation in the UN-mandated no-fly zone over Libya (2011). Post-9/11, the armed forces became heavily committed to the war on terror (2001\u2013present), with lengthy campaigns in Afghanistan (2001\u20132021) and Iraq (2003\u20132009), and more recently as part of the Military intervention against ISIL (2014\u2013present). Britain's military intervention against Islamic State was expanded following a parliamentary vote to launch a bombing campaign over Syria; an extension of the bombing campaign requested by the Iraqi government against the same group. In addition to the aerial campaign, the British Army has trained and supplied allies on the ground and the Special Air Service, the Special Boat Service, and the Special Reconnaissance Regiment (British special forces) has carried out various missions on the ground in both Syria and Iraq.\nThe armed forces have also been called upon to assist with national emergencies through the provisions of the military aid to the civil authorities (MACA) mechanism. This has seen the armed forces assist government departments and civil authorities responding to flooding, food shortages, wildfires, terrorist attacks and the COVID-19 pandemic; the armed forces' support to the latter falls under Operation Rescript, described as the UK's \"biggest ever homeland military operation in peacetime\" by the Ministry of Defence.\nFigures released by the Ministry of Defence on 31 March 2016 show that 7,185 British Armed Forces personnel have lost their lives in medal earning theatres since the end of the Second World War.\nIn 2025, under Prime Minister Keir Starmer, the UK published a new Strategic Defence Review (SDR) calling for a shift to \"warfighting readiness\". Major investments were announced, including the expansion of the SSN-AUKUS attack submarine program to up to 12 boats, acquisition of long-range weapons and advanced air-defence systems, and a possible entry into NATO's nuclear-sharing scheme via the F-35A platform.\nToday.\nCommand.\nKing Charles III, sovereign of the United Kingdom, is the Head of the Armed Forces, with officers and personnel swearing allegiance to him. Long-standing constitutional convention, however, has \"de facto\" vested military authority and associated royal prerogative powers in the prime minister and the secretary of state for defence, with the former (acting with the support of the Cabinet) making the key decisions on the use of the armed forces. As the prime minister is not the formal head of the armed forces, the chief of the defence staff could refuse a direction by them to use the UK's nuclear arsenal.\nThe Ministry of Defence is the government department charged with formulating and executing defence policy. It currently employs 56,860 civilian staff members as of 1 October 2015. The department is administered by the secretary of state for defence who is assisted by the Minister of State for the Armed Forces, Minister for Defence Procurement, and Minister for Veterans' Affairs. Responsibility for the management of the forces is delegated to a number of committees: the Defence Council, Chiefs of Staff Committee, Defence Management Board and three single-service boards. The Defence Council, composed of senior representatives of the services and the Ministry of Defence, provides the \"formal legal basis for the conduct of defence\". The three constituent single-service committees (Admiralty Board, Army Board and Air Force Board) are chaired by the secretary of state for defence.\nThe chief of the defence staff (CDS) is the senior-most officer of the armed forces and is an appointment that can be held by an admiral, air chief marshal or general. Before the practice was discontinued in the 1990s, those who were appointed to the position of CDS had been elevated to the most senior rank in their respective service. The CDS, along with the permanent under secretary, are the principal military advisers to the secretary of state. All three services have their own respective professional chiefs; the First Sea Lord for the Royal Navy, the chief of the general staff for the Army and the chief of the air staff for the Royal Air Force.\nPersonnel.\nAs of 1 July 2025 the British Armed Forces are a professional force with a total strength of 180,779 personnel, consisting of 136,117 UK Regulars and 4,127 Gurkhas, 31,967 Volunteer Reserves and 8,568 \"Other Personnel\". As a percentage breakdown of UK Service Personnel, 77.1% are UK Regulars and Gurkhas, 18.8% are Volunteer Reserves and 4.1% are composed of Other Personnel. In addition, all ex-Regular personnel retain a \"statutory liability for service\" and are liable to be recalled (under Section 52 of the Reserve Forces Act (RFA) 1996) for duty during wartime, which is known as the Regular Reserve. MoD publications since April 2013 no longer report the entire strength of the Regular Reserve, instead they only give a figure for Regular Reserves who serve under a fixed-term reserve contract. These contracts are similar in nature to those of the Volunteer Reserve.\nThe distribution of personnel between the services and categories of service on 1 January 2025 was as follows:\nOn 1 April 2024, most personnel in the UK Regular Forces were stationed in the United Kingdom (around 96%).\nOf the 5,700 personnel stationed overseas, around two thirds were in Europe (66%), while 14% were stationed in North America, 6% in North Africa and the Middle East, 6% in Asia and 5% in Sub-Saharan Africa. 1,230 personnel were distributed across several regions in Germany, primarily North Rhine-Westphalia as part of British Army Germany. However, up to 750 of these were Locally Engaged Civilians.\nDefence expenditure.\nAccording to the Stockholm International Peace Research Institute, the United Kingdom is in sixth place in the world's military spending list in 2024. For comparison: Great Britain spends more in absolute terms than Saudi Arabia, Ukraine, France or Japan, but less than India, Germany, Russia, China or the United States. In September 2011, according to Professor Malcolm Chalmers of the Royal United Services Institute, current \"planned levels of defence spending should be enough for the United Kingdom to maintain its position as one of the world's top military powers, as well as being one of NATO-Europe's top military powers. Its edge \u2013 not least its qualitative edge \u2013 in relation to rising Asian powers seems set to erode, but will remain significant well into the 2020s, and possibly beyond.\" The Strategic Defence and Security Review 2015 committed to spending 2% of GDP on defence and announced a \u00a3178\u00a0billion investment over ten years in new equipment and capabilities. On 8 March 2023 Prime Minister Rishi Sunak announced a further \u00a35bn in defence spending with a long-term goal of an increased spending to 2.5% of GDP.\nNuclear weapons.\nThe United Kingdom is one of five recognised nuclear weapon states under the Non-Proliferation Treaty and maintains an independent nuclear deterrent, currently consisting of four ballistic missile submarines, UGM-133 Trident II submarine-launched ballistic missiles, and 160 operational thermonuclear warheads. This is known as Trident in both public and political discourse (with nomenclature taken after the UGM-133 Trident II ballistic missile). Trident is operated by the Royal Navy Submarine Service, charged with delivering a 'Continuous At-Sea Deterrent' (CASD) capability, whereby one of the \"Vanguard\"-class strategic submarines is always on patrol. According to the British Government, since the introduction of Polaris (Trident's predecessor) in the 1960s, from April 1969 \"the Royal Navy's ballistic missile boats have not missed a single day on patrol\", giving what the Defence Council described in 1980 as a deterrent \"effectively invulnerable to pre-emptive attack\". As of 2015, it has been British Government policy for the \"Vanguard\"-class strategic submarines to carry no more than 40 nuclear warheads, delivered by eight UGM-133 Trident II ballistic missiles. In contrast with the other recognised nuclear weapon states, the United Kingdom operates only a submarine-based delivery system, having decommissioned its tactical WE.177 free-fall bombs in 1998.\nThe House of Commons voted on 18 July 2016 in favour of replacing the \"Vanguard\"-class submarines with a new generation of s. The programme will also contribute to extending the life of the UGM-133 Trident II ballistic missiles and modernise the infrastructure associated with the CASD.\nFormer weapons of mass destruction possessed by the United Kingdom include both biological and chemical weapons. These were renounced in 1956 and subsequently destroyed.\nOverseas military installations.\n&lt;templatestyles src=\"Legend/styles.css\" /&gt;\u00a0\u00a0Overseas military installations of the United Kingdom, and locally raised units of British Overseas Territories.\n&lt;templatestyles src=\"Legend/styles.css\" /&gt;\u00a0\u00a0Military interventions since 2000: Palliser (Sierra Leone); Herrick (Afghanistan); Enduring Freedom (Horn of Africa); Telic (Iraq); Ellamy (Libya); and Shader (Islamic State of Iraq and the Levant).\nThe British Armed Forces historically relied on four Imperial fortress colonies (Bermuda, Gibraltar, Halifax and its environs in Nova Scotia, and Malta), where dockyards were established, naval squadrons based, soldiers garrisoned, and naval and military stores stockpiled. These acted as lynchpins in maintaining British naval supremacy on the Atlantic and its connected seas. As, until the end of the First World War, it was presumed the only navies that might prove a threat were all of countries on, or off, the Atlantic, no Imperial fortress was established in the Pacific or Indian Oceans, to which power would be extended from Bermuda and Malta following the completion of the Panama and Suez canals. Local-service military reserve units were raised in some of the Imperial fortresses (notably Bermuda and Malta), which could be embodied for full time service in war time to reinforce the regular garrisons, and these were funded by the War Office as part of the British Army. After the First World War, the growing belligerence and naval power of the Japanese Empire led to the construction of the Singapore Naval Base. The regular British Armed Forces otherwise were distributed around the world, were required to guard against invasion or rebellion, reinforced in some colonies by locally raised reserve forces. In colonies where there was no strategic requirement, regular forces were rarely stationed, with local governments encouraged to maintain and fund military reserve units as contributions to their own defence (although these units were ultimately under the control of the national, i.e. British, Government via the colonial Governors as defence is not a competency that has been delegated to local governments). Under the North Atlantic Treaty Organisation alliance, and with the steady reduction of both the British Empire and the British Armed Forces over the decades that followed the Second World War, the significance of the three remaining Imperial fortresses (military control of Halifax having passed to the new Dominion government following the 1867 Confederation of Canada, and naval control transferred in 1905 to what was to become the Royal Canadian Navy) rapidly faded. The Bermuda-based North America and West Indies Station was abolished in 1956, and the last regular army units removed from the Bermuda Command in 1957 (leaving only two part-time reserve units), with the naval dockyard in Bermuda reduced to a base, without repair or refit capabilities, in 1951 and finally closed in 1995, following the Cold War (United States and Canadian bases in Bermuda closed in the same period), leaving only the Royal Bermuda Regiment and the Bermuda Sea Cadet Corps there today. Malta became independent in 1964, and the last British armed forces personnel were removed from the former colony in 1979. Gibraltar continues to be used by the regular British Armed Forces, though the naval and military establishment in the colony (now termed a \"British Overseas Territory\") has been reduced to several Royal Naval patrol craft, the locally raised Royal Gibraltar Regiment, and a Royal Air Force Station without aircraft based on it.\nThe British Armed Forces today maintain a number of overseas garrisons and military facilities which enable the country to conduct operations worldwide. The majority of Britain's permanent military installations are located on British Overseas Territories (BOTs) or former colonies which retain close diplomatic ties with the United Kingdom, and located in areas of strategic importance. The most significant of these are the \"Permanent Joint Operating Bases\" (PJOBs), located on the four overseas territories of Cyprus (British Forces Cyprus), Gibraltar (British Forces Gibraltar), the Falkland Islands (British Forces South Atlantic Islands) and Diego Garcia (British Forces British Indian Ocean Territories). While not a PJOB, Ascension Island (another BOT) is home to the airbase RAF Ascension Island, notable for use as a staging post during the 1982 Falklands War, the territory is also the site of a joint UK-US signals intelligence facility.\nQatar is home to RAF Al Udeid, a Royal Air Force outpost at Al Udeid Air Base which serves as the operational headquarters for No. 83 Expeditionary Air Group and its operations across the Middle East. A large Royal Navy Naval Support Facility (NSF) is located in Bahrain, established in 2016 it marks the British return East of Suez. In support of the Five Power Defence Arrangements (FPDA), the United Kingdom retains a naval repair and logistics support facility at Sembawang wharf, Singapore. Other overseas military installations include; British Forces Brunei, British Army Germany, the British Army Training Unit Kenya, British Army Training Unit Suffield in Canada, British Army Training and Support Unit Belize, and British Gurkhas Nepal.\nSome British Overseas Territories also maintain locally raised units and regiments; The Royal Bermuda Regiment, the Falkland Islands Defence Force, the Royal Gibraltar Regiment, the Royal Montserrat Defence Force, the Cayman Islands Regiment, and the Turks and Caicos Regiment. Though their primary mission is \"home defence\", individuals have volunteered for operational duties. The Royal Bermuda Regiment is an amalgam of the Bermuda Militia Artillery (which had been part of the Royal Regiment of Artillery) and the Bermuda Volunteer Rifle Corps, raised in the 1890s as Imperial forces funded by the War Office as part of the British Army, and both antecedent units sent contingents to the Western Front during the First World War. They also sent contingents that served in North-Western Europe, and Italy and North Africa during the Second World War. The Royal Gibraltar Regiment mobilised section-sized units for attachment to British regiments deployed during the Iraq War. The Isle of Man, a Crown dependency hosts a multi-capability recruiting and training unit of the British Army Reserve.\nSince 1969 Britain has had a military satellite communications system, Skynet, initially in large part to support East of Suez bases and deployments. Since 2015 Skynet has offered near global coverage.\nExpeditionary forces.\nThe British Armed Forces place significant importance in the ability to conduct expeditionary warfare. While the armed forces are expeditionary in nature, it maintains a core of \"high readiness\" forces trained and equipped to deploy at very short notice, these include; the Joint Expeditionary Force (Maritime) (Royal Navy), UK Commando Force (Royal Marines), and 16 Air Assault Brigade (British Army). Frequently, these forces will act as part of a larger tri-service effort, under the direction of Permanent Joint Headquarters, or along with like-minded allies under the Joint Expeditionary Force. Similarly, under the auspices of NATO, such expeditionary forces are designed to meet Britain's obligations to the Allied Rapid Reaction Corps and other NATO operations.\nIn 2010, the governments of the United Kingdom and France signed the Lancaster House Treaties which committed both governments to the creation of a Franco-British Combined Joint Expeditionary Force. It is envisaged as a deployable joint force, for use in a wide range of crisis scenarios, up to and including high intensity combat operations. As a joint force it involves all three armed Services: a land component composed of formations at national brigade level, maritime and air components with their associated Headquarters, together with logistics and support functions.\nBranches.\nRoyal Navy.\nThe Royal Navy is a technologically sophisticated naval force, and as of September 2025 consists of 64 commissioned ships with an additional 10 support vessels of various types operated by the Royal Fleet Auxiliary. Command of deployable assets is exercised by the Fleet Commander of the Naval Service. Personnel matters are the responsibility of the Second Sea Lord/Commander-in-Chief Naval Home Command, an appointment usually held by a vice-admiral.\nThe Surface Fleet consists of aircraft carriers, destroyers, frigates, patrol vessels, mine-countermeasure vessels, and other miscellaneous vessels. The Surface Fleet has been structured around a single fleet since the abolition of the Eastern and Western fleets in 1971. The recently built Type 45 destroyers are stealthy and technologically advanced air-defence destroyers. The Royal Navy has commissioned two s, embarking an air-group including the advanced fifth-generation multi-role fighter, the F-35B Lightning.\nA submarine service has existed within the Royal Navy for more than 100 years. The Submarine Service's four nuclear-powered submarines carry Trident II ballistic missiles, forming the United Kingdom's nuclear deterrent. Seven nuclear-powered fleet (attack) submarines have been ordered, with six completed and one under construction. The \"Astute\" class are the most advanced and largest fleet submarines ever built for the Royal Navy and will maintain Britain's nuclear-powered submarine fleet capabilities for decades to come.\nRoyal Marines.\nThe Royal Marines are the Royal Navy's amphibious troops. Consisting of a single manoeuvre brigade (UK Commando Force) and various independent units, the Royal Marines specialise in amphibious, arctic, and mountain warfare. Contained within UK Commando Force are three attached army units; 383 Commando Petroleum Troop RLC, 29th Commando Regiment Royal Artillery, a field artillery regiment based in Plymouth, and 24 Commando Regiment Royal Engineers. The Commando Logistic Regiment consists of personnel from the Army, Royal Marines, and Royal Navy.\nBritish Army.\nThe British Army is the land force of the British Armed Forces, and is made up of the Regular Army and the part-time Army Reserve. The Army is commanded by the Chief of the General Staff, a four-star general within Army Headquarters, based at Andover.\nDeployable combat formations are;\nThe Infantry of the British Army has a strength of 48 battalions (32 regular and 16 reserve), structured under 17 unique regiments. These battalions are trained and equipped for specific roles within their respective Brigade Combat Teams (BCT); Light Infantry, such as the famous 1st Battalion Grenadier Guards, within the 4th Light Brigade Combat Team, fight on foot without armoured vehicles; Light Mechanised Infantry, such as the 1st Battalion Royal Yorkshire Regiment, within the 7th Light Mechanised Brigade Combat Team, operate the Foxhound protected mobility vehicle; Armoured Infantry (to become Heavy Mechanised Infantry under Future Soldier), such as the 1st Battalion Royal Regiment of Fusiliers, within the 20th Armoured Infantry Brigade Combat Team, operate the Warrior infantry fighting vehicle (IFV), but will be equipped with the new Boxer mechanised infantry vehicle from 2024.\nThe four battalions of the Parachute Regiment, forming 16 Air Assault Brigade Combat Team and part of Special Forces Support Group, are the British Army's elite airborne infanteers, held at high readiness and specialising in rapid deployment by parachute and helicopter, widely regarded as the \"fittest, most aggressive, resilient and disciplined regiment in the British Army.\"\nThe Royal Armoured Corps provides the armoured capability of the British Army. The Royal Tank Regiment, Queen's Royal Hussars and Royal Wessex Yeomanry (of the Army Reserve) operate Challenger 2 main battle tanks, which are being upgraded to Challenger 3, and are part of 3rd (UK) Division's Armoured Brigade Combat Teams. Armoured Cavalry regiments, such as the Royal Dragoon Guards, currently operate the Warrior IFV on an interim basis, until Ajax reaches full operating capability. There are six Light Cavalry regiments (three Regular + three Reserve) equipped with the Jackal 2 and Coyote TSV, tasked with providing reconnaissance and fire support. The Household Cavalry, made up of the Life Guards and the Blues and Royals, operate in a dual role of Armoured Cavalry and Mounted Ceremonial on Horse Guards in London, and for state occasions.\nRoyal Air Force.\nThe Royal Air Force has a large operational fleet that fulfils various roles, consisting of both fixed-wing and rotary aircraft. Frontline aircraft are controlled by Air Command, which is organised into five groups defined by function: 1 Group (Air Combat), 2 Group (Air Support), 11 Group (Air and Space operations), 22 Group (training aircraft and ground facilities) and 38 Group (Royal Air Force's Engineering, Logistics, Communications and Medical Operations units). In addition 83 Expeditionary Air Group directs formations in the Middle East and the 38 Group combines the expeditionary combat support and combat service support units of the RAF. Deployable formations consist of Expeditionary Air Wings and squadrons\u2014the basic unit of the Air Force. Independent flights are deployed to facilities in Brunei, the Falkland Islands, Iraq, and the United States.\nThe Royal Air Force operates multi-role and single-role fighters, reconnaissance and patrol aircraft, tankers, transports, helicopters, unmanned aerial vehicles, and various types of training aircraft.\nGround units are also maintained by the Royal Air Force, most prominently the RAF Police and the Royal Air Force Regiment (RAF Regt). The Royal Air Force Regiment essentially functions as the ground defence force of the RAF, optimised for the specialist role of fighting on and around forward airfields, which are densely packed with operationally vital aircraft, equipment, infrastructure and personnel. The Regiment contains nine regular squadrons, supported by five squadrons of the Royal Auxiliary Air Force Regiment. In addition, it provides Forward Air Controllers to defence as well as a contribution to the Special Forces Support Group.\nMinistry of Defence.\nThe Ministry of Defence maintains a number of civilian agencies in support of the British Armed Forces. Although they are civilian, they play a vital role in supporting Armed Forces operations, and in certain circumstances are under military discipline:\nRecruitment.\nAll three services of the British Armed Forces recruit primarily from within the United Kingdom, although citizens from the Commonwealth of Nations and the Republic of Ireland are equally eligible to join. The minimum recruitment age is 16 years (although personnel may not serve on armed operations below 18 years, and if under 18 must also have parental consent to join); the maximum recruitment age depends whether the application is for a regular or reserve role; there are further variations in age limit for different corps/regiments. The normal term of engagement is 22 years; however, the minimum service required before resignation is 4 years, plus, in the case of the Army, any service person below the age of 18. A note to add is that in the United Kingdom, people may join the \"Cadet Forces\" such as the army cadets, Royal Air Force Air Cadets or the sea and Royal Marine Cadets. Young people may join these organisations which are either funded or affiliated with the MOD from the age of 13-18, there is no obligation to then join the armed forces however it teaches key skills in both civilian and military life and is a key recruitment drive for the armed forces. At present, the yearly intake into the armed forces is 11,880 (per the 12 months to 31 March 2014).\nExcluding the Brigade of Gurkhas and the Royal Irish Regiment, as of 1 April 2014 there are approximately 11,200 Black and Minority Ethnic (BME) persons serving as Regulars across the three service branches; of those, 6,610 were recruited from outside the United Kingdom. In total, Black and Minority Ethnic persons represent 7.1% of all service personnel, an increase from 6.6% in 2010.\nSince the year 2000, sexual orientation has not been a factor considered in recruitment, and homosexuals can serve openly in the armed forces. All branches of the forces have actively recruited at Gay Pride events. The forces keep no formal figures concerning the number of gay and lesbian serving soldiers, saying that the sexual orientation of personnel is considered irrelevant and not monitored.\nRole of women.\nWomen have been part of the armed forces, on and off, for centuries, more fully integrated since the early 1990s, including flying fast jets and commanding warships or artillery batteries. As of 1 April 2014, there were approximately 15,840 women serving in the armed forces, representing 9.9% of all service personnel. The first female military pilot was Flight Lieutenant Julie Ann Gibson while Flight Lieutenant Jo Salter was the first fast-jet pilot, the latter flying a Tornado GR1 on missions patrolling the then Northern Iraqi No-Fly Zone. Flight Lieutenant Juliette Fleming and Squadron Leader Nikki Thomas recently were the first Tornado GR4 crew. While enforcing the Libyan No-Fly Zone, Flight Lieutenant Helen Seymour was identified as the first female Eurofighter Typhoon pilot.\nIn August 2011, it was announced that a female lieutenant commander, Sarah West, was to command the frigate . In July 2016, it was announced that women would be allowed to serve in close combat, starting with the Royal Armoured Corps. In July 2017, the Secretary of Defence announced that women would be allowed to enlist in the RAF Regiment from September 2017, a year ahead of schedule. In 2018, women were allowed to apply for all roles in the British military, including the special forces. As of 2024[ [update]], the most senior serving woman is four-star General Dame Sharon Nesmith.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "31731", "revid": "7611264", "url": "https://en.wikipedia.org/wiki?curid=31731", "title": "Foreign relations of the United Kingdom", "text": "&lt;templatestyles src=\"Hlist/styles.css\" /&gt;\nThe diplomatic foreign relations of the United Kingdom are conducted by the Foreign, Commonwealth and Development Office, headed by the foreign secretary. The prime minister and numerous other agencies play a role in setting policy, and many institutions and businesses have a voice and a role.\nThe United Kingdom was the world's foremost power during the 19th and early 20th centuries, most notably during the so-called \"Pax Britannica\"\u2014a period of unrivaled supremacy and unprecedented international peace during the mid-to-late 1800s. The country continued to be widely considered a superpower until the Suez crisis of 1956 and the dismantling of the British Empire left the UK's dominant role in global affairs to be gradually diminished. Nevertheless, the United Kingdom remains a great power and a permanent member of the United Nations Security Council. The UK is a founding member of AUKUS, the Commonwealth, the Council of Europe, the European Court of Human Rights, the G7, the G20, the International Criminal Court, NATO, the OECD, the OSCE, the World Health Organization, and the World Trade Organization, additionally the UK is a member of CPTPP. The UK was also a founding member state of the European Union, however due to the outcome of a 2016 membership referendum, proceedings to withdraw from the EU began in 2017 and concluded when the UK formally left the EU on 31 January 2020, and the transition period on 31 December 2020 with an EU trade agreement. Since the vote and the conclusion of trade talks with the EU, policymakers have begun pursuing new trade agreements with other global partners.\nHistory.\nFollowing the formation of the Kingdom of Great Britain (which united England and Scotland) in 1707, British foreign relations largely continued those of the Kingdom of England. British foreign policy initially focused on achieving a balance of power within Europe, with no one country achieving dominance over the affairs of the continent. This policy remained a major justification for Britain's wars against Napoleon, and for British involvement in the First and Second World Wars. Secondly Britain continued the expansion of its colonial \"First British Empire\" by migration and investment.\nFrance was the chief enemy until the defeat of Napoleon in 1815. It had a much larger population and a more powerful army, but a weaker navy. The British were generally successful in their many wars. The notable exception, the American War of Independence (1775\u20131783), saw Britain, without any major allies, defeated by the American colonials who had the support of France, the Netherlands and (indirectly) Spain. A favoured British diplomatic strategy involved subsidising the armies of continental allies (such as Prussia), thereby turning London's enormous financial power to military advantage. Britain relied heavily on its Royal Navy for security, seeking to keep it the most powerful fleet afloat, eventually with a full complement of bases across the globe. British dominance of the seas was vital to the formation and maintaining of the British Empire, which was achieved through the support of a navy larger than the next two largest navies combined, prior to 1920. The British generally stood alone until the early 20th century, when it became friendly with the U.S. and made alliances with Japan, France and Russia and Germany former antagonist now ally.\n1814\u20131914.\nThe 100 years were generally peaceful\u2014a sort of Pax Britannica enforced by the Royal Navy. There were two important wars, both limited in scope. The Crimean War (1853\u20131856) saw the defeat of Russia and its threat to the Ottoman Empire. The Second Boer War (1899\u20131902) saw the defeat of the two Boer republics in South Africa and Boxer Rebellion happen the same year. London became the world's financial centre, and commercial enterprise expanded across the globe. The \"Second British Empire\" was built with a base in Asia (especially India) and Africa.\n1920s.\nAfter 1918 Britain was a \"troubled giant\" that was less of a dominant diplomatic force in the 1920s than before. It often had to give way to the United States, which frequently exercised its financial superiority. The main themes of British foreign policy included a leading role at the Paris Peace Conference of 1919\u20131920, where Lloyd George worked hard to moderate French demands for revenge on Germany. He was partly successful, but Britain soon had to moderate French policy toward Germany further, as in the Locarno Treaties of 1925. Furthermore, Britain obtained \"mandates\" that allowed it and its dominions to govern most of the former German and Ottoman colonies.\nBritain became an active member of the new League of Nations, but its list of major achievements was slight.\nDisarmament was high on the agenda, and Britain played a major role following the United States in the Washington Naval Conference of 1921 in working toward naval disarmament of the major powers. By 1933 disarmament agreements had collapsed and the issue became rearming for a war against Germany.\nBritain was partially successful in negotiating better terms with United States regarding the large war loans which Britain was obliged to repay. Britain supported the international solution to German reparations through the Dawes Plan and the Young Plan. After the Dawes Plan had helped stabilize Germany's currency and lowered its annual payments, Germany was able to pay its annual reparations using money borrowed from New York banks, and Britain used the money received to pay Washington. The Great Depression starting in 1929 put enormous pressure on the British economy. Britain revived Imperial Preference, which meant low tariffs within the British Empire and higher barriers to trade with outside countries. The flow of money from New York dried up, and the system of reparations and payment of debt died in 1931.\nIn domestic British politics, the emerging Labour Party had a distinctive and suspicious foreign policy based on pacifism. Its leaders believed that peace was impossible because of capitalism, secret diplomacy, and the trade in armaments. Labour stressed material factors that ignored the psychological memories of the Great War and the highly emotional tensions regarding nationalism and the boundaries of countries. Nevertheless, party leader Ramsay MacDonald devoted much of his attention to European policies.\n1930s.\nVivid memories of the horrors and deaths of the First World War inclined many Britons\u2014and their leaders in all parties\u2014to pacifism in the interwar era. This led directly to the appeasement of dictators (notably of Mussolini and of Hitler) in order to avoid their threats of war.\nThe challenge came from those dictators, first from Benito Mussolini, Duce of Italy, then from Adolf Hitler, F\u00fchrer of a much more powerful Nazi Germany. The League of Nations proved disappointing to its supporters; it failed to resolve any of the threats posed by the dictators. British policy involved \"appeasing\" them in the hopes they would be satiated. By 1938 it was clear that war was looming, and that Germany had the world's most powerful military. The final act of appeasement came when Britain and France sacrificed Czechoslovakia to Hitler's demands at the Munich Agreement of September 1938. Instead of satiation, Hitler menaced Poland, and at last Prime Minister Neville Chamberlain dropped appeasement and stood firm in promising to defend Poland (31 March 1939). Hitler however cut a deal with Joseph Stalin to divide Eastern Europe (23 August 1939); when Germany did invade Poland in September 1939, Britain and France declared war, and the British Commonwealth followed London's lead.\nSecond World War.\nHaving signed the Anglo-Polish military alliance in August 1939, Britain and France declared war against Germany in September 1939 in response to Germany's invasion of Poland. This declaration included the Crown colonies and India, which Britain directly controlled. The dominions were independent in foreign policy, though all quickly entered the war against Germany. After the French defeat in June 1940, Britain and its empire stood alone in combat against Germany, until June 1941. The United States gave diplomatic, financial and material support, starting in 1940, especially through Lend Lease, which began in 1941 and attain full strength during 1943. In August 1941, Churchill and Roosevelt met and agreed on the Atlantic Charter, which proclaimed \"the rights of all peoples to choose the form of government under which they live\" should be respected. This wording was ambiguous and would be interpreted differently by the British, Americans, and nationalist movements.\nStarting in December 1941, Japan overran British possessions in Asia, including Hong Kong, Malaya, and especially the key base at Singapore. Japan then marched into Burma, headed toward India. Churchill's reaction to the entry of the United States into the war was that Britain was now assured of victory and the future of the empire was safe, but the rapid defeats irreversibly harmed Britain's standing and prestige as an imperial power. The realisation that Britain could not defend them pushed Australia and New Zealand into permanent close ties with the United States.\nPostwar.\nEconomically in dire straits in 1945 (saddled with debt and dealing with widespread destruction of its infrastructure), Britain systematically reduced its overseas commitments. It pursued an alternate role as an active participant in the Cold War against communism, especially as a founding member of NATO in 1949.\nThe British had built up a very large worldwide Empire, which peaked in size in 1922, after more than half a century of unchallenged global supremacy. The cumulative costs of fighting two world wars, however, placed a heavy burden upon the home economy, and after 1945 the British Empire rapidly began to disintegrate, with all the major colonies gaining independence. By the mid-to-late 1950s, the UK's status as a superpower was gone in the face of the United States and the Soviet Union. Most former colonies joined the \"Commonwealth of Nations\", an organisation of fully independent nations now with equal status to the UK. However it attempted no major collective policies. The last major colony, Hong Kong, was handed over to China in 1997. Fourteen British Overseas Territories maintain a constitutional link to the UK, but are not part of the country per se.\nBritain slashed its involvements in the Middle East after the humiliating Suez Crisis of 1956. However Britain did forge close military ties with the United States, France, and Germany, through the NATO military alliance. After years of debate (and rebuffs), Britain joined the Common Market in 1973; which became the European Union in 1993. However it did not merge financially, and kept the pound separate from the Euro, which partly isolated it from the Euro area crisis. In June 2016, the UK voted to leave the EU.\n21st century.\nForeign policy initiatives of UK governments since the 1990s have included military intervention in conflicts and for peacekeeping, humanitarian assistance programmes and increased aid spending, support for establishment of the International Criminal Court, debt relief for developing countries, prioritisation of initiatives to address climate change, and promotion of free trade. The British approach has been described as \"spread the right norms and sustain NATO\".\nLunn et al. (2008) argue:\nThree key motifs of Tony Blair's 10-year premiership were an activist philosophy of 'interventionism', maintaining a strong alliance with the US and a commitment to placing Britain at the heart of Europe. While the 'special relationship' and the question of Britain's role in Europe have been central to British foreign policy since the Second World War...interventionism was a genuinely new element.\nThe GREAT campaign of 2012 was one of the most ambitious national promotion efforts ever undertaken by any major nation. It was scheduled take maximum advantage of the worldwide attention to the Summer Olympics in London. The goals were to make British more culture visible in order to stimulate trade, investment and tourism. The government partnered with key leaders in culture, business, diplomacy and education. The campaign unified many themes and targets, including business meetings; scholarly conventions; recreational vehicle dealers; parks and campgrounds; convention and visitors bureaus; hotels; bed and breakfast inns; casinos; and hotels.\nIn 2013, the government of David Cameron described its approach to foreign policy by saying:\nFor any given foreign policy issue, the UK potentially has a range of options for delivering impact in our national interest. ... [W]e have a complex network of alliances and partnerships through which we can work... These include \u2013 besides the EU \u2013 the UN and groupings within it, such as the five permanent members of the Security Council (the \u201cP5\u201d); NATO; the Commonwealth; the Organisation for Economic Cooperation and Development; the G8 and G20 groups of leading industrialised nations; and so on.\nThe UK began establishing air and naval facilities in the Persian Gulf, located in the United Arab Emirates, Bahrain and Oman in 2014\u201315. The Strategic Defence and Security Review 2015 highlighted a range of foreign policy initiatives of the UK government. Edward Longinotti notes how current British defence policy is grappling with how to accommodate two major commitments, to Europe and to an \u2018east of Suez\u2019 global military strategy, within a modest defence budget that can only fund one. He points out that Britain's December 2014 agreement to open a permanent naval base in Bahrain underlines its gradual re-commitment east of Suez. By some measures, Britain remains the second most powerful country in the world by virtue of its soft power and \"logistical capability to deploy, support and sustain [military] forces overseas in large numbers.\" Although commentators have questioned the need for global power projection, the concept of \u201cGlobal Britain\u201d put forward by the Conservative government in 2019 signalled more military activity in the Middle East and Pacific, outside of NATO's traditional sphere of influence.\nAt the end of January 2020, the United Kingdom left the European Union, with a subsequent trade agreement with the EU in effect from 1 January 2021, setting out the terms of the UK-EU economic relationship and what abilities the Foreign, Commonwealth &amp; Development Office can use in foreign relations related to trade.\nDiplomatic relations.\nBritish diplomatic relations date back to the 13th century. The United Kingdom has established diplomatic relations with all United Nations members, aside from Bhutan, in addition to 2 Non-UN member states: Holy See, and Kosovo. Moreover, the UK established official relations with the Sovereign Military Order of Malta on 9 October 2024. The following table lists the date from which diplomatic relations were established with other countries:\nSovereignty disputes.\n&lt;br&gt;\nList of territorial disputes involving the United Kingdom:\nCommonwealth of Nations.\nThe UK has varied relationships with the countries that make up the Commonwealth of Nations which originated from the British Empire. Charles III of the United Kingdom is Head of the Commonwealth and is King of 15 of its 56 member states. Those that retain the King as head of state are called Commonwealth realms. Over time several countries have been suspended from the Commonwealth for various reasons. Zimbabwe was suspended because of the authoritarian rule of its President.\nInternational organisations.\nThe United Kingdom is a member of the following international organisations:\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "31733", "revid": "44924033", "url": "https://en.wikipedia.org/wiki?curid=31733", "title": "Dependent territories of the United Kingdom", "text": "Overview of British dependencies\nThere are a number of dependent territories of the United Kingdom throughout the world. As dependencies, they are not part of the UK proper, but nor are they independent states. Each has its own distinct legally defined relationship with the UK, with the monarchy of the United Kingdom as head of state. The remaining Crown colonies of the British Empire were renamed \"British Dependent Territories\" from 1 January 1983 under the British Nationality Act 1981, and were renamed again on 26 February 2002 to \"British Overseas Territories\" by the British Overseas Territories Act 2002. These territories fall into two broad categories: British Overseas Territories and Crown Dependencies.\nBritish Overseas Territories.\nThese former parts of the British Empire are not part of the UK proper, but the British crown and parliament has full sovereignty over each. They have varying degrees of delegated internal self-governance. The UK counts a total of 14 such territories. This includes the UK's view that its Antarctic claim is a dependency, though internationally its legal status is governed by the Antarctic Treaty.\nCrown Dependencies.\nThe Crown Dependencies are self-governing possessions of the British Crown with their own legislative assemblies. They were not part of the British Empire, but have a much older relationship as subjects of the English Crown. \nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "31734", "revid": "11555324", "url": "https://en.wikipedia.org/wiki?curid=31734", "title": "Urea", "text": "Organic compound\n&lt;templatestyles src=\"Chembox/styles.css\"/&gt;\nChemical compound\nUrea, also called carbamide (because it is a diamide of carbonic acid), is an organic compound with chemical formula . This amide has two amino groups (\u2212) joined by a carbonyl functional group (\u2212C(=O)\u2212). It is thus the simplest amide of carbamic acid.\nUrea serves an important role in the cellular metabolism of nitrogen-containing compounds by animals and is the main nitrogen-containing substance in the urine of mammals. \"Urea\" is Neo-Latin, from fr, from grc \" \"\" ()\"\u00a0'urine', itself from Proto-Indo-European \"*h\u2082worsom\".\nIt is a colorless, odorless solid, highly soluble in water, and practically non-toxic (LD50 is 15\u00a0g/kg for rats). Dissolved in water, it is neither acidic nor alkaline. The body uses it in many processes, most notably nitrogen excretion. The liver forms it by combining two ammonia molecules () with a carbon dioxide () molecule in the urea cycle. Urea is widely used in fertilizers as a source of nitrogen (N) and is an important raw material for the chemical industry.\nIn 1828, Friedrich W\u00f6hler discovered that urea can be produced from inorganic starting materials, an important conceptual milestone in chemistry. This showed for the first time that a substance previously known only as a byproduct of life could be synthesized in the laboratory from non-biological starting materials, thereby contradicting the widely held doctrine of vitalism, which stated that organic compounds could only be derived from living organisms.\n&lt;templatestyles src=\"Template:TOC limit/styles.css\" /&gt;\nProperties.\nMolecular and crystal structure.\nThe structure of the molecule of urea is . The urea molecule is planar when in a solid crystal because of sp2 hybridization of the N orbitals. It is non-planar with C2 symmetry when in the gas phase or in aqueous solution, with C\u2013N\u2013H and H\u2013N\u2013H bond angles that are intermediate between the trigonal planar angle of 120\u00b0 and the tetrahedral angle of 109.5\u00b0. In solid urea, the oxygen center is engaged in two N\u2013H\u2013O hydrogen bonds. The resulting hydrogen-bond network is probably established at the cost of efficient molecular packing: The structure is quite open, the ribbons forming tunnels with square cross-section. The carbon in urea is described as sp2 hybridized, the C-N bonds have significant double bond character, and the carbonyl oxygen is relatively basic. Urea's high aqueous solubility reflects its ability to engage in extensive hydrogen bonding with water.\nBy virtue of its tendency to form porous frameworks, urea has the ability to trap many organic compounds. In these so-called clathrates, the organic \"guest\" molecules are held in channels formed by interpenetrating helices composed of hydrogen-bonded urea molecules. In this way, urea-clathrates have been well investigated for separations.\nReactions.\nUrea is a weak base, with a p\"K\"b of 13.9. When combined with strong acids, it undergoes protonation at oxygen to form uronium salts. It is a Lewis base, forming metal complexes of the type .\nUrea reacts with malonic esters to make barbituric acids.\nThermolysis.\nMolten urea decomposes into ammonium cyanate at about 152\u00a0\u00b0C, and into ammonia and isocyanic acid above 160\u00a0\u00b0C:\n&lt;templatestyles src=\"Block indent/styles.css\"/&gt;\nHeating above 160\u00a0\u00b0C yields biuret and triuret via reaction with isocyanic acid:\n&lt;templatestyles src=\"Block indent/styles.css\"/&gt;\n&lt;templatestyles src=\"Block indent/styles.css\"/&gt;\nAt higher temperatures it converts to a range of condensation products, including cyanuric acid , guanidine , and melamine.\nAqueous stability.\nIn aqueous solution, urea slowly equilibrates with ammonium cyanate. This elimination reaction cogenerates isocyanic acid, which can carbamylate proteins, in particular the N-terminal amino group, the side chain amino of lysine, and to a lesser extent the side chains of arginine and cysteine. Each carbamylation event adds 43 daltons to the mass of the protein, which can be observed in protein mass spectrometry. For this reason, pure urea solutions should be freshly prepared and used, as aged solutions may develop a significant concentration of cyanate (20\u00a0mM in 8\u00a0M urea). Dissolving urea in ultrapure water followed by removing ions (i.e. cyanate) with a mixed-bed ion-exchange resin and storing that solution at 4\u00a0\u00b0C is a recommended preparation procedure. However, cyanate will build back up to significant levels within a few days. Alternatively, adding 25\u201350 mM ammonium chloride to a concentrated urea solution decreases formation of cyanate because of the common ion effect.\nAnalysis.\nUrea is readily quantified by a number of different methods, such as the diacetyl monoxime colorimetric method, and the Berthelot reaction (after initial conversion of urea to ammonia via urease). These methods are amenable to high throughput instrumentation, such as automated flow injection analyzers and 96-well micro-plate spectrophotometers.\nRelated compounds.\nUrea is the parent for a class of chemical compounds that share the same functional group. Namely, such compounds have a carbonyl group attached to two organic amine residues: , where groups are hydrogen (\u2013H), organyl or other groups. Examples include carbamide peroxide, allantoin, and hydantoin. Ureas are closely related to biurets and related in structure to amides, carbamates, carbodiimides, and thiocarbamides.\nUses.\nAgriculture.\nMore than 90% of world industrial production of urea is destined for use as a nitrogen-release fertilizer. Urea has the highest nitrogen content of all solid nitrogenous fertilizers in common use. Therefore, it has a low transportation cost per unit of nitrogen nutrient. The most common impurity of synthetic urea is biuret, which impairs plant growth. Urea breaks down in the soil to give ammonium ions (). The ammonium is taken up by the plant through its roots. In some soils, the ammonium is oxidized by bacteria to give nitrate (), which is also a nitrogen-rich plant nutrient. The loss of nitrogenous compounds to the atmosphere and runoff is wasteful and environmentally damaging so urea is sometimes modified to enhance the efficiency of its agricultural use. Techniques to make controlled-release fertilizers that slow the release of nitrogen include the encapsulation of urea in an inert sealant, and conversion of urea into derivatives such as urea-formaldehyde compounds, which degrade into ammonia at a pace matching plants' nutritional requirements.\nResins.\nUrea is a raw material for the manufacture of formaldehyde based resins, such as UF, MUF, and MUPF, used mainly in wood-based panels, for instance, particleboard, fiberboard, OSB, and plywood.\nExplosives.\nUrea can be used in a reaction with nitric acid to make urea nitrate, a high explosive that is used industrially and as part of some improvised explosive devices.\nAutomobile systems.\nUrea is used in Selective Non-Catalytic Reduction (SNCR) and Selective Catalytic Reduction (SCR) reactions to reduce the pollutants in exhaust gases from combustion from diesel, dual fuel, and lean-burn natural gas engines. The BlueTec system, for example, injects a water-based urea solution into the exhaust system. Ammonia () produced by the hydrolysis of urea reacts with nitrogen oxides () and is converted into nitrogen gas () and water within the catalytic converter. The conversion of noxious to innocuous is described by the following simplified global equation:\n&lt;templatestyles src=\"Block indent/styles.css\"/&gt;\nWhen urea is used, a pre-reaction (hydrolysis) occurs to first convert it to ammonia:\n&lt;templatestyles src=\"Block indent/styles.css\"/&gt;\nBeing a solid highly soluble in water (545 g/L at 25\u00a0\u00b0C), urea is much easier and safer to handle and store than the more irritant, caustic and hazardous ammonia (), so it is the reactant of choice. Trucks and cars using these catalytic converters need to carry a supply of diesel exhaust fluid, also sold as AdBlue, a solution of urea in water.\nLaboratory uses.\nUrea in concentrations up to 10\u00a0M is a powerful protein denaturant as it disrupts the noncovalent bonds in the proteins. This property can be exploited to increase the solubility of some proteins. A mixture of urea and choline chloride is used as a deep eutectic solvent (DES), a substance similar to ionic liquid. When used in a deep eutectic solvent, urea gradually denatures the proteins that are solubilized.\nUrea in concentrations up to 8\u00a0M can be used to make fixed brain tissue transparent to visible light while still preserving fluorescent signals from labeled cells. This allows for much deeper imaging of neuronal processes than previously obtainable using conventional one photon or two photon confocal microscopes.\nMedical use.\nUrea-containing creams are used as topical dermatological products to promote rehydration of the skin. Urea 40% is indicated for psoriasis, xerosis, onychomycosis, ichthyosis, eczema, keratosis, keratoderma, corns, and calluses. If covered by an occlusive dressing, 40% urea preparations may also be used for nonsurgical debridement of nails. Urea 40% \"dissolves the intercellular matrix\" of the nail plate. Only diseased or dystrophic nails are removed, as there is no effect on healthy portions of the nail. This drug (as carbamide peroxide) is also used as an earwax removal aid.\nUrea has been studied as a diuretic. It was first used by Dr. W. Friedrich in 1892. In a 2010 study of ICU patients, urea was used to treat euvolemic hyponatremia and was found safe, inexpensive, and simple.\nLike saline, urea has been injected into the uterus to induce abortion, although this method is no longer in widespread use.\nThe blood urea nitrogen (BUN) test is a measure of the amount of nitrogen in the blood that comes from urea. It is used as a marker of renal function, though it is inferior to other markers such as creatinine because blood urea levels are influenced by other factors such as diet, dehydration, and liver function.\nUrea has also been studied as an excipient in drug-coated balloon (DCB) coating formulations to enhance local drug delivery to stenotic blood vessels. Urea, when used as an excipient in small doses (~3 \u03bcg/mm2) to coat DCB surface was found to form crystals that increase drug transfer without adverse toxic effects on vascular endothelial cells.\nUrea labeled with carbon-14 or carbon-13 is used in the urea breath test, which is used to detect the presence of the bacterium \"Helicobacter pylori\" (\"H. pylori\") in the stomach and duodenum of humans, associated with peptic ulcers. The test detects the characteristic enzyme urease, produced by \"H. pylori\", by a reaction that produces ammonia from urea. This increases the pH (reduces the acidity) of the stomach environment around the bacteria. Similar bacteria species to \"H. pylori\" can be identified by the same test in animals such as apes, dogs, and cats (including big cats).\nPhysiology.\nAmino acids from ingested food (or produced from catabolism of muscle protein) that are used for the synthesis of proteins and other biological substances can be oxidized by the body as an alternative source of energy, yielding urea and carbon dioxide. The oxidation pathway starts with the removal of the amino group by a transaminase; the amino group is then fed into the urea cycle. The first step in the conversion of amino acids into metabolic waste in the liver is removal of the alpha-amino nitrogen, which produces ammonia. Because ammonia is toxic, it is excreted immediately by fish, converted into uric acid by birds, and converted into urea by mammals.\nAmmonia () is a common byproduct of the metabolism of nitrogenous compounds. Ammonia is smaller, more volatile, and more mobile than urea. If allowed to accumulate, ammonia would raise the pH in cells to toxic levels. Therefore, many organisms convert ammonia to urea, even though this synthesis has a net energy cost. Being practically neutral and highly soluble in water, urea is a safe vehicle for the body to transport and excrete excess nitrogen.\nUrea is synthesized in the body of many organisms as part of the urea cycle, either from the oxidation of amino acids or from ammonia. In this cycle, amino groups donated by ammonia and L-aspartate are converted to urea, while L-ornithine, citrulline, L-argininosuccinate, and L-arginine act as intermediates. Urea production occurs in the liver and is regulated by \"N\"-acetylglutamate. Urea is then dissolved into the blood (in the reference range of 2.5 to 6.7\u00a0mmol/L) and further transported and excreted by the kidney as a component of urine. In addition, a small amount of urea is excreted (along with sodium chloride and water) in sweat.\nIn water, the amine groups undergo slow displacement by water molecules, producing ammonia, ammonium ions, and bicarbonate ions. For this reason, old, stale urine has a stronger odor than fresh urine.\nHumans.\nThe cycling of and excretion of urea by the kidneys is a vital part of mammalian metabolism. Besides its role as carrier of waste nitrogen, urea also plays a role in the countercurrent exchange system of the nephrons, that allows for reabsorption of water and critical ions from the excreted urine. Urea is reabsorbed in the inner medullary collecting ducts of the nephrons, thus raising the osmolarity in the medullary interstitium surrounding the thin descending limb of the loop of Henle, which makes the water reabsorb.\nBy action of the urea transporter 2, some of this reabsorbed urea eventually flows back into the thin descending limb of the tubule, through the collecting ducts, and into the excreted urine. The body uses this mechanism, which is controlled by the antidiuretic hormone, to create hyperosmotic urine \u2014 i.e., urine with a higher concentration of dissolved substances than the blood plasma. This mechanism is important to prevent the loss of water, maintain blood pressure, and maintain a suitable concentration of sodium ions in the blood plasma.\nThe equivalent nitrogen content (in grams) of urea (in mmol) can be estimated by the conversion factor 0.028\u00a0g/mmol. Furthermore, 1\u00a0gram of nitrogen is roughly equivalent to 6.25\u00a0grams of protein, and 1\u00a0gram of protein is roughly equivalent to 5\u00a0grams of muscle tissue. In situations such as muscle wasting, 1\u00a0mmol of excessive urea in the urine (as measured by urine volume in litres multiplied by urea concentration in mmol/L) roughly corresponds to a muscle loss of 0.67\u00a0gram.\nOther species.\nIn aquatic organisms the most common form of nitrogen waste is ammonia, whereas land-dwelling organisms convert the toxic ammonia to either urea or uric acid. Urea is found in the urine of mammals and amphibians, as well as some fish. Birds and saurian reptiles have a different form of nitrogen metabolism that requires less water, and leads to nitrogen excretion in the form of uric acid. Tadpoles excrete ammonia, but shift to urea production during metamorphosis. Despite the generalization above, the urea pathway has been documented not only in mammals and amphibians, but in many other organisms as well, including birds, invertebrates, insects, plants, yeast, fungi, and even microorganisms.\nAdverse effects.\nUrea can be irritating to skin, eyes, and the respiratory tract. Repeated or prolonged contact with urea in fertilizer form on the skin may cause dermatitis.\nHigh concentrations in the blood can be damaging. Ingestion of low concentrations of urea, such as are found in typical human urine, are not dangerous with additional water ingestion within a reasonable time-frame. Many animals (e.g. camels, rodents or dogs) have a much more concentrated urine which may contain a higher urea amount than normal human urine.\nUrea can cause algal blooms to produce toxins, and its presence in the runoff from fertilized land may play a role in the increase of toxic blooms.\nThe substance decomposes on heating above melting point, producing toxic gases, and reacts violently with strong oxidants, nitrites, inorganic chlorides, chlorites and perchlorates, causing fire and explosion.\nHistory.\nUrea was discovered in urine in 1727 by the Dutch scientist Herman Boerhaave, although this discovery is often attributed to the French chemist Hilaire Rouelle as well as William Cruickshank.\nBoerhaave used the following steps to isolate urea:\nIn 1828, the German chemist Friedrich W\u00f6hler obtained urea artificially by treating silver cyanate with ammonium chloride.\n&lt;templatestyles src=\"Block indent/styles.css\"/&gt;\nThis was one of the first artificial syntheses of organic compounds from inorganic starting materials, without the involvement of living organisms. The results of this experiment implicitly discredited vitalism, the theory that the chemicals of living organisms are fundamentally different from those of inanimate matter. This insight was important for the development of organic chemistry. His discovery prompted W\u00f6hler to write triumphantly to J\u00f6ns Jakob Berzelius:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;I must tell you that I can make urea without the use of kidneys, either man or dog. Ammonium cyanate is urea.\nHis second sentence was incorrect. Ammonium cyanate and urea are two different chemicals with the same empirical formula , which are in chemical equilibrium heavily favoring urea under standard conditions. Regardless, with his discovery, W\u00f6hler secured a place among the pioneers of organic chemistry.\nUremic frost was first described in 1856 by the Austrian physician Anton Drasche. Uremic frost has become rare since the advent of dialysis. It is the classical pre-dialysis era description of crystallized urea deposits over the skin of patients with prolonged kidney failure and severe uremia.\nHistorical preparation.\nUrea was first noticed by Herman Boerhaave in the early 18th century from evaporates of urine. In 1773, Hilaire Rouelle obtained crystals containing urea from human urine by evaporating it and treating it with alcohol in successive filtrations. This method was aided by Carl Wilhelm Scheele's discovery that urine treated by concentrated nitric acid precipitated crystals. Antoine Fran\u00e7ois, comte de Fourcroy and Louis Nicolas Vauquelin discovered in 1799 that the nitrated crystals were identical to Rouelle's substance and invented the term \"urea.\" Berzelius made further improvements to its purification and finally William Prout, in 1817, succeeded in obtaining and determining the chemical composition of the pure substance. In the evolved procedure, urea was precipitated as urea nitrate by adding strong nitric acid to urine. To purify the resulting crystals, they were dissolved in boiling water with charcoal and filtered. After cooling, pure crystals of urea nitrate form. To reconstitute the urea from the nitrate, the crystals are dissolved in warm water, and barium carbonate added. The water is then evaporated and anhydrous alcohol added to extract the urea. This solution is drained off and evaporated, leaving pure urea.\nLaboratory preparation.\nUrea can be produced by heating ammonium cyanate to 60\u00a0\u00b0C.\n&lt;templatestyles src=\"Block indent/styles.css\"/&gt;\nIndustrial production.\nIn 2020, worldwide production capacity was approximately 180 million tonnes.\nFor use in industry, urea is produced from synthetic ammonia and carbon dioxide. As large quantities of carbon dioxide are produced during the ammonia manufacturing process as a byproduct of burning hydrocarbons to generate heat (predominantly natural gas, and less often petroleum derivatives or coal), urea production plants are almost always located adjacent to the site where the ammonia is manufactured.\nSynthesis.\nThe basic process, patented in 1922, is called the \"Bosch\u2013Meiser urea process\" after its discoverers Carl Bosch and Wilhelm Meiser. The process consists of two main equilibrium reactions, with incomplete conversion of the reactants. The first is carbamate formation: the fast exothermic reaction of liquid ammonia with gaseous carbon dioxide () at high temperature and pressure to form ammonium carbamate ():\n&lt;templatestyles src=\"Block indent/styles.css\"/&gt;(\u0394\"H\" = \u2212117\u00a0kJ/mol at 110\u00a0atm and 160\u00a0\u00b0C)\nThe second is urea conversion: the slower endothermic decomposition of ammonium carbamate into urea and water:\n&lt;templatestyles src=\"Block indent/styles.css\"/&gt;(\u0394\"H\" = +15.5\u00a0kJ/mol at 160\u2013180\u00a0\u00b0C)\nThe overall conversion of and to urea is exothermic, with the reaction heat from the first reaction driving the second. The conditions that favor urea formation (high temperature) have an unfavorable effect on the carbamate formation equilibrium. The process conditions are a compromise: the ill-effect on the first reaction of the high temperature (around 190\u00a0\u00b0C) needed for the second is compensated for by conducting the process under high pressure (140\u2013175\u00a0bar), which favors the first reaction. Although it is necessary to compress gaseous carbon dioxide to this pressure, the ammonia is available from the ammonia production plant in liquid form, which can be pumped into the system much more economically. To allow the slow urea formation reaction time to reach equilibrium, a large reaction space is needed, so the synthesis reactor in a large urea plant tends to be a massive pressure vessel.\nReactant recycling.\nBecause the urea conversion is incomplete, the urea must be separated from the unconverted reactants, including the ammonium carbamate. Various commercial urea processes are characterized by the conditions under which urea forms and the way that unconverted reactants are further processed.\nConventional recycle processes.\nIn early \"straight-through\" urea plants, reactant recovery (the first step in \"recycling\") was done by letting down the system pressure to atmospheric to let the carbamate decompose back to ammonia and carbon dioxide. Originally, because it was not economic to recompress the ammonia and carbon dioxide for recycle, the ammonia at least would be used for the manufacture of other products such as ammonium nitrate or ammonium sulfate, and the carbon dioxide was usually wasted. Later process schemes made recycling unused ammonia and carbon dioxide practical. This was accomplished by the \"total recycle process\", developed in the 1940s to 1960s and now called the \"conventional recycle process\". It proceeds by depressurizing the reaction solution in stages (first to 18\u201325 bar and then to 2\u20135 bar) and passing it at each stage through a steam-heated \"carbamate decomposer\", then recombining the resulting carbon dioxide and ammonia in a falling-film \"carbamate condenser\" and pumping the carbamate solution back into the urea reaction vessel.\nStripping recycle process.\nThe \"conventional recycle process\" for recovering and reusing the reactants has largely been supplanted by a stripping process, developed in the early 1960s by Stamicarbon in The Netherlands, that operates at or near the full pressure of the reaction vessel. It reduces the complexity of the multi-stage recycle scheme, and it reduces the amount of water recycled in the carbamate solution, which has an adverse effect on the equilibrium in the urea conversion reaction and thus on overall plant efficiency. Effectively all new urea plants use the stripper, and many total recycle urea plants have converted to a stripping process.\nIn the conventional recycle processes, carbamate decomposition is promoted by reducing the overall pressure, which reduces the partial pressure of both ammonia and carbon dioxide, allowing these gasses to be separated from the urea product solution. The stripping process achieves a similar effect without lowering the overall pressure, by suppressing the partial pressure of just one of the reactants in order to promote carbamate decomposition. Instead of feeding carbon dioxide gas directly to the urea synthesis reactor with the ammonia, as in the conventional process, the stripping process first routes the carbon dioxide through the stripper. The stripper is a carbamate decomposer that provides a large amount of gas-liquid contact. This flushes out free ammonia, reducing its partial pressure over the liquid surface and carrying it directly to a carbamate condenser (also under full system pressure). From there, reconstituted ammonium carbamate liquor is passed to the urea production reactor. That eliminates the medium-pressure stage of the conventional recycle process.\nSide reactions.\nThe three main side reactions that produce impurities have in common that they decompose urea.\nUrea hydrolyzes back to ammonium carbamate in the hottest stages of the synthesis plant, especially in the stripper, so residence times in these stages are designed to be short.\nBiuret is formed when two molecules of urea combine with the loss of a molecule of ammonia.\n&lt;templatestyles src=\"Block indent/styles.css\"/&gt;\nNormally this reaction is suppressed in the synthesis reactor by maintaining an excess of ammonia, but after the stripper, it occurs until the temperature is reduced. Biuret is undesirable in urea fertilizer because it is toxic to crop plants to varying degrees, but it is sometimes desirable as a nitrogen source when used in animal feed.\nIsocyanic acid HNCO and ammonia results from the thermal decomposition of ammonium cyanate , which is in chemical equilibrium with urea:\n&lt;templatestyles src=\"Block indent/styles.css\"/&gt;\nThis decomposition is at its worst when the urea solution is heated at low pressure, which happens when the solution is concentrated for prilling or granulation (see below). The reaction products mostly volatilize into the overhead vapours, and recombine when these condense to form urea again, which contaminates the process condensate.\nCorrosion.\nAmmonium carbamate solutions are highly corrosive to metallic construction materials \u2013 even to resistant forms of stainless steel \u2013 especially in the hottest parts of the synthesis plant such as the stripper. Historically corrosion has been minimized (although not eliminated) by continuous injection of a small amount of oxygen (as air) into the plant to establish and maintain a passive oxide layer on exposed stainless steel surfaces. Highly corrosion resistant materials have been introduced to reduce the need for passivation oxygen, such as specialized duplex stainless steels in the 1990s, and zirconium or zirconium-clad titanium tubing in the 2000s.\nFinishing.\nUrea can be produced in solid forms (prills, granules, pellets or crystals) or as solutions.\nSolid forms.\nFor its main use as a fertilizer urea is mostly marketed in solid form, either as prills or granules. Prills are solidified droplets, whose production predates satisfactory urea granulation processes. Prills can be produced more cheaply than granules, but the limited size of prills (up to about 2.1\u00a0mm in diameter), their low crushing strength, and the caking or crushing of prills during bulk storage and handling make them inferior to granules. Granules are produced by acretion onto urea seed particles by spraying liquid urea in a succession of layers. Formaldehyde is added during the production of both prills and granules in order to increase crushing strength and suppress caking. Other shaping techniques such as pastillization (depositing uniform-sized liquid droplets onto a cooling conveyor belt) are also used.\nLiquid forms.\nSolutions of urea and ammonium nitrate in water (UAN) are commonly used as a liquid fertilizer. In admixture, the combined solubility of ammonium nitrate and urea is so much higher than that of either component alone that it gives a stable solution with a total nitrogen content (32%) approaching that of solid ammonium nitrate (33.5%), though not, of course, that of urea itself (46%). UAN allows use of ammonium nitrate without the explosion hazard. UAN accounts for 80% of the liquid fertilizers in the US.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "31736", "revid": "18377792", "url": "https://en.wikipedia.org/wiki?curid=31736", "title": "Uric acid", "text": "Organic compound\n&lt;templatestyles src=\"Chembox/styles.css\"/&gt;\nChemical compound\nUric acid is a heterocyclic compound of carbon, nitrogen, oxygen, and hydrogen with the formula C5H4N4O3. It forms ions and salts known as urates and acid urates, such as ammonium acid urate. Uric acid is a product of the metabolic breakdown of purine nucleotides, and it is a normal component of urine. High blood concentrations of uric acid can lead to gout and are associated with other medical conditions, including diabetes and the formation of ammonium acid urate kidney stones.\nChemistry.\nUric acid was first isolated from kidney stones in 1776 by Swedish chemist Carl Wilhelm Scheele. In 1882, the Ukrainian chemist Ivan Horbaczewski first synthesized uric acid by melting urea with glycine.\nUric acid displays lactam\u2013lactim tautomerism. Uric acid crystallizes in the lactam form, with computational chemistry also indicating that tautomer to be the most stable. Uric acid is a diprotic acid with p\"K\"a1\u00a0=\u00a05.4 and p\"K\"a2\u00a0=\u00a010.3. At physiological pH, urate predominates in solution.\nBiochemistry.\nThe enzyme xanthine oxidase (XO) catalyzes the formation of uric acid from xanthine and hypoxanthine. XO, which is found in mammals, functions primarily as a dehydrogenase and rarely as an oxidase, despite its name. Xanthine in turn is produced from other purines. Xanthine oxidase is a large enzyme whose active site consists of the metal molybdenum bound to sulfur and oxygen. Uric acid is released in hypoxic conditions (low oxygen saturation).\nWater solubility.\nIn general, the water solubility of uric acid and its alkali metal and alkaline earth salts is rather low. All these salts exhibit greater solubility in hot water than cold, allowing for easy recrystallization. This low solubility is significant for the etiology of gout. The solubility of the acid and its salts in ethanol is very low or negligible. In ethanol/water mixtures, the solubilities are somewhere between the end values for pure ethanol and pure water.\nThe figures given indicate what mass of water is required to dissolve a unit mass of compound indicated. The lower the number, the more soluble the substance in the said solvent.\nGenetic and physiological diversity.\nPrimates.\nIn humans uric acid (actually hydrogen urate ion) is the final oxidation (breakdown) product of purine metabolism and is excreted in urine, whereas in most other mammals, the enzyme uricase further oxidizes uric acid to allantoin. The loss of uricase in higher primates parallels the similar loss of the ability to synthesize ascorbic acid, leading to the suggestion that urate may partially substitute for ascorbate in such species. Both uric acid and ascorbic acid are strong reducing agents (electron donors) and potent antioxidants. In humans, over half the antioxidant capacity of blood plasma comes from hydrogen urate ion.\nHumans.\nThe normal concentration range of uric acid (or hydrogen urate ion) in human blood is 25 to 80\u00a0mg/L for men and 15 to 60\u00a0mg/L for women (but see below for slightly different values). An individual can have serum values as high as 96\u00a0mg/L and not have gout. In humans, about 70% of daily uric acid disposal occurs via the kidneys, and in 5\u201325% of humans, impaired renal (kidney) excretion leads to hyperuricemia. Normal excretion of uric acid in the urine is 270 to 360\u00a0mg per day (concentration of 270 to 360\u00a0mg/L if one litre of urine is produced per day \u2013 higher than the solubility of uric acid because it is in the form of dissolved acid urates), roughly 1% as much as the daily excretion of urea.\nDogs.\nThe Dalmatian has a genetic defect in uric acid uptake by the liver and kidneys, resulting in decreased conversion to allantoin, so this breed excretes uric acid, and not allantoin, in the urine.\nBirds, reptiles and desert-dwelling mammals.\nIn birds and reptiles, and in some desert-dwelling mammals (such as the kangaroo rat), uric acid also is the end product of purine metabolism, but it is excreted in feces as a dry mass. This involves a complex metabolic pathway that is energetically costly in comparison to processing of other nitrogenous wastes such as urea (from the urea cycle) or ammonia, but has the advantages of reducing water loss and preventing dehydration.\nInvertebrates.\n\"Platynereis dumerilii\", a marine polychaete worm, uses uric acid as a sexual pheromone. The female of the species releases uric acid into the water during mating, which induces males to release sperm.\nBacteria.\nUric acid metabolism is done in the human gut by ~1/5 of bacteria species that come from 4 of 6 major phyla. Such metabolism is anaerobic involving uncharacterized ammonia lyase, peptidase, carbamoyl transferase, and oxidoreductase enzymes. The result is that uric acid is converted into xanthine or lactate and the short chain fatty acids such as acetate and butyrate. Radioisotope studies suggest about 1/3 of uric acid is removed in healthy people in their gut with this being roughly 2/3 in those with kidney disease. In mouse models, such bacteria compensate for the loss of uricase leading researchers to raise the possibility \"that antibiotics targeting anaerobic bacteria, which would ablate gut bacteria, increase the risk for developing gout in humans\".\nGenetics.\nAlthough foods such as meat and seafood can elevate serum urate levels, genetic variation is a much greater contributor to high serum urate. A proportion of people have mutations in the urate transport proteins responsible for the excretion of uric acid by the kidneys. Variants of a number of genes, linked to serum urate, have so far been identified: \"SLC2A9\"; \"ABCG2\"; \"SLC17A1\"; \"SLC22A11\"; \"SLC22A12\"; \"SLC16A9\"; \"GCKR\"; \"LRRC16A\"; and \"PDZK1\". GLUT9, encoded by the \"SLC2A9\" gene, is known to transport both uric acid and fructose.\nMyogenic hyperuricemia, as a result of the purine nucleotide cycle running when ATP reservoirs in muscle cells are low, is a common pathophysiologic feature of glycogenoses, such as GSD-III, which is a metabolic myopathy impairing the ability of ATP (energy) production for muscle cells. In these metabolic myopathies, myogenic hyperuricemia is exercise-induced; inosine, hypoxanthine and uric acid increase in plasma after exercise and decrease over hours with rest. Excess AMP (adenosine monophosphate) is converted into uric acid.\nAMP \u2192 IMP \u2192 Inosine \u2192 Hypoxanthine \u2192 Xanthine \u2192 Uric Acid\nClinical significance and research.\nIn human blood plasma, the reference range of uric acid is typically 3.4\u20137.2\u00a0mg per 100\u00a0mL(200\u2013430\u00a0\u03bcmol/L) for men, and 2.4\u20136.1\u00a0mg per 100\u00a0mL for women (140\u2013360\u00a0\u03bcmol/L). Uric acid concentrations in blood plasma above and below the normal range are known as, respectively, hyperuricemia and hypouricemia. Likewise, uric acid concentrations in urine above and below normal are known as hyperuricosuria and hypouricosuria. Uric acid levels in saliva may be associated with blood uric acid levels.\nHigh uric acid.\nHyperuricemia (high levels of uric acid), which induces gout, has various potential origins:\nGout.\nA 2011 survey in the United States indicated that 3.9% of the population had gout, whereas 21.4% had hyperuricemia without having symptoms.\nExcess blood uric acid (serum urate) can induce gout, a painful condition resulting from needle-like crystals of uric acid termed \"monosodium urate crystals\" precipitating in joints, capillaries, skin, and other tissues. Gout can occur where serum uric acid levels are as low as 6\u00a0mg per 100\u00a0mL (357\u00a0\u03bcmol/L), but an individual can have serum values as high as 9.6\u00a0mg per 100\u00a0mL (565\u00a0\u03bcmol/L) and not have gout.\nIn humans, purines are metabolized into uric acid, which is then excreted in the urine. Consumption of large amounts of some types of purine-rich foods, particularly meat and seafood, increases gout risk. Purine-rich foods include liver, kidney, and sweetbreads, and certain types of seafood, including anchovies, herring, sardines, mussels, scallops, trout, haddock, mackerel, and tuna. Moderate intake of purine-rich vegetables, however, is not associated with an increased risk of gout.\nOne treatment for gout in the 19th century was administration of lithium salts; lithium urate is more soluble. Today, inflammation during attacks is more commonly treated with NSAIDs, colchicine, or corticosteroids, and urate levels are managed with allopurinol. Allopurinol, which weakly inhibits xanthine oxidase, is an analog of hypoxanthine that is hydroxylated by xanthine oxidoreductase at the 2-position to give oxipurinol.\nTumor lysis syndrome.\nTumor lysis syndrome, an emergency condition that may result from blood cancers, produces high uric acid levels in blood when tumor cells release their contents into the blood, either spontaneously or following chemotherapy. Tumor lysis syndrome may lead to acute kidney injury when uric acid crystals are deposited in the kidneys. Treatment includes\u00a0hyperhydration to dilute and excrete uric acid via urine, rasburicase to reduce levels of poorly soluble uric acid in blood, or\u00a0allopurinol to inhibit purine catabolism from adding to uric acid levels.\nLesch\u2013Nyhan syndrome.\nLesch\u2013Nyhan syndrome, a rare inherited disorder, is also associated with high serum uric acid levels. Spasticity, involuntary movement, and cognitive retardation as well as manifestations of gout are seen in this syndrome.\nCardiovascular disease.\nHyperuricemia is associated with an increase in risk factors for cardiovascular disease. It is also possible that high levels of uric acid may have a causal role in the development of atherosclerotic cardiovascular disease, but this is controversial and the data are conflicting.\nUric acid stone formation.\nKidney stones can form through deposits of sodium urate microcrystals.\nSaturation levels of uric acid in blood may result in one form of kidney stones when the urate crystallizes in the kidney. These uric acid stones are radiolucent, so do not appear on an abdominal plain X-ray. Uric acid crystals can also promote the formation of calcium oxalate stones, acting as \"seed crystals\".\nDiabetes.\nHyperuricemia is associated with components of metabolic syndrome, including in children.\nLow uric acid.\nLow uric acid (hypouricemia) can have numerous causes. Low dietary zinc intakes cause lower uric acid levels. This effect can be even more pronounced in women taking oral contraceptive medication. Sevelamer, a drug indicated for prevention of hyperphosphataemia in people with chronic kidney failure, can significantly reduce serum uric acid.\nMultiple sclerosis.\nMeta-analysis of 10 case-control studies found that the serum uric acid levels of patients with multiple sclerosis were significantly lower compared to those of healthy controls, possibly indicating a diagnostic biomarker for multiple sclerosis.\nNormalizing low uric acid.\nCorrecting low or deficient zinc levels can help elevate serum uric acid.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "31737", "revid": "34705915", "url": "https://en.wikipedia.org/wiki?curid=31737", "title": "Supreme Court of the United States", "text": "Highest court of jurisdiction in the US\nThe Supreme Court of the United States (SCOTUS) is the highest court in the federal judiciary of the United States. It has ultimate appellate jurisdiction over all U.S. federal court cases, and over state court cases that turn on questions of U.S. constitutional or federal law. It also has original jurisdiction over a narrow range of cases, specifically \"all Cases affecting Ambassadors, other public Ministers and Consuls, and those in which a State shall be Party\". In 1803, the court asserted itself the power of judicial review, the ability to invalidate a statute for violating a provision of the Constitution. It is also able to strike down presidential directives for violating either the Constitution or statutory law.\nUnder Article Three of the United States Constitution, the composition and procedures of the Supreme Court were originally established by the 1st Congress through the Judiciary Act of 1789. The court consists of nine justices\u2014the chief justice of the United States and eight associate justices\u2014who meet at the Supreme Court Building in Washington, D.C. Justices have lifetime tenure, meaning they remain on the court until they die, retire, resign, or are impeached and removed from office. When a vacancy occurs, the president, with the advice and consent of the Senate, appoints a new justice.\nEach justice has a single vote in deciding the cases argued before the court. When in the majority, the chief justice decides who writes the opinion of the court; otherwise, the most senior justice in the majority assigns the task. A justice may write an opinion in concurrence with the court, or they may write a dissent, and these concurrences or dissents may also be joined by other justices. On average, the Supreme Court receives about 7,000 petitions for writs of certiorari each year, but only grants about 80.\nHistory.\nIn 1787, four years after the end of the American Revolutionary War, delegates to the 1787 Constitutional Convention convened in Philadelphia, where they debated the separation of powers between the legislative and executive departments and established parameters for a national judiciary as a third branch of the federal government. In the British tradition, judicial matters had been the responsibility of the royal (executive) authority. During the Constitutional Convention, delegates opposed to having a strong central government argued that national laws could be enforced by state courts. James Madison and others, advocated for a national judicial authority chosen by the national legislature. It was proposed that the judiciary should have a role in checking executive branch power to veto or revise laws.\nThe framers ultimately compromised by sketching only a general outline of the judiciary in Article Three of the United States Constitution, vesting federal judicial power in \"one supreme Court, and in such inferior Courts as the Congress may from time to time ordain and establish.\" They did not delineate the exact powers or prerogatives of the Supreme Court or determine how the judicial branch should be organized.\nThe 1st United States Congress provided the detailed organization of a federal judiciary through the Judiciary Act of 1789. They decided that the Supreme Court, as the country's highest judicial tribunal, would be based in the nation's capital and would be composed of a chief justice and five associate justices. The act also divided the country into judicial districts, which were in turn organized into circuits. Justices were required to hold circuit court twice a year in their assigned judicial district.\nImmediately after signing the act into law, President George Washington nominated John Jay as the court's new chief justice, and John Rutledge, William Cushing, Robert H. Harrison, James Wilson and John Blair Jr. as its associate justices. All six were confirmed by the U.S. Senate on September 26, 1789. Harrison declined to serve, and Washington later nominated James Iredell to replace him.\nThe Supreme Court held its inaugural session from February 2 through February 10, 1790, at the Royal Exchange in New York City, then the U.S. capital. A second session was held there in August 1790. The earliest sessions of the court were devoted to organizational proceedings, as the first cases did not reach it until 1791. When the nation's capital was moved to Philadelphia in 1790, the Supreme Court moved to Philadelphia with it. After initially meeting in present-day Independence Hall, the court established its chambers at city hall. When the capital moved to Washington, D.C., the court was held in the U.S. Capitol Building (see, e.g. Old Supreme Court Chamber) until 1935 when it moved to its own building.\nEarly beginnings.\nUnder chief justices Jay, Rutledge, and Ellsworth (1789\u20131801), the court heard few cases; its first decision was \"West v. Barnes\" (1791), a case involving procedure. As the court initially had only six members, every decision that it made by a majority was also made by two-thirds (voting four to two). However, Congress has always allowed less than the court's full membership to make decisions, starting with a quorum of four justices in 1789. The court lacked a home of its own and had little prestige, a situation not helped by the era's highest-profile case, \"Chisholm v. Georgia\" (1793), which was reversed within two years by the adoption of the Eleventh Amendment.\nThe court's power and prestige grew substantially during the Marshall Court (1801\u20131835). Under Marshall, the court established the power of judicial review over acts of Congress, including specifying itself as the supreme expositor of the Constitution (\"Marbury v. Madison\") and making several important constitutional rulings that gave shape and substance to the balance of power between the federal government and states, notably \"Martin v. Hunter's Lessee\", \"McCulloch v. Maryland\", and \"Gibbons v. Ogden\".\nThe Marshall Court also ended the practice of each justice issuing his opinion \"seriatim\", a remnant of British tradition, and instead issuing a single majority opinion. Also during Marshall's tenure, although beyond the court's control, the impeachment and acquittal of Justice Samuel Chase from 1804 to 1805 helped cement the principle of judicial independence.\nFrom Taney to Taft.\nThe Taney Court (1836\u20131864) made several important rulings, such as \"Sheldon v. Sill\", which held that while Congress may not limit the subjects the Supreme Court may hear, it may limit the jurisdiction of the lower federal courts to prevent them from hearing cases dealing with certain subjects. Nevertheless, it is primarily remembered for its ruling in \"Dred Scott v. Sandford\", which helped precipitate the American Civil War. In the Reconstruction era, the Chase, Waite, and Fuller Courts (1864\u20131910) interpreted the new Civil War amendments to the Constitution and developed the doctrine of substantive due process (\"Lochner v. New York\"; \"Adair v. United States\"). The size of the court was last changed in 1869, when it was set at nine.\nUnder the White and Taft Courts (1910\u20131930), the court held that the Fourteenth Amendment had incorporated some guarantees of the Bill of Rights against the states (\"Gitlow v. New York\"), grappled with the new antitrust statutes (\"Standard Oil Co. of New Jersey v. United States\"), upheld the constitutionality of military conscription (\"Selective Draft Law Cases\"), and brought the substantive due process doctrine to its first apogee (\"Adkins v. Children's Hospital\").\nNew Deal era.\nDuring the Hughes, Stone, and Vinson courts (1930\u20131953), the court gained its own accommodation in 1935 and changed its interpretation of the Constitution, giving a broader reading to the powers of the federal government to facilitate President Franklin D. Roosevelt's New Deal (most prominently \"West Coast Hotel Co. v. Parrish, Wickard v. Filburn\", \"United States v. Darby\", and \"United States v. Butler\"). During World War II, the court continued to favor government power, upholding the internment of Japanese Americans (\"Korematsu v. United States\") and the mandatory Pledge of Allegiance (\"Minersville School District v. Gobitis\"). Nevertheless, \"Gobitis\" was soon repudiated (\"West Virginia State Board of Education v. Barnette\"), and the \"Steel Seizure Case\" restricted the pro-government trend.\nThe Warren Court (1953\u20131969) dramatically expanded the force of Constitutional civil liberties. It held that segregation in public schools violates the Equal Protection Clause of the Fourteenth Amendment (\"Brown v. Board of Education\", \"Bolling v. Sharpe\", and \"Green v. County School Bd.\") and that legislative districts must be roughly equal in population (\"Reynolds v. Sims\"). It recognized a general right to privacy (\"Griswold v. Connecticut\"), limited the role of religion in public school, most prominently \"Engel v. Vitale\" and \"Abington School District v. Schempp\", incorporated most guarantees of the Bill of Rights against the states, prominently \"Mapp v. Ohio\" (the exclusionary rule) and \"Gideon v. Wainwright\" (right to appointed counsel), and required that criminal suspects be apprised of all these rights by police (\"Miranda v. Arizona\"). At the same time, the court limited defamation suits by public figures (\"New York Times Co. v. Sullivan\") and supplied the government with an unbroken run of antitrust victories.\nBurger, Rehnquist, and Roberts.\nThe Burger Court (1969\u20131986) saw a conservative shift. It also expanded \"Griswold\"'s right to privacy to strike down abortion laws (\"Roe v. Wade\") but divided deeply on affirmative action (\"Regents of the University of California v. Bakke\") and campaign finance regulation (\"Buckley v. Valeo\"). It also wavered on the death penalty, ruling first that most applications were defective (\"Furman v. Georgia\"), but later that the death penalty itself was not unconstitutional (\"Gregg v. Georgia\").\nThe Rehnquist Court (1986\u20132005) was known for its revival of judicial enforcement of federalism, emphasizing the limits of the Constitution's affirmative grants of power (\"United States v. Lopez\") and the force of its restrictions on those powers (\"Seminole Tribe v. Florida\", \"City of Boerne v. Flores\"). It struck down single-sex state schools as a violation of equal protection (\"United States v. Virginia\"), laws against sodomy as violations of substantive due process (\"Lawrence v. Texas\") and the line-item veto (\"Clinton v. New York\") but upheld school vouchers (\"Zelman v. Simmons-Harris\") and reaffirmed \"Roe\"'s restrictions on abortion laws (\"Planned Parenthood v. Casey\"). The court's decision in \"Bush v. Gore\", which ended the electoral recount during the 2000 United States presidential election, remains especially controversial with debate ongoing over the rightful winner and whether or not the ruling should set a precedent.\nThe Roberts Court (2005\u2013present) is regarded as more conservative and controversial than the Rehnquist Court. Some of its major rulings have concerned federal preemption (\"Wyeth v. Levine\"), civil procedure (\"Twombly\u2013Iqbal\"), voting rights and federal preclearance (\"Shelby County\"), abortion (\"Gonzales v. Carhart\" and \"Dobbs v. Jackson Women's Health Organization\"), climate change (\"Massachusetts v. EPA\"), same-sex marriage (\"United States v. Windsor\" and \"Obergefell v. Hodges\"), and the Bill of Rights, such as in \"Citizens United v. Federal Election Commission\" (First Amendment), \"Heller\u2013McDonald\u2013Bruen\" (Second Amendment), and \"Baze v. Rees\" (Eighth Amendment).\nComposition.\nNomination, confirmation, and appointment.\nArticle II, Section 2, Clause 2 of the United States Constitution, known as the Appointments Clause, empowers the president to nominate and, with the confirmation (advice and consent) of the United States Senate, to appoint public officials, including justices of the Supreme Court. This clause is one example of the system of checks and balances inherent in the Constitution. The president has the plenary power to nominate, while the Senate possesses the plenary power to reject or confirm the nominee. The Constitution sets no qualifications for service as a justice, such as age, citizenship, residence or prior judicial experience, thus a president may nominate anyone to serve, and the Senate may not set any qualifications or otherwise limit who the president can choose. Nonetheless, the Senate may deny confirmation to a candidate that it deems unqualified or unsuitable for the appointment.\nIn modern times, the confirmation process has attracted considerable attention from the press and advocacy groups, which lobby senators to confirm or to reject a nominee depending on whether their track record aligns with the group's views. The Senate Judiciary Committee conducts hearings and votes on whether the nomination should go to the full Senate with a positive, negative or neutral report. The committee's practice of personally interviewing nominees is relatively recent. The first nominee to appear before the committee was Harlan Fiske Stone in 1925, who sought to quell concerns about his links to Wall Street, and the modern practice of questioning began with John Marshall Harlan II in 1955. Once the committee reports out the nomination, the full Senate considers it. Rejections are relatively uncommon; the Senate has explicitly rejected twelve Supreme Court nominees, most recently Robert Bork, nominated by President Ronald Reagan in 1987.\nAlthough Senate rules do not necessarily allow a negative or tied vote in committee to block a nomination, prior to 2017 a nomination could be blocked by filibuster once debate had begun in the full Senate. President Lyndon B. Johnson's nomination of sitting associate justice Abe Fortas to succeed Earl Warren as Chief Justice in 1968 was the first successful filibuster of a Supreme Court nominee. It included both Republican and Democratic senators concerned with Fortas's ethics. President Donald Trump's nomination of Neil Gorsuch to the seat left vacant by Antonin Scalia's death was the second. Unlike the Fortas filibuster, only Democratic senators voted against cloture on the Gorsuch nomination, citing his perceived conservative judicial philosophy, and the Republican majority's prior refusal to take up President Barack Obama's nomination of Merrick Garland to fill the vacancy. This led the Republican majority to change the rules and eliminate the filibuster for Supreme Court nominations.\nNot every Supreme Court nominee has received a floor vote in the Senate. A president may withdraw a nomination before an actual confirmation vote occurs, typically because it is clear that the Senate will reject the nominee; this occurred with President George W. Bush's nomination of Harriet Miers in 2005. The Senate may also fail to act on a nomination, which expires at the end of the session. President Dwight Eisenhower's first nomination of John Marshall Harlan II in November 1954 was not acted on by the Senate; Eisenhower re-nominated Harlan in January 1955, and Harlan was confirmed two months later. Most recently, the Senate failed to act on the March 2016 nomination of Merrick Garland, as the nomination expired in January 2017, and the vacancy was filled by Neil Gorsuch, an appointee of President Trump.\nOnce the Senate confirms a nomination, the president must prepare and sign a commission, to which the Seal of the Department of Justice must be affixed, before the appointee can take office. The seniority of an associate justice is based on the commissioning date, not the confirmation or swearing-in date. After receiving their commission, the appointee must then take the two prescribed oaths before assuming their official duties. The importance of the oath taking is underscored by the case of Edwin M. Stanton. Although confirmed by the Senate on December 20, 1869, and duly commissioned as an associate justice by President Ulysses S. Grant, Stanton died on December 24, prior to taking the prescribed oaths. He is not, therefore, considered to have been a member of the court.\nBefore 1981, the approval process of justices was usually rapid. From the Truman through Nixon administrations, justices were typically approved within one month. From the Reagan administration to the present, the process has taken much longer and some believe this is because Congress sees justices as playing a more political role than in the past. According to the Congressional Research Service, the average number of days from nomination to final Senate vote since 1975 is 67 days (2.2 months), while the median is 71 days (2.3 months).\nRecess appointments.\nWhen the Senate is in recess, a president may make temporary appointments to fill vacancies. Recess appointees hold office only until the end of the next Senate session (less than two years). The Senate must confirm the nominee for them to continue serving; of the two chief justices and eleven associate justices who have received recess appointments, only Chief Justice John Rutledge was not subsequently confirmed.\nNo U.S. president since Dwight D. Eisenhower has made a recess appointment to the court, and the practice has become rare and controversial even in lower federal courts. In 1960, after Eisenhower had made three such appointments, the Senate passed a \"sense of the Senate\" resolution that recess appointments to the court should only be made in \"unusual circumstances\"; such resolutions are not legally binding but are an expression of Congress's views in the hope of guiding executive action.\nThe Supreme Court's 2014 decision in \"National Labor Relations Board v. Noel Canning\" limited the ability of the president to make recess appointments (including appointments to the Supreme Court); the court ruled that the Senate decides when the Senate is in session or in recess. Writing for the court, Justice Breyer stated, \"We hold that, for purposes of the Recess Appointments Clause, the Senate is in session when it says it is, provided that, under its own rules, it retains the capacity to transact Senate business.\" This ruling allows the Senate to prevent recess appointments through the use of pro-forma sessions.\nTenure.\nLifetime tenure of justices can only be found for US federal judges and the State of Rhode Island's Supreme Court justices, with all other democratic nations and all other US states having set term limits or mandatory retirement ages. Larry Sabato wrote: \"The insularity of lifetime tenure, combined with the appointments of relatively young attorneys who give long service on the bench, produces senior judges representing the views of past generations better than views of the current day.\" Sanford Levinson has been critical of justices who stayed in office despite medical deterioration based on longevity. James MacGregor Burns stated lifelong tenure has \"produced a critical time lag, with the Supreme Court institutionally almost always behind the times\". Proposals to solve these problems include term limits for justices, as proposed by Levinson and Sabato and a mandatory retirement age proposed by Richard Epstein, among others. Alexander Hamilton in \"Federalist 78\" argued that one benefit of lifetime tenure was that, \"nothing can contribute so much to its firmness and independence as permanency in office\".\nArticle Three, Section 1 of the Constitution provides that justices \"shall hold their offices during good behavior\", which is understood to mean that they may serve for the remainder of their lives, until death; furthermore, the phrase is generally interpreted to mean that the only way justices can be removed from office is by Congress via the impeachment process. The Framers of the Constitution chose good behavior tenure to limit the power to remove justices and to ensure judicial independence. No constitutional mechanism exists for removing a justice who is permanently incapacitated by illness or injury, but unable (or unwilling) to resign. The only justice ever to be impeached was Samuel Chase, in 1804. The House of Representatives adopted eight articles of impeachment against him; however, he was acquitted by the Senate, and remained in office until his death in 1811. Two justices, William O. Douglas and Abe Fortas were subjected to hearings from the Judiciary Committee, with Douglas being the subject of hearings twice, in 1953 and again in 1970 and Fortas resigned while hearings were being organized in 1969. On July 10, 2024, Representative Alexandria Ocasia-Cortez filed Articles of Impeachment against justices Clarence Thomas and Samuel Alito, citing their \"widely documented financial and personal entanglements\".\nBecause justices have indefinite tenure, timing of vacancies can be unpredictable. Sometimes they arise in quick succession, as in September 1971, when Hugo Black and John Marshall Harlan II left within days of each other, the shortest period of time between vacancies in the court's history. Sometimes a great length of time passes between vacancies, such as the 11-year span, from 1994 to 2005, from the retirement of Harry Blackmun to the death of William Rehnquist, which was the second longest timespan between vacancies in the court's history. On average a new justice joins the court about every two years.\nDespite the variability, all but four presidents have been able to appoint at least one justice. William Henry Harrison died a month after taking office, although his successor (John Tyler) made an appointment during that presidential term. Likewise, Zachary Taylor died 16 months after taking office, but his successor (Millard Fillmore) also made a Supreme Court nomination before the end of that term. Andrew Johnson, who became president after the assassination of Abraham Lincoln, was denied the opportunity to appoint a justice by a reduction in the size of the court. Jimmy Carter is the only person elected president to have left office after at least one full term without having the opportunity to appoint a justice. Presidents James Monroe, Franklin D. Roosevelt, and George W. Bush each served a full term without an opportunity to appoint a justice, but made appointments during their subsequent terms in office. No president who has served more than one full term has gone without at least one opportunity to make an appointment.\nSize of the court.\nOne of the smallest supreme courts in the world, the U.S. Supreme Court consists of nine members: one chief justice and eight associate justices. The U.S. Constitution does not specify the size of the Supreme Court, nor does it specify any specific positions for the court's members. The Constitution assumes the existence of the office of the chief justice, because it mentions in that \"the Chief Justice\" must preside over impeachment trials of the President of the United States. The power to define the Supreme Court's size and membership has been assumed to belong to Congress, which initially established a six-member Supreme Court composed of a chief justice and five associate justices through the Judiciary Act of 1789.\nThe size of the court was first altered by the Midnight Judges Act of 1801 which would have reduced the size of the court to five members upon its next vacancy (as federal judges have life tenure), but the Judiciary Act of 1802 promptly negated the 1801 act, restoring the court's size to six members before any such vacancy occurred. As the nation's boundaries grew across the continent and as Supreme Court justices in those days had to ride the circuit, an arduous process requiring long travel on horseback or carriage over harsh terrain that resulted in months-long extended stays away from home, Congress added justices to correspond with the growth such that the number of seats for associate justices plus the chief justice became seven in 1807, nine in 1837, and ten in 1863.\nAt the behest of Chief Justice Chase, and in an attempt by the Republican Congress to limit the power of Democrat Andrew Johnson, Congress passed the Judicial Circuits Act of 1866, providing that the next three justices to retire would not be replaced, which would thin the bench to seven justices by attrition. Consequently, one seat was removed in 1866 and a second in 1867. Soon after Johnson left office, the new president Ulysses S. Grant, a Republican, signed into law the Judiciary Act of 1869. This returned the number of justices to nine (where it has since remained), and allowed Grant to immediately appoint two more judges.\nPresident Franklin D. Roosevelt attempted to expand the court in 1937. His proposal envisioned the appointment of one additional justice for each incumbent justice who reached the age of 70years 6months and refused retirement, up to a maximum bench of 15 justices. The proposal was ostensibly to ease the burden of the docket on elderly judges, but the actual purpose was widely understood as an effort to \"pack\" the court with justices who would support Roosevelt's New Deal. The plan, usually called the \"court-packing plan\", failed in Congress after members of Roosevelt's own Democratic Party believed it to be unconstitutional. It was defeated 70\u201320 in the Senate, and the Senate Judiciary Committee reported that it was \"essential to the continuance of our constitutional democracy\" that the proposal \"be so emphatically rejected that its parallel will never again be presented to the free representatives of the free people of America.\"\nThe expansion of a 5\u20134 conservative majority to a 6\u20133 supermajority during the first presidency of Donald Trump led to analysts calling the court the most conservative since the 1930s as well as calls for an expansion in the court's size to fix what some saw as an imbalance, with Republicans having appointed 14 of the 18 justices immediately preceding Amy Coney Barrett. In April 2021, during the 117th Congress, some Democrats in the House of Representatives introduced the Judiciary Act of 2021, a bill to expand the Supreme Court from nine to 13 seats. It met divided views within the party, and Speaker of the House Nancy Pelosi did not bring it to the floor for a vote. Shortly after taking office in January 2021, President Joe Biden established a presidential commission to study possible reforms to the Supreme Court. The commission's December 2021 final report discussed but took no position on expanding the size of the court.\nAt nine members, the U.S. Supreme Court is one of the smallest supreme courts in the world. David Litt argues the court is too small to represent the perspectives of a country the United States' size. Lawyer and legal scholar Jonathan Turley has advocated for 19 justices, with the court being gradually expanded by two new members per presidential term, bringing the U.S. Supreme Court to a similar size as its counterparts in other developed countries. He says that a bigger court would reduce the power of the swing justice, ensure the court has \"a greater diversity of views\", and make confirmation of new justices less politically contentious.\nMembership.\nSitting justices.\nThere are nine justices on the Supreme Court: Chief Justice John Roberts and eight associate justices. Clarence Thomas is the longest-serving justice, with a tenure of days () as of . The most recent justice to join the court is Ketanji Brown Jackson, whose tenure began on June 30, 2022, after being confirmed by the Senate on April 7.\nThis graphical timeline depicts the length of each current Supreme Court justice's tenure (not seniority, as the chief justice has seniority over all associate justices regardless of tenure) on the court:\nCourt demographics.\nThe court has five male and four female justices. Among the nine justices, there are two African American justices (Justices Thomas and Jackson) and one Hispanic justice (Justice Sotomayor). One of the justices was born to at least one immigrant parent: Justice Alito's father was born in Italy.\nAt least six justices are Roman Catholics, one is Jewish, and one is Protestant. It is unclear whether Neil Gorsuch considers himself a Catholic or an Episcopalian. Historically, most justices have been Protestants, including 36 Episcopalians, 19 Presbyterians, 10 Unitarians, 5 Methodists, and 3 Baptists. The first Catholic justice was Roger Taney in 1836, and 1916 saw the appointment of the first Jewish justice, Louis Brandeis. In recent years the historical situation has reversed, as most recent justices have been either Catholic or Jewish.\nThree justices are from the state of New York, two are from Washington, D.C., and one each is from New Jersey, Georgia, Colorado, and Louisiana. Eight of the justices received their Juris Doctor from an Ivy League law school: Neil Gorsuch, Ketanji Brown Jackson, Elena Kagan and John Roberts from Harvard; plus Samuel Alito, Brett Kavanaugh, Sonia Sotomayor and Clarence Thomas from Yale. Only Amy Coney Barrett did not; she received her Juris Doctor at Notre Dame.\nPrevious positions or offices, judicial or federal government, prior to joining the court (by order of seniority following the Chief Justice) include:\nFor much of the court's history, every justice was a man of Northwestern European descent, and almost always Protestant. Diversity concerns focused on geography, to represent all regions of the country, rather than religious, ethnic, or gender diversity. Racial, ethnic, and gender diversity in the court increased in the late 20th century. Thurgood Marshall became the first African-American justice in 1967. Sandra Day O'Connor became the first female justice in 1981. In 1986, Antonin Scalia became the first Italian-American justice. Marshall was succeeded by African-American Clarence Thomas in 1991. O'Connor was joined by Ruth Bader Ginsburg, the first Jewish woman on the Court, in 1993. After O'Connor's retirement, Ginsburg was joined in 2009 by Sonia Sotomayor, the first Hispanic and Latina justice, and in 2010 by Elena Kagan. After Ginsburg's death on September 18, 2020, Amy Coney Barrett was confirmed as the fifth woman in the court's history on October 26, 2020. Ketanji Brown Jackson is the sixth woman and first African-American woman on the court.\nThere have been six foreign-born justices in the court's history: James Wilson (1789\u20131798), born in Caskardy, Scotland; James Iredell (1790\u20131799), born in Lewes, England; William Paterson (1793\u20131806), born in County Antrim, Ireland; David Brewer (1889\u20131910), born to American missionaries in Smyrna, Ottoman Empire (now \u0130zmir, Turkey); George Sutherland (1922\u20131939), born in Buckinghamshire, England; and Felix Frankfurter (1939\u20131962), born in Vienna, Austria-Hungary (later Austria).\nSince 1789, about one-third of the justices have been U.S. military veterans. Samuel Alito is the only veteran serving on the court. Retired justices Stephen Breyer and Anthony Kennedy also served in the U.S. military.\nJudicial leanings.\nJustices are nominated by the president in power, and receive confirmation by the Senate, historically holding many of the views of the nominating president's political party. While justices do not represent or receive official endorsements from political parties, as is accepted practice in the legislative and executive branches, advocacy groups have played a role in the selection, nomination, and confirmation process. The Justices are often informally categorized in the media as being conservatives or liberal. Attempts to quantify the ideologies of jurists include the Segal\u2013Cover score, Martin\u2013Quinn score, and Judicial Common Space score.\nDevins and Baum argue that before 2010, the Court never had clear ideological blocs that fell perfectly along party lines. In choosing their appointments, Presidents often focused more on friendship and political connections than on ideology. Republican presidents sometimes appointed liberals, and Democratic presidents sometimes appointed conservatives. As a result, \"... between 1790 and early 2010 there were only two decisions that the \"Guide to the U.S. Supreme Court\" designated as important and that had at least two dissenting votes in which the Justices divided along party lines, about one-half of one percent.\" Even in the turbulent 1960s and 1970s, Democratic and Republican elites tended to agree on some major issues, especially concerning civil rights and civil liberties\u2014and so did the justices. But since 1991, they argue, ideology has been much more important in choosing justices\u2014all Republican appointees have been committed conservatives, and all Democratic appointees have been liberals. As the more moderate Republican justices retired, the court has become more partisan. The Court became more sharply divided along partisan lines, with justices appointed by Republican presidents taking increasingly conservative positions and those appointed by Democrats taking increasingly liberal positions.\nFollowing the confirmation of Amy Coney Barrett in 2020 after the death of Ruth Bader Ginsburg, the court is composed of six justices appointed by Republican presidents and three appointed by Democratic presidents. It is popularly accepted that Chief Justice Roberts and associate justices Thomas, Alito, Gorsuch, Kavanaugh, and Barrett, appointed by Republican presidents, compose the court's conservative wing, and that Justices Sotomayor, Kagan, and Jackson, appointed by Democratic presidents, compose the court's liberal wing. Prior to Justice Ginsburg's death in 2020, the conservative Chief Justice Roberts was sometimes described as the court's 'median justice' (with four justices more liberal and four more conservative than he is). Darragh Roche argues that Kavanaugh as 2021's median justice exemplifies the rightward shift in the court. Noah Feldman describes the current ideological makeup of the court as a three-way split between the liberal faction, centrist-conservative faction, and arch-conservative faction, each with three members.\n\"FiveThirtyEight\" found the number of unanimous decisions dropped from the 20-year average of nearly 50% to nearly 30% in 2021 while party-line rulings increased from a 60-year average just above zero to a record high 21%. That year Ryan Williams pointed to the party-line votes for confirmations of justices as evidence that the court is of partisan importance to the Senate. In 2022, Simon Lazarus of Brookings critiqued the U.S. Supreme Court as an increasingly partisan institution. A 2024 AP-NORC poll showed 7 in 10 respondents believed the court decides cases to \"fit their own ideologies\" as opposed to \"acting as an independent check on other branches of government by being fair and impartial.\"\nRetired justices.\nThere are two living retired justices of the Supreme Court of the United States: Anthony Kennedy and Stephen Breyer. As retired justices, they no longer participate in the work of the Supreme Court, but may be designated for temporary assignments to sit on lower federal courts, usually the United States Courts of Appeals. Such assignments are formally made by the chief justice, on request of the chief judge of the lower court and with the consent of the retired justice. The status of a retired justice is analogous to that of a circuit or district court judge who has taken senior status, and eligibility of a Supreme Court justice to assume retired status (rather than simply resign from the bench) is governed by the same age and service criteria.\nIn recent times, justices tend to strategically plan their decisions to leave the bench with personal, institutional, ideological, partisan, and political factors playing a role. The fear of mental decline and death often motivates justices to step down. The desire to maximize the court's strength and legitimacy through one retirement at a time, when the court is in recess and during non-presidential election years suggests a concern for institutional health. Finally, especially in recent decades, many justices have timed their departure to coincide with a philosophically compatible president holding office, to ensure that a like-minded successor would be appointed.\nSalary.\nAs of 2024, associate justices receive a yearly salary of $298,500 and the chief justice is paid $312,200 per year. Once a justice meets age and service requirements, the justice may retire with a pension based on the same formula used for federal employees. As with other federal courts judges, their pension can never be less than their salary at the time of retirement according to the Compensation Clause of Article III of the Constitution.\nSeniority and seating.\nFor the most part, the day-to-day activities of the justices are governed by rules of protocol based upon the seniority of justices. The chief justice always ranks first in the order of precedence\u2014regardless of the length of their service. The associate justices are then ranked by the length of their service. The chief justice sits in the center on the bench, or at the head of the table during conferences. The other justices are seated in order of seniority. The senior-most associate justice sits immediately to the chief justice's right; the second most senior sits immediately to their left. The seats alternate right to left in order of seniority, with the most junior justice occupying the last seat. Therefore, since the October 2022 term, the court sits in order from left to right, from the perspective of those facing the court: Barrett, Gorsuch, Sotomayor, Thomas (most senior associate justice), Roberts (chief justice), Alito, Kagan, Kavanaugh, and Jackson. Likewise, when the members of the court gather for official group photographs, justices are arranged in order of seniority, with the five most senior members seated in the front row in the same order as they would sit during Court sessions (currently, from left to right, Sotomayor, Thomas, Roberts, Alito, and Kagan), and the four most junior justices standing behind them, again in the same order as they would sit during Court sessions (Barrett, Gorsuch, Kavanaugh, and Jackson).\nIn the justices' private conferences, the practice is for them to speak and vote in order of seniority, beginning with the chief justice and ending with the most junior associate justice. By custom, the most junior associate justice in these conferences is charged with any menial tasks the justices may require as they convene alone, such as answering the door of their conference room, serving beverages and transmitting orders of the court to the clerk.\nFacilities.\nThe Supreme Court first met on February 1, 1790, at the Merchants' Exchange Building in New York City. When Philadelphia became the capital, the court met briefly in Independence Hall before settling in Old City Hall from 1791 until 1800. After the government moved to Washington, D.C., the court occupied various spaces in the Capitol building until 1935, when it moved into its own purpose-built home. The four-story building was designed by Cass Gilbert in a classical style sympathetic to the surrounding buildings of the Capitol and Library of Congress, and is clad in marble. The building includes the courtroom, justices' chambers, an extensive law library, various meeting spaces, and auxiliary services including a gymnasium. The Supreme Court building is within the ambit of the Architect of the Capitol, but maintains its own Supreme Court Police, separate from the Capitol Police.\nLocated across First Street from the United States Capitol at One First Street NE and Maryland Avenue, the building is open to the public from 9am to 4:30pm weekdays but closed on weekends and holidays. Visitors may not tour the actual courtroom unaccompanied. There is a cafeteria, a gift shop, exhibits, and a half-hour informational film. When the court is not in session, lectures about the courtroom are held hourly from 9:30am to 3:30pm and reservations are not necessary. When the court is in session the public may attend oral arguments, which are held twice each morning (and sometimes afternoons) on Mondays, Tuesdays, and Wednesdays in two-week intervals from October through late April, with breaks during December and February. Visitors are seated on a first-come first-served basis. One estimate is there are about 250 seats available. The number of open seats varies from case to case; for important cases, some visitors arrive the day before and wait through the night. The court releases opinions beginning at 10am on scheduled \"non-argument days\" (also called opinion days) These sessions, which typically last 15 to 30-minute, are also open to the public. From mid-May until the end of June, at least one opinion day is scheduled each week. Supreme Court Police are available to answer questions.\nJurisdiction.\nCongress is authorized by Article III of the federal Constitution to regulate the Supreme Court's appellate jurisdiction.\nOriginal jurisdiction.\nThe Supreme Court has original and exclusive jurisdiction over cases between two or more states but may decline to hear such cases. It also possesses original but not exclusive jurisdiction to hear \"all actions or proceedings to which ambassadors, other public ministers, consuls, or vice consuls of foreign states are parties; all controversies between the United States and a State; and all actions or proceedings by a State against the citizens of another State or against aliens.\"\nIn 1906, the court asserted its original jurisdiction to prosecute individuals for contempt of court in \"United States v. Shipp\". The resulting proceeding remains the only contempt proceeding and only criminal trial in the court's history. The contempt proceeding arose from the lynching of Ed Johnson in Chattanooga, Tennessee the evening after Justice John Marshall Harlan granted Johnson a stay of execution to allow his lawyers to file an appeal. Johnson was removed from his jail cell by a lynch mob, aided by the local sheriff who left the prison virtually unguarded, and hanged from a bridge, after which a deputy sheriff pinned a note on Johnson's body reading: \"To Justice Harlan. Come get your nigger now.\" The local sheriff, John Shipp, cited the Supreme Court's intervention as the rationale for the lynching. The court appointed its deputy clerk as special master to preside over the trial in Chattanooga with closing arguments made in Washington before the Supreme Court justices, who found nine individuals guilty of contempt, sentencing three to 90 days in jail and the rest to 60 days in jail.\nIn all other cases, the court has only appellate jurisdiction, including the ability to issue writs of mandamus and writs of prohibition to lower courts. It considers cases based on its original jurisdiction very rarely; almost all cases are brought to the Supreme Court on appeal. In practice, the only original jurisdiction cases heard by the court are disputes between two or more states.\nAppellate jurisdiction.\nThe court's appellate jurisdiction consists of appeals from federal courts of appeal (through \"certiorari\", certiorari before judgment, and certified questions), the United States Court of Appeals for the Armed Forces (through certiorari), the Supreme Court of Puerto Rico (through \"certiorari\"), the Supreme Court of the Virgin Islands (through \"certiorari\"), the District of Columbia Court of Appeals (through \"certiorari\"), and \"final judgments or decrees rendered by the highest court of a State in which a decision could be had\" (through \"certiorari\"). In the last case, an appeal may be made to the Supreme Court from a lower state court if the state's highest court declined to hear an appeal or lacks jurisdiction to hear an appeal. For example, a decision rendered by one of the Florida District Courts of Appeal can be appealed to the U.S. Supreme Court if (a) the Supreme Court of Florida declined to grant \"certiorari\", e.g. \"Florida Star v. B. J. F.\", or (b) the district court of appeal issued a per curiam decision simply affirming the lower court's decision without discussing the merits of the case, since the Supreme Court of Florida lacks jurisdiction to hear appeals of such decisions. The power of the Supreme Court to consider appeals from state courts, rather than just federal courts, was created by the Judiciary Act of 1789 and upheld early in the court's history, by its rulings in \"Martin v. Hunter's Lessee\" (1816) and \"Cohens v. Virginia\" (1821). The Supreme Court is the only federal court that has jurisdiction over direct appeals from state court decisions, although there are several devices that permit so-called \"collateral review\" of state cases. This \"collateral review\" often only applies to individuals on death row and not through the regular judicial system.\nSince Article Three of the United States Constitution stipulates that federal courts may only entertain \"cases\" or \"controversies\", the Supreme Court cannot decide cases that are moot and it does not render advisory opinions, as the supreme courts of some states may do. For example, in \"DeFunis v. Odegaard\" (1974), the court dismissed a lawsuit challenging the constitutionality of a law school affirmative action policy because the plaintiff student had graduated since he began the lawsuit, and a decision from the court on his claim would not be able to redress any injury he had suffered. However, the court recognizes some circumstances where it is appropriate to hear a case that is seemingly moot. If an issue is \"capable of repetition yet evading review\", the court would address it even though the party before the court would not themselves be made whole by a favorable result. In \"Roe v. Wade\" (1973), and other abortion cases, the court addresses the merits of claims pressed by pregnant women seeking abortions even if they are no longer pregnant because it takes longer than the typical human gestation period to appeal a case through the lower courts to the Supreme Court. Another mootness exception is voluntary cessation of unlawful conduct, in which the court considers the probability of recurrence and plaintiff's need for relief.\nJustices as circuit justices.\nThe United States is divided into thirteen circuit courts of appeals, each of which is assigned a \"circuit justice\" from the Supreme Court. Although this concept has been in continuous existence throughout the history of the republic, its meaning has changed through time. Under the Judiciary Act of 1789, each justice was required to \"ride circuit\", or to travel within the assigned circuit and consider cases alongside local judges. This practice encountered opposition from many justices, who cited the difficulty of travel. Moreover, there was a potential for a conflict of interest on the court if a justice had previously decided the same case while riding circuit. Circuit riding ended in 1891, when the Circuit Court of Appeals Act was passed, and circuit riding was officially abolished by Congress in 1911.\nThe circuit justice for each circuit is responsible for dealing with certain types of applications that, by law and the rules of the court, may be addressed by a single justice. Ordinarily, a justice will resolve such an application by simply endorsing it \"granted\" or \"denied\" or entering a standard form of order; however, the justice may elect to write an opinion, referred to as an in-chambers opinion. Congress has specifically authorized one justice to issue a stay pending certiorari in \u00a0https://. Each justice also decides routine procedural requests, such as for extensions of time.\nBefore 1990, the rules of the Supreme Court also stated that \"a writ of injunction may be granted by any Justice in a case where it might be granted by the Court.\" However, this part of the rule (and all other specific mention of injunctions) was removed in the Supreme Court's rules revision of December 1989. Nevertheless, requests for injunctions under the All Writs Act are sometimes directed to the circuit justice. In the past, circuit justices also sometimes granted motions for bail in criminal cases, writs of \"habeas corpus\", and applications for writs of error granting permission to appeal.\nA circuit justice may sit as a judge on the Court of Appeals of that circuit, but over the past hundred years, this has rarely occurred. A circuit justice sitting with the Court of Appeals has seniority over the chief judge of the circuit. The chief justice has traditionally been assigned to the District of Columbia Circuit, the Fourth Circuit (which includes Maryland and Virginia, the states surrounding the District of Columbia), and since it was established, the Federal Circuit. Each associate justice is assigned to one or two judicial circuits.\nAs of September 28, 2022, the allotment of the justices among the circuits is as follows:\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nFive of the current justices are assigned to circuits on which they previously sat as circuit judges: Chief Justice Roberts (D.C. Circuit), Justice Sotomayor (Second Circuit), Justice Alito (Third Circuit), Justice Barrett (Seventh Circuit), and Justice Gorsuch (Tenth Circuit).\nProcess.\nCase selection.\nNearly all cases come before the court by way of petitions for writs of \"certiorari\", commonly referred to as \"cert\", upon which the court grants a writ of certiorari. The court may review via this process any civil or criminal case in the federal courts of appeals. It may also review by certiorari a final judgment of the highest court of a state if the judgment involves a question of federal statutory or constitutional law. A case may alternatively come before the court as a direct appeal from a three-judge federal district court. The party that petitions the court for review is the \"petitioner\" and the non-mover is the \"respondent\".\nCase names before the court are styled \"petitioner\" v. \"respondent\", regardless of which party initiated the lawsuit in the trial court. For example, criminal prosecutions are brought in the name of the state and against an individual, as in \"State of Arizona v. Ernesto Miranda\". If the defendant is convicted, and his conviction then is affirmed on appeal in the state supreme court, when he petitions for cert the name of the case becomes \"Miranda v. Arizona\".\nThe court also hears questions submitted to it by appeals courts themselves via a process known as certification.\nThe Supreme Court relies on the record assembled by lower courts for the facts of a case and deals solely with the question of how the law applies to the facts presented. There are however situations where the court has original jurisdiction, such as when two states have a dispute against each other, or when there is a dispute between the United States and a state. In such instances, a case is filed with the Supreme Court directly. Examples of such cases include \"United States v. Texas\", a case to determine whether a parcel of land belonged to the United States or to Texas, and \"Virginia v. Tennessee\", a case turning on whether an incorrectly drawn boundary between two states can be changed by a state court, and whether the setting of the correct boundary requires Congressional approval. Although it has not happened since 1794 in the case of \"Georgia v. Brailsford\", parties in an action at law in which the Supreme Court has original jurisdiction may request that a jury determine issues of fact. \"Georgia v. Brailsford\" remains the only case in which the court has empaneled a jury, in this case a special jury. Two other original jurisdiction cases involve colonial era borders and rights under navigable waters in \"New Jersey v. Delaware\", and water rights between riparian states upstream of navigable waters in \"Kansas v. Colorado\".\nA cert petition is voted on at a session of the court called conference. A conference is a private meeting of the nine justices by themselves; the public and the justices' clerks are excluded. The rule of four permits four of the nine justices to grant a writ of \"certiorari\". If it is granted, the case proceeds to the briefing stage; otherwise, the case ends. Except in death penalty cases and other cases in which the court orders briefing from the respondent, the respondent may, but is not required to, file a response to the cert petition. The court grants a petition for cert only for \"compelling reasons\", spelled out in the court's Rule 10. Such reasons include:\nWhen a conflict of interpretations arises from differing interpretations of the same law or constitutional provision issued by different federal circuit courts of appeals, lawyers call this situation a \"circuit split\"; if the court votes to deny a cert petition, as it does in the vast majority of such petitions that come before it, it does so typically without comment. A denial of a cert petition is not a judgment on the merits of a case, and the decision of the lower court stands as the case's final ruling. To manage the high volume of cert petitions received by the court each year (of the more than 7,000 petitions the court receives each year, it will usually request briefing and hear oral argument in 100 or fewer), the court employs an internal case management tool known as the \"cert pool\"; currently, all justices except for Justices Alito and Gorsuch participate in the cert pool.\nWritten evidence.\nThe Court also relies on and cites amicus briefs, law review articles, and other written works for their decisions. While law review article use has increased slightly with one article cited per decision on average, the use of amicus briefs has increased significantly. The use of amicus briefs has received criticism, including the ability of authors to discuss topics outside their expertise (unlike in lower courts), with documented examples of falsehoods in written opinions, often supplied to the justices by amicus briefs from groups advocating a particular outcome. The lack of funding transparency and the lack of a requirement to submit them earlier in the process also make it more difficult to fact-check and understand the credibility of amicus briefs.\nOral argument.\nWhen the court grants a cert petition, the case is set for oral argument. Both parties will file briefs on the merits of the case, as distinct from the reasons they may have argued for granting or denying the cert petition. With the consent of the parties or approval of the court, \"amici curiae\", or \"friends of the court\", may also file briefs. The court holds two-week oral argument sessions each month from October through April. Each side has thirty minutes to present its argument (the court may choose to give more time, although this is rare), and during that time, the justices may interrupt the advocate and ask questions. \nIn 2019, the court adopted a rule generally allowing advocates to speak uninterrupted for the first two minutes of their argument. The petitioner gives the first presentation, and may reserve some time to rebut the respondent's arguments after the respondent has concluded. \"Amici curiae\" may also present oral argument on behalf of one party if that party agrees. The court advises counsel to assume that the justices are familiar with and have read the briefs filed in a case.\nDecision.\nAt the conclusion of oral argument, the case is submitted for decision. Cases are decided by majority vote of the justices. After the oral argument is concluded, usually in the same week as the case was submitted, the justices retire to another conference at which the preliminary votes are tallied and the court sees which side has prevailed. One of the justices in the majority is then assigned to write the court's opinion, also known as the \"majority opinion\", an assignment made by the most senior justice in the majority, with the chief justice always being considered the most senior. Drafts of the court's opinion circulate among the justices until the court is prepared to announce the judgment in a particular case.\nJustices are free to change their votes on a case up until the decision is finalized and published. In any given case, a justice is free to choose whether or not to author an opinion or else simply join the majority or another justice's opinion. There are several primary types of opinions:\nIt is the court's practice to issue decisions in all cases argued in a particular term by the end of that term. Within that term, the court is under no obligation to release a decision within any set time after oral argument. Since recording devices are banned inside the courtroom of the Supreme Court Building, the delivery of the decision to the media has historically been done via paper copies in what was known as the \"Running of the Interns\". However, this practice has become pass\u00e9 as the Court now posts electronic copies of the opinions on its website as they are being announced.\nIt is possible that through recusals or vacancies the court divides evenly on a case. If that occurs, then the decision of the court below is affirmed, but does not establish binding precedent. In effect, it results in a return to the \"status quo ante\". For a case to be heard, there must be a quorum of at least six justices. If a quorum is not available to hear a case and a majority of qualified justices believes that the case cannot be heard and determined in the next term, then the judgment of the court below is affirmed as if the court had been evenly divided. For cases brought to the Supreme Court by direct appeal from a United States District Court, the chief justice may order the case remanded to the appropriate U.S. Court of Appeals for a final decision there. This has only occurred once in U.S. history, in the case of \"United States v. Alcoa\" (1945).\nPublished opinions.\nThe court's opinions are published in three stages. First, a slip opinion is made available on the court's web site and through other outlets. Next, several opinions and lists of the court's orders are bound together in paperback form, called a preliminary print of \"United States Reports\", the official series of books in which the final version of the court's opinions appears. \nAbout a year after the preliminary prints are issued, a final bound volume of \"U.S. Reports\" is issued by the Reporter of Decisions. The individual volumes of \"U.S. Reports\" are numbered so that users may cite this set of reports, or a competing version published by another commercial legal publisher but containing parallel citations, to allow those who read their pleadings and other briefs to find the cases quickly and easily. \nAs of \u00a02019[ [update]], there are:\nAs of \u00a02012[ [update]], the \"U.S. Reports\" have published a total of 30,161 Supreme Court opinions, covering the decisions handed down from February 1790 to March 2012. This figure does not reflect the number of cases the court has taken up, as several cases can be addressed by a single opinion. See, for example, \"Parents v. Seattle\", where \"Meredith v. Jefferson County Board of Education\" was also decided in the same opinion. \nBy a similar logic, \"Miranda v. Arizona\" decided \"Miranda\" and three other cases: \"Vignera v. New York\", \"Westover v. United States\", and \"California v. Stewart\". A more unusual example is The Telephone Cases, which are a single set of interlinked opinions that take up the entire 126th volume of the \"U.S. Reports\".\nOpinions are also collected and published in two unofficial, parallel reporters: \"Supreme Court Reporter\", published by West (now a part of Thomson Reuters), and \"United States Supreme Court Reports, Lawyers' Edition\" (simply known as \"Lawyers' Edition\"), published by LexisNexis. In court documents, legal periodicals and other legal media, case citations generally contain cites from each of the three reporters; for example, citation to \"Citizens United v. Federal Election Commission\" is presented as \"Citizens United v. Federal Election Com'n\", 585 U.S. 50, 130 S. Ct. 876, 175 L. Ed. 2d 753 (2010), with \"S. Ct.\" representing the \"Supreme Court Reporter\", and \"L. Ed.\" representing the \"Lawyers' Edition\".\nCitations to published opinions.\nLawyers use an abbreviated format to cite cases, in the form \"vol U.S. page, pin (year)\", where vol is the volume number, page is the page number on which the opinion begins, and year is the year in which the case was decided. Optionally, pin is used to \"pinpoint\" to a specific page number within the opinion. For instance, the citation for \"Roe v. Wade\" is 410 U.S. 113 (1973), which means the case was decided in 1973 and appears on page 113 of volume 410 of \"U.S. Reports\". For opinions or orders that have not yet been published in the preliminary print, the volume and page numbers may be replaced with \"___\"\nSupreme Court bar.\nIn order to plead before the court, an attorney must first be admitted to the court's bar. Approximately 4,000 lawyers join the bar each year. The bar contains an estimated 230,000 members. In reality, pleading is limited to several hundred attorneys. The rest join for a one-time fee of $200, with the court collecting about $750,000 annually. Attorneys can be admitted as either individuals or as groups. The group admission is held before the current justices of the Supreme Court, wherein the chief justice approves a motion to admit the new attorneys. Lawyers commonly apply for the cosmetic value of a certificate to display in their office or on their resume. They also receive access to better seating if they wish to attend an oral argument. Members of the Supreme Court Bar are also granted access to the collections of the Supreme Court Library.\nTerm.\nA term of the Supreme Court commences on the first Monday of each October, and continues until June or early July of the following year. Each term consists of alternating periods of around two weeks known as \"sittings\" and \"recesses\"; justices hear cases and deliver rulings during sittings, and discuss cases and write opinions during recesses.\nInstitutional powers.\nThe federal court system and the judicial authority to interpret the Constitution received little attention in the debates over the drafting and ratification of the Constitution. The power of judicial review, in fact, is nowhere mentioned in it. Over the ensuing years, the question of whether the power of judicial review was even intended by the drafters of the Constitution was quickly frustrated by the lack of evidence bearing on the question either way. \nNevertheless, the power of judiciary to overturn laws and executive actions it determines are unlawful or unconstitutional is a well-established precedent. Many of the Founding Fathers accepted the notion of judicial review; in Federalist No. 78, Alexander Hamilton wrote: \"A Constitution is, in fact, and must be regarded by the judges, as a fundamental law. It therefore belongs to them to ascertain its meaning, and the meaning of any particular act proceeding from the legislative body. If there should happen to be an irreconcilable variance between the two, that which has the superior obligation and validity ought, of course, to be preferred; or, in other words, the Constitution ought to be preferred to the statute.\"\nThe Supreme Court established its own power to declare laws unconstitutional in \"Marbury v. Madison\" (1803), consummating the American system of checks and balances. In explaining the power of judicial review, Chief Justice John Marshall stated that the authority to interpret the law was the particular province of the courts, part of the \"duty of the judicial department to say what the law is.\" His contention was not that the court had privileged insight into constitutional requirements, but that it was the constitutional duty of the judiciary, as well as the other branches of government, to read and obey the dictates of the Constitution. This decision was criticized by then-President Thomas Jefferson who said, \"the Constitution, on this hypothesis, is a mere thing of wax in the hands of the judiciary, which they may twist and shape into any form they please.\"\nSince the founding of the republic, there has been a tension between the practice of judicial review and the democratic ideals of egalitarianism, self-government, self-determination and freedom of conscience. At one pole are those who view the federal judiciary and especially the Supreme Court as being \"the most separated and least checked of all branches of government.\" Federal judges and justices on the Supreme Court are not required to stand for election by virtue of their tenure \"during good behavior\", and their pay may \"not be diminished\" while they hold their position (). Although subject to the process of impeachment, only one justice has ever been impeached and no Supreme Court justice has been removed from office. At the other pole are those who view the judiciary as the least dangerous branch, with little ability to resist the exhortations of the other branches of government.\nConstraints.\nThe Supreme Court cannot directly enforce its rulings; instead, it relies on respect for the Constitution and for the law for adherence to its judgments. Popular history claims an instance of judicial nonacquiesence in 1832, when the state of Georgia ignored the Supreme Court's decision in \"Worcester v. Georgia\". President Andrew Jackson, who sided with the Georgia courts, is supposed to have remarked, \"John Marshall has made his decision; now let him enforce it!\", but the tale is apocryphal. \nSome state governments in the South also resisted the desegregation of public schools after the 1954 judgment \"Brown v. Board of Education\". More recently, many feared that President Nixon would refuse to comply with the court's order in \"United States v. Nixon\" (1974) to surrender the Watergate tapes. Nixon ultimately complied with the Supreme Court's ruling.\nSupreme Court decisions can be purposefully overturned by constitutional amendment, something that has happened on six occasions:\nRecognizing the difficulty of constitutional amendment, and to avoid the antidemocratic problems inherent to the publication of decisions holding legislation or executive actions unconstitutional, the Court has resorted to self-imposed canons of construction and doctrinal rules, such as the doctrine of constitutional avoidance, to minimize occurrences where the political branches or popular movements should need to reverse the Court via constitutional amendment.\nWhen the court rules on matters involving the interpretation of federal statutes rather than of the Constitution, simple legislative action can reverse the decisions (for example, in 2009 Congress passed the Lilly Ledbetter Fair Pay Act of 2009, superseding the limitations given in \"Ledbetter v. Goodyear Tire &amp; Rubber Co.\" in 2007). Also, the Supreme Court is not immune from political and institutional consideration: lower federal courts and state courts sometimes resist doctrinal innovations, as do law enforcement officials.\nIn addition, the other two branches can restrain the court through other mechanisms. Congress can increase the number of justices, giving the president power to influence future decisions by appointments (as in Roosevelt's court-packing plan discussed above). Congress can pass legislation that restricts the jurisdiction of the Supreme Court and other federal courts over certain topics and cases: this is suggested by language in of Article Three, where the appellate jurisdiction is granted \"with such Exceptions, and under such Regulations as the Congress shall make.\" The court sanctioned such congressional action in the Reconstruction Era case \"Ex parte McCardle\" (1869), although it rejected Congress' power to dictate how particular cases must be decided in \"United States v. Klein\" (1871).\nOn the other hand, through its power of judicial review, the Supreme Court has defined the scope and nature of the powers and separation between the legislative and executive branches of the federal government; for example, in \"United States v. Curtiss-Wright Export Corp.\" (1936), \"Dames &amp; Moore v. Regan\" (1981), and notably in \"Goldwater v. Carter\" (1979), which effectively gave the presidency the power to terminate ratified treaties without the consent of Congress. The court's decisions can also impose limitations on the scope of Executive authority, as in \"Humphrey's Executor v. United States\" (1935), the \"Steel Seizure Case\" (1952), and \"United States v. Nixon\" (1974).\nLaw clerks.\nEach Supreme Court justice hires several law clerks to review petitions for writ of \"certiorari\", research them, prepare bench memorandums, and draft opinions. Associate justices are allowed four clerks. The chief justice is allowed five clerks, but Chief Justice Rehnquist hired only three per year, and Chief Justice Roberts usually hires only four. Generally, law clerks serve a term of one to two years.\nThe first law clerk was hired by Associate Justice Horace Gray in 1882. Oliver Wendell Holmes Jr. and Louis Brandeis were the first Supreme Court justices to use recent law school graduates as clerks, rather than hiring \"a stenographer-secretary\". Most law clerks are recent law school graduates.\nThe first female clerk was Lucile Lomen, hired in 1944 by Justice William O. Douglas. The first African-American, William T. Coleman Jr., was hired in 1948 by Justice Felix Frankfurter. A disproportionately large number of law clerks have obtained law degrees from elite law schools, especially Harvard, Yale, the University of Chicago, Columbia, and Stanford. From 1882 to 1940, 62% of law clerks were graduates of Harvard Law School. Those chosen to be Supreme Court law clerks usually have graduated in the top of their law school class and were often an editor of the law review or a member of the moot court board. By the mid-1970s, clerking previously for a judge in a federal court of appeals had also become a prerequisite to clerking for a Supreme Court justice.\nTen Supreme Court justices previously clerked for other justices: Byron White for Frederick M. Vinson, John Paul Stevens for Wiley Rutledge, William Rehnquist for Robert H. Jackson, Stephen Breyer for Arthur Goldberg, John Roberts for William Rehnquist, Elena Kagan for Thurgood Marshall, Neil Gorsuch for both Byron White and Anthony Kennedy, Brett Kavanaugh also for Kennedy, Amy Coney Barrett for Antonin Scalia, and Ketanji Brown Jackson for Stephen Breyer. Justices Gorsuch and Kavanaugh served under Kennedy during the same term. \nGorsuch is the first justice to clerk for and subsequently serve alongside the same justice, serving alongside Kennedy from April 2017 through Kennedy's retirement in 2018. With the confirmation of Justice Kavanaugh, for the first time a majority of the Supreme Court was composed of former Supreme Court law clerks (Roberts, Breyer, Kagan, Gorsuch and Kavanaugh, now joined by Barrett and Jackson, who replaced Breyer).\nSeveral current Supreme Court justices have also clerked in the federal courts of appeals: John Roberts for Judge Henry Friendly of the United States Court of Appeals for the Second Circuit, Justice Samuel Alito for Judge Leonard I. Garth of the United States Court of Appeals for the Third Circuit, Elena Kagan for Judge Abner J. Mikva of the United States Court of Appeals for the District of Columbia Circuit, Neil Gorsuch for Judge David B. Sentelle of the United States Court of Appeals for the District of Columbia, Brett Kavanaugh for Judge Walter Stapleton of the United States Court of Appeals for the Third Circuit and Judge Alex Kozinski of the United States Court of Appeals for the Ninth Circuit, and Amy Coney Barrett for Judge Laurence Silberman of the U.S. Court of Appeals for the D.C. Circuit.\nPoliticization of the court.\nClerks hired by each of the justices of the Supreme Court are often given considerable leeway in the opinions they draft. \"Supreme Court clerkship appeared to be a nonpartisan institution from the 1940s into the 1980s\", according to a study published in 2009 by the law review of Vanderbilt University Law School. \"As law has moved closer to mere politics, political affiliations have naturally and predictably become proxies for the different political agendas that have been pressed in and through the courts\", former federal court of appeals judge J. Michael Luttig said. \nDavid J. Garrow, professor of history at the University of Cambridge, stated that the court had thus begun to mirror the political branches of government. \"We are getting a composition of the clerk workforce that is getting to be like the House of Representatives\", Professor Garrow said. \"Each side is putting forward only ideological purists.\" According to the \"Vanderbilt Law Review\" study, this politicized hiring trend reinforces the impression that the Supreme Court is \"a superlegislature responding to ideological arguments rather than a legal institution responding to concerns grounded in the rule of law.\"\nCriticism and controversies.\nThe following are some of the criticisms and controversies about the Court that are not discussed in previous sections.\nUnlike in most high courts, the United States Supreme Court has lifetime tenure, an unusual amount of power over elected branches of government, and a difficult constitution to amend. To these, among other factors, have been attributed by some critics the Court's diminished stature abroad and lower approval ratings at home, which have dropped from the mid-60s in the late 1980s to around 40% in the early 2020s. Additional factors cited by critics include the polarization of national politics, ethics scandals, and specific controversial partisan rulings, including the relaxation of campaign finance rules, increased gerrymandering, weakened voting laws, \"Dobbs v. Jackson\" and \"Bush v. Gore\". The continued consolidation of power by the court and, as a result of its rulings, the Republican Party, has sparked debate over when democratic backsliding becomes entrenched single-party rule.\nApproval ratings.\nPublic trust in the court peaked in the late 1980s. Since the 2022 \"Dobbs\" ruling that overturned \"Roe v. Wade\" and devolved the regulation of abortion, Democrats and independents have increasingly lost trust in the court, seen the court as political, and expressed support for reforming the institution. Historically, the court had relatively more trust than other government institutions.\nAfter recording recent high approval ratings in the late 1980s around 66% approval, the court's ratings have declined to an average of around 40% between mid-2021 and February 2024.\nComposition and selection.\nThe electoral college (which elects the President who nominates the justices) and the U.S. Senate which confirms the justices, have selection biases that favor rural states that tend to vote Republican, resulting in a conservative Supreme Court. Ziblatt and Levitsky estimate that 3 or 4 of the seats held by conservative justices on the court would be held by justices appointed by a Democratic president if the Presidency and Senate were selected directly by the popular vote. The three Trump appointees to the court were all nominated by a president who finished second in the popular vote and confirmed by Senators representing a minority of Americans. In addition, Clarence Thomas' confirmation in 1991 and Merrick Garland's blocked confirmation in 2016 were both decided by senators representing a minority of Americans. Greg Price also critiqued the Court as minority rule.\nMoreover, the Federalist Society acted as a filter for judicial nominations during the Trump administration, ensuring the latest conservative justices lean even further to the right. 86% of judges Trump appointed to circuit courts and the Supreme Court were Federalist Society members. David Litt critiques it as \"an attempt to impose rigid ideological dogma on a profession once known for intellectual freedom.\" Kate Aronoff criticizes the donations from special interests like fossil fuel companies and other dark money groups to the Federalist Society and related organizations seeking to influence lawyers and Supreme Court Justices.\nThe 2016 stonewalling of Merrick Garland's confirmation and subsequent filling with Neil Gorsuch has been critiqued as a 'stolen seat' citing precedent from the 20th century of confirmations during election years, while proponents cited three blocked nominations between 1844 and 1866. In recent years, Democrats have accused Republican leaders such as Mitch McConnell of hypocrisy, as they were instrumental in blocking the nomination of Garland, but then quickly confirmed Amy Coney Barrett's nomination even though both vacancies occurred close to an election.\nEthics.\nSCOTUS justices have come under greater scrutiny since 2022, following public disclosures that began with the founder of Faith and Action admissions regarding the organization's long-term influence-peddling scheme, dubbed \"Operation Higher Court\", designed for wealthy donors among the religious right to gain access to the justices through events held by The Supreme Court Historical Society.\nEthical controversies have grown during the 2020s, with reports of justices (and their close family members) accepting expensive gifts, travel, business deals, and speaking fees without oversight or recusals from cases that present conflicts of interest. Spousal income and connections to cases has been redacted from the Justices' ethical disclosure forms while justices, such as Samuel Alito and Clarence Thomas, failed to disclose many large financial gifts including free vacations valued at as much as $500,000. \nIn 2024, Justices Alito and Thomas refused calls to recuse themselves from January 6 cases where their spouses have taken public stances or been involved in efforts to overturn the election. In 2017, Neil Gorsuch sold a property he co-owned for $1.8 million to the CEO of a prominent law firm, who was not listed on his ethics form when reporting a profit of between $250,000 and $500,000.\nThe criticism intensified after the 2024 \"Trump v. United States\" decision granted broad immunity to presidents, with Representative Alexandria Ocasio-Cortez saying she would introduce impeachment articles when Congress is back in session. On July 10, 2024, she filed Articles of Impeachment against Thomas and Alito, citing their \"widely documented financial and personal entanglements\". As of late July 2024, nearly 1.4 million people had signed a moveon.org petition asking Congress to remove Justice Thomas.\nPresident Biden proposed term limits for justices, an enforceable ethics code, and elimination of \"immunity for crimes a former president committed while in office\".\nYale professor of constitutional law Akhil Reed Amar wrote an op-ed for \"The Atlantic\" titled \"Something Has Gone Deeply Wrong at the Supreme Court\".\nOther criticisms of the Court include weakening corruption laws impacting branches beyond the judiciary and citing falsehoods in written opinions, often supplied to the justices by amicus briefs from groups advocating a particular outcome. Allison Orr Larsen, Associate Dean at William &amp; Mary Law School, wrote in \"Politico\" that the court should address this by requiring disclosure of all funders of amicus briefs and the studies they cite, only admit briefs that stay within the expertise of the authors (as is required in lower courts), and require the briefs to be submitted much earlier in the process so the history and facts have time to be challenged and uncovered.\nCode of Conduct.\nOn November 13, 2023, the court issued its first-ever Code of Conduct for Justices of the Supreme Court of the United States to set \"ethics rules and principles that guide the conduct of the Members of the Court.\" The Code has been received by some as a significant first step but does not address the ethics concerns of many notable critics who found the Code was a significantly weakened version of the rules for other federal judges, let alone the legislature and the executive branch, while also lacking an enforcement mechanism. The Code's commentary denied past wrongdoing by saying that the Justices have largely abided by these principles and are simply publishing them now. This has prompted some criticism that the court hopes to legitimize past and future scandals through this Code.\nThe ethics rules guiding the justices are set and enforced by the justices themselves, meaning the members of the court have no external checks on their behavior other than the impeachment of a justice by Congress.\nChief Justice Roberts refused to testify before the Senate Judiciary Committee in April 2023, reasserting his desire for the Supreme Court to continue to monitor itself despite mounting ethics scandals. Lower courts, by contrast, discipline according to the 1973 Code of Conduct for U.S. judges which is enforced by the Judicial Conduct and Disability Act of 1980.\n establishes that the justices hold their office during good behavior. Thus far only one justice (Associate Justice Samuel Chase in 1804) has ever been impeached, and none has ever been removed from office.\nThe lack of external enforcement of ethics or other conduct violations makes the Supreme Court an outlier in modern organizational best-practices. 2024 reform legislation has been blocked by congressional Republicans.\nDemocratic backsliding.\nThomas Keck argues that because the Court has historically not served as a strong bulwark for democracy, the Roberts Court had the opportunity to go down in history as a defender of democracy. However, he believes that if the court shields Trump from criminal prosecution (after ensuring his access to the ballot), then the risks that come with an anti-democratic status-quo of the current court will outweigh the dangers that come from court reform (including court packing). Aziz Z. Huq points to the blocking progress of democratizing institutions, increasing the disparity in wealth and power, and empowering an authoritarian white nationalist movement as evidence that the Supreme Court has created a \"permanent minority\" incapable of being defeated democratically.\nIn a July 2022 research paper entitled \"The Supreme Court's Role in the Degradation of U.S. Democracy\", the Campaign Legal Center, founded by Republican Trevor Potter, asserted that the Roberts Court \"has turned on our democracy\" and was on an \"anti-democratic crusade\" that had \"accelerated and become increasingly extreme with the arrival\" of Trump's three appointees. A 2024 op-ed by legal reporters Dahlia Lithwick and Mark Joseph Stern expressed a similar view.\nIndividual rights.\nSome of the most notable historical decisions that were criticized for failing to protect individual rights include the \"Dred Scott\" (1857) decision that said people of African descent could not be U.S. citizens or enjoy constitutionally protected rights and privileges, \"Plessy v. Ferguson\" (1896) that upheld segregation under the doctrine of \"separate but equal,\" the \"Civil Rights Cases\" (1883) and \"Slaughter-House Cases\" (1873) that all but undermined civil rights legislation enacted during the Reconstruction era.\nHowever, others argue that the court is too protective of some individual rights, particularly those of people accused of crimes or in detention. For example, Chief Justice Warren Burger criticized the exclusionary rule, and Justice Scalia criticized \"Boumediene v. Bush\" for being \"too protective\" of the rights of Guantanamo detainees, arguing habeas corpus should be limited to sovereign territory.\nAfter \"Dobbs v. Jackson Women's Health Organization\" (2022) overturned \"Roe v. Wade\" (1973), \"PBS\" reported that the case could start reconsideration of the doctrine of substantive due process, since a concurrence in the case by Justice Clarence Thomas argued for that. Substantive due process has been the main means used by the Supreme Court to incorporate the Bill of Rights against state and local governments. Justice Thomas has referred to it as 'legal fiction,' preferring the Privileges or Immunities Clause for incorporation. \nJudicial activism.\nThe Supreme Court has been criticized for engaging in judicial activism. This criticism is leveled by those who believe the court should not interpret the law in any way besides through the lens of past precedent or Textualism. However, those on both sides of the political aisle often level this accusation at the court. The debate around judicial activism typically involves accusing the other side of activism, whilst denying that your own side engages in it.\nConservatives often cite the decision in \"Roe v. Wade\" (1973) as an example of liberal judicial activism. In its decision, the court legalized abortion on the basis of a \"right to privacy\" that they found inherent in the Due Process Clause of the Fourteenth Amendment. \"Roe v. Wade\" was overturned nearly fifty years later by \"Dobbs v. Jackson\" (2022), ending the recognition of abortion access as a constitutional right and returning the issue of abortion back to the states. David Litt criticized the decision in \"Dobbs\" as activism on the part of the court's conservative majority because the court failed to respect past precedent, eschewing the principle of \"stare decisis\" that usually guides the court's decisions.\nThe decision in \"Brown v. Board of Education\", which banned racial segregation in public schools was also criticized as activist by conservatives Pat Buchanan, Robert Bork and Barry Goldwater. More recently, \"Citizens United v. Federal Election Commission\" was criticized for expanding upon the precedent in \"First National Bank of Boston v. Bellotti\" (1978) that the First Amendment applies to corporations.\nOutdated and an outlier.\n\"Foreign Policy\" writer Colm Quinn says that a criticism leveled at the court, as well as other American institutions, is that after two centuries they are beginning to look their age. He cites four features of the United States Supreme Court that make it different from high courts in other countries, and help explain why polarization is an issue in the United States court:\nAdam Liptak wrote in 2008 that the court has declined in relevance in other constitutional courts. He cites factors like American exceptionalism, the relatively few updates to the constitution or the courts, the rightward shift of the court and the diminished stature of the United States abroad.\nPower.\nMichael Waldman argued that no other country gives its Supreme Court as much power. Warren E. Burger, before becoming Chief Justice, argued that since the Supreme Court has such \"unreviewable power\", it is likely to \"self-indulge itself\", and unlikely to \"engage in dispassionate analysis\". Larry Sabato wrote that the federal courts, and especially the Supreme Court, have excessive power. Suja A. Thomas argues the Supreme Court has taken most of the constitutionally defined power from juries in the United States for itself thanks in part to the influence of legal elites and companies that prefer judges over juries as well as the inability of the jury to defend its power.\nSome members of Congress considered the results from the 2021\u20132022 term a shift of government power into the Supreme Court, and a \"judicial coup\". The 2021\u20132022 term of the court was the first full term following the appointment of three judges by Republican president Donald Trump \u2014 Neil Gorsuch, Brett Kavanaugh, and Amy Coney Barrett \u2014 which created a six-strong conservative majority on the court. Subsequently, at the end of the term, the court issued a number of decisions that favored this conservative majority while significantly changing the landscape with respect to rights. These included \"Dobbs v. Jackson Women's Health Organization\" which overturned \"Roe v. Wade\" and \"Planned Parenthood v. Casey\" in recognizing abortion is not a constitutional right, \"New York State Rifle &amp; Pistol Association, Inc. v. Bruen\" which made public possession of guns a protected right under the Second Amendment, \"Carson v. Makin\" and \"Kennedy v. Bremerton School District\" which both weakened the Establishment Clause separating church and state, and \"West Virginia v. EPA\" which weakened the power of executive branch agencies to interpret their congressional mandate.\nFederalism debate.\nThere has been debate throughout American history about the boundary between federal and state power. While Framers such as James Madison and Alexander Hamilton argued in \"The Federalist Papers\" that their then-proposed Constitution would not infringe on the power of state governments, others argue that expansive federal power is good and consistent with the Framers' wishes. The Tenth Amendment to the United States Constitution explicitly states that \"powers not delegated to the United States by the Constitution, nor prohibited by it to the States, are reserved to the States respectively, or to the people.\"\nThe court has been criticized for giving the federal government too much power to interfere with state authority. One criticism is that it has allowed the federal government to misuse the Commerce Clause by upholding regulations and legislation which have little to do with interstate commerce, but that were enacted under the guise of regulating interstate commerce; and by voiding state legislation for allegedly interfering with interstate commerce. For example, the Commerce Clause was used by the Fifth Circuit Court of Appeals to uphold the Endangered Species Act, thus protecting six endemic species of insect near Austin, Texas, despite the fact that the insects had no commercial value and did not travel across state lines; the Supreme Court let that ruling stand without comment in 2005. Chief Justice John Marshall asserted Congress's power over interstate commerce was \"complete in itself, may be exercised to its utmost extent, and acknowledges no limitations, other than are prescribed in the Constitution.\" Justice Alito said congressional authority under the Commerce Clause is \"quite broad\"; commentator Robert B. Reich suggests debate over the Commerce Clause continues today.\nAdvocates of states' rights, such as constitutional scholar Kevin Gutzman, have also criticized the court, saying it has misused the Fourteenth Amendment to undermine state authority. Justice Brandeis, in arguing for allowing the states to operate without federal interference, suggested that states should be laboratories of democracy. One critic wrote \"the great majority of Supreme Court rulings of unconstitutionality involve state, not federal, law.\" Others see the Fourteenth Amendment as a positive force that extends \"protection of those rights and guarantees to the state level.\"\nMore recently, in \"Gamble v. United States\", the Court examined the doctrine of \"separate sovereigns\", whereby a criminal defendant can be prosecuted in state court as well as federal court on separate charges for the same offense.\nRuling on political questions.\nSome Court decisions have been criticized for injecting the court into the political arena, and deciding questions that are the purview of the elected branches of government. The \"Bush v. Gore\" decision, in which the Supreme Court intervened in the 2000 presidential election, awarding George W. Bush the presidency over Al Gore, received scrutiny as political based on the controversial justifications used by the five conservative justices to elevate a fellow conservative to the presidency. The ruling was also controversial in applying logic only for that race, as opposed to drawing on or creating consistent precedent.\nSecretive proceedings.\nThe court has been criticized for keeping its deliberations hidden from public view. For example, the increasing use of a 'shadow docket' facilitates the court making decisions in secret without knowing how each Justice came to their decision. In 2024, after comparing the analysis of shadow-docket decisions to Kremlinology, Matt Ford called this trend of secrecy \"increasingly troubling\", arguing the court's power comes entirely from persuasion and explanation.\nA 2007 review of Jeffrey Toobin's book compared the Court to a cartel where its inner-workings are mostly unknown, arguing this lack of transparency reduces scrutiny which hurts ordinary Americans who know little about the nine extremely consequential Justices. A 2010 poll found that 61% of American voters agreed that televising Court hearings would \"be good for democracy\", and 50% of voters stated they would watch Court proceedings if they were televised.\nToo few cases.\nIan Millhiser of Vox speculates that the decades-long decline in cases heard could be due to the increasing political makeup of judges, that he says might be more interested in settling political disputes than legal ones.\nToo slow.\nBritish constitutional scholar Adam Tomkins sees flaws in the American system of having courts (and specifically the Supreme Court) act as checks on the Executive and Legislative branches; he argues that because the courts must wait, sometimes for years, for cases to navigate their way through the system, their ability to restrain other branches is severely weakened. In contrast, various other countries have a dedicated constitutional court that has original jurisdiction on constitutional claims brought by persons or political institutions; for example, the Federal Constitutional Court of Germany, which can declare a law unconstitutional when challenged.\nCritics have accused the Court of \"slow-walking\" important cases relating to former president Donald Trump in order to benefit his election chances in the face of the 2024 United States presidential election. The Court is considering a presidential immunity claim as part of the Federal prosecution of Donald Trump (election obstruction case). Critics argue that the Court has acted slowly in order to delay this case until after the election. They point out that the Court can move quickly when it wants to, as it did when it disregarded typical procedures in \"Bush v. Gore\", granting the petition on a Saturday, receiving briefs on Sunday, holding oral arguments on Monday, and issuing the final opinion on Tuesday. Author Sonja West, of \"Slate\", argues that the Federal prosecution of Donald Trump (election obstruction case) is of similar importance to \"Bush v. Gore\" and should therefore be treated as expeditiously, but the Court seems to be taking the opposite approach.\nLeaks and inadvertent publications.\nSometimes draft opinions are deliberately leaked or inadvertently released before they are published. Such releases are often purported to harm the court's reputation. Chief Justice Roberts has previously described leaks as an \"egregious breach of trust\" that \"undermine the integrity of our operations\" in reference to the leaked draft opinion for \"Dobbs v. Jackson Women's Health Organization\".\nIn addition to leaks, the Court has sometimes mistakenly released opinions before they are ready to be published. On June 26, 2024, the Court inadvertently posted an opinion for \"Moyle v. United States\" to its website that seemed to indicate that the court will temporarily allow abortions in medical emergencies in Idaho. The official opinion was posted the next day, which returned the case to the lower courts without a ruling on the merits.\nSee also.\nSelected landmark Supreme Court decisions.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nBibliography.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;\nFurther reading.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "31739", "revid": "50519108", "url": "https://en.wikipedia.org/wiki?curid=31739", "title": "Chief Justice of the United States", "text": "Chief judicial officer of the United States\nThe chief justice of the United States is the chief judge of the Supreme Court of the United States and is the highest-ranking officer of the U.S. federal judiciary. Article II, Section 2, Clause 2 of the U.S. Constitution grants plenary power to the president of the United States to nominate, and, with the advice and consent of the United States Senate, appoint \"Judges of the Supreme Court\", who serve until they die, resign, retire, or are impeached and convicted. The existence of a chief justice is only explicit in which states that the chief justice shall preside over the impeachment trial of the president; this has occurred three times, for Andrew Johnson, Bill Clinton, and for Donald Trump's first impeachment.\nThe chief justice has significant influence in the selection of cases for review, presides when oral arguments are held, and leads the discussion of cases among the justices. Additionally, when the court renders an opinion, the chief justice, if in the majority, chooses who writes the court's opinion; however, when deciding a case, the chief justice's vote counts no more than that of any other justice. \nWhile nowhere mandated, the presidential oath of office is by tradition administered by the chief justice. The chief justice serves as a spokesperson for the federal government's judicial branch and acts as a chief administrative officer for the federal courts. The chief justice presides over the Judicial Conference and, in that capacity, appoints the director and deputy director of the Administrative Office. The chief justice is an \"ex officio\" member of the Board of Regents of the Smithsonian Institution and, by custom, is elected chancellor of the board.\nSince the Supreme Court was established in 1789, 17 people have served as Chief Justice, beginning with John Jay (1789\u20131795). The current chief justice is John Roberts (since 2005). Five of the 17 chief justices\u2014John Rutledge, Edward Douglass White, Charles Evans Hughes, Harlan Fiske Stone, and William Rehnquist\u2014served as associate justices prior to becoming chief justice. Additionally, Chief Justice William Howard Taft had previously served as president of the United States.\nOrigin, title and appointment.\nThe United States Constitution does not explicitly establish an office of the Chief Justice but presupposes its existence with a single reference in Article I, Section 3, Clause 6: \"When the President of the United States is tried, the Chief Justice shall preside.\" Nothing more is said in the Constitution regarding the office. Article III, Section 1, which authorizes the establishment of the Supreme Court, refers to all members of the court simply as \"judges\". The Judiciary Act of 1789 created the distinctive titles of \"Chief Justice of the Supreme Court of the United States\" and \"Associate Justice of the Supreme Court of the United States\".\nIn 1866, Salmon P. Chase assumed the title of \"Chief Justice of the United States,\" and Congress began using the new title in subsequent legislation. The first person whose Supreme Court commission contained the modified title was Melville Fuller in 1888. The associate justice title was not altered in 1866 and remains as originally created.\nThe chief justice, like all federal judges, is nominated by the president and confirmed to office by the U.S. Senate. Article III, Section 1 of the Constitution specifies that they \"shall hold their Offices during good Behavior.\" This language has been interpreted to mean that judicial appointments are effectively for life and that once in office, a justice's tenure ends only when the justice dies, retires, resigns, or is removed from office through the impeachment process. Since 1789, 15 presidents have made a total of 22 official nominations to the position of chief justice.\nThe salary of the chief justice is set by Congress; as of 2024, the annual salary is $312,200, which is slightly higher than that of associate justices, which is $298,500. The practice of appointing an individual to serve as Chief Justice is grounded in tradition; while the Constitution mandates that there be a chief justice, it is silent on the subject of how one is chosen and by whom. There is no specific constitutional prohibition against using another method to select the chief justice from among those justices properly appointed and confirmed to the Supreme Court.\nThree incumbent associate justices have been nominated by the president and confirmed by the Senate as Chief Justice: Edward Douglass White in 1910, Harlan Fiske Stone in 1941, and William Rehnquist in 1986. A fourth, Abe Fortas, was nominated to the position in 1968 but was not confirmed. As an associate justice does not have to resign their seat on the court in order to be nominated as Chief Justice, Fortas remained an associate justice. Similarly, when Associate Justice William Cushing was nominated and confirmed as Chief Justice in January 1796 but declined the office, he too remained on the court. Two former associate justices subsequently returned to service on the court as Chief Justice. John Rutledge was the first. President Washington gave him a recess appointment in 1795. However, his subsequent nomination to the office was not confirmed by the Senate, and he left office and the court. In 1930, former Associate Justice Charles Evans Hughes was confirmed as Chief Justice. Additionally, in December 1800, former chief justice John Jay was nominated and confirmed to the position a second time but ultimately declined it, opening the way for the appointment of John Marshall.\nPowers and duties.\nAlong with their general responsibilities as a member of the Supreme Court, the chief justice has several unique duties to fulfill.\nImpeachment trials.\nArticle I, Section 3 of the U.S. Constitution stipulates that the chief justice shall preside over the Senate trial of an impeached president of the United States. Three chief justices have presided over presidential impeachment trials: Salmon P. Chase (1868 trial of Andrew Johnson), William Rehnquist (1999 trial of Bill Clinton), and John Roberts (2020 trial of Donald Trump; Roberts declined to preside over Trump's second trial in 2021, which took place after the end of Trump's presidency. Senate president pro-tempore Patrick Leahy presided). All three presidents were acquitted in the Senate. Although the Constitution is silent on the matter, the chief justice would, under Senate rules adopted in 1999 prior to the Clinton trial, preside over the trial of an impeached vice president. This rule was established to preclude the possibility of a vice president presiding over their own trial.\nSeniority.\nMany of the court's procedures and inner workings are governed by the rules of protocol based on the seniority of the justices. The chief justice always ranks first in the order of precedence\u2014regardless of the length of the officeholder's service (even if shorter than that of one or more associate justices). This elevated status has enabled successive chief justices to define and refine both the court's culture and its judicial priorities.\nThe chief justice sets the agenda for the weekly meetings where the justices review the petitions for certiorari, to decide whether to hear or deny each case. The Supreme Court agrees to hear less than one percent of the cases petitioned to it. While associate justices may append items to the weekly agenda, in practice this initial agenda-setting power of the chief justice has significant influence over the direction of the court. Nonetheless, a chief justice's influence may be limited by circumstances and the associate justices' understanding of legal principles; it is definitely limited by the fact that they have only a single vote of nine on the decision whether to grant or deny certiorari.\nDespite the chief justice's elevated stature, their vote carries the same legal weight as the vote of each associate justice. Additionally, they have no legal authority to overrule the verdicts or interpretations of the other eight judges or tamper with them. The task of assigning who shall write the opinion for the majority falls to the most senior justice in the majority. Thus, when the chief justice is in the majority, they always assign the opinion. Early in his tenure, Chief Justice John Marshall insisted upon holdings which the justices could unanimously back as a means to establish and build the court's national prestige. In doing so, Marshall would often write the opinions himself and actively discouraged dissenting opinions. Associate Justice William Johnson eventually persuaded Marshall and the rest of the court to adopt its present practice: one justice writes an opinion for the majority, and the rest are free to write their own separate opinions or not, whether concurring or dissenting.\nThe chief justice's formal prerogative\u2014when in the majority\u2014to assign which justice will write the court's opinion is perhaps their most influential power, as this enables them to influence the historical record. They may assign this task to the individual justice best able to hold together a fragile coalition, to an ideologically amenable colleague, or to themselves. Opinion authors can have a large influence on the content of an opinion; two justices in the same majority, given the opportunity, might write very different majority opinions. A chief justice who knows the associate justices well can therefore do much\u2014by the simple act of selecting the justice who writes the opinion of the court\u2014to affect the general character or tone of an opinion, which in turn can affect the interpretation of that opinion in cases before lower courts in the years to come.\nThe chief justice chairs the conferences where cases are discussed and tentatively voted on by the justices. They normally speak first and so have influence in framing the discussion. Although the chief justice votes first\u2014the court votes in order of seniority\u2014they may strategically pass in order to ensure membership in the majority if desired. It is reported that:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;Chief Justice Warren Burger was renowned, and even vilified in some quarters, for voting strategically during conference discussions on the Supreme Court in order to control the Court's agenda through opinion assignment. Indeed, Burger is said to have often changed votes to join the majority coalition, cast \"phony votes\" by voting against his preferred position, and declined to express a position at conference.\nPresidential oath.\nThe chief justice has traditionally administered the presidential oath of office to new U.S. presidents. This is merely custom, and is not a constitutional responsibility of the chief justice. The Constitution does not require that the presidential oath be administered by anyone in particular, simply that it be taken by the president. Law empowers any federal or state judge, as well as notaries public, to administer oaths and affirmations. The chief justice ordinarily administers the oath of office to newly appointed and confirmed associate justices, whereas the seniormost associate justice will normally swear in a new chief justice.\nIf the chief justice is ill or incapacitated, the oath is usually administered by the seniormost member of the Supreme Court. Eight times, someone other than the chief justice of the United States administered the oath of office to the president.\nOther duties.\nSince the tenure of William Howard Taft, the office of chief justice has moved beyond just first among equals. The chief justice also:\nUnlike Senators and Representatives, who are constitutionally prohibited from holding any other \"office of trust or profit\" of the United States or of any state while holding their congressional seats, the chief justice and the other members of the federal judiciary are not barred from serving in other positions. John Jay served as a diplomat to negotiate the Jay Treaty, and Earl Warren chaired the President's Commission on the Assassination of President Kennedy.\nDisability or vacancy.\nUnder \u00a0https://, when the chief justice is unable to discharge their functions, or when that office is vacant, the chief justice's duties are carried out by the senior associate justice until the disability or vacancy ends. When William Rehnquist was ill in 2004, John Paul Stevens acted in his stead, presiding over oral arguments.\nCurrently, Clarence Thomas is the senior associate justice.\nList of chief justices.\nSince the Supreme Court was established in 1789, the following 17 men have served as chief justice:\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nFurther reading.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "31740", "revid": "46810474", "url": "https://en.wikipedia.org/wiki?curid=31740", "title": "University of Michigan", "text": "Public university in Ann Arbor, Michigan, US\nThe University of Michigan (UMich, U-M, or Michigan) is a public research university in Ann Arbor, Michigan, United States. Founded in 1817, it is the oldest institution of higher education in the state. It is also one of the earliest American research universities and is a founding member of the Association of American Universities.\nThe university has the largest student population in Michigan, enrolling more than 52,000 students, including more than 30,000 undergraduates and 18,000 postgraduates. It is classified as an \"R1: Doctoral Universities \u2013 Very high research activity\" by the Carnegie Classification. It consists of 19 schools and colleges, offering more than 280 degree programs. The university is accredited by the Higher Learning Commission. In 2021, it ranked third among American universities in research expenditures according to the National Science Foundation.\nThe campus spans . It encompasses Michigan Stadium, which is the largest stadium in the United States, as well as the Western Hemisphere, and ranks third globally. The University of Michigan's athletic teams, including 13 men's teams and 14 women's teams competing in intercollegiate sports, are collectively known as the Wolverines. They compete in NCAA Division I (FBS) as a member of the Big Ten Conference. Between 1900 and 2022, athletes from the university earned a total of 185 medals at the Olympic Games, including 86 gold.\nHistory.\nOrigins.\nThree years after founding Fort Pontchartrain du D\u00e9troit on the strait between Lakes Saint Clair and Erie in 1700, the French explorer and later colonial governor of Louisiana, Antoine de la Mothe Cadillac, wrote to King Louis XIV's minister, Louis Ph\u00e9lypeaux de Pontchartrain, in the royal government in Paris, urging the establishment of a seminary in the newly formed parish of Sainte-Anne-de-D\u00e9troit for the education of both Indigenous and French children in piety and the French language. Cadillac had promised to build the colony into what he described as \"Paris de la Nouvelle-France.\" An establishment of instruction, akin to those in Montr\u00e9al and Qu\u00e9bec, would fulfill his vision. Parish records from 1755 identified a \"director of Christian schools,\" suggesting the influence of the gentle St. John Baptist de La Salle and his famous Institute of the Brothers of the Christian Schools. The colony was surrendered to the British monarchy in 1762 following the French and Indian War. The local French population maintained the Christian schools; however, the British, who viewed the colony merely as a trading post, did practically nothing for education, leading to stagnation during their rule from 1763 to 1796.\nWhen the colony came under the control of the Americans with the signing of the Treaty of Paris in 1783, the judges of the territorial government were supposed to be called in to formally define the rights and legal status of the Christian schools under the new constitution. In 1806, parish-minister Gabriel Richard, who presided over the Christian schools, petitioned for land to found a college and suggested that a lottery might be used to support the academies he headed. Subsequently, in 1817, the Territorial government, at the instigation of both Richard and Judge Augustus B. Woodward and with the support of President Thomas Jefferson, passed \"an Act to establish the Catholepistemiad, or University of Michigania\" within the Territory of Michigan. Enacted on August 26, 1817, the Act effectively consolidated the schools into one institution, with Richard serving as vice president and John Monteith as president. Its \"didactors\" had authority not only over the university itself but also over education in the territory in general. The legislative act was signed into law by Acting Governor and Secretary William Woodbridge, Chief Judge Augustus B. Woodward, and Judge John Griffin.\nThe Catholepistemiad, 1817\u20131821.\nThe term \"Catholepistemiad,\" a neologism derived from a blend of Greek and Latin roots, can be loosely translated as \"School of Universal Knowledge\". The corporation was modeled after an institution established in France a decade prior, known as the \"Universit\u00e9 imperial\", under Emperor Napoleon. It included an array of schools and libraries under a single administration, with the authority to establish additional schools across the territory. It was not until Michigan became a state in 1837 that the corporation focused solely on higher education. In September 1817, the university received a subscription from the Zion Masonic Lodge. Of the total amount subscribed to start the university two-thirds came from Zion Lodge and its members.\nThe cornerstone for the first schoolhouse, situated near the intersection of Bates Street and Congress Street in Detroit, was laid on September 24, 1817, and by the following year, a Lancasterian school, taught by Lemuel Shattuck, and a classical academy were operational. Additional schools were established in Monroe and Mackinaw by the end of September 1817. In 1821, by a new enactment, the university itself was created as a \"body politic and corporate\", maintaining its corporate status through various modifications to its charter. The new act placed the corporation under the control of a board of trustees. Monteith, no longer president, joined the board, and Richard served on the board until his death in 1832. The trustees continued to manage the schools and classical academy, but established no new schools. By 1827, all schools had closed, and the Detroit schoolhouse was leased to private teachers.\nEarly years, 1837\u20131851.\nFollowing Michigan's admission to the Union in 1837, an organic act was passed on March 18, 1837, to reorganize the university under a twelve-member board of regents. The regents met in Ann Arbor and accepted the town's proposal for the university to relocate, based on a grant from the Treaty of Fort Meigs on Judge Henry Rumsey's farmland.\nThe approved campus plans for the university were drawn up by the architect Alexander Davis. Davis designed an elaborate Gothic main building with a large lawn in front, wide avenues, and botanical gardens, all arranged to evoke the French \"ch\u00e2teau\" aesthetic. He also provided possible sites for future buildings; however, the plans were never executed. Instead, four houses for professors were authorized. Historians attribute the abandonment of the original plan to the financial constraints the university faced as a result of the Panic of 1837. Construction began in 1839, and in 1841, Mason Hall, the first campus building, was completed, followed by the construction of South College, a nearly identical building to the south, in 1849, leaving a gap for a future grand centerpiece.\nThe first classes in Ann Arbor were held in 1841, with six freshmen and one sophomore taught by two professors, Joseph Whiting and George Palmer Williams. Asa Gray was the first professor appointed following the university's move to Ann Arbor in 1837. He and the regents were both involved in stocking the university library. In 1846, Louis Fasquelle, a native of France, was appointed as the first professor of modern languages, primarily teaching French and writing textbooks. French became the first modern language taught at the university. During the first commencement in 1845, eleven graduates, including Judson Collins, were awarded Bachelor of Arts degrees.\nIn subsequent years, the regents established branches throughout the state to function as preparatory schools for the university. They began with a branch in Pontiac, soon expanding to Kalamazoo, Detroit, Niles, Tecumseh, White Pigeon, and Romeo. However, these branches struggled to enroll students, leading some to merge with local colleges. Notably, Kalamazoo College operated as the Kalamazoo Branch of the University of Michigan from 1840 to 1850.\nThe administration during the early years of the university was complicated and designed to keep it tightly under state authority. The university's business was often intertwined with state affairs. The position of chancellor of the university, created by the organic act in 1837, was never filled, and the positions on the board of regents, appointed by the governor, were often held by state officials. The lieutenant governor, the justices of the Michigan Supreme Court, and the chancellor of the state all served as ex officio members of the board, with the governor himself chairing the board. The regents' powers were shared with a rotating roster of professors, who were responsible for some vague aspects of the university's administrative duties; however, all important decisions had to be made by the governor and his party. There were several attempts to gain independence from the state legislature, but progress was slow until the late 1840s, when the regents gained leverage, supported by Michigan citizens. This shift culminated in a revision of the organic act on April 8, 1851, which freed the university from legislative control, transitioned the regent positions from appointed to elected, and established a president selected by the regents.\n1851 to 1900.\nHenry Philip Tappan became the university's first president in 1852, with the ambition to shape the institution as a model for future universities. During his decade of service, he overhauled the curriculum, expanded the library and museum collections, established the law school, and supervised the construction of the Detroit Observatory. He secularized faculty appointments by prioritizing merit in selections, breaking away from the retrograde tradition of regents distributing positions among Protestant denominations. In 1855, Michigan became the second university in the country to issue Bachelor of Science degrees. The following year, the country's first chemical laboratory was built on campus, specifically designed for chemistry education, providing additional space for classes and laboratories. Tappan's tenure also saw the creation of the Michigan Glee Club, the oldest student organization at the university, and the publication of the first student newspaper, \"The Peninsular Phoenix and Gazetteer\", in 1857. Despite these accomplishments, Tappan's 11-year presidency was marked by considerable tension. His impartial stance on religion faced backlash during a time of heightened religious fervor. Due to changes in the Board of Regents and discontent with his administration, he was forced to resign in 1863.\nIn 1863, Erastus Otis Haven took office as president, having been a professor at the time and needing to prove his right for the presidency. The campus was divided by conflicting views among students, faculty, and regents regarding Tappan's restoration, the homeopathy crisis, and the Civil War. Haven's administration faced routine administrative difficulties and struggled to garner support for increased state aid, despite achieving modest gains. The university, which had received a fixed $15,000 since 1869, still required additional funding. Frustrated, Haven resigned in 1869 to become president of Northwestern, a Methodist institution, a move that sectarians viewed as a setback for secular colleges. The presidency remained vacant from 1869 to 1871, with Professor Henry Simmons Frieze serving as acting president. During this period, the university raised funds for University Hall, overhauled admissions with a diploma system, and introduced coeducation. Women were first admitted in 1870, although Alice Robinson Boise Wood was the first woman to attend classes in 1866. In 1870, Gabriel Franklin Hargo graduated from the law school as the second African American to graduate from a law school in the United States. In 1871, Sarah Killgore became the first woman to graduate from law school and be admitted to the bar of any state in the United States. Frieze, a champion of music education, also established the University Musical Society. By the late 1860s, the university had become one of the largest in the nation, alongside Harvard in Cambridge. However, it faced ongoing issues with student discipline, including class rushes, instances of hazing, and rowdiness in chapel. Frieze attributed these problems to a lack of centralized faculty control.\nJames Burrill Angell became president in 1871 and would remain in the post for nearly four decades. His tenure would be remembered as the most successful in the university's history. Tappan's reforms in the 1850s set the university on a path to becoming an elite institution, but it was Angell who completed that transformation. Shortly after Angell's arrival, University Hall was completed at vast expense; it would remain the university's major academic building right up until the 1950s. During his presidency, Angell restored campus discipline, raised entrance and graduation requirements, and persuaded the legislature to increase state aid. Angell's tenure saw the addition of many extracurricular activities, including the intercollegiate football team. Though a reformer, Angell was not authoritarian; he encouraged open debate and aimed for near-unanimous agreement before implementing changes, rather than pushing through with only a narrow majority. This approach enabled him to address knotty issues on campus, including the long-standing homeopathy problem. Angell transformed the curriculum to focus on electives, expanding course offerings. That led to a faculty of great minds in many fields, from John Dewey in philosophy to Frederick George Novy in bacteriology. In 1875, the university founded the College of Dental Surgery, followed by the establishment of the College of Pharmacy by Albert B. Prescott in 1876. That year, the university awarded its first Doctor of Philosophy degrees: to Victor C. Vaughan in chemistry and William E. Smith in zoology. They were among the first doctoral degrees to be conferred in the nation. The university remained the only institution in the state to grant PhD degrees until the late 1940s.\n&lt;templatestyles src=\"Template:Quote_box/styles.css\" /&gt;\n\"Stand up for America; devote your life to its cause; love your homes, and prove as worthy of our cherished free institutions as they are worthy of your allegiance and service. Let not the high standard of National Honor, raised by the fathers, be lowered by their sons. Let learning, liberty and law be exalted and enthroned.\"\n William McKinley\n, speaking to the first National Convention of the College Republicans in Newberry Hall in 1892\nWith his presidency, Angell focused the university on preparing a new generation of statesmen for public service. Angell himself was frequently called upon by the White House for diplomatic missions. In 1880, President Rutherford Hayes appointed him as Minister to China, where he successfully negotiated an immigration treaty that facilitated foreign student enrollment. Later, in 1887, 1896, and 1897, President Grover Cleveland appointed him to fisheries and waterways commissions. That same year, President William McKinley named him Envoy Extraordinary to Turkey. By the late 19th century, the university had gained an international reputation, in large part due to Angell's diplomatic efforts. During this period, over eighty subjects of the Emperor of Japan were sent to Ann Arbor to study law as part of the opening of that empire to external influence. The university also played a key role in developing the Philippine education, legal, and public health systems during American colonization, largely due to the contributions of Michigan alumni like Dean Conant Worcester and George A. Malcolm. Angell retired in 1909, and seven years later, he died in the President's House, which had been his home for forty-five years. His successor, Harry Burns Hutchins, who was once his student, would lead the university through World War I and the Great Influenza epidemic.\n1900 to 1950.\nIn 1910, Harry Burns Hutchins assumed the presidency, becoming the first alumnus to hold that position. He had spent seven years in Ithaca, New York, where he was called by Andrew Dickson White and Charles Kendall Adams to establish the Cornell Law School. Hutchins then became the dean of the law school at his alma mater, where he introduced the case method of instruction. Hutchins was acting president when Angell was absent. During his presidency, Hutchins established the Graduate School, doubled enrollment, and increased the faculty. He secured more state aid and alumni support to fund the university's capital needs, including the gothic Law Quadrangle, Martha Cook Building, Hill Auditorium, and Michigan Union, which became campus landmarks. Hutchins enhanced the university health service, but wartime distractions plagued his presidency. The influenza epidemic, which caused student deaths from poor care, deeply troubled him. Well-liked by the regents who encouraged him to remain president, nonetheless, Hutchins retired in 1920.\nThe 1920s at the university were marked by the brief tenures of two presidents, Marion LeRoy Burton and Clarence Cook Little. In 1920, when Burton assumed office, a conference on higher education took place at the university, resulting in the establishment of the Association of Governing Boards of Universities and Colleges. Under his leadership, construction boomed on campus, and enrollments increased, propelled by the prosperous economy of the Roaring Twenties. He initiated the annual honors convocation, introduced the deans' conference, and increased university income. Burton fell ill in 1924 and died in 1925. In this emergency, President Emeritus Hutchins was called by the regents to assist, with Alfred Henry Lloyd serving as acting president until Little's arrival. Clarence Cook Little was elected president in 1925. He was considered a polarizing figure due to his progressive stance, which alienated many Roman Catholics. The proposal for establishing a nonsectarian divinity school on campus came after strong advocacy from Charles Foster Kent and received unanimous backing from nearby churches. However, this school was short-lived and was quietly shelved in 1927 during Little's presidency. Little advocated for individualized education and sought to reform curricula, particularly for women. He proposed a curriculum division after two years of study to address knowledge gaps, which led to the University College proposal. This initiative was ultimately abandoned following his resignation in 1929.\nFollowing Little's resignation, Alexander Grant Ruthven, an alumnus, was elected president by unanimous vote. He would lead the university through the Great Depression and World War II. Under Ruthven's leadership, the university administration became more decentralized with the creation of the university council, various divisions, and a system of committees. For years, the university was a backwater in theoretical physics. Nonetheless, this changed under department head Harrison McAllister Randall, who brought theorists Samuel Goudsmit, George Uhlenbeck, and Otto Laporte onto the faculty. Goudsmit mentored famous students at the university, including Robert Bacher and Wu Ta-You, the Father of Chinese Physics, who in turn taught Zhu Guangya and two Nobel laureates, Chen Ning Yang and Tsung-Dao Lee. From 1928 to 1941, the Summer Symposium in Theoretical Physics featured renowned physicists like Niels Bohr, Werner Heisenberg, Paul Dirac, and Erwin Schr\u00f6dinger, with at least fifteen attendees being Nobel laureates or future laureates. Wolfgang Pauli held a visiting professorship at the university in 1931. Stephen Timoshenko created the first U.S. bachelor's and doctoral programs in engineering mechanics when he was a faculty professor at the university. In 1948, shortly after World War II, the Michigan Memorial Phoenix Project was established to honor the hundreds of lives lost from the university during the war. Funded by numerous contributors, including the Ford Motor Company, the Phoenix Project operated the Ford Nuclear Reactor, which established the nation's first academic program in nuclear science and engineering.\n1950 to present.\nFor most of the 19th and early 20th centuries, Michigan stood as the sole university within the state, and its alumni often dominated state politics. However, by the mid-20th century, the landscape began to shift as new universities emerged, many of which were former technical schools, thereby threatening that status quo. When Harlan Hatcher took office as president in 1951, he was entrusted with securing the university's preeminence among the nation and world's burgeoning research institutions. This marked the beginning of a phase of post-war development, during which Hatcher oversaw the construction of North Campus, the founding of Flint Senior College, and the establishment of the Dearborn Center, both of which have since evolved into full-fledged branch campuses. Nonetheless, the tenures of Hatcher and his successor, Robben Wright Fleming, were marked by a sharp rise in campus activism, linked to the Civil Rights Movement and the anti-Vietnam War movement. In 1963, a controversial set of admissions practices collectively known as 'affirmative action' was introduced. It was a radical measure originated by Hobart Taylor Jr., aimed at boosting Black student enrollment at elite universities. Meanwhile, in 1964, a group of faculty hosted the nation's first \"teach-in\" against U.S. foreign policy in Southeast Asia, attended by thousands of students. Thereafter, sit-ins by campus groups prompted administrative crackdowns, leading to more unrest across the campus. Meanwhile, Hatcher controversially dismissed three professors for their refusal to cooperate with Joseph McCarthy's House Un-American Activities Committee during his tenure. Hatcher's successor, Robben Wright Fleming guided the university through a turbulent era of activism. In 1969, a year into Fleming's tenure, a Marxist militant organization known as the Weather Underground was founded on campus. This group, led by members of the Jesse James Gang, an offshoot of the Students for a Democratic Society, employed militant tactics, including taking hostages. A year later, a strike organized by the Black Action Movement, another radical group, resulted in the university agreeing to several demands for minority support. In 1971, the Spectrum Center was founded as the nation's oldest collegiate LGBT student center. Meanwhile, support among students for marijuana legalization was gaining traction on campus, as evidenced by the annual Hash Bash rally that began in 1972. Throughout the 1960s and 1970s, campus unrest began to diminish the university's academic standing, which had been ranked among the top five in the nation. That standing started to decline during Fleming's tenure. Campus unrest persisted during Harold Tafler Shapiro's presidency, which began in 1980, fueled by controversies surrounding the university's national defense initiatives and foreign investments.\nPresident James Duderstadt would succeed Shapiro and remain president until 1996. He supported campus growth and fundraising initiatives. Duderstadt's successor, Lee Bollinger, conducted several major construction projects like the School of Social Work building and the Tisch Hall, named in honor of alumnus Preston Robert Tisch. In 2003, the U.S. Supreme Court heard two cases regarding the university's affirmative action admissions: Grutter v. Bollinger and Gratz v. Bollinger. In 2002, the university elected its first female president, Mary Sue Coleman, by unanimous vote. Throughout her presidency, Michigan's endowment saw continued growth, accompanied by a major fundraising drive known as \"The Michigan Difference\". The Coleman's administration faced labor disputes with the university's labor unions, notably with the Lecturers' Employees Organization and the Graduate Employees Organization. In the early 2000s, the university faced declining state funding, prompting suggestions for privatization. Despite being a state institution de jure, it adopted private funding models. A 2008 legislative panel further recommended converting it to a private institution due to its minimal ties to the state. Mark Schlissel succeeded Coleman in 2014. Before his firing in 2022, Schlissel expanded financial aid offerings, enhanced international engagement, and raised student diversity. He also led initiatives in biosciences and the arts. Schlissel's successor, Santa Ono, would serve a short and lackluster tenure amid the national pro-Palestinian protests before his immediate resignation. The presidency has remained vacant since then, with Domenico Grasso serving as the interim president.\nHistorical links.\nThe founding of the University of Michigan in the 19th century was influenced by the transatlantic Republic of Letters, an intellectual community that spanned Europe and the Americas. Key figures, such as Henry Philip Tappan, were instrumental in aligning the university with the ideals championed by the intellectual community, including liberty, reason, and scientific inquiry. Alumni and faculty from Michigan, like Andrew Dixon White, carried these ideals forward as they shaped other institutions. Notably, Cornell alumni David Starr Jordan and John Casper Branner later introduced these concepts to Stanford University in the late 19th century. Early university leaders like James Burrill Angell contributed to establishing other universities by sharing their insights. Alongside Charles William Eliot of Harvard, Andrew D. White of Cornell, and Noah Porter of Yale, Angell was heavily involved in the early period of Johns Hopkins University as an advisor to the trustees and recommended Daniel Coit Gilman as the first president of the wealthy new foundation. Clark Kerr, the first chancellor of the University of California, Berkeley, referred to Michigan as the \"mother of state universities.\"\nCampus.\nThe University of Michigan's campus in Ann Arbor is divided into four main areas: the Central Campus area, the North Campus area, the North Medical Campus area, and Ross Athletic Campus area. The campus areas include more than 500 major buildings, with a combined area of more than . The Central and Athletic Campus areas are contiguous, while the North Campus area is separated from them, primarily by the Huron River. The North Medical Campus area was developed on Plymouth Road, with several university-owned buildings for outpatient care, diagnostics, and outpatient surgery.\nAll four campus areas are connected by bus services, the majority of which connect the North and Central campus areas. There is a shuttle service connecting the University Hospital, which lies between North and Central campus areas, with other medical facilities throughout northeastern Ann Arbor.\nThere is leased space in buildings scattered throughout the city, many occupied by organizations affiliated with the University of Michigan Health System. In addition to the University of Michigan Golf Course on Ross Athletic Campus, the university operates a second golf course on Geddes Road called Radrick Farms Golf Course. The university also operates a large office building called Wolverine Tower in southern Ann Arbor. The Inglis House is an off-campus facility, which the university has owned since the 1950s. The Inglis House is a mansion used to hold various social events, including meetings of the Board of Regents, and to host visiting dignitaries. Another major off-campus facility is the Matthaei Botanical Gardens, which is located on the eastern outskirts of the City of Ann Arbor.\nCentral Campus Historic District.\nThe original Central Campus spanned , bordered by North University Avenue, South University Avenue, East University Avenue, and State Street. The master plan was developed by Alexander Jackson Davis. The first structures built included four Greek Revival faculty residences in 1840, as well as Mason Hall (1841\u20131950) and South College (1849\u20131950), which functioned as both academic spaces and dormitories. Only one of the original faculty residences remains today; it has been renovated in the Italianate style to serve as the President's House, making it the oldest building on campus. The Chemical Laboratory, built by Albert Jordan in 1856 and operational until 1980, was notable for housing the nation's first instructional chemistry lab. After the completion of the Old Medical Building (1850\u20131914) and the Law Building (1863\u20131950), an open space known as The Diag began to take shape. Among the notable structures on the original Central Campus were University Hall (1872\u20131950), designed by alumnus E. S. Jennison, and the Old Library (1881\u20131918), designed by Henry Van Brunt, who was also the architect behind Memorial Hall (1870) in Cambridge, Massachusetts.\nThe Central Campus today, however, bears little resemblance to its 19th-century appearance, as most of its original structures were demolished and rebuilt in the early 20th century. The rebuilt structures are primarily the work of Albert Kahn, who served as the university's supervising architect during that period. In 1909, Regent William L. Clements became chairman of the Building and Grounds Committee, leading to Albert Kahn's growing influence in the university's architectural development. Clements, impressed by Kahn's work on his industrial projects and residence in Bay City, awarded him multiple university commissions and appointed him as the university's supervising architect. The West Engineering Hall (1910), Natural Science Building (1915), and General Library (1920) were all designed by Kahn. During a period of limited construction funding, these structures exhibited a simple design with minimal ornamentation. However, Kahn's Hill Auditorium (1913), adequately funded by Regent Arthur Hill, features extensive Sullivanesque ornamentation and excellent acoustic design, which was rare for that period.\nBeginning in 1920, the university received greater funding for construction projects, thanks to president Burton's fiscal persuasiveness with the legislature, propelled by a prosperous economy. This allowed campus buildings to be constructed in a grand manner. Kahn's Italian Renaissance Clements Library (1923), Classical Greek Angell Hall (1924), and Art Deco Burton Memorial Tower (1936) all feature unusual and costly materials and are considered some of his most elegant university buildings. The last of Kahn's university commissions was the Ruthven Museums Building (1928), designed in the Renaissance style.\nOther architects who contributed to the Central Campus include Spier &amp; Rohns, who designed Tappan Hall (1894) and the West Medical Building (1904); Smith, Hinchman and Grylls, the architects of the Chemistry Building (1910) and East Engineering Building; and Perkins, Fellows and Hamilton, who designed University High School (1924). The Michigan Union (1919) and Michigan League (1929), completed by alumni Irving Kane Pond and Allen Bartlit Pond, house the university's various student organizations. Alumni Memorial Hall, funded by contributions from alumni in memory of the university's Civil War dead, was completed by Donaldson and Meier. It was designated as the University Museum of Art in 1946.\nThe area just south of The Diag is predominantly Gothic in character, contrasting with the classical designs prevalent in many of Kahn's university buildings. The Martha Cook Building (1915), completed by York and Sawyer, Samuel Parsons, and George A. Fuller in 1915, draws inspiration from England's Knole House and Aston Hall. It was one of the university\u2019s early women\u2019s residences. York and Sawyer also designed the Law Quadrangle, which features a flagstone courtyard by landscape architect Jacob Van Heiningan. The Lawyers' Club, part of the quadrangle, includes a clubhouse, dining hall, and dormitory, modeled after English clubs with an Elizabethan-style lounge and a dining hall inspired by the chapels of Eaton. The Law Library's main reading room showcases craftsmanship from the Rockefeller Church of New York. Hutchins Hall, designed by alumnus James Baird, is named after Harry Burns Hutchins, the fourth president of the university. Following its completion, nearby buildings like the School of Education Building, by Malcomson and Higginbotham, and Emil Lorch's Architecture and Design Building adopted Gothic elements reflecting the style of the Law Quadrangle and Martha Cook Residence.\nThe Central Campus is the location of the College of Literature, Science and the Arts. Most of the graduate and professional schools, including the Law School, Ross School of Business, Gerald R. Ford School of Public Policy, and the School of Dentistry, are on Central Campus. Two main libraries, Hatcher Graduate Library and Shapiro Undergraduate Library, as well as the university's many museums, are also on Central Campus.\nNorth Campus.\nThe North Campus area built independently from the city on a large plot of farmland\u2014approximately \u2014that the university bought in 1952. Architect Eero Saarinen devised the early master plan for the North Campus area and designed several of its buildings in the 1950s, including the Earl V. Moore School of Music Building. The North Campus Diag features a bell tower called Lurie Tower, which contains a grand carillon. The university's largest residence hall, Bursley Hall, is in the North Campus area.\nThe North Campus houses the College of Engineering, the School of Music, Theatre &amp; Dance, the Stamps School of Art &amp; Design, the Taubman College of Architecture and Urban Planning, and an annex of the School of Information. The campus area is served by Duderstadt Center, which houses the Art, Architecture and Engineering Library. Duderstadt Center also contains multiple computer labs, video editing studios, electronic music studios, an audio studio, a video studio, multimedia workspaces, and a 3D virtual reality room. Other libraries located on North Campus include the Gerald R. Ford Presidential Library and the Bentley Historical Library.\nRoss Athletic Campus.\nRoss Athletic Campus is the site for the university's athletic programs, including major sports facilities such as Michigan Stadium, Crisler Center, and Yost Ice Arena. The campus area is also the site of the Buhr library storage facility, Revelli Hall, home of the Michigan Marching Band, the Institute for Continuing Legal Education, and the Student Theatre Arts Complex, which provides shop and rehearsal space for student theatre groups. The university's departments of public safety and transportation services offices are located on Ross Athletic Campus.\nThe University of Michigan Golf Course is located south of Michigan Stadium. It was designed in the late 1920s by Alister MacKenzie, the designer of Augusta National Golf Club in Augusta, Georgia, home of the Masters Tournament. The course opened to the public in 1931 and has one of \"the best holes ever designed by Augusta National architect Alister MacKenzie\" according to the magazine \"Sports Illustrated\" in 2006.\nResidential halls.\nThe University of Michigan's residential system can accommodate approximately 10,000 students. The residence halls are located in three distinct geographic areas on campus: the Central Campus area, The Hill (between the Central Campus area and the main medical campus) and the North Campus area. The largest residence hall has a capacity of 1,270 students, while the smallest accommodates 25 residents.\nOrganization and administration.\nGovernance.\nThe University of Michigan is governed by the Board of Regents, established by the Organic Act of March 18, 1837. It consists of eight members, elected at large in biennial state elections for overlapping eight-year terms. Before the Office of President was established in 1850, the University of Michigan was directly managed by the appointed regents, with a rotating group of professors responsible for carrying out day-to-day administrative duties. The Constitution of the State of Michigan of 1850 restructured the university's administration. It established the Office of the President and transitioned the Board of Regents to an elected body. The state constitution granted the Board of Regents the power to appoint a non-voting presiding president to lead their meetings, effectively elevating the board to the level of a constitutional corporation independent of the state administration and making the University of Michigan the first public institution of higher education in the country so organized. As of 2021\u201322, the Board of Regents is chaired by Jordan B. Acker (B.A. '06).\nThe Board of Regents delegates its power to the university president who serves as the chief executive officer responsible for managing the day-to-day operations of the university, that is, the main campus in Ann Arbor. The president retains authority over the branch campuses in Dearborn and Flint but is not directly involved in their day-to-day management. Instead, two separate chancellors are appointed by the president to serve as chief executive officers overseeing each branch campus. All presidents are appointed by the Board of Regents to serve five-year terms, at the board's discretion, and there are no term limits for university presidents. The board has the authority to either terminate the president's tenure or extend it for an additional term.\nThe presidency of the University of Michigan is currently vacant, with Domenico Grasso (PhD \u201887) serving as the interim president. Following the immediate resignation of Santa Ono, the university's former president, the board of regents named Grasso as the university\u2019s interim president on May 8, 2025. He will lead the university during the search for the next president. Laurie McCauley has been serving as the 17th and current provost of the university since May 2022, and she was recommended by former president Santa Ono to serve a full term through June 30, 2027.\nThe President's House, located at 815 South University Avenue on the Ann Arbor campus, is the official residence and office of the University President. Constructed in 1840, the three-story Italianate President's House is the oldest surviving building on the Ann Arbor campus and a University of Michigan Central Campus Historic District contributing property.\nStudent government.\nThe Central Student Government, housed in the Michigan Union, is the university's student government. As a 501(c)(3) independent organization, it represents students from all colleges and schools, manages student funds on campus, and has representatives from each academic unit. The Central Student Government is separate from the University of Michigan administration.\nOver the years, the Central Student Government has led voter registration drives, revived Homecoming events, changed a football seating policy, and created a Student Advisory Council for Ann Arbor city affairs. A longstanding goal of the Central Student Government has been to create a student-designated seat on the Board of Regents. In 2000 and 2002, students Nick Waun, Scott Trudeau, Matt Petering, and Susan Fawcett ran for the Board of Regents on the statewide ballot as third-party nominees, though none were successful. A 1998 poll by the State of Michigan concluded that a majority of voters would approve adding a student regent position if put to a vote. However, amending the composition of the Board of Regents would require a constitutional amendment in Michigan.\nIn addition to the Central Student Government, each college and school at the University of Michigan has its own independent student governance body. Undergraduate students in the College of Literature, Science, and the Arts are represented by the LS&amp;A Student Government. Engineering Student Government manages undergraduate student government affairs for the College of Engineering. Graduate students enrolled in the Rackham Graduate School are represented by the Rackham Student Government, and law students are represented by the Law School Student Senate as is each other college with its own respective government. In addition, the students who live in the residence halls are represented by the University of Michigan Residence Halls Association, which contains the third most constituents after Central Student Government and LS&amp;A Student Government.\nFinances.\nIn the fiscal year 2022\u201323, the State of Michigan spent $333 million on the university, which represents 3.03% of its total operating revenues of $11 billion. The university is the second-largest recipient of state appropriations for higher education in Michigan for 2022-23, trailing Michigan State University ($372 million). The Office of Budget and Planning reports that Michigan Medicine's auxiliary activities are the largest funding source, contributing $6.05 billion to the Auxiliary Funds, which accounts for 55.1% of the total operating budget. Student tuition and fees contributed $1.95 billion to the General Fund, accounting for 11% of the total budget. Research grants and contracts from the U.S. federal government contributed $1.15 billion to the Expendable Restricted Funds, accounting for 10.4% of the total budget.\nThe university's current (FY 2022\u201323) operating budget has four major sources of funding:\nEndowment.\nThe university's financial endowment, known as the \"University Endowment Fund\", comprises over 12,400 individual funds. Each fund must be spent according to the donor's specifications. Approximately 28% of the total endowment is allocated to support academic programs, while 22% is designated for student scholarships and fellowships. Approximately 19% of the endowment was allocated to Michigan Medicine and can only be used to support research, patient care, or other purposes specified by donors.\nAs of 2023[ [update]], the university's endowment, valued at $17.9 billion, ranks as the tenth largest among all universities in the country. The university ranks 86th in endowment per student. The law school's endowment, totaling over $500 million, has a significantly higher per-student value compared to that of its parent university. It ranks as the eighth wealthiest law school in the nation in 2022.\nSchools and colleges.\nThere are thirteen undergraduate schools and colleges. By enrollment, the three largest undergraduate units are the College of Literature, Science, and the Arts, the College of Engineering, and the Ross School of Business. At the graduate level, the Rackham School of Graduate Studies serves as the central administrative unit of graduate education at the university. There are 18 graduate schools and colleges. Professional degrees are conferred by the Taubman College of Architecture and Urban Planning, the School of Nursing, the School of Dentistry, the Law School, the Medical School, and the College of Pharmacy. Michigan Medicine, the university's health system, comprises the university's three hospitals, dozens of outpatient clinics, and many centers for medical care, research, and education.\nAcademics.\nAdmissions.\nUndergraduate.\n\"U.S. News &amp; World Report\" rates Michigan \"Most Selective\", and The Princeton Review rates its admissions selectivity of 96 out of 99. Admissions are characterized as \"more selective, lower transfer-in\" according to the Carnegie Classification. Michigan received over 83,000 applications for a place in the 2021\u201322 freshman class, making it one of the most applied-to universities in the United States. Of those students accepted to Michigan's Class of 2027, 7,050 chose to attend.\nAdmission is based on academic prowess, extracurricular activities, and personal qualities. The university's admission process is need-blind for domestic applicants. It does not consider legacy preferences. Admissions officials consider a student's standardized test scores, application essay and letters of recommendation to be important academic factors, with emphasis on an applicant's academic record and GPA, while ranking an applicant's high school class rank as 'not considered'. In terms of non-academic materials as of 2022, Michigan ranks character/personal qualities and whether the applicant is a first-generation university applicant as 'important' in making first-time, first-year admission decisions, while ranking extracurricular activities, talent/ability, geographical residence, state residency, volunteer work, work experience and level of applicant's interest as 'considered'. Some applicants to Music, Theatre and Dance and some applicants to the College of Engineering may be interviewed. A portfolio is required and considered for admission for Art, Architecture and the Ross School of Business. Submission of standardized test scores is recommended but not compulsory. Of the 52% of enrolled freshmen in 2023 who submitted SAT scores; the middle 50 percent Composite scores were 1350\u20131530. Of the 18% of the incoming freshman class who submitted ACT scores; the middle 50 percent Composite score was between 31 and 34.\nSince the fall of 2021, the university has had the largest number of students in the state, surpassing Michigan State University's former enrollment leadership. Given the state's shrinking pool of college-age students, there is public concern that the university's expansion could harm smaller schools by drawing away good students. Some of the state's regional public universities and smaller private colleges have already seen significant declines in enrollment, while others face difficulties in maintaining enrollment figures without lowering admission standards.\nThe university experienced an unexpected surge in student enrollment for the 2023 academic year, having admitted more students than it could support. This over-yield situation has placed considerable strain on student housing affordability, heightened faculty members' workloads, and stretched resources thin. The university is now embracing a steady-state admissions management strategy aimed at maintaining a stable class size.\nGraduate.\nThe Horace H. Rackham School of Graduate Studies, the graduate school of the University of Michigan, received a total of 21,554 applications for admission into its doctoral programs for the 2024 admission year, encompassing the Summer and Fall terms. The school extended offers of admission to 2,586 applicants, representing 12.00% of the applicant pool. Subsequently, 1,102 of the offers were accepted, resulting in a yield rate of 42.61% for the academic year. Applicants may submit multiple applications to different doctoral programs and receive multiple offers, but can only matriculate into one program at a time. Doctoral programs that are not administered by Rackham are not included in the statistics.\nThe selectivity of admissions to doctoral programs varies considerably among different disciplines, with certain highly competitive fields exhibiting acceptance rates in the single digits. For instance, in 2023, the field of Business Administration admitted only 5.2% of its 519 applicants. Similarly, the field of Sociology had a selectivity rate of 5.01%, selecting from a pool of 439 applicants. The field of Psychology was even more competitive, with a selectivity rate of 4.1% out of 805 applicants. Other traditionally highly competitive fields include Philosophy, Public Policy &amp; Economics, Political Science, and Robotics.\nHistory of admissions policies.\nIn August 1841, the university first published its admission requirements for incoming freshmen. These criteria placed a strong emphasis on proficiency in ancient languages, particularly Latin and Greek. Prospective students faced an examination process that assessed their knowledge across various subjects, including arithmetic, algebra, English grammar, geography, Latin literature (Virgil and Cicero's Select Orations), Greek literature (Jacob's or Felton's Greek Reader), Latin grammar (Andrews and Stoddard's), and Greek grammar (Sophocles's).\nA decade later, the university made a significant change to its admission policy. In 1851, it introduced a more flexible approach by waiving the ancient language requirement for students not pursuing the traditional collegiate course and allowing admission without examination in classical languages for these students. This adjustment can be viewed as a prelude to scientific education, signaling a gradual shift from the classical curriculum to a more diverse and modern academic offering.\nIn the early days of the university, the admission requirements varied across different departments, and most admissions were based on referrals. However, in 1863, a standardized entrance examination was introduced, establishing a single set of qualifications for admission to all academic and professional departments. The university administration at the time praised the implementation of this entrance examination, recognizing its contribution to enhancing the admission process.\nAffirmative action.\nIn 2003, two lawsuits involving U-M's affirmative action admissions policy reached the U.S. Supreme Court (\"Grutter v. Bollinger\" and \"Gratz v. Bollinger\"). U.S. President George W. Bush publicly opposed the policy before the court issued a ruling. The court found that race may be considered as a factor in university admissions in all public universities and private universities that accept federal funding, but it ruled that a point system was unconstitutional. In the first case, the court upheld the Law School admissions policy, while in the second it ruled against the university's undergraduate admissions policy. The debate continued because in November 2006, Michigan voters passed Proposal 2, banning most affirmative action in university admissions. Under that law, race, gender, and national origin can no longer be considered in admissions. U-M and other organizations were granted a stay from implementation of the law soon after that referendum. This allowed time for proponents of affirmative action to decide legal and constitutional options in response to the initiative results. In April 2014, the Supreme Court ruled in \"Schuette v. Coalition to Defend Affirmative Action\" that Proposal 2 did not violate the U.S. Constitution. The admissions office states that it will attempt to achieve a diverse student body by looking at other factors, such as whether the student attended a disadvantaged school, and the level of education of the student's parents.\nUndergraduate graduation and retention.\nAmong all first-time freshmen students who enrolled at the university in fall 2017, 82.0% graduated within four years (by August 31, 2021); 10.2% graduated in more than four years but in five years or less (after August 31, 2021, and by August 31, 2022); 1.1% graduated in more than five years but in six years or less (after August 31, 2022, and by August 31, 2023). The percentage of undergraduate students from the fall 2022 cohort returning in fall 2023 was 98.0% for full-time freshman students.\nMajors and programs.\nThe university offers 133 undergraduate majors &amp; degrees across the College of Engineering (18), College of Literature, Science, and the Arts (77), College of Pharmacy (1), Ford School of Public Policy (1), LSA Residential College (3), Marsal Family School of Education (3), Ross School of Business (1), School of Dentistry (1), School of Information (2), School of Kinesiology (3), School of Music, Theatre &amp; Dance (16), School of Nursing (1), School of Public Health (2), Stamps School of Art &amp; Design (2), and Taubman College of Architecture &amp; Urban Planning (2). The most popular undergraduate majors, by 2021 graduates, were computer and information sciences (874), business administration and management (610), economics (542), behavioral neuroscience (319), mechanical engineering (316), experimental psychology (312).\nThe Horace H. Rackham School of Graduate Studies offers more than 180 graduate degree programs in collaboration with fourteen other schools and colleges. Nineteen graduate and professional degree programs, including the juris doctor, master of business administration, doctor of dental surgery, master of engineering, doctor of engineering, doctor of medicine, and doctor of pharmacy, are offered exclusively by the schools and colleges; Rackham does not oversee their administration. The university conferred 4,951 graduate degrees, and 709 first professional degrees in 2011\u20132012.\nEmployability.\nThe university is listed among the leading suppliers of undergraduate and graduate alumni to Silicon Valley tech firms. In 2015, the university ranked 6th on the list of top feeder schools for Google, which employed over 500 graduates at the time. The university ranked 10th on the list of top feeder schools for Meta. Google and Meta remain the university's first and second top employers in 2024.\nIn 2022, Michigan Ross ranked 11th among all business schools in the United States according to Poets &amp; Quants, with its MBA graduates earning an average starting base salary of $165,000 and an average sign-on bonus of $30,000.\nThe U.S. Department of Education reports that as of June 2024, federally aided students who attended University of Michigan-Ann Arbor had a median annual income of $83,648 (based on 2020-2021 earnings adjusted to 2022 dollars) five years after graduation. This figure exceeds both the midpoint for 4-year schools of $53,617 and the U.S. real median personal income of $40,460 for the year 2021 adjusted to 2022 dollars. Federally aided bachelor's graduates from the university's largest program, computer and information science, which had over 950 students in the 2020-21 cohort, had a median annual income of $153,297 five years after graduation.\nAccording to data from the U.S. Department of Education, law tops the list of most valuable first professional degrees offered by the university when ranked by earnings potential in 2022, with its federally aided students earning a median salary of $197,273 five years after graduation. Dentistry ($158,677), pharmacy ($142,224), and medicine ($134,187) follow behind in that order.\nThe fields of business administration ($140,827), economics ($108,627), mathematics ($107,395), and statistics ($105,494) are among the bachelor's degree programs with the highest earning potential offered by the university. In 2022, the university's federally aided students in these programs were earning median salaries exceeding the $100,000 threshold five years after graduation. Additionally, various engineering disciplines such as computer engineering ($123,120), aerospace, aeronautical, and astronautical engineering ($113,025), industrial engineering ($109,239), electrical, electronics and communications engineering ($109,107), mechanical engineering ($101,514), chemical engineering ($100,000) are among the top-earning majors. Computer sciences ($153,297) and information science ($125,257) also fall into this high-earning category.\nLibraries and publications.\nThe University of Michigan library system comprises nineteen individual libraries with twenty-four separate collections\u2014roughly 13.3 million volumes as of 2012. The university was the original home of the JSTOR database, which contains about 750,000 digitized pages from the entire pre-1990 backfile of ten journals of history and economics, and has initiated a book digitization program in collaboration with Google. The University of Michigan Press is also a part of the library system.\nSeveral academic journals are published at the university:\nMuseums.\nThe university is also home to several public and research museums including but not limited to the University Museum of Art, University of Michigan Museum of Natural History, Detroit Observatory, Sindecuse Museum of Dentistry, and the LSA Museum of Anthropological Archaeology.\nKelsey Museum of Archeology has a collection of Roman, Greek, Egyptian, and Middle Eastern artifacts. Between 1972 and 1974, the museum was involved in the excavation of the archaeological site of Dibsi Faraj in northern Syria. The Kelsey Museum re-opened November 1, 2009, after a renovation and expansion.\nThe collection of the University of Michigan Museum of Art include nearly 19,000 objects that span cultures, eras, and media and include European, American, Middle Eastern, Asian, and African art, as well as changing exhibits. The Museum of Art re-opened in 2009 after a three-year renovation and expansion. UMMA presents special exhibitions and diverse educational programs featuring the visual, performing, film and literary arts that contextualize the gallery experience.\nThe University of Michigan Museum of Natural History began in the mid-19th century and expanded greatly with the donation of 60,000 specimens by Joseph Beal Steere in the 1870s.\nReputation and rankings.\nThe university places an emphasis on research and on attracting influential academics to join its faculty. It is a large, four-year, residential research university accredited by the Higher Learning Commission. The four-year, full-time undergraduate program comprises the majority of enrollments and emphasizes instruction in the arts, sciences, and professions with a high level of coexistence between graduate and undergraduate programs. The university has \"very high\" research activity and the comprehensive graduate program offers doctoral degrees in the humanities, social sciences, and STEM fields as well as professional degrees in medicine, law, and dentistry. The university has been included on the list of Public Ivies.\nComprehensive rankings.\nThe 2025-2026 \"U.S. News &amp; World Report Best Global Universities\" report ranked the university 21st among world universities with a global score of 83.2.\nUniversity of Michigan-Ann Arbor was ranked 26th among world universities in 2023 by the Academic Ranking of World Universities, based on the number of alumni or staff as Nobel laureates and Fields Medalists, the number of highly cited researchers, the number of papers published in Nature and Science, the number of papers indexed in the Science Citation Index Expanded and Social Sciences Citation Index, and the per capita academic performance of the institution.\nThe 2024 edition of the CWUR Rankings ranked the university 13th nationally and 16th globally, with an overall score of 89.1, taking into account all four areas evaluated by CWUR: education, employability, faculty, and research. This metric evaluates the number of faculty members who have received prestigious academic distinctions (10% weight). The university's employability ranking is 42nd globally, based on the professional success of the university's alumni, measured relative to the institution's size (25% weight). In the education category, the university is ranked 35th globally. This metric assesses the academic success of the university's alumni, measured relative to the institution's size (25% weight).\nIn the 2025 QS World University Rankings, University of Michigan-Ann Arbor was ranked 44th in the world, its lowest position in 10 years, with an overall score of 79.\nSpecific rankings.\nMichigan was ranked 6th in the 2021 \"U.S. News &amp; World Report\" Best Undergraduate Engineering Programs Rankings. Michigan was ranked 3rd in the 2021 \"U.S. News &amp; World Report\" Best Undergraduate Business Programs Rankings. The 2020 \"Princeton Review\" College Hopes &amp; Worries Survey ranked Michigan as the No. 9 \"Dream College\" among students and the No. 7 \"Dream College\" among parents.\nResearch.\nThe University of Michigan is one of the twelve founding members of the Association of American Universities, a consortium of the leading research universities in North America. The university manages the fourth-largest research budget of any university in the United States, with total R&amp;D expenditures of $1.925 billion in 2023. The federal government was the main source of funding, with grants from the National Science Foundation, National Institutes of Health, NASA, Department of Defense, Department of Energy, Department of Transportation, and National Oceanic and Atmospheric Administration (NOAA), collectively accounting for over half of the research volume.\nThe first measurement of the magnetic moment and spin of free electrons and positrons was conducted by H. Richard Crane, an experimental physicist at the university. The university operated the Ford Nuclear Reactor from 1933 to 1955, during which it conducted extensive research related to nuclear energy. It currently hosts several major research centers focusing on optics, reconfigurable manufacturing systems, and wireless integrated microsystems.\nA pioneer in computing technology, the university designed and built the Michigan Terminal System, an early time-sharing computer operating system, and was involved in the development of the NSFnet national backbone, which is regarded as the foundation upon which the global Internet was built. In 2024, the university began collaborating with Los Alamos National Laboratory on high-performance computing and AI research.\nThe first inactivated flu vaccine was developed by Thomas Francis Jr. and Jonas Salk at Michigan. This was the first of many advancements at the university related to vaccination, including the development of a live attenuated influenza vaccine by Hunein Maassab. The university also introduced histotripsy, a non-invasive technique that uses focused ultrasound to treat diseased tissue, and has made significant contributions to medical technology with innovations such as the EKG and gastroscope.\nIn the social sciences, the Klein\u2013Goldberger model, an enhanced macroeconomic model, was developed by Lawrence Klein and Arthur Goldberger at the university. George Katona created consumer confidence measures in the late 1940s. J. David Singer initiated the Correlates of War project in 1963 to compile scientific knowledge about war. The American National Election Studies, established with a National Science Foundation grant in 1977, has been based at the university and partnered with Stanford University since 2005. The Institute for Social Research, established in 1949, is the nation's longest-standing laboratory for interdisciplinary research in the social sciences.\nThe university has been featured in multiple bibliometric rankings that assess its impact on academic publications through citation analysis. The University Ranking by Academic Performance for 2023\u201324 has positioned the university at 16th globally. Additionally, in 2024, the Performance Ranking of Scientific Papers for World Universities ranked it 13th worldwide. The university has a significant presence in the Nature Index, ranking 6th nationally and 23rd globally among research institutions, with a share of 365.97 and a count of 1199 in 2022. The university boasted 28 researchers who were recognized by Clarivate as being highly cited in 2023. In 2019, the university had 120 faculty members who were national academy members, placing it 10th among its peers in this metric.\nUndergraduate students participate in various research projects through the Undergraduate Research Opportunity Program (UROP) as well as the UROP/Creative-Programs.\nThe university is a member of the international research association Universities Research Association and the state-wise University Research Corridor. Beginning in 2005, the university operated the UM-SJTU Joint Institute with Shanghai Jiao Tong University, but in 2025, it withdrew from the partnership due to national security concerns.\nStudent life.\nStudent body.\nAs of fall 2023, the Ann Arbor campus had 52,065 students enrolled: 33,730 undergraduate students and 18,335 graduate students. The total number of employees reached 53,831, which included 21,475 individuals working with Michigan Medicine, 6,114 supplemental staff, 7,820 faculty members, and 18,422 regular staff. The largest college at the university was the College of Literature, Science, and the Arts with 21,973 students (42.2% of the total student body), followed by the College of Engineering (11,113; 21.3%) and Ross School of Business (4,433; 8.1%). All other colleges each hosted less than 5% of the total student population.\nStudents come from all 50 U.S. states and nearly 100 countries. As of 2022, 52% of undergraduate students were Michigan residents, while 43% came from other states. The remainder of the undergraduate student body was composed of international students. Of the total student body, 43,253 (83.1%) were U.S. citizens or permanent residents and 8,812 (16.9%) were international students as of November 2023.\nAs of October 2023, 53% of undergraduate students self-identified as White, 17% as Asian, 7% as Hispanic, 4% as Black, 5% as belonging to two or more races, and 5% as having an unknown racial composition. The remaining 8% of undergraduates were international students.\nAccording to a 2017 report by the New York Times, the median family income of a student at Michigan was $154,000. 66% of students came from families within the top 20% in terms of income. As of 2022, approximately 23% of in-state undergraduate students and 14% of out-of-state students received a Pell Grant.\nGroups and activities.\nBy 2012, the university had 1,438 student organizations. The student body is politically engaged; in one poll, 96% stated they intended to vote in the 2020 presidential election. It is largely progressive, with 43% identifying as very liberal, 33% as somewhat liberal, and 13% moderate. 11% identified as conservative or very conservative. With a history of student activism, some of the most visible groups include those dedicated to causes such as civil rights and labor rights, such as local chapters of Students for a Democratic Society and United Students Against Sweatshops. Conservative groups also organize, such as the Young Americans for Freedom.\nThere are also several engineering projects teams, including the University of Michigan Solar Car Team, which has placed first in the North American Solar Challenge ten times and podium in the World Solar Challenge seven times and the Wolverine Soft student-run game studio, which has released more than 15 video games on itch.io and Steam. Michigan Interactive Investments, the Tamid Israel Investment Group, and the Michigan Economics Society are also affiliated with the university.\nThe university also showcases many community service organizations and charitable projects, including Foundation for International Medical Relief of Children, Dance Marathon at the University of Michigan, The Detroit Partnership, Relay For Life, U-M Stars for the Make-A-Wish Foundation, InnoWorks at the University of Michigan, SERVE, Letters to Success, PROVIDES, Circle K, Habitat for Humanity, and Ann Arbor Reaching Out. Intramural sports are popular, and there are recreation facilities for each of the three campuses.\nThe Michigan Union and Michigan League are student activity centers located on Central Campus; Pierpont Commons is on North Campus. The Michigan Union houses a majority of student groups, including the student government. The William Monroe Trotter House, located east of Central Campus, is a multicultural student center operated by the university's Office of Multi-Ethnic Student Affairs. The University Activities Center (UAC) is a student-run programming organization and is composed of 14 committees. Each group involves students in the planning and execution of a variety of events both on and off campus.\nThe Michigan Marching Band, composed of more than 350 students from almost all of U-M's schools, is the university's marching band. Over 125 years old (with a first performance in 1897), the band performs at every home football game and travels to at least one away game a year. The student-run and led University of Michigan Pops Orchestra is another musical ensemble that attracts students from all academic backgrounds. It performs regularly in the Michigan Theater. The University of Michigan Men's Glee Club, founded in 1859 and the second oldest such group in the country, is a men's chorus with over 100 members. Its eight-member subset a cappella group, the University of Michigan Friars, which was founded in 1955, is the oldest currently running \"a cappella\" group on campus. The University of Michigan is also home to over twenty other a cappella groups, including Amazin' Blue, The Michigan G-Men, and Compulsive Lyres, all of which have competed at the International Championship of Collegiate A Cappella (ICCA) finals in New York City. Compulsive Lyres are the first and only group from Michigan to claim an ICCA title, having won in 2002. The Michigan G-Men are one of only six groups in the country to compete at ICCA finals four times, one of only two TTBB ensembles to do so, and placed third at the competition in 2015. Amazin' Blue placed fourth at ICCA finals in 2017.\nThe University of Michigan also has over 380 cultural and ethnic student organizations on campus. These range the Arab Student Association to Persian Student Association to African Students Association to even the Egyptian Student Association.\nFraternities and sororities.\nFraternities and sororities play a role in the university's social life; approximately seven percent of undergraduate men and 16% of undergraduate women are active in the Greek system. Four different Greek councils\u2014the Interfraternity Council, Multicultural Greek Council, National Pan-Hellenic Council, and Panhellenic Association\u2014represent most Greek organizations. Each council has a different recruitment process.\nNational honor societies such as Phi Beta Kappa, Phi Kappa Phi, and Tau Beta Pi have chapters at U-M. Degrees \"with Highest Distinction\" are recommended to students who rank in the top 3% of their class, \"with High Distinction\" to the next 7%, and \"with Distinction\" to the next 15%. Students earning a minimum overall GPA of 3.4 who have demonstrated high academic achievement and capacity for independent work may be recommended for a degree \"with Highest Honors\", \"with High Honors\", or \"with Honors\". Those students who earn all A's for two or more consecutive terms in a calendar year are recognized as James B. Angell Scholars and are invited to attend the annual Honors Convocation, an event which recognizes undergraduate students with distinguished academic achievements.\nThe University of Michigan hosts three secret societies: Michigauma, Adara, and the Vulcans. Michigauma and Adara were once under the umbrella group \"The Tower Society\", the name referring to their historical locations in the Michigan Union tower. Michigauma was all-male while Adara was all-female, although both later became co-ed.\nMedia and publications.\nThe student newspaper is \"The Michigan Daily\", which was founded in 1890 and is editorially and financially independent from the university. \"The Daily\" publishes daily online content and a weekly print edition. The yearbook is the \"Michiganensian\", founded in 1896. Other student publications at the university include the conservative \"Michigan Review\" and the \"Gargoyle Humor Magazine\".\nWCBN-FM (88.3 FM) is the student-run college radio station which plays in freeform format. WOLV-TV is the student-run television station that is primarily shown on the university's cable television system. WJJX was previously the school's student-run radio station. A carrier current station, it was launched in 1953.\nSafety.\nViolent crime is rare on campus, though there have been a few notorious cases, including Theodore Kaczynski's attempted murder of professor James V. McConnell and research assistant Nicklaus Suino in 1985. In 2022, David DePape, the man convicted of attacking Paul Pelosi, who is the husband of former U.S. House Speaker Nancy Pelosi, targeted Gayle Rubin, an associate professor of anthropology and women\u2019s studies at the university. DePape testified during his trial that he hoped to use Nancy and Paul Pelosi in an effort to get to Gayle Rubin.\nIn 2014, the University of Michigan was named one of 55 higher education institutions under investigation by the Office of Civil Rights \"for possible violations of federal law over the handling of sexual violence and harassment complaints.\" President Barack Obama's White House Task Force to Protect Students from Sexual Assault was organized for such investigations. Seven years later, in 2021, the university attracted national attention when a report commissioned by the university was released that detailed an investigation into sexual assault allegations against doctor Robert Anderson who reportedly abused at least 950 university students, many of whom were athletes, from 1966 to 2003. Several football players from that time say football coach Bo Schembechler ignored and enabled the abuse and told players to \"toughen up\" after being molested. Schembechler reportedly punched his then 10-year-old son Matthew after he reported abuse by Anderson. Following the exposure of a similar history of abuse at Ohio State University, male survivors of both Anderson at Michigan and Strauss at Ohio State spoke out to combat sexual abuse. The University of Michigan settled with the survivors for $490 million.\nAthletics.\nThe university has 27 varsity intercollegiate sports, including 13 men's teams and 14 women's teams. Its intercollegiate sports teams participate in the Big Ten Conference in most sports, with the exception of the women's water polo team, which competes in the Collegiate Water Polo Association. The teams compete at the NCAA Division I level in all sports, including Division I FBS in football.\nThe teams share the nickname \"Wolverines\" with several other collegiate athletic teams in the country, such as the Utah Valley Wolverines, the Grove City Wolverines, and the Morris Brown Wolverines.\nHistory.\nThe university's athletics history dates back to the mid 19th century, with the baseball team founded in 1866, the football team established in 1879, and the men's tennis team originating in 1893.\nIn 1896, the university became a founding member of the Intercollegiate Conference of Faculty Representatives, which later evolved into the Western Conference (1896\u20131899) and eventually became known as the Big Ten Conference (since 1950).\nIn 1905, the university found itself at the center of a national controversy regarding violence and professionalism in college football, which sparked discussions about potentially banning the sport from college campuses. That fall, Stanford President David Starr Jordan wrote a series of articles in Collier's, accusing various universities, including Michigan and its rival Chicago, of engaging in \"professionalism.\" He labeled coach Fielding Yost as the \"czar of Michigan's system\" and claimed he was traveling across the country \"soliciting expert players\" who were not true student athletes. In response, Michigan President James Burrill Angell called for a reform conference on football and appointed Albert Pattengill to represent Michigan at the event.\nThe 1906 Angell Conference in Chicago led to several reform resolutions, including placing faculty in charge of gate receipts, banning summer training and the \"training table,\" and capping admission prices for college athletic events at fifty cents. A key resolution targeted Michigan's highly paid football coach, Fielding Yost, by prohibiting the hiring of professional coaches. Michigan was expelled from the Big Nine Conference in April 1907 for noncompliance, partly due to the Yost Resolution. The university rejoined the conference in 1917 after a nine-year absence.\nIn 1926, Harvard made an agreement to play football against Michigan, dropping Princeton from its schedule due to past rough matches. Princeton perceived this move as a threat to the 'Big Three' relationship, fearing it would lose its status as a rival to Harvard and be relegated to a secondary class. By the 1930s, the 'Big Three' was restored and expanded into the Ivy League in 1939.\nIn 2023, during the NCAA's investigation into sign-stealing allegations against the football team's staff members, the university's board of regents considered the possibility of leaving the Big Ten conference due to dissatisfaction with the conference's handling of the investigation.\nVenues.\nThe Ray Fisher Stadium, constructed in 1923, serves as the home venue for the baseball team. The Alumni Field at Carol Hutchins Stadium, formerly known as the Varsity Diamond, is the home field for the university's softball team. The Yost Ice Arena, opened in 1923, is the home arena for the men's ice hockey team. The Crisler Center, opened in 1967 and previously known as the University Events Building and Crisler Arena, serves as the home venue for the men's and women's basketball teams as well as the women's gymnastics team. The Phyllis Ocker Field, constructed in 1995 and built partially on the site of Regents Field, is the home venue for the university's field hockey teams.\nMichigan Stadium is the largest stadium in the United States, as well as the Western Hemisphere, and ranks third globally. The extra seat in the stadium's capacity is said to be \"reserved\" for former head coach Fritz Crisler. Prior to the construction of Michigan Stadium in 1927, the football team played their home games at Regents Field. In 1902, Dexter M. Ferry donated land adjacent to Regents Field, and the entire complex was renamed Ferry Field. Ferry Field served as the home stadium for the football team until the opening of Michigan Stadium. Today, Ferry Field serves as a tailgating space for Michigan Stadium during football games.\nFight songs and chants.\nThe Michigan fight song, \"The Victors\", was written by student Louis Elbel in 1898. The song was declared by John Philip Sousa to be \"the greatest college fight song ever written.\" The song refers to the teams as being \"the Champions of the West\". At the time, the Big Ten Conference was known as the Western Conference.\nAlthough mainly used at sporting events, the Michigan fight song is often heard at other events as well. U.S. President Gerald Ford had it played by the United States Marine Band as his entrance anthem during his term as president from 1974 to 1977, in preference over the more traditional \"Hail to the Chief\", and the Michigan Marching Band performed a slow-tempo variation of the fight song at his funeral. The fight song is also sung during graduation commencement ceremonies. The university's alma mater song is \"The Yellow and Blue\". A common rally cry is \"Let's Go Blue!\" which has a complementary short musical arrangement written by former students Joseph Carl, a sousaphonist, and Albert Ahronheim, a drum major.\nBefore \"The Victors\" was officially the university's fight song, the song \"There'll Be a Hot Time in the Old Town Tonight\" was considered to be the school song. After Michigan temporarily withdrew from the Western Conference in 1907, a new Michigan fight song, \"Varsity\", was written in 1911 because the line \"champions of the West\" was no longer appropriate.\nAccomplishments.\nThe Michigan football program ranks first in NCAA history in total wins (1,004 through the end of the 2023 season) and tied for 1st among FBS schools in winning percentage (.734). The team won the first Rose Bowl game in 1902. the university had 40 consecutive winning seasons from 1968 to 2007, including consecutive bowl game appearances from 1975 to 2007. The Wolverines have won a record 45 Big Ten championships. The program claims 12 national championships, most recently winning the 2024 National Championship Game, and has produced three Heisman Trophy winners: Tom Harmon (1940), Desmond Howard (1991), and Charles Woodson (1997). In 2025, the university made history by becoming the first institution in intercollegiate sports to have first-round draft picks in all five major professional sports leagues (NFL, NBA, NHL, MLB, and MLS) within the same year. Through the 2025 NFL draft, 421 Michigan football players have been selected into the NFL, with at least one every year since the inaugural draft.\nThe men's ice hockey team, which plays at Yost Ice Arena, has won nine national championships.\nThe men's basketball team, which plays at the Crisler Center, has appeared in five Final Fours and won the national championship in 1989. The program also voluntarily vacated victories from its 1992\u20131993 and 1995\u20131999 seasons in which illicit payments to players took place, as well as its 1992 and 1993 Final Four appearances. The men's basketball team has most recently won back-to-back Big Ten Tournament Championships.\nMore than 250 Michigan athletes or coaches have participated in Olympic events, and as of 2021 its students and alumni have won 155 Olympic medals. Through the 2012 Summer Olympics, 275 Michigan students and coaches had participated in the Olympics, winning medals in each Summer Olympic Games except 1896, and winning gold medals in all but four Olympiads. the university's students/student-coaches (e.g., notably, Michael Phelps) have won a total of 185 Olympic medals: 85 golds, 48 silvers, and 52 bronzes.\nIn 10 of the past 14 years concluding in 2009, the university has finished in the top five of the NACDA Director's Cup, a ranking compiled by the National Association of Collegiate Directors of Athletics to tabulate the success of universities in competitive sports. The university has finished in the top 10 of the Directors' Cup standings in 21 of the award's 29 seasons between 1993\u20132021 and has placed in the top six in nine of the last 10 seasons.\nNotable people.\nBenefactors.\nThe university was assisted in its formation in the 1810s by a Masonic lodge (Zion Lodge No. 1 F&amp;AM), which provided much-needed financial support for its establishment. Of the total amount subscribed to start the university, two-thirds came from the Masonic lodge and its members. Since then, private donors have become an important source of funding for the university. Among the individuals who have made significant donations commemorated at the university are William Wilson Cook, Dexter Mason Ferry, the Ford family, the Nichols family, the Marsal Family, the Tisch Family, William Erastus Upjohn, John Stoughton Newberry, Clara Harrison Stranahan, William K. Brehm, William Morse Davidson, A. Alfred Taubman, Penny W. Stamps, and Ronald Weiser. The Zell Family Foundation, led by Sam and Helen Zell, has donated a total of $152 million to the university over the years. Stephen M. Ross made a $200 million donation to the business school and athletic campus in 2013. Ross made a separate $100 million contribution to the university in 2004. Charles Munger pledged $110 million in 2013 for a graduate residence and fellowships. In 2024, the Hong Kong-based Li Ka Shing Foundation endowed a professorship at the university to support research in biomedical engineering.\nFaculty.\nThe university employs 8,189 faculty members, of whom 3,195 are tenured or on a tenure track. Among them, there are 37 members of the National Academy of Sciences, 62 members of the National Academy of Medicine, 30 members of the National Academy of Engineering, 89 Sloan Research Fellows, 17 Guggenheim Fellows, 99 members of the American Academy of Arts and Sciences, and 17 members of the American Philosophical Society. The university's current and former faculty includes fourteen Nobel laureates, eight Pulitzer Prize winners, three David M. Holland Medal winners, and one John Bates Clark Medal recipient.\nCurrent faculty include physicists Mark Newman, Duncan G. Steel, Steven Cundiff, Stephen Forrest, and Gordon Kane; mathematicians Hyman Bass, Sergey Fomin, William Fulton, Robert Griess, and Melvin Hochster; chemist Melanie Sanford; Pulitzer Prize-winning historian Heather Ann Thompson; National Medal of Science recipients Huda Akil and Robert Axelrod; biostatistician Gon\u00e7alo Abecasis; philosophers Elizabeth S. Anderson, Allan Gibbard, and Peter Railton; and social psychologist Richard E. Nisbett. The faculty also includes feminist legal theorist Catharine MacKinnon, \"Strict Scrutiny\" co-host Leah Litman, engineer James P. Bagian, and A. Galip Ulsoy, co-inventor of the Reconfigurable Manufacturing System.\nPhilosophers who were members of the faculty include pragmatists John Dewey, Charles Horton Cooley, and George H. Mead, along with analytic philosophers William Frankena and Cooper Harold Langford. The faculty included notable writers such as Nobel Prize-winning essayist Joseph Brodsky, Pulitzer Prize-winning poet W. H. Auden, and Robert Frost, the only poet to receive four Pulitzer Prizes for Poetry.\nNotable faculty in physics have included Donald A. Glaser, the inventor of the bubble chamber; Samuel Goudsmit and George Uhlenbeck, discoverers of electron spin; Juris Upatnieks and Emmett Leith, inventors of 3D holography; G\u00e9rard Mourou, inventor of chirped pulse amplification; and H. Richard Crane, who was called \"one of the most distinguished experimental physicists of the 20th century\". Wolfgang Pauli, a pioneer of quantum physics, served as a visiting professor at the university in 1931 and again in 1941. Physicists Martin Lewis Perl and Lawrence W. Jones served as co-advisors to Nobel laureate Samuel C. C. Ting at the university. Perl himself was also awarded the Nobel Prize in Physics in 1995 for his discovery of the tau lepton. Other distinguished physicists who have served on the faculty include Martinus Veltman, Carl Wieman, and Charles H. Townes. Notable mathematicians Raoul Bott, Richard Brauer, Samuel Eilenberg (co-founder of category theory), Frederick Gehring, Herman Goldstine, and Anatol Rapoport have all served on the faculty.\nIn medicine, notable individuals who have served on the faculty include the former director of the National Institutes of Health Francis Collins, the developer of the polio vaccine Jonas Salk, Nobel Prize\u2013winning physiologist Charles B. Huggins, co-discoverer of tumour-inducing viruses Peyton Rous, geneticist James V. Neel, neuroanatomist Elizabeth C. Crosby, and co-discoverer of restriction enzymes Hamilton O. Smith.\nPast faculty have also included \"the founding father of 21st-century sociology\" Charles Tilly, social psychologist Robert Zajonc, chemical engineer Donald L. Katz, Supreme Court justice Henry Billings Brown, Pulitzer Prize-winning composer Leslie Bassett, Pulitzer Prize-winning photographer David C. Turnley, Nobel Prize-winning economist Lawrence R. Klein, and John Bates Clark Medal recipient Kenneth E. Boulding.\nAlumni.\nMichigan alumni include nine Nobel laureates, two Abel Prize winners (Isadore M. Singer and Karen Uhlenbeck), two Fields Medalists (June Huh and Stephen Smale), five Turing Award winners, and 35 Pulitzer Prize winners. By alumni count, Michigan ranks fifth as of 2018,[ [update]] among all universities whose alumni have won Pulitzers.\nMathematics and sciences.\nClaude Shannon, who laid the foundations of the Information Age, ranks among the most distinguished mathematicians from the university. Two Fields medalists Stephen Smale and June Huh, both completed their Ph.D.s in Mathematics at Michigan. Isadore Singer, the Abel Prize-winning mathematician who helped prove the Atiyah\u2013Singer index theorem, studied physics at the university during World War II. Karen Uhlenbeck, the first woman to win the Abel Prize, received her bachelor's degree from the university in 1964. George Dantzig, who developed linear programming, studied at Michigan under G.Y. Rainich, R.L. Wilder, and T.H. Hildebrandt. Other mathematicians from the university include Kenneth Ira Appel, who, along with Wolfgang Haken, solved the famous Four Color Theorem; Leonard Jimmie Savage, who was known for his contributions to Bayesian statistics and decision theory; and Carl R. de Boor, a renowned mathematician in numerical analysis.\nIn physics, Nobel laureate Samuel C. C. Ting, who discovered the J/\u03c8 particle, studied under Martin Lewis Perl, another Nobel-winning physicist who discovered the tau lepton, at the university. Chemist Jerome Karle, who revealed molecular structures, completed his Ph.D. in Physics at Michigan in 1943. His wife, Isabella Karle, an alumna, developed techniques to extract plutonium chloride from plutonium oxide mixtures. Other alumni include nuclear physicist Robert Bacher, a leader of the Manhattan Project; Richard Smalley, who discovered fullerenes; Moses Gomberg, a pioneer in radical chemistry, who later became a professor at the university and taught Frank Spedding, who led the development of the Ames process in the Manhattan Project.\nAlumni in biology and medicine include Marshall Warren Nirenberg, famous for breaking the genetic code, who received his Ph.D. in biochemistry from the university in 1957; Stanley Cohen, who discovered growth factors; and John Jacob Abel, regarded as the father of pharmacology, who studied under the university's physiologist Henry Sewall in 1883. Other alumni include Raymond Pearl, a founder of biogerontology, David Botstein, a leader of the Human Genome Project, and Nancy Ascher, the first woman to perform a liver transplant.\nOther notable alumni in science include Edgar F. Codd, who developed the relational model of data and completed his Ph.D. at Michigan, and computer scientist Michael Stonebraker, who also made contributions to database research. Both Codd and Stonebraker are Turing Award winners.\nLaw and government.\nThe university boasts several holders or candidates of the United States presidency, including Gerald Ford, the 38th president and the Republican Party's nominee for president in 1976; Thomas E. Dewey, who was the Republican Party's nominee for president in both 1944 and 1948; Arthur LeSueur, a Socialist candidate for president in 1916; Gilbert Hitchcock, a Democratic candidate in 1928; Arthur Vandenberg, a Republican presidential hopeful in 1948; Ben Carson, a Republican candidate in 2016; and Larry Elder, a Republican candidate in 2024. John Worth Kern and Burton K. Wheeler both ran for the vice presidency, with Kern representing the Democratic Party alongside William Jennings Bryan in 1908, and Wheeler as a Progressive Party's nominee with Robert La Follette Sr. in 1924.\nAmong the 23 former governors of Michigan who hold formal college degrees, 10 are graduates of the university. (Woodbridge N. Ferris only attended for a year) As of 2021, the university has matriculated 63 U.S. governors or lieutenant governors, including former Governor of Michigan Rick Snyder, first female lieutenant governor of Missouri Harriett Woods, and former Governor of California Culbert Olson. More than 250 Michigan graduates have served as legislators as either a United States Senator (48 graduates) or as a Congressional representative (over 215 graduates), including former House Majority Leader Dick Gephardt, U.S. Representative Justin Amash. Former Los Angeles Mayor Richard Riordan, former Chicago Mayor Lori Lightfoot, and Detroit Mayor Mike Duggan are also Michigan graduates.\nMichigan graduates have held a range of cabinet-level positions, including United States Secretary of State (William Rufus Day); United States Secretary of the Treasury (George M. Humphrey); United States Attorney General (Harry Micajah Daugherty); United States Secretary of the Interior (Kenneth Lee Salazar); United States Secretary of Agriculture (Clinton Anderson, Julius Sterling Morton, Arthur M. Hyde, and Dan Glickman); United States Secretary of Commerce (Roy D. Chapin and Robert P. Lamont); United States Secretary of Health and Human Services (Tom Price); and Director of the United States Office of Management and Budget (Rob Portman). Multiple alumni served in the judicial branch of the U.S. government, including William Rufus Day, Frank Murphy, and George Sutherland, all of whom served as Supreme Court justices. As of 2019, the university has placed onto various State Supreme Courts over 125 graduates, 40 of whom served as Chief Justice.\nForeign alumni include the Prime Minister of Singapore (Lawrence Wong); the current ruler of the Emirate of Ras Al Khaimah (Saud bin Saqr Al Qasimi); the 51st Prime Minister of Italy (Lamberto Dini); the Prime Minister of Antigua and Barbuda 1994\u20132004 (Lester Bird); the 47th President of Costa Rica (Luis Guillermo Sol\u00eds); the Prime Minister of Peru 1993\u20131994 (Alfonso Bustamante); the Prime Minister of Jordan 2012\u20132016 (Abdullah Ensour); the 13th President of Pakistan (Arif Alvi); Chief Secretary of Hong Kong 2007\u20132011 (Henry Tang Ying-yen); Deputy Prime Minister of South Korea 2017\u20132018 (Kim Dong-yeon); Deputy Prime Minister of Bulgaria in the government of Boyko Borisov (Simeon Djankov); Deputy Prime Minister of Madagascar 1997\u20131998 (Herizo Razafimahaleo). British Members of Parliament Terry Davis and Howard Flight are also Michigan graduates. As of 2022, Michigan has matriculated 64 Ambassadors who served as Ambassador in more than 72 countries.\nBusiness and finance.\nMichigan alumni have founded or cofounded many prominent companies, such as Alphabet Inc. (Larry Page), Johnson &amp; Johnson (Edward Mead Johnson), Abbott Laboratories (Wallace Calvin Abbott), Stryker Corporation (Homer Stryker), Emerson Electric Company (John Wesley Emerson), Loews Corporation (Preston Robert Tisch), Devon Energy (J. Larry Nichols), Merrill Lynch (Charles Edward Merrill), Leidos as SAIC (J. Robert Beyster), Rocket Mortgage (Gary Gilbert), Twilio (Jeff Lawson, Evan Cooke, John Wolthuis), Domino's (Tom Monaghan), H&amp;R Block (Henry W. Bloch), Related Companies (Stephen M. Ross), Admiral Group (Henry Engelhardt), Akamai Technologies (Randall Kaplan), and Five Guys (Jerry Murrell)\nThe university counts several patriarchs of influential business dynasties, including George Getty of the renowned Getty family. The university also boasts a number of graduates from affluent families, including heirs and heiresses to major fortunes, such as Josiah K. Lilly Jr. (heir to Eli Lilly and Company); Charles Rudolph Walgreen Jr. (heir to Walgreens); John Gideon Searle (heir to G. D. Searle); Doug Meijer and Hank Meijer (heirs to Meijer); Christopher Ilitch (heir to Ilitch Holdings, Inc.); and Kenneth B. Dart (heir to Dart Container Corporation). Raoul Wallenberg, a member of the prominent Wallenberg family, one of the wealthiest families in the world, studied at the university in 1931.\nAs of May 2024, about 2.8% of all Fortune 1000 executives with MBAs are alumni from Michigan Ross, ranking it as the 6th highest among all business schools in the United States. Alumni have led several companies, including Berkshire Hathaway (Charlie Munger), Ford (James Hackett), General Motors (Roger Smith, Frederick Henderson, and Richard C. Gerstenberg), State Farm Insurance (Jon Farney), Citigroup (John C. Dugan), Tencent (Martin Lau), The Boeing Company (Edgar Gott), Wells Fargo (Timothy J. Sloan), Allstate Corp. (Thomas J. Wilson), American Airlines (Robert Isom), PNC Financial Services (William S. Demchak), General Mills (Stephen Sanger), Turkish Airlines (Temel Kotil), International Paper (John V. Faraci), KB Financial Group (Euh Yoon-dae), Chrysler Group LLC (C. Robert Kidder), BorgWarner Inc. (Timothy M. Manganello), Bunzl (Michael Roney), Celanese (David N. Weidman), JetBlue (Dave Barger), Restaurant Brands International (J. Patrick Doyle), and Bain Capital (Edward Conard).\nEngineering and technology.\nMany alumni have made significant contributions to the fields of engineering and technology, including Lockheed Martin engineer Clarence \"Kelly\" Johnson and Joseph Francis Shea, a key figure in the Apollo program, earned his Ph.D. in 1955. The university produced numerous developers and original authors of widely recognized software programs, such as Thomas Knoll (original author of Adobe Photoshop); Mike Engelhardt (original author of LTspice); Niels Provos (creator of Bcrypt); and Sid Meier (creator of video games series Civilization).\nSeveral astronauts attended Michigan including the all-Michigan crews of both Gemini 4, and Apollo 15. The university claims the only alumni association with a chapter on the Moon, established in 1971 when the crew of Apollo 15 placed a charter plaque for a new University of Michigan Alumni Association on the lunar surface.\nSports.\nFamous athletes who attended the university include professional football quarterback Tom Brady, and Olympic swimmer Michael Phelps. National Hockey League players Zach Hyman, Brendan Morrison, and Michael Cammalleri all played for the university's ice hockey team. National Baseball Hall of Famers George Sisler and Barry Larkin also played baseball at the university.\nMusic and theatre.\nMusical graduates include operatic soprano Jessye Norman, singer Joe Dassin, multiple members of the bands Tally Hall and Vulfpeck, jazz guitarist Randy Napoleon, and Mannheim Steamroller founder Chip Davis. Well-known composers who are alumni include Frank Ticheli, Andrew Lippa, and the Oscar and Tony Award-winning duo Benj Pasek and Justin Paul. Pop superstar Madonna and rock legend Iggy Pop attended but did not graduate.\nIn Hollywood, famous alumni include actors Michael Dunn, Darren Criss, James Earl Jones, and David Alan Grier; actresses Lucy Liu, Gilda Radner, and Selma Blair as well as television director Mark Cendrowski and filmmaker Lawrence Kasdan. Many Broadway and musical theatre actors, including Gavin Creel, Andrew Keenan-Bolger, his sister Celia Keenan-Bolger, and Taylor Louderman attended the university for musical theatre. Emmy Award winner Sanjay Gupta attended both college and medical school at the university.\nLiterature.\nNotable writers who attended the university include playwright Arthur Miller, essayists Susan Orlean, Jia Tolentino, Sven Birkerts, journalists and editors Mike Wallace, Jonathan Chait of \"The New Republic\", Indian author and columnist Anees Jung, Daniel Okrent, and Sandra Steingraber, food critics Ruth Reichl and Gael Greene, novelists Brett Ellen Block, Elizabeth Kostova, Marge Piercy, Brad Meltzer, Betty Smith, and Charles Major, screenwriter Judith Guest, Pulitzer Prize-winning poet Theodore Roethke, National Book Award winners Keith Waldrop and Jesmyn Ward, composer/author/puppeteer Forman Brown, Alireza Jafarzadeh (a Middle East analyst, author, and TV commentator), and memoirist and self-help book author Jerry Newport.\nOther notable alumni.\nActivists associated with the university include Weather Underground leader Bill Ayers, activist Tom Hayden, assisted-suicide advocate Jack Kevorkian. Notable criminals include the Unabomber Ted Kaczynski, America\u2019s first serial Killer H.H. Holmes, mass murderer John Emil List, convicted sex offender Larry Nassar, and kidnappers and murderers of 14-year old Bobby Franks, Nathan F. Leopold, Jr. and Richard Albert Loeb.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\nCitations.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nBibliography.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;\nExternal links.\n&lt;templatestyles src=\"Sister-inline/styles.css\"/&gt; Texts on Wikisource:"}
{"id": "31741", "revid": "35897031", "url": "https://en.wikipedia.org/wiki?curid=31741", "title": "Unemployment", "text": "People without work and actively seeking work\nUnemployment, according to the OECD (Organisation for Economic Co-operation and Development), is the proportion of people above a specified age (usually 15) not being in paid employment or self-employment but currently available for work during the reference period.\nUnemployment is measured by the unemployment rate, which is the number of people who are unemployed as a percentage of the labour force (the total number of people employed added to those unemployed).\nUnemployment can have many sources, such as the following below:\nUnemployment and the status of the economy can be influenced by a country through, for example, fiscal policy. Furthermore, the monetary authority of a country, such as the central bank, can influence the availability and cost for money through its monetary policy.\nIn addition to theories of unemployment, a few categorisations of unemployment are used for more precisely modelling the effects of unemployment within the economic system. Some of the main types of unemployment include structural unemployment, frictional unemployment, cyclical unemployment, involuntary unemployment and classical unemployment. Structural unemployment focuses on foundational problems in the economy and inefficiencies inherent in labor markets, including a mismatch between the supply and demand of laborers with necessary skill sets. Structural arguments emphasize causes and solutions related to disruptive technologies and globalization. Discussions of frictional unemployment focus on voluntary decisions to work based on individuals' valuation of their own work and how that compares to current wage rates added to the time and effort required to find a job. Causes and solutions for frictional unemployment often address job entry threshold and wage rates.\nAccording to the UN's International Labour Organization (ILO), there were 172 million people worldwide (or 5% of the reported global workforce) without work in 2018.\nBecause of the difficulty in measuring the unemployment rate by, for example, using surveys (as in the United States) or through registered unemployed citizens (as in some European countries), statistical figures such as the employment-to-population ratio might be more suitable for evaluating the status of the workforce and the economy if they were based on people who are registered, for example, as taxpayers.\nDefinitions, types, and theories.\nThe state of being without any work yet looking for work is called unemployment. Economists distinguish between various overlapping types of and theories of unemployment, including cyclical or Keynesian unemployment, frictional unemployment, structural unemployment and classical unemployment definition. Some additional types of unemployment that are occasionally mentioned are seasonal unemployment, hardcore unemployment, and hidden unemployment.\nThough there have been several definitions of \"voluntary\" and \"involuntary unemployment\" in the economics literature, a simple distinction is often applied. Voluntary unemployment is attributed to the individual's decisions, but involuntary unemployment exists because of the socio-economic environment (including the market structure, government intervention, and the level of aggregate demand) in which individuals operate. In these terms, much or most of frictional unemployment is voluntary since it reflects individual search behavior. Voluntary unemployment includes workers who reject low-wage jobs, but involuntary unemployment includes workers fired because of an economic crisis, industrial decline, company bankruptcy, or organizational restructuring.\nOn the other hand, cyclical unemployment, structural unemployment, and classical unemployment are largely involuntary in nature. However, the existence of structural unemployment may reflect choices made by the unemployed in the past, and classical (natural) unemployment may result from the legislative and economic choices made by labour unions or political parties.\nThe clearest cases of involuntary unemployment are those with fewer job vacancies than unemployed workers even when wages are allowed to adjust and so even if all vacancies were to be filled, some unemployed workers would still remain. That happens with cyclical unemployment, as macroeconomic forces cause microeconomic unemployment, which can boomerang back and exacerbate those macroeconomic forces.\nReal wage unemployment.\nClassical, natural, or real-wage unemployment, occurs when real wages for a job are set above the market-clearing level, causing the number of job-seekers to exceed the number of vacancies. On the other hand, most economists argue that as wages fall below a livable wage, many choose to drop out of the labour market and no longer seek employment. That is especially true in countries in which low-income families are supported through public welfare systems. In such cases, wages would have to be high enough to motivate people to choose employment over what they receive through public welfare. Wages below a livable wage are likely to result in lower labor market participation in the above-stated scenario. In addition, consumption of goods and services is the primary driver of increased demand for labor. Higher wages lead to workers having more income available to consume goods and services. Therefore, higher wages increase general consumption and as a result demand for labor increases and unemployment decreases.\nMany economists have argued that unemployment increases with increased governmental regulation. For example, minimum wage laws raise the cost of some low-skill laborers above market equilibrium, resulting in increased unemployment as people who wish to work at the going rate cannot (as the new and higher enforced wage is now greater than the value of their labour). Laws restricting layoffs may make businesses less likely to hire in the first place, as hiring becomes more risky.\nHowever, that argument overly simplifies the relationship between wage rates and unemployment by ignoring numerous factors that contribute to unemployment. Some, such as Murray Rothbard, suggest that even social taboos can prevent wages from falling to the market-clearing level.\nIn \"Out of Work: Unemployment and Government in the Twentieth-Century America\", economists Richard Vedder and Lowell Gallaway argue that the empirical record of wages rates, productivity, and unemployment in America validates classical unemployment theory. Their data shows a strong correlation between adjusted real wage and unemployment in the United States from 1900 to 1990. However, they maintain that their data does not take into account exogenous events.\nCyclical unemployment.\nCyclical, deficient-demand, or Keynesian unemployment occurs when there is not enough aggregate demand in the economy to provide jobs for everyone who wants to work. Demand for most goods and services falls, less production is needed and consequently, fewer workers are needed, wages are sticky and do not fall to meet the equilibrium level, and unemployment results. Its name is derived from the frequent ups and downs in the business cycle, but unemployment can also be persistent, such as during the Great Depression.\nWith cyclical unemployment, the number of unemployed workers exceeds the number of job vacancies and so even if all open jobs were filled, some workers would still remain unemployed. Some associate cyclical unemployment with frictional unemployment because the factors that cause the friction are partially caused by cyclical variables. For example, a surprise decrease in the money supply may suddenly inhibit aggregate demand and thus inhibit labor demand.\nKeynesian economists, on the other hand, see the lack of supply of jobs as potentially resolvable by government intervention. One suggested intervention involves deficit spending to boost employment and goods demand. Another intervention involves an expansionary monetary policy to increase the supply of money, which should reduce interest rates, which, in turn, should lead to an increase in non-governmental spending.\nFull employment.\nIn demands based theory, it is possible to abolish cyclical unemployment by increasing the aggregate demand for products and workers. However, the economy eventually hits an \"inflation barrier\" that is imposed by the four other kinds of unemployment to the extent that they exist. Historical experience suggests that low unemployment affects inflation in the short term but not the long term. In the long term, the velocity of money supply measures such as the MZM (\"money zero maturity\", representing cash and equivalent demand deposits) velocity is far more predictive of inflation than low unemployment.\nSome demand theory economists see the inflation barrier as corresponding to the natural rate of unemployment. The \"natural\" rate of unemployment is defined as the rate of unemployment that exists when the labour market is in equilibrium, and there is pressure for neither rising inflation rates nor falling inflation rates. An alternative technical term for that rate is the NAIRU, the \"Non-Accelerating Inflation Rate of Unemployment\". Whatever its name, demand theory holds that if the unemployment rate gets \"too low\", inflation will accelerate in the absence of wage and price controls (incomes policies).\nOne of the major problems with the NAIRU theory is that no one knows exactly what the NAIRU is, and it clearly changes over time. The margin of error can be quite high relative to the actual unemployment rate, making it hard to use the NAIRU in policy-making.\nAnother, normative, definition of full employment might be called the \"ideal\" unemployment rate. It would exclude all types of unemployment that represent forms of inefficiency. This type of \"full employment\" unemployment would correspond to only frictional unemployment (excluding that part encouraging the McJobs management strategy) and so would be very low. However, it would be impossible to attain this full-employment target using only demand-side Keynesian stimulus without getting below the NAIRU and causing accelerating inflation (absent incomes policies). Training programs aimed at fighting structural unemployment would help here.\nTo the extent that hidden unemployment exists, it implies that official unemployment statistics provide a poor guide to what unemployment rate coincides with \"full employment\".\nStructural unemployment.\nStructural unemployment occurs when a labour market is unable to provide jobs for everyone who wants one because there is a mismatch between the skills of the unemployed workers and the skills needed for the available jobs. Structural unemployment is hard to separate empirically from frictional unemployment except that it lasts longer. As with frictional unemployment, simple demand-side stimulus will not work to abolish this type of unemployment easily.\nStructural unemployment may also be encouraged to rise by persistent cyclical unemployment: if an economy suffers from longlasting low aggregate demand, it means that many of the unemployed become disheartened, and their skills (including job-searching skills) become \"rusty\" and obsolete. Problems with debt may lead to homelessness and a fall into the vicious cycle of poverty, which means that people affected in this way may not fit the job vacancies that are created when the economy recovers. The implication is that sustained \"high\" demand may \"lower\" structural unemployment. This theory of persistence in structural unemployment has been referred to as an example of path dependence or \"hysteresis\".\nMuch \"technological unemployment\", caused by the replacement of workers by machines might be counted as structural unemployment. Alternatively, technological unemployment might refer to the way in which steady increases in labour productivity mean that fewer workers are needed to produce the same level of output every year. The fact that aggregate demand can be raised to deal with the problem suggests that the problem is instead one of cyclical unemployment. As indicated by Okun's law, the demand side must grow sufficiently quickly to absorb not only the growing labour force but also the workers who are made redundant by the increased labour productivity.\nSeasonal unemployment may be seen as a kind of structural unemployment since it is linked to certain kinds of jobs (construction and migratory farm work). The most-cited official unemployment measures erase this kind of unemployment from the statistics using \"seasonal adjustment\" techniques. That results in substantial and permanent structural unemployment.\nFrictional unemployment.\nFrictional unemployment is the time period between jobs in which a worker searches for or transitions from one job to another. It is sometimes called \"search unemployment\" and can be voluntary, based on the circumstances of the unemployed individual. Frictional unemployment exists because both jobs and workers are heterogeneous, and a mismatch can result between the characteristics of supply and demand. Such a mismatch can be related to skills, payment, work-time, location, seasonal industries, attitude, taste, and a multitude of other factors. New entrants (such as graduating students) and re-entrants (such as former homemakers) can also suffer a spell of frictional unemployment.\nWorkers and employers accept a certain level of imperfection, risk or compromise, but usually not right away. They will invest some time and effort to find a better match. That is, in fact, beneficial to the economy since it results in a better allocation of resources. However, if the search takes too long and mismatches are too frequent, the economy suffers since some work will not get done. Therefore, governments will seek ways to reduce unnecessary frictional unemployment by multiple means including providing education, advice, training, and assistance such as daycare centers.\nThe frictions in the labour market are sometimes illustrated graphically with a Beveridge curve, a downward-sloping, convex curve that shows a correlation between the unemployment rate on one axis and the vacancy rate on the other. Changes in the supply of or demand for labour cause movements along the curve. An increase or decrease in labour market frictions will shift the curve outwards or inwards.\nHidden unemployment.\nOfficial statistics often underestimate unemployment rates because of hidden, or covered, unemployment. That is the unemployment of potential workers that are not reflected in official unemployment statistics because of how the statistics are collected. In many countries, only those who have no work but are actively looking for work or qualifying for social security benefits are counted as unemployed. Those who have given up looking for work and sometimes those who are on government \"retraining\" programs are not officially counted among the unemployed even though they are not employed.\nThe statistic also does not count the \"underemployed\", those working fewer hours than they would prefer or in a job that fails to make good use of their capabilities. In addition, those who are of working age but are currently in full-time education are usually not considered unemployed in government statistics. Traditional unemployed native societies who survive by gathering, hunting, herding, and farming in wilderness areas may or may not be counted in unemployment statistics.\nLong-term unemployment.\nLong-term unemployment (LTU) is defined in European Union statistics as unemployment lasting for longer than one year (while unemployment lasting over two years is defined as \"very long-term unemployment\"). The United States Bureau of Labor Statistics (BLS), which reports current long-term unemployment rate at 1.9 percent, defines this as unemployment lasting 27 weeks or longer. Long-term unemployment is a component of structural unemployment, which results in long-term unemployment existing in every social group, industry, occupation, and all levels of education.\nIn 2015 the European Commission published recommendations on how to reduce long-term unemployment. These advised governments to:\nIn 2017\u20132019 it implemented the Long-Term Unemployment project to research solutions implemented by EU member states and produce a toolkit to guide government action. Progress was evaluated in 2019.\nMarxian theory of unemployment.\nMarxists share the Keynesian viewpoint of the relationship between economic demand and employment, but with the caveat that the market system's propensity to slash wages and reduce labor participation on an enterprise level causes a requisite decrease in aggregate demand in the economy as a whole, causing crises of unemployment and periods of low economic activity before the capital accumulation (investment) phase of economic growth can continue. According to Karl Marx, unemployment is inherent within the unstable capitalist system and periodic crises of mass unemployment are to be expected. He theorized that unemployment was inevitable and even a necessary part of the capitalist system, with recovery and regrowth also part of the process. The function of the proletariat within the capitalist system is to provide a \"reserve army of labour\" that creates downward pressure on wages. This is accomplished by dividing the proletariat into surplus labour (employees) and under-employment (unemployed). This reserve army of labour fight among themselves for scarce jobs at lower and lower wages. At first glance, unemployment seems inefficient since unemployed workers do not increase profits, but unemployment is profitable within the global capitalist system because unemployment lowers wages which are costs from the perspective of the owners. From this perspective low wages benefit the system by reducing economic rents. Yet, it does not benefit workers; according to Karl Marx, the workers (proletariat) work to benefit the bourgeoisie through their production of capital. Capitalist systems unfairly manipulate the market for labour by perpetuating unemployment which lowers laborers' demands for fair wages. Workers are pitted against one another at the service of increasing profits for owners. As a result of the capitalist mode of production, Marx argued that workers experienced alienation and estrangement through their economic identity. According to Marx, the only way to permanently eliminate unemployment would be to abolish capitalism and the system of forced competition for wages and then shift to a socialist or communist economic system. For contemporary Marxists, the existence of persistent unemployment is proof of the inability of capitalism to ensure full employment.\nLabor force participation rate.\nThe labor force participation rate is the ratio between the labor force and the overall size of their cohort (national population of the same age range). In the West, during the latter half of the 20th century, the labor force participation rate increased significantly because of an increase in the number of women entering the workplace.\nIn the United States, there have been four significant stages of women's participation in the labour force: increases in the 20th century and decreases in the 21st century. Male labor force participation decreased from 1953 to 2013. Since October 2013, men have been increasingly joining the labour force.\nFrom the late 19th century to the 1920s, very few women worked outside the home. They were young single women who typically withdrew from the labor force at marriage unless family needed two incomes. Such women worked primarily in the textile manufacturing industry or as domestic workers. That profession empowered women and allowed them to earn a living wage. At times, they were a financial help to their families.\nBetween 1930 and 1950, female labor force participation increased primarily because of the increased demand for office workers, women's participation in the high school movement, and electrification, which reduced the time that was spent on household chores. From the 1950s to the early 1970s, most women were secondary earners working mainly as secretaries, teachers, nurses, and librarians (pink-collar jobs).\nFrom the mid-1970s to the late 1990s, there was a period of revolution of women in the labor force brought on by various factors, many of which arose from the second-wave feminism movement. Women more accurately planned for their future in the work force by investing in more applicable majors in college that prepared them to enter and compete in the labor market. In the United States, the female labor force participation rate rose from approximately 33% in 1948 to a peak of 60.3% in 2000. As of April 2015, the female labor force participation is at 56.6%, the male labor force participation rate is at 69.4%, and the total is 62.8%.\nA common theory in modern economics claims that the rise of women participating in the US labor force in the 1950s to the 1990s was caused by the introduction of a new contraceptive technology, birth control pills, as well as the adjustment of age of majority laws. The use of birth control gave women the flexibility of opting to invest and to advance their career while they maintained a relationship. By having control over the timing of their fertility, they were not running a risk of thwarting their career choices. However, only 40% of the population actually used the birth control pill.\nThat implies that other factors may have contributed to women choosing to invest in advancing their careers. One factor may be that an increasing number of men delayed the age of marriage, which allowed women to marry later in life without them worrying about the quality of older men. Other factors include the changing nature of work, with machines replacing physical labor, thus eliminating many traditional male occupations, and the rise of the service sector in which many jobs are gender neutral.\nAnother factor that may have contributed to the trend was the Equal Pay Act of 1963, which aimed at abolishing wage disparity based on sex. Such legislation diminished sexual discrimination and encouraged more women to enter the labor market by receiving fair remuneration to help raising families and children.\nAt the turn of the 21st century, the labor force participation began to reverse its long period of increase. Reasons for the change include a rising share of older workers, an increase in school enrollment rates among young workers, and a decrease in female labor force participation.\nThe labor force participation rate can decrease when the rate of growth of the population outweighs that of the employed and the unemployed together. The labor force participation rate is a key component in long-term economic growth, almost as important as productivity.\nA historic shift began around the end of the Great Recession as women began leaving the labor force in the United States and other developed countries. The female labor force participation rate in the United States has steadily decreased since 2009, and as of April 2015, the female labor force participation rate has gone back down to 1988 levels of 56.6%.\nParticipation rates are defined as follows:\nThe labor force participation rate explains how an increase in the unemployment rate can occur simultaneously with an increase in employment. If a large number of new workers enter the labor force but only a small fraction become employed, then the increase in the number of unemployed workers can outpace the growth in employment.\nUnemployment-to-population ratio.\nThe unemployment-to-population ratio calculates the share of unemployed for the whole population. This is in contrast to the unemployment rate, which calculates the percentage of unemployed persons in relation to the \"active\" population. Particularly, many young people between 15 and 24 are studying full-time and so are neither working nor looking for a job. That means that they are not part of the labor force, which is used as the denominator when the unemployment rate is calculated.\nYouth and young adult unemployment.\nYouth unemployment refers to unemployment among people aged 15-24, while young adult unemployment refers to unemployment among ages 25-34. Youth unemployment is in many countries higher than general unemployment. Unemployment of young adults was associated with political instability and revolutions.\nMeasurement.\nThere are also different ways national statistical agencies measure unemployment. The differences may limit the validity of international comparisons of unemployment data. To some degree, the differences remain despite national statistical agencies increasingly adopting the definition of unemployment of the International Labour Organization. To facilitate international comparisons, some organizations, such as the OECD, Eurostat, and International Labor Comparisons Program, adjust data on unemployment for comparability across countries.\nThough many people care about the number of unemployed individuals, economists typically focus on the unemployment rate, which corrects for the normal increase in the number of people employed caused by increases in population and increases in the labour force relative to the population. The unemployment rate is expressed as a percentage and calculated as follows:\nformula_1\nAs defined by the International Labour Organization, \"unemployed workers\" are those who are currently not working but are willing and able to work for pay, currently available to work, and have actively searched for work.\nIndividuals who are actively seeking job placement must make the effort to be in contact with an employer, have job interviews, contact job placement agencies, send out resumes, submit applications, respond to advertisements, or some other means of active job searching within the prior four weeks. Simply looking at advertisements and not responding will not count as actively seeking job placement. Since not all unemployment may be \"open\" and counted by government agencies, official statistics on unemployment may not be accurate. In the United States, for example, the unemployment rate does not take into consideration part-time workers, or those individuals who are not actively looking for employment, due to attending college or having tried to find a job and given up.\nAccording to the OECD, Eurostat, and the US Bureau of Labor Statistics the unemployment rate is the number of unemployed people as a percentage of the labour force.\n\"An unemployed person is defined by Eurostat, according to the guidelines of the International Labour Organization, as:\nThe labour force, or workforce, includes both employed (employees and self-employed) and unemployed people but not the economically inactive, such as pre-school children, school children, students and pensioners.\nThe unemployment rate of an individual country is usually calculated and reported on a monthly, quarterly, and yearly basis by the National Agency of Statistics. Organisations like the OECD report statistics for all of its member states.\nCertain countries provide unemployment compensation for a certain period of time for unemployed citizens who are registered as unemployed at the government employment agency. Furthermore, pension receivables or claims could depend on the registration at the government employment agency.\nIn many countries like in Germany, the unemployment rate is based on the number of people who are registered as unemployed. Other countries like the United States use a labour force survey to calculate the unemployment rate.\nThe ILO describes four different methods to calculate the unemployment rate:\nThe primary measure of unemployment, U3, allows for comparisons between countries. Unemployment differs from country to country and across different time periods. For example, in the 1990s and 2000s, the United States had lower unemployment levels than many countries in the European Union, which had significant internal variation, with countries like the United Kingdom and Denmark outperforming Italy and France. However, large economic events like the Great Depression can lead to similar unemployment rates across the globe.\nIn 2013, the ILO adopted a resolution to introduce new indicators to measure the unemployment rate.\nx 100\nlabour force) / (extended labour force)] \u00d7 100\nEuropean Union (Eurostat).\nEurostat, the statistical office of the European Union, defines unemployed as those persons between age 15 and 74 who are not working, have looked for work in the last four weeks, and are ready to start work within two weeks; this definition conforms to ILO standards. Both the actual count and the unemployment rate are reported. Statistical data are available by member state for the European Union as a whole (EU28) as well as for the eurozone (EA19). Eurostat also includes a long-term unemployment rate, which is defined as part of the unemployed who have been unemployed for more than one year.\nThe main source used is the European Union Labour Force Survey (EU-LFS). It collects data on all member states each quarter. For monthly calculations, national surveys or national registers from employment offices are used in conjunction with quarterly EU-LFS data. The exact calculation for individual countries, resulting in harmonized monthly data, depends on the availability of the data.\nUnited States Bureau of Labor statistics.\nThe Bureau of Labor Statistics measures employment and unemployment (of those over 17 years of age) by using two different labor force surveys conducted by the United States Census Bureau (within the United States Department of Commerce) and/or the Bureau of Labor Statistics (within the United States Department of Labor) that gather employment statistics monthly. The Current Population Survey (CPS), or \"Household Survey\", conducts a survey based on a sample of 60,000 households. The survey measures the unemployment rate based on the ILO definition.\nThe Current Employment Statistics survey (CES), or \"Payroll Survey\", conducts a survey based on a sample of 160,000 businesses and government agencies, which represent 400,000 individual employers. Since the survey measures only civilian nonagricultural employment, it does not calculate an unemployment rate, and it differs from the ILO unemployment rate definition. Both sources have different classification criteria and usually produce differing results. Additional data are also available from the government, such as the unemployment insurance weekly claims report available from the Office of Workforce Security, within the U.S. Department of Labor's Employment and Training Administration. The Bureau of Labor Statistics provides up-to-date numbers via a PDF linked here. The BLS also provides a readable concise current Employment Situation Summary, updated monthly.\nThe Bureau of Labor Statistics also calculates six alternate measures of unemployment, U1 to U6, which measure different aspects of unemployment:\n\"Note: \"Marginally attached workers\" are added to the total labour force for unemployment rate calculation for U4, U5, and U6.\" The BLS revised the CPS in 1994 and among the changes the measure representing the official unemployment rate was renamed U3 instead of U5. In 2013, Representative Hunter proposed that the Bureau of Labor Statistics use the U5 rate instead of the current U3 rate.\nStatistics for the US economy as a whole hide variations among groups. For example, in January 2008, the US unemployment rates were 4.4% for adult men, 4.2% for adult women, 4.4% for Caucasians, 6.3% for Hispanics or Latinos (all races), 9.2% for African Americans, 3.2% for Asian Americans, and 18.0% for teenagers. Also, the US unemployment rate would be at least 2% higher if prisoners and jail inmates were counted.\nThe unemployment rate is included in a number of major economic indices including the US Conference Board's Index of Leading Indicators a macroeconomic measure of the state of the economy.\nLimitations of definition.\nSome critics believe that current methods of measuring unemployment are inaccurate in terms of the impact of unemployment on people as these methods do not take into account the 1.5% of the available working population incarcerated in US prisons (who may or may not be working while they are incarcerated); those who have lost their jobs and have become discouraged over time from actively looking for work; those who are self-employed or wish to become self-employed, such as tradesmen or building contractors or information technology consultants; those who have retired before the official retirement age but would still like to work (involuntary early retirees); those on disability pensions who do not possess full health but still wish to work in occupations suitable for their medical conditions; or those who work for payment for as little as one hour per week but would like to work full time.\nThe last people are \"involuntary part-time\" workers, those who are underemployed, such as a computer programmer who is working in a retail store until he can find a permanent job, involuntary stay-at-home mothers who would prefer to work, and graduate and professional school students who are unable to find worthwhile jobs after they graduated with their bachelor's degrees.\nInternationally, some nations' unemployment rates are sometimes muted or appear less severe because of the number of self-employed individuals working in agriculture. Small independent farmers are often considered self-employed and so cannot be unemployed. That can impact non-industrialized economies, such as the United States and Europe in the early 19th century, since overall unemployment was approximately 3% because so many individuals were self-employed, independent farmers; however, non-agricultural unemployment was as high as 80%.\nUnemployment in official accounts has been estimated to be under-reported by between 1% and 4%.\nMany economies industrialize and so experience increasing numbers of non-agricultural workers. For example, the United States' non-agricultural labour force increased from 20% in 1800 to 50% in 1850 and 97% in 2000. The shift away from self-employment increases the percentage of the population that is included in unemployment rates. When unemployment rates between countries or time periods are compared, it is best to consider differences in their levels of industrialization and self-employment.\nAdditionally, the measures of employment and unemployment may be \"too high\". In some countries, the availability of unemployment benefits can inflate statistics by giving an incentive to register as unemployed. People who do not seek work may choose to declare themselves unemployed to get benefits; people with undeclared paid occupations may try to get unemployment benefits in addition to the money that they earn from their work.\nHowever, in the United States, Canada, Mexico, Australia, Japan, and the European Union, unemployment is measured using a sample survey (akin to a Gallup poll). According to the BLS, a number of Eastern European nations have instituted labour force surveys as well. The sample survey has its own problems because the total number of workers in the economy is calculated based on a sample, rather than a census.\nIt is possible to be neither employed nor unemployed by ILO definitions by being outside of the \"labour force\". Such people have no job and are not looking for one. Many of them go to school or are retired. Family responsibilities keep others out of the labour force. Still others have a physical or mental disability that prevents them from participating in the labour force. Some people simply elect not to work and prefer to be dependent on others for sustenance.\nTypically, employment and the labour force include only work that is done for monetary gain. Hence, a homemaker is neither part of the labour force nor unemployed. Also, full-time students and prisoners are considered to be neither part of the labour force nor unemployed. The number of prisoners can be important. In 1999, economists Lawrence F. Katz and Alan B. Krueger estimated that increased incarceration lowered measured unemployment in the United States by 0.17% between 1985 and the late 1990s.\nIn particular, as of 2005, roughly 0.7% of the US population is incarcerated (1.5% of the available working population). Additionally, children, the elderly, and some individuals with disabilities are typically not counted as part of the labour force and so are not included in the unemployment statistics. However, some elderly and many disabled individuals are active in the labour market.\nIn the early stages of an economic boom, unemployment often rises. That is because people join the labour market (give up studying, start a job hunt, etc.) as a result of the improving job market, but until they have actually found a position, they are counted as unemployed. Similarly, during a recession, the increase in the unemployment rate is moderated by people leaving the labour force or being otherwise discounted from the labour force, such as with the self-employed.\nFor the fourth quarter of 2004, according to OECD (http:// ), normalized unemployment for men aged 25 to 54 was 4.6% in the US and 7.4% in France. At the same time and for the same population, the employment rate (number of workers divided by population) was 86.3% in the US and 86.7% in France. That example shows that the unemployment rate was 60% higher in France than in the US, but more people in that demographic were working in France than in the US, which is counterintuitive if it is expected that the unemployment rate reflects the health of the labour market.\nThose deficiencies make many labour market economists prefer to look at a range of economic statistics such as labour market participation rate, the percentage of people between 15 and 64 who are currently employed or searching for employment, the total number of full-time jobs in an economy, the number of people seeking work as a raw number and not a percentage, and the total number of person-hours worked in a month compared to the total number of person-hours people would like to work. In particular, the National Bureau of Economic Research does not use the unemployment rate but prefers various employment rates to date recessions. Moreover, some articles in prestigious magazines such as The Economist have argued that alternative ways to measure economic misery are needed.\nEffects.\nHigh and the persistent unemployment, in which economic inequality increases, has a negative effect on subsequent long-run economic growth. Unemployment can harm growth because it is a waste of resources; generates redistributive pressures and subsequent distortions; drives people to poverty; constrains liquidity limiting labor mobility; and erodes self-esteem promoting social dislocation, unrest, and conflict. The 2013 winner of the Nobel Prize in Economics, Robert J. Shiller, said that rising inequality in the United States and elsewhere is the most important problem.\nCosts.\nIndividual.\nUnemployed individuals are unable to earn money to meet financial obligations. Failure to pay mortgage payments or to pay rent may lead to homelessness through foreclosure or eviction. Across the United States the growing ranks of people made homeless in the foreclosure crisis are generating tent cities.\nUnemployment increases susceptibility to cardiovascular disease, somatization, anxiety disorders, depression, and suicide. In addition, unemployed people have higher rates of medication use, poor diet, physician visits, tobacco smoking, alcoholic beverage consumption, drug use, and lower rates of exercise. According to a study published in Social Indicator Research, even those who tend to be optimistic find it difficult to look on the bright side of things when unemployed. Using interviews and data from German participants aged 16 to 94, including individuals coping with the stresses of real life and not just a volunteering student population, the researchers determined that even optimists struggled with being unemployed.\nIn 1979, M. Harvey Brenner found that for every 10% increase in the number of unemployed, there is an increase of 1.2% in total mortality, a 1.7% increase in cardiovascular disease, 1.3% more cirrhosis cases, 1.7% more suicides, 4.0% more arrests, and 0.8% more assaults reported to the police.\nA study by Christopher Ruhm in 2000 on the effect of recessions on health found that several measures of health actually improve during recessions. As for the impact of an economic downturn on crime, during the Great Depression, the crime rate did not decrease. The unemployed in the US often use welfare programs such as food stamps or accumulating debt because unemployment insurance in the US generally does not replace most of the income that was received on the job, and one cannot receive such aid indefinitely.\nNot everyone suffers equally from unemployment. In a prospective study of 9,570 individuals over four years, highly conscientious people suffered more than twice as much if they became unemployed. The authors suggested that may because of conscientious people making different attributions about why they became unemployed or through experiencing stronger reactions following failure. There is also the possibility of reverse causality from poor health to unemployment.\nSome researchers hold that many of the low-income jobs are not really a better option than unemployment with a welfare state, with its unemployment insurance benefits. However, since it is difficult or impossible to get unemployment insurance benefits without having worked in the past, those jobs and unemployment are more complementary than they are substitutes. (They are often held short-term, either by students or by those trying to gain experience; turnover in most low-paying jobs is high.)\nAnother cost for the unemployed is that the combination of unemployment, lack of financial resources, and social responsibilities may push unemployed workers to take jobs that do not fit their skills or allow them to use their talents. Unemployment can cause underemployment, and fear of job loss can spur psychological anxiety. As well as anxiety, it can cause depression, lack of confidence, and huge amounts of stress, which is increased when the unemployed are faced with health issues, poverty, and lack of relational support.\nAnother personal cost of unemployment is its impact on relationships. A 2008 study from Covizzi, which examined the relationship between unemployment and divorce, found that the rate of divorce is greater for couples when one partner is unemployed. However, a more recent study has found that some couples often stick together in \"unhappy\" or \"unhealthy\" marriages when they are unemployed to buffer financial costs. A 2014 study by Van der Meer found that the stigma that comes from being unemployed affects personal well-being, especially for men, who often feel as though their masculine identities are threatened by unemployment.\nGender and age.\nUnemployment can also bring personal costs in relation to gender. One study found that women are more likely to experience unemployment than men and that they are less likely to move from temporary positions to permanent positions. Another study on gender and unemployment found that men, however, are more likely to experience greater stress, depression, and adverse effects from unemployment, largely stemming from the perceived threat to their role as breadwinner. The study found that men expect themselves to be viewed as \"less manly\" after a job loss than they actually are and so they engage in compensating behaviors, such as financial risk-taking and increased assertiveness. Unemployment has been linked to extremely adverse effects on men's mental health. Professor Ian Hickie of the University of Sydney said that evidence showed that men have more restricted social networks than women and that men have are heavily work-based. Therefore, the loss of a job for men means the loss of a whole set of social connections as well. That loss can then lead to men becoming socially isolated very quickly. An Australian study on the mental health impacts of graduating during an economic downturn found that the negative mental health outcomes are greater and more scarring for men than women. The effect was particularly pronounced for those with vocational or secondary education.\nCosts of unemployment also vary depending on age. The young and the old are the two largest age groups currently experiencing unemployment. A 2007 study from Jacob and Kleinert found that young people (ages 18 to 24) who have fewer resources and limited work experiences are more likely to be unemployed. Other researchers have found that today's high school seniors place a lower value on work than those in the past, which is likely because they recognize the limited availability of jobs. At the other end of the age spectrum, studies have found that older individuals have more barriers than younger workers to employment, require stronger social networks to acquire work, and are also less likely to move from temporary to permanent positions. Additionally, some older people see age discrimination as the reason for them not getting hired.\nSocial.\nAn economy with high unemployment is not using all of the resources, specifically labour, available to it. Since it is operating below its production possibility frontier, it could have higher output if all of the workforce were usefully employed. However, there is a tradeoff between economic efficiency and unemployment: if all frictionally unemployed accepted the first job that they were offered, they would be likely to be operating at below their skill level, reducing the economy's efficiency.\nDuring a long period of unemployment, workers can lose their skills, causing a loss of human capital. Being unemployed can also reduce the life expectancy of workers by about seven years.\nHigh unemployment can encourage xenophobia and protectionism since workers fear that foreigners are stealing their jobs. Efforts to preserve existing jobs of domestic and native workers include legal barriers against \"outsiders\" who want jobs, obstacles to immigration, and/or tariffs and similar trade barriers against foreign competitors.\nHigh unemployment can also cause social problems such as crime. If people have less disposable income than before, it is very likely that crime levels within the economy will increase.\nA 2015 study published in \"The Lancet\", estimates that unemployment causes 45,000 suicides a year globally.\nPolitical.\nUnemployment can cause of civil disorder, in some cases leading to revolution, particularly totalitarianism. The fall of the Weimar Republic in 1933 and Adolf Hitler's rise to power, which culminated in World War II and the deaths of tens of millions and the destruction of much of the physical capital of Europe, is attributed to the poor economic conditions in Germany at the time, notably a high unemployment rate of above 20%. \nThe hyperinflation in the Weimar Republic is not directly blamed for the Nazi rise, since hyperinflation occurred primarily in 1921 to 1923, the year of Hitler's Beer Hall Putsch. Although hyperinflation has been blamed for damaging the credibility of democratic institutions, the Nazis did not assume government until 1933, ten years after the hyperinflation but in the midst of high unemployment.\nA study found unemployment is associated with decreased trust in democracy. Rising unemployment has traditionally been regarded by the public and the media in any country as a key guarantor of electoral defeat for any government that oversees it. That was very much the consensus in the United Kingdom until 1983, when Thatcher won the 1983 United Kingdom general election, despite overseeing a rise in unemployment from 1.5 million to 3.2 million since the 1979 election and decreasing popular vote percentage.\nBenefits.\nThe primary benefit of unemployment is that people are available for hire, without being headhunted away from their existing employers. That permits both new and old businesses to take on staff.\nUnemployment is argued to be \"beneficial\" to the people who are not unemployed in the sense that it averts inflation, which itself has damaging effects, by providing (in Marxian terms) a reserve army of labour, which keeps wages in check. However, the direct connection between full local employment and local inflation has been disputed by some because of the recent increase in international trade that supplies low-priced goods even while local employment rates rise to full employment.\n Full employment cannot be achieved because workers would shirk if they were not threatened with the possibility of unemployment. The curve for the no-shirking condition (labelled NSC) thus goes to infinity at full employment. The inflation-fighting benefits to the entire economy arising from a presumed optimum level of unemployment have been studied extensively. The Shapiro\u2013Stiglitz model suggests that wages never bid down sufficiently to reach 0% unemployment. That occurs because employers know that when wages decrease, workers will shirk and expend less effort. Employers avoid shirking by preventing wages from decreasing so low that workers give up and become unproductive. The higher wages perpetuate unemployment, but the threat of unemployment reduces shirking.\nBefore current levels of world trade were developed, unemployment was shown to reduce inflation, following the Phillips curve, or to decelerate inflation, following the NAIRU/natural rate of unemployment theory since it is relatively easy to seek a new job without losing a current job. When more jobs are available for fewer workers (lower unemployment), that may allow workers to find the jobs that better fit their tastes, talents and needs.\nAs in the Marxian theory of unemployment, special interests may also benefit. Some employers may expect that employees with no fear of losing their jobs will not work as hard or will demand increased wages and benefit. According to that theory, unemployment may promote general labour productivity and profitability by increasing employers' rationale for their monopsony-like power (and profits).\nOptimal unemployment has also been defended as an environmental tool to brake the constantly accelerated growth of the GDP to maintain levels that are sustainable in the context of resource constraints and environmental impacts. However, the tool of denying jobs to willing workers seems a blunt instrument for conserving resources and the environment. It reduces the consumption of the unemployed across the board and only in the short term. Full employment of the unemployed workforce, all focused toward the goal of developing more environmentally efficient methods for production and consumption, might provide a more significant and lasting cumulative environmental benefit and reduced resource consumption.\nSome critics of the \"culture of work\" such as the anarchist Bob Black see employment as culturally overemphasized in modern countries. Such critics often propose quitting jobs when possible, working less, reassessing the cost of living to that end, creation of jobs that are \"fun\" as opposed to \"work,\" and creating cultural norms in which work is seen as unhealthy. These people advocate an \"anti-work\" ethic for life.\nDecline in work hours.\nAs a result of productivity, the work week declined considerably during the 19th century By the 1920s, the average workweek in the US was 49 hours, but it was reduced to 40 hours (after which overtime premium was applied) as part of the 1933 National Industrial Recovery Act. During the Great Depression, the enormous productivity gains caused by electrification, mass production, and agricultural mechanization were believed to have ended the need for a large number of previously employed workers.\nRemedies.\nSocieties try a number of different measures to get as many people as possible into work, and various societies have experienced close to full employment for extended periods, particularly during the post-World War II economic expansion. The United Kingdom in the 1950s and 1960s averaged 1.6% unemployment, and in Australia, the 1945 \"White Paper on Full Employment in Australia\" established a government policy of full employment, which lasted until the 1970s.\nHowever, mainstream economic discussions of full employment since the 1970s suggest that attempts to reduce the level of unemployment below the natural rate of unemployment will fail but result only in less output and more inflation.\nDemand-side solutions.\nIncreases in the demand for labour move the economy along the demand curve, increasing wages and employment. The demand for labour in an economy is derived from the demand for goods and services. As such, if the demand for goods and services in the economy increases, the demand for labour will increase, increasing employment and wages.\nThere are many ways to stimulate demand for goods and services. Increasing wages to the working class (those more likely to spend the increased funds on goods and services, rather than various types of savings or commodity purchases) is one theory that is proposed. Increased wages are believed to be more effective in boosting demand for goods and services than central banking strategies, which put the increased money supply mostly into the hands of wealthy personnel and institutions. Monetarists suggest that increasing money supply in general increases short-term demand. As for the long-term demand, the increased demand is negated by inflation. A rise in fiscal expenditures is another strategy for boosting aggregate demand.\nProviding aid to the unemployed is a strategy that is used to prevent cutbacks in consumption of goods and services, which can lead to a vicious cycle of further job losses and further decreases in consumption and demand. Many countries aid the unemployed through social welfare programs. Such unemployment benefits include unemployment insurance, unemployment compensation, welfare, and subsidies to aid in retraining. The main goal of such programs is to alleviate short-term hardships and, more importantly, to allow workers more time to search for a job.\nA direct demand-side solution to unemployment is government-funded employment of the able-bodied poor. This was notably implemented in Britain from the 17th century until 1948 in the institution of the workhouse, which provided jobs for the unemployed with harsh conditions and poor wages to dissuade their use. A modern alternative is a job guarantee in which the government guarantees work at a living wage.\nTemporary measures can include public works programs such as the Works Progress Administration. Government-funded employment is not widely advocated as a solution to unemployment except in times of crisis. That is attributed to the public sector jobs existence depending directly on the tax receipts from private sector employment.\nIn the US, the unemployment insurance allowance is based solely on previous income (not time worked, family size, etc.) and usually compensates for one third of previous income. To qualify, people must reside in their respective state for at least a year and work. The system was established by the Social Security Act of 1935. Although 90% of citizens are covered by unemployment insurance, less than 40% apply for and receive benefits. However, the number applying for and receiving benefits increases during recessions. For highly-seasonal industries, the system provides income to workers during the off-season, thus encouraging them to stay attached to the industry.\nMonetary policy and fiscal policy can both be used to increase short-term growth in the economy, increasing the demand for labour and decreasing unemployment.\nSupply-side solutions.\nPrice elasticity of labor supply has been estimated at around 0.7-1.8. Supply-side policies include making the labour market more flexible, reducing income taxes, and removing the minimum wage. Supply-siders argue that their reforms increase long-term growth, create jobs, and reduce unemployment. Other supply-side policies include education to make workers more attractive to employers.\nLabour market flexibility.\nAccording to classical economic theory, markets reach equilibrium where supply equals demand; everyone who wants to sell at the market price can do so. Those who do not want to sell at that price do not; in the labour market, this is classical unemployment or reduced labor force participation. That assumes perfect competition exists in the labour market, specifically that no single entity is large enough to affect wage levels. However, the labor market is not 100% efficient although it may be more efficient than the bureaucracy. Some argue that minimum wages and regulations preventing some people from selling their labour.\nHistory.\nThere are relatively limited historical records on unemployment because it has not always been acknowledged or measured systematically. Industrialization involves economies of scale, which often prevent individuals from having the capital to create their own jobs to be self-employed. An individual who cannot join an enterprise or create a job is unemployed. As individual farmers, ranchers, spinners, doctors and merchants are organized into large enterprises, those who cannot join or compete become unemployed.\nRecognition of unemployment occurred slowly as economies across the world industrialized and bureaucratized. Before that, traditional self-sufficient native societies had no concept of unemployment. The recognition of the concept of \"unemployment\" is best exemplified through the well documented historical records in England. For example, in 16th-century, England no distinction was made between vagrants and the jobless; both were simply categorized as \"sturdy beggars\", who were to be punished and moved on.\n16th century.\nThe closing of the monasteries in the 1530s increased poverty, as the Roman Catholic Church had helped the poor. In addition, there was a significant rise in enclosures during the Tudor period. Also, the population was rising. Those unable to find work had a stark choice: starve or break the law. In 1535, a bill was drawn up calling for the creation of a system of public works to deal with the problem of unemployment, which were to be funded by a tax on income and capital. A law that was passed a year later allowed vagabonds to be whipped and hanged.\nIn 1547, a bill was passed that subjected vagrants to some of the more extreme provisions of the criminal law: two years' servitude and branding with a \"V\" as the penalty for the first offense and death for the second. During the reign of Henry VIII, as many as 72,000 people are estimated to have been executed. In the 1576 Act, each town was required to provide work for the unemployed.\nThe Poor Relief Act 1601, one of the world's first government-sponsored welfare programs, made a clear distinction between those who were unable to work and those able-bodied people who refused employment. Under the Poor Law systems of England and Wales, Scotland and Ireland, a workhouse was a place people unable to support themselves could go to live and work.\nIndustrial Revolution to late 19th century.\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;Poverty was a highly visible problem in the eighteenth century, both in cities and in the countryside. In France and Britain by the end of the century, an estimated 10 percent of the people depended on charity or begging for their food.\u2014\u200a By 1776, some 1,912 parish and corporation workhouses had been established in England and Wales and housed almost 100,000 paupers.\nA description of the miserable living standards of the mill workers in England in 1844 was given by Fredrick Engels in \"The Condition of the Working Class in England in 1844\". In the preface to the 1892 edition, Engels noted that the extreme poverty he had written about in 1844 had largely disappeared. David Ames Wells also noted that living conditions in England had improved near the end of the 19th century and that unemployment was low.\nThe scarcity and the high price of labor in the US in the 19th century was well documented by contemporary accounts, as in the following:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;\"The laboring classes are comparatively few in number, but this is counterbalanced by, and indeed, may be one of the causes of the eagerness by which they call in the use of machinery in almost every department of industry. Wherever it can be applied as a substitute for manual labor, it is universally and willingly resorted to... It is this condition of the labor market, and this eager resort to machinery wherever it can be applied, to which, under the guidance of superior education and intelligence, the remarkable prosperity of the United States is due.\"\u2014\u200a\nScarcity of labor was a factor in the economics of slavery in the United States.\nAs new territories were opened and federal land sales were conducted, land had to be cleared and new homesteads established. Hundreds of thousands of immigrants annually came to the US and found jobs digging canals and building railroads. Almost all work during most of the 19th century was done by hand or with horses, mules, or oxen since there was very little mechanization. The workweek during most of the 19th century was 60 hours. Unemployment at times was between one and two percent.\nThe tight labor market was a factor in productivity gains by allowing workers to maintain or to increase their nominal wages during the secular deflation that caused real wages to rise at various times in the 19th century, especially in its final decades.\n20th century.\nThere were labor shortages during World War I. Ford Motor Co. doubled wages to reduce turnover. After 1925, unemployment gradually began to rise.\nThe 1930s saw the Great Depression impact unemployment across the globe. In Germany and the United States, the unemployment rate reached about 25% in 1932.\nIn some towns and cities in the northeast of England, unemployment reached as high as 70%; the national unemployment level peaked at more than 22% in 1932. Unemployment in Canada reached 27% at the depth of the Depression in 1933. In 1929, the U.S. unemployment rate averaged 3%.\nIn the US, the Works Progress Administration (1935\u201343) was the largest make-work program. It hired men (and some women) off the relief roles (\"dole\") typically for unskilled labor.\nDuring the New Deal, over three million unemployed young men were taken out of their homes and placed for six months into more than 2600 work camps managed by the Civilian Conservation Corps.\nUnemployment in the United Kingdom fell later in the 1930s as the Depression eased, and it remained low (in single figures) after World War II.\nFredrick Mills found that in the US, 51% of the decline in work hours was due to the fall in production and 49% was from increased productivity.\nBy 1972, unemployment in the United Kingdom had crept back up above 1,000,000, and it was even higher by the end of the decade, with inflation also being high. Although the monetarist economic policies of Margaret Thatcher's Conservative government saw inflation reduced after 1979, unemployment soared in the early 1980s and in 1982, it exceeded 3,000,000, a level that had not been seen for some 50 years. That represented one in eight of the workforce, with unemployment exceeding 20% in some places that had relied on declining industries such as coal mining.\nHowever, it was a time of high unemployment in all other major industrialised nations as well. By the spring of 1983, unemployment had risen by 6% in the previous 12 months, compared to 10% in Japan, 23% in the US, and 34% in West Germany (seven years before Reunification).\nUnemployment in the United Kingdom remained above 3,000,000 until the spring of 1987, when the economy enjoyed a boom. By the end of 1989, unemployment had fallen to 1,600,000. However, inflation had reached 7.8%, and the following year, it reached a nine-year high of 9.5%; leading to increased interest rates.\nAnother recession occurred from 1990 to 1992. Unemployment began to increase, and by the end of 1992, nearly 3,000,000 in the United Kingdom were unemployed, a number that was soon lowered by a strong economic recovery. With inflation down to 1.6% by 1993, unemployment then began to fall rapidly and stood at 1,800,000 by early 1997.\n21st century.\nThe official unemployment rate in the 16 European Union (EU) countries that use the euro rose to 10% in December 2009 as a result of another recession. Latvia had the highest unemployment rate in the EU, at 22.3% for November 2009. Europe's young workers have been especially hard hit. In November 2009, the unemployment rate in the EU27 for those aged 15\u201324 was 18.3%. For those under 25, the unemployment rate in Spain was 43.8%. Unemployment has risen in two thirds of European countries since 2010.\nInto the 21st century, unemployment in the United Kingdom remained low and the economy remaining strong, and several other European economies, such as France and Germany, experienced a minor recession and a substantial rise in unemployment.\nIn 2008, when the recession brought on another increase in the United Kingdom, after 15 years of economic growth and no major rises in unemployment. In early 2009, unemployment passed the 2 million mark, and economists were predicting it would soon reach 3 million. However, the end of the recession was declared in January 2010 and unemployment peaked at nearly 2.7 million in 2011, appearing to ease fears of unemployment reaching 3 million. The unemployment rate of Britain's young black people was 47.4% in 2011. 2013/2014 has seen the employment rate increase from 1,935,836 to 2,173,012 as supported by showing the UK is creating more job opportunities and forecasts the rate of increase in 2014/2015 will be another 7.2%.\nThe Great Recession has been called a \"mancession\" because of the disproportionate number of men who lost their jobs as compared to women. The gender gap became wide in the United States in 2009, when 10.5% of men in the labor force were unemployed, compared with 8% of women. Three quarters of the jobs that were lost in the recession in the US were held by men.\nA 26 April 2005 \"Asia Times\" article noted, \"In regional giant South Africa, some 300,000 textile workers have lost their jobs in the past two years due to the influx of Chinese goods\". The increasing US trade deficit with China cost 2.4 million American jobs between 2001\u20132008, according to a study by the Economic Policy Institute (EPI). From 2000 to 2007, the United States lost a total of 3.2 million manufacturing jobs. 12.1% of US military veterans who had served after the September 11 attacks in 2001 were unemployed as of 2011; 29.1% of male veterans aged 18\u201324 were unemployed. As of September 2016, the total veteran unemployment rate was 4.3 percent. By September 2017, that figure had dropped to 3 percent.\nAbout 25,000,000 people in the world's 30 richest countries lost their jobs between the end of 2007 and the end of 2010, as the economic downturn pushed most countries into recession. In April 2010, the US unemployment rate was 9.9%, but the government's broader U-6 unemployment rate was 17.1%. In April 2012, the unemployment rate was 4.6% in Japan. In a 2012 story, the \"Financial Post\" reported, \"Nearly 75 million youth are unemployed around the world, an increase of more than 4 million since 2007. In the European Union, where a debt crisis followed the financial crisis, the youth unemployment rate rose to 18% in 2011 from 12.5% in 2007, the ILO report shows.\" In March 2018, according to US Unemployment Rate Statistics, the unemployment rate was 4.1%, below the 4.5\u20135.0% norm.\nIn 2021, the labor force participation rate for non-white women and women with children declined significantly during the COVID-19 pandemic, with approximately 20 million women leaving the workforce. Men were not nearly as impacted, leading some to describe the phenomenon as a \"she-cession\".\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "31742", "revid": "51069716", "url": "https://en.wikipedia.org/wiki?curid=31742", "title": "Unicode", "text": "Character encoding standard\n&lt;templatestyles src=\"Mono/styles.css\" /&gt;\nUnicode (also known as The Unicode Standard and TUS) is a character encoding standard maintained by the Unicode Consortium designed to support the use of text in all of the world's writing systems that can be digitized. Version 17.0 defines 159,801 characters and 172 scripts used in various ordinary, literary, academic, and technical contexts.\nUnicode has largely supplanted the previous environment of myriad incompatible character sets used within different locales and on different computer architectures. The entire repertoire of these sets, plus many additional characters, were merged into the single Unicode set. Unicode is used to encode the vast majority of text on the Internet, including most web pages, and relevant Unicode support has become a common consideration in contemporary software development. Unicode is ultimately capable of encoding more than 1.1 million characters.\nThe Unicode character repertoire is synchronized with ISO/IEC 10646, each being code-for-code identical with one another. However, \"The Unicode Standard\" is more than just a repertoire within which characters are assigned. To aid developers and designers, the standard also provides charts and reference data, as well as annexes explaining concepts germane to various scripts, providing guidance for their implementation. Topics covered by these annexes include character normalization, character composition and decomposition, collation, and directionality.\nUnicode encodes 3,790 emoji, with the continued development thereof conducted by the Consortium as a part of the standard. The widespread adoption of Unicode was in large part responsible for the initial popularization of emoji outside of Japan.\nUnicode text is processed and stored as binary data using one of several encodings, which define how to translate the standard's abstracted codes for characters into sequences of bytes. \"The Unicode Standard\" itself defines three encodings: UTF-8, UTF-16, and UTF-32, though several others exist. UTF-8 is the most widely used by a large margin, in part due to its backwards-compatibility with ASCII.\nOrigin and development.\nUnicode was originally designed with the intent of transcending limitations present in all text encodings designed up to that point: each encoding was relied upon for use in its own context, but with no particular expectation of compatibility with any other. Indeed, any two encodings chosen were often totally unworkable when used together, with text encoded in one interpreted as garbage characters by the other. Most encodings had only been designed to facilitate interoperation between a handful of scripts\u2014often primarily between a given script and Latin characters\u2014not between a large number of scripts, and not with all of the scripts supported being treated in a consistent manner.\nThe philosophy that underpins Unicode seeks to encode the underlying characters\u2014graphemes and grapheme-like units\u2014rather than graphical distinctions considered mere variant glyphs thereof, that are instead best handled by the typeface, through the use of markup, or by some other means. In particularly complex cases, such as the treatment of orthographical variants in Han characters, there is considerable disagreement regarding which differences justify their own encodings, and which are only graphical variants of other characters.\nAt the most abstract level, Unicode assigns a unique number called a code point to each character. Many issues of visual representation\u2014including size, shape, and style\u2014are intended to be up to the discretion of the software actually rendering the text, such as a web browser or word processor. However, partially with the intent of encouraging rapid adoption, the simplicity of this original model has become somewhat more elaborate over time, and various pragmatic concessions have been made over the course of the standard's development.\nThe first 256 code points mirror the ISO/IEC 8859-1 standard, with the intent of trivializing the conversion of text already written in Western European scripts. To preserve the distinctions made by different legacy encodings, therefore allowing for conversion between them and Unicode without any loss of information, many characters nearly identical to others, in both appearance and intended function, were given distinct code points. For example, the Halfwidth and Fullwidth Forms block encompasses a full semantic duplicate of the Latin alphabet, because legacy CJK encodings contained both \"fullwidth\" (matching the width of CJK characters) and \"halfwidth\" (matching ordinary Latin script) characters.\nHistory.\nThe origins of Unicode can be traced back to the 1980s, to a group of individuals with connections to Xerox's Character Code Standard (XCCS). In 1987, Xerox employee Joe Becker, along with Apple employees Lee Collins and Mark Davis, started investigating the practicalities of creating a universal character set. With additional input from Peter Fenwick and Dave Opstad, Becker published a draft proposal for an \"international/multilingual text character encoding system in August 1988, tentatively called Unicode\". He explained that \"the name 'Unicode' is intended to suggest a unique, unified, universal encoding\".\nIn this document, entitled \"Unicode 88\", Becker outlined a scheme using 16-bit characters:\nUnicode is intended to address the need for a workable, reliable world text encoding. Unicode could be roughly described as \"wide-body ASCII\" that has been stretched to 16\u00a0bits to encompass the characters of all the world's living languages. In a properly engineered design, 16\u00a0bits per character are more than sufficient for this purpose.\nThis design decision was made based on the assumption that only scripts and characters in \"modern\" use would require encoding:\nUnicode gives higher priority to ensuring utility for the future than to preserving past antiquities. Unicode aims in the first instance at the characters published in the modern text (e.g. in the union of all newspapers and magazines printed in the world in 1988), whose number is undoubtedly far below 214 = 16,384. Beyond those modern-use characters, all others may be defined to be obsolete or rare; these are better candidates for private use registration than for congesting the public list of generally useful Unicode.\nIn early 1989, the Unicode working group expanded to include Ken Whistler and Mike Kernaghan of Metaphor, Karen Smith-Yoshimura and Joan Aliprand of Research Libraries Group, and Glenn Wright of Sun Microsystems. In 1990, Michel Suignard and Asmus Freytag of Microsoft and NeXT's Rick McGowan had also joined the group. By the end of 1990, most of the work of remapping existing standards had been completed, and a final review draft of Unicode was ready.\nThe Unicode Consortium was incorporated in California on 3 January 1991, and the first volume of \"The Unicode Standard\" was published that October. The second volume, now adding Han ideographs, was published in June 1992.\nIn 1996, a surrogate character mechanism was implemented in Unicode 2.0, so that Unicode was no longer restricted to 16 bits. This increased the Unicode codespace to over a million code points, which allowed for the encoding of many historic scripts, such as Egyptian hieroglyphs, and thousands of rarely used or obsolete characters that had not been anticipated for inclusion in the standard. Among these characters are various rarely used CJK characters\u2014many mainly being used in proper names, making them far more necessary for a universal encoding than the original Unicode architecture envisioned.\nUnicode Consortium.\nThe Unicode Consortium is a non-profit organization that coordinates Unicode's development. Full members include most of the main computer software and hardware companies (and few others) with any interest in text-processing standards, including Adobe, Apple, Google, IBM, Meta (previously as Facebook), Microsoft, Netflix, and SAP.\nOver the years several countries or government agencies have been members of the Unicode Consortium.\nThe Consortium has the ambitious goal of eventually replacing existing character encoding schemes with Unicode and its standard Unicode Transformation Format (UTF) schemes, as many of the existing schemes are limited in size and scope and are incompatible with multilingual environments.\nThe Unicode Bulldog Award is given to people deemed to be influential in Unicode's development, with recipients including Tatsuo Kobayashi, Thomas Milo, Roozbeh Pournader, Ken Lunde, and Michael Everson.\nScripts covered.\nAs of September 2025[ [update]], a total of 172 scripts (alphabets, abugidas and syllabaries) are included in Unicode, covering most major writing systems in use today. There are still scripts that are not yet encoded, particularly those mainly used in historical, liturgical, and academic contexts. Further additions of characters to the already encoded scripts, as well as symbols, in particular for mathematics and music also occur.\nProposals for adding scripts.\nThe Unicode Roadmap Committee (Michael Everson, Rick McGowan, Ken Whistler, V.S. Umamaheswaran) maintain the list of scripts that are candidates or potential candidates for encoding and their tentative code block assignments on the Unicode Roadmap page of the Unicode Consortium website. For some scripts on the Roadmap, such as Jurchen and Khitan large script, encoding proposals have been made and they are working their way through the approval process. For other scripts, such as Numidian and Rongorongo, no proposal has yet been made, and they await agreement on character repertoire and other details from the user communities involved.\nSome modern invented scripts which have not yet been included in Unicode (e.g., Tengwar) or which do not qualify for inclusion in Unicode due to lack of real-world use (e.g., Klingon) are listed in the ConScript Unicode Registry, along with unofficial but widely used private use area code assignments.\nThere is also a Medieval Unicode Font Initiative focused on special Latin medieval characters. Part of these proposals has been already included in Unicode.\nThe Script Encoding Initiative (SEI), a project created by Deborah Anderson at the University of California, Berkeley, was founded in 2002 with the goal of funding proposals for scripts not yet encoded in the standard. Now run by Anushah Hossain, SEI has become a major source of proposed additions to the standard in recent years. Although SEI collaborates with the Unicode Consortium and the ISO/IEC 10646 standards process, it operates independently, supporting the technical, linguistic, and historical research needed to prepare formal proposals. SEI maintains a database of scripts that have yet to be encoded in the Unicode Standard on the project's website.\nVersions.\nThe Unicode Consortium together with the ISO have developed a shared repertoire following the initial publication of \"The Unicode Standard\": Unicode and the ISO's Universal Coded Character Set (UCS) use identical character names and code points. However, the Unicode versions do differ from their ISO equivalents in two significant ways.\nWhile the UCS is a simple character map, Unicode specifies the rules, algorithms, and properties necessary to achieve interoperability between different platforms and languages. Thus, \"The Unicode Standard\" includes more information, covering in-depth topics such as bitwise encoding, collation, and rendering. It also provides a comprehensive catalog of character properties, including those needed for supporting bidirectional text, as well as visual charts and reference data sets to aid implementers. Previously, \"The Unicode Standard\" was sold as a print volume containing the complete core specification, standard annexes, and code charts. However, version 5.0, published in 2006, was the last version printed this way. Starting with version 5.2, only the core specification, published as a print-on-demand paperback, may be purchased. The full text, on the other hand, is published as a free PDF on the Unicode website.\nA practical reason for this publication method highlights the second significant difference between the UCS and Unicode\u2014the frequency with which updated versions are released and new characters added. \"The Unicode Standard\" has regularly released annual expanded versions, occasionally with more than one version released in a calendar year and with rare cases where the scheduled release had to be postponed. For instance, in April 2020, a month after version 13.0 was published, the Unicode Consortium announced they had changed the intended release date for version 14.0, pushing it back six months to September 2021 due to the COVID-19 pandemic.\nThus far, the following versions of \"The Unicode Standard\" have been published. Update versions, which do not include any changes to character repertoire, are signified by the third number (e.g., \"version 4.0.1\") and are omitted in the table below.\n&lt;templatestyles src=\"template:sticky header/styles.css\"/&gt;\nArchitecture and terminology.\nCodespace and code points.\n\"The Unicode Standard\" defines a \"codespace\": a sequence of integers called \"code points\" in the range from 0 to {{val|1114111}}, notated according to the standard as {{tt|U+0000}}\u2013{{tt|U+10FFFF}}. The codespace is a systematic, architecture-independent representation of \"The Unicode Standard\"; actual text is processed as binary data via one of several Unicode encodings, such as UTF-8.\nIn this normative notation, the two-character prefix codice_1 always precedes a written code point, and the code points themselves are written as hexadecimal numbers.{{Refn|The two-character prefix codice_1 was chosen as an ASCII approximation of {{unichar|U+228E}}.|group=note}} At least four hexadecimal digits are always written, with leading zeros prepended as needed. For example, the code point {{unichar|F7|Division sign}} is padded with two leading zeros, but {{unichar|13254|Egyptian hieroglyph O004}} () is not padded.\nThere are a total of {{val|1112064}} valid code points within the codespace. This number arises from the limitations of the UTF-16 character encoding, which can encode the 216 code points in the range {{tt|U+0000}} through {{tt|U+FFFF}} except for the 211 code points in the range {{tt|U+D800}} through {{tt|U+DFFF}}, which are used as surrogate pairs to encode the 220 code points in the range {{tt|U+10000}} through {{tt|U+10FFFF}}.\nCode planes and blocks.\nThe Unicode codespace is divided into 17 \"planes\", numbered 0 to 16. Plane 0 is the Basic Multilingual Plane (BMP), and contains the most commonly used characters. All code points in the BMP are accessed as a single code unit in UTF-16 encoding and can be encoded in one, two or three bytes in UTF-8. Code points in planes 1 through 16 (the \"supplementary planes\") are accessed as surrogate pairs in UTF-16 and encoded in four bytes in UTF-8.\nWithin each plane, characters are allocated within named \"blocks\" of related characters. The size of a block is always a multiple of 16, and is often a multiple of 128, but is otherwise arbitrary. Characters required for a given script may be spread out over several different, potentially disjunct blocks within the codespace.\nGeneral Category property.\nEach code point is assigned a classification, listed as the code point's General Category property. Here, at the uppermost level code points are categorized as one of Letter, Mark, Number, Punctuation, Symbol, Separator, or Other. Under each category, each code point is then further subcategorized. In most cases, other properties must be used to adequately describe all the characteristics of any given code point.\nThe {{val|1024}} points in the range {{tt|U+D800}}\u2013{{tt|U+DBFF}} are known as \"high-surrogate\" code points, and code points in the range {{tt|U+DC00}}\u2013{{tt|U+DFFF}} ({{val|1024}} code points) are known as \"low-surrogate\" code points. A high-surrogate code point followed by a low-surrogate code point forms a \"surrogate pair\" in UTF-16 in order to represent code points greater than {{tt|U+FFFF}}. In principle, these code points cannot otherwise be used, though in practice this rule is often ignored, especially when not using UTF-16.\nA small set of code points are guaranteed never to be assigned to characters, although third-parties may make independent use of them at their discretion. There are 66 of these \"noncharacters\": {{tt|U+FDD0}}\u2013{{tt|U+FDEF}} and the last two code points in each of the 17 planes (e.g. {{tt|U+FFFE}}, {{tt|U+FFFF}}, {{tt|U+1FFFE}}, {{tt|U+1FFFF}}, ..., {{tt|U+10FFFE}}, {{Tt|U+10FFFF}}). The set of noncharacters is stable, and no new noncharacters will ever be defined. Like surrogates, the rule that these cannot be used is often ignored, although the operation of the byte order mark assumes that {{tt|U+FFFE}} will never be the first code point in a text. The exclusion of surrogates and noncharacters leaves {{val|1111998}} code points available for use.\n\"Private use\" code points are considered to be assigned, but they intentionally have no interpretation specified by \"The Unicode Standard\" such that any interchange of such code points requires an independent agreement between the sender and receiver as to their interpretation. There are three private use areas in the Unicode codespace:\n\"Graphic\" characters are those defined by \"The Unicode Standard\" to have particular semantics, either having a visible glyph shape or representing a visible space. As of Unicode 17.0, there are {{val|159629}} graphic characters.\n\"Format\" characters are characters that do not have a visible appearance but may have an effect on the appearance or behavior of neighboring characters. For example, {{unichar|200C|Zero width non-joiner|nlink=}} and {{unichar|200D|Zero width joiner|nlink=}} may be used to change the default shaping behavior of adjacent characters (e.g. to inhibit ligatures or request ligature formation). There are 172 format characters in Unicode 17.0.\n65 code points, the ranges {{tt|U+0000}}\u2013{{tt|U+001F}} and {{tt|U+007F}}\u2013{{tt|U+009F}}, are reserved as \"control codes\", corresponding to the C0 and C1 control codes as defined in ISO/IEC 6429. {{tt|U+0009}} {{smallcaps|TAB}}, {{tt|U+000A}} {{smallcaps|LINE FEED}}, and {{tt|U+000D}} {{smallcaps|CARRIAGE RETURN}} are widely used in texts using Unicode. In a phenomenon known as mojibake, the C1 code points are improperly decoded according to the Windows-1252 codepage, previously widely used in Western European contexts.\nTogether, graphic, format, control code, and private use characters are collectively referred to as \"assigned characters\". \"Reserved\" code points are those code points that are valid and available for use, but have not yet been assigned. As of Unicode 17.0, there are {{val|814664}} reserved code points.\nAbstract characters.\nThe set of graphic and format characters defined by Unicode does not correspond directly to the repertoire of \"abstract characters\" representable under Unicode. Unicode encodes characters by associating an abstract character with a particular code point. However, not all abstract characters are encoded as a single Unicode character, and some abstract characters may be represented in Unicode by a sequence of two or more characters. For example, a Latin small letter \"i\" with an ogonek, a dot above, and an acute accent, which is required in Lithuanian, is represented by the character sequence {{tt|U+012F}}; {{tt|U+0307}}; {{tt|U+0301}}. Unicode maintains a list of uniquely named character sequences for abstract characters that are not directly encoded in Unicode.\nAll assigned characters have a unique and immutable name by which they are identified. This immutability has been guaranteed since version 2.0 of \"The Unicode Standard\" by its Name Stability policy. In cases where a name is seriously defective and misleading, or has a serious typographical error, a formal alias may be defined that applications are encouraged to use in place of the official character name. For example, {{unichar|A015|YI SYLLABLE WU}} has the formal alias {{sc2|YI SYLLABLE ITERATION MARK}}, and {{unichar|FE18|PRESENTATION FORM FOR VERTICAL RIGHT WHITE LENTICULAR BRAKCET|note=sic}} has the formal alias {{sc2|PRESENTATION FORM FOR VERTICAL RIGHT WHITE LENTICULAR BRACKET}}.\nReady-made versus composite characters.\nUnicode includes a mechanism for modifying characters that greatly extends the supported repertoire of glyphs. This covers the use of combining diacritical marks that may be added after the base character by the user. Multiple combining diacritics may be simultaneously applied to the same character. Unicode also contains precomposed versions of most letter/diacritic combinations in normal use. These make the conversion to and from legacy encodings simpler, and allow applications to use Unicode as an internal text format without having to implement combining characters. For example, codice_3 can be represented in Unicode as {{unichar|65|LATIN SMALL LETTER E}} followed by {{unichar|301|COMBINING ACUTE ACCENT|cwith=\u25cc}}), and equivalently as the precomposed character {{unichar|E9|LATIN SMALL LETTER E WITH ACUTE}}. Thus, users often have multiple equivalent ways of encoding the same character. The mechanism of canonical equivalence within \"The Unicode Standard\" ensures the practical interchangeability of these equivalent encodings.\nAn example of this arises with the Korean alphabet Hangul: Unicode provides a mechanism for composing Hangul syllables from their individual Hangul Jamo subcomponents. However, it also provides {{val|11172}} combinations of precomposed syllables made from the most common jamo.\nCJK characters presently only have codes for uncomposable radicals and precomposed forms. Most Han characters have either been intentionally composed from, or reconstructed as compositions of, simpler orthographic elements called radicals, so in principle Unicode could have enabled their composition as it did with Hangul. While this could have greatly reduced the number of required code points, as well as allowing the algorithmic synthesis of many arbitrary new characters, the complexities of character etymologies and the post-hoc nature of radical systems add immense complexity to the proposal. Indeed, attempts to design CJK encodings on the basis of composing radicals have been met with difficulties resulting from the reality that Chinese characters do not decompose as simply or as regularly as Hangul does.\nThe CJK Radicals Supplement block is assigned to the range {{tt|U+2E80}}\u2013{{tt|U+2EFF}}, and the Kangxi radicals are assigned to {{tt|U+2F00}}\u2013{{tt|U+2FDF}}. The Ideographic Description Sequences block covers the range {{tt|U+2FF0}}\u2013{{tt|U+2FFB}}, but \"The Unicode Standard\" warns against using its characters as an alternate representation for characters encoded elsewhere:\nLigatures.\nMany scripts, including Arabic and Devan\u0101gar\u012b, have special orthographic rules that require certain combinations of letterforms to be combined into special ligature forms. The rules governing ligature formation can be quite complex, requiring special script-shaping technologies such as ACE (Arabic Calligraphic Engine by DecoType in the 1980s and used to generate all the Arabic examples in the printed editions of \"The Unicode Standard\"), which became the proof of concept for OpenType (by Adobe and Microsoft), Graphite (by SIL International), or AAT (by Apple).\nInstructions are also embedded in fonts to tell the operating system how to properly output different character sequences. A simple solution to the placement of combining marks or diacritics is assigning the marks a width of zero and placing the glyph itself to the left or right of the left sidebearing (depending on the direction of the script they are intended to be used with). A mark handled this way will appear over whatever character precedes it, but will not adjust its position relative to the width or height of the base glyph; it may be visually awkward and it may overlap some glyphs. Real stacking is impossible but can be approximated in limited cases (for example, Thai top-combining vowels and tone marks can just be at different heights to start with). Generally, this approach is only effective in monospaced fonts but may be used as a fallback rendering method when more complex methods fail.\nStandardized subsets.\nSeveral subsets of Unicode are standardized: Microsoft Windows since Windows NT 4.0 supports WGL-4 with 657 characters, which is considered to support all contemporary European languages using the Latin, Greek, or Cyrillic script. Other standardized subsets of Unicode include the Multilingual European Subsets: MES-1 (Latin scripts only; 335 characters), MES-2 (Latin, Greek, and Cyrillic; 1062 characters) and MES-3A &amp; MES-3B (two larger subsets, not shown here). MES-2 includes every character in MES-1 and WGL-4.\nThe standard DIN 91379 specifies a subset of Unicode letters, special characters, and sequences of letters and diacritic signs to allow the correct representation of names and to simplify data exchange in Europe. This standard supports all of the official languages of all European Union countries, as well as the German minority languages and the official languages of Iceland, Liechtenstein, Norway, and Switzerland. To allow the transliteration of names in other writing systems to the Latin script according to the relevant ISO standards, all necessary combinations of base letters and diacritic signs are provided.\nRendering software that cannot process a Unicode character appropriately often displays it as an open rectangle, or as {{tt|U+FFFD}} to indicate the position of the unrecognized character. Some systems have made attempts to provide more information about such characters. Apple's Last Resort font will display a substitute glyph indicating the Unicode range of the character, and the SIL International's Unicode fallback font will display a box showing the hexadecimal scalar value of the character.\n{{anchor|UTF|UCS}}Mapping and encodings.\nSeveral mechanisms have been specified for storing a series of code points as a series of bytes.\nUnicode defines two mapping methods: the Unicode Transformation Format (UTF) encodings, and the Universal Coded Character Set (UCS) encodings. An encoding maps (possibly a subset of) the range of Unicode \"code points\" to sequences of values in some fixed-size range, termed \"code units\". All UTF encodings map code points to a unique sequence of bytes. The numbers in the names of the encodings indicate the number of bits per code unit (for UTF encodings) or the number of bytes per code unit (for UCS encodings and UTF-1). UTF-8 and UTF-16 are the most commonly used encodings. UCS-2 is an obsolete subset of UTF-16; UCS-4 and UTF-32 are functionally equivalent.\nUTF encodings include:\nUTF-8 uses one to four 8-bit units (\"bytes\") per code point and, being compact for Latin scripts and ASCII-compatible, provides the de facto standard encoding for the interchange of Unicode text. It is used by FreeBSD and most recent Linux distributions as a direct replacement for legacy encodings in general text handling.\nThe UCS-2 and UTF-16 encodings specify the Unicode byte order mark (BOM) for use at the beginnings of text files, which may be used for byte-order detection (or byte endianness detection). The BOM, encoded as {{unichar|FEFF|Byte order mark}}, has the important property of unambiguity on byte reorder, regardless of the Unicode encoding used; {{tt|U+FFFE}} (the result of byte-swapping {{tt|U+FEFF}}) does not equate to a legal character, and {{tt|U+FEFF}} in places other than the beginning of text conveys the zero-width non-break space.\nThe same character converted to UTF-8 becomes the byte sequence codice_4. \"The Unicode Standard\" allows the BOM \"can serve as a signature for UTF-8 encoded text where the character set is unmarked\". Some software developers have adopted it for other encodings, including UTF-8, in an attempt to distinguish UTF-8 from local 8-bit code pages. However {{IETF RFC|3629}}, the UTF-8 standard, recommends that byte order marks be forbidden in protocols using UTF-8, but discusses the cases where this may not be possible. In addition, the large restriction on possible patterns in UTF-8 (for instance there cannot be any lone bytes with the high bit set) means that it should be possible to distinguish UTF-8 from other character encodings without relying on the BOM.\nIn UTF-32 and UCS-4, one 32-bit code unit serves as a fairly direct representation of any character's code point (although the endianness, which varies across different platforms, affects how the code unit manifests as a byte sequence). In the other encodings, each code point may be represented by a variable number of code units. UTF-32 is widely used as an internal representation of text in programs (as opposed to stored or transmitted text), since every Unix operating system that uses the GCC compilers to generate software uses it as the standard \"wide character\" encoding. Recent versions of the Python programming language (beginning with 2.2) may also be configured to use UTF-32 as the representation for Unicode strings, effectively disseminating such encoding in high-level coded software.\nPunycode, another encoding form, enables the encoding of Unicode strings into the limited character set supported by the ASCII-based Domain Name System (DNS). The encoding is used as part of IDNA, which is a system enabling the use of Internationalized Domain Names in all scripts that are supported by Unicode. Earlier and now historical proposals include UTF-5 and UTF-6.\nGB18030 is another encoding form for Unicode, from the Standardization Administration of China. It is the official character set of the People's Republic of China (PRC). BOCU-1 and SCSU are Unicode compression schemes. The April Fools' Day RFC of 2005 specified two parody UTF encodings, UTF-9 and UTF-18.\nAdoption.\nUnicode, in the form of UTF-8, has been the most common encoding for the World Wide Web since 2008. It has near-universal adoption, and much of the non-UTF-8 content is found in other Unicode encodings, e.g. UTF-16. {{As of|2024}}, UTF-8 accounts for on average 98.3% of all web pages (and 983 of the top 1,000 highest-ranked web pages). Although many pages only use ASCII characters to display content, UTF-8 was designed with 8-bit ASCII as a subset and almost no websites now declare their encoding to only be ASCII instead of UTF-8. Over a third of the languages tracked have 100% UTF-8 use.\nAll internet protocols maintained by Internet Engineering Task Force, e.g. File Transfer Protocol (FTP), have required support for UTF-8 since the publication of {{IETF RFC|2277}} in 1998, which specified that all IETF protocols \"MUST be able to use the UTF-8 charset\".\nOperating systems.\nUnicode has become the dominant scheme for the internal processing and storage of text. Although a great deal of text is still stored in legacy encodings, Unicode is used almost exclusively for building new information processing systems. Early adopters tended to use UCS-2 (the fixed-length two-byte obsolete precursor to UTF-16) and later moved to UTF-16 (the variable-length current standard), as this was the least disruptive way to add support for non-BMP characters. The best known such system is Windows NT (and its descendants, 2000, XP, Vista, 7, 8, 10, and 11), which uses UTF-16 as the sole internal character encoding. The Java and .NET bytecode environments, macOS, and KDE also use it for internal representation. Partial support for Unicode can be installed on Windows 9x through the Microsoft Layer for Unicode.\nUTF-8 (originally developed for Plan 9) has become the main storage encoding on most Unix-like operating systems (though others are also used by some libraries) because it is a relatively easy replacement for traditional extended ASCII character sets. UTF-8 is also the most common Unicode encoding used in HTML documents on the World Wide Web.\nMultilingual text-rendering engines which use Unicode include Uniscribe and DirectWrite for Microsoft Windows, ATSUI and Core Text for macOS, and Pango for GTK+ and the GNOME desktop.\nInput methods.\nBecause keyboard layouts cannot have simple key combinations for all characters, several operating systems provide alternative input methods that allow access to the entire repertoire.\nISO/IEC 14755, which standardises methods for entering Unicode characters from their code points, specifies several methods. There is the \"Basic method\", where a \"beginning sequence\" is followed by the hexadecimal representation of the code point and the \"ending sequence\". There is also a \"screen-selection entry method\" specified, where the characters are listed in a table on a screen, such as with a character map program.\nOnline tools for finding the code point for a known character include Unicode Lookup by Jonathan Hedley and Shapecatcher by Benjamin Milde. In Unicode Lookup, one enters a search key (e.g. \"fractions\"), and a list of corresponding characters with their code points is returned. In Shapecatcher, based on Shape context, one draws the character in a box and a list of characters approximating the drawing, with their code points, is returned.\nEmail.\nMIME defines two different mechanisms for encoding non-ASCII characters in email, depending on whether the characters are in email headers (such as the \"Subject:\"), or in the text body of the message; in both cases, the original character set is identified as well as a transfer encoding. For email transmission of Unicode, the UTF-8 character set and the Base64 or the Quoted-printable transfer encoding are recommended, depending on whether much of the message consists of ASCII characters. The details of the two different mechanisms are specified in the MIME standards and generally are hidden from users of email software.\nThe IETF has defined a framework for internationalized email using UTF-8, and has updated several protocols in accordance with that framework.\nThe adoption of Unicode in email has been very slow.{{citation needed|date=November 2022}} Some East Asian text is still encoded in encodings such as ISO-2022, and some devices, such as mobile phones,{{citation needed|reason=is this outdated?|date=November 2022}} still cannot correctly handle Unicode data. Support has been improving, however. Many major free mail providers such as Yahoo! Mail, Gmail, and Outlook.com support it.\nWeb.\nAll W3C recommendations have used Unicode as their \"document character set\" since HTML 4.0. Web browsers have supported Unicode, especially UTF-8, for many years. There used to be display problems resulting primarily from font related issues; e.g. v6 and older of Microsoft Internet Explorer did not render many code points unless explicitly told to use a font that contains them.\nAlthough syntax rules may affect the order in which characters are allowed to appear, XML (including XHTML) documents, by definition, comprise characters from most of the Unicode code points, with the exception of:\nHTML characters manifest either directly as bytes according to the document's encoding, if the encoding supports them, or users may write them as numeric character references based on the character's Unicode code point. For example, the references codice_5, codice_6, codice_7, codice_8, codice_9, codice_10, codice_11, codice_12, and codice_13 (or the same numeric values expressed in hexadecimal, with codice_14 as the prefix) should display on all browsers as \u0394, \u0419, \u05e7 ,\u0645, \u0e57, \u3042, \u53f6, \u8449, and \ub9d0.\nWhen specifying URIs, for example as URLs in HTTP requests, non-ASCII characters must be percent-encoded.\nFonts.\nUnicode is not in principle concerned with fonts \"per se\", seeing them as implementation choices. Any given character may have many allographs, from the more common bold, italic and base letterforms to complex decorative styles. A font is \"Unicode compliant\" if the glyphs in the font can be accessed using code points defined in \"The Unicode Standard\". The standard does not specify a minimum number of characters that must be included in the font; some fonts have quite a small repertoire.\nFree and retail fonts based on Unicode are widely available, since TrueType and OpenType support Unicode (and Web Open Font Format (WOFF and WOFF2) is based on those). These font formats map Unicode code points to glyphs, but OpenType and TrueType font files are restricted to 65,535 glyphs. Collection files provide a \"gap mode\" mechanism for overcoming this limit in a single font file. (Each font within the collection still has the 65,535 limit, however.) A TrueType Collection file would typically have a file extension of \".ttc\".\nThousands of fonts exist on the market, but fewer than a dozen fonts\u2014sometimes described as \"pan-Unicode\" fonts\u2014attempt to support the majority of Unicode's character repertoire. Instead, Unicode-based fonts typically focus on supporting only basic ASCII and particular scripts or sets of characters or symbols. Several reasons justify this approach: applications and documents rarely need to render characters from more than one or two writing systems; fonts tend to demand resources in computing environments; and operating systems and applications show increasing intelligence in regard to obtaining glyph information from separate font files as needed, i.e., font substitution. Furthermore, designing a consistent set of rendering instructions for tens of thousands of glyphs constitutes a monumental task; such a venture passes the point of diminishing returns for most typefaces.\nNewlines.\nUnicode partially addresses the newline problem that occurs when trying to read a text file on different platforms. Unicode defines a large number of characters that conforming applications should recognize as line terminators.\nIn terms of the newline, Unicode introduced {{unichar|2028|LINE SEPARATOR}} and {{unichar|2029|PARAGRAPH SEPARATOR}}. This was an attempt to provide a Unicode solution to encoding paragraphs and lines semantically, potentially replacing all of the various platform solutions. In doing so, Unicode does provide a way around the historical platform-dependent solutions. Nonetheless, few if any Unicode solutions have adopted these Unicode line and paragraph separators as the sole canonical line ending characters. However, a common approach to solving this issue is through newline normalization. This is achieved with the Cocoa text system in macOS and also with W3C XML and HTML recommendations. In this approach, every possible newline character is converted internally to a common newline (which one does not really matter since it is an internal operation just for rendering). In other words, the text system can correctly treat the character as a newline, regardless of the input's actual encoding.\nIssues.\nCharacter unification.\nHan unification.\nThe Ideographic Research Group (IRG) is tasked with advising the Consortium and ISO regarding Han unification, or Unihan, especially the further addition of CJK unified and compatibility ideographs to the repertoire. The IRG is composed of experts from each region that has historically used Chinese characters. However, despite the deliberation within the committee, Han unification has consistently been one of the most contested aspects of \"The Unicode Standard\" since the genesis of the project.\nExisting character set standards such as the Japanese JIS X 0208 (encoded by Shift JIS) defined unification criteria, meaning rules for determining when a variant Chinese character is to be considered a handwriting/font difference (and thus unified), versus a spelling difference (to be encoded separately). Unicode's character model for CJK characters was based on the unification criteria used by JIS X 0208, as well as those developed by the Association for a Common Chinese Code in China.\nDue to the standard's principle of encoding semantic instead of stylistic variants, Unicode has received criticism for not assigning code points to certain rare and archaic kanji variants, possibly complicating processing of ancient and uncommon Japanese names. Since it places particular emphasis on Chinese, Japanese and Korean sharing many characters in common, Han unification is also sometimes perceived as treating the three as the same thing. Regional differences in the expected forms of characters, in terms of typographical conventions and curricula for handwriting, do not always fall along language boundaries: although Hong Kong and Taiwan both write Chinese languages using Traditional Chinese characters, the preferred forms of characters differ between Hong Kong and Taiwan in some cases.\nLess-frequently-used alternative encodings exist, often predating Unicode, with character models differing from this paradigm, aimed at preserving the various stylistic differences between regional and/or nonstandard character forms. One example is the TRON Code favored by some users for handling historical Japanese text, though not widely adopted among the Japanese public. Another is the CCCII encoding adopted by library systems in Hong Kong, Taiwan and the United States. These have their own drawbacks in general use, leading to the Big5 encoding (introduced in 1984, four years after CCCII) having become more common than CCCII outside of library systems. Although work at Apple based on Research Libraries Group's CJK Thesaurus, which was used to maintain the EACC variant of CCCII, was one of the direct predecessors of Unicode's Unihan set, Unicode adopted the JIS-style unification model.\nThe earliest version of Unicode had a repertoire of fewer than 21,000 Han characters, largely limited to those in relatively common modern usage. As of version 17.0, the standard now encodes more than 101,000 Han characters, and work is continuing to add thousands more\u2014largely historical and dialectal variant characters used throughout the Sinosphere.\nModern typefaces provide a means to address some of the practical issues in depicting unified Han characters with various regional graphical representations. The 'locl' OpenType table allows a renderer to select a different glyph for each code point based on the text locale. The Unicode variation sequences can also provide in-text annotations for a desired glyph selection; this requires registration of the specific variant in the Ideographic Variation Database.\nItalic or cursive characters in Cyrillic.\nIf the appropriate glyphs for characters in the same script differ only in the italic, Unicode has generally unified them, as can be seen in the comparison among a set of seven characters' italic glyphs as typically appearing in Russian, traditional Bulgarian, Macedonian, and Serbian texts at right, meaning that the differences are displayed through smart font technology or manually changing fonts. The same OpenType 'locl' technique is used.\nLocalised case pairs.\nFor use in the Turkish alphabet and Azeri alphabet, Unicode includes a separate dotless lowercase {{serif|I}} (\u0131) and a dotted uppercase {{serif|I}} ({{serif|\u0130}}). However, the usual ASCII letters are used for the lowercase dotted i and the uppercase dotless {{serif|I}}, matching how they are handled in the earlier ISO 8859-9. As such, case-insensitive comparisons for those languages have to use different rules than case-insensitive comparisons for other languages using the Latin script. This can have security implications if, for example, sanitization code or access control relies on case-insensitive comparison.\nBy contrast, the Icelandic eth (\u00f0), the barred D (\u0111) and the retroflex D (\u0256), which usually{{efn|Rarely, the uppercase Icelandic eth may instead be written in an insular style (\ua779) with the crossbar positioned on the stem, particularly if it needs to be distinguished from the uppercase retroflex D (see African Reference Alphabet).|group=note}} look the same in uppercase (\u0110), are given the opposite treatment, and encoded separately in both letter-cases (in contrast to the earlier ISO 6937, which unifies the uppercase forms). Although it allows for case-insensitive comparison without needing to know the language of the text, this approach also has issues, requiring security measures relating to homoglyph attacks.\nDiacritics on lowercase {{serif|I}}.\nWhether the lowercase letter {{serif|I}} is expected to retain its tittle when a diacritic applies also depends on local conventions.\nSecurity.\nUnicode has a large number of homoglyphs, many of which look very similar or identical to ASCII letters. Substitution of these can make an identifier or URL that looks correct, but directs to a different location than expected. Additionally, homoglyphs can also be used for manipulating the output of natural language processing (NLP) systems. Mitigation requires disallowing these characters, displaying them differently, or requiring that they resolve to the same identifier; all of this is complicated due to the huge and constantly changing set of characters.\nA security advisory was released in 2021 by two researchers, one from the University of Cambridge and the other from the University of Edinburgh, in which they assert that the BiDi marks can be used to make large sections of code do something different from what they appear to do. The problem was named \"Trojan Source\". In response, code editors started highlighting marks to indicate forced text-direction changes.\nThe UTF-8 and UTF-16 encodings do not accept all possible sequences of code units. Implementations vary in what they do when reading an invalid sequence, which has led to security bugs.\nMapping to legacy character sets.\nUnicode was designed to provide code-point-by-code-point round-trip format conversion to and from any preexisting character encodings, so that text files in older character sets can be converted to Unicode and then back and get back the same file, without employing context-dependent interpretation. That has meant that inconsistent legacy architectures, such as combining diacritics and precomposed characters, both exist in Unicode, giving more than one method of representing some text. This is most pronounced in the three different encoding forms for Korean Hangul. Since version 3.0, any precomposed characters that can be represented by a combined sequence of already existing characters can no longer be added to the standard to preserve interoperability between software using different versions of Unicode.\nInjective mappings must be provided between characters in existing legacy character sets and characters in Unicode to facilitate conversion to Unicode and allow interoperability with legacy software. Lack of consistency in various mappings between earlier Japanese encodings such as Shift-JIS or EUC-JP and Unicode led to round-trip format conversion mismatches, particularly the mapping of the character JIS X 0208 '\uff5e' (1-33, WAVE DASH), heavily used in legacy database data, to either {{unichar|FF5E|FULLWIDTH TILDE}} (in Microsoft Windows) or {{unichar|301C|WAVE DASH}} (other vendors).\nSome Japanese computer programmers objected to Unicode because it requires them to separate the use of {{unichar|005C|REVERSE SOLIDUS|note=backslash}} and {{unichar|00A5|YEN SIGN}}, which was mapped to 0x5C in JIS X 0201, and a lot of legacy code exists with this usage. (This encoding also replaces tilde '~' 0x7E with macron '\u00af', now 0xAF.) The separation of these characters exists in ISO 8859-1, from long before Unicode.\nIndic scripts.\nIndic scripts such as Tamil and Devanagari are each allocated only 128 code points, matching the ISCII standard. The correct rendering of Unicode Indic text requires transforming the stored logical order characters into visual order and the forming of ligatures (also known as conjuncts) out of components. Some local scholars argued in favor of assignments of Unicode code points to these ligatures, going against the practice for other writing systems, though Unicode contains some Arabic and other ligatures for backward compatibility purposes only. Encoding of any new ligatures in Unicode will not happen, in part, because the set of ligatures is font-dependent, and Unicode is an encoding independent of font variations. The same kind of issue arose for the Tibetan script in 2003 when the Standardization Administration of China proposed encoding 956 precomposed Tibetan syllables, but these were rejected for encoding by the relevant ISO committee (ISO/IEC JTC 1/SC 2).\nThai alphabet support has been criticized for its ordering of Thai characters. The vowels \u0e40, \u0e41, \u0e42, \u0e43, \u0e44 that are written to the left of the preceding consonant are in visual order instead of phonetic order, unlike the Unicode representations of other Indic scripts. This complication is due to Unicode inheriting the Thai Industrial Standard 620, which worked in the same way, and was the way in which Thai had always been written on keyboards. This ordering problem complicates the Unicode collation process slightly, requiring table lookups to reorder Thai characters for collation. Even if Unicode had adopted encoding according to spoken order, it would still be problematic to collate words in dictionary order. E.g., the word {{Wikt-lang|th|\u0e41\u0e2a\u0e14\u0e07}} {{IPA|th|sa d\u025b\u02d0\u014b|}} \"perform\" starts with a consonant cluster \"\u0e2a\u0e14\" (with an inherent vowel for the consonant \"\u0e2a\"), the vowel \u0e41-, in spoken order would come after the \u0e14, but in a dictionary, the word is collated as it is written, with the vowel following the \u0e2a.\nCombining characters.\nCharacters with diacritical marks can generally be represented either as a single precomposed character or as a decomposed sequence of a base letter plus one or more non-spacing marks. For example, \u1e17 (precomposed e with macron and acute above) and \u0113\u0301 (e followed by the combining macron above and combining acute above) should be rendered identically, both appearing as an e with a macron (\u25cc\u0304) and acute accent (\u25cc\u0301), but in practice, their appearance may vary depending upon what rendering engine and fonts are being used to display the characters. Similarly, underdots, as needed in the romanization of Indic languages, will often be placed incorrectly.{{Citation needed|date=July 2011}} Unicode characters that map to precomposed glyphs can be used in many cases, thus avoiding the problem, but where no precomposed character has been encoded, the problem can often be solved by using a specialist Unicode font such as Charis SIL that uses Graphite, OpenType ('gsub'), or AAT technologies for advanced rendering features.\nAnomalies.\n\"The Unicode Standard\" has imposed rules intended to guarantee stability. Depending on the strictness of a rule, a change can be prohibited or allowed. For example, a \"name\" given to a code point cannot and will not change. But a \"script\" property is more flexible, by Unicode's own rules. In version 2.0, Unicode changed many code point \"names\" from version 1. At the same moment, Unicode stated that, thenceforth, an assigned name to a code point would never change. This implies that when mistakes are published, these mistakes cannot be corrected, even if they are trivial (as happened in one instance with the spelling {{sc2|{{typo|BRAKCET}}}} for {{sc2|BRACKET}} in a character name). In 2006 a list of anomalies in character names was first published, and, as of June 2021, there were 104 characters with identified issues, for example:\nWhile Unicode defines the script designator (name) to be \"{{tt|Phags_Pa}}\", in that script's character names, a hyphen is added: {{Unichar|A840|PHAGS-PA LETTER KA}}. This, however, is not an anomaly, but the rule: hyphens are replaced by underscores in script designators."}
{"id": "31743", "revid": "28928298", "url": "https://en.wikipedia.org/wiki?curid=31743", "title": "Uranium", "text": "element with atomic number 92 (U)\nUranium is a chemical element; it has symbol U and atomic number 92. It is a silvery-grey metal in the actinide series of the periodic table. A uranium atom has 92 protons and 92 electrons, of which 6 are valence electrons. Uranium radioactively decays, usually by emitting an alpha particle. The half-life of this decay varies between 159,200 and 4.5\u00a0billion years for different isotopes, making them useful for dating the age of the Earth. The most common isotopes in natural uranium are uranium-238 (which has 146 neutrons and accounts for over 99% of uranium on Earth) and uranium-235 (which has 143 neutrons). Uranium has the highest atomic weight of the primordially occurring elements. Its density is about 70% higher than that of lead and slightly lower than that of gold or tungsten. It occurs naturally in low concentrations of a few parts per million in soil, rock and water, and is commercially extracted from uranium-bearing minerals such as uraninite.\nMany contemporary uses of uranium exploit its unique nuclear properties. Uranium is used in nuclear power plants and nuclear weapons because it is the only naturally occurring element with a fissile isotope \u2013 uranium-235 \u2013 present in non-trace amounts. However, because of the low abundance of uranium-235 in natural uranium (which is overwhelmingly uranium-238), uranium needs to undergo enrichment so that enough uranium-235 is present. Uranium-238 is fissionable by fast neutrons and is fertile, meaning it can be transmuted to fissile plutonium-239 in a nuclear reactor. Another fissile isotope, uranium-233, can be produced from natural thorium and is studied for future industrial use in nuclear technology. Uranium-238 has a small probability for spontaneous fission or even induced fission with fast neutrons; uranium-235, and to a lesser degree uranium-233, have a much higher fission cross-section for slow neutrons. In sufficient concentration, these isotopes maintain a sustained nuclear chain reaction. This generates the heat in nuclear power reactors and produces the fissile material for nuclear weapons. The primary civilian use for uranium harnesses the heat energy to produce electricity. Depleted uranium (238U) is used in kinetic energy penetrators and armor plating.\nThe 1789 discovery of uranium in the mineral pitchblende is credited to Martin Heinrich Klaproth, who named the new element after the recently discovered planet Uranus. Eug\u00e8ne-Melchior P\u00e9ligot was the first person to isolate the metal, and its radioactive properties were discovered in 1896 by Henri Becquerel. Research by Otto Hahn, Lise Meitner, Enrico Fermi and others, such as J. Robert Oppenheimer starting in 1934 led to its use as a fuel in the nuclear power industry and in Little Boy, the first nuclear weapon used in war. An ensuing arms race during the Cold War between the United States and the Soviet Union produced tens of thousands of nuclear weapons that used uranium metal and uranium-derived plutonium-239. Dismantling of these weapons and related nuclear facilities is carried out within various nuclear disarmament programs and costs billions of dollars. Weapon-grade uranium obtained from nuclear weapons is diluted with uranium-238 and reused as fuel for nuclear reactors. Spent nuclear fuel forms radioactive waste, which mostly consists of uranium-238 and poses a significant health threat and environmental impact.\nCharacteristics.\nUranium is a silvery white, weakly radioactive metal. It has a Mohs hardness of 6, sufficient to scratch glass and roughly equal to that of titanium, rhodium, manganese and niobium. It is malleable, ductile, slightly paramagnetic, strongly electropositive and a poor electrical conductor. Uranium metal has a very high density of 19.1\u00a0g/cm3, denser than lead (11.3\u00a0g/cm3), but slightly less dense than tungsten and gold (19.3\u00a0g/cm3).\nUranium metal reacts with almost all non-metallic elements (except noble gases) and their compounds, with reactivity increasing with temperature. Hydrochloric and nitric acids dissolve uranium, but non-oxidizing acids other than hydrochloric acid attack the element very slowly. When finely divided, it can react with cold water; in air, uranium metal becomes coated with a dark layer of uranium dioxide. Uranium in ores is extracted chemically and converted into uranium dioxide or other chemical forms usable in industry.\nIn 1938, Otto Hahn and Fritz Strassman discovered that barium was a product of bombarding Uranium-235 with neutrons, and a year later Lise Meitner and Otto Robert Frisch developed the theory of nuclear fission to explain this new phenomenon, making U-235 the first fissile isotope to be discovered. On bombardment with slow neutrons, uranium-235 most of the time splits into two smaller nuclei, releasing nuclear binding energy and more neutrons. If too many of these neutrons are absorbed by other uranium-235 nuclei, a nuclear chain reaction occurs that results in a burst of heat or (in some circumstances) an explosion. In a nuclear reactor, such a chain reaction is slowed and controlled by a neutron poison, absorbing some of the free neutrons. Such neutron absorbent materials are often part of reactor control rods (see nuclear reactor physics for a description of this process of reactor control). Other naturally occurring isotopes such as Uranium-238 are fissionable, but not fissile, meaning that they only undergo fission when absorbing high energy (fast) neutrons.\nAs little as of uranium-235 can be used to make an atomic bomb. The nuclear weapon detonated over Hiroshima, called Little Boy, relied on uranium fission. However, the first nuclear bomb (the \"Gadget\" used at Trinity) and the bomb that was detonated over Nagasaki (Fat Man) were both plutonium bombs.\nUranium metal has three allotropic forms:\nApplications.\nMilitary.\nThe major application of uranium in the military sector is in high-density penetrating projectiles. This ammunition consists of depleted uranium (DU) alloyed with 1\u20132% other elements, such as titanium or molybdenum. At high impact speed, the density, hardness, and pyrophoricity of the projectile enable the destruction of heavily armored targets. Tank armor and other removable vehicle armor can also be hardened with depleted uranium plates. The use of depleted uranium became politically and environmentally contentious after the use of such munitions by the US, UK and other countries during wars in the Persian Gulf and the Balkans raised health questions concerning uranium compounds left in the soil (see Gulf War syndrome).\nDepleted uranium is also used as a shielding material in some containers used to store and transport radioactive materials. While the metal itself is radioactive, its high density makes it more effective than lead in halting radiation from strong sources such as radium. Other uses of depleted uranium include counterweights for aircraft control surfaces, as ballast for missile re-entry vehicles and as a shielding material. Due to its high density, this material is found in inertial guidance systems and in gyroscopic compasses. Depleted uranium is preferred over similarly dense metals due to its ability to be easily machined and cast as well as its relatively low cost. The main risk of exposure to depleted uranium is chemical poisoning by uranium oxide rather than radioactivity (uranium being only a weak alpha emitter).\nDuring the later stages of World War II, the entire Cold War, and to a lesser extent afterwards, uranium-235 has been used as the fissile explosive material to produce nuclear weapons. Initially, two major types of fission bombs were built: a relatively simple device that uses uranium-235 and a more complicated mechanism that uses plutonium-239 derived from uranium-238. Later, a much more complicated and far more powerful type of fission/fusion bomb (thermonuclear weapon) was built, that uses a plutonium-based device to cause a mixture of tritium and deuterium to undergo nuclear fusion. Such bombs are jacketed in a non-fissile (unenriched) uranium case, and they derive more than half their power from the fission of this material by fast neutrons from the nuclear fusion process.\nCivilian.\nThe main use of uranium in the civilian sector is to fuel nuclear power plants. One kilogram of uranium-235 can theoretically produce about 20\u00a0terajoules of energy (2\u00d71013\u00a0joules), assuming complete fission; as much energy as 1.5\u00a0million kilograms (1,500 tonnes) of coal.\nCommercial nuclear power plants use fuel that is typically enriched to around 3% uranium-235. The CANDU and Magnox designs are the only commercial reactors capable of using unenriched uranium fuel. Fuel used for United States Navy reactors is typically highly enriched in uranium-235 (the exact values are classified). In a breeder reactor, uranium-238 can also be converted into plutonium-239 through the following reaction:\n[&lt;noinclude /&gt;[uranium-238|U]&lt;noinclude /&gt;] + n \u2192 [&lt;noinclude /&gt;[uranium-239|U]&lt;noinclude /&gt;] + \u03b3 \u00a0\u03b2\u2212?? [&lt;noinclude /&gt;[neptunium-239|Np]&lt;noinclude /&gt;] \u00a0\u03b2\u2212?? [&lt;noinclude /&gt;[plutonium-239|Pu]&lt;noinclude /&gt;]\nBefore (and, occasionally, after) the discovery of radioactivity, uranium was primarily used in small amounts for yellow glass and pottery glazes, such as uranium glass and in Fiestaware.\nThe discovery and isolation of radium in uranium ore (pitchblende) by Marie Curie sparked the development of uranium mining to extract the radium, which was used to make glow-in-the-dark paints for clock and aircraft dials. This left a prodigious quantity of uranium as a waste product, since it takes three tonnes of uranium to extract one gram of radium. This waste product was diverted to the glazing industry, making uranium glazes very inexpensive and abundant. Besides the pottery glazes, uranium tile glazes accounted for the bulk of the use, including common bathroom and kitchen tiles which can be produced in green, yellow, mauve, black, blue, red and other colors.\nUranium was also used in photographic chemicals (especially uranium nitrate as a toner), in lamp filaments for stage lighting bulbs, to improve the appearance of dentures, and in the leather and wood industries for stains and dyes. Uranium salts are mordants of silk or wool. Uranyl acetate and uranyl formate are used as electron-dense \"stains\" in transmission electron microscopy, to increase the contrast of biological specimens in ultrathin sections and in negative staining of viruses, isolated cell organelles and macromolecules.\nThe discovery of the radioactivity of uranium ushered in additional scientific and practical uses of the element. The long half-life of uranium-238 (4.47\u00d7109 years) makes it well-suited for use in estimating the age of the earliest igneous rocks and for other types of radiometric dating, including uranium\u2013thorium dating, uranium\u2013lead dating and uranium\u2013uranium dating. Uranium metal is used for X-ray targets in the making of high-energy X-rays.\nHistory.\nPre-discovery use.\nThe use of pitchblende, uranium in its natural oxide form, dates back to at least the year 79 AD, when it was used in the Roman Empire to add a yellow color to ceramic glazes. Yellow glass with 1% uranium oxide was found in a Roman villa on Cape Posillipo in the Gulf of Naples, Italy, by R. T. Gunther of the University of Oxford in 1912. Starting in the late Middle Ages, pitchblende was extracted from the Habsburg silver mines in Joachimsthal, Bohemia (now J\u00e1chymov in the Czech Republic) in the Ore Mountains, and was used as a coloring agent in the local glassmaking industry. In the early 19th century, the world's only known sources of uranium ore were these mines.\nDiscovery.\nThe discovery of the element is credited to the German chemist Martin Heinrich Klaproth. While he was working in his experimental laboratory in Berlin in 1789, Klaproth was able to precipitate a yellow compound (likely sodium diuranate) by dissolving pitchblende in nitric acid and neutralizing the solution with sodium hydroxide. Klaproth assumed the yellow substance was the oxide of a yet-undiscovered element and heated it with charcoal to obtain a black powder, which he thought was the newly discovered metal itself (in fact, that powder was an oxide of uranium). He named the newly discovered element \"Uranit\" after the planet Uranus (named after the primordial Greek god of the sky), which had been discovered eight years earlier by William Herschel. He later renamed it \"Uranium\" to conform to the naming standard.\nIn 1841, Eug\u00e8ne-Melchior P\u00e9ligot, Professor of Analytical Chemistry at the Conservatoire National des Arts et M\u00e9tiers (Central School of Arts and Manufactures) in Paris, isolated the first sample of uranium metal by heating uranium tetrachloride with potassium.\nHenri Becquerel discovered radioactivity by using uranium in 1896. Becquerel made the discovery in Paris by leaving a sample of a uranium salt, K2UO2(SO4)2 (potassium uranyl sulfate), on top of an unexposed photographic plate in a drawer and noting that the plate had become \"fogged\". He determined that a form of invisible light or rays emitted by uranium had exposed the plate.\nDuring World War I when the Central Powers suffered a shortage of molybdenum to make artillery gun barrels and high speed tool steels, they routinely used ferrouranium alloy as a substitute, as it presents many of the same physical characteristics as molybdenum. When this practice became known in 1916 the US government requested several prominent universities to research the use of uranium in manufacturing and metalwork. Tools made with these formulas remained in use for several decades, until the Manhattan Project and the Cold War placed a large demand on uranium for fission research and weapon development.\nFission research.\nA team led by Enrico Fermi in 1934 found that bombarding uranium with neutrons produces beta rays (electrons or positrons from the elements produced; see beta particle). The fission products were at first mistaken for new elements with atomic numbers 93 and 94, which the Dean of the Sapienza University of Rome, Orso Mario Corbino, named ausenium and hesperium, respectively. The experiments leading to the discovery of uranium's ability to fission (break apart) into lighter elements and release binding energy were conducted by Otto Hahn and Fritz Strassmann in Hahn's laboratory in Berlin. Lise Meitner and her nephew, physicist Otto Robert Frisch, published the physical explanation in February 1939 and named the process \"nuclear fission\". Soon after, Fermi hypothesized that fission of uranium might release enough neutrons to sustain a fission reaction. Confirmation of this hypothesis came in 1939, and later work found that on average about 2.5 neutrons are released by each fission of uranium-235. Fermi urged Alfred O. C. Nier to separate uranium isotopes for determination of the fissile component, and on 29 February 1940, Nier used an instrument he built at the University of Minnesota to separate the world's first uranium-235 sample in the Tate Laboratory. Using Columbia University's cyclotron, John Dunning confirmed the sample to be the isolated fissile material on 1 March. Further work found that the far more common uranium-238 isotope can be transmuted into plutonium, which, like uranium-235, is also fissile by thermal neutrons. These discoveries led numerous countries to begin working on the development of nuclear weapons and nuclear power. Despite fission having been discovered in Germany, the \"Uranverein\" (\"uranium club\") Germany's wartime project to research nuclear power and/or weapons was hampered by limited resources, infighting, the exile or non-involvement of several prominent scientists in the field and several crucial mistakes such as failing to account for impurities in available graphite samples which made it appear less suitable as a neutron moderator than it is in reality. Germany's attempts to build a natural uranium / heavy water reactor had not come close to reaching criticality by the time the Americans reached Haigerloch, the site of the last German wartime reactor experiment.\nOn 2 December 1942, as part of the Manhattan Project, another team led by Enrico Fermi was able to initiate the first artificial self-sustained nuclear chain reaction, Chicago Pile-1. An initial plan using enriched uranium-235 was abandoned as it was as yet unavailable in sufficient quantities. Working in a lab below the stands of Stagg Field at the University of Chicago, the team created the conditions needed for such a reaction by piling together 360 tonnes of graphite, 53 tonnes of uranium oxide, and 5.5 tonnes of uranium metal, most of which was supplied by Westinghouse Lamp Plant in a makeshift production process.\nNuclear weaponry.\nTwo types of atomic bomb were developed by the United States during World War II: a uranium-based device (codenamed \"Little Boy\") whose fissile material was highly enriched uranium, and a plutonium-based device (see Trinity test and \"Fat Man\") whose plutonium was derived from uranium-238. Little Boy became the first nuclear weapon used in war when it was detonated over Hiroshima, Japan, on 6 August 1945. Exploding with a yield equivalent to 12,500 tonnes of TNT, the blast and thermal wave of the bomb destroyed nearly 50,000 buildings and killed about 75,000 people (see Atomic bombings of Hiroshima and Nagasaki). \nIn 1943 the Manhattan Project contracted two private companies, Union Carbide and Chevron, to quietly compile a survey of uranium deposits around the world. As the survey results came in, two geology professors studied the results and suggested general guidelines for new sources, including uranium associated with gold mines in the Rand area in South Africa. \nInitially it was believed that uranium was relatively rare, and that nuclear proliferation could be avoided by simply buying up all known uranium stocks, but within a decade large deposits of it were discovered in many places around the world.\nReactors.\nThe X-10 Graphite Reactor at Oak Ridge National Laboratory (ORNL) in Oak Ridge, Tennessee, formerly known as the Clinton Pile and X-10 Pile, was the world's second artificial nuclear reactor (after Enrico Fermi's Chicago Pile) and was the first reactor designed and built for continuous operation. Argonne National Laboratory's Experimental Breeder Reactor I, located at the Atomic Energy Commission's National Reactor Testing Station near Arco, Idaho, became the first nuclear reactor to create electricity on 20 December 1951. Initially, four 150-watt light bulbs were lit by the reactor, but improvements eventually enabled it to power the whole facility (later, the town of Arco became the first in the world to have all its electricity come from nuclear power generated by BORAX-III, another reactor designed and operated by Argonne National Laboratory). The world's first commercial scale nuclear power station, Obninsk in the Soviet Union, began generation with its reactor AM-1 on 27 June 1954. Other early nuclear power plants were Calder Hall in England, which began generation on 17 October 1956, and the Shippingport Atomic Power Station in Pennsylvania, which began on 26 May 1958. Nuclear power was used for the first time for propulsion by a submarine, the USS \"Nautilus\", in 1954.\nPrehistoric naturally occurring fission.\nIn 1972, French physicist Francis Perrin discovered fifteen ancient and no longer active natural nuclear fission reactors in three separate ore deposits at the Oklo mine in Gabon, Africa, collectively known as the Oklo Fossil Reactors. The ore deposit is 1.7\u00a0billion years old; then, uranium-235 constituted about 3% of uranium on Earth. This is high enough to permit a sustained chain reaction, if other supporting conditions exist. The capacity of the surrounding sediment to contain the health-threatening nuclear waste products has been cited by the U.S. federal government as supporting evidence for the feasibility to store spent nuclear fuel at the Yucca Mountain nuclear waste repository.\nContamination and the Cold War legacy.\nAbove-ground nuclear tests by the Soviet Union and the United States in the 1950s and early 1960s and by France into the 1970s and 1980s spread a significant amount of fallout from uranium daughter isotopes around the world. Additional fallout and pollution occurred from several nuclear accidents.\nUranium miners have a higher incidence of cancer. An excess risk of lung cancer among Navajo uranium miners, for example, has been documented and linked to their occupation. The Radiation Exposure Compensation Act, a 1990 law in the US, required $100,000 in \"compassion payments\" to uranium miners diagnosed with cancer or other respiratory ailments.\nDuring the Cold War between the Soviet Union and the United States, huge stockpiles of uranium were amassed and tens of thousands of nuclear weapons were created using enriched uranium and plutonium made from uranium. After the break-up of the Soviet Union in 1991, an estimated 600\u00a0short tons (540\u00a0metric tons) of highly enriched weapons grade uranium (enough to make 40,000 nuclear warheads) had been stored in often inadequately guarded facilities in the Russian Federation and several other former Soviet states. Police in Asia, Europe, and South America on at least 16 occasions from 1993 to 2005 have intercepted shipments of smuggled bomb-grade uranium or plutonium, most of which was from ex-Soviet sources. From 1993 to 2005 the Material Protection, Control, and Accounting Program, operated by the federal government of the United States, spent about US$550 million to help safeguard uranium and plutonium stockpiles in Russia. This money was used for improvements and security enhancements at research and storage facilities.\nSafety of nuclear facilities in Russia has been significantly improved since the stabilization of political and economical turmoil of the early 1990s. For example, in 1993 there were 29 incidents ranking above level 1 on the International Nuclear Event Scale, and this number dropped under four per year in 1995\u20132003. The number of employees receiving annual radiation doses above 20 mSv, which is equivalent to a single full-body CT scan, saw a strong decline around 2000. In November 2015, the Russian government approved a federal program for nuclear and radiation safety for 2016 to 2030 with a budget of 562 billion rubles (ca. 8 billion USD). Its key issue is \"the deferred liabilities accumulated during the 70 years of the nuclear industry, particularly during the time of the Soviet Union\". About 73% of the budget will be spent on decommissioning aged and obsolete nuclear reactors and nuclear facilities, especially those involved in state defense programs; 20% will go in processing and disposal of nuclear fuel and radioactive waste, and 5% into monitoring and ensuring of nuclear and radiation safety.\nOccurrence.\nUranium is a naturally occurring element found in low levels in all rock, soil, and water. It is the highest-numbered element found naturally in significant quantities on Earth and is almost always found combined with other elements. Uranium is the 48th most abundant element in the Earth's crust. The decay of uranium, thorium, and potassium-40 in Earth's mantle is thought to be the main source of heat that keeps the Earth's outer core in the liquid state and drives mantle convection, which in turn drives plate tectonics.\nUranium's concentration in the Earth's crust is (depending on the reference) 2 to 4 parts per million, or about 40 times as abundant as silver. The Earth's crust from the surface to 25\u00a0km (15\u00a0mi) down is calculated to contain 1017\u00a0kg (2\u00d71017\u00a0lb) of uranium while the oceans may contain 1013\u00a0kg (2\u00d71013\u00a0lb). The concentration of uranium in soil ranges from 0.7 to 11 parts per million (up to 15 parts per million in farmland soil due to use of phosphate fertilizers containing uranium impurities), and its concentration in sea water is 3 parts per billion.\nUranium is more plentiful than antimony, tin, cadmium, mercury, or silver, and it is about as abundant as arsenic or molybdenum. Uranium is found in hundreds of minerals, including uraninite (the most common uranium ore), carnotite, autunite, uranophane, torbernite, and coffinite. Significant concentrations of uranium occur in some substances such as phosphate rock deposits, and minerals such as lignite, and monazite sands in uranium-rich ores (it is recovered commercially from sources with as little as 0.1% uranium).\nOrigin.\nLike all elements with atomic weights higher than that of iron, uranium is only naturally formed by the r-process (rapid neutron capture) in supernovae and neutron star mergers. Primordial thorium and uranium are only produced in the r-process, because the s-process (slow neutron capture) is too slow and cannot pass the gap of instability after bismuth. Besides the two extant primordial uranium isotopes, 235U and 238U, the r-process also produced significant quantities of 236U, which has a shorter half-life and so is an extinct radionuclide, having long since decayed completely to 232Th. Further uranium-236 was produced by the decay of 244Pu, accounting for the observed higher-than-expected abundance of thorium and lower-than-expected abundance of uranium. While the natural abundance of uranium has been supplemented by the decay of extinct 242Pu (half-life 375,000\u00a0years) and 247Cm (half-life 16\u00a0million years), producing 238U and 235U respectively, this occurred to an almost negligible extent due to the shorter half-lives of these parents and their lower production than 236U and 244Pu, the parents of thorium: the 247Cm/235U ratio at the formation of the Solar System was .\nBiotic and abiotic.\nSome bacteria, such as \"Shewanella putrefaciens\", \"Geobacter metallireducens\" and some strains of \"Burkholderia fungorum\", can use uranium for their growth and convert U(VI) to U(IV). Recent research suggests that this pathway includes reduction of the soluble U(VI) via an intermediate U(V) pentavalent state.\nOther organisms, such as the lichen \"Trapelia involuta\" or microorganisms such as the bacterium \"Citrobacter\", can absorb concentrations of uranium that are up to 300 times the level of their environment. \"Citrobacter\" species absorb uranyl ions when given glycerol phosphate (or other similar organic phosphates). After one day, one gram of bacteria can encrust themselves with nine grams of uranyl phosphate crystals; this creates the possibility that these organisms could be used in bioremediation to decontaminate uranium-polluted water.\nThe proteobacterium \"Geobacter\" has also been shown to bioremediate uranium in ground water. The mycorrhizal fungus \"Glomus intraradices\" increases uranium content in the roots of its symbiotic plant.\nIn nature, uranium(VI) forms highly soluble carbonate complexes at alkaline pH. This leads to an increase in mobility and availability of uranium to groundwater and soil from nuclear wastes which leads to health hazards. However, it is difficult to precipitate uranium as phosphate in the presence of excess carbonate at alkaline pH. A \"Sphingomonas\" sp. strain BSAR-1 has been found to express a high activity alkaline phosphatase (PhoK) that has been applied for bioprecipitation of uranium as uranyl phosphate species from alkaline solutions. The precipitation ability was enhanced by overexpressing PhoK protein in \"E. coli\".\nPlants absorb some uranium from soil. Dry weight concentrations of uranium in plants range from 5 to 60 parts per billion, and ash from burnt wood can have concentrations up to 4 parts per million. Dry weight concentrations of uranium in food plants are typically lower with one to two micrograms per day ingested through the food people eat.\nProduction and mining.\nWorldwide production of uranium in 2021 was 48,332 tonnes, of which 21,819 t (45%) was mined in Kazakhstan. Other important uranium mining countries are Namibia (5,753\u00a0t), Canada (4,693\u00a0t), Australia (4,192\u00a0t), Uzbekistan (3,500\u00a0t), and Russia (2,635\u00a0t).\nUranium ore is mined in several ways: open pit, underground, in-situ leaching, and borehole mining. Low-grade uranium ore mined typically contains 0.01 to 0.25% uranium oxides. Extensive measures must be employed to extract the metal from its ore. High-grade ores found in Athabasca Basin deposits in Saskatchewan, Canada can contain up to 23% uranium oxides on average. Uranium ore is crushed and rendered into a fine powder and then leached with either an acid or alkali. The leachate is subjected to one of several sequences of precipitation, solvent extraction, and ion exchange. The resulting mixture, called yellowcake, contains at least 75% uranium oxides U3O8. Yellowcake is then calcined to remove impurities from the milling process before refining and conversion.\nCommercial-grade uranium can be produced through the reduction of uranium halides with alkali or alkaline earth metals. Uranium metal can also be prepared through electrolysis of KUF5 or\nUF4, dissolved in molten calcium chloride (CaCl2) and sodium chloride (NaCl) solution. Very pure uranium is produced through the thermal decomposition of uranium halides on a hot filament.\nResources and reserves.\nIt is estimated that 6.1\u00a0million tonnes of uranium exists in ores that are economically viable at US$130 per kg of uranium, while 35\u00a0million tonnes are classed as mineral resources (reasonable prospects for eventual economic extraction).\nAustralia has 28% of the world's known uranium ore reserves and the world's largest single uranium deposit is located at the Olympic Dam Mine in South Australia. There is a significant reserve of uranium in Bakouma, a sub-prefecture in the prefecture of Mbomou in the Central African Republic.\nSome uranium also originates from dismantled nuclear weapons. For example, in 1993\u20132013 Russia supplied the United States with 15,000 tonnes of low-enriched uranium within the Megatons to Megawatts Program.\nAn additional 4.6\u00a0billion tonnes of uranium are estimated to be dissolved in sea water (Japanese scientists in the 1980s showed that extraction of uranium from sea water using ion exchangers was technically feasible). There have been experiments to extract uranium from sea water, but the yield has been low due to the carbonate present in the water. In 2012, ORNL researchers announced the successful development of a new absorbent material dubbed HiCap which performs surface retention of solid or gas molecules, atoms or ions and also effectively removes toxic metals from water, according to results verified by researchers at Pacific Northwest National Laboratory.\nSupplies.\nIn 2005, ten countries accounted for the majority of the world's concentrated uranium oxides: Canada (27.9%), Australia (22.8%), Kazakhstan (10.5%), Russia (8.0%), Namibia (7.5%), Niger (7.4%), Uzbekistan (5.5%), the United States (2.5%), Argentina (2.1%) and Ukraine (1.9%). In 2008, Kazakhstan was forecast to increase production and become the world's largest supplier of uranium by 2009; Kazakhstan has dominated the world's uranium market since 2010. In 2021, its share was 45.1%, followed by Namibia (11.9%), Canada (9.7%), Australia (8.7%), Uzbekistan (7.2%), Niger (4.7%), Russia (5.5%), China (3.9%), India (1.3%), Ukraine (0.9%), and South Africa (0.8%), with a world total production of 48,332 tonnes. Most uranium was produced not by conventional underground\u00a0mining of ores (29% of production), but by in situ leaching (66%).\nIn the late 1960s, UN geologists discovered major uranium deposits and other rare mineral reserves in Somalia. The find was the largest of its kind, with industry experts estimating the deposits at over 25% of the world's then known uranium reserves of 800,000 tons.\nThe ultimate available supply is believed to be sufficient for at least the next 85 years, though some studies indicate underinvestment in the late twentieth century may produce supply problems in the 21st century.\nUranium deposits seem to be log-normal distributed. There is a 300-fold increase in the amount of uranium recoverable for each tenfold decrease in ore grade.\nIn other words, there is little high grade ore and proportionately much more low grade ore available.\nCompounds.\nOxidation states and oxides.\nOxides.\nCalcined uranium yellowcake, as produced in many large mills, contains a distribution of uranium oxidation species in various forms ranging from most oxidized to least oxidized. Particles with short residence times in a calciner will generally be less oxidized than those with long retention times or particles recovered in the stack scrubber. Uranium content is usually referenced to U3O8, which dates to the days of the Manhattan Project when U3O8 was used as an analytical chemistry reporting standard.\nPhase relationships in the uranium-oxygen system are complex. The most important oxidation states of uranium are uranium(IV) and uranium(VI), and their two corresponding oxides are, respectively, uranium dioxide (UO2) and uranium trioxide (UO3). Other uranium oxides such as uranium monoxide (UO), diuranium pentoxide (U2O5), and uranium peroxide (UO4\u00b72H2O) also exist.\nThe most common forms of uranium oxide are triuranium octoxide (U3O8) and UO2. Both oxide forms are solids that have low solubility in water and are relatively stable over a wide range of environmental conditions. Triuranium octoxide is (depending on conditions) the most stable compound of uranium and is the form most commonly found in nature. Uranium dioxide is the form in which uranium is most commonly used as a nuclear reactor fuel. At ambient temperatures, UO2 will gradually convert to U3O8. Because of their stability, uranium oxides are generally considered the preferred chemical form for storage or disposal.\nAqueous chemistry.\nSalts of many oxidation states of uranium are water-soluble and may be studied in aqueous solutions. The most common ionic forms are U3+ (brown-red), U4+ (green), UO2+ (unstable), and UO22+ (yellow), for U(III), U(IV), U(V), and U(VI), respectively. A few solid and semi-metallic compounds such as UO and US exist for the formal oxidation state uranium(II), but no simple ions are known to exist in solution for that state. Ions of U3+ liberate hydrogen from water and are therefore considered to be highly unstable. The UO22+ ion represents the uranium(VI) state and is known to form compounds such as uranyl carbonate, uranyl chloride and uranyl sulfate. UO22+ also forms complexes with various organic chelating agents, the most commonly encountered of which is uranyl acetate.\nUnlike the uranyl salts of uranium and polyatomic ion uranium-oxide cationic forms, the uranates, salts containing a polyatomic uranium-oxide anion, are generally not water-soluble.\nCarbonates.\nThe interactions of carbonate anions with uranium(VI) cause the Pourbaix diagram to change greatly when the medium is changed from water to a carbonate containing solution. While the vast majority of carbonates are insoluble in water (students are often taught that all carbonates other than those of alkali metals are insoluble in water), uranium carbonates are often soluble in water. This is because a U(VI) cation is able to bind two terminal oxides and three or more carbonates to form anionic complexes.\nEffects of pH.\nThe uranium fraction diagrams in the presence of carbonate illustrate this further: when the pH of a uranium(VI) solution increases, the uranium is converted to a hydrated uranium oxide hydroxide and at high pHs it becomes an anionic hydroxide complex.\nWhen carbonate is added, uranium is converted to a series of carbonate complexes if the pH is increased. One effect of these reactions is increased solubility of uranium in the pH range 6 to 8, a fact that has a direct bearing on the long term stability of spent uranium dioxide nuclear fuels.\nHydrides, carbides and nitrides.\nUranium metal heated to reacts with hydrogen to form uranium hydride. Even higher temperatures will reversibly remove the hydrogen. This property makes uranium hydrides convenient starting materials to create reactive uranium powder along with various uranium carbide, nitride, and halide compounds. Two crystal modifications of uranium hydride exist: an \u03b1 form that is obtained at low temperatures and a \u03b2 form that is created when the formation temperature is above 250\u00a0\u00b0C.\nUranium carbides and uranium nitrides are both relatively inert semimetallic compounds that are minimally soluble in acids, react with water, and can ignite in air to form U3O8. Carbides of uranium include uranium monocarbide (UC), uranium dicarbide (UC2), and diuranium tricarbide (U2C3). Both UC and UC2 are formed by adding carbon to molten uranium or by exposing the metal to carbon monoxide at high temperatures. Stable below 1800\u00a0\u00b0C, U2C3 is prepared by subjecting a heated mixture of UC and UC2 to mechanical stress. Uranium nitrides obtained by direct exposure of the metal to nitrogen include uranium mononitride (UN), uranium dinitride (UN2), and diuranium trinitride (U2N3).\nHalides.\nAll uranium fluorides are created using uranium tetrafluoride (UF4); UF4 itself is prepared by hydrofluorination of uranium dioxide. Reduction of UF4 with hydrogen at 1000\u00a0\u00b0C produces uranium trifluoride (UF3). Under the right conditions of temperature and pressure, the reaction of solid UF4 with gaseous uranium hexafluoride (UF6) can form the intermediate fluorides of U2F9, U4F17, and UF5.\nAt room temperatures, UF6 has a high vapor pressure, making it useful in the gaseous diffusion process to separate the rare uranium-235 from the common uranium-238 isotope. This compound can be prepared from uranium dioxide and uranium hydride by the following process:\nUO2 + 4 HF \u2192 UF4 + 2 H2O (500\u00a0\u00b0C, endothermic)\nUF4 + F2 \u2192 UF6 (350\u00a0\u00b0C, endothermic)\nThe resulting UF6, a white solid, is highly reactive (by fluorination), easily sublimes (emitting a vapor that behaves as a nearly ideal gas), and is the most volatile compound of uranium known to exist.\nOne method of preparing uranium tetrachloride (UCl4) is to directly combine chlorine with either uranium metal or uranium hydride. The reduction of UCl4 by hydrogen produces uranium trichloride (UCl3) while the higher chlorides of uranium are prepared by reaction with additional chlorine. All uranium chlorides react with water and air.\nBromides and iodides of uranium are formed by direct reaction of, respectively, bromine and iodine with uranium or by adding UH3 to those element's acids. Known examples include: UBr3, UBr4, UI3, and UI4. UI5 has never been prepared. Uranium oxyhalides are water-soluble and include UO2F2, UOCl2, UO2Cl2, and UO2Br2. Stability of the oxyhalides decrease as the atomic weight of the component halide increases.\nIsotopes.\nUranium, like all elements with an atomic number greater than 82, has no stable isotopes. All isotopes of uranium are radioactive because the strong nuclear force does not prevail over electromagnetic repulsion in nuclides containing more than 82 protons. Nevertheless, the two most stable isotopes, 238U and 235U, have half-lives long enough to occur in nature as primordial radionuclides, with measurable quantities having survived since the formation of the Earth. These two nuclides, along with thorium-232, are the only confirmed primordial nuclides heavier than nearly-stable bismuth-209.\nNatural uranium consists of three major isotopes: uranium-238 (99.28% natural abundance), uranium-235 (0.71%), and uranium-234 (0.0054%). There are also five other trace isotopes: uranium-240, a decay product of plutonium-244; uranium-239, which is formed when 238U undergoes spontaneous fission, releasing neutrons that are captured by another 238U atom; uranium-237, which is formed when 238U captures a neutron but emits two more, which then decays to neptunium-237; uranium-236, which occurs in trace quantities due to neutron capture on 235U and as a decay product of plutonium-244; and finally, uranium-233, which is formed in the decay chain of neptunium-237. Additionally, uranium-232 would be produced by the double beta decay of natural thorium-232, though this energetically possible process has never been observed.\nUranium-238 is the most stable isotope of uranium, with a half-life of about years, roughly the age of the Earth. Uranium-238 is predominantly an alpha emitter, decaying to thorium-234. It ultimately decays through the uranium series, which has 18 members, into lead-206. Uranium-238 is not fissile, but is a fertile isotope, because after neutron activation it can be converted to plutonium-239, another fissile isotope. Indeed, the 238U nucleus can absorb one neutron to produce the radioactive isotope uranium-239. 239U decays by beta emission to neptunium-239, also a beta-emitter, that decays in its turn, within a few days into plutonium-239. 239Pu was used as fissile material in the first atomic bomb detonated in the \"Trinity test\" on 16 July 1945 in New Mexico.\nUranium-235 has a half-life of about years; it is the next most stable uranium isotope after 238U and is also predominantly an alpha emitter, decaying to thorium-231. Uranium-235 is important for both nuclear reactors and nuclear weapons, because it is the only uranium isotope existing in nature on Earth in significant amounts that is fissile. This means that it can be split into two or three fragments (fission products) by thermal neutrons. The decay chain of 235U, which is called the actinium series, has 15 members and eventually decays into lead-207. The constant rates of decay in these decay series makes the comparison of the ratios of parent to daughter elements useful in radiometric dating.\nUranium-236 has a half-life of years and is not found in significant quantities in nature. The half-life of uranium-236 is too short for it to be primordial, though it has been identified as an extinct progenitor of its alpha decay daughter, thorium-232. Uranium-236 occurs in spent nuclear fuel when neutron capture on 235U does not induce fission, or as a decay product of plutonium-240. Uranium-236 is not fertile, as three more neutron captures are required to produce fissile 239Pu, and is not itself fissile; as such, it is considered long-lived radioactive waste.\nUranium-234 is a member of the uranium series and occurs in equilibrium with its progenitor, 238U; it undergoes alpha decay with a half-life of 245,500 years and decays to lead-206 through a series of relatively short-lived isotopes.\nUranium-233 undergoes alpha decay with a half-life of 160,000 years and, like 235U, is fissile. It can be bred from thorium-232 via neutron bombardment, usually in a nuclear reactor; this process is known as the thorium fuel cycle. Owing to the fissility of 233U and the greater natural abundance of thorium (three times that of uranium), 233U has been investigated for use as nuclear fuel as a possible alternative to 235U and 239Pu, though is not in widespread use as of 2022[ [update]]. The decay chain of uranium-233 forms part of the neptunium series and ends at nearly-stable bismuth-209 (half-life ) and stable thallium-205.\nUranium-232 is an alpha emitter with a half-life of 68.9 years. This isotope is produced as a byproduct in production of 233U and is considered a nuisance, as it is not fissile and decays through short-lived alpha and gamma emitters such as 208Tl. It is also expected that thorium-232 should be able to undergo double beta decay, which would produce uranium-232, but this has not yet been observed experimentally.\nAll isotopes from 232U to 236U inclusive have minor cluster decay branches (less than %), and all these bar 233U, in addition to 238U, have minor spontaneous fission branches; the greatest branching ratio for spontaneous fission is about % for 238U, or about one in every two million decays. The shorter-lived trace isotopes 237U and 239U exclusively undergo beta decay, with respective half-lives of 6.752 days and 23.45 minutes.\nIn total, 28 isotopes of uranium have been identified, ranging in mass number from 214 to 242, with the exception of 220. Among the uranium isotopes not found in natural samples or nuclear fuel, the longest-lived is 230U, an alpha emitter with a half-life of 20.23 days. This isotope has been considered for use in targeted alpha-particle therapy (TAT). All other isotopes have half-lives shorter than one hour, except for 231U (half-life 4.2 days) and 240U (half-life 14.1 hours). The shortest-lived known isotope is 221U, with a half-life of 660 nanoseconds, and it is expected that the hitherto unknown 220U has an even shorter half-life. The proton-rich isotopes lighter than 232U primarily undergo alpha decay, except for 229U and 231U, which decay to protactinium isotopes via positron emission and electron capture, respectively; the neutron-rich 240U, 241U, and 242U undergo beta decay to form neptunium isotopes.\nEnrichment.\nIn nature, uranium is found as uranium-238 (99.2742%) and uranium-235 (0.7204%). Isotope separation concentrates (enriches) the fissile uranium-235 for nuclear weapons and most nuclear power plants, except for gas cooled reactors and pressurized heavy water reactors. Most neutrons released by a fissioning atom of uranium-235 must impact other uranium-235 atoms to sustain the nuclear chain reaction. The concentration and amount of uranium-235 needed to achieve this is called a 'critical mass'.\nTo be considered 'enriched', the uranium-235 fraction should be between 3% and 5%. This process produces huge quantities of uranium that is depleted of uranium-235 and with a correspondingly increased fraction of uranium-238, called depleted uranium or 'DU'. To be considered 'depleted', the 235U concentration should be no more than 0.3%. The price of uranium has risen since 2001, so enrichment tailings containing more than 0.35% uranium-235 are being considered for re-enrichment, driving the price of depleted uranium hexafluoride above $130 per kilogram in July 2007 from $5 in 2001.\nThe gas centrifuge process, where gaseous uranium hexafluoride (UF6) is separated by the difference in molecular weight between 235UF6 and 238UF6 using high-speed centrifuges, is the cheapest and leading enrichment process. The gaseous diffusion process had been the leading method for enrichment and was used in the Manhattan Project. In this process, uranium hexafluoride is repeatedly diffused through a silver-zinc membrane, and the different isotopes of uranium are separated by diffusion rate (since uranium-238 is heavier it diffuses slightly slower than uranium-235). The molecular laser isotope separation method employs a laser beam of precise energy to sever the bond between uranium-235 and fluorine. This leaves uranium-238 bonded to fluorine and allows uranium-235 metal to precipitate from the solution. An alternative laser method of enrichment is known as atomic vapor laser isotope separation (AVLIS) and employs visible tunable lasers such as dye lasers. Another method used is liquid thermal diffusion.\nThe only significant deviation from the 235U to 238U ratio in any known natural samples occurs in Oklo, Gabon, where natural nuclear fission reactors consumed some of the 235U some two billion years ago when the ratio of 235U to 238U was more akin to that of low enriched uranium allowing regular (\"light\") water to act as a neutron moderator akin to the process in humanmade light water reactors. The existence of such natural fission reactors which had been theoretically predicted beforehand was proven as the slight deviation of 235U concentration from the expected values were discovered during uranium enrichment in France. Subsequent investigations to rule out any nefarious human action (such as stealing of 235U) confirmed the theory by finding isotope ratios of common fission products (or rather their stable daughter nuclides) in line with the values expected for fission but deviating from the values expected for non-fission derived samples of those elements.\nHuman exposure.\nA person can be exposed to uranium (or its radioactive daughters, such as radon) by inhaling dust in air or by ingesting contaminated water and food. The amount of uranium in air is usually very small; however, people who work in factories that process phosphate fertilizers containing uranium impurities, live near government facilities that made or tested nuclear weapons, live or work near a modern battlefield where depleted uranium weapons have been used, or live or work near a coal-fired power plant, facilities that mine or process uranium ore, or enrich uranium for reactor fuel, may have increased exposure to uranium. Houses or structures that are over uranium deposits (either natural or man-made slag deposits) may have an increased incidence of exposure to radon gas.\nThe health impacts of natural and of deleted uranium are chemical rather than due to radiation.\nThe Occupational Safety and Health Administration (OSHA) has set the permissible exposure limit for uranium exposure in the workplace as 0.25\u00a0mg/m3 over an 8-hour workday. The National Institute for Occupational Safety and Health (NIOSH) has set a recommended exposure limit (REL) of 0.2\u00a0mg/m3 over an 8-hour workday and a short-term limit of 0.6\u00a0mg/m3. At 10\u00a0mg/m3, uranium is immediately dangerous to life and health.\nMost ingested uranium is excreted during digestion. Only 0.5% is absorbed when insoluble forms of uranium, such as its oxide, are ingested, whereas absorption of the more soluble uranyl ion can be up to 5%. However, soluble uranium compounds tend to quickly pass through the body, whereas insoluble uranium compounds, especially when inhaled by way of dust into the lungs, pose a more serious exposure hazard. After entering the bloodstream, the absorbed uranium tends to bioaccumulate and stay for many years in bone tissue because of uranium's affinity for phosphates. Incorporated uranium becomes uranyl ions, which accumulate in bone, liver, kidney, and reproductive tissues.\nElements of high atomic number like uranium exhibit phantom or secondary radiotoxicity through absorption of natural background gamma and X-rays and re-emission of photoelectrons, which in combination with the high affinity of uranium to the phosphate moiety of DNA cause increased single and double strand DNA breaks.\nUranium is not absorbed through the skin, and alpha particles released by uranium cannot penetrate the skin.\nUranium can be decontaminated from steel surfaces and aquifers.\nEffects and precautions.\nNormal functioning of the kidney, brain, liver, heart, and other systems can be affected by uranium exposure, because, besides being weakly radioactive, uranium is a toxic metal. Uranium is also a reproductive toxicant. Radiological effects are generally local because alpha radiation, the primary form of 238U decay, has a very short range, and will not penetrate skin. Alpha radiation from inhaled uranium has been demonstrated to cause lung cancer in exposed nuclear workers. The Centers for Disease Control have published one study stating that neither natural nor depleted uranium have been classified with respect to carcinogenicity. Exposure to its decay products, especially radon, is a significant health threat, and uranium processing produces wastes contaminated with radium which in turn produces radon gas. Because of its long half-life, purified uranium will not produce significant amounts of daughter nuclides for millions of years. Exposure to strontium-90, iodine-131, and other fission products is unrelated to uranium exposure, but may result from medical procedures or exposure to spent reactor fuel or fallout from nuclear weapons.\nAlthough accidental inhalation exposure to a high concentration of uranium hexafluoride has resulted in human fatalities, those deaths were associated with the generation of highly toxic hydrofluoric acid and uranyl fluoride rather than with uranium itself. Finely divided uranium metal presents a fire hazard because uranium is pyrophoric; small grains will ignite spontaneously in air at room temperature.\nUranium metal is commonly handled with gloves as a sufficient precaution. Uranium concentrate is handled and contained so as to ensure that people do not inhale or ingest it.\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "31744", "revid": "24909938", "url": "https://en.wikipedia.org/wiki?curid=31744", "title": "Ungulate", "text": "Group of animals that walk on the tips of their toes or hooves\nUngulates are members of the diverse clade Euungulata, which primarily consists of large mammals with hooves. Once part of the taxon \"Ungulata\" along with paenungulates and tubulidentates, as well as several extinct taxa, \"Ungulata\" has since been determined to be a polyphyletic grouping based on molecular data. As a result, true ungulates had since been reclassified to the newer clade Euungulata in 2001 within the clade Laurasiatheria, while Paenungulata and Tubulidentata had been reclassified to the distant clade Afrotheria. Alternatively, some authors use the name Ungulata to designate the same clade as Euungulata.\nLiving ungulates are divided into two orders: Perissodactyla including equines, rhinoceroses, and tapirs; and Artiodactyla including cattle, antelope, pigs, giraffes, camels, sheep, deer, and hippopotamuses, among others. Cetaceans such as whales, dolphins, and porpoises are also classified as artiodactyls, although they do not have hooves. Most terrestrial ungulates use the hoofed tips of their toes to support their body weight while standing or moving. Two other orders of ungulates, Notoungulata and Litopterna, both native to South America, became extinct at the end of the Pleistocene, around 12,000 years ago.\nThe term means, roughly, \"being hoofed\" or \"hoofed animal\". As a descriptive term, \"ungulate\" normally excludes cetaceans as they do not possess most of the typical morphological characteristics of other ungulates, but they were also descended from early artiodactyls. Ungulates are typically herbivorous and many employ specialized gut bacteria to enable them to digest cellulose, though some members may deviate from this: several species of pigs and the extinct entelodonts are omnivorous, while cetaceans and the extinct mesonychians are carnivorous.\nEtymology.\nUngulate is from the Late Latin adjective 'hoofed'. is a diminutive form of Latin 'nail' (finger nail; toe nail).\nClassifications.\nHistory.\nEuungulata is a clade (or in some taxonomies, a grand order) of mammals. The two extant orders of ungulates are the Perissodactyla (odd-toed ungulates) and Artiodactyla (even-toed ungulates). Hyracoidea (hyraxes), Sirenia (sea cows, dugongs and manatees), Proboscidea (elephants) and Tubulidentata (aardvarks) were in the past grouped within the clade \"Ungulata\", later found to be a polyphyletic and invalid. The three orders of Paenungulata are considered a clade and grouped in the Afrotheria clade, along with Tubulidentata, while Euungulata is grouped under the Laurasiatheria clade.\nIn 2009, morphological and molecular work found that aardvarks, hyraxes, sea cows, and elephants were more closely related to each other and to sengis, tenrecs, and golden moles than to the perissodactyls and artiodactyls, and form the clade Afrotheria. Elephants, sea cows, and hyraxes were grouped together in the clade Paenungulata, while the aardvark has been considered as either a close relative to them or a close relative to sengis in the clade Afroinsectiphilia. This is a striking example of convergent evolution.\nThere is some dispute as to whether this smaller Euungulata is a cladistic (evolution-based) group, or merely a phenetic group (form taxon) or folk taxon (similar, but not necessarily related). Some studies have indeed found the mesaxonian ungulates and paraxonian ungulates to form a monophyletic lineage, closely related to either the Ferae (the carnivorans and the pangolins) in the clade Fereuungulata or to the bats. Other studies found the two orders not that closely related, as some place the perissodactyls as close relatives to bats and Ferae in Pegasoferae and others place the artiodactyls as close relatives to bats.\nTaxonomy.\nBelow is a simplified taxonomy (assuming that ungulates do indeed form a natural grouping) with the extant families, in order of the relationships. Keep in mind that there were still some grey areas of conflict, such as the case with the relationship between the pecoran families and the baleen whale families. See each family for the relationships of the species as well as the controversies in their respective articles.\nPhylogeny.\nBelow is the general consensus of the phylogeny of the ungulate families.\nEvolutionary history.\nPerissodactyla and Artiodactyla include the majority of large land mammals. These two groups first appeared during the late Paleocene, rapidly spreading to a wide variety of species on numerous continents, and have developed in parallel since that time. Some scientists believed that modern ungulates were descended from an evolutionary grade of mammals known as the condylarths. The earliest known member of this group may have been the tiny \"Protungulatum\", a mammal that co-existed with the last of non-avian dinosaurs 66\u00a0million years ago. However, many authorities do not consider it a true placental, let alone an ungulate. The enigmatic dinoceratans were among the first large herbivorous mammals, although their exact relationship with other mammals is still debated with one of the theories being that they might just be distant relatives to living ungulates; the most recent study recovers them as within the true ungulate assemblage, closest to \"Carodnia\".\nIn Australia, the recently extinct marsupial \"Chaeropus\" (\"pig-footed bandicoot\") also developed hooves similar to those of artiodactyls, an example of convergent evolution.\nPerissodactyl evolution.\nPerissodactyls were thought to have evolved from the Phenacodontidae, small, sheep-sized animals that were already showing signs of anatomical features that their descendants would inherit (the reduction of digit I and V for example). By the start of the Eocene, 55\u00a0million years ago (Mya), they had diversified and spread out to occupy several continents. Horses and tapirs both evolved in North America; rhinoceroses appear to have developed in Asia from tapir-like animals and then colonised the Americas during the middle Eocene (about 45\u00a0Mya). Of the approximately 15 families, only three survive (McKenna and Bell, 1997; Hooker, 2005). These families were very diverse in form and size; they included the enormous brontotheres and the bizarre chalicotheres. The largest perissodactyl, an Asian rhinoceros called \"Paraceratherium\", reached , more than twice the weight of an elephant.\nIt has been found in a cladistic study that the anthracobunids and the desmostylians \u2013 two lineages that have been previously classified as Afrotherians (more specifically closer to elephants) \u2013 have been classified as a clade that is closely related to the perissodactyls. The desmostylians were large amphibious quadrupeds with massive limbs and a short tail. They grew to in length and were thought to have weighed more than . Their fossils were known from the northern Pacific Rim, from southern Japan through Russia, the Aleutian Islands and the Pacific coast of North America to the southern tip of Baja California. Their dental and skeletal form suggests desmostylians were aquatic herbivores dependent on littoral habitats. Their name refers to their highly distinctive molars, in which each cusp was modified into hollow columns, so that a typical molar would have resembled a cluster of pipes, or in the case of worn molars, volcanoes. They were the only marine mammals to have gone extinct.\nThe South American meridiungulates contain the somewhat tapir-like pyrotheres and astrapotheres, the mesaxonic litopterns and the diverse notoungulates. As a whole, meridiungulates were said to have evolved from animals like \"Hyopsodus\". For a while their relationships with other ungulates were a mystery. Some paleontologists have even challenged the monophyly of Meridiungulata by suggesting that the pyrotheres may be more closely related to other mammals, such as Embrithopoda (an African order that were related to elephants) than to other South American ungulates. A recent study based on bone collagen has found that at least litopterns and the notoungulates were closely related to the perissodactyls.\nThe oldest known fossils assigned to Equidae date from the early Eocene, 54\u00a0million years ago. They had been assigned to the genus \"Hyracotherium\", but the type species of that genus is now considered not a member of this family, but the other species have been split off into different genera. These early Equidae were fox-sized animals with three toes on the hind feet, and four on the front feet. They were herbivorous browsers on relatively soft plants, and were already adapted for running. The complexity of their brains suggest that they already were alert and intelligent animals. Later species reduced the number of toes, and developed teeth more suited for grinding up grass and other tough plant food.\nRhinocerotoids diverged from other perissodactyls by the early Eocene. Fossils of \"Hyrachyus eximus\" found in North America date to this period. This small hornless ancestor resembled a tapir or small horse more than a rhino. Three families, sometimes grouped together as the superfamily Rhinocerotoidea, evolved in the late Eocene: Hyracodontidae, Amynodontidae and Rhinocerotidae, thus creating an explosion of diversity unmatched for a while until environmental changes drastically eliminated several species.\nThe first tapirids, such as \"Heptodon\", appeared in the early Eocene. They appeared very similar to modern forms, but were about half the size, and lacked the proboscis. The first true tapirs appeared in the Oligocene. By the Miocene, such genera as \"Miotapirus\" were almost indistinguishable from the extant species. Asian and American tapirs were believed to have diverged around 20 to 30\u00a0million years ago; and tapirs migrated from North America to South America around 3\u00a0million years ago, as part of the Great American Interchange.\nPerissodactyls were the dominant group of large terrestrial browsers right through the Oligocene. However, the rise of grasses in the Miocene (about 20\u00a0Mya) saw a major change: the artiodactyl species with their more complex stomachs were better able to adapt to a coarse, low-nutrition diet, and soon rose to prominence. Nevertheless, many perissodactyl species survived and prospered until the late Pleistocene (about 10,000 years ago) when they faced the pressure of human hunting and habitat change.\nArtiodactyl evolution.\nThe artiodactyls were thought to have evolved from a small group of condylarths, Arctocyonidae, which were unspecialized, superficially raccoon-like to bear-like omnivores from the Early Paleocene (about 65 to 60\u00a0million years ago). They had relatively short limbs lacking specializations associated with their relatives (e.g. reduced side digits, fused bones, and hooves), and long, heavy tails. Their primitive anatomy makes it unlikely that they were able to run down prey, but with their powerful proportions, claws, and long canines, they may have been able to overpower smaller animals in surprise attacks. Evidently these mammals soon evolved into two separate lineages: the mesonychians and the artiodactyls.\nThe first artiodactyls looked like today's chevrotains or pigs: small, short-legged creatures that ate leaves and the soft parts of plants. By the Late Eocene (46\u00a0million years ago), the three modern suborders had already developed: Suina (the pig group); Tylopoda (the camel group); and Ruminantia (the goat and cattle group). Nevertheless, artiodactyls were far from dominant at that time: the perissodactyls were much more successful and far more numerous. Artiodactyls survived in niche roles, usually occupying marginal habitats, and it is presumably at that time that they developed their complex digestive systems, which allowed them to survive on lower-grade food. While most artiodactyls were taking over the niches left behind by several extinct perissodactyls, one lineage of artiodactyls began to venture out into the seas.\nCetacean evolution.\nThe traditional theory of cetacean evolution was that cetaceans were related to the mesonychian. These animals had unusual triangular teeth very similar to those of primitive cetaceans. This is why scientists long believed that cetaceans evolved from a form of mesonychian. Today, many scientists believe cetaceans evolved from the same stock that gave rise to hippopotamuses. This hypothesized ancestral group likely split into two branches around https:// million years ago. One branch would evolve into cetaceans, possibly beginning about https:// million years ago with the proto-whale \"Pakicetus\" and other early cetacean ancestors collectively known as Archaeoceti, which eventually underwent aquatic adaptation into the completely aquatic cetaceans. The other branch became the anthracotheres, a large family of four-legged beasts, the earliest of whom in the late Eocene would have resembled skinny hippopotamuses with comparatively small and narrow heads. All branches of the anthracotheres, except that which evolved into Hippopotamidae, became extinct during the Pliocene without leaving any descendants.\nThe family Raoellidae is said to be the closest artiodactyl family to the cetaceans. Consequentially, new theories in cetacean evolution hypothesize that whales and their ancestors escaped predation, not competition, by slowly adapting to the ocean.\nMesonychian evolution.\nMesonychians were depicted as \"wolves on hooves\" and were the first major mammalian predators, appearing in the Paleocene. Early mesonychians had five digits on their feet, which probably rested flat on the ground during walking (plantigrade locomotion), but later mesonychians had four digits that ended in tiny hooves on all of their toes and were increasingly well adapted to running. Like running members of the even-toed ungulates, mesonychians (\"Pachyaena\", for example) walked on their digits (digitigrade locomotion). Mesonychians fared very poorly at the close of the Eocene epoch, with only one genus, \"Mongolestes\", surviving into the Early Oligocene epoch, as the climate changed and fierce competition arose from the better adapted creodonts.\nCharacteristics.\nUngulates are in high diversity in response to sexual selection and ecological events; most ungulates lack a collar bone. Terrestrial ungulates are for the most part herbivores, with some of them being grazers. However, there are exceptions to this as pigs, peccaries, hippos and duikers are known to have an omnivorous diet. Cetaceans are the only modern ungulates that have a carnivorous diet; baleen whales consume significantly smaller animals in relation to their body size, such as small species of fish and krill; toothed whales, depending on the species, can consume a wide range of species: squid, fish, sharks, and other species of mammals such as seals and other whales. In terms of ecosystem ungulates have colonized all corners of the planet, from mountains to the ocean depths; grasslands to deserts and some have been domesticated by humans.\nAnatomy.\nUngulates have developed specialized adaptations, especially in the areas of cranial appendages, dentition, and leg morphology including the modification of the astragalus (one of the ankle bones at the end of the lower leg) with a short, robust head.\nHooves.\nThe hoof is the tip of the toe of an ungulate mammal, strengthened by a thick horny (keratin) covering. The hoof consists of a hard or rubbery sole, and a hard wall formed by a thick nail rolled around the tip of the toe. Both the sole and the edge of the hoof wall normally bear the weight of the animal. Hooves grow continuously, and are constantly worn down by use. In most modern ungulates, the radius and ulna are fused along the length of the forelimb; early ungulates, such as the arctocyonids, did not share this unique skeletal structure. The fusion of the radius and ulna prevents an ungulate from rotating its forelimb. Since this skeletal structure has no specific function in ungulates, it is considered a homologous characteristic that ungulates share with other mammals. This trait would have been passed down from a common ancestor. While the two orders of ungulates colloquial names were based on the number of toes of their members (\"odd-toed\" for the perissodactyls and \"even-toed\" for the terrestrial artiodactyls), it is not an accurate reason they were grouped. Tapirs have four toes in the front, yet they were members of the \"odd-toed\" order; peccaries and modern cetaceans were members of the \"even-toed\" order, yet peccaries have three toes in the front and whales were an extreme example as they have flippers instead of hooves. Scientists had classified them according to the distribution of their weight to their toes.\nPerissodactyls have a mesaxonic foot, meaning that the weight is distributed on the third toe on all legs thanks to the plane symmetry of their feet. There has been a reduction of toes from the common ancestor, with the classic example being horses with their single hooves. In consequence, there was an alternative name for the perissodactyls the nearly obsolete Mesaxonia. Perissodactyls were not the only lineage of mammals to have evolved this trait; the meridiungulates have evolved mesaxonic feet numerous times.\nTerrestrial artiodactyls have a paraxonic foot, meaning that the weight is distributed on the third and the fourth toe on all legs. The majority of these mammals have cloven hooves, with two smaller ones known as the dewclaws that were located further up on the leg. The earliest cetaceans (the archaeocetes), also had this characteristic in the addition of also having both an astragalus and cuboid bone in the ankle, which were further diagnostic traits of artiodactyls.\nIn modern cetaceans, the front limbs had become pectoral fins and the hind parts were internal and reduced. Occasionally, the genes that code for longer extremities cause a modern cetacean to develop miniature legs (known as atavism). The main method of moving is an up-and-down motion with the tail fin, called the fluke, which is used for propulsion, while the pectoral fins together with the entire tail section provide directional control. All modern cetaceans still retain their digits despite the external appearance suggesting otherwise.\nTeeth.\nMost ungulates have developed reduced canine teeth and specialized molars, including bunodont (low, rounded cusps) and hypsodont (high crowned) teeth. The development of hypsodonty has been of particular interest as this adaptation was strongly associated with the spread of grasslands during the Miocene about 25\u00a0million years ago. As forest biomes declined, grasslands spread, opening new niches for mammals. Many ungulates switched from browsing diets to grazing diets, and possibly driven by abrasive silica in grass, hypsodonty became common. However, recent evidence ties the evolution of hypsodonty to open, gritty habitats and not the grass itself. This is termed the Grit, not grass hypothesis.\nSome ungulates completely lack upper incisors and instead have a dental pad to assist in browsing. It can be found in camels, ruminants, and some toothed whales; modern baleen whales were remarkable in that they have baleen instead to filter out the krill from the water. On the other spectrum teeth have been evolved as weapons or sexual display seen in pigs and peccaries, some species of deer, musk deer, hippopotamuses, beaked whales and the Narwhal, with its long canine tooth.\nCranial appendages.\nUngulates have evolved a variety of cranial appendages that can be found in cervoids (with the exception of musk deer). In oxen and antelope, the size and shape of the horns varies greatly but the basic structure is always a pair of simple bony protrusions without branches, often having a spiral, twisted, or fluted form, each covered in a permanent sheath of keratin. The unique horn structure is the only unambiguous morphological feature of bovids that distinguishes them from other pecorans. Male horn development has been linked to sexual selection, while the presence of horns in females is likely due to natural selection. The horns of females are usually smaller than those of males and are sometimes of a different shape. The horns of female bovids are thought to have evolved for defense against predators or to express territoriality, as nonterritorial females, which are able to use crypsis for predator defense, often lack horns.\nRhinoceros horns, unlike those of other horned mammals, consist only of keratin. These horns rest on the nasal ridge of the animal's skull.\nAntlers are unique to cervids and found mostly on males: the only cervid females with antlers are caribou and reindeer, whose antlers are normally smaller than males'. Nevertheless, fertile does of other species of deer have the capacity to produce antlers on occasion, usually due to increased testosterone levels. Each antler grows from an attachment point on the skull called a pedicle. While an antler is growing it is covered with highly vascular skin called velvet, which supplies oxygen and nutrients to the growing bone. Antlers are considered one of the most exaggerated cases of male secondary sexual traits in the animal kingdom, and grow faster than any other mammal bone. Growth occurs at the tip, initially as cartilage that is then mineralized to become bone. Once the antler has achieved its full size, the velvet is lost and the antler's bone dies. This dead bone structure is the mature antler. In most cases, the bone at the base is destroyed by osteoclasts and the antlers eventually fall off. As a result of their fast growth rate antlers place a substantial nutritional demand on deer; they thus can constitute an honest signal of metabolic efficiency and food gathering capability.\nOssicones are horn-like (or antler-like) protuberances found on the heads of giraffes and male okapis. They are similar to the horns of antelopes and cattle save that they are derived from ossified cartilage, and that the ossicones remain covered in skin and fur rather than horn.\nPronghorn cranial appendages are unique. Each \"horn\" of the pronghorn is composed of a slender, laterally flattened blade of bone that grows from the frontal bones of the skull, forming a permanent core. As in the Giraffidae, skin covers the bony cores, but in the pronghorn it develops into a keratinous sheath that is shed and regrown on an annual basis. Unlike the horns of the family Bovidae, the horn sheaths of the pronghorn are branched, each sheath possessing a forward-pointing tine (hence the name pronghorn). The horns of males are well developed.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "31745", "revid": "28438", "url": "https://en.wikipedia.org/wiki?curid=31745", "title": "Udo of Aachen", "text": "Fictional monk from a 1999 hoax\nUdo of Aachen (c.1200\u20131270) is a fictional monk, a creation of British technical writer Ray Girvan, who introduced him in an April Fool's hoax article in 1999. According to the article, Udo was an illustrator and theologian who discovered the Mandelbrot set some 700 years before Benoit Mandelbrot.\nUdo's works were allegedly discovered by the also-fictional Bob Schipke, a Harvard mathematician, who supposedly saw a picture of the Mandelbrot set in an illumination for a 13th-century carol. Girvan also attributed Udo as a mystic and poet whose poetry was set to music by Carl Orff with the haunting \"O Fortuna\" in Carmina Burana. Later Schipke uncovered Udo's work which described how Udo had come to this kind of design while working on a method of determining whether one's soul would reach heaven.\nAspects of the hoax.\nThe poetry of \"O Fortuna\" was actually the work of itinerant goliards, found in the German Benedictine monastery of Benediktbeuern Abbey.\nThe hoax was lent an air of credibility because often medieval monks did discover scientific and mathematical theories, only to have them hidden or shelved due to persecution or simply ignored because publication prior to the invention of the printing press was difficult at best. Mr. Girvan adds to this suggestion by associating Udo with several other more legitimate discoveries where an author was considered ahead of his time in terms of a scientific theory of some sort that is now established as a mainstream theory but was considered fringe science at the time.\nAnother aspect of the deception was that it was very common for pre-20th century mathematicians to spend incredible amounts of time on hand calculations such as a logarithm table or trigonometric functions. Calculating all of the points for a Mandelbrot set is a comparable activity that would seem tedious today but would be routine for people of the time."}
{"id": "31747", "revid": "1210941", "url": "https://en.wikipedia.org/wiki?curid=31747", "title": "USSR", "text": ""}
{"id": "31748", "revid": "4062893", "url": "https://en.wikipedia.org/wiki?curid=31748", "title": "Ultra (cryptography)", "text": "British designation for intelligence from decrypted enemy communications\nUltra was the designation adopted by British military intelligence in June 1941 for wartime signals intelligence obtained by breaking high-level encrypted enemy radio and teleprinter communications at the Government Code and Cypher School (GC&amp;CS) at Bletchley Park. \"Ultra\" eventually became the standard designation among the western Allies for all such intelligence. The name arose because the intelligence obtained was considered more important than that designated by the highest British security classification then used (\"Most Secret\") and so was regarded as being \"Ultra Secret\". Several other cryptonyms had been used for such intelligence.\nThe code name \"Boniface\" was used as a cover name for \"Ultra\". In order to ensure that the successful code-breaking did not become apparent to the Germans, British intelligence created a fictional MI6 master spy, Boniface, who controlled a fictional series of agents throughout Germany. Information obtained through code-breaking was often attributed to the human intelligence from the Boniface network. The U.S. used the codename \"Magic\" for its decrypts from Japanese sources, including the \"Purple\" cipher.\nMuch of the German cipher traffic was encrypted on the Enigma machine. Used properly, the German military Enigma would have been virtually unbreakable; in practice, shortcomings in operation allowed it to be broken. The term \"Ultra\" has often been used almost synonymously with \"Enigma decrypts\". However, Ultra also encompassed decrypts of the German Lorenz SZ 40/42 machines that were used by the German High Command, and the Hagelin machine.\nMany observers, at the time and later, regarded Ultra as immensely valuable to the Allies. Winston Churchill was reported to have told King George VI, when presenting to him Stewart Menzies (head of the Secret Intelligence Service and the person who controlled distribution of Ultra decrypts to the government): \"It is thanks to the secret weapon of General Menzies, put into use on all the fronts, that we won the war!\" F. W. Winterbotham quoted the western Supreme Allied Commander, Dwight D. Eisenhower, at war's end describing Ultra as having been \"decisive\" to Allied victory. Sir Harry Hinsley, Bletchley Park veteran and official historian of British Intelligence in World War II, made a similar assessment of Ultra, saying that while the Allies would have won the war without it, \"the war would have been something like two years longer, perhaps three years longer, possibly four years longer than it was.\" However, Hinsley and others have emphasized the difficulties of counterfactual history in attempting such conclusions, and some historians, such as John Keegan, have said the shortening might have been as little as the three months it took the United States to deploy the atomic bomb.\nSources of intelligence.\nMost Ultra intelligence was derived from reading radio messages that had been encrypted with cipher machines, complemented by material from radio communications using traffic analysis and direction finding. In the early phases of the war, particularly during the eight-month Phoney War, the Germans could transmit most of their messages using land lines and so had no need to use radio. This meant that those at Bletchley Park had some time to build up experience of collecting and starting to decrypt messages on the various radio networks. German Enigma messages were the main source, with those of the German air force (the Luftwaffe) predominating, as they used radio more and their operators were particularly ill-disciplined.\nGerman.\nEnigma.\n\"Enigma\" refers to a family of electro-mechanical rotor cipher machines. These produced a polyalphabetic substitution cipher and were widely thought to be unbreakable in the 1920s, when a variant of the commercial Model D was first used by the Reichswehr. The German Army (\"Heer\"), Navy, Air Force, Nazi party, Gestapo and German diplomats used Enigma machines in several variants. Abwehr (German military intelligence) used a four-rotor machine without a plugboard and Naval Enigma used different key management from that of the army or air force, making its traffic far more difficult to cryptanalyse; each variant required different cryptanalytic treatment. The commercial versions were not as secure and Dilly Knox of GC&amp;CS is said to have broken one before the war.\nGerman military Enigma was first broken in December 1932 by Marian Rejewski and the Polish Cipher Bureau, using a combination of brilliant mathematics, the services of a spy in the German office responsible for administering encrypted communications, and good luck. The Poles read Enigma to the outbreak of World War II and beyond, in France. At the turn of 1939, the Germans made the systems ten times more complex, which required a tenfold increase in Polish decryption equipment, which they could not meet. On 25 July 1939, the Polish Cipher Bureau handed reconstructed Enigma machines and their techniques for decrypting ciphers to the French and British. Gordon Welchman wrote,\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;Ultra would never have got off the ground if we had not learned from the Poles, in the nick of time, the details both of the German military Enigma machine, and of the operating procedures that were in use.\u2014\u200a\nAt Bletchley Park, some of the key people responsible for success against Enigma included mathematicians Alan Turing and Hugh Alexander and, at the British Tabulating Machine Company, chief engineer Harold Keen.\nAfter the war, interrogation of German cryptographic personnel led to the conclusion that German cryptanalysts understood that cryptanalytic attacks against Enigma were possible but were thought to require impracticable amounts of effort and investment. The Poles' early start at breaking Enigma and the continuity of their success gave the Allies an advantage when World War II began.\nLorenz cipher.\nIn June 1941, the Germans started to introduce on-line stream cipher teleprinter systems for strategic point-to-point radio links, to which the British gave the code-name Fish. Several systems were used, principally the Lorenz SZ 40/42 (codenamed \"Tunny\" by the British) and Geheimfernschreiber (\"Sturgeon\"). These cipher systems were cryptanalysed, particularly Tunny, which the British thoroughly penetrated. It was eventually attacked using Colossus machines, which were the first digital programme-controlled electronic computers. In many respects the Tunny work was more difficult than for the Enigma, since the British codebreakers had no knowledge of the machine producing it and no head-start such as that the Poles had given them against Enigma.\nAlthough the volume of intelligence derived from this system was much smaller than that from Enigma, its importance was often far higher because it produced primarily high-level, strategic intelligence that was sent between Wehrmacht high command (Oberkommando der Wehrmacht, OKW). The eventual bulk decryption of Lorenz-enciphered messages contributed significantly, and perhaps decisively, to the defeat of Nazi Germany. Nevertheless, the Tunny story has become much less well known among the public than the Enigma one. At Bletchley Park, some of the key people responsible for success in the Tunny effort included mathematicians W. T. \"Bill\" Tutte and Max Newman and electrical engineer Tommy Flowers.\nItalian.\nIn June 1940, the Italians were using book codes for most of their military messages, except for the Italian Navy, which in early 1941 had started using a version of the Hagelin rotor-based cipher machine C-38. This was broken from June 1941 onwards by the Italian subsection of GC&amp;CS at Bletchley Park.\nJapanese.\nIn the Pacific theatre, a Japanese cipher machine, called \"Purple\" by the Americans, was used for highest-level Japanese diplomatic traffic. It produced a polyalphabetic substitution cipher, but unlike Enigma, was not a rotor machine, being built around electrical stepping switches. It was broken by the US Army Signal Intelligence Service and disseminated as \"Magic\". Detailed reports by the Japanese ambassador to Germany were encrypted on the Purple machine. His reports included reviews of German assessments of the military situation, reviews of strategy and intentions, reports on direct inspections by the ambassador (in one case, of Normandy beach defences), and reports of long interviews with Hitler. The Japanese are said to have obtained an Enigma machine in 1937, although it is debated whether they were given it by the Germans or bought a commercial version, which, apart from the plugboard and internal wiring, was the German \"Heer/Luftwaffe\" machine. Having developed a similar machine, the Japanese did not use the Enigma machine for their most secret communications.\nThe chief fleet communications code system used by the Imperial Japanese Navy was called JN-25 by the Americans, and by early 1942 the US Navy had made considerable progress in decrypting Japanese naval messages. The US Army also made progress on the Japanese Army's codes in 1943, including codes used by supply ships, resulting in heavy losses to their shipping.\nDistribution.\nArmy- and Air Force-related intelligence derived from signals intelligence (SIGINT) sources\u00a0\u2013 mainly Enigma decrypts in Hut 6\u00a0\u2013 was compiled in summaries at GC&amp;CS (Bletchley Park) Hut 3 and distributed initially under the codeword \"BONIFACE\", implying that it was acquired from a well placed agent in Berlin. The volume of the intelligence reports going out to commanders in the field built up gradually.\nNaval Enigma decrypted in Hut 8 was forwarded from Hut 4 to the Admiralty's Operational Intelligence Centre (OIC), which distributed it initially under the codeword \"HYDRO\".\nThe codeword \"ULTRA\" was adopted in June 1941. This codeword was reportedly suggested by Commander Geoffrey Colpoys, RN, who served in the Royal Navy's OIC.\nArmy and Air Force.\nThe distribution of Ultra information to Allied commanders and units in the field involved considerable risk of discovery by the Germans, and great care was taken to control both the information and knowledge of how it was obtained. Liaison officers were appointed for each field command to manage and control dissemination.\nDissemination of Ultra intelligence to field commanders was carried out by MI6, which operated Special Liaison Units (SLU) attached to major army and air force commands. The activity was organized and supervised on behalf of MI6 by Group Captain F. W. Winterbotham. Each SLU included intelligence, communications, and cryptographic elements. It was headed by a British Army or RAF officer, usually a major, known as \"Special Liaison Officer\". The main function of the liaison officer or his deputy was to pass Ultra intelligence bulletins to the commander of the command he was attached to, or to other indoctrinated staff officers. In order to safeguard Ultra, special precautions were taken. The standard procedure was for the liaison officer to present the intelligence summary to the recipient, stay with him while he studied it, then take it back and destroy it.\nBy the end of the war, there were about 40 SLUs serving commands around the world. Fixed SLUs existed at the Admiralty, the War Office, the Air Ministry, RAF Fighter Command, the US Strategic Air Forces in Europe (Wycombe Abbey) and other fixed headquarters in the UK. An SLU was operating at the War HQ in Valletta, Malta. These units had permanent teleprinter links to Bletchley Park.\nMobile SLUs were attached to field army and air force headquarters and depended on radio communications to receive intelligence summaries. The first mobile SLUs appeared during the French campaign of 1940. An SLU supported the British Expeditionary Force (BEF) headed by General Lord Gort. The first liaison officers were Robert Gore-Browne and Humphrey Plowden. A second SLU of the 1940 period was attached to the RAF Advanced Air Striking Force at Meaux commanded by Air Vice-Marshal P H Lyon Playfair. This SLU was commanded by Squadron Leader F.W. \"Tubby\" Long.\nIntelligence agencies.\nIn 1940, special arrangements were made within the British intelligence services for handling BONIFACE and later Ultra intelligence. The Security Service started \"Special Research Unit B1(b)\" under Herbert Hart. In the SIS this intelligence was handled by \"Section V\" based at St Albans.\nRadio and cryptography.\nThe communications system was founded by Brigadier Sir Richard Gambier-Parry, who from 1938 to 1946 was head of MI6 Section VIII, based at Whaddon Hall in Buckinghamshire, UK. Ultra summaries from Bletchley Park were sent over landline to the Section VIII radio transmitter at Windy Ridge. From there they were transmitted to the destination SLUs.\nThe communications element of each SLU was called a \"Special Communications Unit\" or SCU. Radio transmitters were constructed at Whaddon Hall workshops, while receivers were the National HRO, made in the USA. The SCUs were highly mobile and the first such units used civilian Packard cars. The following SCUs are listed: SCU1 (Whaddon Hall), SCU2 (France before 1940, India), SCU3 (RSS Hanslope Park), SCU5, SCU6 (possibly Algiers and Italy), SCU7 (training unit in the UK), SCU8 (Europe after D-day), SCU9 (Europe after D-day), SCU11 (Palestine and India), SCU12 (India), SCU13 and SCU14.\nThe cryptographic element of each SLU was supplied by the RAF and was based on the TYPEX cryptographic machine and one-time pad systems.\nRN Ultra messages from the OIC to ships at sea were necessarily transmitted over normal naval radio circuits and were protected by one-time pad encryption.\nLucy.\nIt is alleged that Ultra information was used by the \"Lucy\" spy ring, headquartered in Switzerland and apparently operated by one man, Rudolf Roessler. This was an extremely well informed, responsive ring that was able to get information \"directly from German General Staff Headquarters\"\u00a0\u2013 often on specific request. It has been alleged that \"Lucy\" was in major part a conduit for the British to feed Ultra intelligence to the Soviets in a way that made it appear to have come from highly placed espionage rather than from cryptanalysis of German radio traffic. The Soviets, however, through an agent at Bletchley, John Cairncross, knew that Britain had broken Enigma. The \"Lucy\" ring was initially treated with suspicion by the Soviets. The information it provided was accurate and timely, however, and Soviet agents in Switzerland (including their chief, Alexander Rad\u00f3) eventually learned to take it seriously. However, the theory that the Lucy ring was a cover for Britain to pass Enigma intelligence to the Soviets has not gained traction. Among others who have rejected the theory, Harry Hinsley, the official historian for the British Secret Services in World War II, stated that \"there is no truth in the much-publicized claim that the British authorities made use of the \u2018Lucy\u2019 ring\u00a0... to forward intelligence to Moscow\".\nUse of intelligence.\nMost deciphered messages, often about relative trivia, were insufficient as intelligence reports for military strategists or field commanders. The organisation, interpretation and distribution of decrypted Enigma message traffic and other sources into usable intelligence was a subtle task.\nAt Bletchley Park, extensive indices were kept of the information in the messages decrypted. For each message the traffic analysis recorded the radio frequency, the date and time of intercept, and the preamble\u00a0\u2013 which contained the network-identifying discriminant, the time of origin of the message, the callsign of the originating and receiving stations, and the indicator setting. This allowed cross referencing of a new message with a previous one. The indices included message preambles, every person, every ship, every unit, every weapon, every technical term and of repeated phrases such as forms of address and other German military jargon that might be usable as \"cribs\".\nThe first decryption of a wartime Enigma message, albeit one that had been transmitted three months earlier, was achieved by the Poles at PC Bruno on 17 January 1940. Little had been achieved by the start of the Allied campaign in Norway in April. At the start of the Battle of France on 10 May 1940, the Germans made a very significant change in the indicator procedures for Enigma messages. However, the Bletchley Park cryptanalysts had anticipated this, and were able\u00a0\u2013 jointly with PC Bruno\u00a0\u2013 to resume breaking messages from 22 May, although often with some delay. The intelligence that these messages yielded was of little operational use in the fast-moving situation of the German advance.\nDecryption of Enigma traffic built up gradually during 1940, with the first two prototype bombes being delivered in March and August. The traffic was almost entirely limited to \"Luftwaffe\" messages. By the peak of the Battle of the Mediterranean in 1941, however, Bletchley Park was deciphering daily 2,000 Italian Hagelin messages. By the second half of 1941 30,000 Enigma messages a month were being deciphered, rising to 90,000 a month of Enigma and Fish decrypts combined later in the war.\nSome of the contributions that Ultra intelligence made to the Allied successes are given below.\nSafeguarding of sources.\nThe Allies were seriously concerned with the prospect of the Axis command finding out that they had broken into the Enigma traffic. The British were more disciplined about such measures than the Americans, and this difference was a source of friction between them.\nTo disguise the source of the intelligence for the Allied attacks on Axis supply ships bound for North Africa, \"spotter\" submarines and aircraft were sent to search for Axis ships. These searchers or their radio transmissions were observed by the Axis forces, who concluded their ships were being found by conventional reconnaissance. They suspected that there were some 400 Allied submarines in the Mediterranean and a huge fleet of reconnaissance aircraft on Malta. In fact, there were only 25 submarines and at times as few as three aircraft.\nThis procedure also helped conceal the intelligence source from Allied personnel, who might give away the secret by careless talk, or under interrogation if captured. Along with the search mission that would find the Axis ships, two or three additional search missions would be sent out to other areas, so that crews would not begin to wonder why a single mission found the Axis ships every time.\nOther deceptive means were used. On one occasion, a convoy of five ships sailed from Naples to North Africa with essential supplies at a critical moment in the North African fighting. There was no time to have the ships properly spotted beforehand. The decision to attack solely on Ultra intelligence went directly to Churchill. The ships were all sunk by an attack \"out of the blue\", arousing German suspicions of a security breach. To distract the Germans from the idea of a signals breach (such as Ultra), the Allies sent a radio message to a fictitious spy in Naples, congratulating him for this success. According to some sources the Germans decrypted this message and believed it.\nIn the Battle of the Atlantic, the precautions were taken to the extreme. In most cases where the Allies knew from intercepts the location of a U-boat in mid-Atlantic, the U-boat was not attacked immediately, until a \"cover story\" could be arranged. For example, a search plane might be \"fortunate enough\" to sight the U-boat, thus explaining the Allied attack.\nSome Germans had suspicions that all was not right with Enigma. Admiral Karl D\u00f6nitz received reports of \"impossible\" encounters between U-boats and enemy vessels which made him suspect some compromise of his communications. In one instance, three U-boats met at a tiny island in the Caribbean Sea, and a British destroyer promptly showed up. The U-boats escaped and reported what had happened. D\u00f6nitz immediately asked for a review of Enigma's security. The analysis suggested that the signals problem, if there was one, was not due to the Enigma itself. D\u00f6nitz had the settings book changed anyway, blacking out Bletchley Park for a period. However, the evidence was never enough to truly convince him that Naval Enigma was being read by the Allies. The more so, since \"B-Dienst\", his own codebreaking group, had partially broken Royal Navy traffic (including its convoy codes early in the war), and supplied enough information to support the idea that the Allies were unable to read Naval Enigma.\nBy 1945, most German Enigma traffic could be decrypted within a day or two, yet the Germans remained confident of its security.\nRole of women in Allied codebreaking.\nAfter encryption systems were \"broken\", there was a large volume of cryptologic work needed to recover daily key settings and keep up with changes in enemy security procedures, plus the more mundane work of processing, translating, indexing, analyzing and distributing tens of thousands of intercepted messages daily. The more successful the code breakers were, the more labor was required. Some 8,000 women worked at Bletchley Park, about three quarters of the work force. Before the attack on Pearl Harbor, the US Navy sent letters to top women's colleges seeking introductions to their best seniors; the Army soon followed suit. By the end of the war, some 7000 workers in the Army Signal Intelligence service, out of a total 10,500, were female. By contrast, the Germans and Japanese had strong ideological objections to women engaging in war work. The Nazis even created a Cross of Honour of the German Mother to encourage women to stay at home and have babies.\nPostwar consequences.\nThe mystery surrounding the discovery of the sunk off the coast of New Jersey by divers Richie Kohler and John Chatterton was unravelled in part through the analysis of Ultra intercepts, which demonstrated that, although \"U-869\" had been ordered by U-boat Command to change course and proceed to North Africa, near Rabat, the submarine had missed the messages changing her assignment and had continued to the eastern coast of the U.S., her original destination.\nIn 1953, the CIA's Project ARTICHOKE, a series of experiments on human subjects to develop drugs for use in interrogations, was renamed Project MKUltra. MK was the CIA's designation for its Technical Services Division and Ultra was in reference to the Ultra project.\nPostwar secrecy.\nSecrecy and initial silence (1945\u20131960s).\nUntil the mid 1970s, the thirty year rule meant that there was no official mention of Bletchley Park. This meant that although there were many operations where codes broken by Bletchley Park played an important role, this was not present in the history of those events. Churchill's series The Second World War did mention Enigma but not that it had been broken.\nWhile it is obvious why Britain and the U.S. went to considerable pains to keep Ultra a secret until the end of the war, it has been a matter of some conjecture why Ultra was kept officially secret for 29 years thereafter, until 1974. During that period, the important contributions to the war effort of a great many people remained unknown, and they were unable to share in the glory of what is now recognised as one of the chief reasons the Allies won the war \u2013 or, at least, as quickly as they did.\nAt least three explanations exist as to why Ultra was kept secret so long. Each has plausibility, and all may be true. First, as David Kahn pointed out in his 1974 \"New York Times\" review of Winterbotham's \"The Ultra Secret\", after the war, surplus Enigmas and Enigma-like machines were sold to Third World countries, which remained convinced of the security of the remarkable cipher machines. Their traffic was not as secure as they believed, however, which is one reason the British made the machines available.\nBy the 1970s, newer computer-based ciphers were becoming popular as the world increasingly turned to computerised communications, and the usefulness of Enigma copies (and rotor machines generally) rapidly decreased. Switzerland developed its own version of Enigma, known as NEMA, and used it into the late 1970s, while the United States National Security Agency (NSA) retired the last of its rotor-based encryption systems, the KL-7 series, in the 1980s.\nA second explanation relates to a misadventure of one of Churchill's predecessors, Stanley Baldwin, between the World Wars, when he publicly disclosed information from decrypted Soviet communications about the General Strike. This had prompted the Soviets to change their ciphers, leading to a blackout.\nThe third explanation is given by Winterbotham, who recounts that two weeks after V-E Day, on 25 May 1945, Churchill requested former recipients of Ultra intelligence not to divulge the source or the information that they had received from it, in order that there be neither damage to the future operations of the Secret Service nor any cause for the Axis to blame Ultra for their defeat.\nPartial disclosures.\nIn 1967, Polish military historian W\u0142adys\u0142aw Kozaczuk in his book \"Bitwa o tajemnice\" (\"Battle for Secrets\") first revealed Enigma had been broken by Polish cryptologists before World War II.\nAlso published in 1967, David Kahn's comprehensive chronicle of the history of cryptography, \"The Codebreakers\", does not mention Bletchley Park, although it does make the claim that Soviet forces were reading Enigma messages by 1942. He also described the 1944 capture of a naval Enigma machine from and gave the first published hint about the scale, mechanisation and operational importance of the Anglo-American Enigma-breaking operation:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;The Allies now read U-boat operational traffic. For they had, more than a year before the theft, succeeded in solving the difficult U-boat systems, and \u2013 in one of the finest cryptanalytic achievements of the war \u2013 managed to read the intercepts on a current basis. For this, the cryptanalysts needed the help of a mass of machinery that filled two buildings.\nLadislas Farago's 1971 best-seller \"The Game of the Foxes\" gave an early garbled version of the myth of the purloined Enigma. According to Farago, it was thanks to a \"Polish-Swedish ring [that] the British obtained a working model of the 'Enigma' machine, which the Germans used to encipher their top-secret messages.\" \"It was to pick up one of these machines that Commander Denniston went clandestinely to a secluded Polish castle [!] on the eve of the war. Dilly Knox later solved its keying, exposing all Abwehr signals encoded by this system.\" \"In 1941 [t]he brilliant cryptologist Dillwyn Knox, working at the Government Code &amp; Cypher School at the Bletchley centre of British code-cracking, solved the keying of the Abwehr's Enigma machine.\"\n1970s.\nThe 1973 public disclosure of Enigma decryption in the book \"Enigma\" by French intelligence officer Gustave Bertrand \u2013 which dealt mainly with the Polish and then Franco-Polish efforts before the Invasion of France and before the Ultra program \u2013 generated pressure to discuss the rest of the Enigma\u2013Ultra story.\nSince it was British and, later, American message-breaking which had been the most extensive, the importance of Enigma decrypts to the prosecution of the war remained unknown despite revelations by the Poles and the French of their early work on breaking the Enigma cipher. This work, which was carried out in the 1930s and continued into the early part of the war, was necessarily uninformed regarding further breakthroughs achieved by the Allies during the balance of the war.\nThe British ban was finally lifted in 1974, the year that a key participant on the distribution side of the Ultra project, F. W. Winterbotham, published \"The Ultra Secret\". Winterbotham's book was written from memory and although officially allowed, there was no access to archives. Public discussion of Bletchley Park's work in the English speaking world finally became accepted, although some former staff considered themselves bound to silence forever.\nOther books such as Anthony Cave Brown's \"Bodyguard of Lies\" and William Stevenson's \"A Man called Intrepid\" were also being written at this time, and the military historian Harold C. Deutsch regards Winterbotham's revelations as only to have anticipated what were going to be a number of revelations.\nPublic interest.\nA succession of books by former participants and others followed. The official history of British intelligence in World War II was published in five volumes from 1979 to 1988, and included further details from official sources concerning the availability and employment of Ultra intelligence. It was chiefly edited by Harry Hinsley, with one volume by Michael Howard. There is also a one-volume collection of reminiscences by Ultra veterans, \"Codebreakers\" (1993), edited by Hinsley and Alan Stripp.\nContinued selective secrecy.\nIn 2012, Alan Turing's last two papers on Enigma decryption were released to Britain's National Archives. The Departmental Historian at GCHQ stated that the seven decades' delay had been due to their \"continuing sensitivity... It wouldn't have been safe to release [them earlier].\"\nHistorical debates on Ultra.\nHolocaust intelligence.\nHistorians and Holocaust researchers have tried to establish when the Allies realized the full extent of Nazi-era extermination of Jews, and specifically, the extermination-camp system. In 1999, the U.S. Government passed the Nazi War Crimes Disclosure Act (P.L. 105-246), making it policy to declassify all Nazi war crime documents in their files; this was later amended to include the Japanese Imperial Government. As a result, more than 600 decrypts and translations of intercepted messages were disclosed; NSA historian Robert Hanyok would conclude that Allied communications intelligence, \"by itself, could not have provided an early warning to Allied leaders regarding the nature and scope of the Holocaust.\"\nFollowing Operation Barbarossa, decrypts in August 1941 alerted British authorities to the many massacres in occupied zones of the Soviet Union, including those of Jews, but specifics were not made public for security reasons. Revelations about the concentration camps were gleaned from other sources, and were publicly reported by the Polish government-in-exile, Jan Karski and the WJC offices in Switzerland a year or more later. A decrypted message referring to \"Einsatz Reinhard\" (the H\u00f6fle telegram), from 11 January 1943 may have outlined the system and listed the number of Jews and others gassed at four death camps the previous year, but codebreakers did not understand the meaning of the message. In summer 1944, Arthur Schlesinger, an OSS analyst, interpreted the intelligence as an \"incremental increase in persecution rather than\u00a0... extermination\".\nOverall effect on the War.\nThe existence of Ultra was kept secret for many years after the war. Since the Ultra story was widely disseminated by Winterbotham in his 1974 book \"The Ultra Secret\", historians have altered the historiography of World War II. For example, Andrew Roberts, writing in the 21st century, stated of Montgomery's handling of the Second Battle of El Alamein, \"Because he had the invaluable advantage of being able to read [Field Marshal Erwin Rommel's] Enigma communications, Montgomery knew how short the Germans were of men, ammunition, food and above all fuel. When he put Rommel's picture up in his caravan he wanted to be seen to be almost reading his opponent's mind. In fact he was reading his mail.\" Over time, Ultra has become embedded in the public consciousness and Bletchley Park has become a significant visitor attraction. As stated by historian Thomas Haigh, \"The British code-breaking effort of the Second World War, formerly secret, is now one of the most celebrated aspects of modern British history, an inspiring story in which a free society mobilized its intellectual resources against a terrible enemy.\"\nEffect on the duration of the War.\nThere has been controversy about the influence of Allied Enigma decryption on the course of World War II with three views \u2013 that without Ultra the outcome of the war would be different, that without Ultra the Allies would have still won but that it was shortened by two years and that while useful Ultra decrypts were largely incidental to the fact and timing of the Allied victory.\nAn oft-repeated assessment is that decryption of German ciphers advanced the end of the European war by no less than two years. Hinsley, who first made this claim, is typically cited as an authority for the two-year estimate.\nWinterbotham's quoting of Eisenhower's \"decisive\" verdict is part of a letter sent by Eisenhower to Menzies after the conclusion of the European war and later found among his papers at the Eisenhower Presidential Library. It allows a contemporary, documentary view of a leader on Ultra's importance:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;July 1945\nDear General Menzies:\nI had hoped to be able to pay a visit to Bletchley Park in order to thank you, Sir Edward Travis, and the members of the staff personally for the magnificent service which has been rendered to the Allied cause.\nI am very well aware of the immense amount of work and effort which has been involved in the production of the material with which you supplied us. I fully realize also the numerous setbacks and difficulties with which you have had to contend and how you have always, by your supreme efforts, overcome them.\nThe intelligence which has emanated from you before and during this campaign has been of priceless value to me. It has simplified my task as a commander enormously. It has saved thousands of British and American lives and, in no small way, contributed to the speed with which the enemy was routed and eventually forced to surrender.\nI should be very grateful, therefore, if you would express to each and every one of those engaged in this work from me personally my heartfelt admiration and sincere thanks for their very decisive contribution to the Allied war effort.\nSincerely,\nDwight D. Eisenhower\nThere is wide disagreement about the importance of codebreaking in winning the crucial Battle of the Atlantic. To cite just one example, the historian Max Hastings states that \"In 1941 alone, Ultra saved between 1.5 and two million tons of Allied ships from destruction.\" This would represent a 40 percent to 53 percent reduction, though it is not clear how this extrapolation was made.\nAnother view is from a history based on the German naval archives written after the war for the British Admiralty by a former U-boat commander and son-in-law of his commander, Grand Admiral Karl D\u00f6nitz. His book reports that several times during the war they undertook detailed investigations to see whether their operations were being compromised by broken Enigma ciphers. These investigations were spurred because the Germans had broken the British naval code and found the information useful. Their investigations were negative, and the conclusion was that their defeat \"was due firstly to outstanding developments in enemy radar...\" The great advance was centimetric radar, developed in a joint British-American venture, which became operational in the spring of 1943. Earlier radar was unable to distinguish U-boat conning towers from the surface of the sea, so it could not even locate U-boats attacking convoys on the surface on moonless nights; thus the surfaced U-boats were almost invisible, while having the additional advantage of being swifter than their prey. The new higher-frequency radar could spot conning towers, and periscopes could even be detected from airplanes. Some idea of the relative effect of cipher-breaking and radar improvement can be obtained from graphs showing the tonnage of merchantmen sunk and the number of U-boats sunk in each month of the Battle of the Atlantic. The graphs cannot be interpreted unambiguously, because it is challenging to factor in many variables such as improvements in cipher-breaking and the numerous other advances in equipment and techniques used to combat U-boats. Nonetheless, the data seem to favor the view of the former U-boat commander\u00a0\u2013 that radar was crucial.\nWhile Ultra certainly affected the course of the Western Front during the war, two factors often argued against Ultra having shortened the overall war by a measure of years are the relatively small role it played in the Eastern Front conflict between Germany and the Soviet Union, and the completely independent development of the U.S.-led Manhattan Project to create the atomic bomb. Author Jeffrey T. Richelson mentions Hinsley's estimate of at least two years, and concludes that \"It might be more accurate to say that Ultra helped shorten the war by three months \u2013 the interval between the actual end of the war in Europe and the time the United States would have been able to drop an atomic bomb on Hamburg or Berlin \u2013 and might have shortened the war by as much as two years had the U.S. atomic bomb program been unsuccessful.\" Military historian Guy Hartcup analyzes aspects of the question but then simply says, \"It is impossible to calculate in terms of months or years how much Ultra shortened the war.\"\nF. W. Winterbotham, the first author to outline the influence of Enigma decryption on the course of World War II, likewise made the earliest contribution to an appreciation of Ultra's \"postwar\" influence, which now continues into the 21st century\u00a0\u2013 and not only in the postwar establishment of Britain's GCHQ (Government Communication Headquarters) and the United States' NSA. \"Let no one be fooled\", Winterbotham admonishes in chapter 3, \"by the spate of television films and propaganda which has made the war seem like some great triumphant epic. It was, in fact, a very narrow shave, and the reader may like to ponder [...] whether [...] we might have won [without] Ultra.\"\nIain Standen, Chief Executive of the Bletchley Park Trust, says of the work done there: \"It was crucial to the survival of Britain, and indeed of the West.\" The Departmental Historian at GCHQ (the Government Communications Headquarters), who identifies himself only as \"Tony\" but seems to speak authoritatively, says that Ultra was a \"major force multiplier. It was the first time that quantities of real-time intelligence became available to the British military.\"\nAccording to the official historian of British Intelligence, Ultra intelligence shortened the war by two to four years, and without it the outcome of the war would have been uncertain.\nContribution to the Cold War.\nPhillip Knightley suggests that Ultra may have contributed to the development of the Cold War. The Soviets received disguised Ultra information, but the existence of Ultra itself was not disclosed by the western Allies. The Soviets, who had clues to Ultra's existence, possibly through Kim Philby, John Cairncross and Anthony Blunt, may thus have felt still more distrustful of their wartime partners.\nDebate continues on whether, had postwar political and military leaders been aware of Ultra's role in Allied victory in World War II, these leaders might have been less optimistic about post-World War II military involvements. Christopher Kasparek writes: \"Had the... postwar governments of major powers realized ... how Allied victory in World War II had hung by a slender thread first spun by three mathematicians [Rejewski, R\u00f3\u017cycki, Zygalski] working on Enigma decryption for the general staff of a seemingly negligible power [Poland], they might have been more cautious in picking their own wars.\" A kindred point concerning postwar American triumphalism is made by British historian Max Hastings, author of \"Inferno: The World at War, 1939\u20131945\".\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nSources.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;\nFurther reading.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "31750", "revid": "37911681", "url": "https://en.wikipedia.org/wiki?curid=31750", "title": "Ukraine", "text": "Country in Eastern Europe\nUkraine is a country in Eastern Europe. It is the second-largest country in Europe after Russia, which borders it to the east and northeast. Ukraine also borders Belarus to the north; Poland and Slovakia to the west; Hungary, Romania and Moldova to the southwest; and the Black Sea and the Sea of Azov to the south and southeast. Kyiv is the nation's capital and largest city, followed by Kharkiv, Odesa, and Dnipro. Ukraine's official language is Ukrainian.\nHumans have inhabited Ukraine since 32,000 BC. During the Middle Ages, it was the site of early Slavic expansion and later became a key centre of East Slavic culture under the state of Kievan Rus', which emerged in the 9th century. Kievan Rus' became the largest and most powerful realm in Europe in the 10th and 11th centuries, but gradually disintegrated into rival regional powers before being destroyed by the Mongols in the 13th century. For the next 600 years the area was contested, divided, and ruled by a variety of external powers, including the Grand Duchy of Lithuania, the Kingdom of Poland, the Polish\u2013Lithuanian Commonwealth, the Austrian Empire, the Ottoman Empire, and the Tsardom of Russia.\nThe Cossack Hetmanate emerged in central Ukraine in the 17th century but was partitioned between Russia and Poland before being absorbed by the Russian Empire in the late 19th century. Ukrainian nationalism developed and, following the Russian Revolution in 1917, the short-lived Ukrainian People's Republic was formed. The Bolsheviks consolidated control over much of the former empire and established the Ukrainian Soviet Socialist Republic, which became a constituent republic of the Soviet Union in 1922. In the early 1930s, millions of Ukrainians died in the Holodomor, a human-made famine. During World War II, Ukraine was occupied by Germany and endured major battles and atrocities, resulting in 7 million civilians killed, including most Ukrainian Jews.\nUkraine gained independence in 1991 as the Soviet Union dissolved, declaring itself neutral. A new constitution was adopted in 1996 as the country transitioned to a free market liberal democracy amid endemic corruption and a legacy of state control. The Orange Revolution of 2004\u20132005 ushered electoral and constitutional reforms. Resurgent political crises prompted a series of mass demonstrations in 2014 known as the Euromaidan, leading to a revolution, at the end of which Russia unilaterally occupied and annexed Ukraine's Crimean Peninsula, and pro-Russian unrest culminated in a war in Donbas with Russian-backed separatists and Russia. In 2022, Russia launched a full-scale invasion of Ukraine, beginning the current phase of the war.\nUkraine is a unitary state and its system of government is a semi-presidential republic. Ukraine has a transition economy and has the lowest nominal GDP per capita in Europe as of 2024, with corruption being a significant issue. Due to its extensive fertile land, the country is an important exporter of grain, though grain production has declined since 2022 due to the Russian invasion, endangering global food security. Ukraine is considered a middle power in global affairs. Its military is the sixth largest in the world with the eighth largest defence budget, and operates one of the world's largest and most diverse drone fleets. Ukraine is a founding member of the United Nations and a member of the Council of Europe, the World Trade Organisation, and the OSCE. It has been in the process of joining the European Union and applied to join NATO in 2022.\n&lt;templatestyles src=\"Template:TOC limit/styles.css\" /&gt;\nName.\nThe name of Ukraine is frequently interpreted as coming from the old Slavic term for 'borderland' as is the word \"krajina\". \nAnother interpretation is that the name of Ukraine means \"region\" or \"country\".\nIn the English-speaking world during most of the 20th century, Ukraine (whether independent or not) was referred to as \"the Ukraine\". This is because in Russian, the word \"ukraina\" means 'borderland' so the definite article would be natural in the English language; this is similar to \", which means 'low lands' and is rendered in English as \"the\" Netherlands\". However, since Ukraine's declaration of independence in 1991, this usage has become politicised and is now rarer, and style guides advise against its use. U.S. ambassador William Taylor said that using \"the Ukraine\" implies disregard for Ukrainian sovereignty. The official Ukrainian position is that \"the Ukraine\" is both grammatically and politically incorrect.\nHistory.\nEarly history.\nEvidence for the earliest securely dated hominin presence in Europe comes from 1.4 million-year-old stone tools from Korolevo, in western Ukraine. Settlement by modern humans in Ukraine and its vicinity dates back to 32,000 BC, with evidence of the Gravettian culture in the Crimean Mountains. By 4,500 BC, the Neolithic Cucuteni\u2013Trypillia culture was flourishing in wide areas of modern Ukraine, including Trypillia and the entire Dnieper-Dniester region. Ukraine is a probable location for the first domestication of the horse. The Kurgan hypothesis places the Volga-Dnieper region of Ukraine and southern Russia as the linguistic homeland of the Proto-Indo-Europeans. Early Indo-European migrations from the Pontic steppes in the 3rd millennium BC spread Yamnaya Steppe pastoralist ancestry and Indo-European languages across large parts of Europe. During the Iron Age, the land was inhabited by Iranian-speaking Cimmerians, Scythians, and Sarmatians. Between 700\u00a0BC and 200\u00a0BC it was part of the Scythian kingdom.\nFrom the 6th\u00a0century BC, Greek, Roman, and Byzantine colonies were established on the north-eastern shore of the Black Sea, such as at Tyras, Olbia, and Chersonesus. These thrived into the 6th\u00a0century AD. The Goths stayed in the area, but came under the sway of the Huns from the 370s. In the 7th\u00a0century, the territory that is now eastern Ukraine was the centre of Old Great Bulgaria. At the end of the century, the majority of Bulgar tribes migrated in different directions, and the Khazars took over much of the land.\nIn the 5th and 6th centuries, the Antes, which some relate as an early Slavic people, lived in Ukraine. Migrations from the territories of present-day Ukraine throughout the Balkans established many South Slavic nations. Northern migrations, reaching almost to Lake Ilmen, led to the emergence of the Ilmen Slavs and Krivichs. Following an Avar raid in 602 and the collapse of the Antes Union, most of these peoples survived as separate tribes until the beginning of the second millennium.\nGolden Age of Kyiv.\nThe establishment of the state of Kievan Rus' remains obscure and uncertain. The state included much of present-day Ukraine, Belarus and the western part of European Russia. According to the \"Primary Chronicle\", the Rus' people initially consisted of Varangians from Scandinavia. In 882, the pagan Prince Oleg (Oleh) conquered Kyiv from Askold and Dir and proclaimed it as the new capital of the Rus'. Anti-Normanist historians however argue that the East Slavic tribes along the southern parts of the Dnieper River were already in the process of forming a state independently. The Varangian elite, including the ruling Rurik dynasty, later assimilated into the Slavic population. Kievan Rus' was composed of several principalities ruled by the interrelated Rurikid \"kniazes\" (\"princes\"), who often fought each other for possession of Kyiv.\nDuring the 10th and 11th\u00a0centuries, Kievan Rus' became the largest and most powerful state in Europe, a period known as its Golden Age. It began with the reign of Vladimir the Great (980\u20131015), who introduced Christianity. During the reign of his son, Yaroslav the Wise (1019\u20131054), Kievan Rus' reached the zenith of its cultural development and military power. The state soon fragmented as the relative importance of regional powers rose again. After a final resurgence under the rule of Vladimir II Monomakh (1113\u20131125) and his son Mstislav (1125\u20131132), Kievan Rus' finally disintegrated into separate principalities following Mstislav's death, though ownership of Kyiv would still carry great prestige for decades. In the 11th and 12th centuries, the nomadic confederacy of the Turkic-speaking Cumans and Kipchaks was the dominant force in the Pontic steppe north of the Black Sea.\nThe Mongol invasions in the mid-13th century devastated Kievan Rus'; following the Siege of Kyiv in 1240, the city was destroyed by the Mongols. In the western territories, the principalities of Halych and Volhynia had arisen earlier, and were merged to form the Principality of Galicia\u2013Volhynia. Daniel of Galicia, son of Roman the Great, re-united much of south-western Rus', including Volhynia, Galicia, as well as Kyiv. He was subsequently crowned by a papal envoy as the first king of Galicia\u2013Volhynia (also known as the Kingdom of Ruthenia) in 1253.\nForeign domination.\nIn 1349, in the aftermath of the Galicia\u2013Volhynia Wars, the region was partitioned between the Kingdom of Poland and the Grand Duchy of Lithuania. From the mid-13th century to the late 1400s, the Republic of Genoa founded numerous colonies on the northern coast of the Black Sea and transformed these into large commercial centres headed by the consul, a representative of the Republic. In 1430, the region of Podolia was incorporated into Poland, and the lands of modern-day Ukraine became increasingly settled by Poles. In 1441, Genghisid prince Haci I Giray founded the Crimean Khanate on the Crimean Peninsula and the surrounding steppes; the Khanate orchestrated Tatar slave raids. Over the next three centuries, the Crimean slave trade would enslave an estimated two million in the region.\nIn 1569, the Union of Lublin established the Polish\u2013Lithuanian Commonwealth, and most of the Ukrainian lands were transferred from Lithuania to the Crown of the Kingdom of Poland, becoming \"de jure\" Polish territory. Under the pressures of Polonisation, many landed gentry of Ruthenia converted to Catholicism and joined the circles of the Polish nobility; others joined the newly created Ruthenian Uniate Church.\nCossack Hetmanate.\nDeprived of native protectors among the Ruthenian nobility, the peasants and townspeople began turning for protection to the emerging Zaporozhian Cossacks. In the mid-17th\u00a0century, a Cossack military quasi-state, the Zaporozhian Host, was formed by Dnieper Cossacks and Ruthenian peasants. Poland exercised little real control over this population, but found the Cossacks to be useful against the Turks and Tatars, and at times the two were allies in military campaigns. However, the continued harsh enserfment of Ruthenian peasantry by Polish szlachta (many of whom were Polonised Ruthenian nobles) and the suppression of the Orthodox Church alienated the Cossacks. The latter did not shy from taking up arms against those they perceived as enemies and occupiers, including the Catholic Church with its local representatives.\nIn 1648, Bohdan Khmelnytsky led the largest of the Cossack uprisings against the Commonwealth and the Polish king, which enjoyed wide support from the local population. Khmelnytsky founded the Cossack Hetmanate, which existed until 1764 (some sources claim until 1782). After Khmelnytsky suffered a crushing defeat at the Battle of Berestechko in 1651, he turned to the Russian tsar for help. In 1654, Khmelnytsky was subject to the Pereiaslav Agreement, forming a military and political alliance with Russia that acknowledged loyalty to the Russian monarch.\nAfter his death, the Hetmanate went through a devastating 30-year war amongst Russia, Poland, the Crimean Khanate, the Ottoman Empire, and Cossacks, known as \"The Ruin\" (1657\u20131686), for control of the Cossack Hetmanate. The Treaty of Perpetual Peace between Russia and Poland in 1686 divided the lands of the Cossack Hetmanate between them, reducing the portion over which Poland had claimed sovereignty to Ukraine west of the Dnieper river. In 1686, the Metropolitanate of Kyiv was annexed by the Moscow Patriarchate through a synodal letter of the Ecumenical Patriarch of Constantinople Dionysius IV, thus placing the Metropolitanate of Kyiv under the authority of Moscow. An attempt to reverse the decline was undertaken by Cossack Hetman Ivan Mazepa (1639\u20131709), who ultimately defected to the Swedes in the Great Northern War (1700\u20131721) in a bid to get rid of Russian dependence, but Hetmanate's capital city Baturyn was sacked (1708) and they were crushed in the Battle of Poltava (1709).\nThe Hetmanate's autonomy was severely restricted since Poltava. In the years 1764\u20131781, Catherine the Great incorporated much of Central Ukraine into the Russian Empire, abolishing the Cossack Hetmanate and the Zaporozhian Sich, and was one of the people responsible for the suppression of the last major Cossack uprising, the Koliivshchyna. After the annexation of Crimea by Russia in 1783, the newly acquired lands, now called Novorossiya, were opened up to settlement by Russians. The tsarist autocracy established a policy of Russification, suppressing the use of the Ukrainian language and curtailing the Ukrainian national identity. The western part of present-day Ukraine was subsequently split between Russia and Habsburg-ruled Austria after the fall of the Polish\u2013Lithuanian Commonwealth in 1795.\n19th and early 20th century.\nThe 19th century saw the rise of Ukrainian nationalism. With growing urbanisation and modernisation and a cultural trend toward romantic nationalism, a Ukrainian intelligentsia committed to national rebirth and social justice emerged. The serf-turned-national-poet Taras Shevchenko (1814\u20131861) and political theorist Mykhailo Drahomanov (1841\u20131895) led the growing nationalist movement. While conditions for its development in Austrian Galicia under the Habsburgs were relatively lenient, the Russian part (historically known as \"Little Russia\" or \"South Russia\") faced severe restrictions, going as far as banning virtually all books from being published in Ukrainian in 1876.\nUkraine, like the rest of the Russian Empire, joined the Industrial Revolution later than most of Western Europe due to the maintenance of serfdom until 1861. Other than near the newly discovered coal fields of the Donbas, and in some larger cities such as Odesa and Kyiv, Ukraine largely remained an agricultural and resource extraction economy. The Austrian part of Ukraine was particularly destitute, which forced hundreds of thousands of peasants into emigration, who created the backbone of an extensive Ukrainian diaspora in countries such as Canada, the United States and Brazil. Some of the Ukrainians settled in the Far East, too. According to the 1897 census, there were 223,000 ethnic Ukrainians in Siberia and 102,000 in Central Asia. An additional 1.6 million emigrated to the east in the ten years after the opening of the Trans-Siberian Railway in 1906. Far Eastern areas with an ethnic Ukrainian population became known as Green Ukraine.\nUkraine plunged into turmoil with the beginning of World War I, and fighting on Ukrainian soil persisted until late 1921. Initially, the Ukrainians were split between Austria-Hungary, fighting for the Central Powers, though the vast majority served in the Imperial Russian Army, which was part of the Triple Entente, under Russia. As the Russian Empire collapsed, the conflict evolved into the Ukrainian War of Independence, with Ukrainians fighting alongside, or against, the Red, White, Black and Green armies, with the Poles, Hungarians (in Transcarpathia), and Germans also intervening at various times.\nAn attempt to create an independent state, the left-leaning Ukrainian People's Republic (UNR), was first announced by Mykhailo Hrushevsky, but the period was plagued by an extremely unstable political and military environment. It was first deposed in a coup d'\u00e9tat led by Pavlo Skoropadskyi, which yielded the Ukrainian State under the German protectorate, and the attempt to restore the UNR under the Directorate ultimately failed as the Ukrainian army was regularly overrun by other forces. The short-lived West Ukrainian People's Republic and Hutsul Republic also failed to join the rest of Ukraine.\nThe result of the conflict was a partial victory for the Second Polish Republic, which annexed the Western Ukrainian provinces, as well as a larger-scale victory for the pro-Soviet forces, which succeeded in dislodging the remaining factions and eventually established the Ukrainian Soviet Socialist Republic (Soviet Ukraine). Meanwhile, modern-day Bukovina was occupied by Romania and Carpathian Ruthenia was admitted to Czechoslovakia as an autonomous region.\nThe conflict over Ukraine, a part of the broader Russian Civil War, devastated the whole of the former Russian Empire, including eastern and central Ukraine. The fighting left over 1.5 million people dead and hundreds of thousands homeless in the former Russian Empire's territory. Famine in 1921 further hit the eastern provinces.\nInter-war period.\nDuring the inter-war period, in Poland, Marshal J\u00f3zef Pi\u0142sudski sought Ukrainian support by offering local autonomy as a way to minimise Soviet influence in Poland's eastern Kresy region. However, this approach was abandoned after Pi\u0142sudski's death in 1935, due to continued unrest among the Ukrainian population, including assassinations of Polish government officials by the Organisation of Ukrainian Nationalists (OUN); with the Polish government responding by restricting rights of people who declared Ukrainian nationality. In consequence, the underground Ukrainian nationalist and militant movement, which arose in the 1920s gained wider support.\nMeanwhile, the recently constituted Soviet Ukraine became one of the founding republics of the Soviet Union. During the 1920s, under the Ukrainisation policy pursued by the national Communist leadership of Mykola Skrypnyk, Soviet leadership at first encouraged a national renaissance in Ukrainian culture and language. Ukrainisation was part of the Soviet-wide policy of Korenisation (literally \"indigenisation\"), which was intended to promote the advancement of native peoples, their language and culture into the governance of their respective republics.\nAround the same time, Soviet leader Vladimir Lenin instituted the New Economic Policy (NEP), which introduced a form of market socialism, allowing some private ownership of small and medium-sized productive enterprises, hoping to reconstruct the post-war Soviet Union that had been devastated by both WWI and later the civil war. The NEP was successful at restoring the formerly war-torn nation to pre-WWI levels of production and agricultural output by the mid-1920s, much of the latter based in Ukraine. These policies attracted many prominent former UNR figures, including former UNR leader Hrushevsky, to return to Soviet Ukraine, where they were accepted, and participated in the advancement of Ukrainian science and culture. In July 1922, arrests and deportations of Ukrainian intellectuals (e.g. university professors) began in Soviet Ukraine and continued throughout the 1920s.\nThis period was cut short when Joseph Stalin became the leader of the USSR following Lenin's death. Stalin did away with the NEP in what became known as the Great Break. Starting from the late 1920s and now with a centrally planned economy, Soviet Ukraine took part in an industrialisation scheme which quadrupled its industrial output during the 1930s.\nNevertheless, Stalin sought to prevent the Ukrainians aspirations for the independence of Ukraine and took severe measures to eliminate Ukrainian peasantry and elite Ukrainian intellectuals and culturists. As a consequence of Stalin's new policy, the Ukrainian peasantry suffered from the programme of collectivisation of agricultural crops. Collectivisation was part of the first five-year plan and was enforced by regular troops and the secret police known as Cheka. Those who resisted were arrested and deported to gulags and work camps. As members of the collective farms were sometimes not allowed to receive any grain until unrealistic quotas were met, millions starved to death in a famine known as the Holodomor or the \"Great Famine\", which was recognised by some countries as an act of genocide perpetrated by Joseph Stalin and other Soviet notables.\nFollowing on the Russian Civil War and collectivisation, the Great Purge, while killing Stalin's perceived political enemies, resulted in a profound loss of a new generation of Ukrainian intelligentsia, known today as the Executed Renaissance.\nWorld War II.\nFollowing the invasion of Poland in September 1939, German and Soviet troops divided the territory of Poland. Thus, Eastern Galicia and Volhynia with their Ukrainian population became part of Ukraine. For the first time in history, the nation was united. Further territorial gains were secured in 1940, when the Ukrainian SSR incorporated the northern and southern districts of Bessarabia, Northern Bukovina, and the Hertsa region from the territories the USSR forced Romania to cede, though it handed over the western part of the Moldavian Autonomous Soviet Socialist Republic to the newly created Moldavian SSR. These territorial gains of the USSR were internationally recognised by the Paris peace treaties of 1947.\nGerman armies invaded the Soviet Union on 22 June 1941, initiating nearly four years of total war. The Axis initially advanced against desperate but unsuccessful efforts of the Red Army. In the battle of Kyiv, the city was acclaimed as a \"Hero City\", because of its fierce resistance. More than 600,000 Soviet soldiers (or one-quarter of the Soviet Western Front) were killed or taken captive there, with many suffering severe mistreatment. After its conquest, most of the Ukrainian SSR was organised within the Reichskommissariat Ukraine, with the intention of exploiting its resources and eventual German settlement. Some western Ukrainians, who had only joined the Soviet Union in 1939, hailed the Germans as liberators, but that did not last long as the Nazis made little attempt to exploit dissatisfaction with Stalinist policies. Instead, the Nazis preserved the collective-farm system, carried out genocidal policies against Jews, deported millions of people to work in Germany, and began a depopulation programme to prepare for German colonisation. They blockaded the transport of food on the Dnieper River.\nAlthough the majority of Ukrainians fought in or alongside the Red Army and Soviet resistance, in Western Ukraine an independent Ukrainian Insurgent Army movement arose (UPA, 1942). It was created as the armed forces of the underground Organisation of Ukrainian Nationalists (OUN). Both organisations, the OUN and the UPA, supported the goal of an independent Ukrainian state on the territory with a Ukrainian ethnic majority. Although this brought conflict with Nazi Germany, at times the Melnyk wing of the OUN allied with the Nazi forces. From mid-1943 until the end of the war, the UPA carried out massacres of ethnic Poles in the Volhynia and Eastern Galicia regions, killing around 100,000 Polish civilians, which brought reprisals. These organised massacres were an attempt by the OUN to create a homogeneous Ukrainian state without a Polish minority living within its borders, and to prevent the post-war Polish state from asserting its sovereignty over areas that had been part of pre-war Poland. After the war, the UPA continued to fight the USSR until the 1950s. At the same time, the Ukrainian Liberation Army, another nationalist movement, fought alongside the Nazis.\nIn total, the number of ethnic Ukrainians who fought in the ranks of the Soviet Army is estimated from 4.5\u00a0million to 7\u00a0million; half of the Pro-Soviet partisan guerrilla resistance units, which counted up to 500,000 troops in 1944, were also Ukrainian. Generally, the Ukrainian Insurgent Army's figures are unreliable, with figures ranging anywhere from 15,000 to as many as 100,000 fighters.\nThe vast majority of the fighting in World War II took place on the Eastern Front. The total losses inflicted upon the Ukrainian population during the war are estimated at 6 million, including an estimated one and a half million Jews killed by the Einsatzgruppen, sometimes with the help of local collaborators. Of the estimated 8.6 million Soviet troop losses, 1.4\u00a0million were ethnic Ukrainians. The Victory Day is celebrated as one of eleven Ukrainian national holidays.\nPost\u2013war Soviet Ukraine.\nThe republic was heavily damaged by the war, and it required significant efforts to recover. More than 700 cities and towns and 28,000 villages were destroyed. The situation was worsened by a famine in 1946\u20131947, which was caused by a drought and the wartime destruction of infrastructure, killing at least tens of thousands of people. In 1945, the Ukrainian SSR became one of the founding members of the United Nations (UN), part of a special agreement at the Yalta Conference, and, alongside Belarus, had voting rights in the UN even though they were not independent. Moreover, Ukraine once more expanded its borders as it annexed Zakarpattia, and the population became much more homogenised due to post-war population transfers, most of which, as in the case of Germans and Crimean Tatars, were forced. As of 1 January 1953, Ukrainians were second only to Russians among adult \"special deportees\", comprising 20% of the total.\nFollowing the death of Stalin in 1953, Nikita Khrushchev became the new leader of the USSR, who began the policies of de-stalinisation and the Khrushchev Thaw. During his term as head of the Soviet Union, Crimea was transferred from the Russian SFSR to the Ukrainian SSR, formally as a friendship gift to Ukraine and for economic reasons. This represented the final extension of Ukrainian territory and formed the basis for the internationally recognised borders of Ukraine to this day. Many top positions in the Soviet Union were occupied by Ukrainians, including notably Leonid Brezhnev, General Secretary of the Communist Party of the Soviet Union from 1964 to 1982. However, it was he and his appointee in Ukraine, Volodymyr Shcherbytsky, who presided over the extensive Russification of Ukraine and who were instrumental in repressing a new generation of Ukrainian intellectuals known as the Sixtiers.\nBy 1950, the republic had fully surpassed pre-war levels of industry and production. Soviet Ukraine soon became a European leader in industrial production and an important centre of the Soviet arms industry and high-tech research, though heavy industry still had an outsided influence. The Soviet government invested in hydroelectric and nuclear power projects to cater to the energy demand that the development carried. On 26 April 1986, however, a reactor in the Chernobyl Nuclear Power Plant exploded, resulting in the Chernobyl disaster, the worst nuclear reactor accident in history.\nIndependence.\nMikhail Gorbachev pursued a policy of limited liberalisation of public life, known as \"perestroika,\" and attempted to reform a stagnating economy. The latter failed, but the democratisation of the Soviet Union fuelled nationalist and separatist tendencies among the ethnic minorities, including Ukrainians. As part of the so-called parade of sovereignties, on 16 July 1990, the newly elected Supreme Soviet of the Ukrainian Soviet Socialist Republic adopted the Declaration of State Sovereignty of Ukraine. After a failed coup by some Communist leaders in Moscow at deposing Gorbachov, outright independence was proclaimed on 24 August 1991. It was approved by 92% of the Ukrainian electorate in a referendum on 1 December. Ukraine's new President, Leonid Kravchuk, went on to sign the Belavezha Accords and made Ukraine a founding member of the much looser Commonwealth of Independent States (CIS), though Ukraine never became a full member of the latter as it did not ratify the agreement founding CIS. These documents sealed the fate of the Soviet Union, which formally voted itself out of existence on 26 December.\nUkraine was initially viewed as having favourable economic conditions in comparison to the other regions of the Soviet Union, though it was one of the poorer Soviet republics by the time of the dissolution. However, during its transition to the market economy, the country experienced deeper economic slowdown than almost all of the other former Soviet Republics. During the recession, between 1991 and 1999, Ukraine lost 60% of its GDP and suffered from hyperinflation that peaked at 10,000% in 1993. The situation only stabilised well after the new currency, the hryvnia, fell sharply in late 1998 partially as a fallout from the Russian debt default earlier that year. The legacy of the economic policies of the nineties was the mass privatisation of state property that created a class of extremely powerful and rich individuals known as the oligarchs. The country then fell into a series of sharp recessions as a result of the Great Recession, the start of the Russo-Ukrainian War in 2014, and finally, the full-scale invasion by Russia in starting from 24 February 2022. Ukraine's economy in general underperformed since the time independence came due to pervasive corruption and mismanagement, which, particularly in the 1990s, led to protests and organised strikes. The war with Russia impeded meaningful economic recovery in the 2010s, while efforts to combat the COVID-19 pandemic, which arrived in 2020, were made much harder by low vaccination rates and, later in the pandemic, by the ongoing invasion.\nFrom the political perspective, one of the defining features of the politics of Ukraine is that for most of the time, it has been divided along two issues: the relation between Ukraine, the West and Russia, and the classical left-right divide. The first two presidents, Kravchuk and Leonid Kuchma, tended to balance the competing visions of Ukraine, though Yushchenko and Yanukovych were generally pro-Western and pro-Russian, respectively. There were two major protests against Yanukovych: the Orange Revolution in 2004, when tens of thousands of people went in protest of election rigging in his favour (Yushchenko was eventually elected president), and another one in the winter of 2013/2014, when more gathered on the Euromaidan to oppose Yanukovych's refusal to sign the European Union\u2013Ukraine Association Agreement. By the end of the protests on 21 February 2014, he fled from Ukraine and was removed by the parliament in what is termed the Revolution of Dignity, but Russia refused to recognise the interim pro-Western government, calling it a \"junta\" and denouncing the events as a coup d'\u00e9tat sponsored by the United States.\nDespite the signing of the Budapest memorandum in 1994, in which Ukraine agreed to hand over nuclear weapons in exchange for guarantees of security and territorial integrity, Russia reacted violently to these developments and started a war against its western neighbour. In late February and early March 2014, it annexed Crimea using its Navy in Sevastopol as well as the so- called little green men; after this succeeded, it then launched a proxy war in the Donbas via the breakaway Donetsk People's Republic and Luhansk People's Republic. The first months of the conflict with the Russian-backed separatists were fluid, but Russian forces then started an open invasion in Donbas on 24 August 2014. Together they pushed back Ukrainian troops to the frontline established in February 2015, i.e. after Ukrainian troops withdrew from Debaltseve. The conflict remained in a sort of frozen state until the early hours of 24 February 2022, when Russia invaded. A year later, Russian troops controlled about 17% of Ukraine's internationally recognised territory, which constitutes 94% of Luhansk Oblast, 73% of Kherson Oblast, 72% of Zaporizhzhia Oblast, 54% of Donetsk Oblast and all of Crimea, though Russia failed with its initial plan, with Ukrainian troops recapturing some territory in counteroffensives.\nThe military conflict with Russia shifted the government's policy towards the West. Shortly after Yanukovych fled Ukraine, the country signed the EU association agreement in June 2014, and its citizens were granted visa-free travel to the European Union three years later. In January 2019, the Orthodox Church of Ukraine was recognised as independent of Moscow, which reversed the 1686 decision of the patriarch of Constantinople and dealt a further blow to Moscow's influence in Ukraine. Finally, amid a full-scale war with Russia, Ukraine was granted candidate status to the European Union on 23 June 2022. A broad anti-corruption drive began in early 2023 with the resignations of several deputy ministers and regional heads during a reshuffle of the government.\nGeography.\nUkraine is the second-largest European country, after Russia, and the largest country entirely in Europe. Lying between latitudes 44\u00b0 and 53\u00b0 N, and longitudes 22\u00b0 and 41\u00b0 E., it is mostly in the East European Plain. Ukraine covers an area of , with a coastline of .\nThe landscape of Ukraine consists mostly of fertile steppes (plains with few trees) and plateaus, crossed by rivers such as the Dnieper, Seversky Donets, Dniester and the Southern Bug as they flow south into the Black Sea and the smaller Sea of Azov. To the southwest, the Danube Delta forms the border with Romania. Ukraine's regions have diverse geographic features, ranging from the highlands to the lowlands. The country's only mountains are the Carpathian Mountains in the west, of which the highest is Hoverla at , and the Crimean Mountains, in the extreme south along the coast.\nUkraine also has a number of highland regions such as the Volyn-Podillia Upland (in the west) and the Near-Dnipro Upland (on the right bank of the Dnieper). To the east there are the south-western spurs of the Central Russian Upland over which runs the border with Russia. Near the Sea of Azov are the Donets Ridge and the Near Azov Upland. The snow melt from the mountains feeds the rivers and their waterfalls.\nSignificant natural resources in Ukraine include lithium, natural gas, kaolin, timber and an abundance of arable land. Ukraine has many environmental issues. Some regions lack adequate supplies of potable water. Air and water pollution affects the country, as well as deforestation, and radiation contamination in the northeast from the 1986 accident at the Chernobyl Nuclear Power Plant. The environmental damage caused by the Russian invasion of Ukraine has been described as an ecocide, the destruction of Kakhovka Dam, severe pollution and millions of tonnes of contaminated debris is estimated to cost over USD 50 billion to repair.\nClimate.\nUkraine is in the mid-latitudes, and generally has a continental climate, except for its southern coasts, which have cold semi-arid and humid subtropical climates. Average annual temperatures range from in the north, to in the south. Precipitation is highest in the west and north and lowest in the east and southeast. Western Ukraine, particularly in the Carpathian Mountains, receives around of precipitation annually, while Crimea and the coastal areas of the Black Sea receive around .\nWater availability from the major river basins is expected to decrease due to climate change, especially in summer. This poses risks to the agricultural sector. The negative impacts of climate change on agriculture are mostly felt in the south of the country, which has a steppe climate. In the north, some crops may be able to benefit from a longer growing season. The World Bank has stated that Ukraine is highly vulnerable to climate change.\nBiodiversity.\nUkraine contains six terrestrial ecoregions: Central European mixed forests, Crimean Submediterranean forest complex, East European forest steppe, Pannonian mixed forests, Carpathian montane conifer forests, and Pontic steppe. There is somewhat more coniferous than deciduous forest. The most densely forested area is Polisia in the northwest, with pine, oak, and birch. There are 45,000 species of animals (mostly invertebrates), with approximately 385 endangered species listed in the Red Data Book of Ukraine. Internationally important wetlands cover over , with the Danube Delta being important for conservation.\nUrban areas.\nUkraine has 457 cities, of which 176 are designated as oblast-class, 279 as smaller -class cities, and two as special legal status cities. There are also 886 urban-type settlements and 28,552 villages.\n&lt;templatestyles src=\"Template:Largest_cities/styles.css\" /&gt;\nPolitics.\nUkraine is a republic under a semi-presidential system with separate legislative, executive, and judicial branches.\nConstitution.\nThe Constitution of Ukraine was adopted and ratified at the 5th session of the Verkhovna Rada, the parliament of Ukraine, on 28 June 1996. The constitution was passed with 315 ayes out of 450 votes possible (300 ayes minimum). All other laws and other normative legal acts of Ukraine must conform to the constitution. The right to amend the constitution through a special legislative procedure is vested exclusively in the parliament. The only body that may interpret the constitution and determine whether legislation conforms to it is the Constitutional Court of Ukraine. Since 1996, the public holiday Constitution Day is celebrated on 28 June. On 7 February 2019, the Verkhovna Rada voted to amend the constitution to state Ukraine's strategic objectives as joining the European Union and NATO.\nGovernment.\nThe president is elected by popular vote for a five-year term and is the formal head of state.\nUkraine's legislative branch includes the 450-seat unicameral parliament, the Verkhovna Rada. The parliament is primarily responsible for the formation of the executive branch and the Cabinet of Ministers, headed by the prime minister. The president retains the authority to nominate the ministers of foreign affairs and of defence for parliamentary approval, as well as the power to appoint the prosecutor general and the head of the Security Service.\nLaws, acts of the parliament and the cabinet, presidential decrees, and acts of the Crimean parliament may be abrogated by the Constitutional Court, should they be found to violate the constitution. Other normative acts are subject to judicial review. The Supreme Court is the main body in the system of courts of general jurisdiction.\nLocal self-government is officially guaranteed. Local councils and city mayors are popularly elected and exercise control over local budgets. The heads of regional and district administrations are appointed by the president in accordance with proposals of the prime minister.\nCourts and law enforcement.\nMartial law was declared when Russia invaded in February 2022, and continues. The courts enjoy legal, financial and constitutional freedom guaranteed by Ukrainian law since 2002. Judges are largely well protected from dismissal (except for gross misconduct). Court justices are appointed by presidential decree for an initial period of five years, after which Ukraine's Supreme Council confirms their positions for life. Although there are still problems, the system is considered to have been much improved since Ukraine's independence in 1991. The Supreme Court is regarded as an independent and impartial body, and has on several occasions ruled against the Ukrainian government. The World Justice Project ranks Ukraine 66 out of 99 countries surveyed in its annual Rule of Law Index.\nProsecutors in Ukraine have greater powers than in most European countries, and according to the European Commission for Democracy through Law \"the role and functions of the Prosecutor's Office is not in accordance with Council of Europe standards\". The conviction rate is over 99%, equal to the conviction rate of the Soviet Union, with suspects often being incarcerated for long periods before trial.\nIn 2010, President Yanukovych formed an expert group to make recommendations on how to \"clean up the current mess and adopt a law on court organisation\". One day later, he stated \"We can no longer disgrace our country with such a court system.\" The criminal judicial system and the prison system of Ukraine remain quite punitive.\nSince 2010 court proceedings can be held in Russian by mutual consent of the parties. Citizens unable to speak Ukrainian or Russian may use their native language or the services of a translator. Previously all court proceedings had to be held in Ukrainian.\nLaw enforcement agencies are controlled by the Ministry of Internal Affairs. They consist primarily of the national police force and various specialised units and agencies such as the State Border Guard and the Coast Guard services. Law enforcement agencies, particularly the police, faced criticism for their heavy handling of the 2004 Orange Revolution. Many thousands of police officers were stationed throughout the capital, primarily to dissuade protesters from challenging the state's authority but also to provide a quick reaction force in case of need; most officers were armed.\nForeign relations.\nFrom 1999 to 2001, Ukraine served as a non-permanent member of the UN Security Council. Historically, Soviet Ukraine joined the United Nations in 1945 as one of the original members following a Western compromise with the Soviet Union. Ukraine has consistently supported peaceful, negotiated settlements to disputes. It has participated in the quadripartite talks on the conflict in Moldova and promoted a peaceful resolution to the conflict in the post-Soviet state of Georgia. Ukraine also has made contributions to UN peacekeeping operations since 1992.\nUkraine considers Euro-Atlantic integration its primary foreign policy objective, but in practice it has always balanced its relationship with the European Union and the United States with strong ties to Russia. The European Union's Partnership and Cooperation Agreement (PCA) with Ukraine went into force in 1998. The European Union (EU) has encouraged Ukraine to implement the PCA fully before discussions begin on an association agreement, issued at the EU Summit in December 1999 in Helsinki, recognises Ukraine's long-term aspirations but does not discuss association.\nIn 1992, Ukraine joined the then-Conference on Security and Cooperation in Europe (now the Organisation for Security and Cooperation in Europe (OSCE)), and also became a member of the North Atlantic Cooperation Council. Ukraine\u2013NATO relations are close and the country has declared interest in eventual membership.\nUkraine is the most active member of the Partnership for Peace (PfP). All major political parties in Ukraine support full eventual integration into the European Union. The Association Agreement between Ukraine and the European Union was signed in 2014. Ukraine long had close ties with all its neighbours, but Russia\u2013Ukraine relations rapidly deteriorated in 2014 due to the annexation of Crimea, energy dependence and payment disputes.The Deep and Comprehensive Free Trade Area (DCFTA), which entered into force in January 2016 following the ratification of the Ukraine\u2013European Union Association Agreement, formally integrates Ukraine into the European Single Market and the European Economic Area. Ukraine receives further support and assistance for its EU-accession aspirations from the International Visegr\u00e1d Fund of the Visegr\u00e1d Group that consists of Central European EU members the Czech Republic, Poland, Hungary and Slovakia.\nIn 2020, in Lublin, Lithuania, Poland and Ukraine created the Lublin Triangle initiative, which aims to create further cooperation between the three historical countries of the Polish\u2013Lithuanian Commonwealth and further Ukraine's integration and accession to the EU and NATO.\nIn 2021, the Association Trio was formed by signing a joint memorandum between the Foreign Ministers of Georgia, Moldova and Ukraine. The Association Trio is a tripartite format for enhanced cooperation, coordination, and dialogue between the three countries (that have signed the Association Agreement with the EU) with the European Union on issues of common interest related to European integration, enhancing cooperation within the framework of the Eastern Partnership, and committing to the prospect of joining the European Union. As of 2021, Ukraine was preparing to formally apply for EU membership in 2024, in order to join the European Union in the 2030s, however, with the Russian invasion of Ukraine in 2022, Ukrainian president Volodymyr Zelenskyy requested that the country be admitted to the EU immediately. Candidate status was granted in June 2022. In recent years, Ukraine has dramatically strengthened its ties with the United States.\nIn June 2025, Ukraine legalised multiple citizenship.\nMilitary.\nAfter the dissolution of the Soviet Union, Ukraine inherited a 780,000-man military force on its territory, equipped with the third-largest nuclear weapons arsenal in the world. In 1992, Ukraine signed the Lisbon Protocol in which the country agreed to give up all nuclear weapons to Russia for disposal and to join the Nuclear Non-Proliferation Treaty as a non-nuclear weapon state. By 1996 the country had become free of nuclear weapons.\nUkraine took consistent steps toward reduction of conventional weapons. It signed the Treaty on Conventional Armed Forces in Europe, which called for reduction of tanks, artillery, and armoured vehicles (army forces were reduced to 300,000). The country planned to convert the current conscript-based military into a professional volunteer military. Ukraine's current military consist of 196,600 active personnel and around 900,000 reservists.\nUkraine played an increasing role in peacekeeping operations. In 2014, the Ukrainian frigate \"Hetman Sagaidachniy\" joined the European Union's counter-piracy Operation Atalanta and was part of the EU Naval Force off the coast of Somalia for two months. Ukrainian troops were deployed in Kosovo as part of the Ukrainian-Polish Battalion. In 2003\u20132005, a Ukrainian unit was deployed as part of the multinational force in Iraq under Polish command. Military units of other states participated in multinational military exercises with Ukrainian forces in Ukraine regularly, including U.S. military forces.\nFollowing independence, Ukraine declared itself a neutral state. The country had a limited military partnership with Russian Federation and other CIS countries and has had a partnership with NATO since 1994. In the 2000s, the government was leaning towards NATO, and deeper cooperation with the alliance was set by the NATO-Ukraine Action Plan signed in 2002. It was later agreed that the question of joining NATO should be answered by a national referendum at some point in the future. Deposed President Viktor Yanukovych considered the then level of co-operation between Ukraine and NATO sufficient, and was against Ukraine joining NATO. During the 2008 Bucharest summit, NATO declared that Ukraine would eventually become a member of NATO when it meets the criteria for accession.\nAs part of its modernisation after the beginning of the Russo-Ukrainian War in 2014, junior officers were allowed to take more initiative and a territorial defence force of volunteers was established. Various defensive weapons including drones were supplied by many countries, but not fighter jets. During the first few weeks of the 2022 Russian invasion the military found it difficult to defend against shelling, missiles and high level bombing; but light infantry used shoulder-mounted weapons effectively to destroy tanks, armoured vehicles and low-flying aircraft. In August 2023, the U.S. officials estimated that up to 70,000 Ukrainian soldiers were killed and 100,000 to 120,000 wounded during the Russian invasion of Ukraine.\nAdministrative divisions.\nThe system of Ukrainian subdivisions reflects the country's status as a unitary state (as stated in the country's constitution) with unified legal and administrative regimes for each unit.\nIncluding Sevastopol and the Autonomous Republic of Crimea that were annexed by the Russian Federation in 2014, Ukraine consists of 27 regions: twenty-four oblasts (provinces), one autonomous republic (Autonomous Republic of Crimea), and two cities of special status\u2014Kyiv, the capital, and Sevastopol. The 24 oblasts and Crimea are subdivided into 136 (districts) and city municipalities of regional significance, or second-level administrative units.\nPopulated places in Ukraine are split into two categories: urban and rural. Urban populated places are split further into cities and urban-type settlements (a Soviet administrative invention), while rural populated places consist of villages and settlements (a generally used term). All cities have a certain degree of self-rule depending on their significance such as national significance (as in the case of Kyiv and Sevastopol), regional significance (within each oblast or autonomous republic) or district significance (all the rest of cities). A city's significance depends on several factors such as its population, socio-economic and historical importance and infrastructure.\nEconomy.\nIn 2021, agriculture was the biggest sector of the economy. Ukraine is one of the world's largest wheat exporters. It remains among the poorest countries in Europe with the lowest nominal GDP per capita. Despite improvements, as in Moldova corruption in Ukraine remains an obstacle to joining the EU; the country was rated 104th out of 180 in the Corruption Perceptions Index for 2023. In 2021, Ukraine's GDP per capita by purchasing power parity was just over $14,000. Despite supplying emergency financial support, the IMF expected the economy to shrink considerably by 35% in 2022 due to Russia's invasion. One 2022 estimate was that post-war reconstruction costs might reach half a trillion dollars.\nIn 2021, the average salary in Ukraine reached its highest level at almost \u20b414,300 (US$525) per month. About 1% of Ukrainians lived below the national poverty line in 2019. Unemployment in Ukraine was 4.5% in 2019. In 2019 5\u201315% of the Ukrainian population were categorised as middle class. In 2020 Ukraine's government debt was roughly 50% of its nominal GDP.\nIn 2021 mineral commodities and light industry were important sectors. Ukraine produces nearly all types of transportation vehicles and spacecraft. The European Union is the country's main trade partner, and remittances from Ukrainians working abroad are important.\nAgriculture.\nUkraine is among the world's top agricultural producers and exporters and is often described as the \"bread basket of Europe\". During the 2020/21 international wheat marketing season (July\u2013June), it ranked as the sixth largest wheat exporter, accounting for nine percent of world wheat trade. The country is also a major global exporter of maize, barley and rapeseed. In 2020/21, it accounted for 12 percent of global trade in maize and barley and for 14 percent of world rapeseed exports. Its trade share is even greater in the sunflower oil sector, with the country accounting for about 50 percent of world exports in 2020/2021.\nAccording to the Food and Agriculture Organisation of the United Nations (FAO), further to causing the loss of lives and increasing humanitarian needs, the likely disruptions caused by the Russo-Ukrainian War to Ukraine's grain and oilseed sectors, could jeopardise the food security of many countries, especially those that are highly dependent on Ukraine and Russia for their food and fertiliser imports. Several of these countries fall into the Least Developed Country (LDC) group, while many others belong to the group of Low-Income Food-Deficit Countries (LIFDCs). For example Eritrea sourced 47 percent of its wheat imports in 2021 from Ukraine. Overall, more than 30 nations depend on Ukraine and the Russian Federation for over 30 percent of their wheat import needs, many of them in North Africa and Western and Central Asia.\nTourism.\nBefore the Russo-Ukrainian war the number of tourists visiting Ukraine was eighth in Europe, according to UN Tourism rankings. Ukraine has numerous tourist attractions: mountain ranges suitable for skiing, hiking and fishing; the Black Sea coastline as a popular summer destination; nature reserves of different ecosystems; and churches, castle ruins and other architectural and park landmarks. Kyiv, Lviv, Odesa and Kamianets-Podilskyi were Ukraine's principal tourist centres, each offering many historical landmarks and extensive hospitality infrastructure. The Seven Wonders of Ukraine and Seven Natural Wonders of Ukraine are selections of the most important landmarks of Ukraine, chosen by Ukrainian experts and an Internet-based public vote. Tourism was the mainstay of Crimea's economy before a major fall in visitor numbers following the Russian annexation in 2014.\nTransport.\nMany roads and bridges were destroyed, and international maritime travel was blocked by the 2022 Russian invasion of Ukraine. Before that it was mainly through the Port of Odesa, from where ferries sailed regularly to Istanbul, Varna and Haifa. The largest ferry company operating these routes was Ukrferry. There are over of navigable waterways on 7 rivers, mostly on the Danube, Dnieper and Pripyat. All Ukraine's rivers freeze over in winter, limiting navigation.\nUkraine's rail network connects all major urban areas, port facilities and industrial centres. The heaviest concentration of railway track is the Donbas region. Although rail freight transport fell in the 1990s, Ukraine is still one of the world's highest rail users. Ukraine also has multiple urban rail systems, particularly three metro (Dnipro, Kharkiv, and Kyiv), two light rail (Kryvyi Rih and Kyiv), two urban \"elektrychka\" (Kamianske\u2013Dnipro\u2013Synelnykove and Kyiv), two cable railway (Kyiv and Odesa), and numerous tramway systems.\nUkraine International Airlines, is the flag carrier and the largest airline, with its head office in Kyiv and its main hub at Kyiv's Boryspil International Airport. It operated domestic and international passenger flights and cargo services to Europe, the Middle East, the United States, Canada, and Asia.\nEnergy.\nEnergy in Ukraine is mainly from gas and coal, followed by nuclear then oil. The coal industry has been disrupted by conflict. Most gas and oil is imported, but since 2015 energy policy has prioritised diversifying energy supply.\nAbout half of electricity generation is nuclear and a quarter coal. The largest nuclear power plant in Europe, the Zaporizhzhia Nuclear Power Plant, is in Ukraine. Fossil fuel subsidies were US$2.2 billion in 2019. Until the 2010s all of Ukraine's nuclear fuel came from Russia, but now most does not.\nSome energy infrastructure was destroyed in the 2022 Russian invasion of Ukraine. The contract to transit Russian gas expires at the end of 2024.\nIn early 2022 Ukraine and Moldova decoupled their electricity grids from the Integrated Power System of Russia and Belarus; and the European Network of Transmission System Operators for Electricity synchronised them with continental Europe.\nInformation technology.\nKey officials may use Starlink as backup. The IT industry contributed almost 5 per cent to Ukraine's GDP in 2021 and in 2022 continued both inside and outside the country.\nDemographics.\nBefore the 2022 Russian invasion of Ukraine the country had an estimated population of over 41 million people, and was the eighth-most populous country in Europe. It is a heavily urbanised country, and its industrial regions in the east and southeast are the most densely populated\u2014about 67% of its total population lives in urban areas. At that time Ukraine had a population density of , and the overall life expectancy in the country at birth was 73 years (68 years for males and 77.8 years for females).\nFollowing the dissolution of the Soviet Union, Ukraine's population hit a peak of roughly 52 million in 1993. However, due to its death rate exceeding its birth rate, mass emigration, poor living conditions, and low-quality health care, the total population decreased by 6.6 million, or 12.8% from the same year to 2014.\nAccording to the 2001 census, ethnic Ukrainians made up roughly 78% of the population, while Russians were the largest minority, at some 17.3% of the population. Small minority populations included: Belarusians (0.6%), Moldovans (0.5%), Crimean Tatars (0.5%), Bulgarians (0.4%), Hungarians (0.3%), Romanians (0.3%), Poles (0.3%), Jews (0.3%), Armenians (0.2%), Greeks (0.2%) and Tatars (0.2%). It was also estimated that there were about 10\u201340,000 Koreans in Ukraine, who lived mostly in the south of the country, belonging to the historical Koryo-saram group, as well as about 47,600 Roma (though the Council of Europe estimates a higher number of about 260,000).\nOutside the former Soviet Union, the largest source of incoming immigrants in Ukraine's post-independence period was from four Asian countries, namely China, India, Pakistan and Afghanistan. In the late 2010s 1.4 million Ukrainians were internally displaced due to the war in Donbas, and in early 2022, over 4.1 million fled the country in the aftermath of the 2022 Russian invasion of Ukraine, causing the Ukrainian refugee crisis. Most male Ukrainian nationals aged 18 to 60 were denied exit from Ukraine. The Ukrainian government estimates that the population in the regions controlled by Ukraine was 25 to 27 million in 2024.\nLanguage.\nAccording to Ukraine's constitution, the state language is Ukrainian. Russian is widely spoken in the country, especially in eastern and southern Ukraine. Most native Ukrainian speakers know Russian as a second language. Russian was the \"de facto\" dominant language of the Soviet Union but Ukrainian also held official status in the republic, and in the schools of the Ukrainian SSR, learning Ukrainian was mandatory.\nUkrainian is the primary language used in the vast majority of Ukraine. 67% of Ukrainians speak Ukrainian as their primary language, while 30% speak Russian as their primary language. In eastern and southern Ukraine, Russian is the primary language in some cities, while Ukrainian is used in rural areas. Hungarian is spoken in Zakarpattia Oblast. There is no consensus among scholars whether Rusyn, also spoken in Zakarpattia, is a distinct language or a dialect of Ukrainian. The Ukrainian government does not recognise Rusyn and Rusyns as a distinct language and people.\nFor a large part of the Soviet era, the number of Ukrainian speakers declined from generation to generation, and by the mid-1980s, the usage of the Ukrainian language in public life had decreased significantly. Following independence, the government of Ukraine began restoring the use of the Ukrainian language in schools and government through a policy of Ukrainisation.\nEffective in August 2012, a new law on regional languages entitled any local language spoken by at least a 10\u00a0percent minority be declared official within that area. Within weeks, Russian was declared a regional language of several southern and eastern oblasts (provinces) and cities. Russian could then be used in the administrative office work and documents of those places.\nIn 2014, following the Revolution of Dignity, the Ukrainian Parliament voted to repeal the law on regional languages, making Ukrainian the sole state language at all levels; however, the repeal was not signed by acting President Turchynov or by President Poroshenko. In 2019, the law allowing for official use of regional languages was found unconstitutional. According to the Council of Europe, this act fails to achieve fair protection of the linguistic rights of minorities. Presently, most foreign films and TV programmes, including Russian ones, are subtitled or dubbed in Ukrainian. Ukraine's 2017 education law bars primary education in public schools in grade five and up in any language but Ukrainian.\nSince 2014, during the Russian-Ukrainian war, Russia has been pursuing a policy of forced Russification of the territories of Ukraine it has occupied \u2014 schools teach only in Russian, even in entirely Ukrainian-speaking settlements. Ukrainian school textbooks have been banned, and those who wish to study in Ukrainian are forced to do so in secret from the occupation authorities. Ukrainian-language literature is also being destroyed on a massive scale. According to the report \"Ukraine: Forced Russified Education Under Occupation\" by Human Rights Watch, the Russian occupation authorities and other high-ranking Russian officials have taken and continue to take measures to eradicate the Ukrainian language in the occupied territories. Ukrainian children forcibly deported to Russia are also subjected to Russification.\nAt the same time, Russian armed aggression provoked significant changes in the Ukrainian society's perception of the language issue: a significant part of the formerly Russian-speaking population switched to speaking Ukrainian, and the popularity of the idea of granting Russian official status throughout all of Ukraine or certain regions of the country has reached its lowest level since polling began in 1997.\nDiaspora.\nThe Ukrainian diaspora comprises Ukrainians and their descendants who live outside Ukraine around the world, especially those who maintain some kind of connection to the land of their ancestors and maintain their feeling of Ukrainian national identity within their own local community. The Ukrainian diaspora is found throughout numerous regions worldwide including other post-Soviet states as well as in Canada, and other countries such as Poland, the United States, the UK and Brazil.\nThe 2022 Russian invasion of Ukraine has led to the Ukrainian refugee crisis in which millions of Ukrainian civilians moved to neighbouring countries. Most crossed into Poland, Slovakia, and the Czech Republic, and others proceeded to at least temporarily settle in Hungary, Moldova, Germany, Austria, Romania and other European countries.\nReligion.\nUkraine has the world's second-largest Eastern Orthodox population, after Russia. A 2021 survey conducted by the Kyiv International Institute of Sociology (KIIS) found that 82% of Ukrainians declared themselves to be religious, while 7% were atheists, and a further 11% found it difficult to answer the question. The level of religiosity in Ukraine was reported to be the highest in Western Ukraine (91%), and the lowest in the Donbas (57%) and Eastern Ukraine (56%).\nIn 2019, 82% of Ukrainians were Christians; out of which 72.7% declared themselves to be Eastern Orthodox, 8.8% Ukrainian Greek Catholics, 2.3% Protestants and 0.9% Latin Church Catholics. Other Christians comprised 2.3%. Judaism, Islam, and Hinduism were the religions of 0.2% of the population each. According to the KIIS study, roughly 58.3% of the Ukrainian Orthodox population were members of the Orthodox Church of Ukraine, and 25.4% were members of the Ukrainian Orthodox Church (Moscow Patriarchate). Protestants are a growing community in Ukraine, who made up 1.9% of the population in 2016, but rose to 2.2% of the population in 2018.\nHealth.\nUkraine's healthcare system is state subsidised and freely available to all Ukrainian citizens and registered residents. However, it is not compulsory to be treated in a state-run hospital as a number of private medical complexes do exist nationwide. The public sector employs most healthcare professionals, with those working for private medical centres typically also retaining their state employment as they are mandated to provide care at public health facilities on a regular basis.\nAll of Ukraine's medical service providers and hospitals are subordinate to the Ministry of Healthcare, which provides oversight and scrutiny of general medical practice as well as being responsible for the day-to-day administration of the healthcare system. Despite this, standards of hygiene and patient-care have fallen.\nUkraine faces a number of major public health issues and is considered to be in a demographic crisis because of its high death rate, low birth rate, and high emigration. A factor contributing to the high death rate is a high mortality rate among working-age males from preventable causes such as alcohol poisoning and smoking.\nActive reformation of Ukraine's healthcare system was initiated right after the appointment of Ulana Suprun as a head of the Ministry of Healthcare. Assisted by deputy Pavlo Kovtoniuk, Suprun first changed the distribution of finances in healthcare. Funds must follow the patient. General practitioners will provide basic care for patients. The patient will have the right to choose one. Emergency medical service is considered to be fully funded by the state. Emergency Medicine Reform is also an important part of the healthcare reform. In addition, patients who suffer from chronic diseases, which cause a high toll of disability and mortality, are provided with free or low-price medicine.\nAs a result of the 2022 Russian invasion of Ukraine, millions in Ukraine suffered physical injuries and psychological traumas. The World Health Organization has documented over 2254 attacks on healthcare in Ukraine since the start of the 2022 Russian invasion of Ukraine. According to the October 2024 data of the World Health Organization Ukraine health needs assessment, 68% of Ukrainians reported that their health declined compared to the pre-war period. The war with Russia worsened Ukrainian children physical and mental health.\nEducation.\nAccording to the Ukrainian constitution, access to free education is granted to all citizens. Complete general secondary education is compulsory in the state schools which constitute the overwhelming majority. Free higher education in state and communal educational establishments is provided on a competitive basis.\nBecause of the Soviet Union's emphasis on total access of education for all citizens, which continues today, the literacy rate is an estimated 99.4%. Since 2005, an eleven-year school programme has been replaced with a twelve-year one: primary education takes four years to complete (starting at age six), middle education (secondary) takes five years to complete; upper secondary then takes three years. Students in the 12th grade take Government tests, which are also referred to as school-leaving exams. These tests are later used for university admissions.\nAmong the oldest is also the Lviv University, founded in 1661. More higher education institutions were set up in the 19th century, beginning with universities in Kharkiv (1805), Kyiv (1834), Odesa (1865) and Chernivtsi (1875) and a number of professional higher education institutions, e.g.: Nizhyn Historical and Philological Institute (originally established as the Gymnasium of Higher Sciences in 1805), a Veterinary Institute (1873) and a Technological Institute (1885) in Kharkiv, a Polytechnic Institute in Kyiv (1898) and a Higher Mining School (1899) in Katerynoslav. Rapid growth followed in the Soviet period. By 1988 the number of higher education institutions increased to 146 with over 850,000 students.\nThe Ukrainian higher education system comprises higher educational establishments, scientific and methodological facilities under national, municipal and self-governing bodies in charge of education. The organisation of higher education in Ukraine is built up in accordance with the structure of education of the world's higher developed countries, as is defined by UNESCO and the UN.\nUkraine produces the fourth largest number of post-secondary graduates in Europe, while being ranked seventh in population. Higher education is either state funded or private. Most universities provide subsidised housing for out-of-city students. It is common for libraries to supply required books for all registered students. Ukrainian universities confer two degrees: the bachelor's degree (4\u00a0years) and the master's degree (5\u20136th\u00a0year), in accordance with the Bologna process. Historically, Specialist degree (usually 5 years) is still also granted; it was the only degree awarded by universities in Soviet times. Ukraine was ranked 66th in 2025 in the Global Innovation Index.\nRegional differences.\nUkrainian is the dominant language in Western Ukraine and in Central Ukraine, while Russian is the dominant language in the cities of Eastern Ukraine and Southern Ukraine. In the Ukrainian SSR schools, learning Russian was mandatory; in modern Ukraine, schools with Ukrainian as the language of instruction offer classes in Russian and in the other minority languages.\nOn the Russian language, on Soviet Union and Ukrainian nationalism, opinion in Eastern Ukraine and Southern Ukraine tends to be the exact opposite of those in Western Ukraine; while opinions in Central Ukraine on these topics tend be less extreme.\nSimilar historical divisions also remain evident at the level of individual social identification. Attitudes toward the most important political issue, relations with Russia, differed strongly between Lviv, identifying more with Ukrainian nationalism and the Ukrainian Greek Catholic Church, and Donetsk, predominantly Russian orientated and favourable to the Soviet era, while in central and southern Ukraine, as well as Kyiv, such divisions were less important and there was less antipathy toward people from other regions.\nHowever, all were united by an overarching Ukrainian identity based on shared economic difficulties, showing that other attitudes are determined more by culture and politics than by demographic differences. Surveys of regional identities in Ukraine have shown that the feeling of belonging to a \"Soviet identity\" is strongest in the Donbas (about 40%) and the Crimea (about 30%).\nDuring elections voters of Western and Central Ukrainian oblasts (provinces) vote mostly for parties (Our Ukraine, Batkivshchyna) and presidential candidates (Viktor Yuschenko, Yulia Tymoshenko) with a pro-Western and state reform platform, while voters in Southern and Eastern oblasts vote for parties (CPU, Party of Regions) and presidential candidates (Viktor Yanukovych) with a pro-Russian and status quo platform. However, this geographical division is decreasing.\nCulture.\nUkrainian customs are heavily influenced by Orthodox Christianity, the dominant religion in the country. Gender roles also tend to be more traditional, and grandparents play a greater role in bringing up children, than in the West. The culture of Ukraine has also been influenced by its eastern and western neighbours, reflected in its architecture, music and art.\nThe Communist era had quite a strong effect on the art and writing of Ukraine. In 1932, Stalin made socialist realism state policy in the Soviet Union when he promulgated the decree \"On the Reconstruction of Literary and Art Organisations\". This greatly stifled creativity. During the 1980s glasnost (openness) was introduced and Soviet artists and writers again became free to express themselves as they wanted.\nAs of 2023[ [update]], UNESCO inscribed 8 properties in Ukraine on the World Heritage List. Ukraine is also known for its decorative and folk traditions such as Petrykivka painting, Kosiv ceramics, and Cossack songs. Between February 2022 and March 2023, UNESCO verified the damage to 247 sites, including 107 religious sites, 89 buildings of artistic or historical interest, 19 monuments and 12 libraries. Since January 2023, the historic centre of Odesa has been inscribed on the List of World Heritage in Danger.\nThe tradition of the Easter eggs, known as pysanky, has long roots in Ukraine. These eggs were drawn on with wax to create a pattern; then, the dye was applied to give the eggs their pleasant colours, the dye did not affect the previously wax-coated parts of the egg. After the entire egg was dyed, the wax was removed leaving only the colourful pattern. This tradition is thousands of years old, and precedes the arrival of Christianity to Ukraine. In the city of Kolomyia near the foothills of the Carpathian Mountains, the museum of Pysanka was built in 2000 and won a nomination as the monument of modern Ukraine in 2007, part of the Seven Wonders of Ukraine action.\nSince 2012, the Ministry of Culture of Ukraine has formed the National Register of Elements of the Intangible Cultural Heritage of Ukraine, which consists of 115 items as of September 2025.\nLibraries.\nThe Vernadsky National Library of Ukraine, is the main academic library and main scientific information centre in Ukraine.\nDuring the Russian invasion of Ukraine the Russians bombed the Maksymovych Scientific Library of the Taras Shevchenko Kyiv National University, Vernadsky National Library of Ukraine, the National Scientific Medical Library of Ukraine and the Kyiv City Youth Library.\nLiterature.\nUkrainian literature has origins in Old Church Slavonic writings, which was used as a liturgical and literary language following Christianisation in the 10th and 11th centuries. Other writings from the time include chronicles, the most significant of which was the \"Primary Chronicle\". Literary activity faced a sudden decline after the Mongol invasion of Kievan Rus', before seeing a revival beginning in the 14th\u00a0century, and was advanced in the 16th\u00a0century with the invention of the printing press.\nThe Cossacks established an independent society and popularised a new kind of epic poem, which marked a high point of Ukrainian oral literature. These advances were then set back in the 17th and early 18th\u00a0centuries, as many Ukrainian authors wrote in Russian or Polish. Nonetheless, by the late 18th\u00a0century, the modern literary Ukrainian language finally emerged. In 1798, the modern era of the Ukrainian literary tradition began with Ivan Kotliarevsky's publication of Eneida in the Ukrainian vernacular.\nBy the 1830s, a Ukrainian romantic literature began to develop, and the nation's most renowned cultural figure, romanticist poet-painter Taras Shevchenko emerged. Whereas Ivan Kotliarevsky is considered to be the father of literature in the Ukrainian vernacular; Shevchenko is the father of a national revival.\nThen, in 1863, the use of the Ukrainian language in print was effectively prohibited by the Russian Empire. This severely curtailed literary activity in the area, and Ukrainian writers were forced to either publish their works in Russian or release them in Austrian controlled Galicia. The ban was never officially lifted, but it became obsolete after the revolution and the Bolsheviks' coming to power.\nUkrainian literature continued to flourish in the early Soviet years when nearly all literary trends were approved. These policies faced a steep decline in the 1930s, when prominent representatives as well as many others were killed by the NKVD during the Great Purge. In general around 223 writers were repressed by what was known as the Executed Renaissance. These repressions were part of Stalin's implemented policy of socialist realism. The doctrine did not necessarily repress the use of the Ukrainian language, but it required that writers follow a certain style in their works.\nLiterary freedom grew in the late 1980s and early 1990s alongside the decline and collapse of the USSR and the reestablishment of Ukrainian independence in 1991.\nArchitecture.\nUkrainian architecture includes the motifs and styles that are found in structures built in modern Ukraine, and by Ukrainians worldwide. These include initial roots which were established in the state of Kievan Rus'. Following the Christianisation of Kievan Rus', Ukrainian architecture has been influenced by Byzantine architecture. After the Mongol invasion of Kievan Rus', the Galician style continued to develop in the Kingdom of Galicia\u2013Volhynia.\nAfter the union with the Tsardom of Russia, architecture in Ukraine began to develop in different directions, with many structures in the larger eastern, Russian-ruled area built in the styles of Russian architecture of that period, whilst the western region of Galicia developed under Polish and Austro-Hungarian architectural influences. Still, a separate Ukrainian Baroque style was developed by the Ukrainian Cossacks in 17th\u201318th centuries, and Ukrainian Art Nouveau had limited success in the 20th century. Ukrainian national motifs would eventually be used during the period of the Soviet Union and in modern independent Ukraine. However, much of the contemporary architectural skyline of Ukraine is dominated by Soviet-style Khrushchyovkas, or low-cost apartment buildings.\nWeaving and embroidery.\nArtisan textile arts play an important role in Ukrainian culture, especially in Ukrainian wedding traditions. Ukrainian embroidery, weaving and lace-making are used in traditional folk dress and in traditional celebrations. Ukrainian embroidery varies depending on the region of origin and the designs have a long history of motifs, compositions, choice of colours and types of stitches. Use of colour is very important and has roots in Ukrainian folklore. Embroidery motifs found in different parts of Ukraine are preserved in the Rushnyk Museum in Pereiaslav.\nNational dress is woven and highly decorated. Weaving with handmade looms is still practised in the village of Krupove, situated in Rivne Oblast. The village is the birthplace of two internationally recognised personalities in the scene of national crafts fabrication: Nina Myhailivna and Uliana Petrivna.\nMusic.\nMusic is a major part of Ukrainian culture, with a long history and many influences. From traditional folk music, to classical and modern rock, Ukraine has produced several internationally recognised musicians including Kirill Karabits, Okean Elzy and Ruslana. Elements from traditional Ukrainian folk music made their way into Western music and even into modern jazz. Ukrainian music sometimes presents a perplexing mix of exotic melismatic singing with chordal harmony. The most striking general characteristic of authentic ethnic Ukrainian folk music is the wide use of minor modes or keys which incorporate augmented second intervals.\nDuring the Baroque period, music had a place of considerable importance in the curriculum of the Kyiv-Mohyla Academy. Much of the nobility was well versed in music with many Ukrainian Cossack leaders such as (Mazepa, Paliy, Holovatyj, Sirko) being accomplished players of the kobza, bandura or torban.\nThe first dedicated musical academy was set up in Hlukhiv in 1738 and students were taught to sing and play violin and bandura from manuscripts. As a result, many of the earliest composers and performers within the Russian empire were ethnically Ukrainian, having been born or educated in Hlukhiv or having been closely associated with this music school. Ukrainian classical music differs considerably depending on whether the composer was of Ukrainian ethnicity living in Ukraine, a composer of non-Ukrainian ethnicity who was a citizen of Ukraine, or part of the Ukrainian diaspora.\nSince the mid-1960s, Western-influenced pop music has been growing in popularity in Ukraine. Folk singer and harmonium player Mariana Sadovska is prominent. Ukrainian pop and folk music arose with the international popularity of groups and performers like Vopli Vidoplyasova, Dakh Daughters, Dakha Brakha, Ivan Dorn and Okean Elzy.\nMedia.\nThe Ukrainian legal framework on media freedom is deemed \"among the most progressive in eastern Europe\", although implementation has been uneven. The constitution and laws provide for freedom of speech and press. The main regulatory authority for the broadcast media is the National Television and Radio Broadcasting Council of Ukraine (NTRBCU), tasked with licencing media outlets and ensure their compliance with the law.\nKyiv dominates the media sector in Ukraine: National newspapers \"Den\", \"Dzerkalo Tyzhnia\", tabloids, such as \"The Ukrainian Week\" or \"Focus\", and television and radio are largely based there, although Lviv is also a significant national media centre. The National News Agency of Ukraine, Ukrinform was founded here in 1918. BBC Ukrainian started its broadcasts in 1992. As of 2022[ [update]] 75% of the population use the internet, and social media is widely used by government and people.\nOn 10 March 2024, creators of a documentary film \"20 Days in Mariupol\" were awarded with the Oscar in the category \"Best Documentary Feature Film\", the first Oscar in Ukraine's history.\nSport.\nUkraine greatly benefited from the Soviet emphasis on physical education. These policies left Ukraine with hundreds of stadia, swimming pools, gymnasia and many other athletic facilities. The most popular sport is football. The top professional league is the Vyscha Liha (\"premier league\").\nMany Ukrainians also played for the Soviet national football team, most notably Ballon d'Or winners Ihor Belanov and Oleh Blokhin. This award was only presented to one Ukrainian after the dissolution of the Soviet Union, Andriy Shevchenko. The national team made its debut in the 2006 FIFA World Cup, and reached the quarterfinals before losing to eventual champions, Italy.\nUkrainian boxers are amongst the best in the world. Since becoming the undisputed cruiserweight champion in 2018, Oleksandr Usyk has also gone on to win the unified WBA (Super), IBF, WBO and IBO heavyweight titles. This feat made him one of only three boxers to have unified the cruiserweight world titles and become a world heavyweight champion. The brothers Vitali and Wladimir Klitschko are former heavyweight world champions who held multiple world titles throughout their careers. Also hailing from Ukraine is Vasyl Lomachenko, a 2008 and 2012 Olympic gold medalist. He is the unified lightweight world champion who ties the record for winning a world title in the fewest professional fights; three.\nSergey Bubka held the record in the Pole vault from 1993 to 2014; with great strength, speed and gymnastic abilities, he was voted the world's best athlete on several occasions.\nBasketball has gained popularity in Ukraine. In 2011, Ukraine was granted a right to organise EuroBasket 2015. Two years later the Ukraine national basketball team finished sixth in EuroBasket 2013 and qualified to FIBA World Cup for the first time in its history. Euroleague participant Budivelnyk Kyiv is the strongest professional basketball club in Ukraine.\nChess is a popular sport in Ukraine. Ruslan Ponomariov is the former world champion. There are about 85 Grandmasters and 198 International Masters in Ukraine. Rugby league is played throughout Ukraine.\nCuisine.\nUkrainian cuisine has been formed by the nation's tumultuous history, geography, culture and social customs. Chicken is the most consumed type of protein, accounting for about half of the meat intake. It is followed by pork and beef. Vegetables such as potatoes, cabbages, mushrooms and beetroots are widely consumed. Pickled vegetables are considered a delicacy. Salo, which is cured pork fat, is considered the national delicacy. Widely used herbs include dill, parsley, basil, coriander and chives.\nUkraine is often called the \"Breadbasket of Europe\", and its plentiful grain and cereal resources such as rye and wheat play an important part in its cuisine; essential in making various kinds of bread. Chernozem, the country's black-coloured highly fertile soil, produces some of the world's most flavourful crops.\nPopular traditional dishes ' (dumplings), \"nalysnyky\" (cr\u00eapes), \"kapusnyak\" (cabbage soup), borscht (sour soup) and (cabbage rolls). Among traditional baked goods are decorated korovai and paska (easter bread). Ukrainian specialties also include Chicken Kiev and Kyiv cake. Popular drinks include \"uzvar\" (kompot made of dried fruits), \"ryazhanka\", and '. Liquor (spirits) are the most consumed type of alcoholic beverage. Alcohol consumption has seen a stark decrease, though by per capita, it remains among the highest in the world.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nPrint sources.\nReference books.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;\nRecent (since 1991).\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;\nHistory.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;\nWorld War II.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "31752", "revid": "27318187", "url": "https://en.wikipedia.org/wiki?curid=31752", "title": "Ulysses S. Grant", "text": "Civil War general, U.S. president from 1869 to 1877\nUlysses S. Grant (born Hiram Ulysses Grant; April 27, 1822\u00a0\u2013 July 23, 1885) was the 18th president of the United States, serving from 1869 to 1877. In 1865, as commanding general, Grant led the Union Army to victory in the American Civil War.\nGrant was born in Ohio and graduated from the United States Military Academy (West Point) in 1843. He served with distinction in the Mexican\u2013American War, but resigned from the army in 1854 and returned to civilian life impoverished. In 1861, shortly after the Civil War began, Grant joined the Union Army, and he rose to prominence after securing victories in the western theater in 1862. In 1863, he led the Vicksburg campaign that gave Union forces control of the Mississippi River and dealt a major strategic blow to the Confederacy. President Abraham Lincoln promoted Grant to lieutenant general and command of all Union armies after his victory at Chattanooga. For thirteen months, Grant fought Robert E. Lee during the high-casualty Overland Campaign, which ended when Lee surrendered to Grant at Appomattox. In 1866, President Andrew Johnson promoted Grant to General of the Army. Later, Grant broke with Johnson over Reconstruction policies. A war hero, drawn in by his sense of duty, Grant was unanimously nominated by the Republican Party and then elected president in 1868.\nAs president, Grant stabilized the post-war national economy, supported congressional Reconstruction and the Fifteenth Amendment, and prosecuted the Ku Klux Klan. Under Grant, the Union was completely restored. An effective civil rights executive, Grant signed a bill to create the United States Department of Justice and worked with Radical Republicans to protect African Americans during Reconstruction. In 1871, he created the first Civil Service Commission, advancing the civil service more than any prior president. Grant was re-elected in the 1872 presidential election, but was inundated by executive scandals during his second term. His response to the Panic of 1873 was ineffective in halting the Long Depression, which contributed to the Democrats winning the House majority in 1874. Grant's Native American policy was to assimilate Indians into Anglo-American culture. In Grant's foreign policy, the \"Alabama\" Claims against Britain were peacefully resolved, but the Senate rejected his proposal to annex Santo Domingo. In the disputed 1876 presidential election, he facilitated the approval by Congress of a peaceful compromise.\nLeaving office in 1877, Grant undertook a world tour, becoming the first president to circumnavigate the world. In 1880, he was unsuccessful in obtaining the Republican nomination for a non-consecutive third term. In 1885, impoverished and dying of throat cancer, Grant wrote his memoirs, covering his life through the Civil War, which were posthumously published and became a major critical and financial success. At his death, Grant was the most popular American and was memorialized as a symbol of national unity. Due to the pseudohistorical and negationist mythology of the Lost Cause of the Confederacy spread by Confederate sympathizers around the turn of the 20th century, historical assessments and rankings of Grant's presidency suffered considerably before they began recovering in the 21st century. Grant's critics take a negative view of his economic mismanagement and the corruption within his administration, while his admirers emphasize his policy towards Native Americans, vigorous enforcement of civil and voting rights for African Americans, and securing North and South as a single nation within the Union. 21st century scholarship has praised Grant's appointments of Cabinet reformers.\nEarly life and education.\nGrant's father, Jesse Root Grant, was a Whig Party supporter and a fervent abolitionist. He and Hannah Simpson were married on June 24, 1821, and their first child, Hiram Ulysses Grant, was born on April 27, 1822. To honor his father-in-law, Jesse named the boy \"Hiram Ulysses\", though he always referred to him as \"Ulysses\". In 1823, the family moved to Georgetown, Ohio, where five siblings were born: Simpson, Clara, Orvil, Jennie, and Mary. At the age of five, Ulysses started at a subscription school and later attended two private schools. In the winter of 1836\u20131837, Grant was a student at Maysville Seminary, and in the autumn of 1838, he attended John Rankin's academy.\nIn his youth, Grant developed an unusual ability to ride and manage horses; his father gave him work driving supply wagons and transporting people. Unlike his siblings, Grant was not required to attend church by his Methodist parents. For the rest of his life, he prayed privately and never officially joined any denomination. To others, including his own son, Grant appeared to be agnostic. Grant was largely apolitical before the war but wrote, \"If I had ever had any political sympathies they would have been with the Whigs. I was raised in that school.\"\nEarly military career and personal life.\nWest Point and first assignment.\nAt Jesse Grant's request, Representative Thomas L. Hamer nominated Ulysses to the United States Military Academy at West Point, New York, in spring 1839. Grant was accepted on July 1. Unfamiliar with Grant, Hamer altered his name, so Grant was enlisted under the name \"U. S. Grant\". Since the initials \"U.S.\" also stood for \"Uncle Sam\", he became known among army colleagues as \"Sam.\"\nInitially, Grant was indifferent to military life, but within a year he reexamined his desire to leave the academy and later wrote that \"on the whole I like this place very much\". He earned a reputation as the \"most proficient\" horseman. Seeking relief from military routine, he studied under Romantic artist Robert Walter Weir, producing nine surviving artworks. He spent more time reading books from the library than his academic texts. On Sundays, cadets were required to march to services at the academy's church, which Grant disliked. Quiet by nature, he established a few intimate friends among fellow cadets, including Frederick Tracy Dent and James Longstreet. He was inspired both by the Commandant, Captain Charles Ferguson Smith, and by General Winfield Scott, who visited the academy to review the cadets. Grant later wrote of the military life, \"there is much to dislike, but more to like.\"\nGrant graduated on June 30, 1843, ranked 21st out of 39 in his class and was promoted the next day to brevet second lieutenant. He planned to resign his commission after his four-year term. He would later write that among the happiest days of his life were the day he left the presidency and the day he left the academy. Despite his excellent horsemanship, he was not assigned to the cavalry, but to the 4th Infantry Regiment. Grant's first assignment was the Jefferson Barracks near St. Louis, Missouri. Commanded by Colonel Stephen W. Kearny, this was the nation's largest military base in the West. Grant was happy with his commander but looked forward to the end of his military service and a possible teaching career.\nMarriage and family.\nIn 1844, Grant accompanied Frederick Dent to Missouri and met his family, including Dent's sister Julia. The two soon became engaged. On August 22, 1848, they were married at Julia's home in St. Louis. Grant's abolitionist father disapproved of the Dents' owning slaves, and neither of Grant's parents attended the wedding. Grant was flanked by three fellow West Point graduates in their blue uniforms, including Longstreet, Julia's cousin.\nThe couple had four children: Frederick, Ulysses Jr. (\"Buck\"), Ellen (\"Nellie\"), and Jesse II. After the wedding, Grant obtained a two-month extension to his leave and returned to St. Louis, where he decided that, with a wife to support, he would remain in the army.\nMexican\u2013American War.\nGrant's unit was stationed in Louisiana as part of the Army of Occupation under Major General Zachary Taylor. In September 1846, President James K. Polk ordered Taylor to march south to the Rio Grande. Marching to Fort Texas, to prevent a Mexican siege, Grant experienced combat for the first time on May 8, 1846, at the Battle of Palo Alto. Grant served as regimental quartermaster, but yearned for a combat role; when finally allowed, he led a charge at the Battle of Resaca de la Palma. He demonstrated his equestrian ability at the Battle of Monterrey by volunteering to carry a dispatch past snipers; he hung off the side of his horse, keeping the animal between him and the enemy. Polk, wary of Taylor's growing popularity, divided his forces, sending some troops (including Grant's unit) to form a new army under Major General Winfield Scott. \nTraveling by sea, Scott's army landed at Veracruz and advanced toward Mexico City. They met the Mexican forces at the battles of Molino del Rey and Chapultepec. For his bravery at Molino del Rey, Grant was brevetted first lieutenant on September 30. At San Cosm\u00e9, Grant directed his men to drag a disassembled mountain howitzer into a church steeple, then reassembled it and bombarded nearby Mexican troops. His bravery and initiative earned him his brevet promotion to captain. On September 14, 1847, Scott's army marched into the city; Mexico ceded the vast territory, including California, to the U.S. on February 2, 1848.\nDuring the war, Grant established a commendable record as a daring and competent soldier and began to consider a career in the army. He studied the tactics and strategies of Scott and Taylor and emerged as a seasoned officer, writing in his memoirs that this is how he learned much about military leadership. In retrospect, although he respected Scott, he identified his own leadership style with Taylor's. Grant later believed the Mexican war was morally unjust and that the territorial gains were designed to expand slavery. He opined that the Civil War was divine punishment for U.S. aggression against Mexico.\nHistorians have pointed to the importance of Grant's experience as an assistant quartermaster during the war. Although he was initially averse to the position, it prepared Grant in understanding military supply routes, transportation systems, and logistics, particularly with regard to \"provisioning a large, mobile army operating in hostile territory\", according to biographer Ronald White. Grant came to recognize how wars could be won or lost by factors beyond the battlefield.\nPost-war assignments and resignation.\nGrant's first post-war assignments took him and Julia to Detroit on November 17, 1848, but he was soon transferred to Madison Barracks, a desolate outpost in upstate New York, in bad need of supplies and repair. After four months, Grant was sent back to his quartermaster job in Detroit. When the discovery of gold in California brought prospectors and settlers to the territory, Grant and the 4th infantry were ordered to reinforce the small garrison there. Grant was charged with bringing the soldiers and a few hundred civilians from New York City to Panama, overland to the Pacific and then north to California. Julia, eight months pregnant with Ulysses Jr., did not accompany him.\nWhile Grant was in Panama, a cholera epidemic killed many soldiers and civilians. Grant organized a field hospital in Panama City, and moved the worst cases to a hospital barge offshore. When orderlies protested having to attend to the sick, Grant did much of the nursing himself, earning high praise from observers. In August, Grant arrived in San Francisco. His next assignment sent him north to Vancouver Barracks in the Oregon Territory.\nGrant tried several business ventures but failed, and in one instance his business partner absconded with $800 of Grant's investment, . After he witnessed white agents cheating local Indians of their supplies, and their devastation by smallpox and measles transferred to them by white settlers, he developed empathy for their plight.\nPromoted to captain on August 5, 1853, Grant was assigned to command Company F, 4th Infantry, at the newly constructed Fort Humboldt in California. Grant arrived at Fort Humboldt on January 5, 1854, commanded by Lieutenant Colonel Robert C. Buchanan. Separated from his family, Grant began to drink. Colonel Buchanan reprimanded Grant for one drinking episode and told Grant to \"resign or reform.\" Grant told Buchanan he would \"resign if I don't reform.\" On Sunday, Grant was found influenced by alcohol, but not incapacitated, at his company's paytable. Keeping his pledge to Buchanan, Grant resigned, effective July 31, 1854. Buchanan endorsed Grant's resignation but did not submit any report that verified the incident. Grant did not face court-martial, and the War Department said: \"Nothing stands against his good name.\" Grant said years later, \"the vice of intemperance (drunkenness) had not a little to do with my decision to resign.\" With no means of support, Grant returned to St. Louis and reunited with his family.\nCivilian struggles, slavery, and politics.\nIn 1854, at age 32, Grant entered civilian life, without any money-making vocation to support his growing family. It was the beginning of seven years of financial struggles and instability. Grant's father offered him a place in the Galena, Illinois, branch of the family's leather business, but demanded Julia and the children stay in Missouri, with the Dents, or with the Grants in Kentucky. Grant and Julia declined. For the next four years, Grant farmed with the help of Julia's slave, Dan, on his brother-in-law's property, \"Wish-ton-wish\", near St. Louis. The farm was not successful and to earn a living he sold firewood on St. Louis street corners.\nIn 1856, the Grants moved to land on Julia's father's farm, and built a home called \"Hardscrabble\" on Grant's Farm; Julia described it as an \"unattractive cabin\". Grant's family had little money, clothes, and furniture, but always had enough food. During the Panic of 1857, which devastated Grant as it did many farmers, Grant pawned his gold watch to buy Christmas gifts. In 1858, Grant rented out Hardscrabble and moved his family to Julia's father's 850-acre plantation. That fall, after having malaria, Grant gave up farming. Fearing that electing a Republican president would lead to a civil war, he voted for Democrat James Buchanan in 1856. He had the same fear in 1860 and preferred Douglas, the Democrat. However he did not vote in 1860 because he lacked the resident requirement in Galena.\nIn 1858, Grant acquired a slave from his father-in-law, a thirty-five-year-old man named William Jones. Although Grant was not an abolitionist at the time, he disliked slavery and could not bring himself to force an enslaved man to work. In March 1859, Grant freed Jones by a manumission deed, potentially worth at least $1,000 (equivalent to $ in 2024).\nGrant moved to St. Louis, taking on a partnership with Julia's cousin Harry Boggs working in the real estate business as a bill collector, again without success and at Julia's prompting ended the partnership. In August, Grant applied for a position as county engineer. He had thirty-five notable recommendations, but Grant was passed over by the Free Soil and Republican county commissioners because he was believed to share his father-in-law's Democratic sentiments.\nIn April 1860, Grant and his family moved north to Galena, accepting a position in his father's leather goods business, \"Grant &amp; Perkins\", run by his younger brothers Simpson and Orvil. In a few months, Grant paid off his debts. The family attended the local Methodist church and he soon established himself as a reputable citizen.\nCivil War.\nOn April 12, 1861, the American Civil War began when Confederate troops attacked Fort Sumter in Charleston, South Carolina. The news came as a shock in Galena, and Grant shared his neighbors' concern about the war. On April 15, Lincoln called for 75,000 volunteers. The next day, Grant attended a mass meeting to assess the crisis and encourage recruitment, and a speech by his father's attorney, John Aaron Rawlins, stirred Grant's patriotism. In an April 21 letter to his father, Grant wrote out his views on the upcoming conflict: \"We have a government and laws and a flag, and they must all be sustained. There are but two parties now, Traitors and Patriots.\"\nEarly commands.\nOn April 18, Grant chaired a second recruitment meeting, but turned down a captain's position as commander of the newly formed militia company, hoping his experience would aid him to obtain a more senior rank. His early efforts to be recommissioned were rejected by Major General George B. McClellan and Brigadier General Nathaniel Lyon. On April 29, supported by Congressman Elihu B. Washburne of Illinois, Grant was appointed military aide to Governor Richard Yates and mustered ten regiments into the Illinois militia. On June 14, again aided by Washburne, Grant was appointed colonel and put in charge of the 21st Illinois Volunteer Infantry Regiment; he appointed John A. Rawlins as his aide-de-camp and brought order and discipline to the regiment. Soon after, Grant and the 21st Regiment were transferred to Missouri to dislodge Confederate forces.\nOn August 5, with Washburne's aid, Grant was appointed brigadier general of volunteers. Major General John C. Fr\u00e9mont, Union commander of the West, passed over senior generals and appointed Grant commander of the District of Southeastern Missouri. On September 2, Grant arrived at Cairo, Illinois, assumed command by replacing Colonel Richard J. Oglesby, and set up his headquarters to plan a campaign down the Mississippi, and up the Tennessee and Cumberland rivers.\nAfter the Confederates moved into western Kentucky, taking Columbus, with designs on southern Illinois, Grant notified Fr\u00e9mont and, without waiting for his reply, advanced on Paducah, Kentucky, taking it without a fight on September 6. Having understood the importance to Lincoln of Kentucky's neutrality, Grant assured its citizens, \"I have come among you not as your enemy, but as your friend.\" On November 1, Fr\u00e9mont ordered Grant to \"make demonstrations\" against the Confederates on both sides of the Mississippi, but prohibited him from attacking.\nBelmont (1861), Forts Henry and Donelson (1862).\nOn November 2, 1861, Lincoln removed Fr\u00e9mont from command, freeing Grant to attack Confederate soldiers encamped in Cape Girardeau, Missouri. On November 5, Grant, along with Brigadier General John A. McClernand, landed 2,500 men at Hunter's Point, and on November 7 engaged the Confederates at the Battle of Belmont. The Union army took the camp, but the reinforced Confederates under Brigadier Generals Frank Cheatham and Gideon J. Pillow forced a chaotic Union retreat. Grant had wanted to destroy Confederate strongholds at Belmont, Missouri, and Columbus, Kentucky, but was not given enough troops and was only able to disrupt their positions. Grant's troops escaped back to Cairo under fire from the fortified stronghold at Columbus. Although Grant and his army retreated, the battle gave his volunteers much-needed confidence and experience.\nColumbus blocked Union access to the lower Mississippi. Grant and lieutenant colonel James B. McPherson planned to bypass Columbus and move against Fort Henry on the Tennessee River. They would then march east to Fort Donelson on the Cumberland River, with the aid of gunboats, opening both rivers and allowing the Union access further south. Grant presented his plan to Henry Halleck, his new commander in the newly created Department of Missouri. Halleck rebuffed Grant, believing he needed twice the number of troops. However, after consulting McClellan, he finally agreed on the condition that the attack would be in close cooperation with Navy flag officer Andrew H. Foote. Foote's gunboats bombarded Fort Henry, leading to its surrender on February 6, 1862, before Grant's infantry even arrived.\nGrant ordered an immediate assault on Fort Donelson, which dominated the Cumberland River. Unaware of the garrison's strength, Grant, McClernand, and Smith positioned their divisions around the fort. The next day McClernand and Smith independently launched probing attacks on apparent weak spots but were forced to retreat. On February 14, Foote's gunboats began bombarding the fort, only to be repulsed by its heavy guns. The next day, Pillow attacked and routed McClernand's division. Union reinforcements arrived, giving Grant a total force of over 40,000 men. Grant was with Foote four miles away when the Confederates attacked. Hearing the battle, Grant rode back and rallied his troop commanders, riding over seven miles of freezing roads and trenches, exchanging reports. When Grant blocked the Nashville Road, the Confederates retreated back into Fort Donelson. On February 16, Foote resumed his bombardment, signaling a general attack. Confederate generals John B. Floyd and Pillow fled, leaving the fort in command of Simon Bolivar Buckner, who submitted to Grant's demand for \"unconditional and immediate surrender\".\nGrant had won the first major victory for the Union, capturing Floyd's entire army of more than 12,000. Halleck was angry that Grant had acted without his authorization and complained to McClellan, accusing Grant of \"neglect and inefficiency\". On March 3, Halleck sent a telegram to Washington complaining that he had no communication with Grant for a week. Three days later, Halleck claimed \"word has just reached me that\u00a0... Grant has resumed his bad habits (of drinking).\" Lincoln, regardless, promoted Grant to major general of volunteers and the Northern press treated Grant as a hero. Playing off his initials, they took to calling him \"Unconditional Surrender Grant\".\nShiloh (1862) and aftermath.\nReinstated by Halleck at the urging of Lincoln and Secretary of War Edwin Stanton, Grant rejoined his army with orders to advance with the Army of the Tennessee into Tennessee. His main army was located at Pittsburg Landing, while 40,000 Confederate troops converged at Corinth, Mississippi. Grant wanted to attack the Confederates at Corinth, but Halleck ordered him not to attack until Major General Don Carlos Buell arrived with his division of 25,000. Grant prepared for an attack on the Confederate army of roughly equal strength. Instead of preparing defensive fortifications, they spent most of their time drilling the largely inexperienced troops while Sherman dismissed reports of nearby Confederates.\nOn the morning of April 6, 1862, Grant's troops were taken by surprise when the Confederates, led by Generals Albert Sidney Johnston and P. G. T. Beauregard, struck first \"like an Alpine avalanche\" near Shiloh church, attacking five divisions of Grant's army and forcing a confused retreat toward the Tennessee River. Johnston was killed and command fell upon Beauregard. One Union line held the Confederate attack off for several hours, giving Grant time to assemble artillery and 20,000 troops near Pittsburg Landing. The Confederates finally broke and captured a Union division, but Grant's newly assembled line held the landing, while the exhausted Confederates, lacking reinforcements, halted their advance.\nBolstered by 18,000 troops from the divisions of Major Generals Buell and Lew Wallace, Grant counterattacked at dawn the next day and regained the field, forcing the disorganized and demoralized rebels to retreat to Corinth. Halleck ordered Grant not to advance more than one day's march from Pittsburg Landing, stopping the pursuit. Although Grant had won the battle, the situation was little changed. Grant, now realizing that the South was determined to fight, would later write, \"Then, indeed, I gave up all idea of saving the Union except by complete conquest.\"\nShiloh was the costliest battle in American history to that point and the staggering 23,746 casualties stunned the nation. Briefly hailed a hero for routing the Confederates, Grant was soon mired in controversy. The Northern press castigated Grant for shockingly high casualties, and accused him of drunkenness during the battle, contrary to the accounts of those with him at the time. Discouraged, Grant considered resigning but Sherman convinced him to stay. Lincoln dismissed Grant's critics, saying \"I can't spare this man; he fights.\" Grant's costly victory at Shiloh ended any chance for the Confederates to prevail in the Mississippi valley or regain its strategic advantage in the West.\nHalleck arrived from St. Louis on April 11, took command, and assembled a combined army of about 120,000 men. On April 29, he relieved Grant of field command and replaced him with Major General George Henry Thomas. Halleck slowly marched his army to take Corinth, entrenching each night. Meanwhile, Beauregard pretended to be reinforcing, sent \"deserters\" to the Union Army with that story, and moved his army out during the night, to Halleck's surprise when he finally arrived at Corinth on May 30.\nHalleck divided his combined army and reinstated Grant as field commander on July 11. Later that year, on September 19, Grant's army defeated Confederates at the Battle of Iuka, then successfully defended Corinth, inflicting heavy casualties. On October 25, Grant assumed command of the District of the Tennessee. In November, after Lincoln's preliminary Emancipation Proclamation, Grant ordered units under his command to incorporate former slaves into the Union Army, giving them clothes, shelter, and wages for their services.\nVicksburg campaign (1862\u20131863).\nThe Union capture of Vicksburg, the last Confederate stronghold on the Mississippi River, was considered vital as it would split the Confederacy in two. Lincoln appointed McClernand for the job, rather than Grant or Sherman. Halleck, who retained power over troop displacement, ordered McClernand to Memphis, and placed him and his troops under Grant's authority.\nOn November 13, 1862, Grant captured Holly Springs and advanced to Corinth. His plan was to attack Vicksburg overland, while Sherman would attack Vicksburg from Chickasaw Bayou. However, Confederate cavalry raids on December 11 and 20 broke Union communications and recaptured Holly Springs, preventing Grant and Sherman from converging on Vicksburg. The most significant of these raids was led by Confederate General Earl Van Dorn, whose successful surprise attack at Holly Springs on December 20, 1862, destroyed Grant\u2019s supply base and forced him to abandon his overland advance on Vicksburg. McClernand reached Sherman's army, assumed command, and independently of Grant led a campaign that captured Confederate Fort Hindman. After the sack of Holly Springs, Grant considered and sometimes adopted the strategy of foraging the land, rather than exposing long Union supply lines to enemy attack.\nFugitive African-American slaves poured into Grant's district, whom he sent north to Cairo to be domestic servants in Chicago. However, Lincoln ended this when Illinois political leaders complained. On his own initiative, Grant set up a pragmatic program and hired Presbyterian chaplain John Eaton to administer contraband camps. Freed slaves picked cotton that was shipped north to aid the Union war effort. Lincoln approved and Grant's program was successful. Grant also worked freed black labor on a canal to bypass Vicksburg, incorporating the laborers into the Union Army and Navy.\nGrant's war responsibilities included combating illegal Northern cotton trade and civilian obstruction. He had received numerous complaints about Jewish speculators in his district. The majority, however, of those involved in illegal trading were not Jewish. To help combat this, Grant required two permits, one from the Treasury and one from the Union Army, to purchase cotton. On December 17, 1862, Grant issued a controversial General Order No. 11, expelling \"Jews, as a class\", from his military district. After complaints, Lincoln rescinded the order on January 3, 1863. Grant finally ended the order on January 17. He later described issuing the order as one of his biggest regrets.\nOn January 29, 1863, Grant assumed overall command. To bypass Vicksburg's guns, Grant slowly advanced his Union army south through water-logged terrain. The plan of attacking Vicksburg from downriver was risky because, east of the river, his army would be distanced from most of its supply lines, and would have to rely on foraging. On April 16, Grant ordered Admiral David Dixon Porter's gunboats south under fire from the Vicksburg batteries to meet up with troops who had marched south down the west side of the river. Grant ordered diversionary battles, confusing Pemberton and allowing Grant's army to move east across the Mississippi. Grant's army captured Jackson. Advancing west, he defeated Pemberton's army at the Battle of Champion Hill on May 16, forcing their retreat into Vicksburg.\nAfter Grant's men assaulted the entrenchments twice, suffering severe losses, they settled in for a siege which lasted seven weeks. During quiet periods of the campaign, Grant would drink on occasion. The personal rivalry between McClernand and Grant continued until Grant removed him from command when he contravened Grant by publishing an order without permission. Pemberton surrendered Vicksburg to Grant on July 4, 1863.\nVicksburg's fall gave Union forces control of the Mississippi River and split the Confederacy. By that time, Grant's political sympathies fully coincided with the Radical Republicans' aggressive prosecution of the war and emancipation of the slaves. The success at Vicksburg was a morale boost for the Union war effort. When Stanton suggested Grant be brought east to run the Army of the Potomac, Grant demurred, writing that he knew the geography and resources of the West better and he did not want to upset the chain of command in the East.\nChattanooga (1863) and promotion.\nOn October 16, 1863, Lincoln promoted Grant to major general in the regular army and assigned him command of the newly formed Division of the Mississippi, which comprised the Armies of the Ohio, the Tennessee, and the Cumberland. After the Battle of Chickamauga, the Army of the Cumberland retreated into Chattanooga, where they were partially besieged. Grant arrived in Chattanooga, where plans to resupply and break the partial siege had already been set. Forces commanded by Major General Joseph Hooker, which had been sent from the Army of the Potomac, approached from the west and linked up with other units moving east from inside the city, capturing Brown's Ferry and opening a supply line to the railroad at Bridgeport.\nGrant planned to have Sherman's Army of the Tennessee, assisted by the Army of the Cumberland, assault the northern end of Missionary Ridge and roll down it on the enemy's right flank. On November 23, Major General George Henry Thomas surprised the enemy in open daylight, advancing the Union lines and taking Orchard Knob, between Chattanooga and the ridge. The next day, Sherman failed to get atop Missionary Ridge, which was key to Grant's plan of battle. Hooker's forces took Lookout Mountain in unexpected success. On the 25th, Grant ordered Thomas to advance to the rifle-pits at the base of Missionary Ridge after Sherman's army failed to take Missionary Ridge from the northeast. Four divisions of the Army of the Cumberland, with the center two led by Major General Philip Sheridan and Brigadier General Thomas J. Wood, chased the Confederates out of the rifle-pits at the base and, against orders, continued the charge up the 45-degree slope and captured the Confederate entrenchments along the crest, forcing a hurried retreat. The decisive battle gave the Union control of Tennessee and opened Georgia, the Confederate heartland, to Union invasion.\nOn March 2, 1864, Lincoln promoted Grant to lieutenant general, giving him command of all Union Armies. Grant's new rank had previously been held only by George Washington. Grant arrived in Washington on March 8 and was formally commissioned by Lincoln the next day at a Cabinet meeting. Grant developed a good working relationship with Lincoln, who allowed Grant to devise his own strategy.\nGrant established his headquarters with General George Meade's Army of the Potomac in Culpeper, Virginia, and met weekly with Lincoln and Stanton in Washington. After protest from Halleck, Grant scrapped a risky invasion of North Carolina and planned five coordinated Union offensives to prevent Confederate armies from shifting troops along interior lines. Grant and Meade would make a direct frontal attack on Robert E. Lee's Army of Northern Virginia, while Sherman\u2014now in command of all western armies\u2014would destroy Joseph E. Johnston's Army of Tennessee and take Atlanta. Major General Benjamin Butler would advance on Lee from the southeast, up the James River, while Major General Nathaniel Banks would capture Mobile. Major General Franz Sigel was to capture granaries and rail lines in the fertile Shenandoah Valley. Grant now commanded 533,000 battle-ready troops spread out over an eighteen-mile front.\nOverland Campaign (1864).\nThe Overland Campaign was a series of brutal battles fought in Virginia during May and June 1864. Sigel's and Butler's efforts failed, and Grant was left alone to fight Lee. On May 4, Grant led the army from his headquarters towards Germanna Ford. They crossed the Rapidan unopposed. On May 5, the Union army attacked Lee in the battle of the Wilderness, a three-day battle with estimated casualties of 17,666 Union and 11,125 Confederate.\nRather than retreat, Grant flanked Lee's army to the southeast and attempted to wedge his forces between Lee and Richmond at Spotsylvania Court House. Lee's army got to Spotsylvania first and a costly battle ensued, lasting thirteen days, with heavy casualties. On May 12, Grant attempted to break through Lee's \"Muleshoe\" salient guarded by Confederate artillery, resulting in one of the bloodiest assaults of the Civil War, known as the Bloody Angle. Unable to break Lee's lines, Grant again flanked the rebels to the southeast, meeting at North Anna, where a battle lasted three days.\nCold Harbor.\nThe then-recent bloody Wilderness campaign had severely diminished Confederate morale; Grant believed breaking through Lee's lines at its weakest point, Cold Harbor, a vital road hub that linked to Richmond, would mean a quick end to the war. Grant already had two corps in position at Cold Harbor with Hancock's corps on the way.\nLee's lines were extended north and east of Richmond and Petersburg for approximately ten miles, but at several points there were no fortifications built yet, including Cold Harbor. On June 1 and 2 both Grant and Lee were waiting for reinforcements to arrive. Hancock's men had marched all night and arrived too exhausted for an immediate attack that morning. Grant postponed the attack until 5 p.m., and then again until 4:30\u00a0a.m. on June 3. However, Grant and Meade did not give specific orders for the attack, leaving it up to the corps commanders to coordinate. Grant had not yet learned that overnight Lee had hastily constructed entrenchments to thwart any breach attempt at Cold Harbor. Grant was anxious to make his move before the rest of Lee's army arrived. On the morning of June 3, with a force of more than 100,000 men, against Lee's 59,000, Grant attacked, not realizing that Lee's army was now well entrenched, much of it obscured by trees and bushes. Grant's army suffered 12,000\u201314,000 casualties, while Lee's army suffered 3,000\u20135,000 casualties, but Lee was less able to replace them.\nThe unprecedented number of casualties heightened anti-war sentiment in the North. After the battle, Grant wanted to appeal to Lee under the white flag for each side to gather up their wounded, most of them Union soldiers, but Lee insisted that a total truce be enacted and while they were deliberating all but a few of the wounded died in the field. Without giving an apology for the disastrous defeat in his official military report, Grant confided in his staff after the battle and years later wrote in his memoirs that he \"regretted that the last assault at Cold Harbor was ever made.\"\nSiege of Petersburg (1864\u20131865).\nUndetected by Lee, Grant moved his army south of the James River, freed Butler from the Bermuda Hundred, and advanced toward Petersburg, Virginia's central railroad hub, resulting in a nine-month siege. Northern resentment grew. Sheridan was assigned command of the Union Army of the Shenandoah and Grant directed him to \"follow the enemy to their death\" in the Shenandoah Valley. After Grant's abortive attempt to capture Petersburg, Lincoln supported Grant in his decision to continue.\nGrant had to commit troops to check Confederate General Jubal Early's raids in the Shenandoah Valley, which were getting dangerously close to Washington. By late July, at Petersburg, Grant reluctantly approved a plan to blow up part of the enemy trenches from a tunnel filled with gunpowder. The massive explosion instantly killed an entire Confederate regiment. The poorly led Union troops under Major General Ambrose Burnside and Brigadier General James H. Ledlie, rather than encircling the crater, rushed into it. Recovering from the surprise, Confederates, led by Major General William Mahone, surrounded the crater and easily picked off Union troops. The Union's 3,500 casualties outnumbered the Confederates' three-to-one. The battle marked the first time that Union black troops, who endured a large proportion of the casualties, engaged in any major battle in the east. Grant admitted that the tactic had been a \"stupendous failure\".\nGrant would later meet with Lincoln and testify at a court of inquiry against Generals Burnside and Ledlie for their incompetence. In his memoirs, he blamed them for that disastrous Union defeat. Rather than fight Lee in a full-frontal attack as he had done at Cold Harbor, Grant continued to force Lee to extend his defenses south and west of Petersburg, better allowing him to capture essential railroad links.\nUnion forces soon captured Mobile Bay and Atlanta and now controlled the Shenandoah Valley, ensuring Lincoln's reelection in November. Sherman convinced Grant and Lincoln to allow his army to march on Savannah. Sherman cut a path of destruction unopposed, reached the Atlantic Ocean, and captured Savannah on December 22. On December 16, after much prodding by Grant, the Union Army under Thomas smashed John Bell Hood's Confederates at Nashville. These campaigns left Lee's forces at Petersburg as the only significant obstacle remaining to Union victory.\nBy March 1865, Lee was trapped and his strength severely weakened. He was running out of reserves to replace the high battlefield casualties and remaining Confederate troops, no longer having confidence in their commander and under the duress of trench warfare, deserted by the thousands. On March 25, in a desperate effort, Lee sacrificed his remaining troops (4,000 Confederate casualties) at Fort Stedman, a Union victory and the last Petersburg line battle.\nSurrender of Lee and Union victory (1865).\nOn April 2, Grant ordered a general assault on Lee's forces; Lee abandoned Petersburg and Richmond, which Grant captured. A desperate Lee and part of his army attempted to link up with the remnants of Joseph E. Johnston's army. Sheridan's cavalry stopped the two armies from converging, cutting them off from their supply trains. Grant sent his aide Orville Babcock to carry his last dispatch to Lee demanding his surrender. Grant immediately rode west, bypassing Lee's army, to join Sheridan who had captured Appomattox Station, blocking Lee's escape route. On his way, Grant received a letter from Lee stating Lee would surrender his army.\nOn April 9, Grant and Lee met at Appomattox Court House. Although Grant felt depressed at the fall of \"a foe who had fought so long and valiantly,\" he believed the Southern cause was \"one of the worst for which a people ever fought.\" Grant wrote out the terms of surrender: \"each officer and man will be allowed to return to his home, not to be disturbed by U.S. authority so long as they observe their paroles and the laws in force where they may reside.\" Lee immediately accepted Grant's terms and signed the surrender document, without any diplomatic recognition of the Confederacy. Lee asked that his former Confederate troops keep their horses, which Grant generously allowed. Grant ordered his troops to stop all celebration, saying the \"war is over; the rebels are our countrymen again.\" Johnston's Tennessee army surrendered on April 26, 1865, Richard Taylor's Alabama army on May 4, and Kirby Smith's Texas army on May 26, ending the war.\nLincoln's assassination.\nOn April 14, 1865, Grant attended a cabinet meeting in Washington. Lincoln invited him and his wife Julia to Ford's Theatre but they declined, because they planned to travel to their home in Burlington. In a conspiracy that also targeted top cabinet members in one last effort to topple the Union, Lincoln was shot by John Wilkes Booth at the theater and died the next morning. Many, including Grant himself, thought that Grant had been a target in the plot. During the subsequent trial, the government tried to prove that Grant had been stalked by Booth's conspirator Michael O'Laughlen. Stanton notified Grant of the president's death and summoned him to Washington. Vice President Andrew Johnson was sworn in as president on April 15. Grant was determined to work with Johnson, and he privately expressed \"every reason to hope\" in the new president's ability to run the government \"in its old channel\".\nCommanding generalship (1865\u20131869).\nAt the war's end, Grant remained commander of the army, with duties that included dealing with Emperor Maximilian and French troops in Mexico, enforcement of Reconstruction in the former Confederate states, and supervision of Indian wars on the western Plains. After the Grand Review of the Armies, Lee and his generals were indicted for treason in Virginia. Johnson demanded they be put on trial, but Grant insisted that they should not be tried, citing his Appomattox amnesty. Charges against Lee were dropped. Grant secured a house for his family in Georgetown Heights in 1865 but instructed Elihu Washburne that for political purposes his legal residence remained in Galena, Illinois. On July 25, 1866, Congress promoted Grant to the newly created rank of General of the Army of the United States.\nTour of the South.\nPresident Johnson's Reconstruction policy included a speedy return of the former Confederates to Congress, reinstating white people to office in the South, and relegating black people to second-class citizenship. On November 27, 1865, Grant was sent by Johnson on a fact-finding mission to the South, to counter a pending less favorable report by Senator Carl Schurz which reported that white people in the South harbored resentment of the North, and that black people suffered from violence and fraud. Grant recommended continuation of the Freedmen's Bureau, which Johnson opposed, but advised against using black troops.\nGrant believed the people of the South were not ready for self-rule and required federal government protection. Concerned that the war led to diminished respect for civil authorities, he continued using the Army to maintain order. Grant's report on the South, which he later recanted, sympathized with Johnson's Reconstruction policies. Although Grant desired former Confederates be returned to Congress, he advocated eventual black citizenship. On December 19, the day after the passage of the Thirteenth Amendment was announced in the Senate, Johnson's response used Grant's report, read aloud to the Senate, to undermine Schurz's final report and Radical opposition to Johnson's policies.\nBreak from Johnson.\nGrant was initially optimistic about Johnson. Despite differing styles, the two got along cordially and Grant attended cabinet meetings concerning Reconstruction. By February 1866, the relationship began to break down. Johnson opposed Grant's closure of the \"Richmond Examiner\" for disloyal editorials and his enforcement of the Civil Rights Act of 1866, passed over Johnson's veto. Needing Grant's popularity, Johnson took Grant on his \"Swing Around the Circle\" tour, a failed attempt to gain national support for lenient policies toward the South. Grant privately called Johnson's speeches a \"national disgrace\" and he left the tour early. On March 2, 1867, overriding Johnson's veto, Congress passed the first of three Reconstruction Acts, using military officers to enforce the policy. Protecting Grant, Congress passed the Command of the Army Act, preventing his removal or relocation, and forcing Johnson to pass orders through Grant.\nIn August 1867, bypassing the Tenure of Office Act, Johnson discharged Secretary of War Edwin Stanton without Senate approval and appointed Grant \"ad interim\" Secretary of War. Stanton was the only remaining cabinet member friendly to the Radicals. Although Grant initially recommended against dismissing Stanton, he accepted the position, not wanting the Army to fall under a conservative appointee who would impede Reconstruction, and managed an uneasy partnership with Johnson.\nIn December 1867, Congress voted to keep Stanton, who was reinstated by a Senate Committee on January 10, 1868. Grant told Johnson he was going to resign the office to avoid fines and imprisonment. Johnson, who believed the law would be overturned, said he would assume Grant's legal responsibility, and reminded Grant that he had promised to delay his resignation until a suitable replacement was found. The following Monday, not willing to wait for the law to be overturned, Grant surrendered the office to Stanton, causing confusion with Johnson. With the backing of his cabinet, Johnson accused Grant of lying and \"duplicity\" at a stormy cabinet meeting, while a shocked and disappointed Grant felt it was Johnson who was lying. The publication of angry messages between Grant and Johnson led to a complete break between them. The controversy led to Johnson's impeachment and trial in the Senate; he was acquitted by one vote. Grant's popularity rose among the Radical Republicans and his nomination for the presidency appeared certain.\nElection of 1868.\nAt the 1868 Republican National Convention, the delegates unanimously nominated Grant for president on the first ballot and Speaker of the House Schuyler Colfax for vice president on the fifth. Although Grant had preferred to remain in the army, he accepted the Republican nomination, believing that he was the only one who could unify the nation. The Republicans advocated \"equal civil and political rights to all\" and African American enfranchisement. The Democrats, having abandoned Johnson, nominated former governor Horatio Seymour of New York for president and Francis Preston Blair Jr. of Missouri for vice president. The Democrats opposed suffrage for African Americans and advocated the immediate restoration of former Confederate states to the Union and amnesty from \"all past political offenses\".\nGrant played no overt role during the campaign and was joined by Sherman and Sheridan in a tour of the West that summer. However, the Republicans adopted his words \"Let us have peace\" as their campaign slogan. Grant's 1862 General Order No. 11 became an issue during the presidential campaign; he sought to distance himself from the order, saying \"I have no prejudice against sect or race, but want each individual to be judged by his own merit.\" The Democrats and their Klan supporters focused mainly on ending Reconstruction, intimidating black people and Republicans, and returning control of the South to the white Democrats and the planter class, alienating War Democrats in the North. Grant won the popular vote and an Electoral College landslide of 214 votes to Seymour's 80. Seymour received a majority of white voters, but Grant was aided by 500,000 votes cast by black people, winning him 52.7 percent of the popular vote. He lost Louisiana and Georgia, primarily due to Ku Klux Klan violence against African-American voters. At the age of 46, Grant was the youngest president yet elected.\nPresidency (1869\u20131877).\nOn March 4, 1869, Grant was sworn in as president by Chief Justice Salmon P. Chase. In his inaugural address, Grant urged the ratification of the Fifteenth Amendment; many African Americans attended his inauguration. He urged that bonds issued during the Civil War should be paid in gold, called for \"proper treatment\" of Native Americans and encouraged their \"civilization and ultimate citizenship\".\nGrant's sparked both criticism and approval. He appointed Elihu B. Washburne Secretary of State and John A. Rawlins Secretary of War. Washburne resigned, and Grant appointed him Minister to France. Grant then appointed former New York Senator Hamilton Fish Secretary of State. Rawlins died in office, and Grant appointed William W. Belknap Secretary of War. Grant appointed New York businessman Alexander T. Stewart Secretary of the Treasury, but Stewart was found legally ineligible by a 1789 law. Grant then appointed Massachusetts Representative George S. Boutwell Secretary of the Treasury. Philadelphia businessman Adolph E. Borie was appointed Secretary of the Navy, but found the job stressful and resigned. Grant then appointed New Jersey's attorney general, George M. Robeson, Secretary of the Navy. Former Ohio Governor Jacob D. Cox (Interior), former Maryland Senator John Creswell (Postmaster-General), and Ebenezer Rockwood Hoar (Attorney General) rounded out the cabinet.\nGrant nominated Sherman to succeed him as general-in-chief and gave him control over war bureau chiefs. When Rawlins took over the War Department he complained that Sherman was given too much authority. Grant reluctantly revoked his order, upsetting Sherman and damaging their friendship. James Longstreet, a former Confederate general, was nominated for Surveyor of Customs of New Orleans; this was met with amazement, and seen as a genuine effort to unite the North and South. In March 1872, Grant signed legislation that established Yellowstone National Park, the first national park. Grant was sympathetic to women's rights, including suffrage, saying he wanted \"equal rights to all citizens\".\nTo make up for his infamous General Order No. 11, Grant appointed more than fifty Jewish people to federal office, including consuls, district attorneys, and deputy postmasters. He appointed Edward S. Salomon territorial governor of Washington, the first time an American Jewish man occupied a governor's seat. In November 1869, reports surfaced of Alexander II of Russia penalizing 2,000 Jewish families for smuggling by expelling them to the interior of the country. In response, Grant publicly supported the Jewish American \"B'nai B'rith\" petition against Alexander. In 1875, Grant proposed a constitutional amendment that limited religious indoctrination in public schools. Schools would be for all children \"irrespective of sex, color, birthplace, or religions\". Grant's views were incorporated into the Blaine Amendment, but it was defeated by the Senate.\nIn October 1871, under the Morrill Act, using federal marshals, Grant prosecuted hundreds of Utah Territory Mormon polygamists. Grant called polygamy a \"crime against decency and morality\". In 1874, Grant signed into law the Poland Act, which made Mormon polygamists subject to trial in District Courts and limited Mormons on juries.\nBeginning in March 1873, under the Comstock Act, Grant prosecuted pornographers, in addition to abortionists. To administer the prosecutions, Grant put in charge a vigorous anti-vice activist and reformer, Anthony Comstock. Comstock headed a federal commission and was empowered to destroy obscene material and hand out arrest warrants to offenders.\nReconstruction.\nGrant was considered an effective civil rights president, concerned about the plight of African Americans. On March 18, 1869, Grant signed into law equal rights for black people, to serve on juries and hold office, in Washington D.C., and in 1870 he signed the Naturalization Act that gave foreign black people citizenship. During his first term, Reconstruction took precedence. Republicans controlled most Southern states, propped up by Republican-controlled Congress, northern money, and southern military occupation. Grant advocated the ratification of the Fifteenth Amendment that said states could not disenfranchise African Americans. Within a year, the three remaining states\u2014Mississippi, Virginia, and Texas\u2014adopted the new amendment\u2014and were admitted to Congress. Grant put military pressure on Georgia to reinstate its black legislators and adopt the amendment. Georgia complied, and on February 24, 1871, its senators were seated in Congress. With all former Confederate states represented, the Union was completely restored under Grant. Under Grant, for the first time in history, Black-American men served in the United States Congress, all from the Southern states.\nIn 1870, to enforce Reconstruction, Congress and Grant created the Justice Department that allowed the Attorney General and the new Solicitor General to prosecute the Klan. Congress and Grant passed three Enforcement Acts, designed to protect black people and Reconstruction governments. Using the Enforcement Acts, Grant crushed the Klan. By October, Grant suspended \"habeas corpus\" in part of South Carolina and sent federal troops to help marshals, who initiated prosecutions. Grant's Attorney General, Amos T. Akerman, who replaced Hoar, was zealous to destroy the Klan. Akerman and South Carolina's U.S. marshal arrested over 470 Klan members, while hundreds of Klansmen fled the state. By 1872, the Klan's power had collapsed and African Americans voted in record numbers in the South. Attorney General George H. Williams, Akerman's replacement, suspended prosecutions of the Klan in 1873, but prior to the election of 1874, changed course and prosecuted the Klan.\nDuring Grant's second term, the North retreated from Reconstruction, while southern conservatives called \"Redeemers\" formed armed groups, the Red Shirts and the White League, who openly used violence, intimidation, voter fraud, and racist appeals to overturn Republican rule. Northern apathy toward black people, the depressed economy and Grant's scandals made it politically difficult for the administration to maintain support for Reconstruction. Power shifted when the House was taken over by Democrats in the 1874 election. Grant ended the Brooks\u2013Baxter War, bringing Reconstruction in Arkansas to a peaceful conclusion. He sent troops to New Orleans in the wake of the Colfax massacre and disputes over the election of Governor William Pitt Kellogg.\nBy 1875, Redeemer Democrats had taken control of all but three Southern states. As violence against black Southerners escalated, Grant's Attorney General Edwards Pierrepont told Republican Governor Adelbert Ames of Mississippi that the people were \"tired of the autumnal outbreaks in the South\", and declined to intervene directly. Grant later regretted not issuing a proclamation to help Ames, having been told Republicans in Ohio would bolt the party if he did. Grant told Congress in January 1875 he could not \"see with indifference Union men or Republicans ostracized, persecuted, and murdered.\" Congress refused to strengthen the laws against violence but instead passed the sweeping Civil Rights Act of 1875 to guarantee black people access to public facilities. However, there was little enforcement and the Supreme Court ruled the law unconstitutional in 1883. In 1876, Grant dispatched troops to South Carolina to keep Republican Governor Daniel Henry Chamberlain in office. After Grant left office, the Compromise of 1877 meant Republicans obtained the White House for Rutherford B. Hayes in return for ending enforcement of racial equality for black people and removing federal troops from the South, marking the end of Reconstruction.\nFinancial affairs.\nSoon after taking office, Grant took conservative steps to return the economy to pre-war monetary standards. During the War, Congress had authorized the Treasury to issue banknotes that, unlike the rest of the currency, were not backed by gold or silver. These \"greenbacks\" were necessary to pay the war debts, but caused inflation and forced gold-backed money out of circulation. On March 18, 1869, Grant signed the Public Credit Act of 1869, which guaranteed bondholders would be repaid in \"coin or its equivalent\". The act committed the government to the full return of the gold standard within ten years. This followed a policy of \"hard currency, economy and gradual reduction of the national debt.\" Grant's own ideas about the economy were simple, and he relied on the advice of businessmen.\nGold corner conspiracy.\nIn April 1869, railroad tycoons Jay Gould and Jim Fisk conspired to corner the gold market in New York. They controlled the Erie Railroad, and a high gold price would allow foreign agriculture buyers to purchase exported crops, shipped east over the Erie's routes. Boutwell's policy of selling gold from the Treasury biweekly, however, kept gold artificially low. Unable to corrupt Boutwell, the schemers built a relationship with Grant's brother-in-law, Abel Corbin, and gained access to Grant. Gould bribed Assistant Treasurer Daniel Butterfield to gain inside information into the Treasury.\nIn July, Grant reduced the sale of Treasury gold to $2,000,000 per month. Fisk told Grant his gold selling policy would destroy the nation. By September, Grant, who was naive regarding finance, was convinced a low gold price would help farmers, and the sale of gold for September was not decreased. On September 23, when the gold price reached &lt;templatestyles src=\"Fraction/styles.css\" /&gt;143+1\u20448, Boutwell rushed to the White House and talked with Grant. On September 24, known as Black Friday, Grant ordered Boutwell to sell, whereupon Boutwell wired Butterfield to sell $4,000,000 in gold. The bull market at Gould's Gold Room collapsed, the price plummeted from 160 to &lt;templatestyles src=\"Fraction/styles.css\" /&gt;133+1\u20443, a bear market panic ensued, Gould and Fisk fled, and economic damages lasted months. By January 1870, the economy resumed its post-war recovery.\nForeign affairs.\nGrant had limited foreign policy experience, so relied heavily on his talented Secretary of State Hamilton Fish. Grant and Fish had cordial friendship. Besides Grant, the main players in foreign affairs were Fish and the chairman of the Senate Foreign Relations Committee Charles Sumner. Sumner, who hated Grant, led the opposition to Grant's plan to annex Santo Domingo, despite fully supporting annexation of Alaska.\nGrant had an expansionist impulse to protect American interests abroad and was a strong advocate of the Monroe Doctrine. For instance, when Tom\u00e1s Fr\u00edas became President of Bolivia in 1872, Grant stressed the importance of maintaining good relations between the U.S. and Bolivia. He had an idealist side to his foreign policy. For instance, Grant appointed a Jewish lawyer, Benjamin F. Peixotto, U.S. Consul in Bucharest, in response to the Romanian persecution of Jews. Grant said that respect \"for human rights is the first duty for those set as rulers\" over the nations.\nTreaty of Washington (1871).\nThe most pressing diplomatic problem in 1869 was the settlement of the \"Alabama Claims\", depredations caused to Union merchant ships by the Confederate warship , built in a British shipyard in violation of neutrality rules. Fish played the central role in formulating and implementing the Treaty of Washington and the Geneva arbitration (1872). Senator Charles Sumner led the demand for reparations, with talk of British Columbia as payment. Sumner, among other politicians, argued that British complicity in arms delivery to the Confederacy via blockade runners prolonged the war. Fish and Treasurer George Boutwell convinced Grant that peaceful relations with Britain were essential, and the two nations agreed to negotiate.\nTo avoid jeopardizing negotiations, Grant refrained from recognizing Cuban rebels who were fighting for independence from Spain, which would have been inconsistent with American objections to the British granting belligerent status to Confederates. A commission in Washington produced a treaty whereby an international tribunal would settle the damage amounts; the British admitted regret, but not fault. The Senate, including Grant critics Sumner and Carl Schurz, approved the Treaty of Washington, which settled disputes over fishing rights and maritime boundaries. The \"Alabama\" Claims settlement was Grant's most successful foreign policy achievement, securing peace with Great Britain. The settlement ($15,500,000) of the \"Alabama\" claims resolved troubled Anglo-American issues and turned Britain into America's strongest ally.\nKorean expedition (1871).\nIn 1871, a U.S. expedition was sent to Korea to open up trade with a country which had a policy that excluded trading with foreign powers, and to learn the fate of U.S. merchant ship SS \"General Sherman\", which had disappeared up the Taedong River in 1866. Grant dispatched a land and naval force consisting of five warships and over 1,200 men, under Admiral John Rodgers, to support a diplomatic delegation, led by U.S. ambassador to China, Frederick Low, sent to negotiate trade and political relations.\nOn June 1, the American ships entered the Ganghwa Straits on the Han River and, as foreign ships were barred from entering the river, onshore Korean garrisons fired upon the ships, but little damage was done. When Rodgers demanded an apology and to begin treaty negotiations, the Korean government refused. On June 10, Rodgers destroyed several Korean forts, culminating in the Battle of Ganghwa, at which 250 Koreans were killed with a loss of 3 Americans. The expedition failed to open up trade and merely strengthened Korea's isolationist policy.\nSanto Domingo (Dominican Republic).\nIn 1869, Grant initiated his plan to annex the Dominican Republic, then called Santo Domingo. Grant believed acquisition would increase the United States' natural resources, strengthen U.S. naval protection to enforce the Monroe Doctrine, safeguard against British obstruction of U.S. shipping, protect a future oceanic canal and stop slavery in Cuba and Brazil, while black people in the United States would have a safe haven from \"the crime of Klu Kluxism\".\nJoseph W. Fabens, an American speculator who represented Buenaventura B\u00e1ez, the president of the Dominican Republic, met with Secretary Fish and proposed annexation. On July 17, Grant sent a military aide Orville E. Babcock to evaluate the islands' resources, local conditions, and B\u00e1ez's terms for annexation, but gave him no diplomatic authority. When Babcock returned to Washington with unauthorized annexation treaties, Grant pressured his cabinet to accept them. Grant ordered Fish to draw up formal treaties, sent to B\u00e1ez by Babcock's return to the island nation. The Dominican Republic would be annexed for $1.5\u00a0million and Saman\u00e1 Bay would be lease-purchased for $2\u00a0million. Generals D.B. Sackett and Rufus Ingalls accompanied Babcock. On November 29, President B\u00e1ez signed the treaties. On December 21, the treaties were placed before Grant and his cabinet.\nGrant's plan, however, was obstructed by Senator Charles Sumner. On December 31, Grant met with Sumner at Sumner's home to gain his support for annexation. Grant left confident that Sumner approved, but what Sumner actually said was disputed by various witnesses. Without appealing to the American public, Grant submitted the treaties on January 10, 1870, to the Senate Foreign Relations Committee, chaired by Sumner, for ratification, but Sumner shelved the bills. Prompted by Grant to stop stalling the treaties, Sumner's committee took action but rejected the bills by a 5-to-2 vote. Sumner opposed annexation largely on anti-imperialist grounds, fearing the United States was \"engaged in forcing upon a weak people the sacrifice of their country.\" Sumner sent the treaties for a full Senate vote, while Grant personally lobbied other senators. Despite Grant's efforts, the Senate defeated the treaties.\nGrant was outraged, and on July 1, 1870, he sacked his appointed Minister to Great Britain, John Lothrop Motley, Sumner's friend and ally. In January 1871, Grant signed a joint resolution to send a commission to investigate annexation. He chose three neutral parties, with Frederick Douglass to be secretary of the commission, that gave Grant the moral high ground from Sumner. Although the commission approved its findings, the Senate remained opposed, forcing Grant to abandon further efforts. Seeking retribution, in March 1871, Grant maneuvered to have Sumner deposed from his powerful Senate chairmanship. The stinging controversy over Santo Domingo overshadowed Grant's foreign diplomacy. Critics complained of Grant's reliance on military personnel to implement his policies.\nCuba and \"Virginius\" Affair.\nAmerican policy under Grant was to remain neutral during the Ten Years' War (1868\u201378) in Cuba against Spanish rule. On the recommendation of Fish and Sumner, Grant refused to recognize the rebels, in effect endorsing Spanish colonial rule, while calling for the abolition of slavery in Cuba. This was done to protect American commerce and to keep peace with Spain.\nThis fragile policy was broken in October 1873, when a Spanish cruiser captured a merchant ship, \"Virginius\", flying the U.S. flag, carrying supplies and men to aid the insurrection. Treating them as pirates, Spanish authorities executed 53 prisoners without trial, including eight Americans. American Captain Joseph Frye and his crew were executed and their bodies mutilated. Enraged Americans called for war with Spain. Grant ordered U.S. Navy Squadron warships to converge on Cuba. On November 27, Fish reached a diplomatic resolution in which Spain's president, Emilio Castelar y Ripoll, expressed his regret, surrendered the \"Virginius\" and the surviving captives. Spain paid $80,000 to the families of the executed Americans.\nFree trade with Hawaii.\nIn the face of strong opposition from Democrats, Grant and Fish secured a free trade treaty in 1875 with Hawaii, incorporating its sugar industry into the U.S. economic sphere. To secure the agreement, King Kal\u0101kaua made a 91-day state visit, the first reigning monarch to set foot in the United States. Despite opposition from Southern Democrats, who wanted to protect American rice and sugar producers, and Democrats, who believed the treaty to be an island annexation attempt and referred to the Hawaiians as an \"inferior\" race, a bill implementing the treaty passed Congress.\nThe treaty gave free access to the U.S. market for sugar and other products grown in Hawaii from September 1876. The U.S. gained lands in the area known as Pu\u02bbu Loa for what would become known as the Pearl Harbor naval base. The treaty led to large investment by Americans in sugar plantations in Hawaii.\nFederal Indian policy.\nWhen Grant took office in 1869, the nation's more than 250,000 Native Americans were governed by 370 treaties. Grant's faith influenced his \"peace\" policy, believing that the \"Creator\" did not place races of men on earth for the \"stronger\" to destroy the \"weaker\". Grant was mostly an assimilationist, wanting Native Americans to adopt European customs, practices, and language, and accept democratic government, leading to eventual citizenship. At Grant's 1869 Inauguration, Grant said \"I will favor any course towards them which tends to their civilization, Christianization and ultimate citizenship.\" Grant appointed Ely S. Parker, an assimilated Seneca and member of his wartime staff, as the Commissioner of Indian Affairs, the first Native American to serve in this position, surprising many.\nIn April 1869, Grant signed legislation establishing an unpaid Board of Indian Commissioners to reduce corruption and oversee the implementation of his \"Peace\" policy, aimed to replace entrepreneurs serving as Native American agents with missionaries and to protect Native Americans on reservations and educate them in farming.\nIn 1870, a setback in Grant's policy occurred over the Marias Massacre, causing public outrage. In 1871, Grant ended the sovereign tribal treaty system; by law individual Native Americans were deemed wards of the federal government. Grant's policy was undermined by Parker's resignation in 1871, denominational infighting among religious agents, and entrenched economic interests. Nonetheless, Indian wars declined overall during Grant's first term, and on October 1, 1872, Major General Oliver Otis Howard negotiated peace with the Apache leader Cochise. On December 28, 1872, another setback took place when General George Crook and the 5th Cavalry massacred about 75 Yavapai Apache Indians at Skeleton Cave, Arizona.\nOn April 11, 1873, Major General Edward Canby was killed in North California by Modoc leader Kintpuash. Grant ordered restraint. The army captured Kintpuash and his followers, who were convicted of Canby's murder and hanged on October 3, while the remaining Modoc were relocated to the Indian Territory. The beginning of the Indian Wars has been dated to this event.\nIn 1874, the army defeated the Comanche at the Battle of Palo Duro Canyon, forcing them to settle at the Fort Sill reservation in 1875. Grant pocket-vetoed a bill in 1874 protecting bison and instead supported Interior Secretary Columbus Delano, who correctly believed killing bison would force Plains Indians to abandon their nomadic lifestyle. In April 1875, another setback occurred: the U.S. Army massacred 27 Cheyenne Indians in Kansas.\nWith the lure of gold discovered in the Black Hills and the westward force of Manifest Destiny, white settlers trespassed on Sioux protected lands. Red Cloud reluctantly entered negotiations on May 26, 1875, but other Sioux chiefs readied for war. Grant told the Sioux leaders to make \"arrangements to allow white persons to go into the Black Hills\" and that their children would attend schools, speak English, and prepare \"for the life of white men.\"\nOn November 3, 1875, under advice from Sheridan, Grant agreed not to enforce excluding miners from the Black Hills, forcing Native Americans onto the Sioux reservation. Sheridan told Grant that the U.S. Army was undermanned and the territory involved was vast, requiring many soldiers.\nDuring the Great Sioux War that started after Sitting Bull refused to relocate to agency land, warriors led by Crazy Horse massacred George Armstrong Custer and his men at the Battle of the Little Big Horn. Angry white settlers demanded retribution. Grant castigated Custer in the press, saying \"I regard Custer's massacre as a sacrifice of troops, brought on by Custer himself, that was wholly unnecessary.\" In September and October 1876, Grant persuaded the tribes to relinquish the Black Hills. Congress ratified the agreement three days before Grant left office in 1877.\nIn spite of Grant's peace efforts, over 200 battles were fought with Native Americans during his presidency. Grant's peace policy survived Custer's death, even after Grant left office in 1877; Indian policy remained under the Interior Department rather than the War Department. The policy was considered humanitarian for its time but later criticized for disregarding tribal cultures.\nElection of 1872 and second term.\nThe Liberal Republicans\u2014reformers, men who supported low tariffs, and those who opposed Grant's prosecution of the Klan\u2014broke from Grant and the Republican Party. The Liberals disliked Grant's alliance with Senators Simon Cameron and Roscoe Conkling, considered to be spoilsmen politicians.\nIn 1872, the Liberals nominated Horace Greeley, a \"New York Tribune\" editor and enemy of Grant, for president, and Missouri governor B. Gratz Brown, for vice president. The Liberals denounced Grantism, corruption, and inefficiency, and demanded withdrawal of federal troops from the South, literacy tests for black voters, and amnesty for Confederates. The Democrats adopted the Greeley-Brown ticket and the Liberals' party platform. Greeley pushed the themes that the Grant administration was failed and corrupt.\nThe Republicans nominated Grant for reelection, with Senator Henry Wilson of Massachusetts as the vice presidential nominee. The Republicans shrewdly borrowed from the Liberal platform, including \"extended amnesty, lowered tariffs, and embraced civil service reform.\" Grant lowered customs duties, gave amnesty to Confederates, and implemented a civil service merit system, neutralizing the opposition. To placate the burgeoning suffragist movement, the Republican platform said women's rights would be treated with \"respectful consideration.\" Concerning Southern policy, Greeley advocated that local government control be given to white people, while Grant advocated federal protection of black people. Grant was supported by Frederick Douglass, prominent abolitionists, and Indian reformers.\nGrant won reelection easily thanks to federal prosecution of the Klan, a strong economy, debt reduction, and lowered tariffs and taxes. He received 56% of the vote and an Electoral College landslide (286 to 66). Most African Americans in the South voted for Grant, while Democratic opposition remained mostly peaceful. Grant lost in six former slave states that wanted an end to Reconstruction. He proclaimed the victory as a personal vindication, but felt betrayed by the Liberals.\nGrant was sworn in by Salmon P. Chase on March 4, 1873. In his second inaugural address, he focused on what he considered the chief issues: freedom and fairness for all Americans and the benefits of citizenship for freed slaves. Grant concluded his address: \"My efforts in the future will be directed towards the restoration of good feelings between the different sections of our common community\". Wilson died in office on November 22, 1875. With Wilson's loss, Grant relied on Fish's guidance more than ever.\nPanic of 1873 and loss of House.\nGrant signed the Coinage Act of 1873, effectively ending the legal basis for bimetallism. The Coinage Act discontinued the standard silver dollar and established the gold dollar as the monetary standard; because the gold supply did not increase as quickly as the population, the result was deflation. Silverites, who wanted more money in circulation to raise the prices farmers received, denounced the move as the \"Crime of 1873\", claiming deflation made debts more burdensome for farmers.\nEconomic turmoil renewed during Grant's second term. In September 1873, Jay Cooke &amp; Company, a New York brokerage house, collapsed after it failed to sell all the bonds issued by Northern Pacific Railway. Other banks and brokerages that owned railroad stocks and bonds were ruined. Grant, who knew little about finance, traveled to New York to consult leading businessmen on how to resolve the crisis, which became known as the Panic of 1873. Grant believed that, as with the collapse of the Gold Ring in 1869, the panic was merely an economic fluctuation. He instructed the Treasury to buy $10\u00a0million in government bonds, which curbed the panic, but the Long Depression, swept the nation. Eighty-nine of the nation's 364 railroads went bankrupt.\nIn 1874, hoping inflation would stimulate the economy, Congress passed the Ferry Bill. Many farmers and workingmen favored the bill, which would have added $64\u00a0million in greenbacks to circulation, but some Eastern bankers opposed it because it would have weakened the dollar. Belknap, Williams, and Delano told Grant a veto would hurt Republicans in the November elections. Grant believed the bill would destroy the credit of the nation and vetoed it despite their objections. Grant's veto placed him in the Republican conservative faction and began the party's commitment to a gold-backed dollar. Grant later pressured Congress for a bill to strengthen the dollar by gradually reducing the greenbacks in circulation. When the Democrats gained a majority in the House after the 1874 elections, the lame-duck Republican Congress did so before the Democrats took office. On January 14, 1875, Grant signed the Specie Payment Resumption Act, which required reduction of greenbacks allowed to circulate and declared that they would be redeemed for gold beginning on January 1, 1879.\nReforms and scandals.\nThe post-Civil War economy brought on massive industrial wealth and government expansion. Speculation, lifestyle extravagance, and corruption in federal offices were rampant. All of Grant's executive departments were investigated by Congress. Grant by nature was honest, trusting, gullible, and loyal to his friends. His responses to malfeasance were mixed: at times appointing cabinet reformers, at others defending culprits.\nGrant in his first term appointed Secretary of the Interior Jacob D. Cox, who implemented civil service reform, including firing unqualified clerks. On October 3, 1870, Cox resigned after a dispute with Grant over handling of a mining claim. Authorized by Congress on March 3, 1871, Grant created and appointed the first Civil Service Commission. Grant's Commission created rules for competitive exams for appointments, ending mandatory political assessments and classifying positions into grades.\nIn November 1871, Grant's appointed New York Collector, Thomas Murphy, resigned. Grant replaced him with Chester A. Arthur, who implemented Boutwell's reforms. A Senate committee investigated the New York Customs House in 1872. Previous Grant-appointed collectors Murphy and Moses H. Grinnell charged lucrative fees for warehouse space, without the legal requirement of listing the goods. This led to Grant firing warehouse owner George K. Leet, for pocketing the exorbitant freight fees. Boutwell's reforms included stricter record-keeping and that goods be stored on company docks. Grant ordered prosecutions by Attorney General George H. Williams and Secretary of the Treasury Boutwell of persons accepting and paying bribes.\nOn March 3, 1873, Grant signed into law an appropriation act that increased pay for federal employees, Congress (retroactive), the judiciary, and the president. Grant's annual salary doubled to $50,000. Critics derided Congress's two-year retroactive $4,000 payment for each Congressman, and the law was partially repealed. Grant kept his much-needed pay raise, while his reputation remained intact.\nIn 1872, Grant signed into law an act that ended private moiety (\"tax collection\") contracts, but an attached rider allowed three more contracts. Boutwell's assistant secretary William A. Richardson hired John B. Sanborn to go after \"individuals and cooperations\" who allegedly evaded taxes. Sanborn aggressively collected $213,000, while splitting $156,000 to others, including Richardson, and the Republican Party campaign committee. During an 1874 Congressional investigation, Richardson denied involvement, but Sanborn said he met with Richardson over the contracts. Congress severely condemned Richardson's permissive manner. Grant appointed Richardson judge of the Court of Claims, and replaced him with reformer Benjamin Bristow. In June, Grant and Congress abolished the moiety system.\nBristow tightened up the Treasury's investigation force, implemented civil service reform, and fired hundreds of corrupt appointees. Bristow discovered Treasury receipts were low, and launched an investigation that uncovered the notorious Whiskey Ring, that involved collusion between distillers and Treasury officials to evade millions in taxes. In mid-April, Bristow informed Grant of the ring. On May 10, Bristow struck hard and broke the ring. Federal marshals raided 32 installations nationwide, leading to 110 convictions and $3,150,000 in fines.\nGrant appointed David Dyer, under Bristow's recommendation, federal attorney to prosecute the Ring in St. Louis, who indicted Grant's friend General John McDonald, supervisor of Internal Revenue. Grant endorsed Bristow's investigation, writing on a letter \"Let no guilty man escape...\" Bristow's investigation discovered Babcock received kickback payments, and that Babcock had secretly forewarned McDonald, the ring's mastermind, of the investigation. On November 22, the jury convicted McDonald. On December 9, Babcock was indicted; Grant refused to believe in Babcock's guilt and was ready to testify in Babcock's favor, but Fish warned that doing so would put Grant in the embarrassing position of testifying against a case prosecuted by his own administration. Instead, on February 12, 1876, Grant gave a deposition in Babcock's defense, expressing that his confidence in his secretary was \"unshaken\". Grant's testimony silenced all but his strongest critics.\nThe St. Louis jury acquitted Babcock, and Grant allowed him to remain at the White House. However, after Babcock was indicted in a frame-up of a Washington reformer, called the Safe Burglary Conspiracy, Grant dismissed him. Babcock kept his position of Superintendent of Public Buildings in Washington.\nThe Interior Department under Secretary Columbus Delano, whom Grant appointed to replace Cox, was rife with fraud and corruption. The exception was Delano's effective oversight of Yellowstone. Grant reluctantly forced Delano's resignation. Surveyor General Silas Reed had set up corrupt contracts that benefited Delano's son, John Delano. Grant's Secretary of the Interior Zachariah Chandler, who succeeded Delano in 1875, implemented reforms, fired corrupt agents and ended profiteering. When Grant was informed by Postmaster General Marshall Jewell of a potential Congressional investigation into an extortion scandal involving Attorney General George H. Williams's wife, Grant fired Williams and appointed reformer Edwards Pierrepont. Grant's new cabinet appointments temporarily appeased reformers.\nAfter the Democrats took control of the House in 1875, more corruption in federal departments was exposed. Among the most damaging scandal involved Secretary of War William W. Belknap, who took quarterly kickbacks from the Fort Sill tradership; he resigned in February 1876. Belknap was impeached by the House but was acquitted by the Senate. Grant's brother Orvil set up \"silent partnerships\" and received kickbacks from four trading posts. Congress discovered that Secretary of the Navy Robeson had been bribed by a naval contractor, but no articles of impeachment were drawn up. In his December 5, 1876, Annual Message, Grant apologized to the nation: \"Failures have been errors of judgement, not of intent.\"\nElection of 1876.\nThe abandonment of Reconstruction played a central role during the 1876 election. Mounting investigations into corruption by the House, controlled by the Democrats, discredited Grant's presidency. Grant did not run for a third term, while the Republicans chose Governor Rutherford B. Hayes of Ohio, a reformer, at their convention. The Democrats nominated Governor Samuel J. Tilden of New York. Voting irregularities in three Southern states caused the election to remain undecided for several months.\nGrant told Congress to settle the matter through legislation and assured both sides that he would not use the army to force a result, except to curb violence. On January 29, 1877, he signed legislation forming an Electoral Commission, which ruled Hayes elected president; to forestall Democratic protests, Republicans agreed to the Compromise of 1877, in which the last troops were withdrawn from Southern capitals. With Reconstruction dead, 80 years of Jim Crow segregation was launched. Grant's \"calm visage\" throughout the election crisis appeased the nation.\nPost-presidency (1877\u20131885).\nAfter leaving the White House, Grant said he \"was never so happy in my life\". The Grants left Washington for New York, to attend the birth of their daughter Nellie's child. Calling themselves \"waifs\", the Grants toured Cincinnati, St. Louis, Chicago, and Galena, without a clear idea of where they would live.\nWorld tour and diplomacy.\nUsing $25,000 (equivalent to $ in 2024) from liquidating an investment in a Nevada-based mining company, the Grants set out on a world tour for approximately two and a half years. On May 16, Grant and his wife left for England aboard the SS \"Indiana\". During the tour, the Grants made stops in Europe, Africa, India, the Middle East and the Far East, meeting with notable dignitaries such as Queen Victoria, Tsar Alexander II, Pope Leo XIII, Otto von Bismarck, Li Hongzhang, and Emperor Meiji.\nAs a courtesy to Grant by the Hayes administration, his touring party received federal transportation on three U.S. Navy ships: a five-month tour of the Mediterranean on the USS \"Vandalia\", travel from Hong Kong to China on the USS \"Ashuelot\", and from China to Japan on the USS \"Richmond\". The Hayes administration encouraged Grant to assume a public unofficial diplomatic role and strengthen American interests abroad during the tour. Homesick, the Grants left Japan on the SS \"City of Tokio\" and landed in San Francisco on September 20, 1879, greeted by cheering crowds. Grant's tour demonstrated to Europe and Asia that the United States was an emerging world power.\nThird term attempt.\nPolitically conservative, Grant was supported by the Stalwarts who, led by Grant's old political ally Roscoe Conkling, saw Grant's renewed popularity as an opportunity, and sought to nominate him for the presidency in 1880. Opponents called it a violation of the unofficial two-term rule in use since George Washington. Grant said nothing publicly but wanted the job and encouraged his men. Washburne urged him to run; Grant demurred. Even so, Conkling and John A. Logan began to organize delegates in Grant's favor. When the convention convened in Chicago in June, more delegates were pledged to Grant than to any other candidate, but he was still short of a majority vote.\nAt the convention, Conkling nominated Grant with an eloquent speech, the most famous line being \"When asked which state he hails from, our sole reply shall be, he hails from Appomattox and its famous apple tree.\" With 378 votes needed for the nomination, the first ballot had Grant at 304, Blaine at 284, Sherman at 93, and the rest to minor candidates. After thirty-six ballots, Blaine's delegates combined with those of other candidates to nominate a compromise candidate: James A. Garfield. A procedural motion made the vote unanimous for Garfield. Grant gave speeches for Garfield but declined to criticize the Democratic nominee, Winfield Scott Hancock, a general who had served under him. Garfield won the election. Grant gave Garfield his public support and pushed him to include Stalwarts in his administration. On July 2, 1881, Garfield was shot by an assassin and died on September 19. On learning of Garfield's death from a reporter, Grant wept.\nBusiness failures.\nIn the 19th century, there were no federal presidential pensions, and the Grants' personal income was $6,000 a year. Grant's world tour had been costly, and he had depleted most of his savings. Wealthy friends bought him a house on New York City's Upper East Side, and to make an income, Grant, Jay Gould, and former Mexican Finance Secretary Mat\u00edas Romero chartered the Mexican Southern Railroad, with plans to build a railroad from Oaxaca to Mexico City. Grant urged President Chester A. Arthur to negotiate a free trade treaty with Mexico. Arthur and the Mexican government agreed, but the United States Senate rejected the treaty in 1883. The railroad was similarly unsuccessful, falling into bankruptcy the following year.\nAt the same time, Grant's son Buck had opened a Wall Street brokerage house with Ferdinand Ward. A conniving man who swindled numerous wealthy men, Ward was at the time regarded as a rising star on Wall Street. The firm, Grant &amp; Ward, was initially successful. In 1883, Grant joined the firm and invested $100,000 (~$ in 2024) of his own money. Ward paid investors abnormally high interest by pledging the company's securities on multiple loans in a process called rehypothecation (now regarded as a Ponzi scheme). Ward, in collusion with banker James D. Fish and kept secret from bank examiners, retrieved the firm's securities from the company's bank vault. When the trades went bad, multiple loans came due, all backed by the same collateral.\nHistorians agree that the elder Grant was likely unaware of Ward's intentions, but it is unclear how much Buck Grant knew. In May 1884, enough investments went bad to convince Ward that the firm would soon be bankrupt. Ward, who assumed Grant was \"a child in business matters\", told him of the impending failure, but assured Grant that this was a temporary shortfall. Grant approached businessman William Henry Vanderbilt, who gave him a personal loan of $150,000. Grant invested the money in the firm, but it was not enough to save it. The fall of Grant &amp; Ward set off the Panic of 1884.\nVanderbilt offered to forgive Grant's debt entirely, but Grant refused. Impoverished but compelled by personal honor, he repaid what he could with his Civil War mementos and the sale or transfer of all other assets. Vanderbilt took the title to Grant's home, although he allowed the Grants to continue to reside there, and pledged to donate the souvenirs to the federal government and insisted the debt had been paid in full. Grant was distraught over Ward's deception and asked privately how he could ever \"trust any human being again.\" In March 1885, he testified against both Ward and Fish. After the collapse of Grant &amp; Ward, there was an outpouring of sympathy for Grant.\nMemoirs, military pension, illness and death.\nGrant attended a service for Civil War veterans in Ocean Grove, New Jersey, on August 4, 1884, receiving a standing ovation from the ten thousand attendees; it would be his last public appearance. In the summer of 1884, Grant complained of a sore throat but put off seeing a doctor until late October, when he learned it was cancer, possibly caused by his frequent cigar smoking. Grant chose not to reveal the seriousness of his condition to his wife, who soon found out from Grant's doctor. In March 1885, \"The New York Times\" announced that Grant was dying of cancer, causing nationwide public concern. Knowing of Grant and Julia's financial difficulties, Congress restored him to the rank of General of the Army with full retirement pay\u2014Grant's assumption of the presidency had required that he resign his commission and forfeit his (and his widow's) pension.\nGrant was nearly penniless and worried about leaving his wife money to live on. He approached \"The Century Magazine\" and wrote a number of articles on his Civil War campaigns for $500 () each. The articles were well received by critics, and the editor, Robert Underwood Johnson, suggested that Grant write a memoir, as Sherman and others had done. The magazine offered him a book contract with a 10% royalty. However, Grant's friend Mark Twain, one of the few who understood Grant's precarious financial condition, offered him an unheard-of 70% royalty. To provide for his family, Grant worked intensely on his memoirs in New York City. His former staff member Adam Badeau assisted with the research, while his son Frederick located documents and did much of the fact-checking. Because of the summer heat and humidity, his doctors recommended that he move upstate to a cottage at the top of Mount McGregor, offered by a family friend.\nOn July 18, 1885, Grant finished his memoir, which includes the events of his life to the end of the Civil War. The \"Personal Memoirs of U. S. Grant\" was a critical and commercial success. Julia Grant eventually received about $450,000 in royalties (). The memoir has been highly regarded by the public, military historians, and literary critics. Grant portrayed himself as an honorable Western hero, whose strength lies in his honesty. He candidly depicted his battles against both the Confederates and internal army foes.\nGrant died in the Mount McGregor cottage on July 23, 1885. Sheridan, then Commanding General of the Army, ordered a day-long tribute to Grant on all military posts, and President Grover Cleveland ordered a thirty-day nationwide period of mourning. After private services, the honor guard placed Grant's body on a funeral train, which traveled to West Point and New York City. A quarter of a million people viewed it in the two days before the funeral. Tens of thousands of men, many of them veterans from the Grand Army of the Republic (GAR), marched with Grant's casket drawn by two dozen black stallions to Riverside Park in Morningside Heights, Manhattan. His pallbearers included Union generals Sherman and Sheridan, Confederate generals Simon Bolivar Buckner and Joseph E. Johnston, Admiral David Dixon Porter, and Senator John A. Logan, the head of the GAR. Following the casket in the procession were President Cleveland, two former living presidents Hayes and Arthur, all of the president's cabinet, and justices of the Supreme Court.\nAttendance at the New York funeral topped 1.5\u00a0million. Ceremonies were held in other major cities around the country, while Grant was eulogized in the press. Grant's body was laid to rest in Riverside Park, first in a temporary tomb, and then on April 17, 1897, in the General Grant National Memorial, known as \"Grant's Tomb\", the largest mausoleum in North America.\nHistorical reputation.\nGrant was hailed across the North as the general who \"saved the Union\", and overall, his military reputation has held up well. Achieving great national fame for his victories at Vicksburg and the surrender at Appomattox, Grant was the most successful general, Union or Confederate, in the American Civil War. He was criticized by the South for using excessive force, and his drinking was often exaggerated by the press and stereotyped by rivals and critics. Historians also debate how effective Grant was at halting corruption. The scandals during his administration stigmatized his political reputation. Despite his administration's scandals, Grant was still respected by most of the nation at the time of his death, as can be indicated by the praise from Democratic president Cleveland and even some former Confederate generals, two of whom had served as his pallbearers.\nHowever, Grant's reputation would decline soon after his death. During the late 19th and early 20th centuries, Grant's reputation was damaged by the \"Lost Cause\" movement and the Dunning School. Grant's reputation particularly fell in the late 1910s and early 1920s because the U.S. deaths in World War I brought back memories of Union deaths in Virginia in 1864, and the scandals of the Warren Harding administration brought back memories of the Grant administration's scandals. Views of Grant reached new lows as he was seen as an unsuccessful president and an unskilled, if lucky, general. In the 1950s, some historians reassessed Grant's military career, shifting the analysis of Grant as the victor by brute force to that of a skillful modern strategist and commander. Historian William S. McFeely's biography, \"Grant\" (1981), won the Pulitzer Prize, and brought renewed scholarly interest in Grant. McFeely believed Grant was an \"ordinary American\" trying to \"make his mark\" during the 19th century. In the 21st century, Grant's reputation improved markedly among historians after the publication of \"Grant\" (2001), by historian Jean Edward Smith. Opinions of Grant's presidency demonstrate a better appreciation of Grant's personal integrity, Reconstruction efforts, and peace policy towards Indians, even when they fell short. H. W. Brands's \"The Man Who Saved the Union\" (2012), Ronald C. White's \"American Ulysses\" (2016), and Ron Chernow's \"Grant\" (2017) continued the elevation of Grant's reputation. White said that Grant \"demonstrated a distinctive sense of humility, moral courage, and determination\", and as president he \"stood up for African Americans, especially fighting against voter suppression perpetrated by the Ku Klux Klan\". White believed that Grant was \"an exceptional person and leader\". Historian Robert Farley writes that the \"Cult of Lee\" and the Dunning School's resentment of Grant for his defeat of Lee and his strong enforcement of Reconstruction resulted in Grant's shoddy treatment by historians.\nIn a 2021 C-SPAN survey ranking presidents from worst to best, Grant was ranked 20 out of 44 presidents, up from his previous ranking of 33 in 2017. This was due to the rehabilitation of his image and legacy in more recent years, with Grant now receiving \"more credit for Reconstruction and his diplomacy than condemnation for his alleged corruption.\"\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nBibliography.\nBiographical.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;\nMilitary and politics.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;\nHistoriography.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;\nFurther reading.\nArticles.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;\nBooks.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;\nExternal links.\nrole=\"presentation\" class=\"wikitable succession-box noprint\" style=\"margin:0.5em auto; font-size:small;clear:both;\""}
{"id": "31753", "revid": "36767729", "url": "https://en.wikipedia.org/wiki?curid=31753", "title": "Ulysses S. Grant/First Inaugural Address", "text": ""}
{"id": "31754", "revid": "36767729", "url": "https://en.wikipedia.org/wiki?curid=31754", "title": "Ulysses S. Grant/Second Inaugural Address", "text": ""}
{"id": "31755", "revid": "194203", "url": "https://en.wikipedia.org/wiki?curid=31755", "title": "Unclassified game", "text": ""}
{"id": "31756", "revid": "48523215", "url": "https://en.wikipedia.org/wiki?curid=31756", "title": "United States Congress", "text": "Bicameral legislature of the United States\nThe United States Congress is the legislative branch of the federal government of the United States. It is a bicameral legislature, including a lower body, the U.S. House of Representatives, and an upper body, the U.S. Senate. They both meet in the United States Capitol in Washington, D.C. \nMembers of Congress are chosen through direct election, though vacancies in the Senate may be filled by a governor's appointment. Congress has a total of 535 voting members, a figure which includes 100 senators and 435 representatives; the House of Representatives has 6 additional non-voting members. The vice president of the United States, as president of the Senate, has a vote in the Senate only when there is a tie.\nCongress convenes for a two-year term (a Congress), commencing every other January. Each Congress is usually split into two sessions, one for each year. Elections are held every even-numbered year on Election Day. The members of the House of Representatives are elected for the two-year term of a Congress. The Reapportionment Act of 1929 established that there be 435 representatives, and the Uniform Congressional District Act requires that they be elected from single-member constituencies or \"districts\". It is also required that the congressional districts be apportioned among states by population every ten years using the U.S. census results, provided that each state has at least one congressional representative. Each senator is elected at-large in their state for a six-year term, with terms staggered, so every two years approximately one-third of the Senate is up for election. Each state, regardless of population or size, has two senators, so currently, there are 100 senators for the 50 states.\nArticle One of the U.S. Constitution requires that members of Congress be at least 25 years old for the House and at least 30 years old for the U.S. Senate, be a U.S. citizen for seven years for the House and nine years for the Senate, and be an inhabitant of the state which they represent. Members in both chambers may stand for re-election an unlimited number of times.\nCongress was created by the U.S. Constitution's First Article and first met in 1789, replacing the Congress of the Confederation in its legislative function. Although not legally mandated, in practice members of Congress since the late 19th century are typically affiliated with one of the two major parties, the Democratic Party or the Republican Party, and only rarely with a third party or independents affiliated with no party. Members can also switch parties at any time, though this is uncommon.\nOverview.\nArticle One of the United States Constitution states, \"All legislative Powers herein granted shall be vested in a Congress of the United States, which shall consist of a Senate and House of Representatives.\" The House and Senate are equal partners in the legislative process\u00a0\u2013 legislation cannot be enacted without the consent of both chambers. The Constitution grants each chamber some unique powers. The Senate ratifies treaties and approves presidential appointments while the House initiates revenue-raising bills.\nThe House initiates and decides impeachment while the Senate votes on conviction and removal of office for impeachment cases. A two-thirds vote of the Senate is required before an impeached person can be removed from office.\nThe term \"Congress\" can also refer to a particular meeting of the legislature. A Congress covers two years; the current one, the 119th Congress, began on January 3, 2025, and will end on January 3, 2027. Since the adoption of the Twentieth Amendment to the United States Constitution, the Congress has started and ended at noon on the third day of January of every odd-numbered year. Members of the Senate are referred to as senators, while members of the House of Representatives are commonly referred to as representatives, congressmen, or congresswomen.\nScholar and representative Lee H. Hamilton asserted that the \"historic mission of Congress has been to maintain freedom\" and insisted it was a \"driving force in American government\" and a \"remarkably resilient institution\". Congress is the \"heart and soul of our democracy\", according to this view, even though legislators rarely achieve the prestige or name recognition of presidents or Supreme Court justices; one wrote that \"legislators remain ghosts in America's historical imagination.\" One analyst argues that it is not a solely reactive institution but has played an active role in shaping government policy and is extraordinarily sensitive to public pressure. Several academics described Congress:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;Congress reflects us in all our strengths and all our weaknesses. It reflects our regional idiosyncrasies, our ethnic, religious, and racial diversity, our multitude of professions, and our shadings of opinion on everything from the value of war to the war over values. Congress is the government's most representative body... Congress is essentially charged with reconciling our many points of view on the great public policy issues of the day.\nCongress is constantly changing and is constantly in flux. In recent times, the American South and West have gained House seats according to demographic changes recorded by the census and includes more women and minorities. While power balances among the different parts of government continue to change, the internal structure of Congress is important to understand along with its interactions with so-called \"intermediary institutions\" such as political parties, civic associations, interest groups, and the mass media.\nThe Congress of the United States serves two distinct purposes that overlap: local representation to the federal government of a congressional district by representatives and a state's at-large representation to the federal government by senators.\nMost incumbents seek re-election, and their historical likelihood of winning subsequent elections exceeds 90 percent.\nThe historical records of the House of Representatives and the Senate are maintained by the Center for Legislative Archives, which is a part of the National Archives and Records Administration.\nCongress is directly responsible for the governing of the District of Columbia, the current seat of the federal government.\nHistory.\n18th century.\nThe First Continental Congress was a gathering of representatives from twelve of the Thirteen Colonies. On July 4, 1776, the Second Continental Congress adopted the Declaration of Independence, referring to the new nation as the \"United States of America\". The Articles of Confederation in 1781 created the Congress of the Confederation, a unicameral body with equal representation among the states in which each state had a veto over most decisions. Congress had executive but not legislative authority, and the federal judiciary was confined to admiralty and lacked authority to collect taxes, regulate commerce, or enforce laws.\nGovernment powerlessness led to the Convention of 1787 which proposed a revised constitution with a two-chamber or \"bicameral\" Congress. Smaller states argued for equal representation for each state. The two-chamber structure had functioned well in state governments. A compromise plan, the Connecticut Compromise, was adopted with representatives chosen by population (benefiting larger states) and exactly two senators chosen by state governments (benefiting smaller states). The ratified constitution created a federal structure with two overlapping power centers so that each citizen \"as an individual\" is subject to the powers of state government and national government. To protect against abuse of power, each branch of government\u00a0\u2013 executive, legislative, and judicial\u00a0\u2013 had a separate sphere of authority and could check other branches according to the principle of the separation of powers. Furthermore, there were checks and balances \"within\" the legislature since there were two separate chambers. The new government became active in 1789.\nPolitical scientist Julian E. Zelizer suggested there were four main congressional eras, with considerable overlap, and included the \"formative era\" (1780s\u20131820s), the \"partisan era\" (1830s\u20131900s), the \"committee era\" (1910s\u20131960s), and the \"contemporary era\" (1970\u2013present).\nFederalists and anti-federalists jostled for power in the early years as political parties became pronounced. With the passage of the Constitution and the Bill of Rights, the anti-federalist movement was exhausted. Some activists joined the Anti-Administration Party that James Madison and Thomas Jefferson were forming about 1790\u20131791 to oppose policies of Treasury Secretary Alexander Hamilton; it soon became the Democratic-Republican Party or the Jeffersonian Republican Party and thus began the era of the First Party System.\n19th century.\nIn 1800, Thomas Jefferson's election to the presidency marked a peaceful transition of power between the parties. John Marshall, 4th chief justice of the Supreme Court, empowered the courts by establishing the principle of judicial review in law in the landmark case \"Marbury v. Madison\" in 1803, effectively giving the Supreme Court a power to nullify congressional legislation.\nThe Civil War, which lasted from 1861 to 1865, resolved the slavery issue and unified the nation under federal authority but weakened the power of states' rights. The Gilded Age (1877\u20131901) was marked by Republican dominance of Congress. During this time, lobbying activity became more intense, particularly during the administration of President Ulysses S. Grant in which influential lobbies advocated for railroad subsidies and tariffs on wool. Immigration and high birth rates swelled the ranks of citizens and the nation grew at a rapid pace. The Progressive Era was characterized by strong party leadership in both houses of Congress and calls for reform; sometimes reformers said lobbyists corrupted politics. The position of Speaker of the House became extremely powerful under leaders such as Thomas Reed in 1890 and Joseph Gurney Cannon.\n20th century.\nBy the beginning of the 20th century, party structures and leadership emerged as key organizers of Senate proceedings.\nA system of seniority, in which long-time members of Congress gained more and more power, encouraged politicians of both parties to seek long terms. Committee chairmen remained influential in both houses until the reforms of the 1970s.\nImportant structural changes included the direct popular election of senators according to the Seventeenth Amendment, ratified on April 8, 1913. Supreme Court decisions based on the Constitution's commerce clause expanded congressional power to regulate the economy. One effect of popular election of senators was to reduce the difference between the House and Senate in terms of their link to the electorate. Lame duck reforms according to the Twentieth Amendment reduced the power of defeated and retiring members of Congress to wield influence despite their lack of accountability.\nThe Great Depression ushered in President Franklin Roosevelt and strong control by Democrats and historic New Deal policies. Roosevelt's election in 1932 marked a shift in government power towards the executive branch. Numerous New Deal initiatives came from the White House rather initiated by Congress. President Roosevelt pushed his agenda in Congress by detailing Executive Branch staff to friendly Senate committees (a practice that ended with the Legislative Reorganization Act of 1946). The Democratic Party controlled both houses of Congress for many years. During this time, Republicans and conservative southern Democrats formed the Conservative Coalition. Democrats maintained control of Congress during World War II. Congress struggled with efficiency in the postwar era partly by reducing the number of standing congressional committees. Southern Democrats became a powerful force in many influential committees although political power alternated between Republicans and Democrats during these years. More complex issues required greater specialization and expertise, such as space flight and atomic energy policy. Senator Joseph McCarthy exploited the fear of communism during the Second Red Scare and conducted televised hearings. In 1960, Democratic candidate John F. Kennedy narrowly won the presidency and power shifted again to the Democrats who dominated both chambers of Congress from 1961 to 1980, and retained a consistent majority in the House from 1955 to 1994.\nCongress enacted Johnson's Great Society program to fight poverty and hunger. The Watergate Scandal had a powerful effect of waking up a somewhat dormant Congress which investigated presidential wrongdoing and coverups; the scandal \"substantially reshaped\" relations between the branches of government, suggested political scientist Bruce J. Schulman. Partisanship returned, particularly after 1994; one analyst attributes partisan infighting to slim congressional majorities which discouraged friendly social gatherings in meeting rooms such as the \"Board of Education\". Congress began reasserting its authority. Lobbying became a big factor despite the 1971 Federal Election Campaign Act. Political action committees or PACs could make substantive donations to congressional candidates via such means as \"soft money\" contributions. While soft money funds were not given to specific campaigns for candidates, the money often benefited candidates substantially in an indirect way and helped reelect candidates. Reforms such as the 2002 Bipartisan Campaign Reform Act limited campaign donations but did not limit \"soft money\" contributions. One source suggests post-Watergate laws amended in 1974 meant to reduce the \"influence of wealthy contributors and end payoffs\" instead \"legitimized PACs\" since they \"enabled individuals to band together in support of candidates\".\nFrom 1974 to 1984, PACs grew from 608 to 3,803 and donations leaped from $12.5million to $120million along with concern over PAC influence in Congress. In 2009, there were 4,600 business, labor and special-interest PACs including ones for lawyers, electricians, and real estate brokers. From 2007 to 2008, 175 members of Congress received \"half or more of their campaign cash\" from PACs.\nFrom 1970 to 2009, the House expanded delegates, along with their powers and privileges representing U.S. citizens in non-state areas, beginning with representation on committees for Puerto Rico's resident commissioner in 1970. In 1971, a delegate for the District of Columbia was authorized, and in 1972 new delegate positions were established for U.S. Virgin Islands and Guam. In 1978, an additional delegate for American Samoa were added.\nIn the late 20th century, the media became more important in Congress's work. Analyst Michael Schudson suggested that greater publicity undermined the power of political parties and caused \"more roads to open up in Congress for individual representatives to influence decisions\". Norman Ornstein suggested that media prominence led to a greater emphasis on the negative and sensational side of Congress, and referred to this as the \"tabloidization\" of media coverage. Others saw pressure to squeeze a political position into a thirty-second soundbite. A report characterized Congress in 2013 as unproductive, gridlocked, and \"setting records for futility\". In October 2013, with Congress unable to compromise, the government was shut down for several weeks and risked a serious default on debt payments, causing 60% of the public to say they would \"fire every member of Congress\" including their own representative. One report suggested Congress posed the \"biggest risk to the U.S. economy\" because of its brinksmanship, \"down-to-the-wire budget and debt crises\" and \"indiscriminate spending cuts\", resulting in slowed economic activity and keeping up to two million people unemployed. There has been increasing public dissatisfaction with Congress, with extremely low approval ratings which dropped to 5% in October 2013.\n21st century.\nIn 2009, Congress authorized another delegate for the Northern Mariana Islands. These six members of Congress enjoy floor privileges to introduce bills and resolutions, and in recent Congresses they vote in permanent and select committees, in party caucuses and in joint conferences with the Senate. They have Capitol Hill offices, staff and two annual appointments to each of the four military academies. While their votes are constitutional when Congress authorizes their House Committee of the Whole votes, recent Congresses have not allowed for that, and they cannot vote when the House is meeting as the House of Representatives.\nOn January 6, 2021, Congress gathered to confirm the election of Joe Biden, when supporters of the outgoing president Donald Trump attacked the building. The session of Congress ended prematurely, and Congress representatives evacuated. Trump supporters occupied Congress until D.C police evacuated the area. The event was the first time since the Burning of Washington by the British during the War of 1812 that the United States Congress was forcefully occupied.\nWomen in Congress.\nVarious social and structural barriers have prevented women from gaining seats in Congress. In the early 20th century, women's domestic roles and the inability to vote forestalled opportunities to run for and hold public office. The two party system and the lack of term limits favored incumbent white men, making the widow's succession \u2013 in which a woman temporarily took over a seat vacated by the death of her husband \u2013 the most common path to Congress for white women.\nWomen candidates began making substantial inroads in the later 20th century, due in part to new political support mechanisms and public awareness of their underrepresentation in Congress. Recruitment and financial support for women candidates were rare until the second-wave feminism movement, when activists moved into electoral politics. Beginning in the 1970s, donors and political action committees like EMILY's List began recruiting, training and funding women candidates. Watershed political moments like the confirmation of Clarence Thomas and the 2016 presidential election created momentum for women candidates, resulting in the Year of the Woman and the election of members of The Squad, respectively.\nWomen of color faced additional challenges that made their ascension to Congress even more difficult. Jim Crow laws, voter suppression and other forms of structural racism made it virtually impossible for women of color to reach Congress prior to 1965. The passage of the Voting Rights Act that year, and the elimination of race-based immigration laws in the 1960s opened the possibility for Black, Asian American, Latina and other non-white women candidates to run for Congress.\nRacially polarized voting, racial stereotypes and lack of institutional support still prevent women of color from reaching Congress as easily as white people. Senate elections, which require victories in statewide electorates, have been particularly difficult for women of color. Carol Moseley Braun became the first woman of color to reach the Senate in 1993. The second, Mazie Hirono, won in 2013.\nIn 2021, Kamala Harris became the first female President of the Senate, which came with her role as the first female Vice President of the United States.\nRole.\nPowers.\nOverview.\nArticle One of the Constitution creates and sets forth the structure and most of the powers of Congress. Sections One through Six describe how Congress is elected and gives each House the power to create its own structure. Section Seven lays out the process for creating laws, and Section Eight enumerates numerous powers. Section Nine is a list of powers Congress does not have, and Section Ten enumerates powers of the state, some of which may only be granted by Congress. Constitutional amendments have granted Congress additional powers. Congress also has implied powers derived from the Constitution's Necessary and Proper Clause.\nCongress has authority over financial and budgetary policy through the enumerated power to \"lay and collect Taxes, Duties, Imposts and Excises, to pay the Debts and provide for the common Defence and general Welfare of the United States\". There is vast authority over budgets, although analyst Eric Patashnik suggested that much of Congress's power to manage the budget has been lost when the welfare state expanded since \"entitlements were institutionally detached from Congress's ordinary legislative routine and rhythm.\" Another factor leading to less control over the budget was a Keynesian belief that balanced budgets were unnecessary.\nThe Sixteenth Amendment in 1913 extended congressional power of taxation to include income taxes without apportionment among the several States, and without regard to any census or enumeration. The Constitution also grants Congress the exclusive power to appropriate funds, and this \"power of the purse\" is one of Congress's primary checks on the executive branch. Congress can borrow money on the credit of the United States, regulate commerce with foreign nations and among the states, and coin money. Generally, the Senate and the House of Representatives have equal legislative authority, although only the House may originate revenue and appropriation bills.\nCongress has an important role in national defense, including the exclusive power to declare war, to raise and maintain the armed forces, and to make rules for the military. Some critics charge that the executive branch has usurped Congress's constitutionally defined task of declaring war. While historically presidents initiated the process for going to war, they asked for and received formal war declarations from Congress for the War of 1812, the Mexican\u2013American War, the Spanish\u2013American War, World War I, and World War II, although President Theodore Roosevelt's military move into Panama in 1903 did not get congressional approval. In the early days after the North Korean invasion of 1950, President Truman described the American response as a \"police action\". According to \"Time\" magazine in 1970, \"U.S. presidents [had] ordered troops into position or action without a formal congressional declaration a total of 149 times.\" In 1993, Michael Kinsley wrote that \"Congress's war power has become the most flagrantly disregarded provision in the Constitution,\" and that the \"real erosion [of Congress's war power] began after World WarII.\" Disagreement about the extent of congressional versus presidential power regarding war has been present periodically throughout the nation's history.\nCongress can establish post offices and post roads, issue patents and copyrights, fix standards of weights and measures, establish Courts inferior to the Supreme Court, and \"make all Laws which shall be necessary and proper for carrying into Execution the foregoing Powers, and all other Powers vested by this Constitution in the Government of the United States, or in any Department or Officer thereof\". Article Four gives Congress the power to admit new states into the Union.\nOne of Congress's foremost non-legislative functions is the power to investigate and oversee the executive branch. Congressional oversight is usually delegated to committees and is facilitated by Congress's subpoena power. Some critics have charged that Congress has in some instances failed to do an adequate job of overseeing the other branches of government. In the Plame affair, critics including Representative Henry A. Waxman charged that Congress was not doing an adequate job of oversight in this case. There have been concerns about congressional oversight of executive actions such as warrantless wiretapping, although others respond that Congress did investigate the legality of presidential decisions. Political scientists Ornstein and Mann suggested that oversight functions do not help members of Congress win reelection. Congress also has the exclusive power of removal, allowing impeachment and removal of the president, federal judges and other federal officers. There have been charges that presidents acting under the doctrine of the unitary executive have assumed important legislative and budgetary powers that should belong to Congress. So-called signing statements are one way in which a president can \"tip the balance of power between Congress and the White House a little more in favor of the executive branch\", according to one account. Past presidents, including Ronald Reagan, George H. W. Bush, Bill Clinton, and George W. Bush, have made public statements when signing congressional legislation about how they understand a bill or plan to execute it, and commentators, including the American Bar Association, have described this practice as against the spirit of the Constitution. There have been concerns that presidential authority to cope with financial crises is eclipsing the power of Congress. In 2008, George F. Will called the Capitol building a \"tomb for the antiquated idea that the legislative branch matters\".\nEnumeration.\nThe Constitution enumerates the powers of Congress in detail. In addition, other congressional powers have been granted, or confirmed, by constitutional amendments. The Thirteenth (1865), Fourteenth (1868), and Fifteenth Amendments (1870) gave Congress authority to enact legislation to enforce rights of African Americans, including voting rights, due process, and equal protection under the law. Generally militia forces are controlled by state governments, not Congress.\nImplicit, commerce clause.\nCongress also has implied powers deriving from the Constitution's Necessary and Proper Clause which permit Congress to \"make all laws which shall be necessary and proper for carrying into Execution the foregoing Powers, and all other Powers vested by this Constitution in the Government of the United States, or in any Department or Officer thereof\". Broad interpretations of this clause and of the Commerce Clause, the enumerated power to regulate commerce, in rulings such as \"McCulloch v. Maryland\", have effectively widened the scope of Congress's legislative authority far beyond that prescribed in Section Eight.\nTerritorial government.\nConstitutional responsibility for the oversight of Washington, D.C., the federal district and national capital, and the U.S. territories of Guam, American Samoa, Puerto Rico, the U.S. Virgin Islands, and the Northern Mariana Islands rests with Congress. The republican form of government in territories is devolved by congressional statute to the respective territories including direct election of governors, the D.C. mayor and locally elective territorial legislatures.\nEach territory and Washington, D.C., elects a non-voting delegate to the U.S. House of Representatives as they have throughout congressional history. They \"possess the same powers as other members of the House, except that they may not vote when the House is meeting as the House of Representatives\". They are assigned offices and allowances for staff, participate in debate, and appoint constituents to the four military service academies for the Army, Navy, Air Force and Coast Guard.\nWashington, D.C., citizens alone among U.S. territories have the right to directly vote for the President of the United States, although the Democratic and Republican political parties nominate their presidential candidates at national conventions which include delegates from the five major territories.\nChecks and balances.\nRepresentative Lee H. Hamilton explained how Congress functions within the federal government:\nTo me the key to understanding it is balance. The founders went to great lengths to balance institutions against each other\u00a0\u2013 balancing powers among the three branches: Congress, the president, and the Supreme Court; between the House of Representatives and the Senate; between the federal government and the states; among states of different sizes and regions with different interests; between the powers of government and the rights of citizens, as spelled out in the Bill of Rights... No one part of government dominates the other.\nThe Constitution provides checks and balances among the three branches of the federal government. Its authors expected the greater power to lie with Congress as described in Article One.\nThe influence of Congress on the presidency has varied from period to period depending on factors such as congressional leadership, presidential political influence, historical circumstances such as war, and individual initiative by members of Congress. The impeachment of Andrew Johnson made the presidency less powerful than Congress for a considerable period afterwards. The 20th and 21st centuries have seen the rise of presidential power under politicians such as Theodore Roosevelt, Woodrow Wilson, Franklin D. Roosevelt, Richard Nixon, Ronald Reagan, and George W. Bush. Congress restricted presidential power with laws such as the Congressional Budget and Impoundment Control Act of 1974 and the War Powers Resolution. The presidency remains considerably more powerful today than during the 19th century. Executive branch officials are often loath to reveal sensitive information to members of Congress because of concern that information could not be kept secret; in return, knowing they may be in the dark about executive branch activity, congressional officials are more likely to distrust their counterparts in executive agencies. Many government actions require fast coordinated effort by many agencies, and this is a task that Congress is ill-suited for. Congress is slow, open, divided, and not well matched to handle more rapid executive action or do a good job of overseeing such activity, according to one analysis.\nThe Constitution concentrates removal powers in the Congress by empowering and obligating the House of Representatives to impeach executive or judicial officials for \"Treason, Bribery, or other high Crimes and Misdemeanors\". Impeachment is a formal accusation of unlawful activity by a civil officer or government official. The Senate is constitutionally empowered and obligated to try all impeachments. A simple majority in the House is required to impeach an official; a two-thirds majority in the Senate is required for conviction. A convicted official is automatically removed from office; in addition, the Senate may stipulate that the defendant be banned from holding office in the future. Impeachment proceedings may not inflict more than this. A convicted party may face criminal penalties in a normal court of law. In the history of the United States, the House of Representatives has impeached sixteen officials, of whom seven were convicted. Another resigned before the Senate could complete the trial. Only three presidents have ever been impeached: Andrew Johnson in 1868, Bill Clinton in 1999, Donald Trump in 2019 and 2021. The trials of Johnson, Clinton, and the 2019 trial of Trump all ended in acquittal; in Johnson's case, the Senate fell one vote short of the two-thirds majority required for conviction. In 1974, Richard Nixon resigned from office after impeachment proceedings in the House Judiciary Committee indicated his removal from office.\nThe Senate has an important check on the executive power by confirming Cabinet officials, judges, and other high officers \"by and with the Advice and Consent of the Senate\". It confirms most presidential nominees, but rejections are not uncommon. Furthermore, treaties negotiated by the President must be ratified by a two-thirds majority vote in the Senate to take effect. As a result, presidential arm-twisting of senators can happen before a key vote; for example, President Obama's secretary of state, Hillary Clinton, urged her former senate colleagues to approve a nuclear arms treaty with Russia in 2010. The House of Representatives has no formal role in either the ratification of treaties or the appointment of federal officials, other than in in the office of the vice president; in such a case, a majority vote in each House is required to confirm a president's nomination of a vice president.\nIn 1803, the Supreme Court established judicial review of federal legislation in \"Marbury v. Madison\", holding that Congress could not grant unconstitutional power to the Court itself. The Constitution did not explicitly state that the courts may exercise judicial review. The notion that courts could declare laws unconstitutional was envisioned by the founding fathers. Alexander Hamilton, for example, mentioned and expounded upon the doctrine in Federalist No. 78. Originalists on the Supreme Court have argued that if the constitution does not say something explicitly it is unconstitutional to infer what it should, might, or could have said. Judicial review means that the Supreme Court can nullify a congressional law. It is a huge check by the courts on the legislative authority and limits congressional power substantially. In 1857, for example, the Supreme Court struck down provisions of a congressional act of 1820 in its Dred Scott decision. At the same time, the Supreme Court can extend congressional power through its constitutional interpretations.\nThe congressional inquiry into St. Clair's Defeat of 1791 was the first congressional investigation of the executive branch. Investigations are conducted to gather information on the need for future legislation, to test the effectiveness of laws already passed, and to inquire into the qualifications and performance of members and officials of the other branches. Committees may hold hearings, and, if necessary, subpoena people to testify when investigating issues over which it has the power to legislate. Witnesses who refuse to testify may be cited for contempt of Congress, and those who testify falsely may be charged with perjury. Most committee hearings are open to the public (the House and Senate intelligence committees are the exception); important hearings are widely reported in the mass media and transcripts published a few months afterwards. Congress, in the course of studying possible laws and investigating matters, generates an incredible amount of information in various forms, and can be described as a publisher. Indeed, it publishes House and Senate reports and maintains databases which are updated irregularly with publications in a variety of electronic formats.\nCongress also plays a role in presidential elections. Both Houses meet in joint session on the sixth day of January following a presidential election to count the electoral votes, and there are procedures to follow if no candidate wins a majority.\nThe main result of congressional activity is the creation of laws, most of which are contained in the United States Code, arranged by subject matter alphabetically under fifty title headings to present the laws \"in a concise and usable form\".\nStructure.\nCongress is split into two chambers\u00a0\u2013 House and Senate\u00a0\u2013 and manages the task of writing national legislation by dividing work into separate committees which specialize in different areas. Some members of Congress are elected by their peers to be officers of these committees. Further, Congress has ancillary organizations such as the Government Accountability Office and the Library of Congress to help provide it with information, and members of Congress have staff and offices to assist them as well. In addition, a vast industry of lobbyists helps members write legislation on behalf of diverse corporate and labor interests.\nCommittees.\nSpecializations.\nThe committee structure permits members of Congress to study a particular subject intensely. It is neither expected nor possible that a member be an expert on all subject areas before Congress. As time goes by, members develop expertise in particular subjects and their legal aspects. Committees investigate specialized subjects and advise the entire Congress about choices and trade-offs. The choice of specialty may be influenced by the member's constituency, important regional issues, prior background and experience. Senators often choose a different specialty from that of the other senator from their state to prevent overlap. Some committees specialize in running the business of other committees and exert a powerful influence over all legislation; for example, the House Ways and Means Committee has considerable influence over House affairs.\nPower.\nCommittees write legislation. While procedures, such as the House discharge petition process, can introduce bills to the House floor and effectively bypass committee input, they are exceedingly difficult to implement without committee action. Committees have power and have been called \"independent fiefdoms\". Legislative, oversight, and internal administrative tasks are divided among about two hundred committees and subcommittees which gather information, evaluate alternatives, and identify problems. They propose solutions for consideration by the full chamber. In addition, they perform the function of \"oversight\" by monitoring the executive branch and investigating wrongdoing.\nOfficer.\nAt the start of each two-year session, the House elects a speaker who does not normally preside over debates but serves as the majority party's leader. In the Senate, the vice president is the \"ex officio\" president of the Senate. In addition, the Senate elects an officer called the president pro tempore. \"Pro tempore\" means \"for the time being\" and this office is usually held by the most senior member of the Senate's majority party and customarily keeps this position until there is a change in party control. Accordingly, the Senate does not necessarily elect a new president pro tempore at the beginning of a new Congress. In the House and Senate, the actual presiding officer is generally a junior member of the majority party who is appointed so that new members become acquainted with the rules of the chamber.\nSupport services.\nLibrary of Congress.\nThe Library of Congress was established by an act of Congress in 1800. It is primarily housed in three buildings on Capitol Hill, but also includes several other sites: the National Library Service for the Blind and Physically Handicapped in Washington, D.C.; the National Audio-Visual Conservation Center in Culpeper, Virginia; a large book storage facility located in Fort Meade, Maryland; and multiple overseas offices. The Library had mostly law books when it was burnt by British forces in 1814 during the War of 1812, but the library's collections were restored and expanded when Congress authorized the purchase of Thomas Jefferson's private library. One of the library's missions is to serve Congress and its staff as well as the American public. It is the largest library in the world with nearly 150 million items including books, films, maps, photographs, music, manuscripts, graphics, and materials in 470 languages.\nCongressional Research Service.\nThe Congressional Research Service, part of the Library of Congress, provides detailed, up-to-date and non-partisan research for senators, representatives, and their staff to help them carry out their official duties. It provides ideas for legislation, helps members analyze a bill, facilitates public hearings, makes reports, consults on matters such as parliamentary procedure, and helps the two chambers resolve disagreements. It has been called the \"House's think tank\" and has a staff of about 900 employees.\nCongressional Budget Office.\nThe Congressional Budget Office (CBO) is a federal agency which provides economic data to Congress.\nIt was created as an independent non-partisan agency by the Congressional Budget and Impoundment Control Act of 1974. It helps Congress estimate revenue inflows from taxes and helps the budgeting process. It makes projections about such matters as the national debt as well as likely costs of legislation. It prepares an annual \"Economic and Budget Outlook\" with a mid-year update and writes \"An Analysis of the President's Budgetary Proposals\" for the Senate's Appropriations Committee. The speaker of the House and the Senate's president pro tempore jointly appoint the CBO director for a four-year term.\nLobbying.\nLobbyists represent diverse interests and often seek to influence congressional decisions to reflect their clients' needs. Lobby groups and their members sometimes write legislation and whip bills. In 2007, there were approximately 17,000 federal lobbyists in Washington, D.C. They explain to legislators the goals of their organizations. Some lobbyists represent non-profit organizations and work pro bono for issues in which they are personally interested.\nPartisanship versus bipartisanship.\nCongress has alternated between periods of constructive cooperation and compromise between parties, known as bipartisanship, and periods of deep political polarization and fierce infighting, known as partisanship. The period after the Civil War was marked by partisanship, as is the case today. It is generally easier for committees to reach accord on issues when compromise is possible. Some political scientists speculate that a prolonged period marked by narrow majorities in both chambers of Congress has intensified partisanship in the last few decades, but that an alternation of control of Congress between Democrats and Republicans may lead to greater flexibility in policies, as well as pragmatism and civility within the institution.\nProcedures.\nSessions.\nA term of Congress is divided into two \"sessions\", one for each year; Congress has occasionally been called into an extra or \"special session\". A new session commences on January3 each year unless Congress decides differently. The Constitution requires Congress to meet at least once each year and forbids either house from meeting outside the Capitol without the consent of the other house.\nJoint sessions.\nJoint sessions of the United States Congress occur on special occasions that require a concurrent resolution from House and Senate. These sessions include counting electoral votes after a presidential election and the president's State of the Union address. The constitutionally mandated report, normally given as an annual speech, is modeled on Britain's Speech from the Throne, was written by most presidents after Jefferson but personally delivered as a spoken oration beginning with Wilson in 1913. Joint Sessions and Joint Meetings are traditionally presided over by the speaker of the House, except when counting presidential electoral votes when the vice president (acting as the president of the Senate) presides.\nBills and resolutions.\nIdeas for legislation can come from members, lobbyists, state legislatures, constituents, legislative counsel, or executive agencies. Anyone can write a bill, but only members of Congress may introduce bills. Most bills are not written by Congress members, but originate from the Executive branch; interest groups often draft bills as well. The usual next step is for the proposal to be passed to a committee for review. A proposal is usually in one of these forms:\nRepresentatives introduce a bill while the House is in session by placing it in the \"hopper\" on the Clerk's desk. It is assigned a number and referred to a committee which studies each bill intensely at this stage. Drafting statutes requires \"great skill, knowledge, and experience\" and sometimes take a year or more. Sometimes lobbyists write legislation and submit it to a member for introduction. Joint resolutions are the normal way to propose a constitutional amendment or declare war. On the other hand, concurrent resolutions (passed by both houses) and simple resolutions (passed by only one house) do not have the force of law but express the opinion of Congress or regulate procedure. Bills may be introduced by any member of either house. The Constitution states: \"All Bills for raising Revenue shall originate in the House of Representatives.\" While the Senate cannot originate revenue and appropriation bills, it has the power to amend or reject them. Congress has sought ways to establish appropriate spending levels.\nEach chamber determines its own internal rules of operation unless specified in the Constitution or prescribed by law. In the House, a Rules Committee guides legislation; in the Senate, a Standing Rules committee is in charge. Each branch has its own traditions; for example, the Senate relies heavily on the practice of getting \"unanimous consent\" for noncontroversial matters. House and Senate rules can be complex, sometimes requiring a hundred specific steps before a bill can become a law. Members sometimes turn to outside experts to learn about proper congressional procedures.\nEach bill goes through several stages in each house including consideration by a committee and advice from the Government Accountability Office. Most legislation is considered by standing committees which have jurisdiction over a particular subject such as Agriculture or Appropriations. The House has twenty standing committees; the Senate has sixteen. Standing committees meet at least once each month. Almost all standing committee meetings for transacting business must be open to the public unless the committee votes, publicly, to close the meeting. A committee might call for public hearings on important bills. Each committee is led by a chair who belongs to the majority party and a ranking member of the minority party. Witnesses and experts can present their case for or against a bill. Then, a bill may go to what is called a \"mark-up\" session, where committee members debate the bill's merits and may offer amendments or revisions. Committees may also amend the bill, but the full house holds the power to accept or reject committee amendments. After debate, the committee votes whether it wishes to report the measure to the full house. If a bill is \"tabled\" then it is rejected. If amendments are extensive, sometimes a new bill with amendments built in will be submitted as a so-called \"clean bill\" with a new number. Both houses have procedures under which committees can be bypassed or overruled but they are rarely used. Generally, members who have been in Congress longer have greater seniority and therefore greater power.\nA bill which reaches the floor of the full house can be simple or complex and begins with an enacting formula such as \"Be it enacted by the Senate and House of Representatives of the United States of America in Congress assembled...\" Consideration of a bill requires, itself, a \"rule\" which is a simple resolution specifying the particulars of debate\u00a0\u2013 time limits, possibility of further amendments, and such. Each side has equal time and members can yield to other members who wish to speak. Sometimes opponents seek to \"recommit\" a bill which means to change part of it. Generally, discussion requires a \"quorum\", usually half of the total number of representatives, before discussion can begin, although there are exceptions. The house may debate and amend the bill; the precise procedures used by the House and Senate differ. A final vote on the bill follows.\nOnce a bill is approved by one house, it is sent to the other which may pass, reject, or amend it. For the bill to become law, both houses must agree to identical versions of the bill. If the second house amends the bill, then the differences between the two versions must be reconciled in a conference committee, an \"ad hoc\" committee that includes senators and representatives sometimes by using a \"reconciliation process\" to limit budget bills. Both houses use a budget enforcement mechanism informally known as \"pay-as-you-go\" or \"paygo\" which discourages members from considering acts that increase budget deficits. If both houses agree to the version reported by the conference committee, the bill passes, otherwise it fails.\nThe Constitution specifies that a majority of members (a quorum) be present before doing business in each house. The rules of each house assume that a quorum is present unless a quorum call demonstrates the contrary and debate often continues despite the lack of a majority.\nVoting within Congress can take many forms, including systems using lights and bells and electronic voting. Both houses use voice voting to decide most matters in which members shout \"aye\" or \"no\" and the presiding officer announces the result. The Constitution requires a recorded vote if demanded by one-fifth of the members present or when voting to override a presidential veto. If the voice vote is unclear or if the matter is controversial, a recorded vote usually happens. The Senate uses roll-call voting, in which a clerk calls out the names of all the senators, each senator stating \"aye\" or \"no\" when their name is announced. In the Senate, the Vice President may cast the tie-breaking vote if present when the senators are equally divided.\nThe House reserves roll-call votes for the most formal matters, as a roll call of all 435 representatives takes quite some time; normally, members vote by using an electronic device. In the case of a tie, the motion in question fails. Most votes in the House are done electronically, allowing members to vote \"yea\" or \"nay\" or \"present\" or \"open\". Members insert a voting \"ID card\" and can change their votes during the last five minutes if they choose; in addition, paper ballots are used occasionally (\"yea\" indicated by green and \"nay\" by red). One member cannot cast a proxy vote for another. Congressional votes are recorded on an online database.\nAfter passage by both houses, a bill is enrolled and sent to the president for approval. The president may sign it making it law or veto it, perhaps returning it to Congress with the president's objections. A vetoed bill can still become law if each house of Congress votes to override the veto with a two-thirds majority. Finally, the president may do nothingneither signing nor vetoing the billand then the bill becomes law automatically after ten days (not counting Sundays) according to the Constitution. But if Congress is adjourned during this period, presidents may veto legislation passed at the end of a congressional session simply by ignoring it; the maneuver is known as a pocket veto, and cannot be overridden by the adjourned Congress.\nPublic interaction.\nAdvantage of incumbency.\nCitizens and representatives.\nSenators face reelection every six years, and representatives every two. Reelections encourage candidates to focus their publicity efforts at their home states or districts. Running for reelection can be a grueling process of distant travel and fund-raising which distracts senators and representatives from paying attention to governing, according to some critics. Although others respond that the process is necessary to keep members of Congress in touch with voters.\nIncumbent members of Congress running for reelection have strong advantages over challengers. They raise more money because donors fund incumbents over challengers, perceiving the former as more likely to win, and donations are vital for winning elections. One critic compared election to Congress to receiving life tenure at a university. Another advantage for representatives is the practice of gerrymandering. After each ten-year census, states are allocated representatives based on population, and officials in power can choose how to draw the congressional district boundaries to support candidates from their party. As a result, reelection rates of members of Congress hover around 90 percent, causing some critics to call them a privileged class. Academics such as Princeton's Stephen Macedo have proposed solutions to fix gerrymandering in the U.S. Senators and representatives enjoy free mailing privileges, called franking privileges; while these are not intended for electioneering, this rule is often skirted by borderline election-related mailings during campaigns.\nExpensive campaigns.\nIn 1971, the cost of running for Congress in Utah was $70,000 but costs have climbed. The biggest expense is television advertisements. Today's races cost more than a million dollars for a House seat, and six million or more for a Senate seat. Since fundraising is vital, \"members of Congress are forced to spend ever-increasing hours raising money for their re-election.\"\nThe Supreme Court has treated campaign contributions as a free speech issue. Some see money as a good influence in politics since it \"enables candidates to communicate with voters\". Few members retire from Congress without complaining about how much it costs to campaign for reelection. Critics contend that members of Congress are more likely to attend to the needs of heavy campaign contributors than to ordinary citizens.\nElections are influenced by many variables. Some political scientists speculate there is a \"coattail effect\" (when a popular president or party position has the effect of reelecting incumbents who win by \"riding on the president's coattails\"), although there is some evidence that the coattail effect is irregular and possibly declining since the 1950s. Some districts are so heavily Democratic or Republican that they are called a safe seat; any candidate winning the primary will almost always be elected, and these candidates do not need to spend money on advertising. But some races can be competitive when there is no incumbent. If a seat becomes vacant in an open district, then both parties may spend heavily on advertising in these races; in California in 1992, only four of twenty races for House seats were considered highly competitive.\nTelevision and negative advertising.\nSince members of Congress must advertise heavily on television, this usually involves negative advertising, which smears an opponent's character without focusing on the issues. Negative advertising is seen as effective because \"the messages tend to stick.\" These advertisements sour the public on the political process in general as most members of Congress seek to avoid blame. One wrong decision or one damaging television image can mean defeat at the next election, which leads to a culture of risk avoidance, a need to make policy decisions behind closed doors, and concentrating publicity efforts in the members' home districts.\nPerceptions.\nProminent Founding Fathers, writing in \"The Federalist Papers\", felt that elections were essential to liberty, that a bond between the people and the representatives was particularly essential, and that \"frequent elections are unquestionably the only policy by which this dependence and sympathy can be effectually secured.\" In 2009, few Americans were familiar with leaders of Congress. The percentage of Americans eligible to vote who did, in fact, vote was 63% in 1960, but has been falling since, although there was a slight upward trend in the 2008 election. Public opinion polls asking people if they approve of the job Congress is doing have, in the last few decades, hovered around 25% with some variation. Scholar Julian Zeliger suggested that the \"size, messiness, virtues, and vices that make Congress so interesting also create enormous barriers to our understanding the institution... Unlike the presidency, Congress is difficult to conceptualize.\" Other scholars suggest that despite the criticism, \"Congress is a remarkably resilient institution... its place in the political process is not threatened... it is rich in resources\" and that most members behave ethically. They contend that \"Congress is easy to dislike and often difficult to defend\" and this perception is exacerbated because many challengers running for Congress run \"against\" Congress, which is an \"old form of American politics\" that further undermines Congress's reputation with the public:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;\nAn additional factor that confounds public perceptions of Congress is that congressional issues are becoming more technical and complex and require expertise in subjects such as science, engineering and economics. As a result, Congress often cedes authority to experts at the executive branch.\nSince 2006, Congress has dropped ten points in the Gallup confidence poll with only nine percent having \"a great deal\" or \"quite a lot\" of confidence in their legislators. Since 2011, Gallup poll has reported Congress's approval rating among Americans at 10% or below three times. Public opinion of Congress plummeted further to 5% in October 2013 after parts of the U.S. government deemed 'nonessential government' shut down.\nSmaller states and bigger states.\nWhen the Constitution was ratified in 1787, the ratio of the populations of large states to small states was roughly twelve to one. The Connecticut Compromise gave every state, large and small, an equal vote in the Senate. Since each state has two senators, residents of smaller states have more clout in the Senate than residents of larger states. But since 1787, the population disparity between large and small states has grown; in 2006, for example, California had seventy times the population of Wyoming. Critics, such as constitutional scholar Sanford Levinson, have suggested that the population disparity works against residents of large states and causes a steady redistribution of resources from \"large states to small states\". Others argue that the Connecticut Compromise was deliberately intended by the Founding Fathers to construct the Senate so that each state had equal footing not based on population, and contend that the result works well on balance.\nMembers and constituents.\nA major role for members of Congress is providing services to constituents. Constituents request assistance with problems. Providing services helps members of Congress win votes and elections and can make a difference in close races. Congressional staff can help citizens navigate government bureaucracies. One academic described the complex intertwined relation between lawmakers and constituents as \"home style\".\nMotivation.\nOne way to categorize lawmakers, according to former University of Rochester political science professor Richard Fenno, is by their general motivation:\nPrivileges.\nOutside income and gifts.\nRepresentative Jim Cooper of Tennessee told Harvard professor Lawrence Lessig that a chief problem with Congress was that members focused on their future careers as lobbyists after serving\u00a0\u2013 that Congress was a \"Farm League for KStreet\". Family members of active legislators have also been hired by lobbying firms, which while not allowed to lobby their family member, has drawn criticism as a conflict of interest.\nMembers of congress have been accused of insider trading, such as in the 2020 congressional insider trading scandal, where members of Congress or their family members have traded on stocks related to work on their committees. One 2011 study concluded that portfolios of members of Congress outperformed both the market and hedge funds, which the authors suggested as evidence of insider trading. Proposed solutions include putting stocks in blind trusts to prevent future insider trading.\nSome members of Congress have gone on lavish trips paid for by outside groups, sometimes bringing family members, which are often legal even if in an ethical gray area.\nPay.\nSome critics complain congressional pay is high compared with a median American income. Others have countered that congressional pay is consistent with other branches of government. Another criticism is that members of Congress are insulated from the health care market due to their coverage. Others have criticized the wealth of members of Congress. In January 2014, it was reported that for the first time over half of the members of Congress were millionaires. Congress has been criticized for trying to conceal pay raises by slipping them into a large bill at the last minute.\nMembers elected since 1984 are covered by the Federal Employees Retirement System (FERS). Like other federal employees, congressional retirement is funded through taxes and participants' contributions. Members of Congress under FERS contribute 1.3% of their salary into the FERS retirement plan and pay 6.2% of their salary in Social Security taxes. And like federal employees, members contribute one-third of the cost of health insurance with the government covering the other two-thirds. The size of a congressional pension depends on the years of service and the average of the highest three years of their salary. By law, the starting amount of a member's retirement annuity may not exceed 80% of their final salary. In 2018, the average annual pension for retired senators and representatives under the Civil Service Retirement System (CSRS) was $75,528, while those who retired under FERS, or in combination with CSRS, was $41,208.\nMembers of Congress make fact-finding missions to learn about other countries and stay informed, but these outings can cause controversy if the trip is deemed excessive or unconnected with the task of governing. For example, \"The Wall Street Journal\" reported in 2009 that lawmaker trips abroad at taxpayer expense had included spas, $300-per-night extra unused rooms, and shopping excursions. Some lawmakers responded that \"traveling with spouses compensates for being away from them a lot in Washington\" and justify the trips as a way to meet officials in other nations.\nBy the Twenty-seventh Amendment, changes to congressional pay may not take effect before the next election to the House of the Representatives. In \"Boehner v. Anderson\", the United States Court of Appeals for the District of Columbia Circuit ruled that the amendment does not affect cost-of-living adjustments.\nPostage.\nThe franking privilege allows members of Congress to send official mail to constituents at government expense. Though they are not permitted to send election materials, borderline material is often sent, especially in the run-up to an election by those in close races. Some academics consider free mailings as giving incumbents a big advantage over challengers.\nProtection.\nMembers of Congress enjoy parliamentary privilege, including freedom from arrest in all cases except for treason, felony, and breach of the peace, and freedom of speech in debate. This constitutionally derived immunity applies to members during sessions and when traveling to and from sessions. The term \"arrest\" has been interpreted broadly, and includes any detention or delay in the course of law enforcement, including court summons and subpoenas. The rules of the House strictly guard this privilege; a member may not waive the privilege on their own but must seek the permission of the whole house to do so. Senate rules are less strict and permit individual senators to waive the privilege as they choose.\nThe Constitution guarantees absolute freedom of debate in both houses, providing in the Speech or Debate Clause of the Constitution that \"for any Speech or Debate in either House, they shall not be questioned in any other Place.\" Accordingly, a member of Congress may not be sued in court for slander because of remarks made in either house, although each house has its own rules restricting offensive speeches, and may punish members who transgress.\nObstructing the work of Congress is a crime under federal law and is known as contempt of Congress. Each member has the power to cite people for contempt but can only issue a contempt citation\u00a0\u2013 the judicial system pursues the matter like a normal criminal case. If convicted in court of contempt of Congress, a person may be imprisoned for up to one year.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nCitations.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;\nFurther reading.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;"}
{"id": "31758", "revid": "35936988", "url": "https://en.wikipedia.org/wiki?curid=31758", "title": "United States congressional delegations from Alabama", "text": "Since Alabama became a U.S. state in 1819, it has sent congressional delegations to the United States Senate and United States House of Representatives. Each state elects two senators to serve for six years, and members of the House to two-year terms. Before becoming a state, the Alabama Territory elected a non-voting delegate at-large to Congress from 1818 to 1819.\nThese are tables of congressional delegations from Alabama to the United States Senate and the United States House of Representatives.\nCurrent delegation.\nAlabama's current congressional delegation in the Congress consists of its two senators, both of whom are Republicans, and its seven representatives: 5 Republicans, 2 Democrats.\nThe current dean of the Alabama delegation is Representative Robert Aderholt, having served in the U.S. Congress since 1997.\nUnited States Senate.\n&lt;templatestyles src=\"template:sticky header/styles.css\"/&gt;\nUnited States House of Representatives.\n1818\u20131819: 1 non-voting delegate.\nStarting on January 29, 1818, Alabama Territory sent a non-voting delegate to the House.\n1819\u20131823: 1 seat.\nAfter statehood on December 14, 1819, Alabama had one seat in the House.\n1823\u20131833: 3 seats.\nFollowing the 1820 census, Alabama had three seats.\n1833\u20131843: 5 seats.\nFollowing the 1830 census, Alabama had five seats. During the 27th Congress, those seats were all elected statewide at-large on a general ticket.\n1843\u20131863: 7 seats.\nFollowing the 1840 census, Alabama resumed the use of districts, now increased to seven.\n1863\u20131873: 6 seats.\nFollowing the 1860 census, Alabama was apportioned six seats.\n1873\u20131893: 8 seats.\nFollowing the 1870 census, Alabama was apportioned eight seats. From 1873 to 1877, the two new seats were elected at large, statewide. After 1877, however, the entire delegation was redistricted.\n1893\u20131913: 9 seats.\nFollowing the 1890 census, Alabama was apportioned nine seats.\n1913\u20131933: 10 seats.\nFollowing the 1910 census, Alabama was apportioned ten seats. At first, the extra seat was elected at-large. Starting with the 1916 elections, the seats were redistricted and a was added.\n1933\u20131963: 9 seats.\nFollowing the 1930 census, Alabama was apportioned nine seats.\n&lt;templatestyles src=\"template:sticky header/styles.css\"/&gt;\n1963\u20131973: 8 seats.\nFollowing the 1960 census, Alabama was apportioned eight seats. During the 88th Congress, those seats were all elected statewide at-large on a general ticket.\n1973\u2013present: 7 seats.\nSince the 1970 census, Alabama has been apportioned seven seats.\n&lt;templatestyles src=\"template:sticky header/styles.css\"/&gt;\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "31759", "revid": "1294203415", "url": "https://en.wikipedia.org/wiki?curid=31759", "title": "United States congressional delegations from Alaska", "text": "Since Alaska became a U.S. state in 1959, it has sent congressional delegations to the United States Senate and United States House of Representatives. Each state elects two senators to serve for six years, and member(s) of the House to two-year terms. Before becoming a state, the Territory of Alaska elected a non-voting delegate at-large to Congress from 1906 to 1959.\nThese are tables of congressional delegations from Alaska to the United States Senate and the United States House of Representatives.\nCurrent delegation.\nAlaska's current congressional delegation in the Congress consists of its two senators, who are both Republicans and its sole representative, who is a Republican. The current dean of the Alaska delegation is Senator Lisa Murkowski having served in the Senate since 2002. Lisa Murkowski is the first elected senator born in Alaska.\n&lt;section begin=\"Current representatives\"/&gt;\n&lt;section end=\"Current representatives\"/&gt;\nUnited States Senate.\nEach state elects two senators by statewide popular vote every six years. The terms of the two senators are staggered so that they are not elected in the same year, meaning that each seat also has a class determining the years in which the seat will be up for election. Alaska's senators are elected in classes 2 and 3.\nThere have been eight senators from Alaska, of whom four have been Democrats and four have been Republicans. William Egan and Ernest Gruening were elected to the Senate on October 6, 1956 for the 84th Congress but did not take the oath of office and were not accorded senatorial privileges, since Alaska was not yet a state. Alaska's current senators, both Republicans, are Dan Sullivan, in office since 2015, and Lisa Murkowski, in office since 2002.\n&lt;templatestyles src=\"template:sticky header/styles.css\"/&gt;\nUnited States House of Representatives.\n1906\u20131959: 1 non-voting delegate.\nStarting on August 14, 1906, Alaska sent a non-voting delegate to the House. From May 17, 1884 to August 24, 1912, Alaska was designated as the District of Alaska. From then to January 3, 1959, it was the Alaska Territory.\n1959\u2013present: 1 seat.\nSince statehood on January 3, 1959, Alaska has had one seat in the House.\n&lt;templatestyles src=\"template:sticky header/styles.css\"/&gt;\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "31760", "revid": "35936988", "url": "https://en.wikipedia.org/wiki?curid=31760", "title": "United States congressional delegations from Hawaii", "text": "Since Hawaii became a state in 1959, it has sent congressional delegations to the United States Senate and United States House of Representatives. Each state elects two senators to serve for six years. Members of the House of Representatives are elected to two-year terms, one from each of Hawaii's congressional districts. Before becoming a state, the Territory of Hawaii elected a non-voting delegate at-large to Congress from 1900 to 1958.\nThe longest-serving senator was Daniel Inouye, from 1963 to 2012\u2014he served as President pro tempore of the United States Senate from 2010 to 2012 as the longest-serving senator. Patsy Mink was the first woman of color to serve in the House, and the first woman to represent Hawaii in Congress.\nCurrent delegation.\nHawaii's current congressional delegation in the 119th Congress consists of its two senators and two representatives, all of whom are Democrats.\nThe current dean of the Hawaii delegation is Mazie Hirono, having served in the Senate since 2013 and in Congress since 2007.\nUnited States Senate.\nEach state elects two senators by statewide popular vote every six years. The terms of the two senators are staggered so that they are not elected in the same year, meaning that each seat also has a class determining the years in which the seat will be up for election. Hawaii's senators are elected in classes 1 and 3.\nThere have been seven senators elected from Hawaii, of whom six have been Democrats and one has been a Republican. Hawaii's current senators, both Democrats, are Mazie Hirono, in office since 2013, and Brian Schatz, in office since 2012.\n&lt;templatestyles src=\"Legend/styles.css\" /&gt;\u00a0\u00a0Democratic (D)\n&lt;templatestyles src=\"Legend/styles.css\" /&gt;\u00a0\u00a0Republican (R)\n&lt;templatestyles src=\"Stack/styles.css\"/&gt;\n&lt;templatestyles src=\"template:sticky header/styles.css\"/&gt;\nU.S. House of Representatives.\nTerritorial delegates.\nThe Territory of Hawaii was an organized incorporated territory of the United States formed by the Hawaiian Organic Act on April 30, 1900, following the annexation of Hawaii. The territory initially consisted of the Hawaiian Islands, although the Palmyra Atoll was separated from Hawaii when it was admitted into the Union.\nThe territorial delegates were elected to two-year terms from the at-large congressional district in the Hawaii Territory. Delegates were allowed to serve on committees, debate, and submit legislation, but were not permitted to vote on bills. The first delegate, Robert William Wilcox, took office on December 15, 1900, and the last delegate, John A. Burns, left office on August 21, 1959, succeeded on the same day by representative Daniel Inouye. Delegates only served in the House of Representatives, as there was no representation in the Senate until Hawaii became a state.\n&lt;templatestyles src=\"Legend/styles.css\" /&gt;\u00a0\u00a0Democratic (D)\n&lt;templatestyles src=\"Legend/styles.css\" /&gt;\u00a0\u00a0Republican (R)\n&lt;templatestyles src=\"Legend/styles.css\" /&gt;\u00a0\u00a0Home Rule (HR)\n&lt;templatestyles src=\"Stack/styles.css\"/&gt;\nRepresentatives from the State of Hawaii.\nMembers of the House of Representatives are elected every two years by popular vote within a congressional district. From in the 86th Congress through the 91st Congress, both of Hawaii's representatives were elected from Hawaii's at-large congressional district, but in 1969, the Hawaii legislature passed a law creating Hawaii's first and second congressional district, which elected representatives to the 92nd Congress. The representatives from the two new districts, Patsy Mink and Spark Matsunaga, were also the last two representatives of the seats in the at-large district. Every ten years, the number of seats in the House apportioned to every state is recalculated based on the state's population as determined by the United States census. Hawaii had one seat until the 1960 United States census allotted Hawaii a second seat, which was first filled by Thomas Gill in the 1962 House elections.\n&lt;templatestyles src=\"Legend/styles.css\" /&gt;\u00a0\u00a0Democratic (D)\n&lt;templatestyles src=\"Legend/styles.css\" /&gt;\u00a0\u00a0Republican (R)\n&lt;templatestyles src=\"Stack/styles.css\"/&gt;\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "31761", "revid": "11952314", "url": "https://en.wikipedia.org/wiki?curid=31761", "title": "U.S. Congress Representatives from Guam", "text": ""}
{"id": "31762", "revid": "11292982", "url": "https://en.wikipedia.org/wiki?curid=31762", "title": "U.S. Congress Representatives from U.S. Virgin Islands", "text": ""}
{"id": "31763", "revid": "11952314", "url": "https://en.wikipedia.org/wiki?curid=31763", "title": "Delegates of American Samoa to the United States Congress", "text": ""}
{"id": "31764", "revid": "35936988", "url": "https://en.wikipedia.org/wiki?curid=31764", "title": "United States congressional delegations from Arizona", "text": "Since Arizona became a U.S. state in 1912, it has sent congressional delegations to the United States Senate and United States House of Representatives, beginning with the 63rd United States Congress in 1913. Before becoming a state, the Arizona Territory elected a non-voting delegate at-large to Congress from 1864 to 1912. Each state elects two senators to serve for six years, and varying numbers of members of the House, depending on state population, to two-year terms. Arizona has sent nine members to the House in each delegation since the 2010 United States census.\nA total of 60 people have served Arizona in the House and 15 have served Arizona in the Senate. The first woman to serve Arizona in the House was Isabella Greenway. Eight women have served Arizona in the House, including Kyrsten Sinema and Martha McSally, who also served Arizona in the Senate, the only women to do so.\nThe current deans, or longest-serving members, of the Arizona delegation are Republican Representatives David Schweikert of the and Paul Gosar of the , who have both served in the House since 2011. Carl Hayden was Arizona's longest-serving senator, and his 56 years as a senator is the sixth-longest tenure in American history.\nCurrent delegation.\nArizona's current congressional delegation in the Congress consists of its two senators, both Democrats, and its nine representatives, six Republicans and three Democrats.\nAs of 2025, the Cook Partisan Voting Index, a measure of how strongly partisan a state is, ranked Arizona's 1st, 2nd, 5th, 8th, and 9th districts as leaning Republican, and the 3rd, 4th, and 7th districts as leaning Democratic. They ranked the 6th district is ranked as even. As a state, Arizona is ranked as leaning Republican, with a score of R+2.\nUnited States Senate.\nSince it became a state in 1912, 14 people have served as a U.S. senator from Arizona. Of those, Martha McSally and Kyrsten Sinema have been the only women. Sinema is also the first openly bisexual member of Congress. Both senators Barry Goldwater and John McCain have been nominated as the Republican candidate for president, in 1964 and 2008 respectively.\nSometimes considered a swing state, elections in Arizona are considered critical for party control of the Senate. Senators are elected every six years depending on their class, with each senator serving a six-year term, and elections for senators occurring every two years, rotating through each class such that in each election, around one-third of the seats in the Senate are up for election. Arizona's senators are elected in classes I and III. Currently, Arizona is represented in the Senate by Ruben Gallego and Mark Kelly.\n&lt;templatestyles src=\"Stack/styles.css\"/&gt;\n&lt;templatestyles src=\"Legend/styles.css\" /&gt;\u00a0\u00a0Democratic (D)\n&lt;templatestyles src=\"Legend/styles.css\" /&gt;\u00a0\u00a0Independent (I)\n&lt;templatestyles src=\"Legend/styles.css\" /&gt;\u00a0\u00a0Republican (R)\n&lt;templatestyles src=\"template:sticky header/styles.css\"/&gt;\nUnited States House of Representatives.\nArizona has had numerous notable representatives in Congress, including Stewart Udall, who resigned to serve as the Secretary of the Interior in the Kennedy administration, his brother, Mo Udall, who came in second in the 1976 Democratic Party presidential primaries, and John Jacob Rhodes, who served as House Minority Leader for the Republican Party during the Watergate scandal.\nEach district uses a popular vote to elect a member of Arizona's delegation in the House of Representatives. Districts are redrawn every ten years, after data from the US Census is collected. From 1863 to 1912, Arizona sent a non-voting delegate to the House of Representatives; when it became a state in 1912, it had one seat in the House. Since then, its representation in the House has grown along with its population. Since 2013, Arizona has had nine congressional districts drawn according to the results of the 2010 United States census.\n&lt;templatestyles src=\"Stack/styles.css\"/&gt;\n1863\u20131912: 1 non-voting delegate.\nStarting on December 5, 1864, Arizona Territory sent a non-voting delegate to the House.\n&lt;templatestyles src=\"Legend/styles.css\" /&gt;\u00a0\u00a0Democratic (D)\n&lt;templatestyles src=\"Legend/styles.css\" /&gt;\u00a0\u00a0Independent (I)\n&lt;templatestyles src=\"Legend/styles.css\" /&gt;\u00a0\u00a0Republican (R)\n&lt;templatestyles src=\"Legend/styles.css\" /&gt;\u00a0\u00a0Unionist (U)\n1912\u20131943: 1 seat.\nFollowing statehood on February 14, 1912, Arizona had one seat in the House.\n&lt;templatestyles src=\"Legend/styles.css\" /&gt;\u00a0\u00a0Democratic (D)\n1943\u20131963: 2 seats.\nFollowing the 1940 census, Arizona was apportioned two seats. For six years, the seats were elected statewide on a general ticket. In 1949, districts were used.\n&lt;templatestyles src=\"Legend/styles.css\" /&gt;\u00a0\u00a0Democratic (D)\n&lt;templatestyles src=\"Legend/styles.css\" /&gt;\u00a0\u00a0Republican (R)\n1963\u20131973: 3 seats.\nFollowing the 1960 census, Arizona was apportioned three seats.\n&lt;templatestyles src=\"Legend/styles.css\" /&gt;\u00a0\u00a0Democratic (D)\n&lt;templatestyles src=\"Legend/styles.css\" /&gt;\u00a0\u00a0Republican (R)\n1973\u20131983: 4 seats.\nFollowing the 1970 census, Arizona was apportioned four seats.\n&lt;templatestyles src=\"Legend/styles.css\" /&gt;\u00a0\u00a0Democratic (D)\n&lt;templatestyles src=\"Legend/styles.css\" /&gt;\u00a0\u00a0Republican (R)\n1983\u20131993: 5 seats.\nFollowing the 1980 census, Arizona was apportioned five seats.\n&lt;templatestyles src=\"Legend/styles.css\" /&gt;\u00a0\u00a0Democratic (D)\n&lt;templatestyles src=\"Legend/styles.css\" /&gt;\u00a0\u00a0Republican (R)\n1993\u20132003: 6 seats.\nFollowing the 1990 census, Arizona was apportioned six seats.\n&lt;templatestyles src=\"Legend/styles.css\" /&gt;\u00a0\u00a0Democratic (D)\n&lt;templatestyles src=\"Legend/styles.css\" /&gt;\u00a0\u00a0Republican (R)\n2003\u20132013: 8 seats.\nFollowing the 2000 census, Arizona was apportioned eight seats.\n&lt;templatestyles src=\"Legend/styles.css\" /&gt;\u00a0\u00a0Democratic (D)\n&lt;templatestyles src=\"Legend/styles.css\" /&gt;\u00a0\u00a0Republican (R)\n2013\u2013present: 9 seats.\nSince the 2010 census, Arizona has been apportioned nine seats.\n&lt;templatestyles src=\"Legend/styles.css\" /&gt;\u00a0\u00a0Democratic (D)\n&lt;templatestyles src=\"Legend/styles.css\" /&gt;\u00a0\u00a0Republican (R)\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "31768", "revid": "36767729", "url": "https://en.wikipedia.org/wiki?curid=31768", "title": "Unmanned space mission", "text": ""}
{"id": "31769", "revid": "41840956", "url": "https://en.wikipedia.org/wiki?curid=31769", "title": "United Nations", "text": "Intergovernmental organization\nThe United Nations (UN) is a global intergovernmental organization established by the signing of the UN Charter on 26 June 1945 with the articulated mission of maintaining international peace and security, to develop friendly relations among states, to promote international cooperation, and to serve as a centre for harmonizing the actions of states in achieving those goals.\nThe United Nations headquarters is located in New York City, with several other offices located in Geneva, Nairobi, Vienna, and The Hague. The UN comprises six principal organizations: the General Assembly, the Security Council, the Economic and Social Council, the International Court of Justice, the Secretariat, and the Trusteeship Council which, together with several specialized agencies and related agencies, make up the United Nations System. There are in total 193 member states and 2 observer states.\nThe UN has primarily focused on economic and social development, particularly during the wave of decolonization in the mid-20th century. The UN has been recognized as a leader of peace and human development, with many officers and agencies having been awarded the Nobel Peace Prize, but has also been criticized for perceived ineffectiveness, bias, and corruption.\nHistory.\nBackground (pre-1941).\nIn the century prior to the UN's creation, several international organizations such as the International Committee of the Red Cross were formed to ensure protection and assistance for victims of armed conflict and strife.\nDuring World War I, several major leaders, especially U.S. president Woodrow Wilson, advocated for a world body to guarantee peace. The winners of the war, the Allies, met to decide on formal peace terms at the Paris Peace Conference. The League of Nations was approved and started operations, but the United States never joined. On 10 January 1920, the League of Nations formally came into being when the Covenant of the League of Nations, ratified by 42 nations in 1919, took effect. The League Council acted as an executive body directing the Assembly's business. It began with four permanent members\u2014the United Kingdom, France, Italy, and Japan.\nAfter some limited successes and failures during the 1920s, the League proved ineffective in the 1930s, as it failed to act against the Japanese invasion of Manchuria in 1933. Forty nations voted for Japan to withdraw from Manchuria but Japan voted against it and walked out of the League instead of withdrawing from Manchuria. It also failed to act against the Second Italo-Ethiopian War, after the appeal for international intervention by Ethiopian Emperor Haile Selassie I at Geneva in 1936 went with no avail, including when calls for economic sanctions against Italy failed. Italy and other nations left the League.\nWhen World War II broke out in 1939, the League effectively closed down.\nDeclarations by the Allies of World War II (1941\u20131944).\nThe first step towards the establishment of the United Nations was the Inter-Allied Conference in London that led to the Declaration of St James's Palace on 12 June 1941. By August 1941, American President Franklin Roosevelt and British Prime Minister Winston Churchill had drafted the Atlantic Charter; which defined goals for the post-war world. At the subsequent meeting of the Inter-Allied Council in London on 24 September 1941, the eight governments in exile of countries under Axis occupation, together with the Soviet Union and representatives of the Free French Forces, unanimously adopted adherence to the common principles of policy set forth by Britain and the United States.\nRoosevelt and Churchill met at the White House in December 1941 for the Arcadia Conference. Roosevelt is considered a founder of the UN, and coined the term \"United Nations\" to describe the Allied countries. Churchill accepted it, noting its use by Lord Byron. The text of the Declaration by United Nations was drafted on 29 December 1941, by Roosevelt, Churchill, and Harry Hopkins. It incorporated Soviet suggestions but included no role for France. One major change from the Atlantic Charter was the addition of a provision for religious freedom, which Stalin approved after Roosevelt insisted.\nRoosevelt's idea of the \"Four Powers\", refers to the four major Allied countries, the United States, the United Kingdom, the Soviet Union, and China, emerged in the Declaration by the United Nations. On New Year's Day 1942, Roosevelt, Churchill, the Soviet Union's former Foreign Minister Maxim Litvinov, and the Chinese Premier T. V. Soong signed the \"Declaration by United Nations\", and the next day the representatives of twenty-two other nations added their signatures. During the war, the United Nations became the official term for the Allies. In order to join, countries had to sign the Declaration and declare war on the Axis powers.\nThe October 1943 Moscow Conference resulted in the Moscow Declarations, including the Four Power Declaration on General Security. This declaration was signed by the Allied Big Four\u2014the United States, the Soviet Union, the United Kingdom, and China\u2014and aimed for the creation \"at the earliest possible date of a general international organization\". This was the first public announcement that a new international organization was being contemplated to replace the League of Nations. The Tehran Conference followed shortly afterwards at which Roosevelt, Churchill and Joseph Stalin, the leader of the Soviet Union, met and discussed the idea of a post-war international organization.\nThe new international organization was formulated and negotiated amongst the delegations from the Allied Big Four at the Dumbarton Oaks Conference from 21 September to 7 October 1944. They agreed on proposals for the aims, structure and functioning of the new organization. It took the conference at Yalta in February 1945, and further negotiations with the Soviet Union, before all the issues were resolved.\nFounding (1945).\nBy 1 March 1945, 21 additional states had signed the Declaration by the United Nations. After months of planning, the UN Conference on International Organization opened in San Francisco on 25 April 1945. It was attended by 50 nations' governments and a number of non-governmental organizations. The delegations of the Big Four chaired the plenary meetings. Previously, Churchill had urged Roosevelt to restore France to its status of a major power after the liberation of Paris in August 1944. The drafting of the Charter of the United Nations was completed over the following two months, and it was signed on 26 June 1945 by the representatives of the 50 countries. The UN officially came into existence at 20:07 (UTC) on 24 October 1945, upon ratification of the Charter by the five permanent members of the Security Council: the United States, the United Kingdom, France, the Soviet Union and China \u2014 and by a majority of the other 46 nations.\nThe first meetings of the General Assembly, with 51 nations represented, and the Security Council took place in London beginning in January 1946. Debates began at once, covering topical issues such as the presence of Russian troops in Iranian Azerbaijan and British forces in Greece. British diplomat Gladwyn Jebb served as interim secretary-general.\nThe General Assembly selected New York City as the site for the headquarters of the UN. Construction began on 14 September 1948 and the facility was completed on 9 October 1952. The Norwegian Foreign Minister, Trygve Lie, was the first elected UN secretary-general.\nCold War (1947\u20131991).\nThough the UN's primary mandate was peacekeeping, the division between the United States and the Soviet Union often paralysed the organization; generally allowing it to intervene only in conflicts distant from the Cold War. Two notable exceptions were a Security Council resolution on 7 July 1950 authorizing a US-led coalition to repel the North Korean invasion of South Korea, passed in the absence of the Soviet Union, and the signing of the Korean Armistice Agreement on 27 July 1953.\nOn 29 November 1947, the General Assembly approved Resolution 181, a proposal to partition British Mandatory Palestine into two states, a Jewish state and an Arab state, with Jerusalem placed under international status. The Partition plan passed 33\u201313 with 10 abstentions and one absent. The plan was accepted by the Jews and rejected by the Arabs of Palestine and the Arab states leading to civil war. Following the declaration of the State of Israel on 15 May 1948, the surrounding Arab armies https:// Palestine, beginning the 1948 Arab\u2013Israeli War. Two years later, Ralph Bunche, a UN official, negotiated an armistice to the resulting conflict, with the Security Council deciding that \"an armistice shall be established in all sectors of Palestine\". On 7 November 1956, the first UN peacekeeping force was established to end the Suez Crisis; however, the UN was unable to intervene against the Soviet Union's simultaneous invasion of Hungary, following the country's revolution.\nOn 14 July 1960, the UN established the United Nations Operation in the Congo (or UNOC), the largest military force of its early decades, to bring order to Katanga, restoring it to the control of the Democratic Republic of the Congo by 11 May 1964. While travelling to meet rebel leader Moise Tshombe during the conflict, Dag Hammarskj\u00f6ld, often named as one of the UN's most effective secretaries-general, died in a plane crash. Months later he was posthumously awarded the Nobel Peace Prize. In 1964, Hammarskj\u00f6ld's successor, U Thant, deployed the UN Peacekeeping Force in Cyprus, which would become one of the UN's longest-running peacekeeping missions.\nWith the spread of decolonization in the 1960s, the UN's membership shot up due to an influx of newly independent nations. In 1960 alone, 17 new states joined the UN, 16 of them from Africa. On 25 October 1971, with opposition from the United States, but with the support of many Third World nations, the People's Republic of China was given the Chinese seat on the Security Council in place of the Republic of China (also known as Taiwan). The vote was widely seen as a sign of waning American influence in the organization. Third World nations organized themselves into the Group of 77 under the leadership of Algeria, which briefly became a dominant power at the UN. On 10 November 1975, a bloc comprising the Soviet Union and Third World nations passed a resolution, over strenuous American and Israeli opposition, declaring Zionism to be a form of racism. The resolution was repealed on 16 December 1991, shortly after the end of the Cold War.\nWith an increasing Third World presence and the failure of UN mediation in conflicts in the Middle East, Vietnam, and Kashmir, the UN increasingly shifted its attention to its secondary goals of economic development and cultural exchange. By the 1970s, the UN budget for social and economic development was far greater than its peacekeeping budget.\nPost-Cold War (1991\u2013present).\nAfter the Cold War, the UN saw a radical expansion in its peacekeeping duties, taking on more missions in five years than it had in the previous four decades. Between 1988 and 2000, the number of adopted Security Council resolutions more than doubled, and the peacekeeping budget increased more than tenfold. The UN negotiated an end to the Salvadoran Civil War, launched a successful peacekeeping mission in Namibia, and oversaw democratic elections in post-apartheid South Africa and post-Khmer Rouge Cambodia. In 1991, the UN authorized a US-led coalition that repulsed Iraq's invasion of Kuwait. Brian Urquhart, the under-secretary-general of the UN from 1971 to 1985, later described the hopes raised by these successes as a \"false renaissance\" for the organization, given the more troubled missions that followed.\nBeginning in the last decades of the Cold War, critics of the UN condemned the organization for perceived mismanagement and corruption. In 1984, American President Ronald Reagan withdrew the United States' funding from the United Nations Educational, Scientific and Cultural Organization (or UNESCO) over allegations of mismanagement, followed by the United Kingdom and Singapore. Boutros Boutros-Ghali, the secretary-general from 1992 to 1996, initiated a reform of the Secretariat, somewhat reducing the size of the organization. His successor, Kofi Annan, initiated further management reforms in the face of threats from the US to withhold its UN dues.\nThough the UN Charter had been written primarily to prevent aggression by one nation against another, in the early 1990s the UN faced several simultaneous, serious crises within Somalia, Haiti, Mozambique, and the nations that previously made up Yugoslavia. The UN mission in Somalia was widely viewed as a failure after the United States' withdrawal following casualties in the Battle of Mogadishu. The UN mission to Bosnia faced worldwide ridicule for its indecisive and confused mission in the face of ethnic cleansing. In 1994, the UN Assistance Mission for Rwanda failed to intervene in the Rwandan genocide amidst indecision in the Security Council.\nFrom the late 1990s to the early 2000s, international interventions authorized by the UN took a wider variety of forms. The United Nations Security Council Resolution 1244 authorized the NATO-led Kosovo Force beginning in 1999. The UN mission in the Sierra Leone Civil War was supplemented by a British military intervention. The invasion of Afghanistan in 2001 was overseen by NATO. In 2003, the United States invaded Iraq despite failing to pass a UN Security Council resolution for authorization, prompting a new round of questioning of the UN's effectiveness.\nUnder the eighth secretary-general, Ban Ki-moon, the UN intervened with peacekeepers in crises such as the War in Darfur in Sudan and the Kivu conflict in the Democratic Republic of the Congo and sent observers and chemical weapons inspectors to the Syrian Civil War. In 2013, an internal review of UN actions in the final battles of the Sri Lankan Civil War in 2009 concluded that the organization had suffered a \"systemic failure\". In 2010, the organization suffered the worst loss of life in its history, when 101 personnel died in the Haiti earthquake. Acting under the United Nations Security Council Resolution 1973 in 2011, NATO countries intervened in the First Libyan Civil War.\nThe Millennium Summit was held in 2000 to discuss the UN's role in the 21st century. The three-day meeting was the largest gathering of world leaders in history, and it culminated in the adoption by all member states of the Millennium Development Goals (or MDGs), a commitment to achieve international development in areas such as poverty reduction, gender equality and public health. Progress towards these goals, which were to be met by 2015, was ultimately uneven. The 2005 World Summit reaffirmed the UN's focus on promoting development, peacekeeping, human rights and global security. The Sustainable Development Goals (or SDGs) were launched in 2015 to succeed the Millennium Development Goals.\nIn addition to addressing global challenges, the UN has sought to improve its accountability and democratic legitimacy by engaging more with civil society and fostering a global constituency. In an effort to enhance transparency, in 2016 the organization held its first public debate between candidates for secretary-general. On 1 January 2017, Portuguese diplomat Ant\u00f3nio Guterres, who had previously served as the UN High Commissioner for Refugees, became the ninth secretary-general. Guterres has highlighted several key goals for his administration, including an emphasis on diplomacy for preventing conflicts, more effective peacekeeping efforts, and streamlining the organization to be more responsive and versatile to international needs.\nOn 13 June 2019, the UN signed a Strategic Partnership Framework with the World Economic Forum in order to \"jointly accelerate\" the implementation of the 2030 Agenda for Sustainable Development.\nThroughout most of its history, the UN has faced funding issues. However they have become severe since 2020, and in 2025, the UN began facing a financial crisis resulting from delays in member state due payments and refusal to pay the amount the UN charges. A major problem is that the United States, the largest contributor, has a law in place since 1994 where it will not pay more than 25% of total UN Peacekeeping fees, however it is currently assessed 27%. Additionally the US and the second largest contributor, China often delay their payments in order to influence the UN on topics such as the Gaza war and Persecution of Uyghurs in China. Other countries have begun to follow suit, triggering a financial crisis. On 19 May 2025, only 61 countries paid their dues on time and in full. The crisis is causing massive budget cuts within the UN, with the UN warning that millions of lives are being put at risk.\nStructure.\nThe United Nations is part of the broader UN System, which includes an extensive network of institutions and entities. Central to the organization are five principal organs established by the UN Charter: the General Assembly, the Security Council, the Economic and Social Council, the International Court of Justice and the UN Secretariat. A sixth principal organ, the Trusteeship Council, suspended its operations on 1 November 1994 upon the independence of Palau; the last remaining UN trustee territory.\nFour of the five principal organs are located at the main UN Headquarters in New York City, while the International Court of Justice is seated in The Hague. Most other major agencies are based in the UN offices at Geneva, Vienna, and Nairobi, and additional UN institutions are located throughout the world. The six official languages of the UN, used in intergovernmental meetings and documents, are Arabic, Chinese, English, French, Russian and Spanish. On the basis of the Convention on the Privileges and Immunities of the United Nations, the UN and its agencies are immune from the laws of the countries where they operate, safeguarding the UN's impartiality with regard to host and member countries.\nBelow the six organs are, in the words of the author Linda Fasulo, \"an amazing collection of entities and organizations, some of which are actually older than the UN itself and operate with almost complete independence from it\". These include specialized agencies, research and training institutions, programmes and funds and other UN entities.\nAll organizations in the UN system obey the \"Noblemaire principle\", which calls for salaries that will attract and retain citizens of countries where compensation is highest, and which ensures equal pay for work of equal value regardless of the employee's nationality. In practice, the International Civil Service Commission, which governs the conditions of UN personnel, takes reference to the highest-paying national civil service. Staff salaries are subject to an internal tax that is administered by the UN organizations.\n&lt;templatestyles src=\"Navbar-header/styles.css\"/&gt;United Nations organs\nGeneral Assembly.\nThe General Assembly is the primary deliberative assembly of the UN. Composed of all UN member states, the assembly gathers at annual sessions at the General Assembly Hall, but emergency sessions can be summoned. The assembly is led by a president, elected by the member states on a rotating regional basis, and 21 vice-presidents. The first session convened on 10 January 1946 in the Methodist Central Hall in London and comprised representatives of 51 nations.\nWhen the General Assembly decides on seminal questions such as those on peace and security, admission of new members and budgetary matters, a two-thirds majority of those present and voting is required. All other questions are decided by a majority vote. Each member has one vote. Apart from the approval of budgetary matters, resolutions are not binding on the members. The Assembly may make recommendations on any matters within the scope of the UN, except matters of peace and security that are under consideration by the Security Council.\nDraft resolutions can be forwarded to the General Assembly by its six main committees:\nAs well as by the following two committees:\nSecurity Council.\nThe Security Council is charged with maintaining peace and security among nations. While other organs of the UN can only make recommendations to member states, the Security Council has the power to make binding decisions that member states have agreed to carry out, under the terms of Charter Article 25. The decisions of the council are known as United Nations Security Council resolutions.\nThe Security Council is made up of fifteen member states: five permanent members (China, France, Russia, the United Kingdom and the United States) and ten non-permanent members (Algeria, Denmark, Greece, Guyana, Pakistan, Panama, the Republic of Korea, Sierra Leone, Slovenia, and Somalia, as of 2025[ [update]]). The five permanent members hold veto power over UN resolutions, allowing a permanent member to block adoption of a resolution, though not debate. The ten temporary seats are held for two-year terms, with five members elected each year by the General Assembly on a regional basis. The presidency of the Security Council rotates alphabetically each month.\nUN Secretariat.\nThe UN Secretariat carries out the day-to-day duties required to operate and maintain the UN system. It is composed of tens of thousands of international civil servants worldwide and headed by the secretary-general, who is assisted by the deputy secretary-general. The Secretariat's duties include providing information and facilities needed by UN bodies for their meetings and carrying out tasks as directed by the Security Council, the General Assembly, the Economic and Social Council, and other UN bodies.\nThe secretary-general acts as the spokesperson and leader of the UN. The position is defined in the UN Charter as the organization's chief administrative officer. Article 99 of the charter states that the secretary-general can bring to the Security Council's attention \"any matter which in their opinion may threaten the maintenance of international peace and security\", a phrase that secretaries-general since Trygve Lie have interpreted as giving the position broad scope for action on the world stage. The office has evolved into a dual role of an administrator of the UN organization and a diplomat and mediator addressing disputes between member states and finding consensus to global issues.\nThe secretary-general is appointed by the General Assembly, after being recommended by the Security Council, where the permanent members have veto power. There are no specific criteria for the post, but over the years it has become accepted that the position shall be held for one or two terms of five years. The current secretary-general is Ant\u00f3nio Guterres of Portugal, who replaced Ban Ki-moon in 2017.\nInternational Court of Justice.\nThe International Court of Justice (or ICJ), sometimes known as the World Court, is the primary judicial organ of the UN. It is the successor to the Permanent Court of International Justice and occupies the body's former headquarters in the Peace Palace in The Hague, Netherlands, making it the only principal organ not based in New York City. The ICJ's main function is adjudicating disputes among nations. Examples of issues they have heard include war crimes, violations of state sovereignty and ethnic cleansing. The court can also be called upon by other UN organs to provide advisory opinions on matters of international law. All UN member states are parties to the ICJ Statute, which forms an integral part of the UN Charter, and non-members may also become parties. The ICJ's rulings are binding upon parties and, along with its advisory opinions, serve as sources of international law. The court is composed of 15 judges appointed to nine-year terms by the General Assembly. Every sitting judge must be from a different nation.\nEconomic and Social Council.\nThe Economic and Social Council (or the ECOSOC) assists the General Assembly in promoting international economic and social co-operation and development. It was established to serve as the UN's primary forum for global issues and is the largest and most complex UN body. The ECOSOC's functions include gathering data, conducting studies and advising and making recommendations to member states. Its work is carried out primarily by subsidiary bodies focused on a wide variety of topics. These include the United Nations Permanent Forum on Indigenous Issues, which advises UN agencies on issues relating to indigenous peoples, the United Nations Forum on Forests, which coordinates and promotes sustainable forest management, the United Nations Statistical Commission, which co-ordinates information-gathering efforts between agencies, and the Commission on Sustainable Development, which co-ordinates efforts between UN agencies and NGOs working towards sustainable development. ECOSOC may also grant consultative status to non-governmental organizations. As of April 2021[ [update]] almost 5,600 organizations have this status.\nSpecialized agencies.\nThe UN Charter stipulates that each primary organ of the United Nations can establish various specialized agencies to fulfill its duties. Specialized agencies are autonomous organizations working with the United Nations and each other through the coordinating machinery of the Economic and Social Council. Each was integrated into the UN system through an agreement with the UN under UN Charter article 57. There are fifteen specialized agencies, which perform functions as diverse as facilitating international travel, preventing and addressing pandemics, and promoting economic development.\nFunds, programmes, and other bodies.\nThe United Nations system includes a myriad of autonomous, separately administered funds, programmes, research and training institutes, and other subsidiary bodies. Each of these entities have their own area of work, governance structure, and budgets such as the World Trade Organization (or the WTO) and the International Atomic Energy Agency (or the IAEA), operate independently of the UN but maintain formal partnership agreements. The UN performs much of its humanitarian work through these institutions, such as preventing famine and malnutrition (the World Food Programme), protecting vulnerable and displaced people (the UNHCR), and combating the HIV/AIDS pandemic (the UNAIDS).\nMembership.\nAll the world's undisputed independent states are members of the United Nations. South Sudan, which joined 14 July 2011, is the most recent addition, bringing a total of 193 UN member states. The UN Charter outlines the membership rules:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;# Membership in the United Nations is open to all other peace-loving states that accept the obligations contained in the present Charter and, in the judgment of the Organization, are able and willing to carry out these obligations.\nIn addition, there are two non-member observer states: the Holy See and Palestine. The Cook Islands and Niue, both states in free association with New Zealand, are full members of several UN specialized agencies and have had their \"full treaty-making capacity\" recognized by the Secretariat.\nIndonesia was the first and the only nation that attempted to withdraw its membership from the United Nations, in protest to the election of Malaysia as a non-permanent member of the Security Council in 1965 during conflict between the two countries. After forming CONEFO as a short-lived rival to the UN, Indonesia resumed its membership in 1966.\nGroup of 77.\nThe Group of 77 (or the G77) at the UN is a loose coalition of developing nations that is designed to promote its members' collective economic interests and create an enhanced joint negotiating capacity in the UN. Seventy-seven nations founded the organization, but by November 2013 the organization had since expanded to 133 member countries. The group was founded 15 June 1964 by the \"Joint Declaration of the Seventy-Seven Countries\" issued at the United Nations Conference on Trade and Development (or the UNCTAD). The group held its first major meeting in Algiers in 1967, where it adopted the Charter of Algiers and established the basis for permanent institutional structures. With the adoption of the New International Economic Order by developing countries in the 1970s, the work of the G77 spread throughout the UN system. Similar groupings of developing states also operate in other UN agencies, such as the Group of 24 (or the G-24), which operates in the IMF on monetary affairs.\nObjectives.\nThe overarching strategy of the United Nations is captured in the United Nations Common Agenda.\nPeacekeeping and security.\nThe UN, after approval by the Security Council, sends peacekeepers to regions where armed conflict has recently ceased or paused to enforce the terms of peace agreements and to discourage combatants from resuming hostilities. Since the UN does not maintain its own military, peacekeeping forces are voluntarily provided by member states. These soldiers are sometimes nicknamed \"Blue Helmets\" because they wear distinctive blue helmets. Peacekeeping forces as a whole received the Nobel Peace Prize in 1988.\nThe UN has carried out 71 peacekeeping operations since 1947, and As of April 2021[ [update]], over 88,000 peacekeeping personnel from 121 nations have been deployed on missions. The largest is the United Nations Mission in South Sudan (or UNMISS), which has close to 19,200 uniformed personnel, and the smallest, the United Nations Military Observer Group in India and Pakistan (or UNMOGIP), consists of 113 civilians and experts charged with monitoring the ceasefire in Jammu and Kashmir. UN peacekeepers with the United Nations Truce Supervision Organization (or UNTSO) have been stationed in the Middle East since 1948, the longest-running active peacekeeping mission.\nA study by the RAND Corporation in 2005 found the UN to be successful in two-thirds of its peacekeeping efforts. It compared efforts at nation-building by the UN to those of the United States and found that 87.5% of UN cases are at peace, as compared with 50% of U.S. cases at peace. Also in 2005, the Human Security Report documented a decline in the number of wars, genocides, and human rights abuses since the end of the Cold War, and presented evidence, albeit circumstantial, that international activism \u2013 mostly spearheaded by the UN \u2013 has been the main cause of the decline in armed conflict. Situations in which the UN has not only acted to keep the peace but also intervened include the Korean War and the authorization of intervention in Iraq after the Gulf War. Further studies published between 2008 and 2021 determined UN peacekeeping operations to be more effective at ensuring long-lasting peace and minimizing civilian casualties.\nThe UN has also drawn criticism for perceived failures. In many cases, member states have shown reluctance to achieve or enforce Security Council resolutions. Disagreements in the Security Council about military action and intervention are seen as having failed to prevent the Bangladesh genocide in 1971, the Cambodian genocide in the 1970s, and the Rwandan genocide in 1994. Similarly, UN inaction is blamed for failing to either prevent the Srebrenica massacre or complete the peacekeeping operations during the Somali Civil War. UN peacekeepers have also been accused of child rape, soliciting prostitutes, and sexual abuse during various peacekeeping missions in the Democratic Republic of the Congo, Haiti, Liberia, Sudan, Burundi, and C\u00f4te d'Ivoire. Scientists cited UN peacekeepers from Nepal as the source of the 2010s Haiti cholera outbreak, which killed more than 8,000 people.\nIn addition to peacekeeping, the UN is also active in encouraging disarmament. Regulation of armaments was included in the writing of the UN Charter in 1945 and was envisioned as a way of limiting the use of human and economic resources for their creation. The advent of nuclear weapons came only weeks after the signing of the charter, resulting in the first resolution of the first General Assembly meeting calling for specific proposals for \"the elimination from national armaments of atomic weapons and of all other major weapons adaptable to mass destruction\". The UN has been involved with arms-limitation treaties such as the Outer Space Treaty, the Treaty on the Non-Proliferation of Nuclear Weapons, the Seabed Arms Control Treaty, the Biological Weapons Convention, the Chemical Weapons Convention, and the Ottawa Treaty. Three UN bodies oversee arms proliferation issues: the International Atomic Energy Agency, the Organisation for the Prohibition of Chemical Weapons and the Comprehensive Nuclear-Test-Ban Treaty Organization Preparatory Commission. Additionally, many peacekeeping missions focus on disarmament: several operations in West Africa disarmed roughly 250,000 former combatants and secured tens of thousands of weapons and millions of munitions.\nHuman rights.\nOne of the UN's primary purposes is \"promoting and encouraging respect for human rights and for fundamental freedoms for all without distinction as to race, sex, language, or religion\", and member states pledge to undertake \"joint and separate action\" to protect these rights.\nIn 1948, the General Assembly adopted a Universal Declaration of Human Rights, drafted by a committee headed by American diplomat and activist Eleanor Roosevelt, and including the French lawyer Ren\u00e9 Cassin. The document proclaims basic civil, political and economic rights common to all human beings, though its effectiveness towards achieving these ends has been disputed since its drafting. The Declaration serves as a \"common standard of achievement for all people and all nations\" rather than a legally binding document, but it has become the basis of two binding treaties, the International Covenant on Civil and Political Rights and the International Covenant on Economic, Social and Cultural Rights. In practice, the UN is unable to take significant action against human rights abuses without a Security Council resolution, though it does substantial work in investigating and reporting abuses.\nIn 1979, the General Assembly adopted the Convention on the Elimination of All Forms of Discrimination against Women; followed by the Convention on the Rights of the Child in 1989. With the end of the Cold War, the push for human rights action took on new impetus. The United Nations Commission on Human Rights was formed in 1993 to oversee human rights issues for the UN, following the recommendation of that year's World Conference on Human Rights. Jacques Fomerand, a scholar of the UN, describes the organization's mandate as \"broad and vague\", with only \"meagre\" resources to carry it out. In 2006, it was replaced by a Human Rights Council consisting of 47 nations. Also in 2006, the General Assembly passed a Declaration on the Rights of Indigenous People, and in 2011 it passed its first resolution recognizing the rights of members of the LGBTQ+ community.\nOther UN bodies responsible for women's rights issues include the United Nations Commission on the Status of Women, the United Nations Development Fund for Women and the United Nations International Research and Training Institute for the Advancement of Women. The UN Permanent Forum on Indigenous Issues, one of three bodies with a mandate to oversee issues related to indigenous peoples, held its first session in 2002.\nEconomic development and humanitarian assistance.\nAnother primary purpose of the UN is \"to achieve international co-operation in solving international problems of an economic, social, cultural and humanitarian character\". Numerous bodies have been created to work towards this goal, primarily under the authority of the General Assembly and the ECOSOC. In 2000, the 192 UN member states agreed to achieve eight Millennium Development Goals by 2015. The Sustainable Development Goals were launched in 2015 to succeed the Millennium Development Goals. The SDGs have an associated financing framework called the Addis Ababa Action Agenda.\nThe UN Development Programme (or the UNDP), an organization for grant-based technical assistance, is one of the leading bodies in the field of international development. The organization also publishes the UN Human Development Index, a comparative measure ranking countries by poverty, literacy, education, life expectancy, and other factors. The Food and Agriculture Organization (or the FAO) promotes agricultural development and food security. The United Nations Children's Fund (or UNICEF) was created in 1946 to aid European children after the Second World War and expanded its mission to provide aid around the world and to uphold the convention on the Rights of the Child.\nThe World Bank Group and the International Monetary Fund (or the IMF) are independent, specialized agencies and observers within the UN framework. They were initially formed separately from the UN through the Bretton Woods Agreement. The World Bank provides loans for international development, while the IMF promotes international economic co-operation and gives emergency loans to indebted countries.\nThe World Health Organization (or WHO), which focuses on international health issues and disease eradication, is another of the UN's largest agencies. In 1980, the agency announced that the eradication of smallpox had been completed. In subsequent decades, WHO eradicated polio, river blindness, and leprosy. The Joint United Nations Programme on HIV/AIDS (or UNAIDS) coordinated the organization's response to the AIDS epidemic. The UN Population Fund, which also dedicates part of its resources to combating HIV, is the world's largest source of funding for reproductive health and family planning services.\nAlong with the International Red Cross and Red Crescent Movement, the UN takes a leading role in coordinating emergency relief. The World Food Programme (or the WFP) provides food aid in response to famine, natural disasters, and armed conflict. The organization feeds an average of 90 million people in 80 nations per year. The Office of the United Nations High Commissioner for Refugees (or the UNHCR) works to protect the rights of refugees, asylum seekers and stateless people. The UNHCR and the WFP programmes are funded by voluntary contributions from governments, corporations, and individuals, though the UNHCR's administrative costs are paid for by the UN's primary budget.\nEnvironment and climate.\nBeginning with the formation of the UN Environmental Programme (or the UNEP) in 1972, the UN has made environmental issues a prominent part of its agenda. A lack of success in the first two decades of UN work in this area led to the Earth Summit in Rio de Janeiro, Brazil, in 1992; which sought to give new impetus to these efforts. In 1988, the UNEP and the World Meteorological Organization (or the WMO), another UN organization, established the Intergovernmental Panel on Climate Change, which assesses and reports on research on global warming. The UN-sponsored Kyoto Protocol set legally binding emissions reduction targets for ratifying states.\nOther global issues.\nSince the UN's creation, over 80 colonies have attained independence. The General Assembly adopted the Declaration on the Granting of Independence to Colonial Countries and Peoples in 1960 with no votes against but abstentions from all major colonial powers. The UN works towards decolonization through groups including the UN Committee on Decolonization. The committee lists seventeen remaining \"non-self-governing territories\", the largest and most populous of which is the Western Sahara.\nThe UN also declares and co-ordinates international observances that bring awareness to issues of international interest or concern; examples include World Tuberculosis Day, Earth Day, and the International Year of Deserts and Desertification.\nStarting in 2023, the https:// has organized an annual UN Open Source Week to facilitate collaborative and international technological projects, AI policies, and governance.\nFunding.\n&lt;templatestyles src=\"Template:Bar chart/styles.css\"/&gt;\nThe UN budget for 2024 was $3.59 billion, not including additional resources donated by members, such as peacekeeping forces. Including specialized agencies of the UN, the UN System Chief Executives Board for Coordination reports total expenses of $67.4 billion in 2022 for 43 United Nations entities.\nThe UN is financed from assessed and voluntary contributions from its member states. The General Assembly approves the regular budget and determines the assessment for each member. This is broadly based on the relative capacity of each nation to pay, as measured by its gross national income (or GNI), with adjustments for external debt and low per capita income.\nThe Assembly has established the principle that the UN should not be unduly dependent on any one member to finance its operations. Thus, there is a \"ceiling\" rate, setting the maximum amount that any member can be assessed for the regular budget. In December 2000, the Assembly revised the scale of assessments in response to pressure from the United States. As part of that revision, the regular budget ceiling was reduced from 25% to 22%. For the least developed countries (or LDCs), a ceiling rate of 0.01% is applied. In addition to the ceiling rates, the minimum amount assessed to any member nation (or \"floor\" rate) is set at 0.001% of the UN budget ($31,000 for the two-year budget 2021\u20132022).\nA large share of the UN's expenditure addresses its core mission of peace and security, and this budget is assessed separately from the main organizational budget. The peacekeeping budget for the 2021\u20132022 fiscal year is $6.38 billion, supporting 66,839 personnel deployed in 12 missions worldwide. UN peace operations are funded by assessments, using a formula derived from the regular funding scale that includes a weighted surcharge for the five permanent Security Council members, who must approve all peacekeeping operations. This surcharge serves to offset discounted peacekeeping assessment rates for less developed countries. The largest contributors to the UN peacekeeping budget for 2023\u20132024 are: the United States (26.94%), China (18.68%), Japan (8.03%), Germany (6.11%), the United Kingdom (5.35%), France (5.28%), Italy (3.18%), Canada (2.62%), South Korea (2.57%) and Russia (2.28%).\nSpecial UN programmes not included in the regular budget, such as UNICEF and the World Food Programme, are financed by voluntary contributions from member governments, corporations, and private individuals.\nAssessments and reviews.\nSeveral studies have examined the Security Council's responsiveness to armed conflict. Findings suggests that the council is more likely to meet and deliberate on conflicts that are more intense and have led to more humanitarian suffering, but that its responsiveness is also shaped by the political interests of member states and in particular of the permanent members.\nUN peacekeeping missions are assessed to be generally successful. A book looking at 47 peace operations by Virginia Page Fortna of Columbia University found that UN-led conflict resolution usually resulted in long-term peace.\nPolitical scientists Hanne Fjelde, Lisa Hultman and Desiree Nilsson of Uppsala University studied twenty years of data on peacekeeping missions, concluding that they were more effective at reducing civilian casualties than counterterrorism operations by nation states.\nGeorgetown University professor Lise Howard postulates that UN peacekeeping operations are more effective due to their emphasis on \"verbal persuasion, financial inducements and coercion short of offensive military force, including surveillance and arrest\", which are likelier to change the behavior of warring parties.\nBritish historian Paul Kennedy states that while the organization has suffered some major setbacks, \"when all its aspects are considered, the UN has brought great benefits to our generation and will bring benefits to our children's and grandchildren's generations as well.\"\nIn 2012, then French President Fran\u00e7ois Hollande stated that \"France trusts the United Nations. She knows that no state, no matter how powerful, can solve urgent problems, fight for development and bring an end to all crises. France wants the UN to be the centre of global governance\". In his 1953 address to the United States Committee for United Nations Day, American President Dwight D. Eisenhower expressed his view that, for all its flaws, \"the United Nations represents man's best organized hope to substitute the conference table for the battlefield\".\nJacques Fomerand, a professor in political sciences, writes that the \"accomplishments of the United Nations in the last 60 years are impressive in their own terms. Progress in human development during the 20th century has been dramatic, and the UN and its agencies have certainly helped the world become a more hospitable and livable place for millions\".\nReviewing the first 50 years of the UN's history, the author Stanley Meisler writes that \"the United Nations never fulfilled the hopes of its founders, but it accomplished a great deal nevertheless\", citing its role in decolonization and its many successful peacekeeping efforts.\nAwards.\nA number of agencies and individuals associated with the UN have won the Nobel Peace Prize in recognition of their work. Two secretaries-general, Dag Hammarskj\u00f6ld and Kofi Annan, were each awarded the prize; as were Ralph Bunche, a UN negotiator, Ren\u00e9 Cassin, a contributor to the Universal Declaration of Human Rights, and the American Secretary of State Cordell Hull for his role in the organization's founding. Lester B. Pearson, the Canadian Secretary of State for External Affairs, was awarded the prize in 1957 for his role in organizing the UN's first peacekeeping force to resolve the Suez Crisis.\nUNICEF won the prize in 1965, the International Labour Organization in 1969, the UN Peacekeeping Forces in 1988, the International Atomic Energy Agency (which reports to the UN) in 2005, and the UN-supported Organisation for the Prohibition of Chemical Weapons in 2013. The UN High Commissioner for Refugees was awarded the prize in 1954 and 1981, becoming one of only two recipients to win the prize twice. The UN as a whole was awarded the prize in 2001, sharing it with Annan. In 2007, the IPCC received the prize \"for their efforts to build up and disseminate greater knowledge about man-made climate change, and to lay the foundations for the measures that are needed to counteract such change.\"\nOn March 21, 2025, the joint Universities of Leuven and Louvain (Belgium) awarded the UNO an honorary degree which was given in the hands of Ant\u00f3nio Guterres.\nCriticism.\nRole.\nIn a sometimes-misquoted statement, American President George W. Bush stated in February 2003\u2014referring to UN uncertainty towards Iraqi provocations under the Saddam Hussein regime\u2014that \"free nations will not allow the UN to fade into history as an ineffective, irrelevant debating society.\"\nIn 2020, former American President Barack Obama, in his memoir \"A Promised Land\" noted, \"In the middle of the Cold War, the chances of reaching any consensus had been slim, which is why the UN had stood idle as Soviet tanks rolled into Hungary or U.S. planes dropped napalm on the Vietnamese countryside. Even after the Cold War, divisions within the Security Council continued to hamstring the UN's ability to tackle problems. Its member states lacked either the means or the collective will to reconstruct failing states like Somalia, or prevent an ethnic slaughter in places like Sri Lanka.\"\nSince its founding, there have been many calls for reform of the UN but little consensus on how to do so. Some want the UN to play a greater or more effective role in world affairs, while others want its role reduced to humanitarian work.\nRepresentation and structure.\nCore features of the UN apparatus, such as the veto privileges of some nations in the Security Council, are often described as fundamentally undemocratic, contrary to the UN mission, and a main cause of inaction on genocides and crimes against humanity.\nJacques Fomerand state that the most enduring divide in views of the UN is \"the North\u2013South split\" between richer Northern nations and developing Southern nations. Southern nations tend to favour a more empowered UN with a stronger General Assembly, allowing them a greater voice in world affairs, while Northern nations prefer an economically laissez-faire UN that focuses on transnational threats such as terrorism.\nThere have been numerous calls for the UN Security Council's membership to be increased, for different ways of electing the UN's secretary-general, and for a UN Parliamentary Assembly (UNPA).\nExclusion of nations.\nAfter World War II, the French Committee of National Liberation was late to be recognized by the United States as the government of France, and so the country was initially excluded from the conferences that created the new organization. Future French president Charles de Gaulle criticized the UN, famously calling it a \"machin\" (contraption), and was not convinced that a global security alliance would help maintain world peace, preferring direct defence treaties between countries.\nFollowing the Chinese Civil War, the government of China was disputed between the Chinese Nationalist Party and the Chinese Communist Party. After the foundation of the People's Republic of China (PRC) on 1 October 1949, the government of the Republic of China (ROC) retreated to the island of Taiwan, continuing to claim that it was the sole government of China. After the civil war, the United Nations continued recognizing the ROC as the official government of China. In 1971, amid growing debate over the representation of the Chinese people on the mainland, the General Assembly passed a resolution recognizing the PRC as \"the only legitimate representatives of China to the United Nations.\" Critics allege that this position reflects a failure of the organization's development goals and guidelines, and it garnered renewed scrutiny during the COVID-19 pandemic, when Taiwan was denied membership into the World Health Organization despite its relatively effective response to the virus. Support for Taiwan's inclusion in the UN remains challenged by the People's Republic of China, which claims the territories controlled by Taiwan as their own territory.\nIndependence.\nThroughout the Cold War, both the United States and the Soviet Union repeatedly accused the UN of favouring the other. In 1950, the Soviet Union boycotted the organization in protest to China's seat at the UN Security Council being given to the anti-communist Republic of China. Three years later, the Soviets effectively forced the resignation of UN Secretary-General Trygve Lie by refusing to acknowledge his administration due to his support of the Korean War.\nIronically, the United States had simultaneously scrutinized the UN for employing communists and Soviet sympathizers, following a high-profile accusation that Alger Hiss, an American who had taken part in the establishment of the UN, had been a Soviet spy. American Senator Joseph McCarthy claimed that the UN Secretariat under Secretary-General Lie harboured American communists, leading to further pressure that the UN chief resign. The United States saw nascent opposition to the UN in the 1960s, particularly amongst conservatives, with groups such as the John Birch Society stating that the organization was an instrument for communism. Popular opposition to the UN was expressed through bumper stickers and signs with slogans such as \"Get the U.S. out of the U.N. and the U.N. out of the U.S.!\" and \"You can't spell communism without U.N.\"\nNational sovereignty.\nIn the United States, there were concerns about supposed threats to national sovereignty, most notably promoted by the John Birch Society, which mounted a nationwide campaign in opposition to the UN during the 1960s.\nBeginning in the 1990s, the same concern appeared with the American Sovereignty Restoration Act, which has been introduced multiple times in the United States Congress. In 1997, an amendment containing the bill received a floor vote, with 54 representatives voting in favour. The 2007 version of the bill () was authored by U.S. Representative Ron Paul, to effect the United States' withdrawal from the United Nations. It would repeal various laws pertaining to the UN, terminate authorization for funds to be spent on the UN, terminate UN presence on American property, and withdraw diplomatic immunity for UN employees. It would provide up to two years for the United States to withdraw. The \"Yale Law Journal\" cited the Act as proof that \"the United States's complaints against the United Nations have intensified.\" The most recent iteration, As of 2022[ [update]], is H.R.7806, introduced by Mike D. Rogers.\nAlleged pro-Palestinian bias.\nThe UN's attention to Israel's treatment of Palestinians has been considered excessive by Israeli diplomat Dore Gold and pro-Israeli individuals and organisations such as British scholar Robert S. Wistrich, American legal scholar Alan Dershowitz, Australian politician Mark Dreyfus, and the Anti-Defamation League. The UNHRC has likewise been accused of anti-Israel bias by Ex-President of the United States George W. Bush, who complained that the Council focused too much attention on Israel and not enough on adversaries of the US such as Cuba, Venezuela, North Korea and Iran.\nAmerican state lawmakers have proposed legislation to block various UN programs deemed to threaten U.S. sovereignty. In 2023, Tennessee enacted legislation to block the implementation of programs \"originating in, or traceable to, the United Nations or a subsidiary entity of the United Nations,\" including Agenda 21 and the 2030 Agenda. In her confirmation hearing before the Senate panel to be the U.S. ambassador to the United Nations, Elise Stefanik, described the UN's attitude toward Israel as \"anti-semitic\".\nEffectiveness.\nAccording to international relations scholar Edward Luck, the United States has preferred a feeble United Nations in major projects undertaken by the organization to forestall UN interference with, or resistance to, American policies. \"The last thing the U.S. wants is an independent UN throwing its weight around\", Luck said. Similarly, former US Ambassador to the United Nations Daniel Patrick Moynihan explained that \"The Department of State desired that the United Nations prove utterly ineffective in whatever measures it undertook. The task was given to me, and I carried it forward with not inconsiderable success.\"\nIn 1994, former special representative of the secretary-general of the UN to Somalia Mohamed Sahnoun published \"Somalia: The Missed Opportunities\", a book in which he analyses the reasons for the failure of the 1992 UN intervention in Somalia. Sahnoun claims that between the start of the Somali civil war in 1988 and the fall of the Siad Barre regime in January 1991, the UN missed at least three opportunities to prevent major human tragedies. When the UN tried to provide humanitarian assistance, they were totally outperformed by NGOs, whose competence and dedication sharply contrasted with the UN's excessive caution and bureaucratic inefficiencies. Sahnoun warned that if radical reform were not undertaken, then the UN would continue to respond to such crises with inept improvisation.\nBeyond specific instances or areas of alleged ineffectiveness, some scholars debate the overall effectiveness of the UN. Adherents to the realist school of international relations take a pessimistic position, arguing that the UN is not an effective organization because it is dominated and constrained by great powers. Liberal scholars counter that it is an effective organization because it has proved capable of solving many problems by working around the restrictions imposed by powerful member states. The UN is generally considered by scholars to be more effective in realms such as public health, and humanitarian assistance. The ineffectiveness of enforcing territorial integrity in the 21st century have led to debate on possible re-emergence of the right of conquest.\nInefficiency and corruption.\nCritics have also accused the UN of bureaucratic inefficiency, waste, and corruption. In 1976, the General Assembly established the Joint Inspection Unit to seek out inefficiencies within the UN system. During the 1990s, the United States withheld dues citing inefficiency and only started repayment on the condition that a major reforms initiative be introduced. In 1994, the Office of Internal Oversight Services (or the OIOS) was established by the General Assembly to serve as an efficiency watchdog.\nIn 2004, the UN faced accusations that its recently ended Oil-for-Food Programme \u2014 in which Iraq had been allowed to trade oil for basic needs to relieve the pressure of sanctions \u2014 had suffered from widespread corruption, including billions of dollars of kickbacks. An independent inquiry created by the UN found that many of its officials had been involved in the scheme, and raised significant questions about the role of Kojo Annan, the son of Kofi Annan.\nHymn to the United Nations.\nOn the request of then United Nations Secretary-General U Thant, a Hymn to the United Nations was performed on the occasion of its 26th anniversary, on 24 October 1971, by Pau Casals, the lyrics to which were penned by the poet W. H. Auden.\nThant first approached Casals, who was a personal friend, looking to create a hymn to peace and hoping for the song to be based on the preamble of the Charter of the United Nations. Thant later commissioned Auden to write the poem after Casals requested one to set to music. Auden completed his work in three days time. The finished work, scored for chorus and orchestra, takes approximately seven minutes to play. However, there were never any plans to adopt the song as the organization's official anthem.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\nCitations.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nBibliography.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;\nFurther reading.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "31770", "revid": "21878292", "url": "https://en.wikipedia.org/wiki?curid=31770", "title": "List of ships named USS Arizona", "text": "Arizona has been the name of three ships of the United States Navy and will be the name of a future submarine. \nSee also.\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n List of ships with the same or similar names\nThis article includes a with the same or similar names. If an [ internal link] for a specific ship led you here, you may wish to change the link to point directly to the intended ship article, if one exists."}
{"id": "31771", "revid": "50860679", "url": "https://en.wikipedia.org/wiki?curid=31771", "title": "Urban legend", "text": "Form of modern folklore\nUrban legend (sometimes modern legend, urban myth, or simply legend) is a genre of folklore concerning stories about an unusual (usually scary) or humorous event that many people believe to be true but largely are not.\nThese legends can be entertaining but often concern mysterious and troubling events, such as disappearances and strange objects or entities. Urban legends may confirm moral standards, reflect prejudices, or be a way to make sense of societal anxieties. \nIn the past, urban legends were most often circulated orally, at gatherings and around the campfire for instance. Now, they can be spread by any media, including newspapers, mobile news apps, e-mail, and most often, social media. Some urban legends have passed through the years/decades with only minor changes, in where the time period takes place. Generic urban legends are often altered to suit regional variations, but the lesson or moral generally remains the same.\nOrigin and structure.\nThe term \"urban legend\", as used by folklorists, has appeared in print since at least 1968, when it was used by Richard Dorson. Jan Harold Brunvand, professor of English at the University of Utah, introduced the term to the general public in a series of popular books published beginning in 1981. Brunvand used his collection of legends, \"The Vanishing Hitchhiker: American Urban Legends &amp; Their Meanings\" (1981) to make two points: first, that legends and folklore do not occur exclusively in so-called primitive or traditional societies, and second, that one could learn much about urban and modern culture by studying such tales.\nMany urban legends are framed as complete stories with plot and characters. The compelling appeal of a typical urban legend is its elements of mystery, horror, fear, or humor. Often they serve as cautionary tales. Some urban legends are morality tales that depict someone acting in a disagreeable manner, only to wind up in trouble, hurt, or dead.\nUrban legends will often try to invoke a feeling of disgust in the reader which tends to make these stories more memorable and potent. Elements of shock value can be found in almost every form of urban legend and are partially what makes these tales so impactful. An urban legend may include elements of the supernatural or paranormal.\nPropagation and belief.\nAs Jan Brunvand points out, antecedent legends including some of the motifs, themes and symbolism of the urtexts can readily be identified. Cases that may have been at least partially inspired by real events include \"The Death Car\" (traced by Richard Dorson to Michigan, United States); \"the Solid Cement Cadillac\" and the possible origin of \"The Hook\" in the 1946 series of Lovers' Lane murders in Texarkana, Texas, United States. The urban legend that Coca-Cola developed the drink Fanta to sell in Nazi Germany without public backlash originated as the actual tale of German Max Keith, who invented the drink and ran Coca-Cola's operations in Germany during World War II.\nThe narrator of an urban legend may claim it happened to a friend (or to a friend of a friend), which serves to personalize, authenticate and enhance the power of the narrative while distancing the teller from the tall tale. Many urban legends depict horrific crimes, contaminated foods, or other situations that would potentially affect many people. Anyone believing such stories might feel compelled to warn loved ones. On occasion, news organizations, school officials and even police departments have issued warnings concerning the latest threat. According to the \"Lights Out\" rumor, street gang members would drive without headlights until a compassionate motorist responded with the traditional flashing of headlights, whereupon a prospective new gang member would have to murder the citizen as a requirement of initiation. A fax retelling this legend received at the Nassau County, Florida, fire department was forwarded to police, and from there to all city departments. The Minister of Defence for Canada was taken in by it also; he forwarded an urgent security warning to all Ontario Members of Parliament.\nUrban legends typically include common elements: the tale is retold on behalf of the original witness or participant; dire warnings are often given for those who might not heed the advice or lesson contained therein (a typical element of many e-mail phishing scams); and the tale is often touted as \"something a friend told me\", the friend being identified by first name only or not identified at all. Such legends seem to be believable and even provocative, as some readers are led in turn to pass them on, including on social media platforms that instantly reach millions worldwide. Many are essentially extended jokes, told as if they were true events.\nPersistent urban legends do often maintain a degree of plausibility, as in the story a serial killer deliberately hiding in the back seat of a car. Another such example since the 1970s has been the recurring rumor that the Procter &amp; Gamble Company was associated with Satan-worshippers because of details within its 19th-century \"57\" trademark. The legend interrupted the company's business to the point that it stopped using the trademark.\nRelation to mythology.\nThe earliest term by which these narratives were known, \"urban belief tales\", highlights what was then thought of as a key property: their tellers regarded the stories as true accounts, and the device of the FOAF (acronym for \"Friend of a Friend\" invented by English writer and folklorist Rodney Dale in 1976) was a spurious but significant effort at authentication. The coinage leads in turn to the terms \"FOAFlore\" and \"FOAFtale\". While at least one classic legend, the \"Death Car\", has been shown to have some basis in fact, folklorists have an interest in debunking those narratives only to the degree that establishing non-factuality warrants the assumption that there must be some other reason why the tales are told, re-told and believed. As in the case of myth, the narratives are believed because they construct and reinforce the worldview of the group within which they are told, or \"because they provide us with coherent and convincing explanations of complex events\".\nSocial scientists have started to draw on urban legends in order to help explain complex socio-psychological beliefs, such as attitudes to crime, childcare, fast food, SUVs and other \"family\" choices. The authors make an explicit connection between urban legends and popular folklore, such as \"Grimm's Fairy Tales\", where similar themes and motifs arise. For that reason, it is characteristic of groups within which a given narrative circulates to vehemently reject claims or demonstrations of non-factuality; an example would be the expressions of outrage by police officers who are told that adulteration of Halloween treats by strangers (the subject of periodic moral panics) occurs extremely rarely, if at all.\nDocumentation.\nThe Internet has made it easier both to spread and to debunk urban legends. For instance, the Usenet newsgroup \"alt.folklore.urban\" and several other websites, most notably snopes.com, focus on discussing, tracking, and analyzing urban legends. The United States Department of Energy had a now-discontinued service called Hoaxbusters that dealt with computer-distributed hoaxes and legends. The most notable such hoaxes are known as creepypastas, which are typically horror stories written anonymously. Although most are regarded as obviously false, some, such as the Slender Man, have gained a following of people that do believe in them. \nTelevision shows such as \"Urban Legends\", ', and later ', feature re-enactments of urban legends, detailing the accounts of the tales and (typically later in an episode) revealing any factual basis they may have. The Discovery Channel TV show \"MythBusters\" (2003\u20132016) tried to prove or disprove several urban legends by attempting to reproduce them using the scientific method.\nThe 1998 film \"Urban Legend\" featured students discussing popular urban legends while at the same time falling victim to killings re-enacting them. The 1999 film \"The Blair Witch Project\" purposefully positioned itself as an urban legend to gain viral hype and succeeded in fooling many that it was based on a real disappearance. The lack of widespread social media and search engines helped it proliferate in the months leading up to its release.\nBetween 1992 and 1998 \"The Guardian\" newspaper \"Weekend\" section published the illustrated \"Urban Myths\" column by Phil Healey and Rick Glanvill, with content taken from a series of four books: \"Urban Myths\", \"The Return of Urban Myths\", \"Urban Myths Unplugged\", and \"Now! That's What I Call Urban Myths\". The 1994 comics anthology the \"Big Book of Urban Legends\", written by Robert Boyd, Jan Harold Brunvand, and Robert Loren Fleming, featured 200 urban legends, displayed as comics.\nThe British writer Tony Barrell has explored urban legends in a long-running column in \"The Sunday Times\". These include the story that Orson Welles began work on a Batman movie in the 1940s, which was to feature James Cagney as the Riddler and Marlene Dietrich as Catwoman; the persistent rumour that the rock singer Courtney Love is the granddaughter of Marlon Brando; and the idea that a famous 1970s poster of Farrah Fawcett contains a subliminal sexual message concealed in the actress's hair.\nGenres.\nCrime.\nAs with traditional urban legends, many internet rumors are about crimes or crime waves \u2013 either fictional or based on real events that have been largely exaggerated. Such stories can be problematic, both because they purport to be relevant modern news and because they do not follow the typical patterns of urban legends.\nMedicine.\nSome legends are medical folklore, such as the claim that eating watermelon seeds will result in a watermelon growing in the stomach, or that going outdoors just after showering will result in catching a cold. Many old wives' tales have grown around the identification of ailments, real and imagined, and the recommended remedies, rituals, and home-grown medical treatments to treat them.\nInternet.\nInternet urban legends are those spread through the internet, as through Usenet or email or more recently through other social media. They can also be linked to viral online content. Some take the form of chain letters and spread by e-mail, directing the reader to share them or to meet a terrible fate, and following a recognizable outline of hook, threat, and finally request.\nParanormal.\nParanormal urban-legend stories usually involve someone encountering something supernatural, such as a cryptid\u2014for instance, Bigfoot or Mothman, legendary creatures for which evidence is lacking but which have legions of believers. Research shows that people experiencing sudden or surprising events (such as a Bigfoot sighting) may significantly overestimate the duration of the event.\nMarketing.\nCompanies have been accused of hiding \"secret messages\" behind their logos or packaging, as in the case of the old Procter &amp; Gamble symbol, supposedly an occult figure that gave panache to the brand. (If the thirteen stars in the symbol were connected a certain way, it would show three sixes in a row or looked at the 3 curls at the bottom they form the inverted 6s.) Similarly, a video of a Christian woman \"exposing\" Monster Energy for using the Hebrew letter \"vav\" ( \u05d5 ), forming the letter \"M\", to disguise the number 666 went viral on Facebook.\nReferences.\nCitations.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
