{"id": "37080", "revid": "19014806", "url": "https://en.wikipedia.org/wiki?curid=37080", "title": "Thought", "text": "Cognitive process independent of the senses\nIn their most common sense, thought and thinking refer to cognitive processes that occur independently of direct sensory stimulation. Core forms include judging, reasoning, concept formation, problem solving, and deliberation. Other processes, such as entertaining an idea, memory, or imagination, are also frequently considered types of thought. Unlike perception, these activities can occur without immediate input from the sensory organs. In a broader sense, any mental event\u2014including perception and unconscious processes\u2014may be described as a form of thought. The term can also denote not the process itself, but the resulting mental states or systems of ideas.\nA variety of theories attempt to explain the nature of thinking. Platonism holds that thought involves discerning eternal forms and their interrelations, distinguishing these pure entities from their imperfect sensory imitations. Aristotelianism interprets thinking as instantiating the universal essence of an object within the mind, derived from sense experience rather than a changeless realm. Conceptualism, closely related to Aristotelianism, identifies thinking with the mental evocation of concepts. Inner speech theories suggest that thought takes the form of silent verbal expression, sometimes in a natural language and sometimes in a specialized \"mental language,\" or Mentalese, as proposed by the language of thought hypothesis. Associationism views thought as the succession of ideas governed by laws of association, while behaviorism reduces thinking to behavioral dispositions that generate intelligent actions in response to stimuli. More recently, computationalism compares thought to information processing, storage, and transmission in computers.\nDifferent types of thinking are recognized in philosophy and psychology. Judgement involves affirming or denying a proposition; reasoning draws conclusions from premises or evidence. Both depend on concepts acquired through concept formation. Problem solving aims at achieving specific goals by overcoming obstacles, while deliberation evaluates possible courses of action before selecting one. Episodic memory and imagination internally represent objects or events, either as faithful reproductions or novel rearrangements. Unconscious thought refers to mental activity that occurs without conscious awareness and is sometimes invoked to explain solutions reached without deliberate effort.\nThe study of thought spans many disciplines. Phenomenology examines the subjective experience of thinking, while metaphysics addresses how mental processes relate to matter in a naturalistic framework. Cognitive psychology treats thought as information processing, whereas developmental psychology explores its growth from infancy to adulthood. Psychoanalysis emphasizes unconscious processes, and fields such as linguistics, neuroscience, artificial intelligence, biology, and sociology also investigate different aspects of thought. Related concepts include the classical laws of thought (identity, non-contradiction, excluded middle), counterfactual thinking (imagining alternatives to reality), thought experiments (testing theories through hypothetical scenarios), critical thinking (reflective evaluation of beliefs and actions), and positive thinking (focusing on beneficial aspects of situations, often linked to optimism).\nDefinition.\nThe terms \"thought\" and \"thinking\" are used in different ways in psychology and philosophy. In their most common sense, they refer to conscious processes that occur independently of direct sensory input. This includes activities such as considering an idea, evaluating a preposition, or making a judgement. In this sense, memory and imagination count as forms of thought, while perception does not. In a narrower sense, only the most typical cases are called thought-specifically conscious, conceptual or linguistic processes such as judging, inferring, problem-solving, and deliberating. Sometimes, however, the terms are understood in a much broader sense to include all mental processes, conscious or unconscious. In this wide usage, they can be treated as synonymous with mind, as in the Cartesian tradition (where the mind is described as a \"thinking thing\") and in the cognitive sciences. Some accounts further add that only processes leading to intelligent behavior should count as thought. A common contrast in the literature is drawn between thinking and feeling. In this distinction, thinking is seen as a rational, dispassionate activity, while feeling involves direct emotional engagement.\nThe words thought and thinking can also refer to the results of these processes, such as beliefs, mental states, or systems of ideas held by an individual or shared within a group. Academic discussions often leave implicit which of these senses is intended.\nThe word \"thought\" derives from Old English \"\u00feoht\" or \"ge\u00feoht\", from the stem of \"\u00feencan (\"\"to conceive in the mind, consider\").\nTheories of thinking.\nMany different theories of thinking have been developed. They attempt to describe the main features and processes involved in thinking. These theories are not necessarily mutually exclusive, meaning that some of them can be combined without contradiction. \nPlatonism.\nAccording to Platonism, thinking is a spiritual activity in which the mind perceives and examines Platonic forms and their relationships. This process is described as a kind of silent inner dialogue, where the soul \"talks to itself.\" Platonic forms are understood as universals that exist in a changeless, non-physical realm, distinct from the sensory world. Examples include the forms of goodness, beauty, unity, and sameness. The challenge of thinking, in this view, lies in recognizing and distinguishing these true forms from the imperfect copies and imitations found in the physical world. For instance, one must separate the idea of beauty itself from mere beautiful objects. A central difficulty for this theory is explaining how humans can think about or learn these transcendent forms if they exist in a different realm. Plato addresses this issue with his theory of recollection, which claims that the soul was once in direct contact with the forms before birth and can therefore \"remember\" them. However, this solution relies on metaphysical assumptions that are not widely accepted in modern philosophy.\nAristotelianism and conceptualism.\nAccording to Aristotelianism, the mind thinks about an object by instantiating its essence. For example, when thinking about trees, the mind instantiates \"tree-ness.\" Unlike actual trees, this instantiation does not occur in matter but in the mind, though the universal essence is the same in both cases. In contrast with Platonism, universals are not seen as timeless forms existing in a separate intelligible realm. Instead, they exist only insofar as they are instantiated. The mind comes to recognize universals through abstraction from experience, a view that avoids some objections directed against Platonism.\nConceptualism is closely related. It holds that thinking consists in mentally evoking concepts. While some concepts may be innate, most are acquired through abstraction from sensory experience before they can be used in thought.\nCritics argue that both theories face difficulties. One problem is explaining the logical structure of thought. For instance, to think that it will either rain or snow, it is not enough to instantiate the essences of rain and snow or to evoke the relevant concepts. The disjunctive relation between them is not captured in this way. Another challenge lies in providing a clear account of how the mind acquires essences or concepts through abstraction.\nInner speech theory.\nInner speech theories hold that thinking is a form of inner speech. This position, sometimes called \"psychological nominalism,\" maintains that thinking consists of silently evoking words and connecting them to form mental sentences. A person's awareness of their own thoughts is explained as a kind of overhearing of one's silent monologue. Three central features are often associated with inner speech: it is in some sense similar to hearing sounds, it involves the use of language, and it constitutes a motor plan that could be used for actual speech. The link between thinking and language is supported by evidence that thinking is often accompanied by muscle activity in the speech organs. Such activity may facilitate thought in certain cases but is not required for thinking in general. Some versions of the theory propose that thinking does not occur in ordinary languages like English or French but in a specialized symbolic system with its own syntax. This is known as the language of thought hypothesis.\nInner speech theory has strong intuitive appeal since introspection suggests that many thoughts are accompanied by inner speech. Critics argue, however, that not all forms of thinking are linguistic. Daydreaming, for example, has been cited as a case of non-linguistic thought. This debate is significant for the question of whether animals can think. If thinking necessarily depends on language, then there is a sharp divide between humans and other animals, since only humans possess sufficiently complex languages. But if non-linguistic thought exists, then this gap may be smaller, suggesting that some animals are capable of thought as well.\nLanguage of thought hypothesis.\nThere are various theories about the relation between language and thought. One prominent version in contemporary philosophy is called the language of thought hypothesis. It states that thinking happens in the medium of a mental language. This language, often referred to as \"Mentalese\", is similar to regular languages in various respects: it is composed of words that are connected to each other in syntactic ways to form sentences. This claim does not merely rest on an intuitive analogy between language and thought. Instead, it provides a clear definition of the features a representational system has to embody in order to have a linguistic structure. On the level of syntax, the representational system has to possess two types of representations: atomic and compound representations. Atomic representations are basic whereas compound representations are constituted either by other compound representations or by atomic representations. On the level of semantics, the semantic content or the meaning of the compound representations should depend on the semantic contents of its constituents. A representational system is linguistically structured if it fulfills these two requirements.\nThe language of thought hypothesis states that the same is true for thinking in general. This would mean that thought is composed of certain atomic representational constituents that can be combined as described above. Apart from this abstract characterization, no further concrete claims are made about how human thought is implemented by the brain or which other similarities to natural language it has. The language of thought hypothesis was first introduced by Jerry Fodor. He argues in favor of this claim by holding that it constitutes the best explanation of the characteristic features of thinking. One of these features is \"productivity\": a system of representations is \"productive\" if it can generate an infinite number of unique representations based on a low number of atomic representations. This applies to thought since human beings are capable of entertaining an infinite number of distinct thoughts even though their mental capacities are quite limited. Other characteristic features of thinking include \"systematicity\" and \"inferential coherence\". Fodor argues that the language of thought hypothesis is true as it explains how thought can have these features and because there is no good alternative explanation. Some arguments against the language of thought hypothesis are based on neural networks, which are able to produce intelligent behavior without depending on representational systems. Other objections focus on the idea that some mental representations happen non-linguistically, for example, in the form of maps or images.\nComputationalists have been especially interested in the language of thought hypothesis since it provides ways to close the gap between thought in the human brain and computational processes implemented by computers. The reason for this is that processes over representations that respect syntax and semantics, like inferences according to the modus ponens, can be implemented by physical systems using causal relations. The same linguistic systems may be implemented through different material systems, like brains or computers. In this way, computers can \"think\".\nAssociationism.\nAn important view in the empiricist tradition has been associationism, the view that thinking consists in the succession of ideas or images. This succession is seen as being governed by laws of association, which determine how the train of thought unfolds. These laws are different from logical relations between the contents of thoughts, which are found in the case of drawing inferences by moving from the thought of the premises to the thought of the conclusion. Various laws of association have been suggested. According to the laws of similarity and contrast, ideas tend to evoke other ideas that are either very similar to them or their opposite. The law of contiguity, on the other hand, states that if two ideas were frequently experienced together, then the experience of one tends to cause the experience of the other. In this sense, the history of an organism's experience determines which thoughts the organism has and how these thoughts unfold. But such an association does not guarantee that the connection is meaningful or rational. For example, because of the association between the terms \"cold\" and \"Idaho\", the thought \"this coffee shop is cold\" might lead to the thought \"Russia should annex Idaho\".\nOne form of associationism is imagism. It states that thinking involves entertaining a sequence of images where earlier images conjure up later images based on the laws of association. One problem with this view is that we can think about things that we cannot imagine. This is especially relevant when the thought involves very complex objects or infinities, which is common, for example, in mathematical thought. One criticism directed at associationism in general is that its claim is too far-reaching. There is wide agreement that associative processes as studied by associationists play some role in how thought unfolds. But the claim that this mechanism is sufficient to understand all thought or all mental processes is usually not accepted.\nBehaviorism.\nAccording to behaviorism, thinking consists in behavioral dispositions to engage in certain publicly observable behavior as a reaction to particular external stimuli. On this view, having a particular thought is the same as having a disposition to behave in a certain way. This view is often motivated by empirical considerations: it is very difficult to study thinking as a private mental process but it is much easier to study how organisms react to a certain situation with a given behavior. In this sense, the capacity to solve problems not through existing habits but through creative new approaches is particularly relevant. The term \"behaviorism\" is also sometimes used in a slightly different sense when applied to thinking to refer to a specific form of inner speech theory. This view focuses on the idea that the relevant inner speech is a derivative form of regular outward speech. This sense overlaps with how behaviorism is understood more commonly in philosophy of mind since these inner speech acts are not observed by the researcher but merely inferred from the subject's intelligent behavior. This remains true to the general behaviorist principle that behavioral evidence is required for any psychological hypothesis.\nOne problem for behaviorism is that the same entity often behaves differently despite being in the same situation as before. This problem consists in the fact that individual thoughts or mental states usually do not correspond to one particular behavior. So thinking that the pie is tasty does not automatically lead to eating the pie, since various other mental states may still inhibit this behavior, for example, the belief that it would be impolite to do so or that the pie is poisoned.\nComputationalism.\nComputationalist theories of thinking, often found in the cognitive sciences, understand thinking as a form of information processing. These views developed with the rise of computers in the second part of the 20th century, when various theorists saw thinking in analogy to computer operations. On such views, the information may be encoded differently in the brain, but in principle, the same operations take place there as well, corresponding to the storage, transmission, and processing of information. But while this analogy has some intuitive attraction, theorists have struggled to give a more explicit explanation of what computation is. A further problem consists in explaining the sense in which thinking is a form of computing. The traditionally dominant view defines computation in terms of Turing machines, though contemporary accounts often focus on neural networks for their analogies. A Turing machine is capable of executing any algorithm based on a few very basic principles, such as reading a symbol from a cell, writing a symbol to a cell, and executing instructions based on the symbols read. This way it is possible to perform deductive reasoning following the inference rules of formal logic as well as simulating many other functions of the mind, such as language processing, decision making, and motor control. But computationalism does not only claim that thinking is in some sense similar to computation. Instead, it is claimed that thinking just is a form of computation or that the mind is a Turing machine.\nComputationalist theories of thought are sometimes divided into functionalist and representationalist approaches. Functionalist approaches define mental states through their causal roles but allow both external and internal events in their causal network. Thought may be seen as a form of program that can be executed in the same way by many different systems, including humans, animals, and even robots. According to one such view, whether something is a thought only depends on its role \"in producing further internal states and verbal outputs\". Representationalism, on the other hand, focuses on the representational features of mental states and defines thoughts as sequences of intentional mental states. In this sense, computationalism is often combined with the language of thought hypothesis by interpreting these sequences as symbols whose order is governed by syntactic rules.\nVarious arguments have been raised against computationalism. In one sense, it seems trivial since almost any physical system can be described as executing computations and therefore as thinking. For example, it has been argued that the molecular movements in a regular wall can be understood as computing an algorithm since they are \"isomorphic to the formal structure of the program\" in question under the right interpretation. This would lead to the implausible conclusion that the wall is thinking. Another objection focuses on the idea that computationalism captures only some aspects of thought but is unable to account for other crucial aspects of human cognition.\nTypes of thinking.\nA great variety of types of thinking are discussed in the academic literature. A common approach divides them into those forms that aim at the creation of theoretical knowledge and those that aim at producing actions or correct decisions, but there is no universally accepted taxonomy summarizing all these types.\nEntertaining, judging, and reasoning.\nThinking is often identified with the act of judging. A judgment is a mental operation in which a proposition is evoked and then either affirmed or denied. It involves deciding what to believe and aims at determining whether the judged proposition is true or false. Various theories of judgment have been proposed. The traditionally dominant approach is the combination theory. It states that judgments consist in the combination of concepts. On this view, to judge that \"all men are mortal\" is to combine the concepts \"man\" and \"mortal\". The same concepts can be combined in different ways, corresponding to different forms of judgment, for example, as \"some men are mortal\" or \"no man is mortal\".\nOther theories of judgment focus more on the relation between the judged proposition and reality. According to Franz Brentano, a judgment is either a belief or a disbelief in the existence of some entity. In this sense, there are only two fundamental forms of judgment: \"A exists\" and \"A does not exist\". When applied to the sentence \"all men are mortal\", the entity in question is \"immortal men\", of whom it is said that they do not exist. Important for Brentano is the distinction between the mere representation of the content of the judgment and the affirmation or the denial of the content. The mere representation of a proposition is often referred to as \"entertaining a proposition\". This is the case, for example, when one considers a proposition but has not yet made up one's mind about whether it is true or false. The term \"thinking\" can refer both to judging and to mere entertaining. This difference is often explicit in the way the thought is expressed: \"thinking that\" usually involves a judgment whereas \"thinking about\" refers to the neutral representation of a proposition without an accompanying belief. In this case, the proposition is merely \"entertained\" but not yet \"judged\". Some forms of thinking may involve the representation of objects without any propositions, as when someone is thinking about their grandmother.\nReasoning is one of the most paradigmatic forms of thinking. It is the process of drawing conclusions from premises or evidence. Types of reasoning can be divided into deductive and non-deductive reasoning. Deductive reasoning is governed by certain rules of inference, which guarantee the truth of the conclusion if the premises are true. For example, given the premises \"all men are mortal\" and \"Socrates is a man\", it follows deductively that \"Socrates is mortal\". Non-deductive reasoning, also referred to as defeasible reasoning or non-monotonic reasoning, is still rationally compelling but the truth of the conclusion is not ensured by the truth of the premises. Induction is one form of non-deductive reasoning, for example, when one concludes that \"the sun will rise tomorrow\" based on one's experiences of all the previous days. Other forms of non-deductive reasoning include the inference to the best explanation and analogical reasoning.\nFallacies are faulty forms of thinking that go against the norms of correct reasoning. Formal fallacies concern faulty inferences found in deductive reasoning. Denying the antecedent is one type of formal fallacy, for example, \"If Othello is a bachelor, then he is male. Othello is not a bachelor. Therefore, Othello is not male\". Informal fallacies, on the other hand, apply to all types of reasoning. The source of their flaw is to be found in the \"content\" or the \"context\" of the argument. This is often caused by ambiguous or vague expressions in natural language, as in \"Feathers are light. What is light cannot be dark. Therefore, feathers cannot be dark\". An important aspect of fallacies is that they seem to be rationally compelling on the first look and thereby seduce people into accepting and committing them. Whether an act of reasoning constitutes a fallacy does not depend on whether the premises are true or false but on their relation to the conclusion and, in some cases, on the context.\nConcept formation.\nConcepts are general notions that constitute the fundamental building blocks of thought. They are rules that govern how objects are sorted into different classes. A person can only think about a proposition if they possess the concepts involved in this proposition. For example, the proposition \"wombats are animals\" involves the concepts \"wombat\" and \"animal\". Someone who does not possess the concept \"wombat\" may still be able to read the sentence but cannot entertain the corresponding proposition. Concept formation is a form of thinking in which new concepts are acquired. It involves becoming familiar with the characteristic features shared by all instances of the corresponding type of entity and developing the ability to identify positive and negative cases. This process usually corresponds to learning the meaning of the word associated with the type in question. There are various theories concerning how concepts and concept possession are to be understood. The use of metaphor may aid in the processes of concept formation.\nAccording to one popular view, concepts are to be understood in terms of abilities. On this view, two central aspects characterize concept possession: the ability to discriminate between positive and negative cases and the ability to draw inferences from this concept to related concepts. Concept formation corresponds to acquiring these abilities. It has been suggested that animals are also able to learn concepts to some extent, due to their ability to discriminate between different types of situations and to adjust their behavior accordingly.\nProblem solving.\nIn the case of problem solving, thinking aims at reaching a predefined goal by overcoming certain obstacles. This process often involves two different forms of thinking. On the one hand, \"divergent thinking\" aims at coming up with as many alternative solutions as possible. On the other hand, \"convergent thinking\" tries to narrow down the range of alternatives to the most promising candidates. Some researchers identify various steps in the process of problem solving. These steps include recognizing the problem, trying to understand its nature, identifying general criteria the solution should meet, deciding how these criteria should be prioritized, monitoring the progress, and evaluating the results.\nAn important distinction concerns the type of problem that is faced. For \"well-structured problems\", it is easy to determine which steps need to be taken to solve them, but executing these steps may still be difficult. For ill-structured problems, on the other hand, it is not clear what steps need to be taken, i.e. there is no clear formula that would lead to success if followed correctly. In this case, the solution may sometimes come in a flash of insight in which the problem is suddenly seen in a new light. Another way to categorize different forms of problem solving is by distinguishing between algorithms and heuristics. An algorithm is a formal procedure in which each step is clearly defined. It guarantees success if applied correctly. The long multiplication usually taught in school is an example of an algorithm for solving the problem of multiplying big numbers. Heuristics, on the other hand, are informal procedures. They are rough rules-of-thumb that tend to bring the thinker closer to the solution but success is not guaranteed in every case even if followed correctly. Examples of heuristics are working forward and working backward. These approaches involve planning one step at a time, either starting at the beginning and moving forward or starting at the end and moving backward. So when planning a trip, one could plan the different stages of the trip from origin to destiny in the chronological order of how the trip will be realized, or in the reverse order.\nObstacles to problem solving can arise from the thinker's failure to take certain possibilities into account by fixating on one specific course of action. There are important differences between how novices and experts solve problems. For example, experts tend to allocate more time for conceptualizing the problem and work with more complex representations whereas novices tend to devote more time to executing putative solutions.\nDeliberation and decision.\nDeliberation is an important form of practical thinking. It aims at formulating possible courses of action and assessing their value by considering the reasons for and against them. This involves foresight to anticipate what might happen. Based on this foresight, different courses of action can be formulated in order to influence what will happen. Decisions are an important part of deliberation. They are about comparing alternative courses of action and choosing the most favorable one. Decision theory is a formal model of how ideal rational agents would make decisions. It is based on the idea that they should always choose the alternative with the highest expected value. Each alternative can lead to various possible outcomes, each of which has a different value. The expected value of an alternative consists in the sum of the values of each outcome associated with it multiplied by the probability that this outcome occurs. According to decision theory, a decision is rational if the agent chooses the alternative associated with the highest expected value, as assessed from the agent's own perspective.\nVarious theorists emphasize the practical nature of thought, i.e. that thinking is usually guided by some kind of task it aims to solve. In this sense, thinking has been compared to trial-and-error seen in animal behavior when faced with a new problem. On this view, the important difference is that this process happens inwardly as a form of simulation. This process is often much more efficient since once the solution is found in thought, only the behavior corresponding to the found solution has to be outwardly carried out and not all the others.\nEpisodic memory and imagination.\nWhen thinking is understood in a wide sense, it includes both episodic memory and imagination. In episodic memory, events one experienced in the past are relived. It is a form of mental time travel in which the past experience is re-experienced. But this does not constitute an exact copy of the original experience since the episodic memory involves additional aspects and information not present in the original experience. This includes both a feeling of familiarity and chronological information about the past event in relation to the present. Memory aims at representing how things actually were in the past, in contrast to imagination, which presents objects without aiming to show how things actually are or were. Because of this missing link to actuality, more freedom is involved in most forms of imagination: its contents can be freely varied, changed, and recombined to create new arrangements never experienced before. Episodic memory and imagination have in common with other forms of thought that they can arise internally without any stimulation of the sensory organs. But they are still closer to sensation than more abstract forms of thought since they present sensory contents that could, at least in principle, also be perceived.\nUnconscious thought.\nConscious thought is the paradigmatic form of thinking and is often the focus of the corresponding research. But it has been argued that some forms of thought also happen on the unconscious level. Unconscious thought is thought that happens in the background without being experienced. It is therefore not observed directly. Instead, its existence is usually inferred by other means. For example, when someone is faced with an important decision or a difficult problem, they may not be able to solve it straight away. But then, at a later time, the solution may suddenly flash before them even though no conscious steps of thinking were taken towards this solution in the meantime. In such cases, the cognitive labor needed to arrive at a solution is often explained in terms of unconscious thoughts. The central idea is that a cognitive transition happened and we need to posit unconscious thoughts to be able to explain how it happened.\nIt has been argued that conscious and unconscious thoughts differ not just concerning their relation to experience but also concerning their capacities. According to unconscious thought theorists, for example, conscious thought excels at simple problems with few variables but is outperformed by unconscious thought when complex problems with many variables are involved. This is sometimes explained through the claim that the number of items one can consciously think about at the same time is rather limited whereas unconscious thought lacks such limitations. But other researchers have rejected the claim that unconscious thought is often superior to conscious thought. Other suggestions for the difference between the two forms of thinking include that conscious thought tends to follow formal logical laws while unconscious thought relies more on associative processing and that only conscious thinking is conceptually articulated and happens through the medium of language.\nIn various disciplines.\nPhenomenology.\nPhenomenology is the science of the structure and contents of experience. The term \"cognitive phenomenology\" refers to the experiential character of thinking or what it feels like to think. Some theorists claim that there is no distinctive cognitive phenomenology. On such a view, the experience of thinking is just one form of sensory experience. According to one version, thinking just involves hearing a voice internally. According to another, there is no experience of thinking apart from the indirect effects thinking has on sensory experience. A weaker version of such an approach allows that thinking may have a distinct phenomenology but contends that thinking still depends on sensory experience because it cannot occur on its own. On this view, sensory contents constitute the foundation from which thinking may arise.\nAn often-cited thought experiment in favor of the existence of a distinctive cognitive phenomenology involves two persons listening to a radio broadcast in French, one who understands French and the other who does not. The idea behind this example is that both listeners hear the same sounds and therefore have the same non-cognitive experience. In order to explain the difference, a distinctive cognitive phenomenology has to be posited: only the experience of the first person has this additional cognitive character since it is accompanied by a thought that corresponds to the meaning of what is said. Other arguments for the experience of thinking focus on the direct introspective access to thinking or on the thinker's knowledge of their own thoughts.\nPhenomenologists are also concerned with the characteristic features of the experience of thinking. Making a judgment is one of the prototypical forms of cognitive phenomenology. It involves epistemic agency, in which a proposition is entertained, evidence for and against it is considered, and, based on this reasoning, the proposition is either affirmed or rejected. It is sometimes argued that the experience of truth is central to thinking, i.e. that thinking aims at representing how the world is. It shares this feature with perception but differs from it in the way how it represents the world: without the use of sensory contents.\nOne of the characteristic features often ascribed to thinking and judging is that they are predicative experiences, in contrast to the pre-predicative experience found in immediate perception. On such a view, various aspects of perceptual experience resemble judgments without being judgments in the strict sense. For example, the perceptual experience of the front of a house brings with it various expectations about aspects of the house not directly seen, like the size and shape of its other sides. This process is sometimes referred to as apperception. These expectations resemble judgments and can be wrong. This would be the case when it turns out upon walking around the \"house\" that it is no house at all but only a front facade of a house with nothing behind it. In this case, the perceptual expectations are frustrated and the perceiver is surprised. There is disagreement as to whether these pre-predicative aspects of regular perception should be understood as a form of cognitive phenomenology involving thinking. This issue is also important for understanding the relation between thought and language. The reason for this is that the pre-predicative expectations do not depend on language, which is sometimes taken as an example for non-linguistic thought. Various theorists have argued that pre-predicative experience is more basic or fundamental since predicative experience is in some sense built on top of it and therefore depends on it.\nAnother way how phenomenologists have tried to distinguish the experience of thinking from other types of experiences is in relation to \"empty intentions\" in contrast to \"intuitive intentions\". In this context, \"intention\" means that some kind of object is experienced. In \"intuitive intentions\", the object is presented through sensory contents. \"Empty intentions\", on the other hand, present their object in a more abstract manner without the help of sensory contents. So when perceiving a sunset, it is presented through sensory contents. The same sunset can also be presented non-intuitively when merely thinking about it without the help of sensory contents. In these cases, the same properties are ascribed to objects. The difference between these modes of presentation concerns not what properties are ascribed to the presented object but how the object is presented. Because of this commonality, it is possible for representations belonging to different modes to overlap or to diverge. For example, when searching one's glasses one may think to oneself that one left them on the kitchen table. This empty intention of the glasses lying on the kitchen table are then intuitively fulfilled when one sees them lying there upon arriving in the kitchen. This way, a perception can confirm or refute a thought depending on whether the empty intuitions are later fulfilled or not.\nMetaphysics.\nThe mind\u2013body problem concerns the explanation of the relationship that exists between minds, or mental processes, and bodily states or processes. The main aim of philosophers working in this area is to determine the nature of the mind and mental states/processes, and how\u2014or even if\u2014minds are affected by and can affect the body.\nHuman perceptual experiences depend on stimuli which arrive at one's various sensory organs from the external world and these stimuli cause changes in one's mental state, ultimately causing one to feel a sensation, which may be pleasant or unpleasant. Someone's desire for a slice of pizza, for example, will tend to cause that person to move his or her body in a specific manner and in a specific direction to obtain what he or she wants. The question, then, is how it can be possible for conscious experiences to arise out of a lump of gray matter endowed with nothing but electrochemical properties. A related problem is to explain how someone's propositional attitudes (e.g. beliefs and desires) can cause that individual's neurons to fire and his muscles to contract in exactly the correct manner. These comprise some of the puzzles that have confronted epistemologists and philosophers of mind from at least the time of Ren\u00e9 Descartes.\nThe above reflects a classical, functional description of how we work as cognitive, thinking systems. However the apparently irresolvable mind\u2013body problem is said to be overcome, and bypassed, by the embodied cognition approach, with its roots in the work of Heidegger, Piaget, Vygotsky, Merleau-Ponty and the pragmatist John Dewey.\nThis approach states that the classical approach of separating the mind and analysing its processes is misguided: instead, we should see that the mind, actions of an embodied agent, and the environment it perceives and envisions, are all parts of a whole which determine each other. Therefore, functional analysis of the mind alone will always leave us with the mind\u2013body problem which cannot be solved.\nPsychology.\nPsychologists have concentrated on thinking as an intellectual exertion aimed at finding an answer to a question or the solution of a practical problem. Cognitive psychology is a branch of psychology that investigates internal mental processes such as problem solving, memory, and language; all of which are used in thinking. The school of thought arising from this approach is known as cognitivism, which is interested in how people mentally represent information processing. It had its foundations in the Gestalt psychology of Max Wertheimer, Wolfgang K\u00f6hler, and Kurt Koffka, and in the work of Jean Piaget, who provided a theory of stages/phases that describes children's cognitive development.\nCognitive psychologists use psychophysical and experimental approaches to understand, diagnose, and solve problems, concerning themselves with the mental processes which mediate between stimulus and response. They study various aspects of thinking, including the psychology of reasoning, and how people make decisions and choices, solve problems, as well as engage in creative discovery and imaginative thought. Cognitive theory contends that solutions to problems either take the form of algorithms: rules that are not necessarily understood but promise a solution, or of heuristics: rules that are understood but that do not always guarantee solutions. Cognitive science differs from cognitive psychology in that algorithms that are intended to simulate human behavior are implemented or implementable on a computer. In other instances, solutions may be found through insight, a sudden awareness of relationships.\nIn developmental psychology, Jean Piaget was a pioneer in the study of the development of thought from birth to maturity. In his theory of cognitive development, thought is based on actions on the environment. That is, Piaget suggests that the environment is understood through assimilations of objects in the available schemes of action and these accommodate to the objects to the extent that the available schemes fall short of the demands. As a result of this interplay between assimilation and accommodation, thought develops through a sequence of stages that differ qualitatively from each other in mode of representation and complexity of inference and understanding. That is, thought evolves from being based on perceptions and actions at the sensorimotor stage in the first two years of life to internal representations in early childhood. Subsequently, representations are gradually organized into logical structures which first operate on the concrete properties of the reality, in the stage of concrete operations, and then operate on abstract principles that organize concrete properties, in the stage of formal operations. In recent years, the Piagetian conception of thought was integrated with information processing conceptions. Thus, thought is considered as the result of mechanisms that are responsible for the representation and processing of information. In this conception, speed of processing, cognitive control, and working memory are the main functions underlying thought. In the neo-Piagetian theories of cognitive development, the development of thought is considered to come from increasing speed of processing, enhanced cognitive control, and increasing working memory.\nPositive psychology emphasizes the positive aspects of human psychology as equally important as the focus on mood disorders and other negative symptoms. In \"Character Strengths and Virtues\", Peterson and Seligman list a series of positive characteristics. One person is not expected to have every strength, nor are they meant to fully capsulate that characteristic entirely. The list encourages positive thought that builds on a person's strengths, rather than how to \"fix\" their \"symptoms\".\nPsychoanalysis.\nThe \"id\", \"ego\" and \"super-ego\" are the three parts of the \"psychic apparatus\" defined in Sigmund Freud's structural model of the psyche; they are the three theoretical constructs in terms of whose activity and interaction mental life is described. According to this model, the uncoordinated instinctual trends are encompassed by the \"id\", the organized realistic part of the psyche is the \"ego\", and the critical, moralizing function is the \"super-ego\".\nFor psychoanalysis, the unconscious does not include all that is not conscious, rather only what is actively repressed from conscious thought or what the person is averse to knowing consciously. In a sense this view places the self in relationship to their unconscious as an adversary, warring with itself to keep what is unconscious hidden. If a person feels pain, all he can think of is alleviating the pain. Any of his desires, to get rid of pain or enjoy something, command the mind what to do. For Freud, the unconscious was a repository for socially unacceptable ideas, wishes or desires, traumatic memories, and painful emotions put out of mind by the mechanism of psychological repression. However, the contents did not necessarily have to be solely negative. In the psychoanalytic view, the unconscious is a force that can only be recognized by its effects\u2014it expresses itself in the symptom.\nThe collective unconscious, sometimes known as collective subconscious, is a term of analytical psychology, coined by Carl Jung. It is a part of the unconscious mind, shared by a society, a people, or all humanity, in an interconnected system that is the product of all common experiences and contains such concepts as science, religion, and morality. While Freud did not distinguish between \"individual psychology\" and \"collective psychology\", Jung distinguished the collective unconscious from the personal subconscious particular to each human being. The collective unconscious is also known as \"a reservoir of the experiences of our species\".\nIn the \"Definitions\" chapter of Jung's seminal work \"Psychological Types\", under the definition of \"collective\" Jung references \"representations collectives\", a term coined by Lucien L\u00e9vy-Bruhl in his 1910 book \"How Natives Think\". Jung says this is what he describes as the collective unconscious. Freud, on the other hand, did not accept the idea of a collective unconscious.\nRelated concepts and theories.\nLaws of thought.\nTraditionally, the term \"laws of thought\" refers to three fundamental laws of logic: the law of contradiction, the law of excluded middle, and the principle of identity. These laws by themselves are not sufficient as axioms of logic but they can be seen as important precursors to the modern axiomatization of logic. The \"law of contradiction\" states that for any proposition, it is impossible that both it and its negation are true: formula_1. According to the \"law of excluded middle\", for any proposition, either it or its opposite is true: formula_2. The principle of identity asserts that any object is identical to itself: formula_3. There are different conceptions of how the laws of thought are to be understood. The interpretations most relevant to thinking are to understand them as prescriptive laws of how one should think or as formal laws of propositions that are true only because of their form and independent of their content or context. Metaphysical interpretations, on the other hand, see them as expressing the nature of \"being as such\".\nWhile there is a very wide acceptance of these three laws among logicians, they are not universally accepted. Aristotle, for example, held that there are some cases in which the law of excluded middle is false. This concerns primarily uncertain future events. On his view, it is currently \"not ... either true or false that there will be a naval battle tomorrow\". Modern intuitionist logic also rejects the law of excluded middle. This rejection is based on the idea that mathematical truth depends on verification through a proof. The law fails for cases where no such proof is possible, which exist in every sufficiently strong formal system, according to G\u00f6del's incompleteness theorems. Dialetheists, on the other hand, reject the law of contradiction by holding that some propositions are both true and false. One motivation of this position is to avoid certain paradoxes in classical logic and set theory, like the liar's paradox and Russell's paradox. One of its problems is to find a formulation that circumvents the principle of explosion, i.e. that anything follows from a contradiction.\nSome formulations of the laws of thought include a fourth law: the principle of sufficient reason. It states that everything has a sufficient reason, ground, or cause. It is closely connected to the idea that everything is intelligible or can be explained in reference to its sufficient reason. According to this idea, there should always be a full explanation, at least in principle, to questions like why the sky is blue or why World War II happened. One problem for including this principle among the laws of thought is that it is a metaphysical principle, unlike the other three laws, which pertain primarily to logic.\nCounterfactual thinking.\nCounterfactual thinking involves mental representations of non-actual situations and events, i.e. of what is \"contrary to the facts\". It is usually \"conditional\": it aims at assessing what would be the case if a certain condition had obtained. In this sense, it tries to answer \"What if\"-questions. For example, thinking after an accident that one would be dead if one had not used the seatbelt is a form of counterfactual thinking: it assumes, contrary to the facts, that one had not used the seatbelt and tries to assess the result of this state of affairs. In this sense, counterfactual thinking is normally counterfactual only to a small degree since just a few facts are changed, like concerning the seatbelt, while most other facts are kept in place, like that one was driving, one's gender, the laws of physics, etc. When understood in the widest sense, there are forms of counterfactual thinking that do not involve anything contrary to the facts at all. This is the case, for example, when one tries to anticipate what might happen in the future if an uncertain event occurs and this event actually occurs later and brings with it the anticipated consequences. In this wider sense, the term \"subjunctive conditional\" is sometimes used instead of \"counterfactual conditional\". But the paradigmatic cases of counterfactual thinking involve alternatives to past events.\nCounterfactual thinking plays an important role since we evaluate the world around us not only by what actually happened but also by what could have happened. Humans have a greater tendency to engage in counterfactual thinking after something bad happened because of some kind of action the agent performed. In this sense, many regrets are associated with counterfactual thinking in which the agent contemplates how a better outcome could have been obtained if only they had acted differently. These cases are known as upward counterfactuals, in contrast to downward counterfactuals, in which the counterfactual scenario is worse than actuality. Upward counterfactual thinking is usually experienced as unpleasant, since it presents the actual circumstances in a bad light. This contrasts with the positive emotions associated with downward counterfactual thinking. But both forms are important since it is possible to learn from them and to adjust one's behavior accordingly to get better results in the future.\nThought experiments.\nThought experiments involve thinking about imaginary situations, often with the aim of investigating the possible consequences of a change to the actual sequence of events. It is a controversial issue to what extent thought experiments should be understood as actual experiments. They are experiments in the sense that a certain situation is set up and one tries to learn from this situation by understanding what follows from it. They differ from regular experiments in that imagination is used to set up the situation and counterfactual reasoning is employed to evaluate what follows from it, instead of setting it up physically and observing the consequences through perception. Counterfactual thinking, therefore, plays a central role in thought experiments.\nThe Chinese room argument is a famous thought experiment proposed by John Searle. It involves a person sitting inside a closed-off room, tasked with responding to messages written in Chinese. This person does not know Chinese but has a giant rule book that specifies exactly how to reply to any possible message, similar to how a computer would react to messages. The core idea of this thought experiment is that neither the person nor the computer understands Chinese. This way, Searle aims to show that computers lack a mind capable of deeper forms of understanding despite acting intelligently.\nThought experiments are employed for various purposes, for example, for entertainment, education, or as arguments for or against theories. Most discussions focus on their use as arguments. This use is found in fields like philosophy, the natural sciences, and history. It is controversial since there is a lot of disagreement concerning the epistemic status of thought experiments, i.e. how reliable they are as evidence supporting or refuting a theory. Central to the rejection of this usage is the fact that they pretend to be a source of knowledge without the need to leave one's armchair in search of any new empirical data. Defenders of thought experiments usually contend that the intuitions underlying and guiding the thought experiments are, at least in some cases, reliable. But thought experiments can also fail if they are not properly supported by intuitions or if they go beyond what the intuitions support. In the latter sense, sometimes counter thought experiments are proposed that modify the original scenario in slight ways in order to show that initial intuitions cannot survive this change. Various taxonomies of thought experiments have been suggested. They can be distinguished, for example, by whether they are successful or not, by the discipline that uses them, by their role in a theory, or by whether they accept or modify the actual laws of physics.\nCritical thinking.\nCritical thinking is a form of thinking that is reasonable, reflective, and focused on determining what to believe or how to act. It holds itself to various standards, like clarity and rationality. In this sense, it involves not just cognitive processes trying to solve the issue at hand but at the same time meta-cognitive processes ensuring that it lives up to its own standards. This includes assessing both that the reasoning itself is sound and that the evidence it rests on is reliable. This means that logic plays an important role in critical thinking. It concerns not just formal logic, but also informal logic, specifically to avoid various informal fallacies due to vague or ambiguous expressions in natural language. No generally accepted standard definition of \"critical thinking\" exists but there is significant overlap between the proposed definitions in their characterization of critical thinking as careful and goal-directed. According to some versions, only the thinker's own observations and experiments are accepted as evidence in critical thinking. Some restrict it to the formation of judgments but exclude action as its goal.\nA concrete everyday example of critical thinking, due to John Dewey, involves observing foam bubbles moving in a direction that is contrary to one's initial expectations. The critical thinker tries to come up with various possible explanations of this behavior and then slightly modifies the original situation in order to determine which one is the right explanation. But not all forms of cognitively valuable processes involve critical thinking. Arriving at the correct solution to a problem by blindly following the steps of an algorithm does not qualify as critical thinking. The same is true if the solution is presented to the thinker in a sudden flash of insight and accepted straight away.\nCritical thinking plays an important role in education: fostering the student's ability to think critically is often seen as an important educational goal. In this sense, it is important to convey not just a set of true beliefs to the student but also the ability to draw one's own conclusions and to question pre-existing beliefs. The abilities and dispositions learned this way may profit not just the individual but also society at large. Critics of the emphasis on critical thinking in education have argued that there is no universal form of correct thinking. Instead, they contend that different subject matters rely on different standards and education should focus on imparting these subject-specific skills instead of trying to teach universal methods of thinking. Other objections are based on the idea that critical thinking and the attitude underlying it involve various unjustified biases, like egocentrism, distanced objectivity, indifference, and an overemphasis of the theoretical in contrast to the practical.\nPositive thinking.\nPositive thinking is an important topic in positive psychology. It involves focusing one's attention on the positive aspects of one's situation and thereby withdrawing one's attention from its negative sides. This is usually seen as a global outlook that applies especially to thinking but includes other mental processes, like feeling, as well. In this sense, it is closely related to optimism. It includes expecting positive things to happen in the future. This positive outlook makes it more likely for people to seek to attain new goals. It also increases the probability of continuing to strive towards pre-existing goals that seem difficult to reach instead of just giving up.\nThe effects of positive thinking are not yet thoroughly researched, but some studies suggest that there is a correlation between positive thinking and well-being. For example, students and pregnant women with a positive outlook tend to be better at dealing with stressful situations. This is sometimes explained by pointing out that stress is not inherent in stressful situations but depends on the agent's interpretation of the situation. Reduced stress may therefore be found in positive thinkers because they tend to see such situations in a more positive light. But the effects also include the practical domain in that positive thinkers tend to employ healthier coping strategies when faced with difficult situations. This effects, for example, the time needed to fully recover from surgeries and the tendency to resume physical exercise afterward.\nBut it has been argued that whether positive thinking actually leads to positive outcomes depends on various other factors. Without these factors, it may lead to negative results. For example, the tendency of optimists to keep striving in difficult situations can backfire if the course of events is outside the agent's control. Another danger associated with positive thinking is that it may remain only on the level of unrealistic fantasies and thereby fail to make a positive practical contribution to the agent's life. Pessimism, on the other hand, may have positive effects since it can mitigate disappointments by anticipating failures.\nPositive thinking is a recurrent topic in the self-help literature. Here, often the claim is made that one can significantly improve one's life by trying to think positively, even if this means fostering beliefs that are contrary to evidence. Such claims and the effectiveness of the suggested methods are controversial and have been criticized due to their lack of scientific evidence. In the New Thought movement, positive thinking figures in the law of attraction, the pseudoscientific claim that positive thoughts can directly influence the external world by attracting positive outcomes.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "37081", "revid": "12608568", "url": "https://en.wikipedia.org/wiki?curid=37081", "title": "State of the Union", "text": "Annual report by the president of the United States\nIn the United States, the State of the Union Address (sometimes abbreviated to SOTU) is an annual message delivered by the president of the United States to a joint session of the United States Congress near the beginning of most calendar years on the current condition of the nation. The speech generally includes reports on the nation's budget, economy, news, agenda, progress, achievements and the president's priorities and legislative proposals.\nThe address fulfills the requirement in of the U.S. Constitution for the president to periodically \"give to the Congress Information of the State of the Union, and recommend to their Consideration such Measures as he shall judge necessary and expedient\". During most of the country's first century, the president primarily submitted only a written report to Congress. After 1913, Woodrow Wilson, the 28th U.S. president, began the regular practice of delivering the address to Congress in person as a way to rally support for the president's agenda, while also submitting a more detailed report. With the advent of radio and television, the address is now broadcast live in all United States time zones on many networks.\nThe speech is generally held in January or February, and an invitation to the president is extended to use the chamber of the House by the speaker of the House. Starting in 1981, Ronald Reagan, the 40th U.S. president, began the practice of newly inaugurated presidents delivering an address to Congress in the first year of their term but not designating that speech an official \"State of the Union\".\nFormality.\nThe practice arises from a duty of the president under the State of the Union Clause of the U.S. Constitution:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;He shall from time to time give to the Congress information of the State of the Union and recommend to their Consideration such Measures as he shall judge necessary and expedient.\u2014\u200a\nThough the language of the clause is not specific, since the 1930s, the president has made this report annually in late January or early February. Between 1934 and 2024 the date has been as early as January 3, and as late as March 7.\nWhile not required to deliver a speech, every president since Woodrow Wilson, with the notable exception of Herbert Hoover, has made at least one State of the Union report as a speech delivered before a joint session of Congress. Before then, most presidents delivered the State of the Union as a written report.\nSince Franklin Roosevelt, the State of the Union is given typically each January before a joint session of the United States Congress and is held in the House of Representatives chamber of the United States Capitol. Newly inaugurated presidents generally deliver an address to Congress in February of the first year of their term, but this speech is not officially considered to be a \"State of the Union\".\nWhat began as a communication between president and Congress has become in effect a communication between the president and the people of the United States. Since the advent of radio, and then television, the speech has been broadcast live in all United States time zones on most networks, preempting scheduled programming. Since at least the 1960s, in order to reach the largest audience, the speech has typically been given at 9 p.m. (Eastern Time, ).\nHistory.\nGeorge Washington delivered the first regular annual message before a joint session of Congress on January 8, 1790, in New York City, then the provisional U.S. capital. In 1801, Thomas Jefferson discontinued the practice of delivering the address in person, regarding it as too monarchical (similar to the Speech from the Throne). Instead, the address was written and then sent to Congress to be read by a clerk until 1913 when Woodrow Wilson re-established the practice despite some initial controversy, and an in-person address to Congress has been delivered nearly every year since. However, there have been exceptions to this rule, with some messages being given solely in writing, and others given both in writing and orally (either in a speech to Congress or through broadcast media). The last president to give a written message without a spoken address was Jimmy Carter in 1981, days before his term ended after his defeat by Ronald Reagan.\nFor many years, the speech was referred to as \"the President's Annual Message to Congress\". The actual term \"State of the Union\" first emerged in 1934 when Franklin D. Roosevelt used the phrase, becoming its generally accepted name since 1947.\nPrior to 1934, the annual message was delivered at the end of the calendar year, in December. The ratification of the 20th Amendment on January 23, 1933, changed the opening of Congress from early March to early January, affecting the delivery of the annual message. Since 1934, the message or address has been delivered to Congress early in the calendar year.\nThe Twentieth Amendment also established January 20 as the beginning of the presidential term. In years when a new president is inaugurated, the outgoing president may deliver a final State of the Union message, but none has done so since Jimmy Carter sent a written message in 1981. In 1953 and 1961, Congress received both a written State of the Union message from the outgoing president and a separate State of the Union speech by the incoming president. Since 1981, in recognition that the responsibility of reporting the State of the Union formally belongs to the president who held office during the past year, newly inaugurated presidents have not officially called their first speech before Congress a \"State of the Union\" message.\nWarren Harding's 1922 speech was the first to be broadcast on radio, albeit to a limited audience, while Calvin Coolidge's 1923 speech was the first to be broadcast across the nation. President Roosevelt's address in 1936 was the first delivered in the evening, but this precedent was not followed again until the 1960s. Harry S. Truman's 1947 address was the first to be broadcast on television. In 1968, television networks in the United States for the first time imposed no time limit for their coverage of a State of the Union address. Delivered by Lyndon B. Johnson, this address was followed by extensive televised commentary by, among others, Daniel Patrick Moynihan and Milton Friedman. Bill Clinton's 1997 address was the first broadcast available live on the World Wide Web.\nRonald Reagan's 1986 State of the Union Address was the first to have been postponed. He had planned to deliver the speech on January 28, 1986, but it was delayed for a week following the Space Shuttle \"Challenger\" disaster that morning. Reagan instead addressed the nation from the Oval Office about the disaster.\nIn 1999, Bill Clinton became the first president to deliver an in-person State of the Union address while standing trial for impeachment; the speech occurred the same day that Clinton's defense team made its opening statement in Clinton's impeachment trial, though he did not mention the proceeding.\nOn January 23, 2019, the 2019 State of the Union speech by Donald Trump, originally planned for January 29 was canceled after an exchange of letters with Speaker of the House Nancy Pelosi in which she stated she would not proceed with a vote on a resolution to permit him to deliver the speech in the House chamber until the end of 2018\u201319 United States federal government shutdown. This decision rescinded an earlier invitation from the speaker, reportedly the first time in American history that a speaker had \"disinvited\" the president from delivering the address. Trump and Pelosi later agreed to hold the speech on February 5.\nDelivery of the speech.\nBecause the address is made to a joint session of Congress, the House and Senate must each pass a resolution setting a date and time for the joint session. Then, a formal invitation is made by the speaker of the House to the president typically several weeks before the appointed date.\nInvitations.\nEvery member of Congress can bring one guest to the State of the Union address. The president may invite up to 24 guests to be seated in a box with the first lady. The speaker of the House may invite up to 24 guests in the speaker's box. Seating for Congress on the main floor is by a first-in, first-served basis with no reservations. The Cabinet, Supreme Court justices, members of the Diplomatic Corps, and military leaders (the Joint Chiefs of Staff and commandant of the Coast Guard) have reserved seating.\nProtocol of entry into the House chamber.\nBy approximately 8:30\u00a0p.m. on the night of the address, the members of the House have gathered in their seats for the joint session. Then, the deputy sergeant at arms addresses the speaker and loudly announces the vice president and members of the Senate, who enter and take the seats assigned for them.\nThe speaker, and then the vice president, specify the members of the House and Senate, respectively, who will escort the president into the House chamber. The deputy sergeant at arms addresses the speaker again and loudly announces, in order, the dean of the Diplomatic Corps, the chief justice of the United States and the associate justices, and the Cabinet, each of whom enters and takes their seats when called. The justices take the seats nearest to the speaker's rostrum and adjacent to the sections reserved for the Cabinet and the members of the Joint Chiefs of Staff.\nJust after 9:00\u00a0pm, as the president reaches the door to the chamber, the House sergeant at arms stands just inside the doors, faces the speaker, and waits until the president is ready to enter the chamber. When the president is ready, the sergeant at arms announces the entrance, loudly stating the phrase: \"Mister/Madam Speaker, the president of the United States!\"\nAs applause and cheering begin, the president slowly walks toward the speaker's rostrum, followed by members of the congressional escort committee. The president's approach is slowed by pausing to shake hands, hug, kiss, and autograph copies of the speech for members of Congress. After taking a place at the clerk's desk, the president hands two envelopes containing copies of the speech to the speaker and vice president. The ovation lasts an average of 2 to 3 minutes\nAfter continuing applause from the attendees has diminished, the speaker introduces the president to the representatives and senators, typically stating: \"Members of Congress, I have the high privilege and distinct honor of presenting to you the President of the United States.\" This leads to a further round of applause and, eventually, the beginning of the address by the president. The speaker may opt not to introduce the president, as was demonstrated in 2019 and 2024.\nDesignated survivor and other logistics.\nCustomarily, one cabinet member (the designated survivor) does not attend the speech, in order to provide continuity in the line of succession if a catastrophe disables the president, the vice president, and other succeeding officers gathered in the House chamber. Additionally, since the September 11 attacks in 2001, a few members of Congress have been asked to relocate to undisclosed locations for the duration of the speech to form a rump Congress in the event of a disaster. Since 2003, each chamber of Congress has formally named a separate designated survivor.\nBoth the speaker and the vice president sit at the speaker's desk, behind the president for the duration of the speech. If either is unavailable, the next highest-ranking member of the respective house substitutes. Once the chamber settles down from the president's arrival, the speaker officially presents the president to the joint session of Congress. The president then delivers the speech from the podium at the front of the House Chamber.\nFor the 2011 address, Senator Mark Udall of Colorado proposed a break in the tradition of seating Republicans and Democrats on opposite sides of the House; this was in response to the 2011 Tucson Shooting in which Representative Gabby Giffords was shot and wounded in an assassination attempt. Approximately 60 legislators signed on to Udall's proposal; a similar plan for the 2012 address garnered bipartisan seating commitments from more than 160 lawmakers. Efforts to intersperse the parties during the State of the Union have since waned, and by the 2016 address, seating had largely returned to the traditional partisan arrangement.\nContent of the speech.\nThe contents of the speeches typically contain information and status updates of the country and federal government during the incumbent president's administration. It has become customary to use the phrase \"The State of the Union is strong\", sometimes with slight variations, since President Ronald Reagan introduced it in his 1983 address. It has been repeated by every president in nearly every year since, with the exception of George H. W. Bush. Gerald Ford's 1975 address had been the first to use the phrasing \"The State of the Union is...\", though Ford completed the sentence with \"not good.\"\nSince Reagan's 1982 address, it has also become common for presidents of both parties to honor special guests sitting in the gallery, such as American citizens or visiting heads of state. During that 1982 address, Reagan acknowledged Lenny Skutnik for his act of heroism following the crash of Air Florida Flight 90. Since then, the term \"Lenny Skutniks\" has been used to refer to individuals invited to sit in the gallery, and then cited by the president, during the State of the Union.\nState of the Union speeches usually last a little over an hour, partly because of the large amounts of applause that occur from the audience throughout. The applause is often political in tone, with many portions of the speech being applauded only by members of the president's own party. As non-political officeholders, members of the Supreme Court or the Joint Chiefs of Staff rarely applaud in order to retain the appearance of political impartiality. In recent years, the presiding officers of the House and the Senate, the speaker and the vice president, respectively, have departed from the neutrality expected of presiding officers of deliberative bodies, as they, too, stand and applaud in response to the remarks of the president with which they agree.\nOpposition response.\nSince 1966, the speech has been followed on television by a response or rebuttal by a member of the major political party opposing the president's party. The response is typically broadcast from a studio with no audience. In 1970, the Democratic Party put together a TV program with their speech to reply to President Nixon, as well as a televised response to Nixon's written speech in 1973. The same was done by Democrats for President Reagan's speeches in 1982 and 1985. The response is not always produced in a studio; in 1997, the Republicans for the first time delivered the response in front of high school students. In 2010, Virginia governor Bob McDonnell gave the Republican response from the House of Delegates chamber of the Virginia State Capitol in Richmond, in front of about 250 attendees.\nIn 2004, the Democratic Party's response was delivered in Spanish for the first time, by New Mexico governor Bill Richardson. In 2011, Minnesota congresswoman Michele Bachmann also gave a televised response for the Tea Party Express, a first for a political movement. In 2024, the Republican response was delivered by Senator Katie Britt on March 8 (Women's International Day) from her kitchen table. The first Independent response was delivered by Robert F. Kennedy Jr.\nSignificance.\nAlthough much of the pomp and ceremony behind the State of the Union address is governed by tradition rather than law, in modern times, the event is seen as one of the most important in the US political calendar. It is one of the few instances when all three branches of the US government are assembled under one roof: members of both houses of Congress constituting the legislature, the president and Cabinet constituting the executive, and the chief justice and associate justices of the Supreme Court constituting the judiciary. In addition, the military is represented by the Joint Chiefs of Staff, while foreign governments are represented by the dean of the Diplomatic Corps. The address has also been used as an opportunity to honor the achievements of some ordinary Americans, who are typically invited by the president to sit with the first lady.\nLocal versions.\nCertain U.S. states have a similar annual address given by the governor. For most of them, it is called the State of the State address. In Iowa, it is called the Condition of the State Address; in Kentucky, Massachusetts, Pennsylvania, and Virginia, the speech is called the State of the Commonwealth address. The mayor of the District of Columbia gives a State of the District address. American Samoa has a State of the Territory address given by the governor. Puerto Rico has a State Address given by the governor. In Guam, the governor delivers an annual State of the Island Address.\nSome cities or counties also have an annual State of the City Address given by the mayor, county commissioner or board chair, including Sonoma County, California; Orlando, Florida; Gwinnett County, Georgia; Cincinnati, Ohio; New Haven, Connecticut; Parma, Ohio; Detroit, Michigan; Seattle, Washington; Birmingham, Alabama; Boston, Massachusetts; Los Angeles, California; Buffalo, New York; Rochester, New York; San Antonio, Texas; McAllen, Texas; and San Diego, California. The Mayor of the Metropolitan Government of Nashville and Davidson County in Nashville, Tennessee gives a speech similar called the State of Metro Address. Some university presidents give a State of the University address at the beginning of every academic term. Some elementary and secondary schools and school districts also hold a \"State of the School(s)\" address at the beginning of each calendar year. Private companies usually have a \"State of the Corporation\" or \"State of the Company\" address given by the respective CEO. As well, the commissioners of some North American professional sports leagues, in particular Major League Soccer and the Canadian Football League, deliver annual \"State of the League\" addresses, usually in conjunction with events surrounding their respective leagues' championship games.\nThe State of the Union model has also been adopted by the European Union. In France, President Emmanuel Macron initiated a similar event in 2017, again in 2018, but the practice did not continue the following years.\nIn Spain, the Congress of Deputies adopted the tradition under the name \"Debate on the State of the Nation\" in 1983. The prime minister gives an address for an undetermined length of time, and afterwards each of the parliamentary groups have the chance to respond in an address with a maximum length of thirty minutes. These are sorted by the amount of deputies that each parliamentary group holds, thus starting with the leader of the Opposition. Since its creation, it has taken place in every non-election year except for 2021, where Prime Minister Pedro S\u00e1nchez was forced to cancel it due to the COVID-19 pandemic.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "37085", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=37085", "title": "Software bug", "text": "Inherent flaw in computer instructions\nA software bug is a design defect (bug) in computer software. A computer program with many or serious bugs may be described as \"buggy\". \nThe effects of a software bug range from minor (such as a misspelled word in the user interface) to severe (such as frequent crashing).\nIn 2002, a study commissioned by the US Department of Commerce's National Institute of Standards and Technology concluded that \"software bugs, or errors, are so prevalent and so detrimental that they cost the US economy an estimated $59\u00a0billion annually, or about 0.6 percent of the gross domestic product\".\nSince the 1950s, some computer systems have been designed to detect or auto-correct various software errors during operations.\nTerminology.\n\"Mistake metamorphism\" (from Greek \"meta\" = \"change\", \"morph\" = \"form\") refers to the evolution of a defect in the final stage of software deployment. Transformation of a \"mistake\" committed by an analyst in the early stages of the software development lifecycle, which leads to a \"defect\" in the final stage of the cycle has been called \"mistake metamorphism\".\nDifferent stages of a mistake in the development cycle may be described as mistake,anomaly, fault, failure, error, exception, crash, glitch, bug, defect, incident, or side effect. \nExamples.\nSoftware bugs have been linked to disasters.\nControversy.\nSometimes the use of \"bug\" to describe the behavior of software is contentious due to perception. Some suggest that the term should be abandoned; contending that \"bug\" implies that the defect arose on its own and push to use \"defect\" instead since it more clearly indicates they are caused by a human.\nSome contend that \"bug\" may be used to cover up an intentional design decision. In 2011, after receiving scrutiny from US Senator Al Franken for recording and storing users' locations in unencrypted files,\nApple called the behavior a bug. However, Justin Brookman of the Center for Democracy and Technology directly challenged that portrayal, stating \"I'm glad that they are fixing what they call bugs, but I take exception with their strong denial that they track users.\"\nPrevention.\nPreventing bugs as early as possible in the software development process is a target of investment and innovation.\nLanguage support.\nNewer programming languages tend to be designed to prevent common bugs based on vulnerabilities of existing languages. Lessons learned from older languages such as BASIC and C are used to inform the design of later languages such as C# and Rust.\nA compiled language allows for detecting some typos (such as a misspelled identifier) before runtime which is earlier in the software development process than for an interpreted language.\nLanguages may include features such as a static type system, restricted namespaces and modular programming. For example, for a typed, compiled language (like C):\n float num = \"3\";\nis syntactically correct, but fails type checking since the right side, a string, cannot be assigned to a float variable. Compilation fails \u2013 forcing this defect to be fixed before development progress can resume. With an interpreted language, a failure would not occur until later at runtime.\nSome languages exclude features that easily lead to bugs, at the expense of slower performance \u2013 the principle being that it is usually better to write simpler, slower correct code than complicated, buggy code. For example, Java does not support pointer arithmetic which can be very fast but may lead to memory corruption or segmentation faults if not used with great caution.\nSome languages include features that add runtime overhead in order to prevent common bugs. For example, many languages include runtime bounds checking and a way to recover from out-of-bounds errors instead of crashing.\nTechniques.\nStyle guidelines and defensive programming can prevent easy-to-miss typographical errors (typos).\nFor example, most C-family programming languages allow the omission of braces around an instruction block if there's only a single instruction. The following code executes function only if is true:\n if (condition)\n foo();\nBut this code always executes :\n if (condition);\n foo();\nUsing braces - even if they're not strictly required - reliably prevents this error:\n if (condition) {\n foo();\nEnforcement of conventions may be manual (i.e. via code review) or via automated tools such as linters.\nSpecification.\nSome contend that writing a program specification, which states the intended behavior of a program, can prevent bugs. Others, however, contend that formal specifications are impractical for anything but the shortest programs, because of problems of combinatorial explosion and indeterminacy.\nSoftware testing.\nOne goal of software testing is to find bugs. Measurements during testing can provide an estimate of the number of likely bugs remaining. This becomes more reliable the longer a product is tested and developed.\nAgile practices.\nAgile software development may involve frequent software releases with relatively small changes. Defects are revealed by user feedback.\nWith test-driven development (TDD), unit tests are written while writing the production code, and the production code is not considered complete until all tests have been written and complete successfully.\nStatic analysis.\nTools for static code analysis help developers by inspecting the program text beyond the compiler's capabilities to spot potential problems. Although in general the problem of finding all programming errors given a specification is not solvable (see halting problem), these tools exploit the fact that human programmers tend to make certain kinds of simple mistakes when writing software.\nInstrumentation.\nTools to monitor the performance of the software as it is running, either specifically to find problems such as bottlenecks or to give assurance as to correct working, may be embedded in the code explicitly (perhaps as simple as a statement saying codice_1), or provided as tools. It is often a surprise to find that most of the time is taken by a piece of code, and this removal of assumptions might cause the code to be rewritten.\nOpen source.\nOpen source development allows anyone to examine source code. A school of thought popularized by Eric S. Raymond as Linus's law says that popular open-source software has more chance of having few or no bugs than other software, because \"given enough eyeballs, all bugs are shallow\". This assertion has been disputed, however: computer security specialist Elias Levy wrote that \"it is easy to hide vulnerabilities in complex, little understood and undocumented source code,\" because, \"even if people are reviewing the code, that doesn't mean they're qualified to do so.\" An example of an open-source software bug was the 2008 OpenSSL vulnerability in Debian.\nDebugging.\n\"Debugging\" can be a significant part of the software development lifecycle. Maurice Wilkes, an early computing pioneer, described his realization in the late 1940s that\n\u201ca good part of the remainder of my life was going to be spent in finding errors in my own programs\u201d.\nTypically, the first step in locating a bug is to reproduce it reliably. If unable to reproduce the issue, a programmer cannot find the cause of the bug and therefore cannot fix it.\nSome bugs are revealed by inputs that may be difficult to reproduce. One cause of the Therac-25 radiation machine deaths was a bug (specifically, a race condition) that occurred only when the machine operator very rapidly entered a treatment plan; it took days of practice to become able to do this, so the bug did not manifest in testing or when the manufacturer attempted to reproduce it. Other bugs may stop occurring whenever the setup is augmented to help find the bug, such as running the program with a debugger; these are called \"heisenbugs\" (humorously named after the Heisenberg uncertainty principle).\nSometimes, a bug is not an isolated flaw, but represents an error of thinking or planning on the part of the programmers. Often, such a \"logic error\" requires a section of the program to be overhauled or rewritten. a process known as code refactoring.\nA code review, stepping through the code and imagining or transcribing the execution process, may often find errors without ever reproducing the bug as such.\nA program known as a debugger can help a programmer find faulty code by examining the inner workings of a program, such as executing code line-by-line and viewing variable values. \nAs an alternative to using a debugger, code may be instrumented with logic to output debug information to trace program execution and view values. Output is typically to console, window, log file or a hardware output, potentially driving an indicator LED.\nSince the 1990s, particularly following the Ariane 5 Flight 501 disaster, interest in automated aids to debugging rose, such as static code analysis by abstract interpretation.\nIn an embedded system, the software is often modified to work around a hardware bug since software modifications can be cheaper and less disruptive than modifying the hardware.\nManagement.\nBugs are managed via activities like documenting, categorizing, assigning, reproducing, correcting and releasing the corrected code. \nTools are often used to track bugs and other issues with software. Typically, different tools are used by the software development team to track their workload than by customer service to track user feedback. \nA tracked item is often called \"bug\", \"defect\", \"ticket\", \"issue\", \"feature\", or for agile software development, \"story\" or \"epic\". Items are often categorized by aspects such as severity, priority and version number(s) affected.\nIn a process sometimes called triage, choices are made for each bug about whether and when to fix it based on information such as the bug's severity and priority and external factors such as development schedules. Triage generally does not include an investigation into the cause. Triage may occur regularly and minimally consists of reviewing new bugs since the previous triage, possibly extending to all open bugs. Attendees may include the project manager, the development manager, the test manager, the build manager, and technical experts.\nSeverity.\n\"Severity\" is a measure of impact the bug has. This impact may be data loss, financial, loss of goodwill and wasted effort. Severity levels are not standardized, but differ by context such as industry and tracking tool. For example, a crash in a video game has a different impact than a crash in a bank server. Severity levels might be \"crash or hang\", \"no workaround\" (user cannot accomplish a task), \"has workaround\" (user can still accomplish the task), \"visual defect\" (a misspelling for example), or \"documentation error\". Another example set of severities: \"critical\", \"high\", \"low\", \"blocker\", \"trivial\". The severity of a bug may be a separate category to its priority for fixing, or the two may be quantified and managed separately.\nA bug severe enough to delay the release of the product is called a \"show stopper\".\nPriority.\n\"Priority\" describes the importance of resolving the bug in relation to other bugs. Priorities might be numerical, such as 1 through 5, or named, such as \"critical\", \"high\", \"low\", and \"deferred\". The values might be similar or identical to severity ratings, even though priority is a different aspect. \nPriority may be a combination of the bug's severity with the level of effort to fix. A bug with low severity but easy to fix may get a higher priority than a bug with moderate severity that requires significantly more effort to fix. \nPatch.\nBugs of sufficiently high priority may warrant a special release which is sometimes called a \"patch\". \nMaintenance release.\nA software release that emphasizes bug fixes may be called a \"maintenance\" release \u2013 to differentiate it from a release that emphasizes new features or other changes.\nKnown issue.\nIt is common practice to release software with known, low-priority bugs or other issues. Possible reasons include but are not limited to:\nImplications.\nThe amount and type of damage a software bug may cause affects decision-making, processes and policy regarding software quality. In applications such as human spaceflight, aviation, nuclear power, health care, public transport or automotive safety, since software flaws have the potential to cause human injury or even death, such software will have far more scrutiny and quality control than, for example, an online shopping website. In applications such as banking, where software flaws have the potential to cause serious financial damage to a bank or its customers, quality control is also more important than, say, a photo editing application.\nOther than the damage caused by bugs, some of their cost is due to the effort invested in fixing them. In 1978, Lientz et al. showed that the median of projects invest 17 percent of the development effort in bug fixing. In 2020, research on GitHub repositories showed the median is 20%.\nCost.\nIn 1994, NASA's Goddard Space Flight Center managed to reduce their average number of errors from 4.5 per 1,000 lines of code (SLOC) down to 1 per 1000 SLOC.\nAnother study in 1990 reported that exceptionally good software development processes can achieve deployment failure rates as low as 0.1 per 1000 SLOC. This figure is iterated in literature such as \"Code Complete\" by Steve McConnell, and the \"NASA study on Flight Software Complexity\". Some projects even attained zero defects: the firmware in the IBM Wheelwriter typewriter which consists of 63,000 SLOC, and the Space Shuttle software with 500,000 SLOC.\nBenchmark.\nTo facilitate reproducible research on testing and debugging, researchers use curated benchmarks of bugs:\nTypes.\nSome notable types of bugs:\nDesign error.\nA bug can be caused by insufficient or incorrect design based on the specification. For example, given that the specification is to alphabetize a list of words, a design bug might occur if the design does not account for symbols; resulting in incorrect alphabetization of words with symbols.\nArithmetic.\nNumerical operations can result in unexpected output, slow processing, or crashing.\nSuch a bug can be from a lack of awareness of the qualities of the data storage such as a loss of precision due to rounding, numerically unstable algorithms, arithmetic overflow and underflow, or from lack of awareness of how calculations are handled by different software coding languages such as division by zero which in some languages may throw an exception, and in others may return a special value such as NaN or infinity.\nControl flow.\nA control flow bug, or logic error, is characterized by code that does not fail with an error, but does not have the expected behavior, such as infinite looping, infinite recursion, incorrect comparison in a conditional such as using the wrong comparison operator, and the off-by-one error.\nIn politics.\n\"Bugs in the System\" report.\nThe Open Technology Institute, run by the group, New America, released a report \"Bugs in the System\" in August 2016 stating that U.S. policymakers should make reforms to help researchers identify and address software bugs. The report \"highlights the need for reform in the field of software vulnerability discovery and disclosure.\" One of the report's authors said that Congress has not done enough to address cyber software vulnerability, even though Congress has passed a number of bills to combat the larger issue of cyber security.\nGovernment researchers, companies, and cyber security experts are the people who typically discover software flaws. The report calls for reforming computer crime and copyright laws.\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "37086", "revid": "50930963", "url": "https://en.wikipedia.org/wiki?curid=37086", "title": "Battle of Lechfeld", "text": "Part of the Hungarian invasions of Europe, 955\nThe Battle of Lechfeld, also known as the Second Battle of Lechfeld, was a series of military engagements over the course of three days from 10\u201312 August 955 in which the Kingdom of Germany, led by King Otto I the Great, fought the Hungarian army led by \"Harka \"Bulcs\u00fa and the chieftains L\u00e9l and S\u00far. The traditional view is that with the German victory, further invasions by the Magyars into Latin Europe ended, although it is more accurate to say that the battle resulted in a stalemate between the Hungarians and Germans over the next several decades.\nThe Hungarians invaded the Duchy of Bavaria in late June or early July 955 with 8,000\u201310,000 horse archers, infantry, and siege engines, intending to draw the main German army, under Otto I, into battle in the open field and destroy it. The Hungarians laid siege to Augsburg on the river Lech. Otto I advanced to relieve the city with an army of 8,000 heavy cavalry and infantry, divided into eight legions.\nAs Otto I approached Augsburg on 10 August, a Hungarian surprise attack destroyed the Duchy of Bohemia rearguard legion. The Hungarian force stopped to plunder the German camp and Conrad, Duke of Lorraine led a counter-attack with heavy cavalry, dispersing the Hungarians. Otto I then brought his army into battle against the main Hungarian army that barred his way to Augsburg. The German heavy cavalry defeated the lightly armed and armored Hungarians in close combat, but the latter retreated in good order. Otto I did not pursue, returning to Augsburg for the night and sending out messengers to order all local German forces to hold the river crossings in Eastern Bavaria and so prevent the Hungarians from returning to their homeland. On 11 and 12 August, the Hungarian defeat was transformed into disaster, as heavy rainfall and flooding slowed the retreating Hungarians and allowed German troops to hunt them down and kill them all. The Hungarian leaders were captured, taken to Augsburg, and hanged.\nThe German victory preserved the Kingdom of Germany and ended nomad incursions into Western Europe. Otto I was proclaimed emperor and father of the fatherland by his army after the victory, and went on to be crowned Holy Roman Emperor in 962 largely on the basis of his strengthened position after the Battle of Lechfeld.\nThe Second Battle of Lechfeld gave rise to the well-known account of the Horn of Lehel: Lehel, the Hungarian chieftain, slew Conrad, prince of the Germans with his horn.\nHistorical sources.\nThe most important source is a monograph commissioned by Ulrich of Augsburg, which describes the series of actions from the German point of view. Another source is the chronicler Widukind of Corvey, who provides some important details.\nBackground.\nIn 947, Berthold, Duke of Bavaria, a competent military leader, died and was succeeded by Henry I, brother of King Otto I. Aventinus stated that the Hungarians invaded Bavaria for this reason, but they weren't able to penetrate deep into East Francia. In the following years, the Germans started to threaten Transdanubia, with border clashes erupting along the Enns River. According to Hrotsvitha, Henry brought back much booty and prisoners from the Avars due to these. In 952, Otto put Italy under the protection of the Bavarian army, and westward invasions by the Magyars stopped temporarily. However, 953 saw rebellion in Francia under the leadership of the king's son Liudolf, Duke of Swabia, and son-in-law Conrad, Duke of Lorraine, mainly because of the occupation of Italy. In 954, these men called in the Hungarians, who then plundered the Rhineland and devastated France. The warriors returned from this successful adventure safely through Burgundy and Northern Italy.\nThe year 955 started badly for King Otto. Despite his best efforts, the archbishop of Salzburg joined the enemy. Harold [\"\"] was blinded and exiled to Tyrol, while his wealth was taken by Henry's vassals, but this upset many more Bavarian counts, who took up arms against the king. In spite of the growing of the resistance, Otto gained a shining victory at M\u00fchldorf, proceeding to lay siege to Regensburg. Much of the city had already burned down, however its defenders long endured bombardment by Otto's siege engines before surrendering due to hunger, as no relief arrived. The internal situation hardly improved after Otto's defeat of the rebellion, as the nephews of Prince Hermann of Saxony frequently raided the duchy, allying with Polabian principalities. In early July Otto received Hungarian legates, who claimed to come in peace, but who the Germans suspected were actually assessing the outcome of the rebellion. After a few days, he let them go with some small gifts.\nSoon, couriers from Otto I's brother Henry I, Duke of Bavaria, arrived to inform Otto I in Magdeburg of a Hungarian invasion. According to Prince-Bishop Ulrich, \"they devastated the land of Noricum from the Danube to the Black Forest, which goes to the mountainous regions\". According to Widukind, \"he (Otto) started the march against the enemy like he wouldn't get tired in the previous war, only taking some of the Saxons by him, as the Slavic war threatened them\". Saxony was distant from Augsburg and its environs, and considerable time would have elapsed waiting for Saxons' arrival. Ulm was chosen as the place to gather the anti-Hungarian forces. The battle took place six weeks after the first report of an invasion, and historian Hans Delbr\u00fcck asserts that they could not have possibly made the march in time.\nThe King ordered his troops to concentrate on the Danube, in the vicinity of Neuburg and Ingolstadt. He did this to march on the Hungarian line of communications and catch them in their rear while they were raiding northeast of Augsburg. It was also a central point of concentration for all the contingents that were assembling. Strategically, therefore, this was the best location for Otto I to concentrate his forces before making the final descent upon the Hungarians.\nThere were other troops that had an influence on the course of the battle. On previous occasions, in 932 and 954 for example, there had been Hungarian incursions that had invaded the German lands to the south of the Danube, and then retreated back to their native country via Lotharingia, to the West Frankish Kingdom and finally, through Italy. That is to say, a wide sweeping U-turn that initially started westward, then progressed to the south, and then finally to the east back to their homeland; and thus escaping retribution in German territory. The King was aware of the escape of these Hungarians on the above-mentioned occasions, and was determined to trap them. He therefore ordered his brother, Archbishop Bruno, to keep the Lotharingian forces in Lotharingia. With a powerful force of knights pressing them from the west, and an equally strong force of knights chasing them from the east, the Hungarians would be unable to escape.\nLocated south of Augsburg, the Lechfeld is the flood plain that lies along the river Lech. The battle appears as the second Battle of Augsburg in Hungarian historiography. The first Battle of Lechfeld happened in the same area forty-five years earlier.\nPrelude.\nGerhard writes that the Hungarian forces advanced across the Lech to the river Iller and ravaged the lands in between. They then withdrew from the Iller and placed Augsburg, a border city of Swabia, under siege. Augsburg had been heavily damaged during a rebellion against Otto I in 954. The city was defended by Bishop Ulrich. He ordered his contingent of soldiers not to fight the Hungarians in the open, but to reinforce the main south gate of the fortress instead. He motivated them with the 23rd Psalm (\"Yea, though I walk through the valley of the shadow of death\"). While this defense was going on, the King was raising an army to march south. Simon of K\u00e9za mentions that the Hungarians harassed Augsburg with attacks all day and night. That would seem to indicate that before the real siege they wished to take the city by sudden onslaughts.\nAfter it had become apparent that this tactic wouldn't work, a major action took place on 8 August at the eastern gate, into which the Magyars tried to storm in large numbers, suspecting that it would be more weakly defended because of its limited accessibility. Ulrich led his professional \"milites\" (knights, soldiers) out into the field to engage the enemy in close combat. Ulrich writes of himself that he was unarmed, wearing only a stola while mounted on a warhorse, and all the arrows and stones bypassed him. According to him, the Hungarians could have entered the gates at any time; however, they lost their commander during the attack, and withdrew to their camp, taking the body. At first the defenders thought that the Hungarians were victorious and resuming the siege, only to realize that they were going back to the other side of the Lech.\nDuring the night, the defenders took positions in all the towers of the city, and the Hungarians completely surrounded it with siege engines and infantry, who were driven forward by the whips of the Hungarian leaders. Next day, when the fights had scarcely started, they were informed by the traitor Berchtold of Risinesburg that Otto I had deployed his troops nearby. The siege was suspended, and the Hungarian leaders withdrew to hold a war council. As the Hungarians departed, Count Dietpald of Dillingen used the opportunity to lead soldiers to Otto I's camp during the night.\nOpposing forces.\nAccording to Widukind, Otto I had at his disposal eight \"legiones\" (divisions) that included three from Bavaria, two from Swabia, one from Franconia under Duke Conrad and one well-trained legion from Bohemia, under a prince of an unknown name, son of Boleslaus I. The eighth division, commanded by Otto I, and slightly larger than the others, included Saxons, Thuringians, and the King's personal guard, the \"legio regia\". The King's contingent consisted of hand-picked troops. A late Roman legion had 1,000 men, so Otto I's army may have numbered 7,000\u20139,000 troops. Augsburg was defended by professional \"milites\" (soldiers).\nThe Hungarians, also known as the Magyars, had a very different structure and fighting style than the Ottonian military. The Magyars preferred fighting at a distance with mounted archers over fighting in close combat with melee weapons, furthermore, the Magyars wore much lighter armor than Otto I's men. While there is some debate as to the number of mounted archers included in the Magyar forces, historians believe there was anywhere between 8,000\u201310,000 mounted archers. While this fighting style was effective, especially during raids against small villages and small military forces, historians have pointed out some weaknesses. One such weakness is the difficulty that came with raising horses that were suited for battle. Not only do horses require a large area to graze, but training them to be comfortable in battle takes a significant amount of time. This weakness was the biggest factor that limited the number of mounted archers available for the Hungarians. Another weakness is the fact that the bows used by the Magyars proved ineffective during inclement weather like rain. Without the ability to play to their strength, the Magyars would be forced to rely on melee combat, which was another weakness for them.\nBattle.\nOn 9 August, the German scouts reported that the Hungarian army was in the vicinity. Otto I deployed his army for battle the next day. It is likely that Otto and Ulrich had communicated in the previous days, and informing the king that the city needed a relief force quickly. He departed from Ulm within seven days at most. The order of march of the German army was as follows: the three Bavarian contingents, the Frankish contingent under Duke Conrad, the royal unit (the center), the two contingents of Swabians and the Bohemian contingent guarding the supply train in the rear. The Bavarians were placed at the head of column, according to Delbr\u00fcck, because they were marching through Bavarian territory and they therefore knew the territory best. All of these were mounted. They could achieve a maximum distance of 25 kilometers per day. The German army marched through woodland that protected them from the Hungarian arrow-storm, but also made it more difficult to see the Hungarian movements.\nAccording to the chronicler Widukind of Corvey, Otto I \"pitched his camp in the territory of the city of Augsburg and joined there the forces of Henry I, Duke of Bavaria, who was himself lying mortally ill nearby, and by Duke Conrad with a large following of Franconian knights. Conrad's unexpected arrival encouraged the warriors so much that they wished to attack the enemy immediately.\"\nThe arrival of Conrad, the exiled Duke of Lotharingia (Lorraine) and Otto I's son-in-law, was particularly heartening because he had recently thrown in his lot with the Magyars, but now returned to fight under Otto I; in the ensuing battle he lost his life. A legion of Swabians was commanded by Burchard III, Duke of Swabia, who had married Otto I's niece Hedwig. Also among those fighting under Otto I was Boleslaus I, Duke of Bohemia. Otto I himself led the \"legio regia\", stronger than any of the others in both numbers and quality.\nThe main Hungarian army blocked Otto I's way to Augsburg. A contingent of Hungarian horse-archers crossed the river west of Augsburg and immediately attacked the Bohemian legion from the flank. The Bohemians were routed and the two Swabian legions were badly damaged. The Hungarians stopped to plunder the German baggage train and Duke Conrad the Red used the opportunity to attack the vulnerable Hungarians and shatter them. Conrad returned to Otto I with captured Hungarian banners. Conrad's victory prevented the German army from being encircled.\nOtto I rallied his men with a speech in which he claimed the Germans had better weapons than the Hungarians. Otto I then led the German army into battle with the main Hungarian force, defeating them. How the main Ottonian military defeated the Hungarians, however, is somewhat unclear. This is because Widukind's account of the battle is remarkably short and lacking in detail, which is surprising considering the significance of the battle. This has left some historians to speculate how the battle played out, based on the strategies outlined in Vegetius's \"Epitome of Military Science\", which heavily influenced Ottonian strategy. According to these historians, while the infantry approached the center of the Magyar formation, Conrad's cavalry, posted on the left wing and protected on its flank from nearby cliffs, would stay out of range of the Hungarian archers but would also attempt to draw them more to their right. Meanwhile the royal legion, under Otto I's personal leadership, engaged the enemy from the right. Although the King's forces suffered losses from the archers, this gave the royal legion the opportunity to directly assault the Magyars in close combat, which was not the Magyar's area of strength. Conrad's forces would then wheel in from Otto I's left wing, putting the Hungarians in danger of being enveloped. Seeing the day going against them, the Hungarians retreated in ordered formations across the Lech to the east. Otto I's army pursued, killing every captured Hungarian. The Germans took the Hungarian camp, liberating prisoners and reclaiming booty.\nHowever, Otto I wisely and for several reasons did not chase the Magyars much longer that day. Although the Hungarians suffered heavy losses, so did the king's forces. Three legions, in the rear of the relief column, had been decimated. Furthermore, because of their heavy equipment, Otto I's men were no doubt more affected by the stifling heat than their lightly armored opponents. Simply put, the King and his men were in no position to pursue and destroy the Magyars that day, leaving the initial battle a draw. The Magyars were also known to pull off feigned retreats, when they would lure their opponents into more advantageous positions, like open fields, then they would turn and defeat them, a notable example having occurred in 910 against East Frankish forces. This time the King instead opted to spend the night after the battle in Augsburg. On 11 August he specifically issued the order that all river crossings were to be held. This was done so that as many of the Hungarians as possible, and specifically their leaders, could be captured and killed. This strategy proved successful, as Duke Henry of Bavaria captured a number of their leaders and killed them. Some Hungarians tried to flee across an unknown river but were swept away by the current. Some sought refuge in nearby villages. The destruction of the Hungarian army continued on 12 August, when heavy rainfall and flooding allowing the German troops, operating from nearby fortifications, to kill almost all the fleeing Hungarian soldiers. The majority of these fortifications had been built and fortified during the reign of Otto I's father, Henry I of Saxony, as part of his defense-in-depth strategy against enemy invaders. If these had not been in place, it is very likely that the Hungarians could have completed an orderly retreat once the floodwaters had receded and the Battle of Lechfeld would have remained a draw.\nThe captured Magyars were either executed or sent back to their ruling prince, Taksony, missing their ears and noses. The Hungarian leaders L\u00e9l, Bulcs\u00fa and S\u00far were executed after the battle. Duke Conrad was also killed, after he loosened his mail armour in the summer heat and an arrow struck his throat.\nAftermath.\nUpon destruction of the Hungarian forces, the German army proclaimed Otto I father of the fatherland and emperor. In 962, on the strength of this, Otto I went to Rome and had himself crowned Holy Roman Emperor by Pope John XII. Historian Pierre Rich\u00e9 writes that Otto I was regarded by many thereafter as a \"new Charlemagne\", which also led to him being called \"Otto the Great.\"\nThe Hungarian leaders Bulcs\u00fa, Lehel and S\u00far were taken to Regensburg and hanged with many other Hungarians.\nThe German annihilation of the Hungarian army definitively ended the attacks of Magyar nomads against Latin Europe. One of Otto's allies, the bishop of Cremona, claimed that the victory at Lechfeld left the Hungarians so cowed that they would not \"dare to mutter.\" The Hungarian historian Gyula Krist\u00f3 calls it a \"catastrophic defeat\". Following the tactical disaster, the Hungarians reached the end of almost a century as Europe's dominant military. Moreover, after 955, the Hungarians completely ceased all campaigns westwards. In addition, Otto I did not launch any further military campaigns against them; their leader Fajsz was dethroned following their defeat and succeeded as Grand Prince of the Hungarians by Taksony.\nAnalysis.\nThis battle has been viewed as a symbolic victory for the knightly cavalry, who would define European warfare in the High Middle Ages, over the nomadic light cavalry that characterized warfare during the Early Middle Ages in Central and Eastern Europe.\nPaul K. Davis writes, the \"Magyar defeat ended more than 90 years of their pillaging western Europe and convinced survivors to settle down, creating the basis for the state of Hungary.\"\nIn popular culture.\nThe battle was dramatized in season 1, episode 1 of the German documentary series, \"Die Deutschen\", titled \"Otto und das Reich\" (\"Otto and the Empire\").\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nSources.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "37087", "revid": "46469420", "url": "https://en.wikipedia.org/wiki?curid=37087", "title": "Steve McConnell", "text": "American software engineer\nSteven C. McConnell is an author of software engineering textbooks such as \"Code Complete\", \"Rapid Development\", and \"Software Estimation\". He is cited as an expert in software engineering and project management.\nCareer.\nMcConnell graduated with a bachelor's degree in philosophy, minoring in computer science, at Whitman College in Walla Walla, Washington, and a master's degree in software engineering from Seattle University. He then pursued a career in the desktop software industry, working at Microsoft, Boeing, the Russell Investment Group and several other Seattle area firms. At Microsoft, McConnell worked on TrueType as part of Windows 3.1. At Boeing, he worked on a Strategic Defense Initiative project.\nMcConnell published his first book, \"Code Complete\", in 1993.\nFrom 1996 to 1998, he was the editor of the \"Best Practices\" column in the IEEE Software magazine. From 1998 to 2002, he served as the editor-in-chief of the magazine.\n\"The New York Times\" has quoted McConnell stating that there are \"15 to 50 defects per 1,000 lines of code in delivered software\" on average.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "37091", "revid": "4246661", "url": "https://en.wikipedia.org/wiki?curid=37091", "title": "Jess (programming language)", "text": "Jess is a rule engine for the Java computing platform, written in the Java programming language. It was developed by Ernest Friedman-Hill of Sandia National Laboratories. It is a superset of the CLIPS language. It was first written in late 1995. The language provides rule-based programming for the automation of an expert system, and is often termed as an \"expert system shell\". In recent years, intelligent agent systems have also developed, which depend on a similar ability.\nRather than a procedural paradigm, where one program has a loop that is activated only one time, the declarative paradigm used by Jess applies a set of rules to a set of facts continuously by a process named \"pattern matching\". Rules can modify the set of facts, or can execute any Java code. It uses the Rete algorithm to execute rules.\nLicense.\nThe licensing for Jess is freeware for education and government use, and is proprietary software, needing a license, for commercial use. In contrast, CLIPS, which is the basis and starting code for Jess, is free and open-source software.\nCode examples.\nCode examples:\n (if (&gt; ?a ?b) then ?a else ?b))\n(deffacts myroom\n (furniture chair)\n (furniture table)\n (furniture bed)\n(deftemplate car\n (slot color)\n (slot mileage)\n (slot value)\nSample code:\n(deffacts blood-bank ; put names &amp; their types into working memory\n (blood-donor (name \"Alice\")(type \"A\"))\n (blood-donor (name \"Agatha\")(type \"A\"))\n (blood-donor (name \"Bob\")(type \"B\"))\n (blood-donor (name \"Barbara\")(type \"B\"))\n (blood-donor (name \"Jess\")(type \"AB\"))\n (blood-donor (name \"Karen\")(type \"AB\"))\n (blood-donor (name \"Onan\")(type \"O\"))\n (blood-donor (name \"Osbert\")(type \"O\"))\n(defrule can-give-to-same-type-but-not-self ; handles A &gt; A, B &gt; B, O &gt; O, AB &gt; AB, but not N1 &gt; N1\n (blood-donor (name ?name)(type ?type))\n (blood-donor (name ?name2)(type ?type2 &amp;:(eq ?type ?type2) &amp;: (neq ?name ?name2) ))\n (printout t ?name \" can give blood to \" ?name2 crlf)\n(defrule O-gives-to-others-but-not-itself ; O to O cover in above rule\n (blood-donor (name ?name)(type ?type &amp;:(eq ?type \"O\")))\n (blood-donor (name ?name2)(type ?type2 &amp;: (neq ?type ?type2) &amp;: (neq ?name ?name2) ))\n (printout t ?name \" can give blood to \" ?name2 crlf)\n(defrule A-or-B-gives-to-AB ; case O gives to AB and AB gives to AB already dealt with\n (blood-donor (name ?name)(type ?type &amp;:(or (eq ?type \"A\") (eq ?type \"B\" ))))\n (blood-donor (name ?name2)(type ?type2 &amp;: (eq ?type2 \"AB\") &amp;: (neq ?name ?name2) ))\n (printout t ?name \" can give blood to \" ?name2 crlf)\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "37096", "revid": "122189", "url": "https://en.wikipedia.org/wiki?curid=37096", "title": "Floating-point unit", "text": "Part of a computer system\nA floating-point unit (FPU), numeric processing unit (NPU), colloquially math coprocessor, is a part of a computer system specially designed to carry out operations on floating-point numbers. Typical operations are addition, subtraction, multiplication, division, and square root. Modern designs generally include a fused multiply-add instruction, which was found to be very common in real-world code. Some FPUs can also perform various transcendental functions such as exponential or trigonometric calculations, but the accuracy can be low, so some systems prefer to compute these functions in software.\nFloating-point operations were originally handled in software in early computers. Over time, manufacturers began to provide standardized floating-point libraries as part of their software collections. Some machines, those dedicated to scientific processing, would include specialized hardware to perform some of these tasks with much greater speed. The introduction of microcode in the 1960s allowed these instructions to be included in the system's instruction set architecture (ISA). Normally these would be decoded by the microcode into a series of instructions that were similar to the libraries, but on those machines with an FPU, they would instead be routed to that unit, which would perform them much faster. This allowed floating-point instructions to become universal while the floating-point hardware remained optional; for instance, on the PDP-11 one could add the floating-point processor unit at any time using plug-in expansion cards.\nThe introduction of the microprocessor in the 1970s led to a similar evolution as the earlier mainframes and minicomputers. Early microcomputer systems performed floating point in software, typically in a vendor-specific library included in ROM. Dedicated single-chip FPUs began to appear late in the decade, but they remained rare in real-world systems until the mid-1980s, and using them required software to be re-written to call them. As they became more common, the software libraries were modified to work like the microcode of earlier machines, performing the instructions on the main CPU if needed, but offloading them to the FPU if one was present. By the late 1980s, semiconductor manufacturing had improved to the point where it became possible to include an FPU with the main CPU, resulting in designs like the i486 and 68040. These designs were known as an \"integrated FPU\"s, and from the mid-1990s, FPUs were a standard feature of most CPU designs except those designed as low-cost as embedded processors.\nIn modern designs, a single CPU will typically include several arithmetic logic units (ALUs) and several FPUs, reading many instructions at the same time and routing them to the various units for parallel execution. By the 2000s, even embedded processors generally included an FPU as well.\nHistory.\nIn 1954, the IBM 704 had floating-point arithmetic as a standard feature, one of its major improvements over its predecessor the IBM 701. This was carried forward to its successors the 709, 7090, and 7094.\nIn 1963, Digital announced the PDP-6, which had floating point as a standard feature.\nIn 1963, the GE-235 featured an \"Auxiliary Arithmetic Unit\" for floating point and double-precision calculations.\nHistorically, some systems implemented floating point with a coprocessor rather than as an integrated unit (but now in addition to the CPU, e.g. GPUs\u00a0\u2013 that are coprocessors not always built into the CPU\u00a0\u2013 have FPUs as a rule, while first generations of GPUs did not). This could be a single integrated circuit, an entire circuit board or a cabinet. Where floating-point calculation hardware has not been provided, floating-point calculations are done in software, which takes more processor time, but avoids the cost of the extra hardware. For a particular computer architecture, the floating-point unit instructions may be emulated by a library of software functions; this may permit the same object code to run on systems with or without floating-point hardware. Emulation can be implemented on any of several levels: in the CPU as microcode, as an operating system function, or in user-space code. When only integer functionality is available, the CORDIC methods are most commonly used for transcendental function evaluation.\nIn most modern computer architectures, there is some division of floating-point operations from integer operations. This division varies significantly by architecture; some have dedicated floating-point registers, while some, like Intel x86, go as far as independent clocking schemes.\nCORDIC routines have been implemented in Intel x87 coprocessors (8087, 80287, 80387) up to the 80486 microprocessor series, as well as in the Motorola 68881 and 68882 for some kinds of floating-point instructions, mainly as a way to reduce the gate counts (and complexity) of the FPU subsystem.\nFloating-point operations are often pipelined. In earlier superscalar architectures without general out-of-order execution, floating-point operations were sometimes pipelined separately from integer operations.\nThe modular architecture of Bulldozer microarchitecture uses a special FPU named FlexFPU, which uses simultaneous multithreading. Each physical integer core, two per module, is single-threaded, in contrast with Intel's Hyperthreading, where two virtual simultaneous threads share the resources of a single physical core.\nFloating-point library.\nSome floating-point hardware only supports the simplest operations: addition, subtraction, and multiplication. But even the most complex floating-point hardware has a finite number of operations it can support\u00a0\u2013 for example, no FPUs directly support arbitrary-precision arithmetic.\nWhen a CPU is executing a program that calls for a floating-point operation that is not directly supported by the hardware, the CPU uses a series of simpler floating-point operations. In systems without any floating-point hardware, the CPU emulates it using a series of simpler fixed-point arithmetic operations that run on the integer arithmetic logic unit.\nThe software that lists the necessary series of operations to emulate floating-point operations is often packaged in a floating-point library.\nIntegrated FPUs.\nIn some cases, FPUs may be specialized, and divided between simpler floating-point operations (mainly addition and multiplication) and more complicated operations, like division. In some cases, only the simple operations may be implemented in hardware or microcode, while the more complex operations are implemented as software.\nIn some current architectures, the FPU functionality is combined with SIMD units to perform SIMD computation; an example of this is the augmentation of the x87 instructions set with SSE instruction set in the x86-64 architecture used in newer Intel and AMD processors.\nAdd-on FPUs.\nSeveral models of the PDP-11, such as the PDP-11/45, PDP-11/34a, PDP-11/44, and PDP-11/70, supported an add-on floating-point unit to support floating-point instructions. The PDP-11/60,261 MicroPDP-11/23 and several VAX models could execute floating-point instructions without an add-on FPU (the MicroPDP-11/23 required an add-on microcode option), and offered add-on accelerators to further speed the execution of those instructions.\nIn the 1980s, it was common in IBM PC/compatible microcomputers for the FPU to be entirely separate from the CPU, and typically sold as an optional add-on. It would only be purchased if needed to speed up or enable math-intensive programs.\nThe IBM PC, XT, and most compatibles based on the 8088 or 8086 had a socket for the optional 8087 coprocessor. The AT and 80286-based systems were generally socketed for the 80287, and 80386/80386SX-based machines\u00a0\u2013 for the 80387 and 80387SX respectively, although early ones were socketed for the 80287, since the 80387 did not exist yet. Other companies manufactured co-processors for the Intel x86 series. These included Cyrix and Weitek. Acorn Computers opted for the WE32206 to offer single, double and extended precision to its ARM powered Archimedes range, introducing a gate array to interface the ARM2 processor with the WE32206 to support the additional ARM floating-point instructions. Acorn later offered the FPA10 coprocessor, developed by ARM, for various machines fitted with the ARM3 processor.\nCoprocessors were available for the Motorola 68000 family, the 68881 and 68882. These were common in Motorola 68020/68030-based workstations, like the Sun-3 series. They were also commonly added to higher-end models of Apple Macintosh and Commodore Amiga series, but unlike IBM PC-compatible systems, sockets for adding the coprocessor were not as common in lower-end systems.\nThere are also add-on FPU coprocessor units for microcontroller units (MCUs/\u03bcCs)/single-board computer (SBCs), which serve to provide floating-point arithmetic capability. These add-on FPUs are host-processor-independent, possess their own programming requirements (operations, instruction sets, etc.) and are often provided with their own integrated development environments (IDEs).\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "37101", "revid": "7548079", "url": "https://en.wikipedia.org/wiki?curid=37101", "title": "Think tank", "text": "Organization that performs policy research and advocacy\nA think tank, or public policy institute, is an organization that performs research and advocacy concerning topics such as social policy, political strategy, economics, military, technology, and culture. Most think tanks are non-governmental organizations, but some are semi-autonomous agencies within a government, and some are associated with particular political parties, businesses, or the military. Think tanks are often funded by individual donations, with many also accepting government grants.\nThink tanks publish articles and studies, and sometimes draft legislation on particular matters of policy or society. This information is then used by governments, businesses, media organizations, social movements, or other interest groups. Think tanks range from those associated with highly academic or scholarly activities to those that are overtly ideological and pushing for particular policies, with a wide range among them in terms of the quality of their research. Later generations of think tanks have tended to be more ideologically oriented.\nModern think tanks began as a phenomenon in the United Kingdom in the 19th and early 20th centuries, with most of the rest being established in other English-speaking countries. Before 1945, they focused on the economic issues associated with industrialization and urbanization. During the Cold War, many more American and other Western think tanks were established, which often guided government Cold War policy. Since 1991, more think tanks have been established in non-Western parts of the world. Over half of all think tanks that exist today were established after 1980. As of 2023, there were more than 11,000 think tanks globally.&lt;templatestyles src=\"Template:TOC limit/styles.css\" /&gt;\nHistory.\nAccording to historian Jacob Soll, while the term \"think tank\" is modern, with its origin \"traced to the humanist academies and scholarly networks of the 16th and 17th centuries,\" evidence shows that, \"in Europe, the origins of think tanks go back to the 800s when emperors and kings began arguing with the Catholic Church about taxes. A tradition of hiring teams of independent lawyers to advise monarchs about their financial and political prerogatives against the church spans from Charlemagne all the way to the 17th century, when the kings of France were still arguing about whether they had the right to appoint bishops and receive a cut of their income.\"\nSoll cites as an early example the , created in Paris around 1620 by the brothers Pierre and Jacques Dupuy and also known after 1635 as the . The Club de l'Entresol, active in Paris between 1723 and 1731, was another prominent example of an early independent think tank focusing on public policy and current affairs, especially economics and foreign affairs.\n19th century.\nSeveral major current think tanks were founded in the 19th century. The Royal United Services Institute was founded in 1831 in London, and the Fabian Society in 1884.\n20th century.\nThe oldest United States\u2013based think tank, the Carnegie Endowment for International Peace, was founded in Washington, D.C., in 1910 by philanthropist Andrew Carnegie. Carnegie charged trustees to use the fund to \"hasten the abolition of international war, the foulest blot upon our civilization.\" The Brookings Institution was founded shortly thereafter in 1916 by Robert S. Brookings and was conceived as a bipartisan \"research center modeled on academic institutions and focused on addressing the questions of the federal government.\"\nAfter 1945, the number of policy institutes increased, with many small new ones forming to express various issues and policy agendas. Until the 1940s, most think tanks were known only by the name of the institution. During the Second World War, think tanks were often referred to as \"brain boxes\".\nBefore the 1950s, the phrase \"think tank\" did not refer to organizations. From its first appearances in the 1890s up to the 1950s, the phrase was most commonly used in American English to colloquially refer to the braincase or especially in a pejorative context to the human brain itself when commenting on an individual's failings (in the sense that something was wrong with that person's \"think tank\"). Around 1958, the first organization to be regularly described in published writings as \"the Think Tank\" (note the title case and the use of the definite article) was the Center for Advanced Study in the Behavioral Sciences. However, the Center does not count itself as and is not perceived to be a think tank in the contemporary sense. During the 1960s, the phrase \"think tank\" was attached more broadly to meetings of experts, electronic computers, and independent military planning organizations. The prototype and most prominent example of the third category was the RAND Corporation, which was founded in 1946 as an offshoot of Douglas Aircraft and became an independent corporation in 1948. In the 1970s, the phrase became more specifically defined in terms of RAND and others. During the 1980s and 1990s, the phrase evolved again to arrive at its broader contemporary meaning of an independent public policy research institute.\nFor most of the 20th century, such institutes were found primarily in the United States, along with much smaller numbers in Canada, the United Kingdom, and Western Europe. Although think tanks had also existed in Japan for some time, they generally lacked independence, having close associations with government ministries or corporations. There has been a veritable proliferation of \"think tanks\" around the world that began during the 1980s as a result of globalization, the end of the Cold War, and the emergence of transnational problems. Two-thirds of all the think tanks that exist today were established after 1970 and more than half were established since 1980.\nThe effect of globalisation on the proliferation of think tanks is most evident in regions such as Africa, Eastern Europe, Central Asia, and parts of Southeast Asia, where there was a concerted effort by other countries to assist in the creation of independent public policy research organizations. A survey performed by the Foreign Policy Research Institute's Think Tanks and Civil Societies Program underscores the significance of this effort and documents the fact that most of the think tanks in these regions have been established since 1992.\n21st century.\nAs of 2014[ [update]], there were more than 11,000 of these institutions worldwide. Many of the more established think tanks, created during the Cold War, are focused on international affairs, security studies, and foreign policy.\nThe median think tank publishes 138 articles a year, albeit there is substantial variation, with the Brookings Institution having published 3,880 reports in 2020 alone. Other prolific publishers include the Wilson Center or the CSIS.\nTypes.\nThink tanks vary by ideological perspectives, sources of funding, topical emphasis and prospective consumers. Funding may also represent who or what the institution wants to influence; in the United States, for example, \"Some donors want to influence votes in Congress or shape public opinion, others want to position themselves or the experts they fund for future government jobs, while others want to push specific areas of research or education.\"\nMcGann distinguishes think tanks based on independence, source of funding and affiliation, grouping think tanks into autonomous and independent, quasi-independent, government affiliated, quasi-governmental, university affiliated, political-party affiliated or corporate.\nA new trend, resulting from globalization, is collaboration between policy institutes in different countries. For instance, the Carnegie Endowment for International Peace operates offices in Washington, D.C., Beijing, Beirut, Brussels and formerly in Moscow, where it was closed in April 2022.\nThe Think Tanks and Civil Societies Program (TTCSP) at the University of Pennsylvania, led by James McGann, annually rates policy institutes worldwide in a number of categories and presents its findings in the Global Go-To Think Tanks rating index. However, this method of the study and assessment of policy institutes has been criticized by researchers such as Enrique Mendizabal and Goran Buldioski, Director of the Think Tank Fund, assisted by the Open Society Institute. As the TTCSP ended its operations in 2021, the platform ThinkTankAlert started ranking think tanks globally based on their inter-citation patterns in 2025.\nActivities.\nThink tanks may attempt to broadly inform the public by holding conferences to discuss issues which they may broadcast; encouraging scholars to give public lectures, testifying before committees of governmental bodies; publishing and widely distributing books, magazines, newsletters or journals; creating mailing lists to distribute new publications; and engaging in social media.90\nThink tanks may privately influence policy by having their members accept bureaucratic positions, having members serve on political advisory boards, inviting policy-makers to events, allowing individuals to work at the think tank; employing former policy-makers; or preparing studies for policy makers.95\nGovernmental theory.\nThe role of think tanks has been conceptualized through the lens of social theory. German political scientist Dieter Plehwe argues that think tanks function as \"knowledge actors\" within a network of relationships with other knowledge actors. Such relationships including citing academics in publications or employing them on advisory boards, as well as relationships with media, political groups and corporate funders. They argue that these links allow for the construction of a \"discourse coalition\" with a common aim, citing the example of deregulation of trucking, airlines, and telecommunications in the 1970s.369 Plehwe argues that this deregulation represented a discourse coalition between the Ford Motor Company, FedEx, neo-liberal economists, the Brookings Institution and the American Enterprise Institute.372\nElite theory considers how an \"elite\" influence the actions of think tanks and potentially bypass the political process, analysing the social background and values of those who work in think tanks. Pautz criticizes this viewpoint because there is in practice a variety of viewpoints in think tanks and argues it dismisses the influence that ideas can have.424\nAdvocacy.\nIn some cases, corporate interests, military interests and political groups have found it useful to create policy institutes, advocacy organizations, and think tanks. For example, The Advancement of Sound Science Coalition was formed in the mid-1990s to dispute research finding an association between second-hand smoke and cancer. Military contractors may spend a portion of their tender on funding pro-war think tanks. According to an internal memorandum from Philip Morris Companies referring to the United States Environmental Protection Agency (EPA), \"The credibility of the EPA is defeatable, but not on the basis of ETS [environmental tobacco smoke] alone... It must be part of a larger mosaic that concentrates all the EPA's enemies against it at one time.\"\nAccording to the progressive media watchdog Fairness &amp; Accuracy in Reporting, both left-wing and right-wing policy institutes are often quoted and rarely identified as such. The result is that think tank \"experts\" are sometimes depicted as neutral sources without any ideological predispositions when, in fact, they represent a particular perspective. In the United States, think tank publications on education are subjected to expert review by the National Education Policy Center's \"Think Twice\" think tank review project.\nA 2014 \"New York Times\" report asserted that foreign governments buy influence at many United States think tanks. According to the article: \"More than a dozen prominent Washington research groups have received tens of millions of dollars from foreign governments in recent years while pushing United States government officials to adopt policies that often reflect the donors' priorities.\"\nGlobal think tanks.\nAfrican think tanks.\nGhana.\nGhana's first president, Kwame Nkrumah, set up various state-supported think tanks in the 1960s. By the 1990s, a variety of policy research centers sprang up in Africa set up by academics who sought to influence public policy in Ghana.\nOne such think tank was The Institute of Economic Affairs, Ghana, which was founded in 1989 when the country was ruled by the Provisional National Defence Council. The IEA undertakes and publishes research on a range of economic and governance issues confronting Ghana and Sub-Saharan Africa. It has also been involved in bringing political parties together to engage in dialogue. In particular it has organised Presidential debates every election year since the Ghanaian presidential election, 1996.\nNotable think tanks in Ghana include:\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nSomalia.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nSouth Africa.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nAsian think tanks.\nAfghanistan.\nAfghanistan has a number of think tanks that are in the form of governmental, non-governmental, and corporate organizations.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nBangladesh.\nBangladesh has a number of think tanks that are in the form of governmental, non-governmental, and corporate organizations.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nChina.\nIn China a number of think tanks are sponsored by governmental agencies such as Development Research Center of the State Council, but still retain sufficient non-official status to be able to propose and debate ideas more freely. In January 2012, the first non-official think tank in mainland China, South Non-Governmental Think-Tank, was established in the Guangdong province. In 2009 the China Center for International Economic Exchanges was founded.\nHong Kong.\nIn Hong Kong, early think tanks established in the late 1980s and early 1990s focused on political development, including the first direct Legislative Council members election in 1991 and the political framework of \"One Country, Two Systems\", manifested in the Sino-British Joint Declaration. After the transfer of sovereignty to China in 1997, more think tanks were established by various groups of intellectuals and professionals. They have various missions and objectives including promoting civic education; undertaking research on economic, social and political policies; and promoting \"public understanding of and participation in the political, economic, and social development of the Hong Kong Special Administrative Region\".\nThink tanks in Hong Kong include:\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nIndia.\nIndia has the world's second-largest number of think tanks. Most are based in New Delhi, and a few are government-sponsored. There are few think tanks that promote environmentally responsible and climate resilient ideas like Centre for Science and Environment, Centre for Policy Research and World Resources Institute. There are other prominent think tanks like Observer Research Foundation, Tillotoma Foundation, and Centre for Civil Society.\nIn Mumbai, Strategic Foresight Group is a global think tank that works on issues such as water diplomacy, peace and conflict and foresight (futures studies). Think tanks with a development focus include those like the National Centre for Cold-chain Development ('NCCD'), which serve to bring an inclusive policy change by supporting the Planning Commission and related government bodies with industry-specific inputs \u2013 in this case, set up at the behest of the government to direct cold chain development. Some think tanks have a fixed set of focus areas and they work towards finding out policy solutions to social problems in the respective areas.\nInitiatives such as National e-Governance Plan (to automate administrative processes) and National Knowledge Network (NKN) (for data and resource sharing amongst education and research institutions), if implemented properly, should help improve the quality of work done by think tanks.\nSome notable think tanks in India include:\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nIndonesia.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nIraq.\nOver 50 think tanks have emerged in Iraq, particularly in the Kurdistan Region. Iraq's leading think tank is the Middle East Research Institute (MERI), based in Erbil. MERI is an independent non-governmental policy research organization, established in 2014 and publishes in English, Kurdish, and Arabic. It was listed in the global ranking by the United States's Lauder Institute of the University of Pennsylvania as 46th in the Middle East.\nIsrael.\nThere are many think tank teams in Israel, including:\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nSouth Korea.\nIn South Korea, think tanks are prolific and influential and are a government go-to. Think tanks are prolific in the Korean landscape. Many policy research organisations in Korea focus on economy and most research is done in public think tanks. There is a strong emphasis on the knowledge-based economy and, according to one respondent, think tank research is generally considered high quality.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nJapan.\nJapan has over 100 think tanks, most of which cover not only policy research but also economy, technology and so on. Some are government related, but most of the think tanks are sponsored by the private sector.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nKazakhstan.\nInstitute of World Economics and Politics (IWEP) at the Foundation of the First President of the Republic of Kazakhstan was created in 2003. IWEP activities aimed at research problems of the world economy, international relations, geopolitics, security, integration and Eurasia, as well as the study of the First President of the Republic of Kazakhstan and its contribution to the establishment and strengthening of Kazakhstan as an independent state, the development of international cooperation and the promotion of peace and stability.\nThe Kazakhstan Institute for Strategic Studies under the President of the RK (KazISS) was established by the Decree of the President of RK on 16 June 1993. Since its foundation the main mission of the Kazakhstan Institute for Strategic Studies under the President of the Republic of Kazakhstan, as a national think tank, is to maintain analytical and research support for the President of Kazakhstan.\nMalaysia.\nMost Malaysian think tanks are related either to the government or a political party. Historically they focused on defense, politics and policy. However, in recent years, think tanks that focus on international trade, economics, and social sciences have also been founded.\nNotable think tanks in Malaysia include:\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nPakistan.\nPakistan's think tanks mainly revolve around social policy, internal politics, foreign security issues, and regional geo-politics. Most of these are centered on the capital, Islamabad. One such think tank is the Sustainable Development Policy Institute (SDPI), which focuses on policy advocacy and research particularly in the area of environment and social development.\nAnother policy research institute based in Islamabad is the Institute of Social and Policy Sciences (I-SAPS) which works in the fields of education, health, disaster risk reduction, governance, conflict and stabilization. Since 2007 \u2013 2008, I-SAPS has been analyzing public expenditure of federal and provincial governments.\nPhilippines.\nThink tanks in the Philippines could be generally categorized in terms of their linkages with the national government. Several were set up by the Philippine government for the specific purpose of providing research input into the policy-making process.\nRussia.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nSri Lanka.\nSri Lanka has a number of think tanks that are in the form of governmental, non-governmental and corporate organizations.\nSingapore.\nThere are several think tanks in Singapore that advise the government on various policies and as well as private ones for corporations within the region. Many of them are hosted within the local public educational institutions.\nAmong them are the Singapore Institute of International Affairs (SIIA), Institute of Southeast Asian Studies (ISEAS), and the S. Rajaratnam School of International Studies.\nTaiwan.\nIn 2017 Taiwan had 58 think tanks. As in most countries there is a mix of government- and privately-funded think tanks.\nTaiwanese think tanks in alphabetical order:\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nThailand.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nUnited Arab Emirates.\nThe UAE has been a center for political oriented think tanks which concentrate on both regional and global policy. Notable think tank have emerged in the global debate on terrorism, education &amp; economical policies in the MENA region. Think tanks include:\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nEuropean think tanks.\nBelgium.\nBrussels hosts most of the European Institutions, hence a large number of international think tanks are based there. Notable think tanks are Bruegel, the Centre for European Policy Studies (CEPS), Centre for the New Europe (CNE), the European Centre of International Political Economy (ECIPE), the European Policy Centre (EPC), the Friends of Europe, the Global Governance Institute (GGI), Liberales, and Sport and Citizenship, among others.\nBulgaria.\nBulgaria has a number of think tanks providing expertise and shaping policies, including Institute of Modern Politics.\nFinland.\nFinland has several small think tanks that provide expertise in very specific fields. Notable think tanks include:\nIn addition to specific independent think tanks, the largest political parties have their own think tank organizations. This is mainly due to support granted by state for such activity. The corporate world has focused their efforts to central representative organization Confederation of Finnish Industries, which acts as think tank in addition to negotiating salaries with workers unions. Furthermore, there is the Finnish Business and Policy Forum (\"Elinkeinoel\u00e4m\u00e4n valtuuskunta\", EVA). Agricultural and regional interests, associated with The Central Union of Agricultural Producers and Forest Owners (\"Maa- ja mets\u00e4taloustuottajain Keskusliitto\", MTK) and the Centre Party, are researched by Pellervo Economic Research (\"Pellervon taloustutkimus\", PTT). The Central Organisation of Finnish Trade Unions (\"Suomen Ammattiliittojen Keskusj\u00e4rjest\u00f6\", SAK) and the Social Democratic Party are associated with the Labour Institute for Economic Research (\"Palkansaajien tutkimuslaitos\", PT). Each of these organizations often release forecasts concerning the national economy.\nFrance.\nThe French Institute of International Relations (IFRI) was founded in 1979 and is the third oldest think tank of western Europe, after Chatham House (UK, 1920) and the Stockholm International Peace Research Institute (Sweden, 1960). The primary goals of IFRI are to develop applied research in the field of public policy related to international issues, and foster interactive and constructive dialogue between researchers, professionals, and opinion leaders. France also hosts the European Union Institute for Security Studies (EUISS), a Paris-based agency of the European Union and think tank researching security issues of relevance for the EU. There are also a number of pro-business think tanks, notably the Paris-based Fondation Concorde. The foundation focuses on increasing the competitiveness of French SME's and aims to revive entrepreneurship in France.\nOn the left, the main think tanks in France are the Fondation Jean-Jaur\u00e8s, which is organizationally linked to the French Socialist Party, and Terra Nova. Terra Nova is an independent left-leaning think tank, although it is nevertheless considered to be close to the Socialists. It works on producing reports and analyses of current public policy issues from a progressive point of view, and contributing to the intellectual renewal of social democracy.\nGermany.\nIn Germany all of the major parties are loosely associated with research foundations that play some role in shaping policy, but generally from the more disinterested role of providing research to support policymakers than explicitly proposing policy. These include the Konrad-Adenauer-Stiftung (Christian Democratic Union-aligned), the Friedrich-Ebert-Stiftung (Social Democratic Party-aligned), the Hanns-Seidel-Stiftung (Christian Social Union-aligned), the (aligned with the Greens), Friedrich Naumann Foundation (Free Democratic Party-aligned) and the Rosa Luxemburg Foundation (aligned with Die Linke).\nThe German Institute for International and Security Affairs is a foreign policy think tank. Atlantic Community is an independent, non-partisan and non-profit organization set up as a joint project of Atlantische Initiative e.V. and Atlantic Initiative United States. The Institute for Media and Communication Policy deals with media-related issues. Transparency International is a think tank on the role of corporate and political corruption in international development.\nGreece.\nIn Greece there are many think tanks, also called research organisations or institutes.\nLatvia.\nThe oldest think tank in Latvia is the Latvian Institute of International Affairs. LIIA is a non governmental and non partisan foundation, established in 1992, and their research and advocacy mainly focuses on Latvian foreign policy; Transatlantic relations; European Union policies, including its neighborhood policy and Eastern Partnership; and multilateral and bilateral relations with Russia.\nNetherlands.\nAll major political parties in the Netherlands have state-sponsored research foundations that play a role in shaping policy. The Dutch government also has its own think tank: the Scientific Council for Government Policy. The Netherlands furthermore hosts the Netherlands Institute of International Relations Clingendael, or Clingendael Institute, an independent think tank and diplomatic academy which studies various aspects of international relations.\nPoland.\nThere is a large pool of think tanks in Poland on a wide variety of subjects. The oldest state-sponsored think tank is The Western Institute in Pozna\u0144 (Polish: \"Instytut Zachodni\"). The second oldest is the Polish Institute of International Affairs (PISM) established in 1947. Another notable state-sponsored think tank is the Centre for Eastern Studies (OSW), which specializes in the countries neighboring Poland and in the Baltic Sea region, the Balkans, Turkey, the Caucasus and Central Asia. Among the private think tanks notable organizations include the Institute for Structural Research (IBS) on economic policy, The Casimir Pulaski Foundation on foreign policy, the Institute of Public Affairs (ISP) on social policy, and the Sobieski Institute.\nPortugal.\nFounded in 1970, the SEDES is one of the oldest Portuguese civic associations and think tanks. Contradit\u00f3rio think tank was founded in 2008. Contradit\u00f3rio is a non-profit, independent and non-partisan think tank.\nRomania.\nThe Romanian Academic Society (SAR), founded in 1996, is a Romanian think tank for policy research.\nRussia.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nSerbia.\nThe Foundation for the Advancement of Economics (FREN) was founded in 2005 by the Belgrade University's Faculty of Economics.\nSlovakia.\nThink tanks originating in Slovakia:\nInternational think tanks with presence in Slovakia:\nSpain.\nThe Elcano Royal Institute was created in 2001 following the example of the Royal Institute of International Affairs (Chatham House) in the UK, although it is closely linked to (and receives funding from) the government in power.\nFormer Prime Minister Jos\u00e9 Maria Aznar presides over the Fundaci\u00f3n para el Analisis y los Estudios Sociales (FAES), a policy institute that is associated with the conservative Popular Party (PP). Also linked to the PP is the Grupo de Estudios Estrat\u00e9gicos (GEES), which is known for its defense- and security-related research and analysis. For its part, the Fundaci\u00f3n Alternativas is independent but close to left-wing ideas. The Socialist Partido Socialista Obrero Espa\u00f1ol (PSOE) created Fundaci\u00f3n Ideas in 2009 and dissolved it in January 2014. Also in 2009, the centrist Union, Progress and Democracy (UPyD) created Fundaci\u00f3n Progreso y Democracia (FPyD).\nSweden.\nTimbro is a free market think tank and book publisher based in Stockholm.\nSwitzerland.\nThink tanks based within Switzerland include:\nUkraine.\nAs of 2022, there are nearly 100 registered think tanks in Ukraine, including:\nUnited Kingdom.\nIn Britain, think tanks play a similar role to the United States, attempting to shape policy, and indeed there is some cooperation between British and American think tanks. For example, the London-based think tank Chatham House and the Council on Foreign Relations were both conceived at the Paris Peace Conference, 1919 and have remained sister organisations.\nThe Bow Group, founded in 1951, is the oldest centre-right think tank and many of its members have gone on to serve as Members of Parliament or Members of the European Parliament. Past chairmen have included Conservative Party leader Michael Howard, Margaret Thatcher's longest-serving Cabinet Minister Geoffrey Howe, Chancellor of the Exchequer Norman Lamont and former British Telecom chairman Christopher Bland.\nSince 2000, a number of influential centre-right think tanks have emerged including Policy Exchange, Centre for Social Justice and most recently Onward.\nOceanian think tanks.\nAustralia.\nMost Australian think tanks are based at universities \u2013 for example, the Melbourne Institute \u2013 or are government-funded \u2013 for example, the Productivity Commission or the CSIRO.\nPrivate sources fund about 20 to 30 \"independent\" Australian think tanks. The best-known of these think tanks play a much more limited role in Australian public and business policy-making than do their equivalents in the United States. However, in the past decade the number of think tanks has increased substantially. Prominent think tanks on the right include the Centre for Independent Studies, the Sydney Institute and the Institute of Public Affairs. Prominent think tanks on the left include the McKell Institute, Per Capita, the Australia Institute, the Lowy Institute and the Centre for Policy Development.\nThink tanks in Australia include:\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nNew Zealand.\nThink tanks based in New Zealand include:\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nNorth American think tanks.\nCanada.\nCanada has many notable think tanks (listed in alphabetical order). Each has specific areas of interest with some overlaps.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nUnited States.\nAs the classification is most often used today, the oldest American think tank is the Carnegie Endowment for International Peace, founded in 1910. The Institute for Government Research, which later merged with two organizations to form the Brookings Institution, was formed in 1916. Other early twentieth century organizations now classified as think tanks include the Hoover Institution (1919), The Twentieth Century Fund (1919, and now known as the Century Foundation), the National Bureau of Economic Research (1920), the Council on Foreign Relations (1921), and the Social Science Research Council (1923). The Great Depression and its aftermath spawned several economic policy organizations, such as the National Planning Association (1934), the Tax Foundation (1937), and the Committee for Economic Development (1943).\nIn collaboration with the Douglas Aircraft Company, the Air Force set up the RAND Corporation in 1946 to develop weapons technology and strategic defense analysis.\nThe Hudson Institute is a conservative American think tank founded in 1961 by futurist, military strategist, and systems theorist Herman Kahn and his colleagues at the RAND Corporation. Recent members include Mike Pompeo, the former secretary of state under Donald Trump who joined in 2021.\nMore recently, progressive and liberal think tanks have been established, most notably the Center for American Progress and the Center for Research on Educational Access and Leadership (CREAL). The organization has close ties to former United States President Barack Obama and other prominent Democrats.\nThink tanks have been important allies for United States presidents since the Reagan administration, writing and suggesting policies to implement, and providing staff for the administration. For recent conservative presidents, think tanks such as The Heritage Foundation, the Hoover Institution, and the American Enterprise Institute (AEI) were closely associated with the Reagan administration. The George H. W. Bush administration worked closely with AEI, and the George W. Bush administration worked closely with AEI and the Hoover Institution. The Trump administration works closely with the Heritage Foundation. For recent liberal presidents, the Progressive Policy Institute and its parent the Democratic Leadership Council were closely associated with the Clinton administration, and the Center for American Progress was closely associated with the Obama and Biden administrations.\nThink tanks help shape both foreign and domestic policy. They receive funding from private donors, and members of private organizations. By 2013, the largest 21 think tanks in the US spent more than US$1billion per year. Think tanks may feel more free to propose and debate controversial ideas than people within government. The progressive media watchdog Fairness and Accuracy in Reporting (FAIR) has identified the top 25 think tanks by media citations, noting that from 2006 to 2007 the number of citations declined 17%. The FAIR report reveals the ideological breakdown of the citations: 37% conservative, 47% centrist, and 16% liberal. Their data show that the most-cited think tank was the Brookings Institution, followed by the Council on Foreign Relations, the American Enterprise Institute, The Heritage Foundation, and the Center for Strategic and International Studies.\nIn 2016, in response to scrutiny about think tanks appearing to have a \"conflict of interest\" or lack transparency, executive vice president, Martin S. Indyk of Brookings Institution \u2013 the \"most prestigious think tank in the world\" admitted that they had \"decided to prohibit corporations or corporate-backed foundations from making anonymous contributions.\" In August 2016, \"The New York Times\" published a series on think tanks that blur the line. One of the cases the journalists cited was Brookings, where scholars paid by a seemingly independent think tank \"push donors' agendas amplifying a culture of corporate influence in Washington.\"\nU.S. government think tanks.\nGovernment think tanks are also important in the United States, particularly in the security and defense field. These include the Center for Technology and National Security Policy at the National Defense University, the Center for Naval Warfare Studies at the Naval War College, and the Strategic Studies Institute at the U.S. Army War College.\nThe government funds, wholly or in part, activities at approximately 30 Federally Funded Research and Development Centers (FFRDCs). FFRDCs, are unique independent nonprofit entities sponsored and funded by the United States government to meet specific long-term technical needs that cannot be met by any other single organization. FFRDCs typically assist government agencies with scientific research and analysis, systems development, and systems acquisition. They bring together the expertise and outlook of government, industry, and academia to solve complex technical problems. These FFRDCs include the RAND Corporation, the MITRE Corporation, the Institute for Defense Analyses, the Aerospace Corporation, the MIT Lincoln Laboratory, and other organizations supporting various departments within the United States Government.\nSimilar to the above quasi-governmental organizations are Federal Advisory Committees. These groups, sometimes referred to as commissions, are a form of think tank dedicated to advising the US Presidents or the Executive branch of government. They typically focus on a specific issue and as such, might be considered similar to special interest groups. However, unlike special interest groups these committees have come under some oversight regulation and are required to make formal records available to the public. As of 2002, about 1,000 of these advisory committees were described in the FACA searchable database.\nSouth American think tanks.\nResearch done by Enrique Mendizabal shows that South American think tanks play various roles depending on their origins, historical development and relations to other policy actors. In this study, Orazio Bellettini from Grupo FARO suggests that they:\nHow a policy institute addresses these largely depends on how they work, their ideology vs. evidence credentials, and the context in which they operate including funding opportunities, the degree and type of competition they have and their staff.\nThis functional method addresses the inherit challenge of defining a think tank. As Simon James said in 1998, \"Discussion of think tanks...has a tendency to get bogged down in the vexed question of defining what we mean by 'think tank'\u2014an exercise that often degenerates into futile semantics.\" It is better (as in the Network Functions Approach) to describe what the organisation should do. Then the shape of the organisation should follow to allow this to happen. The following framework (based on Stephen Yeo's description of think tanks' mode of work) is described in Enrique Mendizabal's blog \"onthinktanks\":\nFirst, policy institutes may work in or base their funding on one or more of:\nSecond, policy institutes may base their work or arguments on:\nAccording to the National Institute for Research Advancement, a Japanese policy institute, think tanks are \"one of the main policy actors in democratic societies ..., assuring a pluralistic, open and accountable process of policy analysis, research, decision-making and evaluation\". A study in early 2009 found a total of 5,465 think tanks worldwide. Of that number, 1,777 were based in the United States and approximately 350 in Washington, DC, alone.\nArgentina.\nAs of 2009, Argentina is home to 122 think tanks, many specializing in public policy and economics issues. Argentina ranks fifth in the number of these institutions worldwide.\nBrazil.\nWorking on public policies, Brazil hosts, for example, Instituto Liberdade, a University-based Center at Tecnopuc inside the Pontif\u00edcia Universidade Cat\u00f3lica do Rio Grande do Sul, located in the South Region of the country, in the city of Porto Alegre. Instituto Liberdade is among the Top 40 think tanks in Latin America and the Caribbean, according to the 2009 Global Go To Think Tanks Index a report from the University of Pennsylvania's Think Tanks and Civil Societies Program (TTCSP).\nFunda\u00e7\u00e3o Getulio Vargas (Getulio Vargas Foundation (FGV)) is a Brazilian higher education institution. Its original goal was to train people for the country's public- and private-sector management. Today it hosts faculties (Law, Business, Economics, Social Sciences and Mathematics), libraries, and also research centers in Rio, S\u00e3o Paulo and Brasilia. It is considered by \"Foreign Policy\" magazine to be a top-five \"policymaker think tank\" worldwide.\nThe Igarap\u00e9 Institute is a Brazilian think tank focusing on public, climate, and digital security.\nTranscontinental countries (Asia-Europe).\nArmenia.\nAccording to a 2020 report, there are 32 think tanks or similar institutions in Armenia.\nThe government closed the Noravank Foundation, a government-affiliated think tank, in 2018 after almost two decades of operation. However, other think tanks continue to operate, include the Caucasus Institute, the Caucasus Research Resource Center-Armenia (CRRC-Armenia) (which publishes the \"Caucasus Barometer\" annual public opinion survey of the South Caucasus, the \"Enlight\" Public Research Center, and the AMBERD research center at the Armenian State University of Economics.\nAzerbaijan.\nAccording to research done by the University of Pennsylvania, there are a total of 12 think tanks in Azerbaijan.\nThe Center for Economic and Social Development, or CESD; in Azeri, Azerbaijan, \u0130qtisadi v\u0259 Sosial \u0130nki\u015faf M\u0259rk\u0259zi (\u0130S\u0130M) is an Azeri think tank, non-profit organization, NGO based in Baku, Azerbaijan. The center was established in 2005. CESD focuses on policy advocacy and reform, and is involved with policy research and capacity building.\nThe Economic Research Center (ERC) is a policy-research oriented non-profit think tank established in 1999 with a mission to facilitate sustainable economic development and good governance in the new public management system of Azerbaijan. It seeks to do this by building favorable interactions between the public, private and civil society and working with different networks both in local (EITI NGO Coalition, National Budget Group, Public Coalition Against Poverty, etc.) and international levels (PWYP, IBP, ENTO, ALDA, PASOS, WTO NGO Network etc.).\nThe Center for Strategic Studies under the President of Azerbaijan is a governmental, non-profit think tank founded in 2007. It focuses on domestic and foreign policy.\nRussia.\nAccording to the Foreign Policy Research Institute, Russia has 112 think tanks, while Russian think tanks claimed four of the top ten spots in 2011's \"Top Thirty Think Tanks in Central and Eastern Europe\".\nNotable Russian think tanks include:\nTurkey.\nTurkish think tanks are relatively new, having emerged in the 1960's. There are at least 20 think tanks in the country, both independent and supported by government. Many of them are sister organizations of political parties, universities or companies some are independent and others are supported by government. Most Turkish think tanks provide research and ideas, yet they play less important roles in policy making than American think tanks. Turksam, Tasam and the \"Journal of Turkish Weekly\" are the leading information sources.\nThe oldest and most influential think tank in Turkey is ESAM (The Center for Economic and Social Research; ) which was established in 1969 and has headquarters in Ankara. There are also branch offices of ESAM in Istanbul, Bursa, Konya and elsewhere. ESAM has strong international relationships, especially with Muslim countries and societies. Ideologically it performs policies, produces ideas and manages projects in parallel to Milli G\u00f6r\u00fc\u015f and also influences political parties and international strategies. The founder and leader of Milli G\u00f6r\u00fc\u015f, Necmettin Erbakan, was very concerned with the activities and brainstorming events of ESAM. In The Republic of Turkey, two presidents, four prime ministers, various ministers, many members of the parliament, and numerous mayors and bureaucrats have been members of ESAM.\nThe Turkish Economic and Social Studies Foundation (TESEV) is another leading think tank. Established in 1994, TESEV is an independent non-governmental think tank, analyzing social, political and economic policy issues facing Turkey. TESEV has raised issues about Islam and democracy, combating corruption, state reform, and transparency and accountability. TESEV serve as a bridge between academic research and policy-making. Its core program areas are democratization, good governance, and foreign policy.\nOther notable Turkish think tanks are the International Strategic Research Organisation (USAK), the Foundation for Political, Economic and Social Research (SETA), and the Wise Men Center for Strategic Studies (B\u0130LGESAM).\nPublic opinion.\nA poll by the British firm Cast From Clay found that only 20 percent of Americans trusted think tanks in 2018.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nFurther reading.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "37104", "revid": "28481209", "url": "https://en.wikipedia.org/wiki?curid=37104", "title": "Emissions trading", "text": "Market-based approach used to control pollution\nEmissions trading is a market-oriented approach to controlling pollution by providing economic incentives for reducing the emissions of pollutants. The concept is also known as cap and trade (CAT) or emissions trading scheme (ETS). One prominent example is carbon emission trading for CO2 and other greenhouse gases which is a tool for climate change mitigation. Other schemes include sulfur dioxide and other pollutants.\nIn an emissions trading scheme, a central authority or governmental body allocates or sells a limited number (a \"cap\") of permits that allow a discharge of a specific quantity of a specific pollutant over a set time period. Polluters are required to hold permits in amount equal to their emissions. Polluters that want to increase their emissions must buy permits from others willing to sell them.\nEmissions trading is a type of flexible environmental regulation that allows organizations and markets to decide how best to meet policy targets. This is in contrast to command-and-control environmental regulations such as best available technology (BAT) standards and government subsidies.\nIntroduction.\nPollution is a prime example of a market externality. An externality is an effect of some activity on an entity (such as a person) that is not party to a market transaction related to that activity. Emissions trading is a market-based approach to address pollution. The overall goal of an emissions trading plan is to minimize the cost of meeting a set emissions target.\nIn an emissions trading system, the government sets an overall limit on emissions, and defines permits (also called allowances), or limited authorizations to emit, up to the level of the overall limit. The government may sell the permits, but in many existing schemes, it gives permits to participants (regulated polluters) equal to each participant's baseline emissions. The baseline is determined by reference to the participant's historical emissions. To demonstrate compliance, a participant must hold permits at least equal to the quantity of pollution it actually emitted during the time period. If every participant complies, the total pollution emitted will be at most equal to the sum of individual limits. Because permits can be bought and sold, a participant can choose either to use its permits exactly (by reducing its own emissions); or to emit less than its permits, and perhaps sell the excess permits; or to emit more than its permits, and buy permits from other participants. In effect, the buyer pays a charge for polluting, while the seller gains a reward for having reduced emissions.\nEmissions Trading results in the incorporation of economic costs into the costs of production which incentivizes corporations to consider investment returns and capital expenditure decisions with a model that includes the price of carbon and greenhouse gases (GHG).\nIn many schemes, organizations which do not pollute (and therefore have no obligations) may also trade permits and financial derivatives of permits.\nIn some schemes, participants can bank allowances to use in future periods. In some schemes, a proportion of all traded permits must be retired periodically, causing a net reduction in emissions over time. Thus, environmental groups may buy and retire permits, driving up the price of the remaining permits according to the law of demand. In most schemes, permit owners can donate permits to a nonprofit entity and receive a tax deductions. Usually, the government lowers the overall limit over time, with an aim towards a national emissions reduction target.\nThere are active trading programs in several air pollutants. An earlier application was the US national market to reduce acid rain. The United States now has several regional markets in nitrogen oxides.\nHistory.\nThe efficiency of what later was to be called the \"cap-and-trade\" approach to air pollution abatement was first demonstrated in a series of micro-economic computer simulation studies between 1967 and 1970 for the National Air Pollution Control Administration (predecessor to the United States Environmental Protection Agency's Office of Air and Radiation) by Ellison Burton and William Sanjour. These studies used mathematical models of several cities and their emission sources in order to compare the cost and effectiveness of various control strategies. Each abatement strategy was compared with the \"least-cost solution\" produced by a computer optimization program to identify the least-costly combination of source reductions in order to achieve a given abatement goal. In each case it was found that the least-cost solution was dramatically less costly than the same amount of pollution reduction produced by any conventional abatement strategy. Burton and later Sanjour along with Edward H. Pechan continued improving and advancing these computer models at the newly created U.S. Environmental Protection Agency. The agency introduced the concept of computer modeling with least-cost abatement strategies (i.e., emissions trading) in its 1972 annual report to Congress on the cost of clean air. This led to the concept of \"cap and trade\" as a means of achieving the \"least-cost solution\" for a given level of abatement.\nThe development of emissions trading over the course of its history can be divided into four phases:\nIn the United States, the acid rain related emission trading system was principally conceived by C. Boyden Gray, a G.H.W. Bush administration attorney. Gray worked with the Environmental Defense Fund (EDF), who worked with the EPA to write the bill that became law as part of the Clean Air Act of 1990. The new emissions cap on NOx and gases took effect in 1995, and according to \"Smithsonian\" magazine, those acid rain emissions dropped 3 million tons that year.\nEconomics.\nIt is possible for a country to reduce emissions using a command-and-control approach, such as regulation, direct and indirect taxes. The cost of that approach differs between countries because the Marginal Abatement Cost Curve (MAC)\u2014the cost of eliminating an additional unit of pollution\u2014differs by country.\nCoase model.\nCoase (1960) argued that social costs could be accounted for by negotiating property rights according to a particular objective. Coase's model assumes perfectly operating markets and equal bargaining power among those arguing for property rights. \nIn Coase's model, efficiency, i.e., achieving a given reduction in emissions at lowest cost, is promoted by the market system. This can also be looked at from the perspective of having the greatest flexibility to reduce emissions. Flexibility is desirable because the marginal costs, that is to say, the incremental costs of reducing emissions, varies among countries. Emissions trading allows emission reductions to be first made in locations where the marginal costs of abatement are lowest (Bashmakov \"et al\"., 2001). Over time, efficiency can also be promoted by allowing \"banking\" of permits (Goldemberg \"et al\"., 1996, p.\u00a030). This allows polluters to reduce emissions at a time when it is most efficient to do so.\nEquity.\nOne of the advantages of Coase's model is that it suggests that fairness (equity) can be addressed in the distribution of property rights, and that regardless of how these property rights are assigned, the market will produce the most efficient outcome. In reality, according to the held view, markets are not perfect, and it is therefore possible that a trade-off will occur between equity and efficiency (Halsn\u00e6s \"et al\"., 2007).\nTrading.\nIn an emissions trading system, permits may be traded by emitters who are liable to hold a sufficient number of permits in system. Some analysts argue that allowing others to participate in trading, e.g., private brokerage firms, can allow for better management of risk in the system, e.g., to variations in permit prices (Bashmakov \"et al.\", 2001). It may also improve the efficiency of system. According to Bashmakov \"et al\". (2001), regulation of these other entities may be necessary, as is done in other financial markets, e.g., to prevent abuses of the system, such as insider trading.\nIncentives and allocation.\nEmissions trading gives polluters an incentive to reduce their emissions. However, there are possible perverse incentives that can exist in emissions trading. Allocating permits on the basis of past emissions (\"grandfathering\") can result in firms having an incentive to maintain emissions. For example, a firm that reduced its emissions would receive fewer permits in the future (IMF, 2008, pp.\u00a025\u201326). There are costs that emitters do face, e.g., the costs of the fuel being used, but there are other costs that are not necessarily included in the price of a good or service. These other costs are called external costs (Halsn\u00e6s \"et al.\", 2007). This problem can also be criticized on ethical grounds, since the polluter is being paid to reduce emissions (Goldemberg \"et al\"., 1996, p.\u00a038). On the other hand, a permit system where permits are auctioned rather than given away, provides the government with revenues. These revenues might be used to improve the efficiency of overall climate policy, e.g., by funding energy efficiency programs (ACEEE 2019) or reductions in distortionary taxes (Fisher \"et al\"., 1996, p.\u00a0417).\nIn Coase's model of social costs, either choice (grandfathering or auctioning) leads to efficiency. In reality, grandfathering subsidizes polluters, meaning that polluting industries may be kept in business longer than would otherwise occur. Grandfathering may also reduce the rate of technological improvement towards less polluting technologies (Fisher \"et al.\", 1996, p.\u00a0417).\nWilliam Nordhaus argues that allocations cost the economy as they cause the under utilization an efficient form of taxation. Nordhaus argues that normal income, goods or service taxes distort efficient investment and consumption, so by using pollution taxes to generate revenue an emissions scheme can increase the efficiency of the economy.\nForm of allocation\nThe economist Ross Garnaut states that permits allocated to existing emitters by 'grandfathering' are not 'free'. As the permits are scarce they have value and the benefit of that value is acquired in full by the emitter. The cost is imposed elsewhere in the economy, typically on consumers who cannot pass on the costs.\nMarket and least-cost.\nSome economists have urged the use of market-based instruments such as emissions trading to address environmental problems instead of prescriptive \"command-and-control\" regulation. Command and control regulation is criticized for being insensitive to geographical and technological differences, and therefore inefficient; however, this is not always so, as shown by the WWII rationing program in the U.S. in which local and regional boards made adjustments for these differences.\nAfter an emissions limit has been set by a government political process, individual companies are free to choose how or whether to reduce their emissions. Failure to report emissions and surrender emission permits is often punishable by a further government regulatory mechanism, such as a fine that increases costs of production. Firms will choose the least-cost way to comply with the pollution regulation, which will lead to reductions where the least expensive solutions exist, while allowing emissions that are more expensive to reduce.\nUnder an emissions trading system, each regulated polluter has flexibility to use the most cost-effective combination of buying or selling emission permits, reducing its emissions by installing cleaner technology, or reducing its emissions by reducing production. The most cost-effective strategy depends on the polluter's marginal abatement cost and the market price of permits. In theory, a polluter's decisions should lead to an economically efficient allocation of reductions among polluters, and lower compliance costs for individual firms and for the economy overall, compared to command-and-control mechanisms.\nMeasuring, reporting, verification and enforcement.\nIn some industrial processes, emissions can be physically measured by inserting sensors and flowmeters in chimneys and stacks, but many types of activity rely on theoretical calculations instead of measurement. Depending on local legislation, measurements may require additional checks and verification by government or third party auditors, prior or post submission to the local regulator.\nEnforcement methods include fines and sanctions for polluters that have exceeded their allowances. Concerns include the cost of MRV and enforcement, and the risk that facilities may lie about actual emissions.\nPollution markets.\nAn emission license directly confers a right to emit pollutants up to a certain rate.\nIn contrast, a pollution license for a given location confers the right to emit pollutants at a rate which will cause no more than a specified increase at the pollution-level. For concreteness, consider the following model.\nAs an example, consider three countries along a river (as in the fair river sharing setting).\nSo the matrix formula_7 in this case is a triangular matrix of ones.\nEach pollution-license for location formula_13 permits its holder to emit pollutants that will cause at most this level of pollution at location formula_13. Therefore, a polluter that affects water quality at a number of points has to hold a portfolio of licenses covering all relevant monitoring-points. In the above example, if country 2 wants to emit a unit of pollutant, it should purchase two permits: one for location 2 and one for location 3.\nMontgomery shows that, while both markets lead to efficient license allocation, the market in pollution-licenses is more widely applicable than the market in emission-licenses.\nInternational emissions trading.\nThe nature of the pollutant plays a very important role when policy-makers decide which framework should be used to control pollution. CO2 acts globally, thus its impact on the environment is generally similar wherever in the globe it is released. So the location of the originator of the emissions does not matter from an environmental standpoint.\nThe policy framework is different for regional pollutants (e.g. SO2 and NOx, and also mercury) because the impact of these pollutants may differ by location. The same amount of a regional pollutant can exert a very high impact in some locations and a low impact in other locations, so it matters where the pollutant is released. This is known as the \"Hot Spot\" problem.\nA Lagrange framework is commonly used to determine the least cost of achieving an objective, in this case the total reduction in emissions required in a year. In some cases, it is possible to use the Lagrange optimization framework to determine the required reductions for each country (based on their MAC) so that the total cost of reduction is minimized. In such a scenario, the Lagrange multiplier represents the market allowance price (P) of a pollutant, such as the current market price of emission permits in Europe and the US.\nCountries face the permit market price that exists in the market that day, so they are able to make individual decisions that would minimize their costs while at the same time achieving regulatory compliance. This is also another version of the Equi-Marginal Principle, commonly used in economics to choose the most economically efficient decision.\nPrices versus quantities, and the safety valve.\nThere has been longstanding debate on the relative merits of \"price\" versus \"quantity\" instruments to achieve emission reductions.\nAn emission cap and permit trading system is a \"quantity\" instrument because it fixes the overall emission level (quantity) and allows the price to vary. Uncertainty in future supply and demand conditions (market volatility) coupled with a fixed number of pollution permits creates an uncertainty in the future price of pollution permits, and the industry must accordingly bear the cost of adapting to these volatile market conditions. The burden of a volatile market thus lies with the industry rather than the controlling agency, which is generally more efficient. However, under volatile market conditions, the ability of the controlling agency to alter the caps will translate into an ability to pick \"winners and losers\" and thus presents an opportunity for corruption.\nIn contrast, an emission tax is a \"price\" instrument because it fixes the price while the emission level is allowed to vary according to economic activity. A major drawback of an emission tax is that the environmental outcome (e.g. a limit on the amount of emissions) is not guaranteed. On one hand, a tax will remove capital from the industry, suppressing possibly useful economic activity, but conversely, the polluter will not need to hedge as much against future uncertainty since the amount of tax will track with profits. The burden of a volatile market will be borne by the controlling (taxing) agency rather than the industry itself, which is generally less efficient. An advantage is that, given a uniform tax rate and a volatile market, the taxing entity will not be in a position to pick \"winners and losers\" and the opportunity for corruption will be less.\nAssuming no corruption and assuming that the controlling agency and the industry are equally efficient at adapting to volatile market conditions, the best choice depends on the sensitivity of the costs of emission reduction, compared to the sensitivity of the benefits (i.e., climate damage avoided by a reduction) when the level of emission control is varied.\nA third option, known as a \"safety valve\", is a hybrid of the price and quantity instruments. The system is essentially an emission cap and permit trading system but the maximum (or minimum) permit price is capped. Emitters have the choice of either obtaining permits in the marketplace or buying them from the government at a specified trigger price (which could be adjusted over time). The system is sometimes recommended as a way of overcoming the fundamental disadvantages of both systems by giving governments the flexibility to adjust the system as new information comes to light. It can be shown that by setting the trigger price high enough, or the number of permits low enough, the safety valve can be used to mimic either a pure quantity or pure price mechanism.\nComparison with other methods of emission reduction.\nCap and trade is the textbook example of an \"emissions trading program\". Other market-based approaches include baseline-and-credit, and pollution tax. They all put a price on pollution (for example, see carbon price), and so provide an economic incentive to reduce pollution beginning with the lowest-cost opportunities. By contrast, in a command-and-control approach, a central authority designates pollution levels each facility is allowed to emit. Cap and trade essentially functions as a tax where the tax rate is variable based on the relative cost of abatement per unit, and the tax base is variable based on the amount of abatement needed.\nBaseline and credit.\nIn a baseline and credit program, polluters can create permits, called credits or offsets, by reducing their emissions below a baseline level, which is often the historical emissions level from a designated past year. Such credits can be bought by polluters that have a regulatory limit.\nPollution tax.\nEmissions fees or environmental tax is a surcharge on the pollution created while producing goods and services. For example, a carbon tax is a tax on the carbon content of fossil fuels that aims to discourage their use and thereby reduce carbon dioxide emissions. The two approaches are overlapping sets of policy designs. Both can have a range of scopes, points of regulation, and price schedules. They can be fair or unfair, depending on how the revenue is used. Both have the effect of increasing the price of goods (such as fossil fuels) to consumers. A comprehensive, upstream, auctioned cap-and-trade system is very similar to a comprehensive, upstream carbon tax. Yet, many commentators sharply contrast the two approaches.\nThe main difference is what is defined and what derived. A tax is a price control, while a cap-and-trade system is a quantity control instrument. That is, a tax is a unit price for pollution that is set by authorities, and the market determines the quantity emitted; in cap and trade, authorities determine the amount of pollution, and the market determines the price. This difference affects a number of criteria.\nResponsiveness to inflation: Cap-and-trade has the advantage that it adjusts to inflation (changes to overall prices) automatically, while emissions fees must be changed by regulators.\nResponsiveness to cost changes: It is not clear which approach is better. It is possible to combine the two into a safety valve price: a price set by regulators, at which polluters can buy additional permits beyond the cap.\nResponsiveness to recessions: This point is closely related to responsiveness to cost changes, because recessions cause a drop in demand. Under cap and trade, the emissions cost automatically decreases, so a cap-and-trade scheme adds another automatic stabilizer to the economy\u2014in effect, an automatic fiscal stimulus. However, a lower pollution price also results in reduced efforts to reduce pollution. If the government is able to stimulate the economy regardless of the cap-and-trade scheme, an excessively low price causes a missed opportunity to cut emissions faster than planned. Instead, it might be better to have a price floor (a tax). This is especially true when cutting pollution is urgent, as with greenhouse gas emissions. A price floor also provides certainty and stability for investment in emissions reductions: recent experience from the UK shows that nuclear power operators are reluctant to invest on \"un-subsidized\" terms unless there is a guaranteed price floor for carbon (which the EU emissions trading scheme does not presently provide).\nResponsiveness to uncertainty: As with cost changes, in a world of uncertainty, it is not clear whether emissions fees or cap-and-trade systems are more efficient\u2014it depends on how fast the marginal social benefits of reducing pollution fall with the amount of cleanup (e.g., whether inelastic or elastic marginal social benefit schedule).\nOther: The magnitude of the tax will depend on how sensitive the supply of emissions is to the price. The permit price of cap-and-trade will depend on the pollutant market. A tax generates government revenue, but full-auctioned emissions permits can do the same. A similar upstream cap-and-trade system could be implemented. An upstream carbon tax might be the simplest to administer. Setting up a complex cap-and-trade arrangement that is comprehensive has high institutional needs.\nCommand-and-control regulation.\nCommand and control is a system of regulation that prescribes emission limits and compliance methods for each facility or source. It is the traditional approach to reducing air pollution.\nCommand-and-control regulations are more rigid than incentive-based approaches such as pollution fees and cap and trade. An example of this is a performance standard which sets an emissions goal for each polluter that is fixed and, therefore, the burden of reducing pollution cannot be shifted to the firms that can achieve it more cheaply. As a result, performance standards are likely to be more costly overall. The additional costs would be passed to end consumers.\nTrading systems.\nApart from the dynamic development in carbon emission trading, other pollutants have also been targeted.\nUnited States.\nSulfur dioxide.\nAn early example of an emission trading system has been the sulfur dioxide (SO2) trading system under the framework of the Acid Rain Program of the 1990 Clean Air Act in the U.S. Under the program, which is essentially a cap-and-trade emissions trading system, SO2 emissions were reduced by 50% from 1980 levels by 2007. Some experts argue that the cap-and-trade system of SO2 emissions reduction has reduced the cost of controlling acid rain by as much as 80% versus source-by-source reduction. The SO2 program was challenged in 2004, which set in motion a series of events that led to the 2011 Cross-State Air Pollution Rule (CSAPR). Under the CSAPR, the national SO2 trading program was replaced by four separate trading groups for SO2 and NOx.\nSO2 emissions from Acid Rain Program sources have fallen from 17.3 million tons in 1980 to about 7.6 million tons in 2008, a decrease in emissions of 56 percent. A 2014 EPA analysis estimated that implementation of the Acid Rain Program avoided between 20,000 and 50,000 incidences of premature mortality annually due to reductions of ambient PM2.5 concentrations, and between 430 and 2,000 incidences annually due to reductions of ground-level ozone.\nNitrogen oxides.\nIn 2003, the Environmental Protection Agency (EPA) began to administer the Budget Trading Program (NBP) under the State Implementation Plan (also known as the \"NOx SIP Call\"). The Budget Trading Program was a market-based cap and trade program created to reduce emissions of nitrogen oxides (NOx) from power plants and other large combustion sources in the eastern United States. NOx is a prime ingredient in the formation of ground-level ozone (smog), a pervasive air pollution problem in many areas of the eastern United States. The NBP was designed to reduce NOx emissions during the warm summer months, referred to as the ozone season, when ground-level ozone concentrations are highest. In March 2008, EPA again strengthened the 8-hour ozone standard to 0.075 parts per million (ppm) from its previous 0.08 ppm.\nOzone season emissions decreased by 43 percent between 2003 and 2008, even while energy demand remained essentially flat during the same period. CAIR will result in $85 billion to $100 billion in health benefits and nearly $2 billion in visibility benefits per year by 2015 and will substantially reduce premature mortality in the eastern United States.\nNOx reductions due to the Budget Trading Program have led to improvements in ozone and PM2.5, saving an estimated 580 to 1,800 lives in 2008.\nA 2017 study in the \"American Economic Review\" found that the Budget Trading Program decreased emissions and ambient ozone concentrations. The program reduced expenditures on medicine by about 1.5% ($800 million annually) and reduced the mortality rate by up to 0.5% (2,200 fewer premature deaths, mainly among individuals 75 and older).\nVolatile organic compounds.\nIn the United States the Environmental Protection Agency (EPA) classifies Volatile Organic Compounds (VOCs) as gases emitted from certain solids and liquids that may have adverse health effects. These VOCs include a variety of chemicals that are emitted from a variety of different products. These include products such as gasoline, perfumes, hair spray, fabric cleaners, PVC, and refrigerants; all of which can contain chemicals such as benzene, acetone, methylene chloride, freons, formaldehyde.\nVOCs are also monitored by the United States Geological Survey for its presence in groundwater supply. The USGS concluded that many of the nations aquifers are at risk to low-level VOC contamination. The common symptoms of short levels of exposure to VOCs include headaches, nausea, and eye irritation. If exposed for an extended period of time the symptoms include cancer and damage to the central nervous system.\nChina.\nIn an effort to reverse the adverse consequences of air pollution, in 2006, China started to consider a national pollution permit trading system in order to use market-based mechanisms to incentivize companies to cut pollution. This has been based on a previous pilot project called the Industrial emission trading pilot scheme, which was launched in 2002. Four provinces, three municipalities and one state-owned enterprise were involved in this pilot project (also known as the 4+3+1 project).80 They are Shandong, Shanxi, Jiangsu, Henan, Shanghai, Tianjin, Liuzhou and China Huaneng Group, a state-owned company in the power industry.\nIn 2014, when the Chinese government started considering a national level pollution permit trading system again, there were more than 20 local pollution permit trading platforms. The Yangtze River Delta region as a whole has also run test trading, but the scale was limited. In the same year, the Chinese government proposed establishing a carbon market, focused on CO2 reduction later in the decade, and it is a separate system from the pollution permit trading.\nFollowing these regional efforts, China established its national Emissions Trading System in 2017.28\nA 2021 study in \"PNAS\" found that China's emissions trading system effectively reduced firm emissions despite low carbon prices and infrequent trading. The system reduced total emissions by 16.7% and emission intensity by 9.7%.\nLinked trading systems.\nDistinct cap-and-trade systems can be linked together through the mutual or unilateral recognition of emissions allowances for compliance. Linking systems creates a larger carbon market, which can reduce overall compliance costs, increase market liquidity and generate a more stable carbon market. Linking systems can also be politically symbolic as it shows willingness to undertake a common effort to reduce GHG emissions. Some scholars have argued that linking may provide a starting point for developing a new, bottom-up international climate policy architecture, whereby multiple unique systems successively link their various systems.\nIn 2014, the U.S. state of California (which is the world's fifth largest economy if it were a nation, between Germany and the United Kingdom in size) and the Canadian province of Qu\u00e9bec successfully linked their systems. In 2015, the provinces of Ontario and Manitoba agreed to join the linked system between Quebec and California. On 22 September 2017, the premiers of Quebec and Ontario, and the Governor of California, signed the formal agreement establishing the linkage.\nRenewable energy certificates.\nRenewable Energy Certificates (occasionally referred to as or \"green tags\"), are a largely unrelated form of market-based instruments that are used to achieve renewable energy targets, which may be environmentally motivated (like emissions reduction targets), but may also be motivated by other aims, such as energy security or industrial policy.\nCriticism.\nDistributional effects.\nThe US Congressional Budget Office (CBO, 2009) examined the potential effects of the American Clean Energy and Security Act on US households. This act relies heavily on the free allocation of permits. The Bill was found to protect low-income consumers, but it was recommended that the Bill be made more efficient by reducing welfare provisions for corporations, and that more resources be made available for consumer relief. A cap-and-trade initiative in the U.S. Northeast caused concerns it would be regressive and poorer households would absorb most of the new tax.\nEffectiveness.\nA 2008 book compiling research on emissions trading in the European Union cautiously endorses the effectiveness of emissions trading in practice: \"Notably for the greenhouse gas emissions problem, emissions trading seems to be very much suited to reaching the necessary reductions in a cost-effective way.\" Recent empirical research supports this assessment. A 2024 systematic review and meta-analysis of 80 ex-post evaluations across 21 carbon-pricing systems found average emissions reductions of approximately 5\u201321% after implementation (around 4\u201315% after correcting for publication bias). Firm-level causal evidence also indicates that the EU Emissions Trading System reduced regulated manufacturers\u2019 CO\u2082 emissions by 14\u201316% without detectable losses in output or employment, achieved primarily through investments that lowered emissions intensity and no observed carbon leakage.\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "37105", "revid": "49736723", "url": "https://en.wikipedia.org/wiki?curid=37105", "title": "Metin Ka\u00e7an", "text": "Turkish writer (1961\u20132013)\nMetin Ka\u00e7an (15 November 1961 \u2013 6 January 2013) was a Turkish author, who is best known for his novel \"A\u011f\u0131r Roman\" (\"Cholera Street\"), which was translated into German (Ka\u00e7an 2003), and a movie (\"A\u011f\u0131r Roman\"), directed by Mustafa Alt\u0131oklar (1999), was based on it.\nLiterary career.\nBesides \"A\u011f\u0131r Roman\", Ka\u00e7an is also the author of the novel \"F\u0131nd\u0131k Sekiz\", a collection of short stories, \"A ship to the Islands\" (\"Adalara Vapur\", Ka\u00e7an 2002), and a book written in a mixed style between prose and poetry, entitled \"The Tiger at Withdrawal\" (\"Harman Kaplan\", Ka\u00e7an 1999).\nMuch of Ka\u00e7an's writings deals with life in Istanbul, in particular its poor quarter Dolapdere (not far from Taksim Square). To Dolapdere, he sarcastically gave the name \"Cholera\" (\"Kolera\" in Turkish) in \"A\u011f\u0131r Roman\", thereby recalling both its shabbiness and the fact that the great Polish poet Adam Mickiewicz died there from the cholera in 1855. Mickiewicz Museum at Dolapdere, still open to visitors today, figures in \"A\u011f\u0131r Roman\". The title of this novel plays ingeniously with the polysemy of the Turkish word \"Roman\", which means both \"gypsy\" and \"novel\". Also, together with the adjective \"a\u011f\u0131r\", which means \"heavy\" or \"slow\" in Turkish, \"Roman\" is the designation for a special kind of street music, played by some of the novel's protagonists.\n\"A\u011f\u0131r Roman\" tells the tragic story of a young hero, who grows up in \"Cholera\" quarter but finally fails and commits suicide. His failure parallels the failure of the quarter itself, whose ancient structures as well as its multi-ethnic and multi-religious composition disintegrate.\n\"F\u0131nd\u0131k Sekiz\" tells a story about two cars, that appear sometimes as personified figures, and that take the semi-autobiographical protagonist Meto on a mystical journey. At the same time, Meto's conflict with a woman, who manages to have him thrown into prison through fraudulent statements, is related, which might reflect some of Ka\u00e7an's own experiences.\nKa\u00e7an's style is heavily imbued with Turkish slang. This choice gives his writings a non-conformistic, frequently vulgar, but overall extremely vivid and creative tone, which has been hailed, among others, by Y\u0131ld\u0131z Ecevit. Other characteristics of his writing are the personification of natural phenomena\nand inanimate items such as cars (in particular in \"F\u0131nd\u0131k Sekiz\"), autobiographical details (Ka\u00e7an grew up in Dolapdere), the blurring of the limitations of poetry and prose, and references to mysticism, in particular Muslim mysticism (Sufism). His best-selling novel, \"A\u011f\u0131r Roman\" was translated into French by Actes Sud in 2010.\nRape conviction.\nIn 1995, Ka\u00e7an was arrested for torturing and raping his ex-girlfriend. He was released on bail pending the outcome of the trial, which ended five years later with a prison sentence of eight years and nine months. The appeals court upheld Ka\u00e7an's conviction. In 2006, he was caught by the police in his hometown while attending the funeral of a relative and sent to prison to serve his sentence. After spending close to four years in prison, the remainder of his sentence was commuted.\nDeath.\nKa\u00e7an committed suicide by jumping from the Bosphorus Bridge on 6 January 2013. On that day, he took a cab in Esenler on the European side of Istanbul and requested to be driven to \u00dcsk\u00fcdar on the Asian side. He asked the driver to stop on the bridge so that he could take photographs. He got out of the vehicle, ran to the edge of the bridge, and threw himself off. His brother confirmed the suicide on 8 January. His body washed ashore on 18 January, twelve days after his disappearance, on the coast of Marmara Sea at Beylikd\u00fcz\u00fc. Ka\u00e7an was 51 years old.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "37107", "revid": "27015025", "url": "https://en.wikipedia.org/wiki?curid=37107", "title": "United Nations Foundation", "text": "Charitable organization\nThe United Nations Foundation is a charitable organization headquartered in Washington, D.C., that supports the United Nations and its activities. It was established in 1998 with a $1billion gift to the United Nations by philanthropist Ted Turner, who believed the UN was crucial for addressing the world's problems. Originally primarily a grantmaker, the UN Foundation has evolved into a strategic partner to the UN, mobilizing support to advance the Sustainable Development Goals (SDGs), and help the UN address issues such as climate change, global health, gender equality, human rights, data and technology, peace, and humanitarian responses. The UN Foundation's main work occurs through building public-private partnerships, communities, initiatives, campaigns, and alliances to broaden support for the UN and solve global problems. The UN Foundation has helped build awareness and advocate for action on, among others, antimicrobial resistance, regional action on climate change, local implementation of the SDGs, as well as global campaigns such as Nothing But Nets against malaria, the Measles &amp; Rubella Initiative, the Clean Cooking Alliance, Girl Up, Shot@Life, and the Digital Impact Alliance, among others. In March 2020, the UN Foundation was also a key founder of the COVID-19 Solidarity Response Fund on behalf of the World Health Organization (WHO), helping to raise over $200million USD within the first six weeks to support the global response to the COVID-19 pandemic.\nThe UN Foundation was founded with the intent to build support for UN causes and to advocate for the United States to honor its financial commitments to the UN. Since then, the UN Foundation and its U.S. advocacy sister organization, the Better World Campaign, have built advocacy campaigns, provided grants, connected experts, advocates, and decision-makers, and driven public awareness in order to support the UN and its priorities and programs worldwide. The UN Foundation is now supported by a variety of philanthropic, corporate, government, and individual donors, and it continues to serve as a substantial source of private funding to the United Nations. In conjunction with the UN, it established the United Nations Fund for International Partnerships to serve as the UN counterpart to the Foundation.\nThe UN Foundation has made a cumulative disbursement of more than $1.5billion in grants to the UN system. The UN Foundation also works with UN partners in order to provide policy, advocacy, event, and communications recommendations and support. The UN Foundation's budgetary breakdown in 2019 was $95.8million to program services, $5.7million to fundraising, and $8.9million going to management and overhead.\nHistory.\nIn 1998, Ted Turner, an American media proprietor, producer, and philanthropist, announced his decision to make a $1billion contribution to the United Nations at the annual United Nations Association of the USA gala dinner, with then UN Secretary-General Kofi Annan present. This $1billion donation was used to establish the United Nations Foundation.\nBeyond his philanthropy work, Turner is best known for founding cable television network CNN, as well as the Turner Broadcasting System (TBS) and Turner Entertainment Company. Turner chose to donate to the UN and create the UN Foundation because he was a prior donor to similar causes and felt strongly that the UN was critical to solve the world\u2019s most pressing challenges. Turner\u2019s gift was also inspired by the fact that the UN was underfunded at the time because the U.S. government had not met its allocated financial contribution to the UN due to a lack of sufficient appropriations in the federal budget. Prior to his $1billion donation to the UN, Turner was already an active philanthropist and had been a proponent for the protection of the environment, especially in combating global warming. Turner believed that his $100million per year donation over the course of 10 years would make a difference to the United Nations, and that he could use this donation to encourage other wealthy members of society to make financial contributions to the work of the UN. In 1996, Turner was worth $3.2billion due to his shares in Time Warner (which had bought TBS that year). By giving away nearly a third of his wealth while still living, Ted Turner became a noted participant in the Giving Pledge movement.\nLeadership.\nThe UN Foundation is led by President and Chief Executive Officer Elizabeth Cousens, who stepped into the role in January 2020 after serving for four years as Deputy CEO. Prior to her time at the UN Foundation, Cousens served as U.S. Ambassador to the UN Economic and Social Council, and Alternate Representative to the UN General Assembly where she led U.S. negotiations on the SDGs. Kathy Calvin, the former President of AOL Time Warner Foundation, served as the UN Foundation\u2019s prior President and CEO from 2013 through 2019. Timothy E. Wirth, a former United States Congressman, U.S. Senator, and the first Undersecretary of State for Global Affairs in U.S. President Bill Clinton's administration, served as the UN Foundation's first President from 1998 to 2013. Ted Turner serves as the chairman of the board. Other current board members include Queen Rania Al-Abdullah of Jordan, former Deputy Secretary-General of the UN Mark Malloch-Brown, Founder and Chairman Emeritus of Infosys N. R. Narayana Murthy, Master of University College Oxford Valerie Amos, CEO of Verizon Communications Hans Vestberg, Former Prime Minister of Norway Gro Harlem Brundtland, Nobel Peace Prize winner Muhammad Yunus, President of the University of Miami Julio Frenk, Chairman of Endeavor Brazil F\u00e1bio Colletti Barbosa, Senior Partner of the Southbridge Group Dr. Frannie L\u00e9autier, Chair of the Captain Planet Foundation Laura Turner Seydel, and former UN Foundation Presidents Timothy E. Wirth and Kathy Calvin.\nBackground of Foundation's involvement with the UN.\nWhen the UN Foundation was founded, it was created to assist the UN with a variety of key issues, and bring attention to particular global problems. A top priority was to build upon previously-successful UN programs, including children's health, population and family planning issues, global environmental agreements, and the safe removal of land-mines. Additionally, it aimed to work with the private sector to raise funding for UN causes, and to raise awareness of the UN and its programs amongst the American population.\nThe UN Foundation has had a close relationship with the UN and its leadership from the beginning in order to set goals and provide funding for particular programs. Over time, the UN Foundation evolved from being a simple funding conduit to creating and fostering its own initiatives and communities, which enabled it to support UN priorities more deeply and diversely. At present, it continues to build and expand its role as a strategic partner to the UN across multiple sectors. The UN Foundation\u2019s current main issue areas include the UN\u2019s Sustainable Development Goals (SDGs), gender equality, climate and environment, global health, and enhancing global cooperation. The Foundation has also continued working in areas of US-UN engagement and advancing data and technology for the SDGs.\nIssue areas.\nSustainable Development Goals.\nOne of the UN Foundation\u2019s overarching goals is to share awareness, advance progress, understand critical gaps, and activate communities in support of the Sustainable Development Goals. The SDGs, a set of 17 interlinked goals designed to be a \"blueprint to achieve a better and more sustainable future for all,\" were adopted in 2015 by the United Nations General Assembly as a successor to the Millennium Development Goals. While all of the UN Foundation\u2019s programs connect to at least one SDG (and often multiple), the Foundation also works to foster awareness and support of the SDGs among civil society, the private sector, academia, and engaged communities. The Foundation is also a key partner in , which runs parallel to the UN General Assembly week and engages diverse partnerships for the SDGs. The Foundation also houses the Business Council for the United Nations, which creates connections between the private sector and the UN in support of the UN\u2019s priorities and the SDGs.\nGlobal Health.\nGlobal health, often with a focus on women and children, has been one of the UN Foundation\u2019s key global issue areas. Over the Foundation\u2019s first 20 years, 72% of grants fell under the issue area of global health. The Foundation works closely with private sector partners and UN agencies in order to address a variety of health issues including universal health coverage, antimicrobial resistance, and the response and recovery from COVID-19. In addition, the UN Foundation has built partnerships and campaigns to address issues such as measles and rubella, childhood vaccination, and malaria.\nThe Measles &amp; Rubella Initiative, launched in 2001, is a partnership between the UN Foundation, the American Red Cross, UNICEF, the U.S. Centers for Disease Control and Prevention (CDC), and the World Health Organization (WHO) in order to provide measles vaccinations to children across the African continent. This campaign not only focuses on vaccinating children, but also putting into place health infrastructure, and promoting better access to health-care across the continent. In ten years, the initiative has protected more than 5.5billion children from measles.\nThe UN Foundation also runs the Nothing But Nets Campaign, which is targeted at reducing malaria across the African continent. This campaign originally started when \"Sports Illustrated\" writer Rick Reilley published an article asking his readers to donate money to a campaign to buy mosquito nets for those in Africa suffering from malaria. With support from the UN Foundation, Reilley's project got off the ground, and has to-date provided over 13million nets across Africa. Today, the campaign also encourages Americans to learn about, advocate for, and donate to malaria eradication efforts.\nThe UN Foundation's Shot@Life campaign educates, connects and empowers Americans to champion vaccines as one of the most cost-effective ways to save the lives of children in developing countries. The campaign encourages Americans to learn about, advocate for, and donate vaccines to decrease vaccine-preventable childhood deaths. As of 2019, Shot@Life had protected over $4.1billion in U.S. funding for global childhood immunization programs and helped provide more than 82million vaccines through direct grant support to UN partners.\nThe UN Foundation is a communications and advocacy partner for the Global Polio Eradication Initiative, a partnership that includes Rotary International, the Gates Foundation, UNICEF, CDC, and WHO. The initiative is dedicated to globally eradicating polio through vaccinations and has protected 2billion children from polio.\nEvery Woman Every Child was launched by UN Secretary-General Ban Ki-moon during the United Nations Millennium Development Goals Summit in 2010 and aims to save and improve the lives of millions of women, children and adolescents around the world by 2030. It is a global effort to mobilize international and national action by governments, multilaterals, the private sector and civil society to address health challenges facing women and children around the world.\nIn March 2020, at the onset of the COVID-19 pandemic, the UN Foundation launched the COVID-19 Solidarity Response Fund in partnership with the Swiss Philanthropy Foundation to raise funds for WHO\u2019s COVID-19 response. The fund raised over $200million within six weeks, which went toward WHO\u2019s efforts to track and understand the spread of the virus, to mobilize protective equipment to frontline health workers, and to develop vaccines, tests, and treatments. Beneficiaries of the fund were later expanded to include UNICEF, the Coalition for Epidemic Preparedness Innovations (CEPI), the World Food Programme (WFP), the UN Refugee Agency (UNHCR), and the United Nations Relief and Works Agency for Palestine Refugees in the Near East (UNRWA). The foundation also helped raise funds for the Access to COVID-19 Tools Accelerator campaign by accepting donations through the ACT Together Fund.\nIn September 2020, the UN Foundation launched its Unite for Health campaign, focused on the need for global collaboration amidst the disruption of health systems and services due to COVID-19, and the rollback of global health progress it brought.\nGirls and Women.\nGender equality is another one of the Foundation\u2019s key issue areas. The Foundation has several campaigns and initiatives that address gender issues; their work includes girls\u2019 leadership and empowerment, family planning and contraceptive access, maternal health, and gender data. All its work on gender equality is represented by the Equal Everywhere brand and campaign that calls for all levels of society to make equality the reality of every girl and woman.\nThe UN Foundation launched the Girl Up campaign in September 2010. This \"for girls, by girls\" campaign offers leadership development and education opportunities for adolescent girls. Through Girl Up's support, girls create middle school, high school, or campus clubs, which then plan events to raise money and awareness for the importance of women's issues. Money raised by the clubs is often used to support girls in developing countries so that they have the opportunity to become educated, healthy, safe, counted, and positioned to be the next generation of leaders.\nThe Universal Access Project works to achieve universal access to reproductive health care by convening donors and advocates to protect and strengthen global sexual and reproductive health and rights. Its goal is to increase and maintain the U.S. involvement and funding for global family planning by protecting key investments.\nFamily Planning 2020, an outcome of the 2012 London Summit on Family Planning, addresses the policy, financing, delivery and socio-cultural barriers to women accessing contraceptive information, services and supplies. Led by an 23-member Reference Group, operated daily by a Secretariat, and hosted by the United Nations Foundation, FP2020 is based on the principle that all women, no matter where they live should have access to lifesaving contraceptives.\nData2X, a collaborative technical and advocacy platform, was formed after former U.S. Secretary of State Hillary Rodham Clinton called for its creation in a policy speech in July 2012 and cited the lack of reliable and regular data on the lives of women and girls. Through research, advocacy and communications, Data2X works to improve the availability, quality, and use of gender data to make a practical difference in the lives of women and girls worldwide.\nClimate and Environment.\nThe UN Foundation\u2019s priorities include tackling global climate change, advancing climate diplomacy and negotiations, building cross-issue intersections, and communicating climate science to the public. The Foundation\u2019s climate team works with partners in the NGO sector, the UN, governments, and private corporations to come up with solutions and provide funding to programs related to this issue. The UN Foundation also advocates for the Paris Agreement on climate change, particularly in the United States where it supports the U.S. Climate Alliance, a bipartisan coalition of states and unincorporated self-governing territories that are committed to upholding the objectives of the 2015 Paris Agreement.\nThe UN Foundation also fosters the Clean Cooking Alliance, an initiative supporting large-scale adoption of clean and safe household cooking solutions as a way to save lives, improve livelihoods, empower women, and reduce climate change emissions. The alliance works with public, private, and non-profit partners to overcome market barriers that hamper the production, deployment, and use of clean cookstoves and fuels in the developing world. It works to develop standards for cleaner stoves and to increase public and policymaker awareness of the health and environmental benefits of improved stoves.\nGlobal Leadership Awards.\nThe We the Peoples Global Leadership Awards are awarded to seven people in various categories in New York City each year.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "37114", "revid": "22831189", "url": "https://en.wikipedia.org/wiki?curid=37114", "title": "Sufi", "text": ""}
{"id": "37116", "revid": "48442598", "url": "https://en.wikipedia.org/wiki?curid=37116", "title": "UNFCCC", "text": ""}
{"id": "37118", "revid": "50889400", "url": "https://en.wikipedia.org/wiki?curid=37118", "title": "Arabic calligraphy", "text": "Calligraphy using the Arabic script\nArabic calligraphy is the artistic practice of handwriting and calligraphy based on the Arabic alphabet. It is known in Arabic as \"khatt\" (), derived from the words 'line', 'design', or 'construction'. Kufic is the oldest form of the Arabic script.\nFrom an artistic point of view, Arabic calligraphy has been known and appreciated for its diversity and great potential for development. In fact, it has been linked in Arabic culture to various fields such as religion, art, architecture, education and craftsmanship, which in turn have played an important role in its advancement.\nAlthough most Islamic calligraphy is in Arabic and most Arabic calligraphy is Islamic, the two are not identical. Coptic or other Christian manuscripts in Arabic, for example, have made use of calligraphy. Likewise, there is Islamic calligraphy in Persian and Ottoman Turkish.\nArabic alphabet.\nThe Arabic alphabet is one of the most widely used scripts in the world. Many scholars believe that the alphabet was created around the 4th century CE. The alphabet consists of 28 letters written from right to left. Each letter can be written in four ways, depending on where the letter is placed in a word. These four locations are also known as initial, medial, final and isolated. All letters can connect from the right side (i.e. to the preceding letter), but some do not connect from the left side (i.e. to the subsequent letter).\nThree letters can also represent long vowels in certain contexts, namely \u0101lif (\u0627), w\u0101w (\u0648), and y\u0101 (\u064a).\nImplements.\nThe pens used for Arabic calligraphy vary from Latin calligraphy. The tools used for calligraphy are different assortments of pens and calligraphy ink. The most common calligraphy pen used is Qalam.\nKhamish pen.\nThe Khamish pen also known as a reed pen is used by Arab, Turkish, and Iranian calligraphers. The reed of the pen is grown along rivers. Although this pen has been used for over 500 years, preparing the pen is a lengthy process.\nJava pen.\nThe Java pen is known for the tool's hardness and ability to create sharp edges. The pen is good to use for small scripts.\nHandam pen.\nThe Handam pen consists of the same strength as the Java pen. The pen is good to use for all kinds of scripts.\nCeli pen.\nThe Celi pen is used for large writing in Arabic calligraphy. These pens are made from hardwood and cut and drilled.\nEvolution.\nArabic calligraphy evolved from a tool for communication and documentation to an artistic form in the span of 13 centuries, it was also implemented in various other fields such as mathematics and astronomy. It is a central form of decoration in Islamic art, such as decorative design and architecture. Historians consider to be the earliest Arab calligrapher. The evolution of Arabic calligraphy led to the appearance of various scripts, including cursive styles such as Nastaliq and Ruq'ah, and more square, angular styles such as Kufic. The linguistic features of Arabic scripts are shared between all scripts despite differences in styles.\nScripts.\nKufic.\nOriginally used for inscription on stone and metal, the Kufic style of Arabic calligraphy received its name due to its birth in the city of Kufa, Iraq. This script is one of the oldest scripts used in Arabic and Islamic calligraphy; due to this, the style has undergone many evolutions and changes in its life course, as many attempts were made to perfect it. However, this also led to the development of many different variations of this script, such as the floriated Kufic, square Kufic, knotted Kufic, and many others. This also means there are a few distinguishing features of the Kufic script.\nThe Kufic style has been used almost exclusively for Arabic, as opposed to other languages, such as Persian and Urdu, that are written in systems derived from Arabic; a single exception to this is a series of Persian rhymes found on a building in Ghazni from the 11th century.\nNaskh.\nKnown as the Naskh or Naskh\u012b script, this script is said to have originated from Mecca and Medina. The script is used as a cursive script, for example on papyrus and paper. The origins of the style are debated by scholars, but some believe it initially stemmed from the Thuluth script.But recent discoveries in Jabal Sala in Medina have proven that the Naskh script precedes the Thuluth script and that it existed before Ibn Muqla al-Shirazi. One of the main usages for this script was for writing the Quran but it was also used for inscription on metal antiquities, woods and other objects of decorative purpose. The main evolutionary periods for this script were the 3rd and 4th centuries AH, coinciding with the evolution of other similar popular styles such as the Rayhani, Thulth, and Muhaqqaq.\nOther scripts.\nThe Thuluth, Nasta'liq and Diwani scripts are other scripts used for Arabic scripting.\nThe Thuluth script, used during the medieval times, is known as one of the oldest scripts to exist. The script was used in mosques and for Quranic text due to the appearance of the text.\nThe Nasta'liq script is used more for Persian than Arabic scripting. Because of the downward slant to the left, the script is seen as different from the other scripts.\nThe Diwani script was created during the Ottoman era. The lining and lettering of this script creates a sense of closeness when writing. Due to this reason, it is difficult to read since the letters intertwine.\nA few other examples:\nLegacy.\nType design and type setting.\nArabic calligraphy serves as a major source of inspiration for Arabic type design. For example, the Amiri typeface is inspired by the Naskh script used at the Amiri Press in Cairo.\nThe shift from Arabic calligraphy to Arabic typefaces presents technical challenges.\nIslamic world and civilization.\nCredited to be the one that catalyzed the growth of Arabic calligraphy; with the earliest works of Arabic calligraphy being featured in copies of the Quran dating back to the first century of Islam's revelation such as Birmingham Quran Manuscript, Codex Parisino-Petropolitanus and several others.\nArabic calligraphy can be on occasion be found in Mosques with engravings of Quranic verses / Ayah present on parts of the architecture itself.\nThe most widely recognized example of Arabic Calligraphy on a place of Islamic worship is the Kaaba present in Mecca, Saudi Arabia.\nArabic calligraphy specializes into the term \"Islamic calligraphy\" when it is associated with the Islamic world.\nArt.\nEL Seed, a French-Tunisian graffiti artist, makes use of Arabic calligraphy in his various art projects, in a style called \"calligraffiti\".\nThe \"Hurufiyya\" ( \"letters\") movement, since its beginnings in the early 20th century, uses the artistic manipulation of Arabic calligraphy and typography in abstraction.\n\"Taking Shape: Abstraction From the Arab World, 1950s-1980s\", a 2020 installation at New York University's Grey Art Gallery, explored how Arabic calligraphy, with its ancient presence in visual art, influenced abstract art in the Arab world. For Madiha Omar, the Arabic alphabet was a means of expressing a secular identity and appropriating Western painting, while Omar El-Nagdi explored the inherent divinity of Arabic calligraphy.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "37119", "revid": "29615425", "url": "https://en.wikipedia.org/wiki?curid=37119", "title": "Arabic Calligraphy", "text": ""}
{"id": "37120", "revid": "49683800", "url": "https://en.wikipedia.org/wiki?curid=37120", "title": "Nigger", "text": "Racial slur against black people\nNigger is a racial slur directed at black people. References to \"nigger\" have been increasingly replaced by the euphemistic \"the N-word\", notably in cases where \"nigger\" is mentioned but not directly used. In an instance of linguistic reappropriation, the term \"nigger\" is also used casually and fraternally among African Americans, most commonly in the form of \"nigga\", whose spelling reflects the phonology of African-American English.\nThe origin of the word lies with the Latin adjective \"niger\" ([\u02c8n\u026a\u0261\u025br]), meaning \"black\". It was initially seen as a relatively neutral term, essentially synonymous with the English word \"negro\". Early attested uses during the Atlantic slave trade (16th\u201319th century) often conveyed a merely patronizing attitude. The word took on a derogatory connotation from the mid-18th century onward, and \"degenerated into an overt slur\" by the middle of the 19th century. Some authors still used the term in a neutral sense up until the later part of the 20th century, at which point the use of \"nigger\" became increasingly controversial regardless of its context or intent.\nBecause the word \"nigger\" has historically \"wreaked symbolic violence, often accompanied by physical violence\", it began to disappear from general popular culture from the second half of the 20th century onward, with the exception of cases derived from intra-group usage such as hip-hop culture. The \"Merriam-Webster Online Dictionary\" describes the term as \"perhaps the most offensive and inflammatory racial slur in English\". The \"Oxford English Dictionary\" writes that \"this word is one of the most controversial in English, and is liable to be considered offensive or taboo in almost all contexts (even when used as a self-description)\". At the trial of O. J. Simpson, prosecutor Christopher Darden referred to it as \"the filthiest, dirtiest, nastiest word in the English language\". Intra-group usage has been criticized by some contemporary Black American authors, a group of them (the \"eradicationists\") calling for the total abandonment of its usage (even under the variant \"nigga\"), which they see as contributing to the \"construction of an identity founded on self-hate\". In wider society, the inclusion of the word \"nigger\" in classic works of literature (as in Mark Twain's 1884 book \"The Adventures of Huckleberry Finn\") and in more recent cultural productions (such as Quentin Tarantino's 1994 film \"Pulp Fiction\" and 2012 film \"Django Unchained\") has sparked controversy and ongoing debate.\nThe word \"nigger\" has also been historically used to designate \"any person considered to be of low social status\" (as in the expression \"white nigger\") or \"any person whose behavior is regarded as reprehensible\". In some cases, with awareness of the word's offensive connotation, but without intention to cause offense, it can refer to a \"victim of prejudice likened to that endured by African Americans\" (as in John Lennon's 1972 song \"Woman Is the Nigger of the World\").\nEtymology and history.\nEarly use.\nThe word \"nigger\", then spelled in English \"neger\" or \"niger\", appeared in the 16th century as an adaptation of French \"n\u00e8gre\", itself from Spanish \"negro\". They go back to the Latin adjective \"niger\" ([\u02c8n\u026a\u0261\u025br]), meaning \"black\".\nIn its original English-language usage, \"nigger\" (also spelled \"niger\") was a word for a dark-skinned individual. The earliest known published use of the term dates from 1574, in a work alluding to \"the Nigers of Aethiop, bearing witnes\". According to the Oxford English Dictionary, the first derogatory usage of the term \"nigger\" was recorded two centuries later, in 1775.\nIn the colonial America of 1619, John Rolfe used \"negars\" in describing the African slaves shipped to the Virginia colony. Later American English spellings, \"neger\" and \"neggar\", prevailed in New York under the Dutch and in metropolitan Philadelphia's Moravian and Pennsylvania Dutch communities; the African Burial Ground in New York City originally was known by the Dutch name (Cemetery of the Negro). An early occurrence of \"neger\" in American English dates from 1625 in Rhode Island. Lexicographer Noah Webster suggested the \"neger\" spelling in place of \"negro\" in his 1806 dictionary.\n18th- and 19th-century United States.\nDuring the late 18th and early 19th centuries, the word \"nigger\" also described an actual labor category, which African American laborers adopted for themselves as a social identity, and thus white people used the descriptor word as a distancing or derogatory epithet, as if \"quoting black people\" and their non-standard language. During the early 1800s to the late 1840s fur trade in the Western United States, the word was spelled \"niggur\", and is often recorded in the literature of the time. George Ruxton used it in his \"mountain man\" lexicon, without pejorative connotation. \"Niggur\" was evidently similar to the modern use of \"dude\" or \"guy\". This passage from Ruxton's \"Life in the Far West\" illustrates the word in spoken form\u2014the speaker here referring to himself: \"Travler, marm, this niggur's no travler; I ar' a trapper, marm, a mountain-man, wagh!\" It was not used as a term exclusively for blacks among mountain men during this period, as Indians, Mexicans, and Frenchmen and Anglos alike could be a \"niggur\". \"The noun slipped back and forth from derogatory to endearing.\"\nBy 1859, the term was clearly used to offend, in an attack on abolitionist John Brown.\nThe term \"colored\" or \"negro\" became a respectful alternative. In 1851, the Boston Vigilance Committee, an abolitionist organization, posted warnings to the \"Colored People of Boston and vicinity\". Writing in 1904, journalist Clifton Johnson documented the \"opprobrious\" character of the word \"nigger\", emphasizing that it was chosen in the South precisely because it was more offensive than \"colored\" or \"negro\". By the turn of the century, \"colored\" had become sufficiently mainstream that it was chosen as the racial self-identifier for the National Association for the Advancement of Colored People (NAACP). In 2008 Carla Sims, its communications director, said \"the term 'colored' is not derogatory, [the NAACP] chose the word 'colored' because it was the most positive description commonly used [in 1909, when the association was founded]. It's outdated and antiquated but not offensive.\"\nMark Twain, in the autobiographic book \"Life on the Mississippi\" (1883), used the term within quotes, indicating reported speech, but used the term \"negro\" when writing in his own narrative persona. Joseph Conrad published a novella in Britain with the title \"The Nigger of the \"Narcissus\"\" (1897); in the United States, it was released as \"The Children of the Sea: A Tale of the Forecastle\"; the original had been called \"the ugliest conceivable title\" in a British review and American reviewers understood the change as reflecting American \"refinement\" and \"prudery.\"\n20th-century United States.\nA style guide to British English usage, H.W. Fowler's \"A Dictionary of Modern English Usage\", states in the first edition (1926) that applying the word \"nigger\" to \"others than full or partial negroes\" is \"felt as an insult by the person described, &amp; betrays in the speaker, if not deliberate insolence, at least a very arrogant inhumanity\"; but the second edition (1965) states \"N. has been described as 'the term that carries with it all the obloquy and contempt and rejection which whites have inflicted on blacks'\". The quoted formula goes back to the writings of the American journalist Harold R. Isaacs, who used it in several writings between 1963 and 1975. Black characters in Nella Larsen's 1929 novel \"Passing\" view its use as offensive; one says \"I'm really not such an idiot that I don't realize that if a man calls me a nigger, it's his fault the first time, but mine if he has the opportunity to do it again.\"\nBy the late 1960s, the social change brought about by the civil rights movement had legitimized the racial identity word \"black\" as mainstream American English usage to denote black-skinned Americans of African ancestry. President Thomas Jefferson had used this word of his slaves in his \"Notes on the State of Virginia\" (1785), but \"black\" had not been widely used until the later 20th century. (See black pride, and, in the context of worldwide anti-colonialism initiatives, \"N\u00e9gritude\".)\nIn the 1980s, the term \"African American\" was advanced analogously to such terms as \"German American\" and \"Irish American\", and was adopted by major media outlets. Moreover, as a compound word, \"African American\" resembles the vogue word \"Afro-American\", an early-1970s popular usage. Some Black Americans continue to use the word \"nigger\", often spelled as \"nigga\" and \"niggah\", without irony, either to neutralize the word's impact or as a sign of solidarity.\nUsage.\nSurveys from 2006 showed that the American public widely perceived usage of the term to be wrong or unacceptable, but that nearly half of whites and two-thirds of blacks knew someone personally who referred to blacks by the term. Nearly one-third of whites and two-thirds of blacks said they had personally used the term within the last five years.\nPolitical use.\n\"Niggers in the White House\" was written in reaction to an October 1901 White House dinner hosted by Republican President Theodore Roosevelt, who had invited Booker T. Washington\u2014an African-American presidential advisor\u2014as a guest. The poem reappeared in 1929 after First Lady Lou Hoover, wife of President Herbert Hoover, invited Jessie De Priest, the wife of African-American congressman Oscar De Priest, to a tea for congressmen's wives at the White House. The identity of the author\u2014who used the byline \"unchained poet\"\u2014remains unknown.\nIn explaining his refusal to be conscripted to fight the Vietnam War (1955\u20131975), professional boxer Muhammad Ali said, \"No Vietcong ever called me nigger.\" Later, his modified answer was the title of a documentary, \"No Vietnamese Ever Called Me Nigger\" (1968), about the front-line lot of the U.S. Army black soldier in combat in Vietnam. An Ali biographer reports that, when interviewed by Robert Lipsyte in 1966, the boxer actually said, \"I ain't got no quarrel with them Viet Cong.\"\nOn February 28, 2007, the New York City Council symbolically banned the use of the word \"nigger\"; however, there is no penalty for using it. This formal resolution also requests excluding from Grammy Award consideration every song whose lyrics contain the word; however, Ron Roecker, vice president of communication for the Recording Academy, doubted it will have any effect on actual nominations.\nThe word can be invoked politically for effect. When Detroit mayor Kwame Kilpatrick came under intense scrutiny for his conduct in 2008, he deviated from an address to the city council, saying, \"In the past 30 days, I've been called a nigger more than any time in my entire life.\" Opponents accused him of \"playing the race card\" to save his political life.\nCultural use.\nThe implicit racism of the word \"nigger\" has generally rendered its use taboo. Magazines and newspapers typically do not use this word but instead print censored versions such as \"n*gg*r\", \"n**ger\", \"n\u2014\u2014\" or \"the N-word\"; see below.\nThe use of \"nigger\" in older literature has become controversial because of the word's modern meaning as a racist insult. One of the most enduring controversies has been the word's use in Mark Twain's novel \"Adventures of Huckleberry Finn\" (1885). \"Huckleberry Finn\" was the fifth most challenged book during the 1990s, according to the American Library Association. The novel is written from the point of view, and largely in the language, of an uneducated white boy, who is drifting down the Mississippi River on a raft with an adult escaped slave, Jim. The word \"nigger\" is used (mostly about Jim) over 200 times. Twain's advocates note that the novel is composed in then-contemporary vernacular usage, not racist stereotype, because Jim, the black man, is a sympathetic character.\nIn 2011, a new edition published by NewSouth Books replaced the word \"nigger\" with \"slave\" and also removed the word \"injun\". The change was spearheaded by Twain scholar Alan Gribben in the hope of \"countering the 'pre-emptive censorship'\" that results from the book's being removed from school curricula over language concerns. The changes sparked outrage from critics Elon James, Alexandra Petri and Chris Meadows.\nIn his 1999 memoir \"All Souls\", Irish-American Michael Patrick MacDonald describes how many white residents of the Old Colony Housing Project in South Boston used this meaning to degrade the people considered to be of lower status, whether white or black.\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;Of course, no one considered himself a nigger. It was always something you called someone who could be considered anything less than you. I soon found out there were a few black families living in Old Colony. They'd lived there for years and everyone said that they were okay, that they weren't niggers but just black. It felt good to all of us to not be as bad as the hopeless people in D Street or, God forbid, the ones in Columbia Point, who were both black and niggers. But now I was jealous of the kids in Old Harbor Project down the road, which seemed like a step up from Old Colony...\nIn an academic setting.\nThe word's usage in literature has led to it being a point of discussion in university lectures as well. In 2008, Arizona State University English professor Neal A. Lester created what has been called \"the first ever college-level class designed to explore the word 'nigger'\". Starting in the following decade, colleges struggled with attempts to teach material about the slur in a sensitive manner. In 2012, a sixth grade Chicago teacher Lincoln Brown was suspended after repeating the contents of a racially charged note being passed around in class. Brown later filed a federal civil rights lawsuit against the headmaster and the Chicago public schools. A New Orleans high school also experienced controversy in 2017. Such increased attention prompted Elizabeth Stordeur Pryor, the daughter of Richard Pryor and a professor at Smith College, to give a talk opining that the word was leading to a \"social crisis\" in higher education.\nIn addition to Smith College, Emory University, Augsburg University, Southern Connecticut State University, and Simpson College all suspended professors in 2019 over referring to the word \"nigger\" by name in classroom settings. In two other cases, a professor at Princeton decided to stop teaching a course on hate speech after students protested his utterance of \"nigger\" and a professor at DePaul had his law course cancelled after 80% of the enrolled students transferred out. Instead of pursuing disciplinary action, a student at the College of the Desert challenged his professor in a viral class presentation which argued that her use of the word in a lecture was not justified.\nIn the workplace.\nIn 2018, the head of the media company Netflix, Reed Hastings, fired his chief communications officer, Jonathan Friedland, for using the word twice during internal discussions about sensitive words. In explaining why, Hastings wrote:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;[The word's use] in popular media like music and film have created some confusion as to whether or not there is ever a time when the use of the N-word is acceptable. For non-Black people, the word should not be spoken as there is almost no context in which it is appropriate or constructive (even when singing a song or reading a script). There is not a way to neutralize the emotion and history behind the word in any context. The use of the phrase 'N-word' was created as a euphemism, and the norm, with the intention of providing an acceptable replacement and moving people away from using the specific word. When a person violates this norm, it creates resentment, intense frustration, and great offense for many.\nThe following year, screenwriter Walter Mosley turned down a job after his human resources department took issue with him using the word to describe racism that he experienced as a black man.\nWhile defending Laurie Sheck, a professor who was cleared of ethical violations for quoting \"I Am Not Your Negro\" by James Baldwin, John McWhorter wrote that efforts to condemn racist language by white Americans had undergone mission creep. Similar controversies outside the United States have occurred at the University of Western Ontario in Canada and the Madrid campus of Syracuse University. In June 2020, Canadian news host Wendy Mesley was suspended and replaced with a guest host after she attended a meeting on racial justice and, in the process of quoting a journalist, used \"a word that no-one like me should ever use\". In August 2020, BBC News, with the agreement of victim and family, mentioned the slur when reporting on a physical and verbal assault on the black NHS worker and musician K-Dogg. Within the week the BBC received over 18,600 complaints, the black radio host David Whitely resigned in protest, and the BBC apologized.\nIn 2021, in Tampa, Florida, a 27-year-old black employee at a Dunkin' Donuts punched a 77-year-old white customer after the customer had repeatedly called the employee a nigger. The customer fell to the floor and hit his head. Three days later, he died, having suffered a skull fracture and brain contusions. The employee was arrested and charged with manslaughter. In a plea bargain, the employee pled guilty to felony battery, and was sentenced to two years of house arrest. In 2022, in explaining why the employee did not receive any jail time, Grayson Kamm, a spokesman for Hillsborough State Attorney Andrew Warren, said \"Two of the primary factors were the aggressive approach the victim took toward the defendant and everyone working with the defendant, and that the victim repeatedly used possibly the most aggressive and offensive term in the English language.\"\nIntra-group versus intergroup usage.\nBlack listeners often react to the term differently, depending on whether it is used by white speakers or by black speakers. In the former case, it is regularly understood as insensitive or insulting; in the latter, it may carry notes of in-group disparagement, or it may be understood as neutral or affectionate, a possible instance of reappropriation.\nIn the black community, \"nigger\" is often rendered as \"nigga\". This usage has been popularized by the rap and hip-hop music cultures and is used as part of an in-group lexicon and speech. It is not necessarily derogatory and is often used to mean \"homie\" or \"friend\".\nAcceptance of intra-group usage of the word \"nigga\" is still debated, although it has established a foothold amongst younger generations. The NAACP denounces the use of both \"nigga\" and \"nigger\". As of 2001, trends indicated that usage of the term in intragroup settings is increasing even amongst white youth, due to the popularity of rap and hip hop culture. Linguist Keith Allan rejects the view that \"nigger\" is always a slur, arguing that it is also used as a marker of camaraderie and friendship, comparable to the British and Australian term \"mate\" or the American \"buddy\".\nAccording to Arthur K. Spears in \"Diverse Issues in Higher Education, 2006\":\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;In many African-American neighborhoods, nigga is simply the most common term used to refer to any male, of any race or ethnicity. Increasingly, the term has been applied to any person, male or female. \"Where y'all niggas goin?\" is said with no self-consciousness or animosity to a group of women, for the routine purpose of obtaining information. The point: \"nigga\" is evaluatively neutral in terms of its inherent meaning; it may express positive, neutral, or negative attitudes;\nKevin Cato, meanwhile, observes:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;For instance, a show on Black Entertainment Television, a cable network aimed at a Black audience, described the word nigger as a \"term of endearment\". \"In the African American community, the word \"nigga\" (not \"nigger\") brings out feelings of pride.\" (Davis1.) Here the word evokes a sense of community and oneness among Black people. Many teens I interviewed felt the word had no power when used amongst friends, but when used among white people the word took on a completely different meaning. In fact, comedian Alex Thomas on BET stated, \"I still better not hear no white boy say that to me... I hear a white boy say that to me, it means 'White boy, you gonna get your ass beat.'\"\nAddressing the use of \"nigger\" by black people, philosopher and public intellectual Cornel West said in 2007:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;There's a certain rhythmic seduction to the word. If you speak in a sentence, and you have to say \"cat\", \"companion\", or \"friend\", as opposed to \"nigger\", then the rhythmic presentation is off. That rhythmic language is a form of historical memory for Black people... When Richard Pryor came back from Africa, and decided to stop using the word onstage, he would sometimes start to slip up, because he was so used to speaking that way. It was the right word at the moment to keep the rhythm together in his sentence making.\n2010s: Increase in use and controversy.\nIn the 2010s, \"nigger\" in its various forms saw use with increasing frequency by African Americans amongst themselves or in self-expression, the most common swear word in hip hop music lyrics. Ta-Nehisi Coates suggested that it continues to be unacceptable for non-blacks to utter while singing or rapping along to hip-hop, and that by being so restrained it gives white Americans (specifically) an impression of what it is like to not be entitled to \"do anything they please, anywhere\". A concern often raised is whether frequent exposure will inevitably lead to a dilution of the extremely negative perception of the word among the majority of non-black Americans who currently consider its use unacceptable and shocking.\nRelated words.\nDerivatives.\nIn several English-speaking countries, \"Niggerhead\" or \"nigger head\" was used as a descriptive name for many sorts of things, including commercial products, places, plants and animals. It also is or was a colloquial technical term in industry, mining, and seafaring. \"Nigger\" as \"hidden defect\" derives from \"nigger in the woodpile\", a US slave-era phrase denoting escaped slaves hiding in train-transported woodpiles. In the 1840s, the \"Morning Chronicle\" newspaper report series \"London Labour and the London Poor\", by Henry Mayhew, records the usages of both \"nigger\" and the similar-sounding word \"niggard\" denoting a false bottom for a grate.\nIn American English, \"nigger lover\" initially applied to abolitionists, then to white people sympathetic towards black Americans. The portmanteau word \"wigger\" ('White' + 'nigger') denotes a white person emulating \"street Black behavior\", hoping to gain acceptance to the hip hop, thug, and gangsta sub-cultures. Norman Mailer wrote of the antecedents of this phenomenon in 1957 in his essay \"The White Negro\".\nIn Ukraine, the word \"zigger\" (\"Ukrainian\": '\u0437\u0456\u0433\u0433\u0435\u0440') is sometimes used as a derogatory term by Ukrainians to refer to Russian soldiers and those who follow the Russian government's propaganda. The word comes from replacing the first letter of \"nigger\" with a Z, which is a reference to the \"Z\" tactical symbol used by Russian troops and Russian nationalists. It is used as a more offensive alternative to calling someone a \"vatnik.\"\n\"The N-word\" euphemism.\n&lt;templatestyles src=\"Template:Quote_box/styles.css\" /&gt;\nNotable usage\nThe prosecutor [Christopher Darden], his voice trembling, added that the \"N-word\" was so vile he would not utter it. \"It's the filthiest, dirtiest, nastiest word in the English language.\"\n\u2014\u2009Kenneth B. Noble, January 14, 1995 \"The New York Times\"\nOne of the first uses of \"the N-word\" euphemism by a major public figure came during the racially contentious O. J. Simpson murder case in 1995. Key prosecution witness Detective Mark Fuhrman, of the Los Angeles Police Department\u2014who denied using racist language on duty\u2014impeached himself with his prolific use of \"nigger\" in tape recordings about his police work. Co-prosecutor Christopher Darden refused to say the actual word, calling it \"the filthiest, dirtiest, nastiest word in the English language\". Media personnel who reported on Fuhrman's testimony substituted \"the N-word\" for \"nigger\".\nSimilar-sounding words.\n (Latin for \"black\") occurs in Latinate scientific nomenclature and is the root word for some homophones of \"nigger\"; sellers of niger seed (used as bird feed), sometimes use the spelling \"Nyjer\" seed. The classical Latin pronunciation sounds similar to the English , occurring in biologic and anatomic names, such as \"Hyoscyamus niger\" (black henbane), and even for animals that are in fact not black, such as \"Sciurus niger\" (fox squirrel).\n is the Latin feminine form of (black), used in biologic and anatomic names such as substantia nigra (black substance).\nThe word \"niggardly\" (miserly) is etymologically unrelated to \"nigger\", derived from the Old Norse word (stingy) and the Middle English word . In the US, this word has been misinterpreted as related to \"nigger\" and taken as offensive. In January 1999, David Howard, a white Washington, D.C., city employee, was compelled to resign after using \"niggardly\"\u2014in a financial context\u2014while speaking with black colleagues, who took umbrage. After reviewing the misunderstanding, Mayor Anthony A. Williams offered to reinstate Howard to his former position. Howard refused reinstatement but took a job elsewhere in the mayor's government.\n is the Spanish word for 'black', and is commonly a part of place names and proper names, particularly in the Southwest of the United States.\nDenotational extension.\nThe denotations of \"nigger\" also include non-black/non-white and other disadvantaged people. Some of these terms are self-chosen, to identify with the oppression and resistance of black Americans; others are ethnic slurs used by outsiders.\nJerry Farber's 1967 essay collection, \"The Student as Nigger\", used the word as a metaphor for what he saw as the role forced on students. Farber had been, at the time, frequently arrested as a civil rights activist while beginning his career as a literature professor.\nIn his 1968 autobiography \"White Niggers of America: The Precocious Autobiography of a Quebec \"Terrorist\"\", Pierre Valli\u00e8res, a leader, refers to the oppression of the Qu\u00e9b\u00e9cois people in North America.\nIn 1969, in the course of being interviewed by the British magazine \"Nova\", artist Yoko Ono said \"woman is the nigger of the world\"; three years later, her husband, John Lennon, published the song of the same name\u2014about the worldwide phenomenon of discrimination against women\u2014which was socially and politically controversial to US sensibilities.\n\"Sand nigger\", an ethnic slur against Arabs, and \"timber nigger\" and \"prairie nigger\", ethnic slurs against Native Americans, are examples of the racist extension of \"nigger\" upon other non-white peoples.\nIn 1978, singer Patti Smith used the word in \"Rock N Roll Nigger\". One year later in 1979, English singer Elvis Costello used the phrase \"white nigger\" in his song \"Oliver's Army\". The slur usually remains uncensored on radio stations, but Costello's usage of the word came under scrutiny, particularly after he used racial slurs during a drunken argument with Stephen Stills and Bonnie Bramlett in 1979. In the same year, Costello's father published a letter in \"Rolling Stone\" defending his son against accusations of racism, stating \"Nothing could be further from the truth. My own background has meant that I am passionately opposed to any form of prejudice based on religion or race\u00a0... His mother comes from the tough multiracial area of Liverpool, and I think she would still beat the tar out of him if his orthodoxy were in doubt\". \nHistorian Eugene Genovese, noted for bringing a Marxist perspective to the study of power, class and relations between planters and slaves in the South, uses the word pointedly in \"The World the Slaveholders Made\" (1988).\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;For reasons common to the slave condition all slave classes displayed a lack of industrial initiative and produced the famous Lazy Nigger, who under Russian serfdom and elsewhere was white. Just as not all Blacks, even under the most degrading forms of slavery, consented to become niggers, so by no means all or even most of the niggers in history have been Black.\nThe editor of \"Green Egg\", a magazine described in \"The Encyclopedia of American Religions\" as a significant periodical, published an essay entitled \"Niggers of the New Age\". This argued that Neo-Pagans were treated badly by other parts of the New Age movement.\nOther languages.\nOther languages, particularly Romance languages, have words that sound similar to or share etymological roots with \"nigger\" but do not necessarily mean the same. In some of these languages, the words refer to the color black in general and are not specifically used to refer to black people. When used to refer to black people, these words have acquired varying degrees of offensiveness, ranging from completely neutral (as in Spanish ) to highly racist (as in Finnish ). Examples of related words in other languages include:\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "37123", "revid": "28481209", "url": "https://en.wikipedia.org/wiki?curid=37123", "title": "Theories of political behavior", "text": "Aspect of political science\nTheories of political behavior, as an aspect of political science, attempt to quantify and explain the influences that define a person's political views, ideology, and levels of political participation, especially in relation to the role of politicians and their impact on public opinion . Political behavior is the subset of human behavior that involves politics and power. Theorists who have had an influence on this field include Karl Deutsch and Theodor Adorno.\nLong-term influences on political orientation.\nInteraction with the political views of parental figures is often thought of as the primary long-term influence on political orientation and willingness to take part in the political system.\nTeachers and other educational authority figures are also often thought to have a significant impact on political orientation. During the 2003\u20132004 school year, In the United States, students spent an average of 180.4 days in primary and secondary education each year, with a school day being defined as approximately 6.7 class hours. This means that on average a student will spend around 1,208.68 hours in class each year. Post-secondary education appears to have an impact on both voting rates and political identification; as a study of 9,784,931 college students found that they voted at a rate of 68.5% in the 2016 Presidential Election compared to the average of 46.1% for citizens aged 18\u201329 who voted.\nPeers also affect political orientation. Friends often, but not necessarily, have the advantage of being part of the same generation, which collectively develops a unique set of societal issues; Eric L. Dey has argued that \"socialisation is the process through which individuals acquire knowledge, habits, and value orientations that will be useful in the future.\" The ability to relate on this common level is what fuels and enables future ideological growth.\nSociologists and political scientists debate the relationship between age and the formation of political attitudes. The impressionable years hypothesis postulates that political orientation is solidified during early adulthood. By contrast, the \"increasing persistence hypothesis\" posits that attitudes become less likely to change as individuals become older, while the \"life-long openness hypothesis\" proposes that the attitudes of individuals remain flexible regardless of age.\nShort-term influences on political orientation.\nShort-term factors also affect voting behavior; the media and the impact of individual election issues are among these factors. These factors differ from the long-term factors as they are often short-lived. However, they can be just as crucial in modifying political orientation. The ways in which these two sources are interpreted often relies on the individuals specific political ideology formed by the long-term factors.\nMost political scientists agree that the mass media have a profound impact on voting behavior. One author asserts that \"few would argue with the notion that the institutions of the mass media are important to contemporary politics ... in the transition to liberal democratic politics in the Soviet Union and Eastern Europe the media was a key battleground.\"\nSecond, there are election issues. These include campaign issues, debates and commercials. Election years and political campaigns can shift certain political behaviors based on the candidates involved, which have different degrees of effectiveness in influencing voters.\nThe influence of social groups on political outcomes.\nRecently, some political scientists have been interested in many studies which aimed to analyze the relation between the behavior of social groups and the political outcomes. Some of the social groups included in their studies have been age demographics, gender, and ethnic groups. This can be understood through the lenses of pluralism or social identity theory.\nFor example, in U.S. politics, the effect of ethnic groups and gender has a great influence on the political outcomes. Hispanic Americans have a profound social impact on the political outcome of their vote and are emerging as a strong up-and-coming political force. The most noticeable increase in Hispanic American voting was in the 2000 presidential election, although the votes did not share a socially common political view at that time. In the 2006 election, the Hispanic American vote aided tremendously in the election of Florida Senator Mel Martinez, although in the 2004 presidential election, about 44% of Latin Americans voted for Republican President George W. Bush. However, Hispanic Americans have the lowest voting rate in the United States, with only 47.6% voting in the 2016 Presidential Election in the United States. Currently illegal immigration has been claiming the most attention and Hispanic Americans, although not unanimous, are concerned with the education, employment and deportation of illegal immigrants in the United States. Although the majority of Hispanic Americans vote for Democratic candidates, Cuban Americans are likely the most conservative of Latinos, with 54% of Cuban American voters casting ballots for Donald Trump in the 2016 Presidential Election, compared to an average of 35% of all Latinos who voted. Although this was represents a net decrease in support for the Republican Party among Cuban Americans, it continues a trend created by the exile of many Cubans after the Cuban Revolution.\nAfrican Americans have the second highest voting rates in the United States and even surpassed white voters in the 2008 Presidential Election, although this has declined in the 2016 Presidential Election. In the 2008 Presidential Election and 2012 Presidential election, African Americans voted overwhelmingly for Democratic candidate, Barack Obama. This trend of African Americans voting for candidates of the Democratic Party continued into the 2016 Presidential Election.\nWomen in the United States have, in the past 30 years, surpassed male voting rates, with the 2016 Presidential Election having a ratio between females and males of 52 to 48. This trend is often referred to as the Gender Gap and when combined with the tendency of women to vote for Democratic candidates, their effect on political outcomes is extremely important.\nBiology and political science.\nInterdisciplinary studies in biology and political science aim to identify correlates of political behavior with biological aspects, for example the linkage of biology and political orientation, but also with other aspects like partisanship and voting behavior. This field of study is typically referred to genopolitics although it is sometimes referred to as biopolitics, although the term also has other meanings originating from the work of Michel Foucault.\nThe study of possible genetic bases of political behavior has grown since the 1980s. The term genopolitics was coined by political scientist James Fowler in the early-2000s to describe research into identifying specific transporter/receptor genes responsible for ideological orientation beyond the sociopsychological realm of political socialisation.\nOther research on genopolitics includes the article entitled \"Do Genes Contribute to the \u201cGender Gap\u201d\" which also attempts to explore genetic influences between the sexes and whether or not they contribute to political preferences. The authors concluded that \u201cthe findings support the claim that the environment (social or other) cannot be used in isolation to explain behavior differences between males and females, nor can all differences in modern political behaviors between the sexes simply be attributed to genes or presumptions about primitive man.\"\nPolitical participation.\nPolitical scientists also aim to understand what drives individuals to participate in the democratic process, either by voting, volunteering for campaigns, signing petitions or protesting. Participation cannot always be explained by rational behavior. The voting paradox, for example, points out that it cannot be in a citizen's self-interest to vote because the effort it takes to vote will almost always outweigh the benefits of voting, particularly considering a single vote is unlikely to change an electoral outcome. Political scientists instead propose that citizens vote for psychological or social reasons. Studies show, for example, that individuals are more likely to vote if they see their friends have voted or if someone in their household has received a nudge to vote.\nPolitical psychology.\nPolitical psychology aims to explain political behavior through psychological analysis. Examples of theories include right-wing authoritarianism, social dominance orientation, and system justification theory.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\n26. Pantoja, A. D., &amp; Segura, G. M. (2003). The Latino Vote in the 2004 Election: Political Behavior and Electoral Trends. American Political Science Review, 97(3), 495-507.\nThis source would provide the accurate data and analysis for the 44% of Latino voters supporting George W. Bush in 2004, clarifying the voting patterns during that election.\n27. Pew Research Center (2016). Hispanic Voter Turnout in the 2016 U.S. Election. Pew Research Center.\nThis report would offer the statistic on Hispanic American voter turnout in the 2016 election (47.6%), providing an up-to-date and reliable source.\n28. Romer, D. (2018). Latino Political Participation and Immigration Issues: The Impact of Policies on Voting Behavior. Journal of Politics, 80(1), 122-135.\nA study focusing on the concerns of Hispanic voters, particularly their stance on immigration, and how it influences their political behavior.\n29. Cohen, R. (2017). Cuban Americans in the 2016 Election: Shifting Allegiances and Political Behavior. Hispanic Journal of Political Science, 25(2), 40-56.\nProvides analysis of Cuban American voting trends, including the shift in voting behavior toward Trump in 2016.\n30. Hinojosa, R. (2018). The Cuban American Political Tradition: From Exile to Electorate. Latino Studies, 16(3), 317-335.\nExplores the historical context of Cuban American voting behavior, particularly their political exile following the Cuban Revolution and how it shapes current political affiliations."}
{"id": "37124", "revid": "11935096", "url": "https://en.wikipedia.org/wiki?curid=37124", "title": "Austin Powers (character)", "text": "Fictional character\nSir Austin Danger Powers is a fictional character from the \"Austin Powers\" series of films, and is created and portrayed by Mike Myers. He is the protagonist of ' (1997), ' (1999) and \"Austin Powers in Goldmember\" (2002).\nHe is a womanizing, hard-partying British spy embodying the Swinging London mod and hippie culture of the 1960s. Along with his nemesis Dr. Evil, he was frozen in a cryonics experiment, then unfrozen years later. The series' humor follows his attempts to adjust to the modern world as he continues to try to save it from terrorism.\nPersonality.\nThe character of Austin Powers was primarily a spoof of secret agents in 1960s spy films and spy spoof films, being influenced by Sean Connery's James Bond, Peter Sellers's Evelyn Tremble in \"Casino Royale\" (1967), Matt Helm in Dean Martin's films, James Coburn's Derek Flint in \"Our Man Flint\" (1966) and \"In Like Flint\" (1967), and Michael Caine's Harry Palmer in films such as \"The Ipcress File\" (1965) and \"Funeral in Berlin\" (1966), especially his thick horn-rimmed glasses. Myers was also influenced by the flamboyant dress sense of Jason King, played by Peter Wyngarde.\nThe character of Austin Powers represents an archetype of 1960s Swinging London, with his advocacy for free love, his use of obscure expressions and his clothing style (including crushed velvet suits and Beatle boots).\nDevelopment.\nMyers, Matthew Sweet and Susanna Hoffs formed the faux British 1960s band Ming Tea after Myers's \"Saturday Night Live\" stint in the early 1990s. The band members all performed under pseudonyms with 1960s' personas. Myers adopted the pseudonym and character of Austin Powers.\nThis group made a number of live club and television performances in character. Myers's then wife, Robin Ruzan, encouraged him to write a film based on Austin Powers. Obituaries of Simon Dee (1935\u20132009), the radio and BBC television presenter, stated that his \"Sixties grooviness\" made him the inspiration for the character.\nHeavily influenced by British pop culture growing up, Mike Myers has claimed that his British-born father was the inspiration behind Austin Powers.\nIn popular culture.\nThe May 2010 game \"Red Dead Redemption\" features an achievement called Austin Overpowered, requiring players to clear out hideouts in the New Austin region of the game.\nIn November 2010, he was voted #23 in \"Entertainment Weekly\"'s list \"The 100 Greatest Characters of the Last 20 Years.\"\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "37125", "revid": "13105798", "url": "https://en.wikipedia.org/wiki?curid=37125", "title": "Australian Security Intelligence Organisation", "text": "Australian domestic intelligence agency\nThe Australian Security Intelligence Organisation (ASIO ) is the domestic intelligence and national security agency of the Australian Government, responsible for protecting Australia from espionage, sabotage, foreign interference, politically motivated violence, terrorism, and attacks on the national defence system. ASIO is a primary entity of the Australian Intelligence Community.\nASIO has a wide range of surveillance powers to collect human and signals intelligence. Generally, ASIO operations requiring police powers of arrest and detention under warrant are co-ordinated with the Australian Federal Police (AFP) and/or with state and territory police forces. The organisation is comparable to that of the United States' FBI or the British MI5.\nASIO Central Office is in Canberra, with a local office being located in each mainland state and territory capital. A new $630 million Central Office named after Ben Chifley, the prime minister who created the organisation, was officially opened by then-prime minister Kevin Rudd on 23 July 2013.\nCommand, control and organisation.\nASIO is established and regulated under \"Australian Security Intelligence Organisation Act 1979 (Cwth)\", responsible to the federal parliament through the Minister for Home Affairs. ASIO also reports to the Parliamentary Joint Committee on Intelligence and Security, Senate\u2019s Legal and Constitutional Affairs Committee and is subject to independent review by the Inspector-General of Intelligence and Security. The head of ASIO is the Director-General of Security, who oversees the strategic management of ASIO within guidelines issued by the Attorney-General. The current Director-General of Security is Mike Burgess, who assumed office on 16 September 2019.\nIn 2018, ASIO had an average of 1,980 staff. Changes since to security measures have meant that the specific headcount is classified and not publicly available. The identity of ASIO officers other than the director-general and deputy director-generals remains an official secret. While ASIO is an equal opportunity employer, there has been some media comment of its apparent difficulty in attracting people from a Muslim or Middle Eastern background. Furthermore, ASIO has undergone a period of rapid growth with some 70% of its officers having joined since 2002, leading to what Paul O'Sullivan, director-general from 2005 to 2009, called 'an experience gap'.\nAustralian Security Intelligence Organisation Act 1979.\nThe Australian Security Intelligence Organisation Act 1979 (the ASIO Act) is an Act of the Parliament of Australia which replaced the \"Australian Security Intelligence Organisation Act 1956\", which had established ASIO as a statutory body. ASIO had been established in 1949 by Prime Minister Ben Chifley's Directive for the Establishment and Maintenance of a Security Service under the executive power of the Constitution, under the control of the Director-General of Security and responsible to the Attorney-General.\nAfter passage of the \"National Security Legislation Amendment Act 2014\" by the Australian Parliament, ASIO officers are exempt from prosecution for a wide range of illegal activities in the course of conducting \"operations\". ASIO officers may carry arms, and the Minister responsible has the ability under certain conditions to approve the provision of any weapon or training to any specified person, even outside of ASIO officers.\nOfficers of the organisation.\nOfficers of ASIO are employed under the ASIO Act, and are classed as \"Officers of the Commonwealth\" for the purposes of the Crimes Act 1914, which among other provisions makes impersonating an ASIO officer a criminal offence. The ASIO Act also makes the identification of ASIO officers a criminal offence punishable by one year imprisonment.\nPowers and accountability.\nSpecial investigative powers.\nThe special investigative powers available to ASIO officers under warrant signed by the attorney-general include:\nThe director-general also has the power to independently issue a warrant should a serious security situation arise and a warrant requested of the attorney-general has not yet been granted.\nAn ASIO officer may, without warrant, ask an operator of an aircraft or vessel questions about the aircraft or vessel, its cargo, crew, passengers, stores or voyage; and to produce supporting documents relating to these questions.\nSpecial terrorism investigative powers.\nWhen investigating terrorism, the director-general may seek a warrant from an independent judicial authority to allow:\nImmunity from prosecution.\nThe Act does not specifically state the types of crimes that ASIO officers receive immunity from, rather stating the exceptions in which an officer would not be indemnified by the Commonwealth. Section 35k (1) defines these activities as not being immune from liability for special intelligence conduct during special intelligence operations. An ASIO officer would be deemed to have committed a crime if they were to participate in any of the following activities under any circumstances:\nCollection of foreign intelligence.\nASIO's authority relates exclusively to domestic intelligence and intervention, however ASIO may take part in domestic intelligence gathering relating to a foreign threat alongside the Australian Secret Intelligence Service, Australian Signals Directorate and Australian Geospatial-Intelligence Organisation. ASIO has the power to collect foreign intelligence within Australia on the issuance of a warrant by the Attorney-General. \nAccountability.\nBecause of the nature of its work, ASIO does not make details of its activities public and law prevents the identities of ASIO officers from being disclosed. ASIO and the Australian Government say that operational measures ensuring the legality of ASIO operations have been established.\nASIO briefs the attorney-general on all major issues affecting security and they are also informed of operations when considering granting warrants enabling the special investigative powers of ASIO. Furthermore, the attorney-general issues guidelines with respect to the conduct of ASIO investigations relating to politically motivated violence and its functions of obtaining intelligence relevant to security.\nASIO reports to several governmental and parliamentary committees dealing with security, legislative and financial matters. This includes the Parliamentary Joint Committee on Intelligence and Security and the Senate\u2019s Legal and Constitutional Affairs Committee. A classified annual report is provided to the government, an unclassified edited version of which is tabled in federal parliament.\nThe Office of the Inspector-General of Intelligence and Security was established in 1986 to provide additional oversight of Australia\u2019s security and intelligence agencies. The inspector-general has complete access to all ASIO records and has a range of inquisitorial powers.\nRelationships with foreign agencies and services.\nAustralia\u2019s intelligence and security agencies maintain close working relationships with the foreign and domestic intelligence and security agencies of other nations. As of 22 October 2008, ASIO has established liaison relationships with 311 authorities in 120 countries.\nHistory.\nPre-ASIO.\nThe Australian Government assumed responsibility for national security and intelligence on federation in 1901, and took over various state agencies and had to rationalise their functions. There was considerable overlap between the civil and military authorities. Similarly, there was also no Commonwealth agency responsible for enforcing federal laws. At the outbreak of World War I, no Australian government agency was dedicated to security, intelligence or law enforcement. The organisation of security intelligence in Australia took on more urgency with a perceived threat posed by agents provocateurs, fifth columnists and saboteurs within Australia.\nIn 1915, the British government arranged for the establishment of a Commonwealth branch of the Imperial Counter Espionage Bureau in Australia. The branch came to be known as the Australian Special Intelligence Bureau (SIB) in January 1916, and maintained a close relationship with state police forces, and later with the Commonwealth Police Force, created in 1917, to conduct investigations independent of state police forces. After the war, on 1 November 1919, the SIB and Commonwealth Police were merged to form the Investigation Branch within the Attorney General's Department.\nProvoked by World War II, the Commonwealth Security Service was formed in 1941 to investigate organisations and individuals considered likely to be subversive or actively opposed to national interests; to investigate espionage and sabotage; to vet defence force personnel and workers in defence-related industries; to control the issue of passports and visas; and was responsible for the security of airports and wharves, and factories engaged in manufacture of munitions and other items necessary for Australia\u2019s war effort. It was also responsible for radio security. In June 1945 it produced a report warning of the danger of the Communist Party of Australia.\nRobert Frederick Bird Wake, one of the foundation directors of ASIO, is credited the creation of the Australian intelligence community in 1949, as claimed by Valdemar Wake, in his biography \"No Ribbons or Medals\" of his father's work as a counter espionage officer.&lt;ref name=\"Valdemar/Hereward\"&gt;&lt;/ref&gt;&lt;ref name=\"austlit/C524531\"&gt;&lt;/ref&gt;&lt;ref name=\"authorsden/30207\"&gt;&lt;/ref&gt; Wake worked closely with Director-General Reed. During World War II, Reed conducted an inquiry into Wake's performance as a security officer and found that he was competent and innocent of the charges laid by the Army's commander-in-chief, General Thomas Blamey. This was the start of a relationship between Reed and Wake that lasted for more than 10 years. Wake was seen as the operational head of ASIO.\nEstablishment and 'The Case'.\nFollowing the end of World War II, the joint United States-UK Venona project uncovered sensitive British and Australian government data being transmitted through Soviet diplomatic channels. Officers from MI5 were dispatched to Australia to assist local investigations. The leak was eventually tracked to a spy ring operating from the Soviet Embassy in Canberra. Allied Western governments expressed disaffection with the state of security in Australia.\nOn 9 March 1949, Prime Minister Ben Chifley created the post of Director-General of Security and appointed South Australian Supreme Court Justice Geoffrey Reed to the post. On 16 March 1949, Chifley issued a Directive for the Establishment and Maintenance of a Security Service. The Security Service's first authorised telephone interceptions were in June 1949, followed in July by a raid on the Sydney office of the Communist Party of Australia. In August 1949, Reed advised the Prime Minister that he had decided to name the service the 'Australian Security Intelligence Organization' [\"sic\"].\nThe new service was to be modelled on the Security Service of the United Kingdom MI5 and an MI5 liaison team (including Sir Roger Hollis) was attached to the fledgling ASIO during the early 1950s. Historian Robert Manne describes this early relationship as \"special, almost filial\" and continues \"ASIO's trust in the British counter-intelligence service appears to have been near-perfect\".\nThe Labor Government was defeated at the December 1949 federal election, and in March 1950 the new prime minister, Robert Menzies, appointed the Deputy Director of Military Intelligence, Charles Spry, as the second Director-General of Security, commencing on 9 July 1950. Wake resigned shortly after Spry's appointment. On 6 July 1950, a Directive of Prime Minister Menzies set out the Charter of the Australian Security Intelligence Organization, which expanded on Chifley's 1949 Directive. ASIO was converted to a statutory body on 13 December 1956 by the \"Australian Security Intelligence Organisation Act 1956\" (later repealed by the \"Australian Security Intelligence Organisation Act 1979\", the current legislation as amended to 2007). Spry would continue to hold the post until January 1970. The spelling of the organisation was amended by legislation in 1999 to bring it into line with the Australian standard form 'organisation'.\nThe operation to crack the Soviet spy ring in Canberra consumed much of the resources of ASIO during the 1950s. This operation became internally known as \"The Case\". Among the prime suspects of the investigations were Wally Clayton, a prominent member of the Australian Communist Party, and two diplomats with the Department of External Affairs, Jim Hill and Ian Milner. However, no charges resulted from the investigations, because Australia did not have any laws against peacetime espionage at the time.\nThe Petrov Affair.\n5 February 1951 saw the arrival in Sydney of Vladimir Mikhaylovich Petrov, Third Secretary of the Soviet Embassy. An ASIO field officer identified Petrov as a possible 'legal', an agent of the Soviet Ministry of State Security (MGB, a forerunner to the KGB) operating under diplomatic immunity. The Organisation began gently cultivating Petrov through another agent, Dr. Michael Bialoguski, with the eventual goal of orchestrating his defection. Ultimately, Petrov was accused by the Soviet Ambassador of several lapses in judgement that would have led to his imprisonment and probable execution upon his return to the Soviet Union. Petrov feared for his life and accepted the defection life-line provided by ASIO.\nThe actual defection occurred on 3 April 1954. Petrov was spirited to a safe house by ASIO officers, but his disappearance and the seeming reluctance of Australian authorities to search for him made the Soviets increasingly suspicious. Fearing a defection by Petrov, MVD officers dramatically escorted his wife Evdokia to a waiting aeroplane in Sydney. There was doubt as to whether she was leaving by choice or through coercion and so Australian authorities initially did not act to prevent her being bundled into the plane. However, ASIO was in communication with the pilot and learned through relayed conversations with a flight attendant that if Evdokia spoke to her husband she might consider seeking asylum in Australia.\nAn opportunity to allow her to speak with her husband came when the Director-General of Security, Charles Spry, was informed that the MVD agents had broken Australian law by carrying firearms on an airliner in Australian airspace and so could be detained. When the aeroplane landed in Darwin for refuelling, the Soviet party and other passengers were asked to leave the plane. Police, acting on ASIO orders, quickly disarmed and restrained the two MVD officers and Evdokia was taken into the terminal to speak to her husband via telephone. After speaking to him, she became convinced he was alive and speaking freely and asked the Administrator of the Northern Territory for political asylum.\nThe affair sparked controversy in Australia when circumstantial links were noted between the leader of the Australian Labor Party and the Communist Party of Australia (and hence to the Soviet spy ring). H.V. Evatt, the leader of the Labor Party at the time, accused Prime Minister Robert Menzies of arranging the Petrov defection to discredit him. The accusations lead to a disastrous split in the Labor party.\nPetrov was able to provide information on the structure of the Soviet intelligence apparatus in the mid-1950s, information that was highly valuable to the United States. It was by obtaining this information that the Organisation's reputation in the eyes of the United States was greatly enhanced.\nIn fact, when Brigadier Spry retired, the Deputy Director of the CIA sent the following tribute:\nThe relationship between the CIA and ASIO started as a very personal one. The real substantive relationship started with Sir Charles' visit in 1955... Since Sir Charles' first visit, the relationships with ASIO have continued to become closer and closer until today we have no secrets, regardless of classification or sensitivity, that are not made available to ASIO if it is pertinent to Australia\u2019s internal security... I feel, as does the Director, a type of mutual trust in dealing with ASIO that is exceeded by no other service in the world today.\nThe Cold War.\nASIO's counter-intelligence successes continued throughout the Cold War. Following an elaborate investigation between 1961 and 1963, ASIO recommended the ejection of the First Secretary of the Soviet Embassy, Ivan Skripov, and his declaration as \"persona non grata\". Skripov had been refining Kay Marshall,&lt;ref name=\"theaustralian/Blundell/Butt\"&gt;&lt;/ref&gt; an English-Australian woman&lt;ref name=\"thewest/b881629087z\"&gt;&lt;/ref&gt; as an agent for Soviet intelligence; however, she was in fact an agent of ASIO.\nIn April 1983, ASIO uncovered more Soviet attempts at espionage and Valery Ivanov, who also held the post of First Secretary at the Soviet Embassy, was declared \"persona non grata\". He was ejected from Australia on the grounds that he had performed duties in violation of his diplomatic status.\nPenetration by the KGB.\nThese successes were marred, however, by the penetration of ASIO by a KGB mole in the 1970s. Due to the close defence and intelligence ties between Australia and the United States, ASIO became a backdoor to American intelligence. Upon realising ASIO was compromised, the United States pulled back on the information it shared with Australia.\nFollowing a strenuous internal audit and a joint Federal Police investigation, George Sadil was accused of being the mole. Sadil had been a Russian interpreter with ASIO for some 25 years and highly classified documents were discovered in his place of residence. Federal Police arrested Sadil in June 1993 and charged him under the Crimes Act 1914 with several espionage and official secrets related offences. However, parts of the case against him collapsed the following year.\nSadil was committed to trial in March 1994, but the Director of Public Prosecutions decided not to proceed with the more serious espionage-related charges after reviewing the evidence against him. Sadil's profile did not match that of the mole and investigators were unable to establish any kind of money trail between him and the KGB.\nSadil pleaded guilty in December 1994 to thirteen charges of \"removing ASIO documents contrary to his duty\", and was sentenced to three months imprisonment. He was subsequently released on a 12-month good behaviour bond. It is believed that another ASIO officer, now retired, is suspected of being the mole but no prosecution attempts have been made.\nIn November 2004, former KGB Major-General Oleg Kalugin confirmed to the Australian Broadcasting Corporation's \"Four Corners\" programme that the KGB had in fact infiltrated ASIO in the late 1970s and early 1980s.\nASIO acknowledged in October 2016 that it had been infiltrated.\nIn 2023, the mole was identified as Ian George Peacock. Peacock's code name within the KGB was \"Mira\".\nSydney 2000 Olympic Games.\nASIO began planning for the 2000 Olympic and Paralympic Games, held in Sydney, as early as 1995. A specific Olympics Coordination Branch was created in 1997, and began recruiting staff with \"specialised skills\" the following year. In 1998, ASIO \"strengthened information collection and analytical systems, monitored changes in the security environment more broadly, improved its communications technology and provided other agencies with strategic security intelligence assessments to assist their Olympics security planning\".\nThe Olympics Coordination Branch also began planning for the Federal Olympic Security Intelligence Centre (FOSIC) in 1998. FOSIC was to \"provide security intelligence advice and threat assessments to State and Commonwealth authorities during the Sydney 2000 Games\".\nSurveillance of anti-coal activists.\nIn 2012 it was reported that ASIO had been monitoring the actions of Australians protesting against the coal industry, and was increasing its efforts from previous years. Minister Martin Ferguson said that he was particularly concerned about protests relating to the Hazelwood power station in Victoria. An unnamed security source told \"The Age\" newspaper that \"providing advice and intelligence to safeguard [critical infrastructure] is clearly within ASIO's responsibilities... ASIO has a clear role, including protection against sabotage. And it's clear [environmental] activists pose a greater threat to energy facilities than terrorists.\" A spokesperson for Attorney General Nicola Roxon described ASIO's responsibility in monitoring political action groups as \"limited to activity that is, or has the potential to be, violent for the purposes of achieving a political objective\". Australian Greens party leader Bob Brown described ASIO monitoring environmentalists as a \"political weapon\" used by the Government for the benefit of \"foreign-owned mining corporations\".\nChinese intelligence activity.\nNicola Roxon, the Attorney-General of Australia, blocked Chinese, state-owned company Huawei from seeking a supply contract for the National Broadband Network, on the advice of the ASIO. The Australian government feared Huawei would provide backdoor access for Chinese cyber espionage.\nIn May 2013, ABC News claimed that China stole blueprints to the headquarters of the ASIO.\nSheri Yan and Roger Uren were investigated by ASIO on suspicion of spying for China. Uren, former Assistant Secretary responsible for the Asia section of the Office of National Assessments, was found to have removed documents pertaining to Chinese intelligence operations in Australia, and kept them in his apartment. Yan was suspected of undertaking influence operations on behalf of the Chinese Communist Party, and introducing Colonel Liu Chaoying, a military intelligence officer, to Australian contacts.\nExpansion of powers, 2020.\nIn 2020, Peter Dutton, then Minister for Home Affairs, introduced the Australian Security Intelligence Organisation Amendment Bill 2020, which expanded ASIO's questioning powers to cover espionage, foreign interference, and political violence, while the age which ASIO can compulsively question minors has been reduced to 14 from 16. Furthermore, ASIO can authorize the usage of tracking devices without warrants. The Law Council of Australia criticized the bill and compared it to the 2020 Hong Kong national security law, due to its expansion of questioning powers to cover political violence, which the LCA argued could be used against acts of lawful protest, a claim that ASIO head Mike Burgess rejected.\nIn March 2021, ASIO's director-general Mike Burgess told \"The Guardian Australia\" that the agency had removed a \"nest of spies\" from an unidentified country in 2020. Burgess also acknowledged that ASIO's counter-terrorism case load relating to right-wing extremism had increased from 30% to 40% since the Christchurch mosque shootings in New Zealand. During a 2021 speech, Burgess also confirmed that ASIO would use new terminology including \"ideologically motivated violent extremism\" to refer to right-wing extremism and \"religiously motivated violent extremism\" to refer to Islamic extremism. \nIn early May 2024, \"The Washington Post\" reported that ASIO had expelled two officers from India's foreign intelligence service, the Research and Analysis Wing in 2020, alleging they were part of a \"nest of spies\" who had sought to cultivate politicians, monitor diaspora communities and obtain classified information. The \"Post\" reported that the 2020 incident was part of a series of clashes between RAW and other Western domestic security services in Germany and the United Kingdom.\nIn early November 2025, Burgess stated that three unidentified countries were capable of assassinating perceived political dissidents in Australia. He also confirmed that ASIO had disrupted a foreign intelligence gathering operation involving an Australian citizen, who had been instructed to gather information about Australia's economy, critical minerals and the AUKUS defence pact.\nIranian state-sponsored terrorism.\nIn late August 2025, an ASIO investigation found that Islamic Revolutionary Guard Corps (IGRC) was implicated in an arson attack on a Jewish restaurant in Sydney in October 2024 and a Melbourne synagogue in December 2024. In response, the Australian Prime Minister Anthony Albanese declared the Iranian Ambassador Ahmad Sadeghi and three other Iranian officials persona non grata and withdrew Australian diplomats from Tehran. The Australian Government also confirmed it would designate the IRGC as a terrorist organisation.\nRoyal commissions, inquiries and reviews.\nRoyal Commission on Intelligence and Security, 1974\u201377.\nOn 21 August 1974, Prime Minister Gough Whitlam announced the establishment of the Royal Commission on Intelligence and Security to inquire into Australia's intelligence agencies. Justice Robert Hope of the Supreme Court of New South Wales was appointed as Royal Commissioner. In 1977 the First Hope Commission made many findings about, and recommendations on, ASIO in the Fourth Report, some of which had been preempted by the Whitlam and Fraser governments. The commission marked the first review of the organisation and was fundamental to securing it as part of Australia's state defensive apparatus. In a secret supplementary report, much of which remains classified, Hope indicated his belief that ASIO's past conduct was the result of its infiltration by a hostile foreign intelligence agency. In a 1998 interview Hope stated that saw some of his major recommendations as having been wrong.\nThe Commission found that ASIO provided the CIA with information about prominent Australian politicians and government officials. The information included accusations of subversive activities and details of private lives.\nProtective Security Review, 1978\u201379.\nFollowing the Sydney Hilton bombing in 1978, the government commissioned Justice Hope with conducting a review into national protective security arrangements and into co-operation between Federal and State authorities in regards to security. In the report concluded in 1979, Justice Hope designated ASIO as the agency responsible for national threat assessments in terrorism and politically motivated violence. He also recommended that relations between ASIO and State and Territory police forces be regulated by arrangements between governments.\nRoyal Commission on Australian Security and Intelligence Agencies, 1983\u201384.\nFollowing the publicity surrounding the expulsion of Valery Ivanov, First Secretary at the Soviet Embassy in Canberra, the Government established a Royal Commission to review the activities of Australian security and intelligence agencies. Justice Hope was again Royal Commissioner.\nJustice Hope completed his report in December 1984. His recommendations included that:\nJustice Hope also recommended that amendments to the ASIO Act provide that \"it is not the purpose of the Act that the right of lawful advocacy, protest or dissent should be affected or that exercising those rights should, by themselves, constitute activity prejudicial to security\".\nPost-Cold War review, 1992.\nIn early 1992, Prime Minister Paul Keating commissioned a review \"of the overall impact of changes in international circumstances on the roles and priorities of the Australian intelligence agencies\". In his statement of 21 July 1992, Keating said:\nConsistent with the philosophy of a separation of the assessment, policy and foreign intelligence collection functions, the Government considers that the existing roles of the individual agencies remain valid in the 1990s. The rationale outlined by Mr Justice Hope for ASIO as a freestanding, non-executive, advisory intelligence security agency remains relevant in the 1990s and the Government has therefore decided that ASIO should continue to have the roles and responsibilities laid down in existing legislation.\nThe Soviet threat certainly formed an important component of ASIO's activities, but threats from other sources of foreign interference and politically motivated violence have been important to ASIO for some time, and will remain so. However, the implications for ASIO of the changes in the former Soviet Union and Eastern Europe are more far-reaching than for the other agencies. The Government has therefore decided that while ASIO's capacity to meet its responsibilities must be maintained, there is scope for resource reductions.\nThe resource reductions mentioned were a cut of 60 staff and a $3.81 million budget decrease.\nInquiry into National Security, 1993.\nFollowing the trial of George Sadil over the ASIO mole scandal and from concern about the implications of material having been removed from ASIO without authority, the Prime Minister announced the appointment of Mr Michael Cook AO (former head of the Office of National Assessments) to inquire into various aspects of national security. The review was completed in 1994.\nParliamentary Joint Committee inquiries.\nThe Parliamentary Joint Committee completed several reviews and inquiries into ASIO during the 1990s. The first concerned the security assessment process. Another was held in September into \"the nature, scope and appropriateness of the way in which ASIO reports to the Australian public on its activities\". The Committee concluded that \"the total package of information available to the Australian community about ASIO's operations exceeds that available to citizens in other countries about their domestic intelligence agencies.\" Pursuant to this, recommendations were made regarding the ASIO website and other publicly accessible information.\nTransfer to Home Affairs.\nIn July 2018, then-Prime Minister Malcolm Turnbull announced the creation of the Department of Home Affairs - a new ministry to include the Australian Federal Police, the Australian Border Force, and the Australian Security Intelligence Organisation. This meant the transfer of ASIO away from the Attorney-General's Department, although the Attorney-General would remain responsible for approving ASIO warrants. This move was somewhat criticised, with John Blaxland from the Australian National University warning against tampering with a system that was \"arguably the envy of the world\", saying \"I have yet to see any compelling evidence that what we have is not working, or that there is a compellingly better option out there.\"\nIn July 2024, it was reported that ASIO was to be moved back to its original setting within the Attorney-General's Department. Under the new arrangement, the Department of Home Affairs retains responsibility for national security policy, its design and implementation, while operational control of ASIO shifts back to the Attorney-General's Department.\nCriticisms and controversies.\nInfiltration by Soviet spies.\nFrom the earliest years of ASIO's existence, possibly from its inception, the organization has been infiltrated by Soviet spies. This was admitted by ASIO beginning in 2016, though other sources had made earlier allegations that Soviet spies had deeply infiltrated ASIO at nearly all levels of intelligence and operations.\nRaids on ASIO Central Office, 1973.\nAccusations against ASIO were raised by the Attorney-General Lionel Murphy following a series of bombings from 1963 to 1970 on the consulate of Communist Yugoslavia in Australia by Croatian far-right militia. Murphy alleged that ASIO had withheld information on the group which could have led to preventative measures taken against further bomb attacks (however, Murphy was a member of the recently sworn in Labor government, which still held a deep-seated suspicion of ASIO).\nOn 15 March 1973, Murphy and the Commonwealth Police raided the ASIO offices in Melbourne. While some claim the raid was disastrous, serving little purpose other than to shake-up both ASIO and the Whitlam government, the findings of such investigations were not published.\nThe Sydney Hilton bombing allegations of conspiracy, 1978.\nOn 13 February 1978, the Sydney Hilton Hotel was bombed, one of the few domestic terrorist incidents on Australian soil. The Hotel was the location for the Commonwealth Heads of Government Meeting (CHOGM). Three people in the street were killed \u2013 two council workers and a policeman \u2013 and several others injured. Former police officer Terry Griffiths, who was injured in the explosion, provided some evidence that suggested ASIO might have orchestrated the bombing or been aware of the possibility and allowed it to proceed. In 1985, the Director-General of Security issued a specific denial of the allegation. In 1991 the New South Wales parliament unanimously called for a joint State-Federal inquiry into the bombing. However, the Federal government vetoed any inquiry.\nAnti-terrorism bungle, 2001.\nA few weeks after the 11 September 2001 attacks on the United States, mistakes led ASIO to incorrectly raid the home of Bilal Daye and his wife. It has been revealed that the search warrant was for a different address. The couple subsequently sought damages and the embarrassing incident was settled out of court in late 2005, with all material relating to the case being declared strictly confidential.\nKim Beazley-Ratih Hardjono investigation, 2004.\nIn June 2004, Kim Beazley was accused of having a \"special relationship\" with Ratih Hardjono when he was defence minister. Hardjono was allegedly accused of \"inappropriately\" photographing a secure Australian Defence facility, working with the embassy ID, and having a close working relationship with her uncle, a senior officer in BAKIN (Indonesian Intelligence). In July, journalist Greg Sheridan contacted the then head of ASIO, Dennis Richardson, and discussed a classified operational investigation. Later in July members of the Attorney General's department were still investigating the original allegation, making Richardson's comments premature and inaccurate. The whole episode was a salient reminder to politicians in Canberra of the British experience of 'agents of influence' and honeypots. Ratih Hardjono was married to Bruce Grant in the 1990s.\nDetention and removal of Scott Parkin, 2005.\nIn September 2005, the visa of American citizen, Scott Parkin, was cancelled after Director-General of Security, Paul O'Sullivan, issued an adverse security assessment of the visiting peace activist. Parkin was detained in Melbourne and held in custody for five days before being escorted under guard to Los Angeles, where he was informed that he was required to pay the Australian Government A$11,700 for the cost of his detention and removal. Parkin challenged the adverse security assessment in the Federal Court in a joint civil action with two Iraqi refugees, Mohammed Sagar and Muhammad Faisal, who faced indefinite detention on the island of Nauru after also receiving adverse security assessments in 2005.\nPrior to his removal, Parkin had given talks on the role of U.S. military contractor Halliburton in the Iraq war and led a small protest outside the Sydney headquarters of Halliburton subsidiary KBR. The Attorney-General at that time, Philip Ruddock, refused to explain the reasons for Parkin's removal, leading to speculation that ASIO had acted under pressure from the United States. This was denied by O'Sullivan before a Senate committee, where he gave evidence that ASIO based its assessment only on Parkin's activities in Australia. O'Sullivan refused to answer questions before a later Senate committee hearing after his legal counsel told the Federal Court that ASIO did not necessarily base its assessment solely on Parkin's activities in Australia.\nKidnap and false imprisonment of Izhar ul-Haque, 2007.\nOn 12 November 2007, the Supreme Court of New South Wales dismissed charges brought against a young medical student, Izhar ul-Haque. ASIO and the Australian Federal Police had investigated ul-Haque for allegedly training with Lashkar-e-Toiba in Pakistan, a declared terrorist organisation under the \"Security Legislation Amendment (Terrorism) Act 2002\". However, the case against the medical student collapsed when it was revealed that ASIO officers had engaged in improper conduct during the investigation. Justice Michael Adams determined that because ul-Haque was falsely led to believe that he was legally compelled to comply with the ASIO officers, the conduct of at least one of the investigating ASIO officers constituted false imprisonment and kidnap at common law, and therefore key evidence against ul-Haque was inadmissible.\nArchival material.\nNon-current ASIO files are stored at the National Archives of Australia, and can be released to the public under the \"Archives Act 1983\" after 30 years, unless they fall into any of 16 exemption categories itemised in section 33 of the \"Archives Act\".\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "37126", "revid": "36418353", "url": "https://en.wikipedia.org/wiki?curid=37126", "title": "Australian Secret Intelligence Service", "text": "Australian foreign intelligence agency\nThe Australian Secret Intelligence Service (ASIS ) is the foreign intelligence agency of the Commonwealth of Australia, responsible for gathering, processing, and analysing national security information from around the world, primarily through the use of human intelligence. The service was formed in 1952, however its existence remained secret within much of the government and to the public until 1972. ASIS is a primary entity of the Australian Intelligence Community.\nASIS is part of the Department of Foreign Affairs and Trade (DFAT) portfolio and has its headquarters in Canberra. Its director-general, currently Kerri Hartland, reports to the minister for foreign affairs. The service is comparable to the CIA (US) and MI6 (UK).\nHistory.\nOn 13 May 1952, in a meeting of the Executive Council, Prime Minister Robert Menzies established ASIS by executive order under s.\u00a061 of the Australian constitution, appointing Alfred Deakin Brookes as the first director-general of ASIS. The existence of ASIS remained secret even within the Australian Government until 1972.\nIts Charter of 15 December 1954 described ASIS's role as 'to obtain and distribute secret intelligence, and to plan for and conduct special operations as may be required'. ASIS was expressly required to \"operate outside Australian territory\". A Ministerial Directive of 15 August 1958 indicated that its special operations role included conducting \"special political action\". It also indicated that the organisation would come under the control and supervision of the Minister for External Affairs rather than the Minister for Defence. At the time, ASIS was substantially modelled on the United Kingdom's Secret Intelligence Service, also known as MI6. ASIS was at one time referred to as MO9.\nOn 1 November 1972, the existence of ASIS was sensationally revealed by \"The Daily Telegraph\" which ran an expos\u00e9 of recruitment of ASIS agents from Australian universities for espionage activities in Asia. Soon after \"The Australian Financial Review\" published a more in-depth piece on ASIS, the Australian Security Intelligence Organisation (ASIO), and the then Joint Intelligence Organisation (JIO), Defence Signals Division (DSD) and Office of National Assessments (ONA). It stated that \"[t]he ASIS role is to collect and disseminate facts only. It is not supposed to be in the analytical or policy advising business though this is clearly difficult to avoid at times\". The Ministerial Statement of 1977 stated that the \"main function\" of ASIS was to \"obtain, by such means and subject to such conditions as are prescribed by the Government, foreign intelligence for the purpose of the protection or promotion of Australia or its interests\".\nOn 21 August 1974, Prime Minister Gough Whitlam established the Royal Commission on Intelligence and Security (the first Hope Royal Commission, 1974\u201377) to investigate the country's intelligence agencies. On 25 October 1977, Prime Minister Malcolm Fraser publicly announced the existence of ASIS and its functions on a recommendation of the Commission.\nIn 1992, two reports were prepared on ASIS by officers within the Department of Prime Minister and Cabinet and the Office of National Assessments for the Secretaries Committee on Intelligence and Security (SCNS) and the National Security Committee (NSC). The Richardson Report in June examined the roles and relationships of the collection agencies (ASIO, ASIS and DSD) in the post-Cold War era. The Hollway Report in December examined shortfalls in Australia's foreign intelligence collection. Both reports endorsed the structure and roles of the organisations and commended the performance of ASIS.\nThe \"Intelligence Services Act 2001\" (ISA) converted ASIS to a statutory body. The Act set out the functions of ASIS and the limits on those functions. Use of weapons by ASIS were prohibited (except for self-defence). Conduct of violent or para-military operations was also curtailed. The Act authorised the responsible minister to issue directions to the agency and required Ministerial authorisation for intelligence collection activities involving Australians, but limited the circumstances in which that could be done. The Act requires the responsible minister to make rules regulating the communication and retention of intelligence information concerning Australian persons, and provides for the establishment of a parliamentary oversight committee, then called the Parliamentary Joint Committee on ASIO, ASIS and DSD.\nThe \"Intelligence Services Amendment Act 2004\" removed ISA prohibitions on ASIS operatives carrying firearms (but only for protection) and allows ASIS to work with foreign intelligence agencies such as the CIA or MI6 in the planning of paramilitary and violent operations provided ASIS is not involved in the execution of the operations.\nRoyal Commissions examining ASIS.\nThree Royal Commissions have examined, among other things, ASIS and its operations: in 1974 and 1983 (the Hope Royal Commissions), and in 1994 (the Samuels and Codd Royal Commission).\nFirst Hope Royal Commission.\nOn 21 August 1974, the Whitlam Government appointed Justice Robert Hope to conduct a Royal Commission into the structure of Australian security and intelligence services, the nature and scope of the intelligence required and the machinery for ministerial control, direction and coordination of the security services. The Hope Royal Commission delivered eight reports, four of which were tabled in Parliament on 5 May 1977 and 25 October 1977. Aside from the observation that ASIS was \"singularly well run and well managed\", the report(s) on ASIS were not released. Results from the other reports included the Australian Security Intelligence Organisation Act 1979, the establishment of the ONA and the passage of the \"Office of National Assessments Act 1977\".\nSecond Hope Royal Commission.\nOn 17 May 1983, the Hawke Government reappointed Justice Hope to conduct a second Royal Commission into Australia's intelligence agencies. The inquiry was to examine progress in implementing the previous recommendations; arrangements for developing policies, assessing priorities and coordinating activities among the organisations; ministerial and parliamentary accountability; complaints procedures; financial oversight and the agencies' compliance with the law. As with the first Hope Royal Commission, the reports on ASIS and DSD, which included draft legislation on ASIS, were not made public.\nSamuels and Codd Royal Commission.\nIn response to a \"Four Corners\" TV program aired on 21 February 1994, the Minister for Foreign Affairs Gareth Evans announced on 23 February 1994 a \"root and branch\" review of ASIS. The Government appointed Justice Gordon Samuels and Mr Mike Codd to inquire into the effectiveness and suitability of existing arrangements for control and accountability, organisation and management, protection of sources and methods, and resolution of grievances and complaints. The Royal Commission reported in March 1995.\n\"Four Corners\" reporter Ross Coulthart made allegations regarding intelligence held by ASIS on Australians. He claimed that \"ASIS secretly holds tens of thousands of files on Australian citizens, a database completely outside privacy laws\". The allegation was investigated and denied by Samuels and Codd (see below), but the Minister did acknowledge that ASIS maintained files. The Minister said: \"ASIS does have some files, as one would expect in an organisation of that nature, even though its brief extends to activities outside the country rather than inside. They are essentially of an administrative nature.\" However, Samuels and Codd did find that certain grievances of the former officers were well founded. They appeared to support the officers' concerns regarding the grievance procedures:\nBearing in mind the context in which the members of ASIS work, it is not surprising that there should develop a culture which sets great store by faithfulness and stoicism and tends to elevate conformity to undue heights and to regard the exercise of authority rather than consultation as the managerial norm.\nHowever, Samuels and Codd observed that the information published in the \"Four Corners\" program was \"skewed towards the false\", that \"the level of factual accuracy about operational matters was not high\", and, quoting an aphorism, that \"what was disturbing was not true and what was true was not disturbing\". They concluded that the disclosure of the information was unnecessary and unjustifiable and had damaged the reputation of ASIS and Australia overseas. The commissioners stated that \"evidence presented to us of action and reaction in other countries satisfies us that the publication was damaging\": They rejected any suggestion that ASIS was unaccountable or \"out of control\". They said, \"its operational management is well structured and its tactical decisions are thoroughly considered and, in major instances, subject to external approval\". They recommended that complaints regarding ASIS operations continue to be handled by the Inspector-General of Intelligence and Security (IGIS) but that staff grievances be handled by the Administrative Appeals Tribunal.\nIn addition to their recommendations, Samuels and Codd put forward draft legislation to provide a statutory basis for ASIS and to protect various information from disclosure. The Samuels and Codd Bill, like the bulk of the reports, was not made public.\nActivities.\nSince 2004, ASIS has been running anti-people smuggling operations inside countries such as Pakistan, Sri Lanka and Indonesia.\nIn 2013, intelligence provided by ASIS was crucial to the capture, after a 14-month manhunt, of a rogue soldier from the Afghan National Army, who had killed three Australian soldiers. The joint operation involved ASIS, AGO, the Defence Intelligence Organisation and Australian Signals Directorate, along with Britain's MI6 and Special Air Service, the United States' CIA and National Security Agency, and Pakistan's Inter-Services Intelligence.\nIn 2021, ASIS had deployed a small team to provide security and to help with the evacuation of Australian nationals and the nation's informants during the Kabul airlift in Afghanistan.\nControversies.\nASIS in Chile 1973.\nAn ASIS station was established in Chile out of the Australian embassy in July 1971 at the request of the CIA and authorised by then Liberal Party Foreign Minister William McMahon. New Labor Prime Minister Gough Whitlam was informed of the operation in February 1973 and signed a document ordering the closure of the operation several weeks later. On 1 July 1973, the ASIS station in Chile reported that it had shut down and destroyed all records. However, the last ASIS agent did not leave Chile until October 1973, one month after the CIA-backed 1973 Chilean coup d'\u00e9tat had brought down the Allende government. Two officers of ASIO were also based in Santiago, working as migration officers during this period. The incident was one of two that caused a confrontation between Whitlam and Bill Robertson, the director-general of ASIS, which culminated in Robertson's sacking on 21 October 1975, with effect from 7 November, just four days before Whitlam's own dismissal in the 1975 Australian constitutional crisis. Whitlam said Robertson had disobeyed instructions by delaying the closure of the ASIS station in Chile in 1973 and not informing Whitlam that ASIS had an active agent in East Timor in 1975. Robertson disputed the details in a personal statement lodged with the National Archives in 2009.\nASIS's involvement in Chile was revealed in 1974 when Whitlam set up the First Hope Royal Commission to investigate Australia's security services. Whitlam told parliament that \"when my government took office, Australian intelligence personnel were working as proxies of the CIA in destabilising the government of Chile\". After the coup by Augusto Pinochet, Whitlam's government created a special program for Chilean refugees to come to Australia. Under the program, about 6,000 Chileans came to Australia between 1974 and 1981 and hundreds more joined them as part of a family reunion program.\nThe National Archives of Australia holds documents related to ASIS operations to help the CIA undermine the government of Allende in the years 1971-1974. In 2021, the archives refused a request from Clinton Fernandes, professor of International and Political Studies at the University of New South Wales, to access records relating to ASIS operations in Chile. Heavily redacted versions of some documents were released to Fernandes in June 2021. The documents show that the ASIS base in Chile assisted the CIA's destabilisation of Allende's government by handling CIA-recruited Chilean assets and filing intelligence reports to CIA headquarters in Langley, Virginia. In November 2021, the Administrative Appeals Tribunal (AAT) upheld the decision to reject Fernandes's request for access to the documents. The AAT said the release of documents would \"cause damage to the security, defence or international relations of the Commonwealth\". Most of the AAT hearing was held behind closed doors, because Attorney-General Michaelia Cash issued a public interest certificate, suppressing the disclosure of evidence provided by ASIS, ASIO and the Department of Foreign Affairs and Trade.\nThe Favaro affair.\nDuring the lead-up to Indonesia's invasion of East Timor in 1975, ASIS paid a Dili-based Australian businessman, Frank Favaro, for information on local political developments. The leaking of his identity in late 1975 was another factor in the confrontation between Whitlam and Robertson. Bill Robertson disputed the reason for his dismissal in documents lodged with the National Archives in 2009.\nThe Sheraton Hotel incident.\nOn 30 November 1983, ASIS garnered unwanted negative attention when a training operation held at the Sheraton Hotel, now the Mercure (Spring Street), in Melbourne went wrong. The exercise was to be a mock surveillance and hostage rescue of foreign intelligence officers. In March 1983, ASIS had begun training a covert team of civilians at Swan Island in Victoria whose role was to protect or release Australians who may be threatened or captured by terrorists overseas. The military in 1981 had established a counter-terrorist unit for operations only in Australia. The personnel involved in the training operation included ten operators, four ASIS officers and six ASIS civilian trainees, and two commandos from the Army Reserve 1st Commando Regiment with a sergeant participating as an observer in the hotel foyer.\nThe training operation involved junior officers who had undergone three weeks' prior training and who were given considerable leeway in planning and executing the operation. The mock hostage rescue was staged on the 10th floor of the hotel without the permission of the hotel's owner or staff. When ASIS operators were refused entry into a hotel room, they broke down the door with sledgehammers. The hotel manager, Nick Rice, was notified of a disturbance on the 10th floor by a hotel guest. When he went to investigate, he was forced back into the lift by an ASIS operator who rode the lift down to the ground floor and forcibly ejected Rice into the lobby. Believing a robbery was in progress, Rice called the police. When the lift started returning to the ground floor, ASIS operators emerged wearing masks and openly brandishing 9mm Browning pistols and Heckler &amp; Koch MP5 submachine guns, two of them with silencers. They forced their way through the lobby to the kitchen, where two getaway cars were waiting outside the kitchen door. Police stopped one of the cars and arrested the occupants \u2013 two ASIS officers and three ASIS civilian trainees \u2013 who refused to produce any form of identification.\nWithin two days, the minister for foreign affairs, Bill Hayden announced that an \"immediate and full\" investigation would be conducted under the auspices of the second Hope Royal Commission, which was still in progress. A report was prepared and tabled by February 1984. It described the exercise as being \"poorly planned, poorly supervised and poorly run\", and recommended that measures be taken in training to improve planning and eliminate adverse impacts on the public.\nVictoria Police conducted their own investigation but were frustrated because ASIS Director-General John Ryan refused to cooperate. Bill Hayden offered to provide the real names of the seven officers involved, in confidence. Premier of Victoria John Cain told Hayden that \"as far as the police were concerned, there was no such thing as information in confidence\".\nFollowing the incident, \"The Sunday Age\" disclosed the names, or the assumed names, of five of the operators involved. The journalist noted that \"according to legal advice taken by \"The Sunday Age\" there is no provision that prevents the naming of an ASIS agent\". Although not included within the public version of the report, the Hope Royal Commission prepared an appendix that would appear to have dealt with the security and foreign relations consequences of \"The Sunday Age\"'s disclosure of participants' names. Subsequently, in \"A v Hayden\", the High Court held that the Commonwealth owed no enforceable duty to ASIS officers to maintain confidentiality of their names or activities.\nAt the time of the Sheraton Hotel incident, the extant Ministerial Directive permitted ASIS to undertake \"covert action\", including \"special operations\" which, roughly described, comprised \"unorthodox, possibly para-military activity, designed to be used in case of war or some other crisis\". Following the incident and the recommendations of the Royal Commission, the covert action function was apparently abolished. The functions of ASIS can be found in section 6 of the Intelligence Services Act, as can those functions which are proscribed by the act.\nUltimately, in executing the operation, the operators were found to have used considerable force, menacing a number of the staff and guests with weapons and physically assaulting the hotel manager. Hope found Ryan to be at fault for authorising the training operation in a public place using concealed weapons. Ryan resigned in February 1984. Hope said it was not part of his terms of reference to make findings or recommendations on whether any individual had committed any offence. However, he did note that the individuals could potentially be prosecuted by the State of Victoria with a long list of criminal offences, including possession of firearms without a licence, possession of prohibited implements (including machine guns, silencers and housebreaking tools), aggravated burglary in possession of a firearm, common assault, wilful damage to property, possession of a disguise without lawful excuse and numerous motor vehicle offences. More than a year after the raid, the Victorian Director of Public Prosecutions concluded that while certain offences had been committed, including criminal damage and assault with a weapon, there was insufficient evidence to charge any person with a specific offence.\nVictorian Holdings Ltd, the company managing the hotel, subsequently took legal action against the Commonwealth on behalf of itself and 14 hotel staff. The matter was settled out of court; the hotel was offered $300,000 in damages. The total payout to the hotel and staff was $365,400.\nInvolvement in Papua New Guinea.\nBetween 1989 and 1991, ASIS came under scrutiny following allegations relating to its role and activities in Papua New Guinea. It was alleged that ASIS had been involved in training Papua New Guinean troops to suppress independence movements in Irian Jaya and Bougainville. In 1997 it was alleged that ASIS and DSD had failed to collect, or the Government had failed to act upon, intelligence regarding the role and presence of Sandline contractors in relation to the independence movement in Bougainville.\n\"Four Corners\" program.\nTowards the end of 1993, ASIS became the subject of media attention after allegations were made by former ASIS officers that ASIS was unaccountable and out of control. The Sunday Telegraph alleged that \"ASIS regularly flouted laws, kept dossiers on Australian citizens ... and hounded agents out of the service with little explanation\". In particular, it alleged that agents were being targeted in a purge by being threatened with criminal charges relating to their official conduct, reflecting a pattern which suggested to some that ASIS or a senior ASIS officer had been \"turned\" by a foreign intelligence service.\nOn 21 February 1994, \"Four Corners\" ran a program which aired the key allegations. Two former ASIS officers made claims regarding cultural and operational tensions between ASIS and the Department of Foreign Affairs and Trade. They claimed that embassy staff had maliciously or negligently compromised activities involving the running of foreign informants and agents and the defection of foreign agents to Australia. They claimed that their grievances had been ignored and that they were \"deserted in the field\" and made scapegoats by ASIS. The officers and the reporter, Ross Coulthart, also made claims regarding operational activities and priorities: the officers claimed that ASIS advice had been ignored by DFAT and the reporter repeated claims regarding ASIS operations aimed at destabilising the Aquino Government in the Philippines. He also made claims regarding ASIS assistance to MI6 in the Falklands War, in Hong Kong and in Kuwait for the benefit of British interests, including commercial interests, and potentially to the detriment of Australian national interests. The bulk of the personal statements by the officers concerned their private grievances. They raised two issues of public interest regarding the effect of secrecy on the operation of grievance procedures and the extent to which the Minister for Foreign Affairs and Trade was aware of or in control of ASIS operations. The reporter directly raised the issue of the appropriateness of ASIS operations, particularly with respect to priority setting in overseas postings and operations, cooperation with foreign intelligence services, and the privacy of Australian persons and organisations. By implication, the program queried the extent to which ASIS was or should be accountable to the Minister, to Government and to Parliament.\nThe following day, the Shadow Minister for Foreign Affairs called for an independent judicial inquiry into the allegations. He expressed particular concern about the nature of ASIS cooperation with foreign agencies and the defects in ASIS grievance procedures. He later called for the inquiry to examine the \"poisoned relationship between ASIS and DFAT\". A Democrats spokesperson called for a standing parliamentary committee.\nTwo days after the program aired, the Samuels and Codd Royal Commission was convened by Minister for Foreign Affairs Gareth Evans.\nRatih Hardjono, Bruce Grant and Gareth Evans.\nOn 19 February 2000, Singapore journalist Susan Sim accused Ratih Harjono of working for her uncle, a senior BAKIN (Indonesian intelligence service) intelligence officer while working for the President of Indonesia. Earlier in her career as a journalist, Ratih was married to Bruce Grant, who during this period was senior policy adviser to Gareth Evans, co-authoring the book, \"Australia's foreign relations: in the world of the 1990s\". Gareth Evans was responsible for ASIS from 1988 to 1996.\nAlleged management and staffing problems.\nIn 2005, \"The Bulletin\" ran an article based on allegations by serving ASIS officers that alluded to gross mismanagement of intelligence operations, staff assignments, and taskings, particularly with respect to the war on terrorism. The unnamed officers pointed out various problems within the agency that were plaguing the organisation's ability to collect vital and timely intelligence. By this, they meant the recruitment of \"...young mostly white university educated agents with limited language skills and little knowledge of Islam against poor, zealous extremists intent on becoming suicide bombers\", the \"inappropriate\" assignment of \"...young female IOs [intelligence officers] against Islamic targets...\", in addition to poor staff retention rates, and general lack of officers possessing significant practical field experience. The officers also cited a lack of proper support given to intelligence officers tasked against terrorist targets, and the doctoring of intelligence by ASIS management, as also contributing to the lack of progress of the agency in the war on terrorism.\nAustralia\u2013East Timor spying scandal.\nIt was revealed in 2013 that ASIS planted devices to listen to the East Timorese government during negotiations over the Greater Sunrise oil and gasfields.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;\nCredit.\nA large portion of the history of ASIS in this article was adapted from the Parliament of Australia https:// of Intelligence Services Act 2001."}
{"id": "37127", "revid": "48937253", "url": "https://en.wikipedia.org/wiki?curid=37127", "title": "Australian Signals Directorate", "text": "Australian signals intelligence agency\nThe Australian Signals Directorate (ASD), formerly the Defence Signals Directorate, is a statutory agency of the Government of Australia responsible for signals intelligence, providing intelligence support to Australian military operations, conducting cyberwarfare and ensuring information security. The ASD is a part of the larger Australian Intelligence Community, and its role within the so-called Five Eyes intelligence-sharing alliance is to monitor signals intelligence in South and East Asia. The Australian Cyber Security Centre (ACSC) is an agency within the ASD.\nThe unit was established in 1947 by executive order as the Defence Signals Bureau within the Department of Defence, and underwent several name changes until its current name ASD was adopted in 2013. ASD was converted to a statutory body by the \"Intelligence Services Act 2001\". ASD is based in Canberra, at the Defence Department Headquarters at Russell Offices. As of February 2020, Rachel Noble is the Director-General of ASD, replacing Mike Burgess, who was appointed Director-General of Security in September 2019.\nIn April 2018, a proposal to empower ASD to collect intelligence on Australians was backed by former Minister for Home Affairs Peter Dutton, but was strongly opposed by some in Cabinet who argued it was not necessary. Under legislation, the Australian Security Intelligence Organisation (ASIO) and the Australian Federal Police (AFP) are already allowed to seek assistance from ASD in conducting investigations on Australian citizens and businesses.\nHistory.\nThe Directorate has operated under a number of different names since its founding:\nASD commissioned an official history in 2019, which will cover the organisation's history from its establishment to 2001.\nRoles and responsibilities.\nThe principal functions of ASD are to collect and disseminate foreign signals intelligence (SIGINT) and to provide information security products and services to the Australian Government and Australian Defence Force (ADF), its foreign partners and militaries.\nASD operates at least three receiving stations: \nASD also maintains a workforce at Pine Gap in central Australia.\nADSCS and Shoal Bay are part of the United States signals intelligence and ECHELON analysis network. These stations also contribute signals intelligence for many Australian Government bodies, as well as the other UKUSA partners.\nElectronic warfare operators in the Royal Australian Corps of Signals work closely with ASD. 7 Signal Regiment (Electronic Warfare) at Borneo Barracks, Cabarlah, Queensland is also associated with ASD..\nIn addition, it has been reported that many Australian embassies and overseas missions also house small facilities which provide a flow of signals intelligence to ASD.\nUKUSA Agreement (Five Eyes).\nAustralia joined the UKUSA Agreement in 1948, a multilateral agreement for cooperation in signals intelligence between Australia, Canada, New Zealand, the United Kingdom, and the United States. The alliance is also known as the Five Eyes. Other countries, known as \"third parties\", such as West Germany, the Philippines, and several Nordic countries also joined the UKUSA community. As the Agreement was a secret treaty, its existence was not even disclosed to the Australian Prime Minister until 1973, when Gough Whitlam insisted on seeing it. The existence of the UKUSA Agreement was discovered by the Australian government during the 1973 Murphy raids on the headquarters of the Australian Security Intelligence Organisation (ASIO). After learning about the agreement, Whitlam discovered that Pine Gap, a secret surveillance station close to Alice Springs, Australia, had been operated by the U.S. Central Intelligence Agency (CIA). Pine Gap is now operated jointly by both Australia and the United States.\nThe existence of the Agreement was not disclosed to the public until 2005. On 25 June 2010, for the first time, the full text of the agreement was publicly released by the United Kingdom and the United States, and can now be http://. Under the agreement, ASD's intelligence is shared with UKUSA signals intelligence partner agencies: \nOrganisational structure.\nThe Australian Signals Directorate is led by a Director-General and a Principal Deputy Director-General who oversee strategy. The ASD also comprises the Australian Cyber Security Centre, a Signals Intelligence and Network Operations Group, and a Corporate and Capability Group.\nSIGINT and Network Operations Group.\nThe Signals Intelligence and Network Operations Group is responsible for signals intelligence collection, analysis and production, and ASD's network based access and effects operations. The Group comprises an Intelligence Division and a Network Operations and Access Division responsible for foreign signals intelligence and offensive cyber operations.\nDefence SIGINT and Cyber Command.\nThe Defence Signals-Intelligence (SIGINT) and Cyber Command (DSCC) was established in January 2018 by the Chief of the Defence Force consolidating all ADF personnel within the ASD within the Joint Cyber Unit and Joint SIGINT Unit. The Commander of the DSCC is responsible to the Head of Information Warfare under the Chief of Joint Capabilities to the Chief of the Defence Force.\nSee also.\n&lt;templatestyles src=\"Stack/styles.css\"/&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "37129", "revid": "40943757", "url": "https://en.wikipedia.org/wiki?curid=37129", "title": "Darkthrone", "text": "Norwegian black metal band\nDarkthrone is a Norwegian black metal band from Kolbotn, Akershus. Formed in 1986 as a death metal band named Black Death, in 1991, Darkthrone transitioned to a black metal style influenced by Bathory and Celtic Frost and emerged as one of the leading bands in the Norwegian black metal scene.\nIn 1991, the band released their debut studio album, \"Soulside Journey\", their only death metal release before transitioning to black metal. Their first three black metal albums, \"A Blaze in the Northern Sky\" (1992), \"Under a Funeral Moon\" (1993), and \"Transilvanian Hunger\" (1994), are frequently referred to as the \"Unholy Trinity\". These albums are regarded as the epitome of the band's career and among the most influential black metal releases.\nDarkthrone has been a duo consisting of Fenriz and Nocturno Culto since guitarist Zephyrous departed the band in 1993. While they have aimed to remain outside the mainstream music scene, in recent years they collaborated with the National Library of Norway for exhibitions focused on Norwegian cultural history and black metal. Since 2006, Darkthrone's music has diverged from traditional black metal, incorporating classic heavy metal, punk, and speed metal, while their more recent albums have also incorporated elements of doom metal.\nHistory.\nDeath metal years: 1986\u20131991.\nThe band that would eventually become Darkthrone formed in late 1986 in Kolbotn, a small town south of Oslo. Originally known as Black Death, they were a death metal band composed of Gylve Nagell, Ivar Enger, and Anders Risberget. Their primary inspirations included Autopsy, Venom, Hellhammer, Celtic Frost, Slayer, and Nocturnus. In late 1987, the band changed their name to Darkthrone and welcomed Dag Nilsen as a new member. Ted Skjellum joined in the spring of 1988, replacing Risberget. During 1988 and 1989, the band independently released four demo tapes: \"Land of Frost\", \"A New Dimension\", \"Thulcandra\", and \"Cromlech\".\nSubsequently, they signed a four-album contract with independent record label Peaceville Records. In 1990, they recorded their debut studio album, \"Soulside Journey\". Due to a limited budget, the band was unable to secure the studio they wanted. However, thanks to support from members of Nihilist and Entombed, they were able to record their album at Sunlight Studios. Although primarily a death metal record, it featured elements of black metal in terms of artwork and songwriting.\nFollowing the release of this album, the band continued writing and recording new material, working on tape until a complete album was ready. These tracks were entirely instrumental, showcasing the band's gradual shift toward black metal. In 1996, the completed album \"Goatlord\" was released, featuring vocals added by Fenriz.\nBlack metal years.\nEarly black metal years: 1991\u20131994.\nIn 1991, influenced by Euronymous of Mayhem, Darkthrone adopted the aesthetic that would come to symbolize the black metal scene, donning corpse paint and using pseudonyms. Gylve Nagell became \"Fenriz\", Ted Skjellum became \"Nocturno Culto\", and Ivar Enger adopted the name \"Zephyrous.\" In August 1991, they recorded their second album, released in early 1992 and titled \"A Blaze in the Northern Sky\". This album featured Darkthrone's first black metal recordings, although Peaceville Records was initially skeptical about releasing it due to the band's drastic shift from their original death metal style. After the album was recorded, bassist Dag Nilsen left the band, stating that he did not wish to play black metal. He is credited only as \"session bass\" and does not appear in any photographs on the album.\nThe band's third album, \"Under a Funeral Moon\", was recorded in the summer of 1992 and released in early 1993. It marked Darkthrone's complete transition to black metal and is considered a landmark in the development of the genre. This album was the last collaboration with guitarist Zephyrous.\nFollowing this, their fourth album, \"Transilvanian Hunger\", was released in February 1994. It was the first album to feature only two members, Nocturno Culto and Fenriz. Fenriz was credited with all instrumentation and songwriting, while Nocturno Culto contributed solely vocals. From this point forward, the band operated as a duo. \"Transilvanian Hunger\" was characterized by a raw, low-fidelity recording style, monotone riffing, and minimal melody. The album's release sparked some controversy, as Norwegian black metal musician Varg Vikernes wrote half the lyrics and the booklet contained the phrase \"Norsk Arisk Black Metal,\" which translates to \"Norwegian Aryan Black Metal\" in English.\nWith Moonfog Records: 1995\u20132004.\nDarkthrone transitioned to another independent label, Moonfog Productions, for their subsequent releases. It was run by Satyr of Satyricon.\nTheir fifth album, \"Panzerfaust\", was released in 1995. Its production resembled that of \"Transilvanian Hunger\", with Fenriz credited for all instrumentation and songwriting, while Nocturno Culto contributed solely vocals. Vikernes wrote the lyrics for \"Quintessence\". Their sixth album, \"Total Death\", was released in 1996 and is notable for featuring lyrics penned by four other black metal musicians, with none written by the band's primary lyricist, Fenriz.\nDuring the years 1993\u20131995, Fenriz was involved in numerous side projects. This included his solo dark ambient project Neptune Towers, his solo folk black metal project Isengard, recording an album with Satyr as part of the trio Storm, and playing bass on D\u00f8dheimsgard's debut album. Additionally, he resumed playing drums for Valhall, which he co-founded in 1988 before leaving in 1990 to focus on Darkthrone.\nOn 6 April 1996, Easter Eve, Darkthrone played their last show at \"A Night of Unholy Black Metal\" in a sold-out Rockefeller in Oslo.\nIn 1999, Darkthrone released the album \"Ravishing Grimness\", followed by \"Plaguewielder\" in 2001. While \"Transilvanian Hunger\" and \"Panzerfaust\" featured songs exclusively written by Fenriz, these two albums contained tracks predominantly composed by Nocturno Culto and were both recorded at Ronny Le Tekr\u00f8e's studio in Toten, Norway. This accounts for the somewhat \"clearer\" sound on those records.\nIn the latter part of the 1990s, two tribute albums dedicated to Darkthrone were released: \"Darkthrone Holy Darkthrone\" in 1998 and \"The Next Thousand Years Are Ours\" in 1999. The band also released \"Preparing for War\", a compilation featuring songs from 1988 to 1994. In 2002, the intro of their song \"Kathaarian Life Code\" was included in the final scene of the film \"Demonlover\".\nIn 2003, the band released the album \"Hate Them\". Although this record and the following one incorporate electronic introductions, they remain faithful to Darkthrone's early black metal style. \"Sardonic Wrath\" was released in 2004, the band's last album with Moonfog Productions and their final work recorded exclusively in the black metal style. This album received a nomination for Norway's Alarm Awards, but the entry was withdrawn at the band's request. Their subsequent releases exhibited strong crust punk influences.\nPunk-influenced years: 2005\u20132010.\nIn 2005, Darkthrone announced their return to Peaceville Records after departing from the label in 1994. They also launched their own record label, Tyrant Syndicate Productions, to release their future albums. To commemorate their return, Peaceville reissued the \"Preparing for War\" compilation, which included a bonus CD of demos and a DVD of live performances. Additionally, Darkthrone's first four albums were re-released with video interviews about each.\nIn January 2006, the group released the EP \"Too Old, Too Cold\", featuring the track \"High on Cold War,\" performed by Enslaved's vocalist Grutle Kjellson. The EP also included a cover of \"Love in a Void\" by Siouxsie and the Banshees. For the first time in their career, the band produced a music video for the EP's title track. \"Too Old, Too Cold\" marked Darkthrone's first record to chart, reaching the top 15 of the best-selling singles in Norway and Denmark. That same year, Darkthrone released their eleventh album, \"The Cult Is Alive\". This album marked a stylistic shift, incorporating crust punk elements. While Darkthrone's black metal roots remained evident, their departure from the genre's typical sound was increasingly pronounced. \"The Cult Is Alive\" was the first Darkthrone album to enter the album chart in Norway, debuting at number 22.\nIn July 2007, the band released the EP \"NWOBHM\" (an acronym for 'New Wave of Black Heavy Metal', a play on the original 'New Wave of British Heavy Metal') as a preview of their next album. In September of that year, Darkthrone released \"F.O.A.D.\" (an acronym for \"Fuck Off and Die\"). This phrase was frequently employed by numerous thrash metal and punk bands during the 1980s. While the music continued the punk-oriented style introduced on \"The Cult Is Alive\", the band placed greater emphasis on traditional heavy metal.\nAlso in 2007, Nocturno Culto completed and released \"The Misanthrope\", a film exploring black metal and life in Norway. The film included some of his own solo recordings. In October 2008, \"Dark Thrones and Black Flags\" was released, following a style similar to that of the previous album. In 2010, the band released \"Circle the Wagons\", which showcased significantly fewer crust punk elements in favor of strong speed metal and traditional heavy metal characteristics.\n2010\u2013present.\nIn late 2010, Peaceville acquired the rights to the band's Moonfog albums and reissued \"Panzerfaust\" as a two-disc set and on vinyl. The reissue of \"Total Death\" was scheduled for 14 March 2011. In July 2012, Darkthrone announced a new album titled \"The Underground Resistance\", which was released on 25 February 2013. This album marked a complete departure from black metal and blackened crust, returning to classic heavy metal and speed metal. The band released their 16th studio album, \"Arctic Thunder\", on 14 October 2016. This album represented another musical shift, featuring a rawer, more blackened sound reminiscent of their 1990s output, but with the classic metal influences of the previous record.\nOn 22 October 2016, the band announced via Facebook that they would release a compilation album entitled \"The Wind of 666 Black Hearts\". Released on 25 November 2016, the album comprised rehearsals recorded in 1991 and 1992 for songs that later appeared on \"A Blaze in the Northern Sky\" and \"Under a Funeral Moon\".\nIn March 2019, Darkthrone announced the release of their seventeenth studio album, \"Old Star\", which came out on 31 May of the same year. The album featured a stronger emphasis on doom metal than previous releases, with their Candlemass influences becoming more apparent. In January 2021, the band announced they had completed the recording of a new album. In April 2021, a box set containing early and rare material titled \"Shadows of Iconoclasm\" was unveiled. The band's eighteenth studio album, \"Eternal Hails...\", was released on 25 June 2021, through Peaceville Records on physical media and digital platforms. The album continued the band's incorporation of traditional doom metal, heavily influenced by Candlemass, alongside inspiration from other bands such as Trouble and Black Sabbath. The band's nineteenth album, \"Astral Fortress\", was released on 28 October 2022. The band's twentieth album, \"It Beckons Us All...\", was released on 26 April 2024. It was recorded in April and May 2023 at Chaka Khan Studio in Oslo.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "37131", "revid": "29417873", "url": "https://en.wikipedia.org/wiki?curid=37131", "title": "Burzum", "text": "Norwegian black metal project\nBurzum (; ) is a Norwegian music project founded by former Mayhem member Varg Vikernes in 1991. Although Burzum never played live performances, it became a staple of the early Norwegian black metal scene and is considered one of the most influential acts in black metal's history. The word \"burzum\" means \"darkness\" in the Black Speech, a fictional language crafted by \"The Lord of the Rings\" writer J. R. R. Tolkien. Burzum's lyrics and imagery are often inspired by fantasy and Norse mythology.\nVikernes founded Burzum in 1991 and recorded the first four Burzum albums between January 1992 and March 1993. From 1994 to 2009, Vikernes was imprisoned for the murder of Mayhem guitarist \u00d8ystein \"Euronymous\" Aarseth and the arson of eight churches. While imprisoned, he recorded two dark ambient albums using only synthesizers, as he had no access to drums, guitar, or bass.\nSince his release from prison in 2009, he has recorded several more albums, including those in the dark ambient and neofolk genres. Vikernes ended the Burzum project in 2020 before announcing a new album in 2024.\nHistory.\nEarly years (1988\u20131992).\nVarg Vikernes began making music in 1988 with the band Kalashnikov. The following year, the name was changed to Uruk-Hai, after the creatures from J. R. R. Tolkien's \"The Lord of the Rings\". In 1990 and 1991, Vikernes played guitar for the death metal band Old Funeral, which also consisted of members who would later form the band Immortal. He appears on the Old Funeral EP \"Devoured Carcass\". Vikernes left Old Funeral in 1991 to concentrate on creating his own musical visions. He had a short-lived project called Satanel, along with Abbath Doom Occulta. He then began a solo project under the name Burzum. The word \"burzum\" means \"darkness\" in the Black Speech, a language crafted by Tolkien. Soon after recording two demo tapes, he became part of the Norwegian black metal scene. With his demo tapes, he had attracted attention from \u00d8ystein \"Euronymous\" Aarseth of Mayhem, who had just recently formed Deathlike Silence Productions. Aarseth then signed Burzum to the label, and shortly after, Vikernes\u2015under the pseudonym of Count Grishnackh (another name adapted from \"The Lord of the Rings\")\u2015began to record Burzum's self-titled debut album. According to Vikernes' autobiography on his website, he had intended to record the album in the worst recording quality possible (due to this being a typical trademark of the early Norwegian black metal scene), while still making it sound acceptable. Burzum's eponymous debut album was released in 1992, being the second album released on Deathlike Silence Productions. The song \"War\" from this album had a guest appearance from Euronymous, playing a guitar solo \"just for fun\", according to Vikernes.\nVikernes has stated that he had never played any live shows with Burzum, though at one point was interested in it, so Samoth of Emperor accompanied him as a session bassist, though only appearing on the \"Aske\" EP. Additionally, Erik Lancelot was hired to be the band's drummer, though did not record on any Burzum material, and along with Samoth did not play a live show. Vikernes had by then lost his interest in playing live concerts, and stated that he \"didn't even need session musicians anymore\". Therefore, Samoth and Lancelot had parted ways with Burzum. \"Det som engang var\" was released as Burzum's second album in 1993, recorded in 1992.\nImprisonment (1993\u20132009).\nMay 1994 saw the release of \"Hvis lyset tar oss\", a new album of previously recorded material from 1992. Burzum remained as a solo project until 1994, when Vikernes was arrested for the murder of Euronymous and the burnings of several churches in Norway. During his time in prison, Vikernes released his next album, titled \"Filosofem\", on 1 January 1996. Recorded in March 1993, \"Filosofem\" was the last recording Vikernes made before his imprisonment. \"Burzum / Aske\", a compilation comprising the \"Burzum\" album and \"Aske\" EP, was released in 1995. While imprisoned, Vikernes managed to record two other albums in a dark ambient style. They were released as \"Dau\u00f0i Baldrs\" (1997) and \"Hli\u00f0skj\u00e1lf\" (1999). Both of these albums were created with a synthesizer, as Vikernes was prohibited from using any other instruments in prison.\nIn 1998, all Burzum albums released up to that point were re-released as vinyl picture discs in a special box set called \"1992\u20131997\"; however \"Filosofem\" did not contain \"Rundtg\u00e5ing av den transcendentale egenhetens st\u00f8tte\" due to its length. The regular vinyl issue of \"Filosofem\" on Misanthropy was a double album, containing tracks 1\u20133 on the A-side, the \"Decrepitude\" tracks on side 2 and \"Rundtg\u00e5ing av den transcendentale egenhetens st\u00f8tte\" on side 3, with an etched D-side.\nPost-imprisonment and retirement (2009\u20132020).\n Soon after being released, Vikernes started writing new tracks (nine metal tracks and an ambient intro and outro) for an upcoming Burzum album. According to Vikernes' recounts, several record companies were interested in releasing his first album in eleven years. He stated about the new album, \"I want to take my time, and make it the way I want it. It will be metal, and the fans can expect genuine Burzum.\"\nThe album was going to be originally titled \"Den hvite guden\" (\"The White God\"), but he later decided to change it to \"Belus\", which was released by the independent record label Byelobog Productions (\"byelobog\" is the transliteration of \"\u0431\u0435\u043b\u043e\u0431\u043e\u0433\" in Slavic languages, meaning 'white god', Belobog) on 8 March 2010. It was also announced that a movie would be released in 2010, based on Varg Vikernes' life in the early 1990s. The movie would mainly draw inspiration from the book \"Lords of Chaos\", with the film being of the same name. Vikernes expressed his contempt towards both the movie and the book upon which it is based.\nA second new album of original Burzum material, \"Fallen\", was released on 7 March 2011, followed by a compilation album, \"From the Depths of Darkness\", containing re-recordings of tracks from Burzum's self-titled album and \"Det som engang var\", on 28 November 2011. A third new studio album of original material, titled \"Umskiptar\", was released in May 2012. \"S\u00f4l austan, M\u00e2ni vestan\" (\"East of the Sun, West of the Moon\"), Burzum's first electronic album since 1999, was released in May 2013. On 27 April 2013, a song was posted on the official YouTube channel of Vikernes, titled \"Back to the Shadows\". In a blog post, Vikernes stated that \"Back to the Shadows\" would be the last metal track released by Burzum. Another album, \"The Ways of Yore,\" was released in June 2014.\nAfter a period of inactivity, Vikernes posted a video on his YouTube account in June 2018 announcing that \"[he had] moved on [from Burzum]\", saying \"bye bye\" to the project.\nIn July 2018, a YouTube user named Hermann posted unreleased materials of Uruk-Hai from 1988 to 1990 and Burzum's Bergen prison recordings from 1994, which he received from Tiziana Stupia.\nIn October 2019, Vikernes posted a tweet saying he intended to release another album as Burzum. He announced that the tentative name of the album would be \"Thul\u00eaan Mysteries\", which would have 23 songs. The tracks from the album were previously used as background music on Vikernes' YouTube channel, which was taken down the same year. Vikernes also said that the music of \"Thul\u00eaan Mysteries\" is meant to be used as a background soundtrack for his MYFAROG role-playing game. On 18 December, Vikernes tweeted the album cover for \"Thul\u00eaan Mysteries\" and announced its release date as 13 March 2020. Vikernes has stated it would be Burzum's last album at the time.\n\"Burzum NEW\" (2023\u20132024, 2025\u2013present).\nOn 8 September 2023, Vikernes released a new song titled \"The Reincarnation of \u00d3dinn\". In April 2024, Vikernes announced the return of the project on his X account, with a new album titled \"The Land of Thul\u00ea\". It is the first metal album since \"Umskiptar\". From late April to May 2024 Vikernes released numerous singles from the upcoming album under the name \"Burzum (NEW)\". Vikernes claimed on his X account that he is only releasing the music as \"Burzum (NEW)\" because he does not control the Burzum accounts on major streaming services, and that the album will be released as \"Burzum\" when complete. However, the album art Vikernes used for the released singles all list the project name as \"Burzum (NEW)\". On November 4, 2024, Vikernes announced the end of the project on his X account saying \"I stopped making music (again), because I appreciate a lot more to work with my books, and especially #ReconQuest. And I don't have the time for it all.\". On April 24, 2025, Vikernes released his first song in nearly a year with no prior information about it titled \"What Will Come\". This was followed by three new singles released in May.\nMusical style.\nBurzum's music includes both straightforward black metal as well as dark ambient, neofolk and neo-medieval music. It is often minimalist and dark, with repetition and simple song structures. Vikernes has described Burzum as a kind of \"spell\" or recreation of an imaginary world tied in with Pagan history. Each album, he claims, was designed as a kind of \"spell\" in itself, with each beginning song intending to make the listener more susceptible to \"magic\", the following songs to inspire a \"trance-like state of mind\", and the last song to carry the listener into a \"world of fantasy\" (dreams, for the listener would fall asleep\u2014Burzum was supposed to have been evening music). Vikernes claims the intent to create this fantasy world came from dissatisfaction with the real world. He has stated the \"message\" of Burzum can be found in the lyrics of the first song of the first album (\"Feeble Screams from Forests Unknown\").\nBurzum's lyrics and imagery are often inspired by fantasy and Norse mythology, and do not feature the far-right political views Vikernes is known for. In a 2010 interview, Vikernes said: \"Burzum is not a political or religious band, or even an anti-religious band. Burzum is music; art if you like, and the interpretation of art lies in the eye of the beholder. I might be Nordic, heterosexual and have a Pagan ideology myself, but why would I expect the fans of my music to be just like me?\". Despite the project's apolitical nature, Burzum is included on Meta's Dangerous Individuals and Organizations list.\nBurzum's early work was influenced by Tolkien; for example, Vikernes' early moniker \"Count Grishnackh\" is taken from an orc character called Grishn\u00e1kh in Tolkien's works. \"Burzum\" is a word of the Black Speech of Mordor meaning \"darkness\" (though Vikernes views what Christians consider \"darkness\" as \"light\"), and is inscribed on the One Ring in Tolkien's \"The Lord of the Rings\".\nInfluences and legacy.\nVikernes said that the German thrash metal band Destruction \"changed my approach towards playing my instrument\" despite having only discovered them in 1991. In a 1996 interview with \"Terrorizer\", Vikernes also named Bathory's \"Blood Fire Death\" and Celtic Frost's \"Morbid Tales\" as significant influences on Burzum. He credited Dead Can Dance's \"Within the Realm of a Dying Sun\" for being one of Burzum's first non-metal influences. Other non-metal artists he has cited as influence include the Cure, Depeche Mode and New Order. In 2011, Vikernes said that he no longer kept up with new black metal bands and listened mostly to the Cure instead.\nBurzum is widely considered to be the most influential act in black metal history. Bands that have listed Burzum as an influence include Deafheaven, Liturgy, Wolves in the Throne Room and Altar of Plagues. The project has also inspired musicians from other genres, including Chelsea Wolfe, Mount Eerie, Thurston Moore, and Vision Eternel.\nDiscography.\nMain releases\nEPs\nSingles\nDemos and promos\nMusic videos\nCompilation albums\nBox sets\nOther appearances\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "37135", "revid": "48986655", "url": "https://en.wikipedia.org/wiki?curid=37135", "title": "Barbecue", "text": "Cooking method and apparatus\nBarbecue or barbeque (often shortened to BBQ worldwide; barbie or barby in Australia and New Zealand) is a term used with significant regional and national variations to describe various cooking methods that employ live fire and smoke to cook food. The term is also generally applied to the devices associated with those methods, the broader cuisines that these methods produce, and the meals or gatherings at which this style of food is cooked and served. The cooking methods associated with barbecuing vary significantly.\nThe various regional variations of barbecue can be broadly categorized into those methods which use direct and those which use indirect heating. Indirect barbecues are associated with US cuisine, in which meat is heated by roasting or smoking over wood or charcoal. These methods of barbecue involve cooking using smoke at low temperatures with long cooking times, for several hours. Elsewhere, barbecuing more commonly refers to the more direct application of heat, grilling of food over hot coals or a gas fire. This technique is usually done over direct, dry heat or a hot fire for a few minutes. Within these broader categorizations are further national and regional differences.\nEtymology and spelling.\nThe English word \"barbecue\" and its cognates in other languages come from the Spanish word \"barbacoa\", which has its origin in an indigenous American word. Etymologists believe this to be derived from \"barabicu\" found in the language of the Arawak people of the Caribbean and the Timucua people of Florida; it has entered some European languages in the form of \"barbacoa\". The \"Oxford English Dictionary\" (OED) traces the word to Hispaniola and translates it as a \"framework of sticks set upon posts\". \nA popular folk etymology of the word says that the term is derived from the French (\"from beard to tail\") signifying a whole animal being roasted on a spit, but this origin for the word is not supported by academic etymology.\nThe term itself has two spellings in English: \"barbecue\" and \"barbeque\". While in most countries the spelling \"barbecue\" is used, the spelling \"barbeque\" is occasionally used in Australia, New Zealand, and the US.\nHistory.\nSpanish explorer Gonzalo Fern\u00e1ndez De Oviedo y Vald\u00e9s was the first to use the word \"barbecoa\" in print in Spain in 1526 in the \"Diccionario de la Lengua Espa\u00f1ola (2nd Edition) of the Real Academia Espa\u00f1ola\". After Columbus landed in the Americas in 1492, the Spaniards apparently found Ta\u00edno roasting meat over a grill consisting of a wooden framework resting on sticks above a fire. This framework was also used to store food above ground and for sleeping. The flames and smoke rose and enveloped the meat, giving it a certain flavor. Spaniards called the framework a barbacoa.\nAnother form of \"barbacoa\" involves digging a hole in the ground, burning logs in it and placing stones in it to absorb and retain heat. Large cuts of meat, often wrapped in leaves, often a whole goat or lamb, are placed above a pot so the juices can be used to make a broth. It is then covered with maguey leaves and coal, and set alight. The cooking process takes a few hours. Olaudah Equiano, an African abolitionist, described this method of roasting alligators among the \"Mosquito people\" (Miskito people) on his journeys to Cabo Gracias a Dios on the Mosquito Coast, in his narrative \"The Interesting Narrative of the Life of Olaudah Equiano\".\nLinguists have suggested the word was loaned successively into Spanish, then Portuguese, French, and English. In the form \"barbacado\", the word was used in English in 1648 by the supposed Beauchamp Plantagenet in the tract \"A description of the province of New Albion\": \"the Indians in stead of salt doe barbecado or dry and smoak fish\".\nAccording to the \"Oxford English Dictionary\", the first recorded use in modern form was in 1661, in Edmund Hickeringill's \"Jamaica Viewed\": \"Some are slain, And their flesh forthwith Barbacu'd and eat\"; it also appears in 1672 in the writings of John Lederer following his travels in the North American southeast in 1669\u20131670. \nThe first known use as a noun was in 1697 by the English buccaneer William Dampier. In his \"New Voyage Round the World\", Dampier wrote, \"and lay there all night, upon our Borbecu's, or frames of Sticks, raised about 3 foot [] from the Ground\".\nAs early as the 1730s, New England Puritans were familiar with barbecue, as on 4 November 1731, New London, Connecticut, resident Joshua Hempstead wrote in his diary: \"I was at Madm Winthrops at an Entertainment, or Treat of Colln [Colonel] or Samll Brownes a Barbaqued.\" Samuel Johnson's 1755 dictionary gave the following definitions:\nWhile the standard modern English spelling of the word is \"barbecue\", variations including \"barbeque\" and truncations such as \"bar-b-q\" or \"BBQ\" may also be found. The spelling \"barbeque\" is given in Merriam-Webster as a variant, whereas the Oxford Dictionaries explain that it is a misspelling which is not accepted in standard English and is best avoided. In the Southeastern United States, the word barbecue is used predominantly as a noun referring to roast pork, while in the Southwestern states cuts of beef are often cooked.\nAssociations.\nBecause the word \"barbecue\" came from native groups, Europeans gave it \"savage connotations\". This association with barbarians and \"savages\" is strengthened by Edmund Hickeringill's work \"Jamaica Viewed: with All the Ports, Harbours, and their Several Soundings, Towns, and Settlements\" through its descriptions of cannibalism. However, according to Andrew Warnes, there is very little proof that Hickeringill's tale of cannibalism in the Caribbean is even remotely true. Another notable false depiction of cannibalistic barbecues appears in Theodor de Bry's \"Great Voyages\", which in Warnes's eyes, \"present smoke cookery as a custom quintessential to an underlying savagery [...] that everywhere contains within it a potential for cannibalistic violence\". Today, people in the US associate barbecue with \"classic Americana\".\nStyles.\nIn American English usage, grilling refers to a fast process over high heat while barbecuing usually refers to a slow process using indirect heat or hot smoke, similar to some forms of roasting. In a typical US home grill, food is cooked on a grate directly over hot charcoal, while in a US barbecue the coals are dispersed to the sides or at a significant distance from the grate. In British usage, barbecueing refers to a fast cooking process done directly over high heat, while grilling refers to cooking under a source of direct, moderate-to-high heat\u2014known in the United States as broiling. Its South American versions are the southern Brazilian churrasco and the Southern Cone asado.\nFor barbecue in the United States, each Southern locale has its own variety of barbecue, particularly sauces. In recent years, the regional variations have blurred as restaurants and consumers experiment and adapt the styles of other regions. South Carolina is the only state that traditionally features all four recognized barbecue sauces, including mustard-based, vinegar-based, and light and heavy tomato-based sauces. North Carolina sauces vary by region; eastern North Carolina uses a vinegar-based sauce, the center of the state uses Lexington-style barbecue, with a combination of ketchup and vinegar as its base, and western North Carolina uses a heavier ketchup base. Memphis barbecue is best known for tomato- and vinegar-based sauces. In some Memphis establishments and in Kentucky, meat is rubbed with dry seasoning (dry rubs) and smoked over hickory wood without sauce. The finished barbecue is then served with barbecue sauce on the side. Kansas City barbecue is barbecue that originated in Kansas City, Missouri in the early 20th century. It is known for slow-smoked meats (including pork, beef, chicken, turkey, lamb, sausage, and sometimes fish) cooked over various woods, seasoned with a dry rub, and served with a thick, sweet, tomato-based sauce made from brown sugar, molasses, and tomatoes. St. Louis\u2013style barbecue refers to spare ribs associated with the St. Louis area. These are usually grilled rather than slow-cooked over indirect heat with smoke which is typically associated with the term \"barbecue\" in the United States. Although St. Louis\u2013style barbecue takes inspiration from other styles of barbecue it still retains its own distinct style. St. Louis style barbecue is known for its distinctive approach to ribs and sauce. The hallmark is the St. Louis\u2013style spare rib cut, which is a rectangular, meaty cut with excess cartilage trimmed off for a uniform appearance and more meat compared to baby back ribs\nIn South Africa, braais are informal gatherings of people who convene around an open fire for any occasion and at any location with a grill. They are linked to the consistent warm weather of South Africa that leads to much communal, outdoor activity. The act of convening around a grill is reminiscent of past generations gathering around open fires after a hunt, solidifying the braais' importance to tradition. Modernity has expanded grilling to the use of gas grills, but steel grill grates and campfires are often used. The use of a gas grill is frowned upon and the use of charcoal is accepted, but wood is seen as the best method to cook the meat.\nIt is expected that people attending a braai bring snacks, drinks, and other meat to eat until the main meal has finished cooking on the grill. This potluck-like activity is known as \"bring and braai\". Cooking on the braai is a bonding experience for fathers and sons, while women prepare salads and other side dishes in kitchens or other areas away from the grill. Examples of meat prepared for a braai are lamb, steaks, spare ribs, sausages, chicken, and fish. Mielie pap, also known as \"Krummel pap\", is a crumbled cornmeal that is often served as a side dish.\nTechniques.\nBarbecuing encompasses multiple types of cooking techniques. The original technique is cooking using smoke at low temperatures\u2014usually around \u2014and significantly longer cooking times (several hours), known as smoking.\nGrilling is done over direct, dry heat, usually over a hot fire over for a few minutes. Grilling and smoking are done with wood, charcoal, gas, electricity, or pellets. The time difference between smoking and grilling is because of the temperature difference; at low temperatures used for smoking, meat takes several hours to reach the desired internal temperature.\nSmoking.\nSmoking is the process of flavoring, cooking, and/or preserving food by exposing it to smoke from burning or smoldering material, most often wood. Meat and fish are the most common smoked foods, though cheeses, vegetables, nuts, and ingredients used to make beverages such as beer or smoked beer are also smoked.\nGrilling.\nGrilling is a form of cooking that involves a dry heat applied to the food, either from above or below. Grilling is an effective technique in order to cook meat or vegetables quickly since it involves a significant amount of direct, radiant heat. Outside of the US, this is the most common technique when cooking classic barbecue foods, although some variants of grilling require direct, but moderate heat.\nThe words \"barbecue\" and \"grilling\" are often used interchangeably, although some argue that barbecue is a type of grilling, and that grilling involves the use of a higher level of heat to sear the food, while barbecuing is a slower process over a low heat.\nIn practice, the lines blur because it is hard to define what is low temperature and what is high temperature and because many champion barbecue cooks now cook meats such as beef brisket at higher temperatures than was traditional.\nOther uses.\nThe term \"barbecue\" is also used to designate a flavor added to food items, the most prominent of which are potato chips.\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "37137", "revid": "45925219", "url": "https://en.wikipedia.org/wiki?curid=37137", "title": "Arabidopsis", "text": "Genus of flowering plants\nArabidopsis (rockcress) is a genus of small flowering plants in the cabbage and mustard family, Brassicaceae. \"Arabidopsis\" species are native to temperate and subarctic Eurasia and North America, North Africa, and the mountains of eastern tropical Africa. This genus is of great interest since it contains thale cress (\"Arabidopsis thaliana\"), one of the model organisms used for studying plant biology and the first plant to have its entire genome sequenced. Changes in thale cress are easily observed, making it a very useful model.\nStatus.\n12 species of \"Arabidopsis\" are currently accepted, and a further eight subspecies recognised. This delimitation is quite recent and is based on morphological and molecular phylogenies by O'Kane and Al-Shehbaz and others.\nTheir findings confirm the species formerly included in \"Arabidopsis\" made it polyphyletic. The most recent reclassification moves two species previously placed in \"Cardaminopsis\" and \"Hylandra\" and three species of \"Arabis\" into \"Arabidopsis\", but excludes 50 that have been moved into the new genera \"Beringia, Crucihimalaya, Ianhedgea, Olimarabidopsis\", and \"Pseudoarabidopsis\".\nIn the last two decades, \"Arabidopsis thaliana\" has gained much interest from the scientific community as a model organism for research on numerous aspects of plant biology. The Arabidopsis Information Resource (TAIR) is a curated online information source for \"Arabidopsis thaliana\" genetic and molecular biology research, and The Arabidopsis Book is an online compilation of invited chapters on \"Arabidopsis thaliana\" biology. (Note that as of 2013 no further chapters will be published.) In Europe, the model organism resource centre for \"Arabidopsis thaliana\" germplasm, bioinformatics and molecular biology resources (including GeneChips) is the Nottingham Arabidopsis Stock Centre (NASC) whilst in North America germplasm services are provided by the Arabidopsis Biological Resource Center (ABRC) based at Ohio State University. The ordering system for ABRC was incorporated into the TAIR database in June 2001 whilst NASC has always (since 1991) hosted its own ordering system and genome browser.\nIn 1982, the crew of the Soviet Salyut 7 space station grew some Arabidopsis, thus becoming the first plants to flower and produce seeds in space. They had a life span of 40 days. \"Arabidopsis thaliana\" seeds were taken to the Moon on the Chang'e 4 lander in 2019, as part of a student experiment. As of May 2022 \"Arabidopsis thaliana\" has successfully been grown in samples of lunar soil.\n\"Arabidopsis\" is quite similar to the \"Boechera\" genus.\nList of species and subspecies.\n12 species are accepted.\nReclassified species.\nThe following species previously placed in \"Arabidopsis\" are not currently considered part of the genus.\nCytogenetics.\nCytogenetic analysis has shown the haploid chromosome number (n) is variable and varies across species in the genus:\n\"A. thaliana\" is n=5 and the DNA sequencing of this species was completed in 2001. \"A. lyrata\" has n=8 but some subspecies or populations are tetraploid. Various subspecies \"A. arenosa\" have n=8 but can be either 2n (diploid) or 4n (tetraploid).\n\"A. suecica\" is n=13 (5+8) and is an amphidiploid species originated through hybridization between \"A. thaliana\" and diploid \"A. arenosa\".\n\"A. neglecta\" is n=8, as are the various subspecies of \"A. halleri\".\nAs of 2005, \"A. cebennensis\", \"A. croatica\" and \"A. pedemontana\" have not been investigated cytologically.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "37138", "revid": "50525440", "url": "https://en.wikipedia.org/wiki?curid=37138", "title": "Arabidopsis thaliana", "text": "Model plant species in the family Brassicaceae\n&lt;templatestyles src=\"Template:Taxobox/core/styles.css\" /&gt;\nArabidopsis thaliana, the thale cress, mouse-ear cress or arabidopsis, is a small plant from the mustard family (Brassicaceae), native to Eurasia and Africa. Commonly found along the shoulders of roads and in disturbed land, it is generally considered a weed.\nA winter annual with a relatively short lifecycle, \"A. thaliana\" is a popular model organism in plant biology and genetics. For a complex multicellular eukaryote, \"A. thaliana\" has a relatively small genome of around 135 megabase pairs. It was the first plant to have its genome sequenced, and is an important tool for understanding the molecular biology of many plant traits, including flower development and light sensing.\nDescription.\n\"Arabidopsis thaliana\" is an annual (rarely biennial) plant, usually growing to 20\u201325\u00a0cm tall. The leaves form a rosette at the base of the plant, with a few leaves also on the flowering stem. The basal leaves are green to slightly purplish in color, 1.5\u20135\u00a0cm long, and 2\u201310\u00a0mm broad, with an entire to coarsely serrated margin; the stem leaves are smaller and unstalked, usually with an entire margin. Leaves are covered with small, unicellular hairs called trichomes. The flowers are 3\u00a0mm in diameter, arranged in a corymb; their structure is that of the typical Brassicaceae. The fruit is a silique 5\u201320\u00a0mm long, containing 20\u201330 seeds. Roots are simple in structure, with a single primary root that grows vertically downward, later producing smaller lateral roots. These roots form interactions with rhizosphere bacteria such as \"Bacillus megaterium\".\n\"A. thaliana\" can complete its entire lifecycle in six weeks. The central stem that produces flowers grows after about 3 weeks, and the flowers naturally self-pollinate. In the lab, \"A. thaliana\" may be grown in Petri plates, pots, or hydroponics, under fluorescent lights or in a greenhouse.\nTaxonomy.\nThe plant was first described in 1577 in the Harz Mountains by Johannes Thal (1542\u20131583), a physician from Nordhausen, Th\u00fcringen, Germany, who called it \"Pilosella siliquosa\". In 1753, Carl Linnaeus renamed the plant \"Arabis thaliana\" in honor of Thal. In 1842, German botanist Gustav Heynhold erected the new genus \"Arabidopsis\" and placed the plant in that genus. The generic name, \"Arabidopsis\", comes from Greek, meaning \"resembling \"Arabis\"\" (the genus in which Linnaeus had initially placed it).\nThousands of natural inbred accessions of \"A. thaliana\" have been collected from throughout its natural and introduced range. These accessions exhibit considerable genetic and phenotypic variation, which can be used to study the adaptation of this species to different environments.\nDistribution and habitat.\n\"A. thaliana\" is native to Europe, Asia, and Africa, and its geographic distribution is rather continuous from the Mediterranean to Scandinavia and Spain to Greece. It also appears to be native in tropical alpine ecosystems in Africa and perhaps South Africa. It has been introduced and naturalized worldwide, including in North America around the 17th century.\n\"A. thaliana\" readily grows and often pioneers rocky, sandy, and calcareous soils. It is generally considered a weed, due to its widespread distribution in agricultural fields, roadsides, railway lines, waste ground, and other disturbed habitats, but due to its limited competitive ability and small size, it is not categorized as a noxious weed. Like most Brassicaceae species, \"A. thaliana\" is edible by humans in a salad or cooked, but it does not enjoy widespread use as a spring vegetable.\nUse as a model organism.\nBotanists and biologists began to research \"A. thaliana\" in the early 1900s, and the first systematic description of mutants was done around 1945. \"A. thaliana\" is now widely used for studying plant sciences, including genetics, evolution, population genetics, and plant development. Although \"A. thaliana\" the plant has little direct significance for agriculture, \"A. thaliana\" the model organism has revolutionized our understanding of the genetic, cellular, and molecular biology of flowering plants.\nThe first mutant in \"A. thaliana\" was documented in 1873 by Alexander Braun, describing a double flower phenotype (the mutated gene was likely \"Agamous\", cloned and characterized in 1990). Friedrich Laibach (who had published the chromosome number in 1907) did not propose \"A. thaliana\" as a model organism, though, until 1943. His student, Erna Reinholz, published her thesis on \"A. thaliana\" in 1945, describing the first collection of \"A. thaliana\" mutants that they generated using X-ray mutagenesis. Laibach continued his important contributions to \"A. thaliana\" research by collecting a large number of accessions (often questionably referred to as \"ecotypes\"). With the help of Albert Kranz, these were organised into a large collection of 750 natural accessions of \"A. thaliana\" from around the world.\nIn the 1950s and 1960s, John Langridge and George R\u00e9dei played an important role in establishing \"A. thaliana\" as a useful organism for biological laboratory experiments. R\u00e9dei wrote several scholarly reviews instrumental in introducing the model to the scientific community. The start of the \"A. thaliana\" research community dates to a newsletter called \"Arabidopsis\" Information Service, established in 1964. The first International \"Arabidopsis\" Conference was held in 1965, in G\u00f6ttingen, Germany.\nIn the 1980s, \"A. thaliana\" started to become widely used in plant research laboratories around the world. It was one of several candidates that included maize, petunia, and tobacco. The latter two were attractive, since they were easily transformable with the then-current technologies, while maize was a well-established genetic model for plant biology. The breakthrough year for \"A. thaliana\" as a model plant was 1986, in which T-DNA-mediated transformation and the first cloned \"A. thaliana\" gene were described.\nGenomics.\nNuclear genome.\nDue to the small size of its genome, and because it is diploid, \"Arabidopsis thaliana\" is useful for genetic mapping and sequencing \u2014 with about 157 megabase pairs and five chromosomes, \"A. thaliana\" has one of the smallest genomes among plants. It was long thought to have the smallest genome of all flowering plants, but that title is now considered to belong to plants in the genus \"Genlisea\", order Lamiales, with \"Genlisea tuberosa\", a carnivorous plant, showing a genome size of approximately 61\u00a0Mbp. It was the first plant genome to be sequenced, completed in 2000 by the \"Arabidopsis\" Genome Initiative. The most up-to-date version of the \"A. thaliana\" genome is maintained by the Arabidopsis Information Resource.\nThe genome encodes ~27,600 protein-coding genes and about 6,500 non-coding genes. However, the Uniprot database lists 39,342 proteins in their \"Arabidopsis\" reference proteome. Among the 27,600 protein-coding genes 25,402 (91.8%) are now annotated with \"meaningful\" product names, although a large fraction of these proteins is likely only poorly understood and only known in general terms (e.g. as \"DNA-binding protein without known specificity\"). Uniprot lists more than 3,000 proteins as \"uncharacterized\" as part of the reference proteome.\nChloroplast genome.\nThe plastome of \"A. thaliana\" is a 154,478 base-pair-long DNA molecule, a size typically encountered in most flowering plants (see the list of sequenced plastomes). It comprises 136 genes coding for small subunit ribosomal proteins (\"rps\", in yellow: see figure), large subunit ribosomal proteins (\"rpl\", orange), hypothetical chloroplast open reading frame proteins (\"ycf\", lemon), proteins involved in photosynthetic reactions (green) or in other functions (red), ribosomal RNAs (\"rrn\", blue), and transfer RNAs (\"trn\", black).\nMitochondrial genome.\nThe mitochondrial genome of \"A. thaliana\" is 367,808 base pairs long and contains 57 genes. There are many repeated regions in the arabidopsis mitochondrial genome. The largest repeats recombine regularly and isomerize the genome. Like most plant mitochondrial genomes, the arabidopsis mitochondrial genome exists as a complex arrangement of overlapping branched and linear molecules \"in vivo\".\nGenetics.\nGenetic transformation of \"A. thaliana\" is routine, using \"Agrobacterium tumefaciens\" to transfer DNA into the plant genome. The current protocol, termed \"floral dip\", involves simply dipping flowers into a solution containing \"Agrobacterium\" carrying a plasmid of interest and a detergent. This method avoids the need for tissue culture or plant regeneration.\nThe \"A. thaliana\" gene knockout collections are a unique resource for plant biology made possible by the availability of high-throughput transformation and funding for genomics resources. The site of T-DNA insertions has been determined for over 300,000 independent transgenic lines, with the information and seeds accessible through online T-DNA databases. Through these collections, insertional mutants are available for most genes in \"A. thaliana\".\nCharacterized accessions and mutant lines of \"A. thaliana\" serve as experimental material in laboratory studies. The most commonly used background lines are L\"er\" (Landsberg \"erecta\"), and Col, or Columbia. Other background lines less-often cited in the scientific literature are Ws, or Wassilewskija, C24, Cvi, or Cape Verde Islands, Nossen, etc. (see for ex.) Sets of closely related accessions named Col-0, Col-1, etc., have been obtained and characterized; in general, mutant lines are available through stock centers, of which best-known are the Nottingham Arabidopsis Stock Center-NASC and the Arabidopsis Biological Resource Center-ABRC in Ohio, USA.\nThe Col-0 accession was selected by R\u00e9dei from within a (nonirradiated) population of seeds designated 'Landsberg' which he received from Laibach. Columbia (named for the location of R\u00e9dei's former institution, University of Missouri-Columbia) was the reference accession sequenced in the Arabidopsis Genome Initiative. The Later (Landsberg erecta) line was selected by R\u00e9dei (because of its short stature) from a Landsberg population he had mutagenized with X-rays. As the L\"er\" collection of mutants is derived from this initial line, L\"er\"-0 does not correspond to the Landsberg accessions, which designated La-0, La-1, etc.\nTrichome formation is initiated by the GLABROUS1 protein. Knockouts of the corresponding gene lead to glabrous plants. This phenotype has already been used in gene editing experiments and might be of interest as visual marker for plant research to improve gene editing methods such as CRISPR/Cas9.\nNon-Mendelian inheritance controversy.\nIn 2005, scientists at Purdue University proposed that \"A. thaliana\" possessed an alternative to previously known mechanisms of DNA repair, producing an unusual pattern of inheritance, but the phenomenon observed (reversion of mutant copies of the \"HOTHEAD\" gene to a wild-type state) was later suggested to be an artifact because the mutants show increased outcrossing due to organ fusion.\nLifecycle.\nThe plant's small size and rapid lifecycle are also advantageous for research. Having specialized as a spring ephemeral, it has been used to found several laboratory strains that take about 6 weeks from germination to mature seed. The small size of the plant is convenient for cultivation in a small space, and it produces many seeds. Further, the selfing nature of this plant assists genetic experiments. Also, as an individual plant can produce several thousand seeds, each of the above criteria leads to \"A. thaliana\" being valued as a genetic model organism.\nCellular biology.\nArabidopsis is often the model for study of SNAREs in plants. This has shown SNAREs to be heavily involved in vesicle trafficking. Zheng et al. 1999 found an arabidopsis SNARE called &lt;templatestyles src=\"Template:Visible anchor/styles.css\" /&gt;AtVTI1a is probably essential to Golgi-vacuole trafficking. This is still a wide open field and plant SNAREs' role in trafficking remains understudied.\nDNA repair.\nThe DNA of plants is vulnerable to ultraviolet light, and DNA repair mechanisms have evolved to avoid or repair genome damage caused by UV. Kaiser et al. showed that in \"A. thaliana\" cyclobutane pyrimidine dimers (CPDs) induced by UV light can be repaired by expression of CPD photolyase.\nGermination in lunar regolith.\nOn May 12, 2022, NASA announced that specimens of \"Arabidopsis thaliana\" had been successfully germinated and grown in samples of lunar regolith. While the plants successfully germinated and grew into seedlings, they were not as robust as specimens that had been grown in volcanic ash as a control group, although the experiments also found some variation in the plants grown in regolith based on the location the samples were taken from, as \"A. thaliana\" grown in regolith gathered during Apollo 12 &amp; Apollo 17 were more robust than those grown in samples taken during Apollo 11.\nDevelopment.\nFlower development.\n\"A. thaliana\" has been extensively studied as a model for flower development. The developing flower has four basic organs - sepals, petals, stamens, and carpels (which go on to form pistils). These organs are arranged in a series of whorls, four sepals on the outer whorl, followed by four petals inside this, six stamens, and a central carpel region. Homeotic mutations in \"A. thaliana\" result in the change of one organ to another\u2014in the case of the \"agamous\" mutation, for example, stamens become petals and carpels are replaced with a new flower, resulting in a recursively repeated sepal-petal-petal pattern.\nObservations of homeotic mutations led to the formulation of the ABC model of flower development by E. Coen and E. Meyerowitz. According to this model, floral organ identity genes are divided into three classes - class A genes (which affect sepals and petals), class B genes (which affect petals and stamens), and class C genes (which affect stamens and carpels). These genes code for transcription factors that combine to cause tissue specification in their respective regions during development. Although developed through study of \"A. thaliana\" flowers, this model is generally applicable to other flowering plants.\nLeaf development.\nStudies of \"A. thaliana\" have provided considerable insights with regards to the genetics of leaf morphogenesis, particularly in dicotyledon-type plants. Much of the understanding has come from analyzing mutants in leaf development, some of which were identified in the 1960s, but were not analysed with genetic and molecular techniques until the mid-1990s. \"A. thaliana\" leaves are well suited to studies of leaf development because they are relatively simple and stable.\nUsing \"A. thaliana\", the genetics behind leaf shape development have become more clear and have been broken down into three stages: The initiation of the leaf primordium, the establishment of dorsiventrality, and the development of a marginal meristem. Leaf primordia are initiated by the suppression of the genes and proteins of class I \"KNOX\" family (such as \"SHOOT APICAL MERISTEMLESS\"). These class I KNOX proteins directly suppress gibberellin biosynthesis in the leaf primordium. Many genetic factors were found to be involved in the suppression of these class I \"KNOX\" genes in leaf primordia (such as \"ASYMMETRIC LEAVES1,\" \"BLADE-ON-PETIOLE1\", \"SAWTOOTH1\", etc.). Thus, with this suppression, the levels of gibberellin increase and leaf primordium initiate growth.\nThe establishment of leaf dorsiventrality is important since the dorsal (adaxial) surface of the leaf is different from the ventral (abaxial) surface.\nMicroscopy.\n\"A. thaliana\" is well suited for light microscopy analysis. Young seedlings on the whole, and their roots in particular, are relatively translucent. This, together with their small size, facilitates live cell imaging using both fluorescence and confocal laser scanning microscopy. By wet-mounting seedlings in water or in culture media, plants may be imaged uninvasively, obviating the need for fixation and sectioning and allowing time-lapse measurements. Fluorescent protein constructs can be introduced through transformation. The developmental stage of each cell can be inferred from its location in the plant or by using fluorescent protein markers, allowing detailed developmental analysis.\nPhysiology.\nLight sensing, light emission, and circadian biology.\nThe photoreceptors phytochromes A, B, C, D, and E mediate red light-based phototropic response. Understanding the function of these receptors has helped plant biologists understand the signaling cascades that regulate photoperiodism, germination, de-etiolation, and shade avoidance in plants. The genes \"FCA\", \"fy\", \"fpa\", \"LUMINIDEPENDENS\" (\"ld\"), \"fly\", \"fve\" and \"FLOWERING LOCUS C\" (\"FLC\") are involved in photoperiod triggering of flowering and vernalization. Specifically Lee et al 1994 find \"ld\" produces a homeodomain and Blazquez et al 2001 that \"fve\" produces a WD40 repeat.\nThe UVR8 protein detects UV-B light and mediates the response to this DNA-damaging wavelength.\n\"A. thaliana\" was used extensively in the study of the genetic basis of phototropism, chloroplast alignment, and stomal aperture and other blue light-influenced processes. These traits respond to blue light, which is perceived by the phototropin light receptors. Arabidopsis has also been important in understanding the functions of another blue light receptor, cryptochrome, which is especially important for light entrainment to control the plants' circadian rhythms. When the onset of darkness is unusually early, \"A. thaliana\" reduces its metabolism of starch by an amount that effectively requires division.\nLight responses were even found in roots, previously thought to be largely insensitive to light. While the gravitropic response of \"A. thaliana\" root organs is their predominant tropic response, specimens treated with mutagens and selected for the absence of gravitropic action showed negative phototropic response to blue or white light, and positive response to red light, indicating that the roots also show positive phototropism.\nIn 2000, Dr. Janet Braam of Rice University genetically engineered \"A. thaliana\" to glow in the dark when touched. The effect was visible to ultrasensitive cameras.\nMultiple efforts, including the Glowing Plant project, have sought to use \"A. thaliana\" to increase plant luminescence intensity towards commercially viable levels.\nThigmomorphogenesis (Touch response).\nIn 1990, Janet Braam and Ronald W. Davis determined that \"A. thaliana\" exhibits thigmomorphogenesis in response to wind, rain and touch. Four or more touch induced genes in \"A. thaliana\" were found to be regulated by such stimuli. In 2002, Massimo Pigliucci found that \"A. thaliana\" developed different patterns of branching in response to sustained exposure to wind, a display of phenotypic plasticity.\nOn the Moon.\nOn January 2, 2019, China's Chang'e-4 lander brought \"A. thaliana\" to the moon. A small microcosm 'tin' in the lander contained \"A. thaliana\", seeds of potatoes, and silkworm eggs. As plants would support the silkworms with oxygen, and the silkworms would in turn provide the plants with necessary carbon dioxide and nutrients through their waste, researchers will evaluate whether plants successfully perform photosynthesis, and grow and bloom in the lunar environment.\nSecondary metabolites.\n&lt;templatestyles src=\"Template:Visible anchor/styles.css\" /&gt;Thalianin is an arabidopsis root triterpene. Potter \"et al.\", 2018 finds synthesis is induced by a combination of at least 2 facts, cell-specific transcription factors (TFs) and the accessibility of the chromatin.\nPlant\u2013pathogen interactions.\nUnderstanding how plants achieve resistance is important to protect the world's food production, and the agriculture industry. Many model systems have been developed to better understand interactions between plants and bacterial, fungal, oomycete, viral, and nematode pathogens. \"A. thaliana\" has been a powerful tool for the study of the subdiscipline of plant pathology, that is, the interaction between plants and disease-causing pathogens.\nThe use of \"A. thaliana\" has led to many breakthroughs in the advancement of knowledge of how plants manifest plant disease resistance. The reason most plants are resistant to most pathogens is through nonhost resistance - not all pathogens will infect all plants. An example where \"A. thaliana\" was used to determine the genes responsible for nonhost resistance is \"Blumeria graminis\", the causal agent of powdery mildew of grasses. \"A. thaliana\" mutants were developed using the mutagen ethyl methanesulfonate and screened to identify mutants with increased infection by \"B. graminis\". The mutants with higher infection rates are referred to as\" PEN \"mutants due to the ability of \"B. graminis\" to penetrate \"A. thaliana\" to begin the disease process. The \"PEN\" genes were later mapped to identify the genes responsible for nonhost resistance to \"B. graminis\".\nIn general, when a plant is exposed to a pathogen, or nonpathogenic microbe, an initial response, known as PAMP-triggered immunity (PTI), occurs because the plant detects conserved motifs known as pathogen-associated molecular patterns (PAMPs). These PAMPs are detected by specialized receptors in the host known as pattern recognition receptors (PRRs) on the plant cell surface.\nThe best-characterized PRR in \"A. thaliana\" is FLS2 (Flagellin-Sensing2), which recognizes bacterial flagellin, a specialized organelle used by microorganisms for the purpose of motility, as well as the ligand flg22, which comprises the 22 amino acids recognized by FLS2. Discovery of FLS2 was facilitated by the identification of an \"A. thaliana\" ecotype, Ws-0, that was unable to detect flg22, leading to the identification of the gene encoding FLS2. . Both flagellin and UV-C act similarly to increase homologous recombination in \"A. thaliana\", as demonstrated by Molinier et al. 2006. Beyond this somatic effect, they found this to extend to subsequent generations of the plant.\nA second PRR, EF-Tu receptor (EFR), identified in \"A. thaliana\", recognizes the bacterial EF-Tu protein, the prokaryotic elongation factor used in protein synthesis, as well as the laboratory-used ligand elf18. Using \"Agrobacterium\"-mediated transformation, a technique that takes advantage of the natural process by which \"Agrobacterium\" transfers genes into host plants, the EFR gene was transformed into \"Nicotiana benthamiana\", tobacco plant that does not recognize EF-Tu, thereby permitting recognition of bacterial EF-Tu thereby confirming EFR as the receptor of EF-Tu.\nBoth FLS2 and EFR use similar signal transduction pathways to initiate PTI. \"A. thaliana\" has been instrumental in dissecting these pathways to better understand the regulation of immune responses, the most notable one being the mitogen-activated protein kinase (MAP kinase) cascade. Downstream responses of PTI include callose deposition, the oxidative burst, and transcription of defense-related genes.\nPTI is able to combat pathogens in a nonspecific manner. A stronger and more specific response in plants is that of effector-triggered immunity (ETI), which is dependent upon the recognition of pathogen effectors, proteins secreted by the pathogen that alter functions in the host, by plant resistance genes (R-genes), often described as a gene-for-gene relationship. This recognition may occur directly or indirectly via a guardee protein in a hypothesis known as the guard hypothesis. The first R-gene cloned in \"A. thaliana\" was \"RPS2\" (resistance to \"Pseudomonas syringae\" 2), which is responsible for recognition of the effector avrRpt2. The bacterial effector avrRpt2 is delivered into \"A. thaliana\" via the Type III secretion system of \"P. syringae\" pv. \"tomato\" strain DC3000. Recognition of avrRpt2 by RPS2 occurs via the guardee protein RIN4, which is cleaved. Recognition of a pathogen effector leads to a dramatic immune response known as the hypersensitive response, in which the infected plant cells undergo cell death to prevent the spread of the pathogen.\nSystemic acquired resistance (SAR) is another example of resistance that is better understood in plants because of research done in \"A. thaliana\". Benzothiadiazol (BTH), a salicylic acid (SA) analog, has been used historically as an antifungal compound in crop plants. BTH, as well as SA, has been shown to induce SAR in plants. The initiation of the SAR pathway was first demonstrated in \"A. thaliana\" in which increased SA levels are recognized by nonexpresser of PR genes 1 (\"NPR1\") due to redox change in the cytosol, resulting in the reduction of \"NPR1. NPR1\", which usually exists in a multiplex (oligomeric) state, becomes monomeric (a single unit) upon reduction. When NPR1 becomes monomeric, it translocates to the nucleus, where it interacts with many TGA transcription factors, and is able to induce pathogen-related genes such as \"PR1\". Another example of SAR would be the research done with transgenic tobacco plants, which express bacterial salicylate hydroxylase, nahG gene, requires the accumulation of SA for its expression\nAlthough not directly immunological, intracellular transport affects susceptibility by incorporating - or being tricked into incorporating - pathogen particles. For example, the \"Dynamin-related protein 2b/drp2b\" gene helps to move invaginated material into cells, with some mutants increasing \"PstDC3000\" virulence even further.\nEvolutionary aspect of plant-pathogen resistance.\nPlants are affected by multiple pathogens throughout their lifetimes. In response to the presence of pathogens, plants have evolved receptors on their cell surfaces to detect and respond to pathogens. \"Arabidopsis thaliana\" is a model organism used to determine specific defense mechanisms of plant-pathogen resistance. These plants have special receptors on their cell surfaces that allow for detection of pathogens and initiate mechanisms to inhibit pathogen growth. They contain two receptors, FLS2 (bacterial flagellin receptor) and EF-Tu (bacterial EF-Tu protein), which use signal transduction pathways to initiate the disease response pathway. The pathway leads to the recognition of the pathogen causing the infected cells to undergo cell death to stop the spread of the pathogen. Plants with FLS2 and EF-Tu receptors have shown to have increased fitness in the population. This has led to the belief that plant-pathogen resistance is an evolutionary mechanism that has built up over generations to respond to dynamic environments, such as increased predation and extreme temperatures.\n\"A. thaliana\" has also been used to study SAR.\nThis pathway uses benzothiadiazol, a chemical inducer, to induce transcription factors, mRNA, of SAR genes. This accumulation of transcription factors leads to inhibition of pathogen-related genes.\nPlant-pathogen interactions are important for an understanding of how plants have evolved to combat different types of pathogens that may affect them. Variation in resistance of plants across populations is due to variation in environmental factors. Plants that have evolved resistance, whether it be the general variation or the SAR variation, have been able to live longer and hold off necrosis of their tissue (premature death of cells), which leads to better adaptation and fitness for populations that are in rapidly changing environments. In the future, comparisons of the pathosystems of wild populations + their coevolved pathogens with wild-wild hybrids of known parentage may reveal new mechanisms of balancing selection. In life history theory we may find that \"A. thaliana\" maintains certain alleles due to pleitropy between plant-pathogen effects and other traits, as in livestock.\nResearch in \"A. thaliana\" suggests that the immunity regulator protein family EDS1 in general co-evolved with the CCHELO family of nucleotide-binding\u2013leucine-rich-repeat-receptors (NLRs). Xiao et al. 2005 have shown that the powdery mildew immunity mediated by \"A. thaliana\"'s RPW8 (which has a CCHELO domain) is dependent on two members of this family: \"EDS1\" itself and \"PAD4\".\n\"RESISTANCE TO PSEUDOMONAS SYRINGAE 5/RPS5\" is a disease resistance protein which guards \"AvrPphB SUSCEPTIBLE 1/PBS1\". \"PBS1\", as the name would suggest, is the target of \"AvrPphB\", an effector produced by \"Pseudomonas syringae\" pv. \"phaseolicola\".\nOther research.\nOngoing research on \"A. thaliana\" is being performed on the International Space Station by the European Space Agency. The goals are to study the growth and reproduction of plants from seed to seed in microgravity.Plant-on-a-chip devices in which \"A. thaliana\" tissues can be cultured in semi-\"in vitro\" conditions have been described. Use of these devices may aid understanding of pollen-tube guidance and the mechanism of sexual reproduction in \"A. thaliana.\"\nResearchers at the University of Florida were able to grow the plant in lunar soil originating from the Sea of Tranquillity.\nSelf-pollination.\n\"A. thaliana\" is a predominantly self-pollinating plant with an outcrossing rate estimated at less than 0.3%. An analysis of the genome-wide pattern of linkage disequilibrium suggested that self-pollination evolved roughly a million years ago or more. Meioses that lead to self-pollination are unlikely to produce significant beneficial genetic variability. However, these meioses can provide the adaptive benefit of recombinational repair of DNA damages during formation of germ cells at each generation. Such a benefit may have been sufficient to allow the long-term persistence of meioses even when followed by self-fertilization. A physical mechanism for self-pollination in \"A. thaliana\" is through pre-anthesis autogamy, such that fertilisation takes place largely before flower opening.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "37139", "revid": "34738792", "url": "https://en.wikipedia.org/wiki?curid=37139", "title": "Long Island Rail Road", "text": "Commuter rail system on Long Island, New York\nThe Long Island Rail Road (reporting mark LI), or LIRR, is a railroad in the southeastern part of the U.S. state of New York, stretching from Manhattan to the eastern tip of Suffolk County on Long Island. The railroad currently operates a public commuter rail service, with its freight operations contracted to the New York and Atlantic Railway. With an average weekday ridership of 354,800 passengers in 2016, it is the busiest commuter railroad in North America. It is also one of the world's few commuter systems that run 24/7 year-round. It is publicly owned by the Metropolitan Transportation Authority, which refers to it as MTA Long Island Rail Road. In 2024, the system had a ridership of , or about per weekday as of the third quarter of 2025.\nThe LIRR logo combines the circular MTA logo with the text \"Long Island Rail Road\", and appears on the sides of trains. The LIRR is one of two commuter rail systems owned by the MTA, the other being the Metro-North Railroad in the northern suburbs of the New York area. Established in 1834 (the first section between the Brooklyn waterfront and Jamaica opened on April 18, 1836) and having operated continuously since then, it is the oldest railroad in the United States still operating under its original name and charter.\nThere are 126 stations and more than of track on its two main lines running the full length of the island and eight major branches, with the passenger railroad system totaling . As of 2018[ [update]], the LIRR's budget for expenditures was $1.6 billion plus $450 million for debt service, which it supports through the collection of fares (which cover 43% of total expenses) along with dedicated taxes and other MTA revenue.\nHistory.\nThe Long Island Rail Road Company was chartered in 1834 to provide a daily service between New York City and Boston via a ferry connection between its Greenport, New York, terminal on Long Island's North Fork and Stonington, Connecticut. This service was superseded in 1849 by the land route through Connecticut that became part of the New York, New Haven and Hartford Railroad. The LIRR refocused its attentions towards serving Long Island, in competition with other railroads on the island. In the 1870s, railroad president Conrad Poppenhusen and his successor, Austin Corbin acquired all the railroads and consolidated them into the LIRR.\nThe LIRR was unprofitable for much of its history. In 1900, the Pennsylvania Railroad (PRR) bought a controlling interest as part of its plan for direct access to Manhattan, which began on September 8, 1910. The wealthy PRR subsidized the LIRR during the first half of the new century, allowing expansion and modernization. Electric operation began in 1905.\nAfter World War II, the railroad industry's downturn and dwindling profits caused the PRR to stop subsidizing the LIRR, and the LIRR went into receivership in 1949. The State of New York, realizing how important the railroad was to Long Island's future, began to subsidize the railroad in the 1950s and continued doing so into the 1960s. In June 1965, the state finalized an agreement to buy the LIRR from the PRR for $65 million. The LIRR was placed under the control of a new Metropolitan Commuter Transit Authority. The MCTA was rebranded the Metropolitan Transportation Authority in 1968 when it incorporated several other New York City-area transit agencies. With MTA subsidies the LIRR modernized further, continuing to be the busiest commuter railroad in the United States.\nThe LIRR is one of the few railroads that have survived as intact companies from their original charters to the present.\n21st century expansions.\nEast Side Access.\nThe East Side Access project built a LIRR spur to Grand Central Terminal that will run in part via the lower level of the existing 63rd Street Tunnel. The East Side Access project added a new eight-track terminal called Grand Central Madison underneath the existing Grand Central Terminal. The project was first proposed in the 1968 Program for Action, but due to various funding shortfalls, construction did not start until 2007. As of April 2018[ [update]], the project was expected to cost $11.1 billion and was tentatively scheduled to start service in December 2022. It opened on January 25, 2023, with limited shuttle service between Jamaica and Grand Central. Full service to Grand Central began on February 27, 2023.\nSeveral \"readiness projects\" were also completed to increase peak-hour capacity across the LIRR system in preparation for expanded peak-hour service after the completion of East Side Access. The LIRR constructed a new platform for Atlantic Terminal-bound trains at Jamaica station, converting most Atlantic Branch service between these two stations into a high-frequency shuttle. The LIRR also installed a new storage track east of Massapequa and extended one east of Great Neck station, in addition to expanding the train yard at Ronkonkoma. An expansion of the yard at Port Washington was also proposed, but as of September 2022[ [update]], the MTA has not come to an agreement with the Town of North Hempstead, resulting in the project being postponed indefinitely.\nThere are also plans to build a new station in the Queens neighborhood of Sunnyside, in between the New York terminals and the Woodside station, serving as a rail hub for all LIRR branches and potentially some Amtrak, Metro North (New Haven Line) and New Jersey Transit trains, as well. The Sunnyside station is to be built after the completion of East Side Access, due to current capacity constraints.\nMain Line projects.\nIn 2012, the LIRR started adding a second track along the formerly single-tracked section of the Main Line between Farmingdale and Ronkonkoma stations to increase track capacity and allow for enhanced service options. The project was completed in September 2018.\nAs part of the preparations for East Side Access's opening, the LIRR also widened the two-track sections of the Main Line between Floral Park and Hicksville stations to three tracks, in addition to eliminating each of the grade crossings and rebuilding all of the stations along this stretch of the Main Line. Work on the third-track project started in September 2018. The project was completed in 2022, in time for the opening of East Side Access.\nThe larger Belmont Park Redevelopment Project called for a new Elmont station between the Queens Village and Bellerose stations on the Main Line, to better serve the new UBS Arena in the Nassau County neighborhood of Elmont. It was the first new station built by the LIRR in nearly 50 years; the last new station added was the former Southampton College station on the Montauk Branch, which opened in 1976 and closed in 1998, due to low ridership and the high cost of installing high-level platforms for the then-new C3 railcars. Elmont's eastbound platform officially opened in November 2021, while the westbound platform opened in October 2022.\nMajor stations.\nThe LIRR operates out of four western terminals in New York City. These terminals are:\nIn addition, the Jamaica station is a major hub station and transfer point in Jamaica, Queens. It has ten tracks and six platforms, plus yard and bypass tracks. Passengers can transfer between trains on all LIRR lines except the Port Washington Branch. The sixth platform opened in February 2020, and exclusively serves Atlantic Branch shuttle trains to Brooklyn. Transfer is also made to separate facilities for three subway services at the Sutphin Boulevard\u2013Archer Avenue\u2013JFK Airport station ( E \u200b, \u200b J , and \u200b Z trains), a number of bus routes, and the AirTrain automated people mover to JFK Airport. The railroad's headquarters are next to the station.\nPassenger lines and services.\n&lt;templatestyles src=\"Routemap/styles.css\"/&gt;\nThe Long Island Rail Road system has eleven passenger branches, three of which are main trunk lines:\nThere are eight minor branches. For scheduling and advertising purposes some of these branches are divided into sections; this is the case with the Montauk Branch, which is known as the Babylon Branch service in the electrified portion of the line between Jamaica and Babylon, while the diesel service beyond Babylon to Montauk is referred to as Montauk Branch service. All branches except the Port Washington Branch pass through Jamaica; the trackage west of Jamaica (except the Port Washington Branch) is known as the City Terminal Zone. The City Terminal Zone includes portions of the Main Line, Atlantic, and Montauk Branches, as well as the Amtrak-owned East River Tunnels to Penn Station.\nFormer branches.\nThe railroad has dropped a number of branches due to lack of ridership over the years. Part of the Rockaway Beach Branch became part of the IND Rockaway Line of the New York City Subway, while others were downgraded to freight branches, and the rest abandoned entirely. Additionally, the Long Island Rail Road operated trains over portions of the Brooklyn Rapid Transit (BRT) elevated and subway lines until 1917.\nAdditional services.\nIn addition to its daily commuter patronage, the LIRR also offers the following services:\nIntermodal connections.\nPenn Station offers connections with Amtrak intercity trains and NJ Transit commuter trains, as well as the PATH, New York City Subway, and New York City Bus systems. Grand Central offers connections with Metro-North Railroad, as well as the subway and bus systems. Additionally, almost all stations in Brooklyn and Queens offer connections with the New York City Bus system, and several stations also have transfers to New York City Subway stations. Transfers to Nassau Inter-County Express and Suffolk County Transit buses are available at many stations in Nassau and Suffolk counties, respectively.\nFare structure.\nLike Metro-North Railroad and NJ Transit, the Long Island Rail Road fare system is based on the distance a passenger travels, as opposed to the New York City Subway and the area's bus systems, which charge a flat rate. The railroad is broken up into eight non-consecutively numbered fare zones. Zone 1, the City Terminal Zone, includes Penn Station, Grand Central, all stations in Brooklyn, all stations in Queens west of Jamaica on the Main Line, and Mets\u2013Willets Point.\nZone 3 includes Jamaica as well as all other stations in eastern Queens except Far Rockaway. Zones 4 and 7 include all stations in Nassau County, plus Far Rockaway and Belmont Park in Queens. Zones 9, 10, 12 and 14 include all stations in Suffolk County. Each zone contains many stations, and the same fare applies for travel between any station in the origin zone and any station in the destination zone.\nPeak and off-peak fares.\nPeak fares are charged during the week on trains that arrive at western terminals between 6\u00a0AM and 10\u00a0AM, and for trains that depart from western terminals between 4\u00a0PM and 8\u00a0PM. Any passenger holding an off-peak ticket on a peak train is required to pay a step up fee. Passengers can buy tickets from ticket agents or ticket vending machines (TVMs) or on the train from conductors, but will incur an on-board penalty fee for doing so. This fee is waived for customers boarding at a station without a ticket office or ticket machine, senior citizens, people with disabilities or Medicare customers.\nThere are several types of tickets: one way, round trip, peak, off-peak, AM peak or off-peak senior/disabled, peak child, and off-peak child. On off-peak trains, passengers can buy a family ticket for children who are accompanied by an 18-year-old for $0.75 if bought from the station agent or TVM, $1.00 on the train. Senior citizen/disabled passengers traveling during the morning peak hours are required to pay the AM peak senior citizen/disabled rate. This rate is not charged during PM peak hours.\nCommuters can also buy a peak or off-peak ten trip ride, a weekly unlimited or an unlimited monthly pass. Monthly passes are good on any train regardless of the time of day, within the fare zones specified on the pass.\nThe LIRR charged off-peak fares at all times during the COVID-19 pandemic. Peak fares were reinstated on March 1, 2022, and several new discounts and ticket options were introduced at the same time.\nSpecial fares.\nDuring the summer the railroad offers special summer package ticket deals to places such as Long Beach, Jones Beach, the Hamptons, Montauk, and Greenport. Passengers traveling to the Hamptons and Montauk on the \"Cannonball\" can reserve a seat in the all-reserved Parlor Cars.\nPrior to November 2021, passengers going to Belmont Park had to buy a special ticket to go from Jamaica to Belmont Park (or vice versa). Weekly and monthly passes were not accepted at Belmont Park. With the opening of Elmont station in November 2021, Belmont Park and Elmont were placed into fare zone 4.\nCityTicket.\nIn 2003, the LIRR and Metro-North started a pilot program in which passengers traveling within New York City were allowed to buy one-way tickets for $2.50. The special reduced-fare CityTicket, proposed by the New York City Transit Riders Council, was formally introduced in 2004. The discounted fares were initially only available for travel on Saturdays and Sundays. In March 2022, it was expanded to include all off-peak trains throughout the week for $5. The MTA announced plans in December 2022 to allow CityTickets to be used on peak trains as well; governor Kathy Hochul confirmed these plans the next month. The peak CityTickets, as announced in July 2023, would cost $7 each. As part of a one-year pilot program starting in July 2024, monthly tickets for LIRR trips entirely within New York City would also receive a 10% discount.\nCityTicket is valid for travel within Zones 1 and 3 on the Long Island Rail Road. CityTickets can only be bought before boarding \u2013 except at Mets\u2013Willets Point, where they can be purchased on board; they must be used on the day of purchase.\nCityTicket is not valid for travel to the Elmont station (located in Nassau County, just east of the Queens-Nassau border) \u2013 or the Far Rockaway or special event-only Belmont Park station (located in Queens, just west of the Queens-Nassau border) \u2013 and are all within Zone 4. \nFreedom Ticket.\nIn late 2017, the MTA was slated to launch a pilot that will allow LIRR, bus and subway service to use one ticket. The proposal for the ticket, called the \"Freedom Ticket,\" was initially put forth by the New York City Transit Riders Council (NYCTRC) in 2007. The NYCTRC wrote a proof of concept report in 2015. At the time of the report, express bus riders from Southeast Queens had some of the longest commutes in the city, with their commutes being 96 minutes long, yet they paid a premium fare of $6.50.\nRiders who take the dollar van to the subway paid $4.75 to get to Manhattan in 65 minutes; riders who only took the bus and subway paid $2.75 to get to Manhattan in 86 minutes; and riders who took the LIRR paid $10 to get to Manhattan in 35 minutes. Unlike the CityTicket, the Freedom Ticket would be valid for off-peak and multidirectional travel; have free transfers to the subway and bus system; and be capped at $215 per month. At the time, monthly CityTickets cost $330 per month.\nThe Freedom Ticket will initially be available for sale at the Atlantic Terminal, Nostrand Avenue, and East New York stations in Brooklyn and at the Laurelton, Locust Manor, Rosedale, and St. Albans stations in Queens. Riders, under the pilot, would be able to purchase one-way, weekly, or monthly passes that will be valid on the LIRR, on buses, and the subway. The fare will be higher than the price of a ride on the MetroCard, but it will be lower than the combined price of an LIRR ticket and a MetroCard, and it will allow unlimited free transfers between the LIRR, buses, and subway.\nThe former head of the MTA, Thomas Prendergast, announced at the January 2017 board meeting that the plan would be explored in a field study to determine fares and the impact on existing service. The plan is intended to fill approximately 20,000 unused seats of existing trains to Atlantic Terminal and Penn Station (or about 50% to 60% of peak trains in each direction), while at the same time providing affordable service to people with long commutes. The details were to be announced in spring 2017, and the pilot would last six months.\nThe MTA Board voted to approve a six-month pilot for a similar concept, the Atlantic Ticket, in May 2018. The Atlantic Ticket is similar in that it would allow LIRR riders in southeast Queens to purchase a one-way ticket to or from Atlantic Terminal for $5. The Atlantic Ticket would start in June 2018. The success of the pilot program has led the MTA to extend the program up to the summer of 2020 and renewed calls for the program to be implemented within New York City, where the fare for the Freedom Ticket\u2014if approved\u2014would cost US$2.75 and include free transfers between the LIRR &amp; Metro-North, bus, and subway.\nFar Rockaway Ticket.\nIn May 2023, the MTA announced that, as part of wider fare changes and in response to requests from Far Rockaway residents, a discounted ticket option \u2013 the Far Rockaway Ticket \u2013 would be introduced for travelers traveling on the Far Rockaway Branch between Far Rockaway and other stations within New York City (excluding Belmont Park). This new ticket would provide the same discounts as a regular CityTicket, while also having protections against fare evasion, given the unique nature of the Far Rockaway Branch's route. Tickets can only be purchased at the Far Rockaway station or on the MTA's TrainTime app in the station's vicinity; geolocation restrictions on the TrainTime app only allow purchase of discounted tickets within the vicinity of the Far Rockaway station.\nThe Far Rockaway Ticket became available for purchase on August 20, 2023.\nOMNY.\nIn 2017, it was announced that the MetroCard fare payment system, used on New York City-area rapid transit and bus systems, would be phased out and replaced by OMNY, a contactless fare payment system. Fare payment would be made using Apple Pay, Google Pay, debit/credit cards with near-field communication enabled, or radio-frequency identification cards. As part of the implementation of OMNY, the MTA also plans to use the system in the Long Island Rail Road and Metro-North Railroad.\nCombo Ticket.\nIn December 2022, the MTA announced the launch of an additional fare for use on journeys that utilize both of its railroad systems via Grand Central. The fare is priced as $8 more than an adult off-peak ticket from an origin station on one system to Grand Central. It is valid on both peak and off-peak trains.\nTrain operations.\nThe LIRR is relatively isolated from the rest of the national rail system despite operating out of Penn Station, the nation's busiest rail terminal. It connects with other railroads in just two locations:\nAll LIRR trains have an engineer (driver in non-US English) who operates the train, and a conductor who is responsible for the safe movement of the train, fare collection and on-board customer service. In addition, trains may have one or more assistant conductors to assist with fare collection and other duties. The LIRR is one of the last railroads in the United States to use mechanical interlocking control towers to regulate rail traffic.\nAs of 2016[ [update]], the LIRR has 8 active control towers. All movements on the LIRR are under the control of the Movement Bureau in Jamaica, which gives orders to the towers that control a specific portion of the railroad. Movements in Amtrak territory are controlled by Penn Station Control Center or PSCC, run jointly by the LIRR and Amtrak. The PSCC controls as far east as Harold Interlocking, in Sunnyside, Queens. The PSCC replaced several towers.\nThe Jamaica Control Center, operational since the third quarter of 2010, controls the area around Jamaica terminal by direct control of interlockings. This replaced several towers in Jamaica including Jay and Hall towers at the west and east ends of Jamaica station respectively. At additional locations, line side towers control the various switches and signals in accordance with the timetable and under the direction of the Movement Bureau in Jamaica.\nSignal and safety systems.\nToday's LIRR signal system has evolved from its legacy Pennsylvania Railroad (PRR)-based system, and the railroad utilizes a variety of wayside railroad signals including position light, color light and dwarf signals. In addition, much of the LIRR is equipped with a bi-directional Pulse code cab signaling called automatic speed control (ASC), though portions of the railway still retain single direction, wayside-only signaling. Unlike other railroads, which began using color-light signals in the 20th century, the LIRR did not begin using signals with color lights on its above ground sections until 2006.\nSome portions of the railway lack automatic signals and cab signals completely, instead train and track car movements are governed only by timetable and verbal/written train orders, although these areas are gradually receiving modern signals. Many other signals and switching systems on the LIRR are being modernized and upgraded as part of the Main Line's Third Track Project, most notably at Mineola, where the system is being completely redone and modernized.\nOn portions of the railroad equipped with ASC, engineers consult the speed display unit, which is capable of displaying seven speed indications. As a result of a December 1, 2013, train derailment in the Bronx on the Metro-North Railroad, railroads with similar cab signal systems to Metro-North, such as the LIRR, were ordered to modify the systems to enforce certain speed limit changes, which has resulted in lower average speeds and actual speed limits across the LIRR.\nPower transmission.\nThe LIRR's electrified lines are powered via a third rail at 750 volts DC.\nRolling stock.\nElectric fleet.\nThe LIRR's electric fleet consists of 836 M7, 170 M3, and several hundred M9 electric multiple unit cars in married pairs, meaning each car needs the other one to operate, with each car containing its own engineer's cab. The trainsets typically range from 6 to 12 cars long.\nIn September 2013, MTA announced that the LIRR would procure new M9 railcars from Kawasaki. A 2014 MTA forecast indicated that the LIRR would need 416 M9 railcars; 180 to replace the outdated M3 railcars and an additional 236 railcars for the additional passengers expected once the East Side Access project is complete. The first M9s entered revenue service on September 11, 2019.\nDiesel and dual-mode fleets.\nThe LIRR also uses 134 C3 bilevel coaches powered by 24 DE30AC diesel-electric locomotives and 20 DM30AC dual-mode locomotives. They are used mostly on non-electrified branches, including the Port Jefferson, Oyster Bay, Montauk, Central, and Greenport Branches. There are also 23 MP15AC locomotives in use as work trains and yard switchers.\nNamed trains.\nFor most of its history LIRR has served commuters, but it had many named trains, some with all-first class seating, parlor cars, and full bar service. Few of them lasted past World War II, but some names were revived during the 1950s and 1960s, as the railroad expanded its east end parlor car service with luxury coaches and Pullman cars from railroads that were discontinuing their passenger trains.\nFreight service.\nThe LIRR and other railroads that became part of the system have always had freight service, though this has diminished. The process of shedding freight service accelerated with the acquisition of the railroad by New York State. In the 21st century, there has been some appreciation of the need for better railroad freight service in New York City and on Long Island. Both areas are primarily served by trucking for freight haulage \u2013 an irony in a region with the most extensive rail transit service in the Americas, as well as the worst traffic conditions. \nProposals for a Cross-Harbor Rail Tunnel for freight have existed for years to alleviate these issues, and, in recent years, there have been many new pushes for its construction by officials. Financial issues, as well as bureaucracy, remain major hurdles in constructing it.\nIn May 1997, freight service was franchised on a 20-year term to the New York and Atlantic Railway (NYAR), a short line railroad owned by the Anacostia and Pacific Company. \nIt has its own equipment and crews, but uses the rail facilities of the LIRR. To the east, freight service operates to the end of the West Hempstead Branch, to Huntington on the Port Jefferson Branch, to Bridgehampton on the Montauk Branch, and to Riverhead on the Main Line. On the western end it provides service on the surviving freight-only tracks of the LIRR: the Bay Ridge and Bushwick branches; the \"Lower Montauk\" between Jamaica and Long Island City; and to an interchange connection at Fresh Pond Junction in Queens with the CSX, Canadian Pacific Kansas City, and Providence and Worcester railroads.\nFreight branches.\nSome non-electrified lines are used only for freight:\nPlanned service expansions.\nElectrification projects.\nAs part of the 2020\u20132024 MTA Capital Program, the MTA proposed electrifying the LIRR's Central Branch, which would for enhanced service options and capacity, and to mitigate service disruptions, should one arise. Although funding was initially allocated through the 2020\u20132024 MTA Capital Program, the project was ultimately put on hold.\nThere have also been many pushes by residents and politicians over the past several decades \u2013 most recently by former New York State Senator Jim Gaughran \u2013 to electrify the remainder of the Port Jefferson Branch between the Huntington and Port Jefferson stations, in addition to the remainder of the Oyster Bay Branch between the East Williston and Oyster Bay, to enhance service in the served areas and to upgrade service capacities along the lines; electrifying these lines could lead to more frequent direct service to and from Manhattan, as diesel trains are not allowed in Penn Station and dual-mode trains exceed the height clearance for the 63rd Street Tunnel into Grand Central Madison.\nLaw enforcement.\nThe Long Island Rail Road Police Department, founded in 1868, was absorbed along with the Metro-North Railroad Police Department to form the Metropolitan Transportation Authority Police Department (MTA Police) in 1998.\nCriticism and controversy.\nPassenger issues.\nThe LIRR has a long history of tense relations with its passengers. Daily commuters have long had complaints about the LIRR's service. According to a 1999 article in \"The New York Times,\" the LIRR's service woes were long considered part of the \"unholy trinity of life on Long Island,\" along with the Long Island Lighting Company's high rates and the Long Island Expressway's traffic snarls. Various commuter advocacy groups have been formed to try to represent those interests, in addition to the state mandated LIRR Commuters Council.\nThe LIRR has been criticized for not providing additional service to the East End of Long Island as the twin forks continue to grow in popularity as a year-round tourist and residential destination. Demand is evidenced by flourishing for-profit bus services such as the Hampton Jitney and the Hampton Luxury Liner and the early formative stages of a new East End Transportation Authority. Local politicians have joined the public outcry for the LIRR to either improve the frequency of east end services, or turn the operation over to a local transportation authority.\nCritics claim that the on-time performance (OTP) calculated by the LIRR is manipulated to be artificially high. Because the LIRR does not release any raw timing data nor does it have independent (non-MTA) audits it is impossible to verify this claim, or the accuracy of the current On Time Performance measurement. The percentage measure is used by many other US passenger railroads but the criticism over accuracy is specific to the LIRR. As defined by the LIRR, a train is \"on time\" if it arrives at a station within 5 minutes and 59 seconds of the scheduled time. The criterion was 4 minutes and 59 seconds until the LIRR changed it because of a bug in their computer systems.\nCritics believe the OTP measure does not reflect what commuters experience on a daily basis. The LIRR publishes the current OTP in a monthly booklet called TrainTalk. TrainTalk was previously known as \"Keeping Track.\" A more accurate way to measure delays and OTP has been proposed. Called the \"Passenger Hours Delayed\" index, it can measure the total person-hours of a specific delay. This would be useful in comparing the performance of specific days or incidents, day-to-day (or week-to-week) periods, but has not been adopted.\nRidership has increased from 81 million passengers in 2011 to 89.3 million passengers in 2016, which is the railroad's highest ridership since 1949. The all-time highest ridership was in 1929, when 119\u00a0million passengers rode 1.89 billion passenger miles. This increase in ridership has been attributed to the increased usage of the LIRR by millennials, and the increase of reverse-peak travel.\nPension and disability fraud scandal.\nA \"New York Times\" investigation in 2008 showed that 25% of LIRR employees who had retired since 2000 filed for disability payments from the federal Railroad Retirement Board and 97% of them were approved to receive disability pension. The total collected was more than $250,000,000 over eight years. As a result, Railroad Retirement agents from Chicago inspected the Long Island office of the Railroad Retirement Board on September 23, 2008. New York Governor David Paterson issued a statement calling for Congress to conduct a full review of the board's mission and daily activities. Officials at the board's headquarters responded to the investigation stating that all occupational disability annuities were issued in accordance with applicable laws.\nOn November 17, 2008, a former LIRR pension manager was arrested and charged with official misconduct for performing outside work without permission. However, these charges were all dismissed for \"no merit\" by Supreme Court Judge Kase on December 11, 2009, on the grounds that the prosecution had misled the grand jury in the indictment.\nA report produced in September 2009 by the Government Accountability Office stated that the rate at which retirees were rewarded disability claims was above the norm for the industry in general and indicated \"troubling\" practices that may indicate fraud, such as the use of a very small group of physicians in making diagnoses.\nAnother series of arrests on October 27, 2011, included two doctors and a former union official.\nAccording to court documents, from 1998 through 2011, 79% of LIRR retirees obtained federal disability when they retired. On August 6, 2013, a doctor and two consultants were found guilty in connection with the accusations and sentenced to prison.\nOvertime fraud scandals.\nIn 2018, LIRR foreman Raymond Murphy was discovered at or near his home on 10 separate occasions whilst claiming overtime pay. Murphy earned $405,021 in 2017, of which $295,490 was overtime. According to reports, he was allowed to retire with a full public pension before being reprimanded or punished.\nIn 2021, LIRR employee and track inspector Thomas Caputo and co-conspirators John Nugent and Joseph Balestra were federally convicted for large-scale overtime fraud. Caputo was paid approximately $461,000 in 2018, of which $344,000 was supposed overtime. He claimed to have worked 3,864 overtime hours, an average of more than 10 hours of overtime for all 365 days the year. Phone, bank, email, and other records revealed many of these hours were fraudulent: Caputo was clocked in during vacation and while attending outside social events such as a bowling league.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "37141", "revid": "37005538", "url": "https://en.wikipedia.org/wiki?curid=37141", "title": "Triskaidekaphobia", "text": "Fear of the number 13\nTriskaidekaphobia ( , ; from grc \" \"\u03c4\u03c1\u03b5\u03b9\u03c3\u03ba\u03b1\u03af\u03b4\u03b5\u03ba\u03b1\" (treiska\u00eddeka)\"\u00a0'thirteen' and grc \" \"\u03c6\u03cc\u03b2\u03bf\u03c2\" (ph\u00f3bos)\"\u00a0'fear') is fear or avoidance of the number 13. It is also a reason for the fear of Friday the 13th, called \"paraskevidekatriaphobia\" (from gre \" \"\u03a0\u03b1\u03c1\u03b1\u03c3\u03ba\u03b5\u03c5\u03ae\" (paraskevi)\"\u00a0'Friday' gre \" \"\u03b4\u03b5\u03ba\u03b1\u03c4\u03c1\u03b5\u03af\u03c2\" (dekatre\u00eds)\"\u00a0'thirteen' and grc \" \"\u03c6\u03cc\u03b2\u03bf\u03c2\" (ph\u00f3bos)\"\u00a0'fear') or \"friggatriskaidekaphobia\" (from non \" Frigg\"\u00a0'Frigg' and from grc \" \"\u03c4\u03c1\u03b5\u03b9\u03c3\u03ba\u03b1\u03af\u03b4\u03b5\u03ba\u03b1\" (treiska\u00eddeka)\"\u00a0'thirteen' and grc \" \"\u03c6\u03cc\u03b2\u03bf\u03c2\" (ph\u00f3bos)\"\u00a0'fear').\nThe term was used as early as in 1910 by Isador Coriat in \"Abnormal Psychology\".\nOrigins.\nThe supposed unlucky nature of the number 13 has several theories of origin. Although several authors claim it is an older belief, no such evidence has been documented so far. In fact, the earliest attestation of 13 being unlucky is first found after the Middle Ages in Europe.\nPlaying cards.\nTarot card games have been attested since at least around 1450 with the Visconti-Sforza Tarot. One of the trump cards in tarot represents Death, and is numbered 13 in several variants.\nIn 1781, Antoine Court de G\u00e9belin writes of this card's presence in the Tarot of Marseilles that the number thirteen was \"toujours regarde comme malheureux\" (\"always considered as unlucky\"). In 1784, Johann Gottlob Immanuel Breitkopf cites G\u00e9belin, and reaffirms that the tarot card number 13 is death and misfortune (\"Der Tod, Ungl\u00fcck\").\n13 at a table.\nSince at least 1774, a superstition of \"thirteen at a table\" has been documented: if 13 people sit at a table, then one of them must die within a year. The origin of the superstition is unclear and various theories of its source have been presented over the years.\nIn 1774, Johann August Ephraim G\u00f6tze speculated:\n\"Da ich aus der Erfahrung weis, da\u00df der Aberglaube nichts liebers, als Religionssachen, zu seinen Beweisen macht; so glaube ich bey nahe nicht zu irren, wenn ich den Ursprung des Gegenw\u00e4rtigen mit der Zahl XIII, von der Stelle des Evangelii herleite, wo der Heiland, bey der Ostermahlzeit, mit zw\u00f6lf J\u00fcngern zu Tische sa\u00df.\"\nSince I know from experience that superstition loves nothing better than religious matters as its proofs, I believe I'm almost certainly unmistaken when I derive the origin of the matter of the number XIII from the passage of the Gospel where the Savior sat at table with twelve disciples at the Easter meal.\nFrom the 1890s, a number of English-language sources reiterated the idea that at the Last Supper, Judas, the disciple who betrayed Jesus, was the 13th to sit at the table. The Bible says nothing about the order in which the Apostles sat, but there were thirteen people at the table.\nIn 1968, Douglas Hill in \"Magic and Superstition\" recounts a Norse myth about 12 gods having a dinner party in Valhalla. The trickster god Loki, who was not invited, arrived as the 13th guest, and arranged for H\u00f6\u00f0r to shoot Balder with a mistletoe-tipped arrow. This story was also echoed in \"Holiday Folklore, Phobias and Fun\" by folklore historian Donald Dossey, citing Hill. However, in the \"Prose Edda\" by Snorri Sturluson, the story about Loki and Balder does not emphasize that there are 12 gods, nor does it talk about a dinner party or the number 13.\nEvents related to \"unlucky\" 13.\nEffect on US Shuttle program mission naming.\nThe disaster that occurred on Apollo 13 may have been a factor that led to a renaming that prevented a mission called STS-13. STS-41-G was the name of the thirteenth Space Shuttle flight. However, originally STS-41-C was the mission originally numbered STS-13. STS-41-C was the eleventh orbital flight of the space shuttle program.\nThe numbering system of the Space Shuttle was changed to a new one after STS-9. The new naming scheme started with STS-41B, the previous mission was STS-9, and the thirteenth mission (what would have been STS-13) would be STS-41C. The new scheme had first number stand for the U.S. fiscal year, the next number was a launch site (1 or 2), and the next was the number of the mission numbered with a letter for that period.\nIn the case of the actual 13th flight, the crew was apparently not superstitious and made a humorous mission patch that had a black cat on it. Also, that mission re-entered and landed on Friday the 13th which one crew described as being \"pretty cool\". Because of the way the designations and launch manifest work, the mission numbered STS-13 might not have actually been the 13th to launch as was common throughout the shuttle program; indeed it turned out to be the eleventh. One of the reasons for this was when a launch had to be scrubbed, which delayed its mission.\nIn a 2016 news article, NASA stated that the numbering system change was due to a much higher frequency of planned launches (pre-Challenger disaster). As it was, the Shuttle program did have a disaster on its \"one-hundred\" and thirteenth mission going by date of launch, which was STS-107. The actual mission STS-113 was successful, and had actually launched earlier due to the nature of the launch manifest.\nOmission of 13th rooms.\nHotels, buildings and elevator manufacturers have also avoided using the number 13 for rooms and floors based on triskaidekaphobia.\nSeveral notable streets in London lack a No. 13, including \"Fleet Street, Park Lane, Oxford Street, Praed Street, St. James's Street, Haymarket and Grosvenor Street.\"\nThirteenth floor.\nThe thirteenth floor is a designation of a level of a multi-level building that is often omitted in countries where the number 13 is considered unlucky. Omitting the 13th floor may take a variety of forms; the most common include denoting what would otherwise be considered the thirteenth floor as level 14, giving the thirteenth floor an alternative designation such as \"12A\" or \"M\" (the thirteenth letter of the Latin alphabet), or closing the 13th floor to public occupancy or access (e.g., by designating it as a mechanical floor).\nReasons for omitting a thirteenth floor include triskaidekaphobia on the part of the building's owner or builder, or a desire by the building owner or landlord to prevent problems that may arise with superstitious tenants, occupants, or customers. In 2002, based on an internal review of records, Dilip Rangnekar of Otis Elevators estimated that 85% of the buildings with at least thirteen floors with Otis brand elevators did not have a floor named the 13th floor. Early tall-building designers, fearing a fire on the 13th floor, or fearing tenants' superstitions about the rumor, decided to omit having a 13th floor listed on their elevator numbering. This practice became commonplace, and eventually found its way into American mainstream culture and building design.\nVancouver city planners have banned the practice of skipping 4s and 13s, since it could lead to mistakes by first responders, for example going to the wrong floor.\nOrigin.\nThe origin of skipping the thirteenth floor when installing elevators is not known. However, during the advent of early skyscrapers, New York architectural critics warned developers not to exceed the height of the 13th floor. These critics insisted that buildings rising above the 13th floor () would lead to increased street congestion, ominous shadows and lower property values. Nevertheless, in a work published in 1939, sociologist Otto Neurath compared the use of money in an economy, which he saw as unnecessary, to the superstition of not installing the thirteenth floor: merely a social convention.\nResearch.\nIn a 2007 Gallup poll, 13 percent of American adults reported that they would be bothered if given a hotel room on the thirteenth floor, while 9 percent indicated that they would be sufficiently bothered to request a room on a different floor. Research on thirteenth-floor effects on real estate values presents a mixed picture. Several prominent American real estate developers have claimed that they are unaware of any reduction in the value of thirteenth-floor offices or apartments. On the other hand, in studies conducted in Russia, Antipov and Pokryshevskaya, and Burakov found that thirteenth-floor apartments were less likely to sell compared to apartments on twelfth or fourteenth floors. This effect, however, was eliminated if developers offered buyers a 10% or greater discount on the cost of thirteenth-floor apartments.\nLucky 13.\nIn some regions, 13 is or has been considered a lucky number. For example, prior to the First World War, 13 was considered to be a lucky number in France, even being featured on postcards and charms. In more modern times, 13 is lucky in Italy except in some contexts, such as sitting at the dinner table. In Cantonese-speaking areas, including Hong Kong and Macau, the number 13 is considered lucky because it sounds similar to the Cantonese words meaning \"sure to live\" (as opposed to the unlucky number 14 which in Cantonese sounds like the words meaning \"sure to die\"). Colgate University was started by 13 men with $13 and 13 prayers, so 13 is considered a lucky number. Friday the 13th is the luckiest day at Colgate.\nA number of sportspeople are known for wearing the number 13 jersey and performing successfully. On November 23, 2003, the Miami Dolphins retired the number 13 for Dan Marino, who played quarterback for the Dolphins from 1983 to 1999. Kurt Warner, St. Louis Rams quarterback (NFL MVP, 1999 &amp; 2001, and Super Bowl XXXIV MVP) also wore number 13. Wilt Chamberlain, 13-time NBA All-Star, has had his No. 13 Jersey retired by the NBA's Golden State Warriors, Philadelphia 76ers, Los Angeles Lakers, Harlem Globetrotters, and Kansas University Jayhawks, all of which he played for. In 1966, the Portugal national football team achieved their best-ever result at the World Cup final tournaments by finishing third, thanks to a Mozambican-born striker, Eusebio, who has scored nine goals at World Cup \u2013 four of them in a 5-3 quarterfinal win over North Korea \u2013 and won the Golden Boot award as the tournament's top scorer while wearing the number 13. In the 1954 and 1974 World Cup finals, Germany's Max Morlock and Gerd M\u00fcller, respectively, played and scored in the final, wearing the number 13. More recent footballers playing successfully while wearing number 13, include Michael Ballack, Alessandro Nesta, and Rafinha. Among other sportspeople who have chosen 13 as their number, are Venezuelans Dave Concepci\u00f3n, Omar Vizquel, Oswaldo Guill\u00e9n and Pastor Maldonado due to the number being considered lucky in Venezuelan culture. Swedish-born hockey player Mats Sundin, who played 14 of his 18 NHL seasons for the Toronto Maple Leafs, setting team records for goals and points, had his number 13 retired by the team on 15 October 2016.\nOutside of the sporting industry, 13 is used as a lucky number by other individuals, including Taylor Swift who has made prominent use of the number 13 throughout her career.\nIn popular culture.\nSome conspiracy theorists have suggested that the thirteenth floor in government buildings is not really missing, but actually contains top-secret governmental departments, or more generally that it is proof of something sinister or clandestine going on. This implication is often carried over, implicitly or explicitly, into popular culture; for example in:\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;^1 The main reason for this was stated to be to increase the number of car sales in the second half of the year. Even though 70% of new cars are bought during the first four months of the year, some consumers believe that the calendar year of registration does not accurately reflect the real age of a new car, since cars bought in January will most likely have been manufactured the previous year, while those bought later in the year will be actually made in the same year.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;^2 Tuesday is generally unlucky in Greece for the fall of Byzantium Tues 29th May 1453. In Spanish-speaking countries, there is a proverb: En martes no te cases, ni te embarques 'On Tuesday, do not get married or set sail'.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "37142", "revid": "196446", "url": "https://en.wikipedia.org/wiki?curid=37142", "title": "Zeno of Citium", "text": "Hellenistic philosopher, founder of Stoicism (c. 334\u2013c. 262 BC)\nZeno of Citium (; , ; c. 334 \u2013 c. 262 BC) was a Hellenistic philosopher from Citium (, ), Cyprus. \nHe was the founder of the Stoic school of philosophy, which he taught in Athens from about 300 BC.\nBased on the moral ideas of the Cynics, Stoicism laid great emphasis on goodness and peace of mind gained from living a life of virtue in accordance with nature. It proved very popular, and flourished as one of the major schools of philosophy from the Hellenistic period through to the Roman era, and enjoyed revivals in the Renaissance as Neostoicism and in the current era as Modern Stoicism.\nLife.\nZeno was born c. 334 BC, in the city of Citium in Cyprus,\nHis ancestry is disputed between Phoenician and Greek, because Citium contained both Phoenician and Greek inhabitants. This interpretation reflects early post-Victorian scholarship, which often equated culture with ancestry and failed to recognise the distinct, syncretic identity of ancient Cyprus - a society that integrated Greek and Near Eastern influences within its own local traditions.\nAncient biographical sources describe him as a Greek philosopher, reflecting the Hellenic linguistic and intellectual traditions of his time. While a number of contemporary and modern historians regard Zeno as a Phoenician, other modern scholars have contested this arguing for a Greek or Greco-Phoenician background. He had a Greek name, a Greek higher education and that there is no evidence he knew a language other than Greek, but in a carbonised papyrus from Herculaneum he is \"mocked for his poor command of the Greek language\". His father, Mnaseas, had a name ambiguously meaningful both in Phoenician (\"one causing to forget\") and in Greek (\"mindful\"). His mother and her name are not recorded.\nZeno received a Greek education and spent most of his life in Athens, where he founded the Stoic school of philosophy. He became a respected figure in Athenian society and was honoured with a public funeral after his death.\nMost of the details known about his life come from the biography and anecdotes preserved by Diogenes La\u00ebrtius in his \"Lives and Opinions of Eminent Philosophers\" written in the 3rd century AD, a few of which are confirmed by the \"Suda\" (a 10th-century Byzantine encyclopedia). Diogenes reports that Zeno's interest in philosophy began when \"he consulted the oracle to know what he should do to attain the best life, and that the gods' response was that he should take on the complexion of the dead. Whereupon, perceiving what this meant, he studied ancient authors.\" Zeno became a wealthy merchant.\nOn a voyage from Phoenicia to Peiraeus he survived a shipwreck, after which he went to Athens and visited a bookseller. There he encountered Xenophon's \"Memorabilia\". He was so pleased with the book's portrayal of Socrates that he asked the bookseller where men like Socrates were to be found. Just then, Crates of Thebes\u00a0\u2013 the most famous Cynic living at that time in Greece\u00a0\u2013 happened to be walking by, and the bookseller pointed to him.\nDiogenes La\u00ebrtius describes Zeno as a haggard, dark-skinned person, living a spare, ascetic life despite his wealth. This coincides with the influences of Cynic teaching, and was, at least in part, continued in his Stoic philosophy. From the day Zeno became Crates\u2019 pupil, he showed a strong bent for philosophy, though with too much native modesty to assimilate \"Anaideia;\" Cynic \u201cshamelessness\u201d and the disregard for societal norms in favor of freedom. An example of this may be found in the writings of Apuleius who narrates an incident where Crates and Hipparchia, his wife and fellow Cynic, engaged in a public act of sexual intercourse and, as such, drew a crowd. Zeno, upon catching sight of this, covered them both with his cloak so as to prevent bystanders from witnessing the copulating couple, displaying his own inability to be apathetic to the expectations of society. Hence Crates, desirous of curing this defect in him, gave him a potful of lentil-soup to carry through the Ceramicus (the pottery district); and when he saw that Zeno was ashamed and tried to keep it out of sight, Crates broke the pot with a blow of his staff. As Zeno began to run off in embarrassment with the lentil-soup flowing down his legs, Crates chided, \"Why run away, my little Phoenician? Nothing terrible has befallen you.\"\nAccording to his contemporaries, Zeno was attracted only to boys and other men, and Diogenes La\u00ebrtius mentions by name at least one with whom he was enamored, a young man named Chremonides (who may or may not be the Athenian statesman and general Chremonides).\nApart from Crates, Zeno studied under the philosophers of the Megarian school, including Stilpo, and the dialecticians Diodorus Cronus, and Philo. He is also said to have studied Platonist philosophy under the direction of Xenocrates, and Polemo.\nZeno began teaching in the colonnade in the Agora of Athens known as the Stoa Poikile (Greek \u03a3\u03c4\u03bf\u1f70 \u03a0\u03bf\u03b9\u03ba\u03af\u03bb\u03b7) in 301 BC. His disciples were initially called \"Zenonians,\" but eventually they came to be known as \"Stoics,\" a name previously applied to poets who congregated in the Stoa Poikile.\nAmong the admirers of Zeno was king Antigonus II Gonatas of Macedonia, who, whenever he came to Athens, would visit Zeno. Zeno is said to have declined an invitation to visit Antigonus in Macedonia, although their supposed correspondence preserved by La\u00ebrtius is undoubtedly the invention of a later writer. Zeno instead sent his friend and disciple Persaeus, who had lived with Zeno in his house. Among Zeno's other pupils there were Aristo of Chios, Sphaerus, and Cleanthes who succeeded Zeno as the head (\"scholarch\") of the Stoic school in Athens.\nZeno is said to have declined Athenian citizenship when it was offered to him, fearing that he would appear unfaithful to his native land, where he was highly esteemed, and where he contributed to the restoration of its baths, after which his name was inscribed upon a pillar there as \"Zeno the philosopher\". We are also told that Zeno was of an earnest, gloomy disposition; that he preferred the company of the few to the many; that he was fond of burying himself in investigations; and that he disliked verbose and elaborate speeches. Diogenes La\u00ebrtius has preserved many clever and witty remarks by Zeno, although these anecdotes are generally considered unreliable.\nZeno died around 262 BC.&lt;templatestyles src=\"Citation/styles.css\"/&gt;[a] La\u00ebrtius reports about his death:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;\nAs he was leaving the school he tripped and fell, breaking his toe. Striking the ground with his fist, he quoted the line from the \"Niobe\":\nI come, I come, why dost thou call for me?\nand died on the spot through holding his breath.\nAt Zeno's funeral an epitaph was composed for him stating:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;And if thy native country was Phoenicia,\nWhat need to slight thee? Came not Cadmus thence,\nWho gave to Greece her books and art of writing?\nThis signified that even though Zeno was of non-Greek background the Greeks still respected him, comparing him to the legendary Phoenician hero Cadmus who had brought the alphabet to the Greeks, as Zeno had brought Stoicism to them and was described as \"the noblest man of his age\" with a bronze statue being built in his honor.\nDuring his lifetime, Zeno received appreciation for his philosophical and pedagogical teachings. Among other things, Zeno was honored with the golden crown, and a tomb was built in honor of his moral influence on the youth of his era.\nThe crater Zeno on the Moon is named in his honour.\nPhilosophy.\nFollowing the ideas of the Old Academy, Zeno divided philosophy into three parts: logic (a wide subject including rhetoric, grammar, and the theories of perception and thought); physics (not just science, but the divine nature of the universe as well); and ethics, the end goal of which was to achieve eudaimonia through the right way of living according to Nature. Because Zeno's ideas were later expanded upon by Chrysippus and other Stoics, it can be difficult to determine precisely what he thought. But his general views can be outlined as follows:\nLogic.\nIn his treatment of logic, Zeno was influenced by Stilpo and the other Megarians. Zeno urged the need to lay down a basis for logic because the wise person must know how to avoid deception. Cicero accused Zeno of being inferior to his philosophical predecessors in his treatment of logic, and it seems true that a more exact treatment of the subject was laid down by his successors, including Chrysippus. Zeno divided true conceptions into the comprehensible and the incomprehensible, permitting for free-will the power of assent (\"sinkatathesis\"/\u03c3\u03c5\u03bd\u03ba\u03b1\u03c4\u03ac\u03b8\u03b5\u03c3\u03b9\u03c2) in distinguishing between sense impressions. Zeno said that there were four stages in the process leading to true knowledge, which he illustrated with the example of the flat, extended hand, and the gradual closing of the fist:\nZeno stretched out his fingers, and showed the palm of his hand, \u2013 \"Perception,\" \u2013 he said, \u2013 \"is a thing like this.\"\u2013 \nThen, when he had closed his fingers a little, \u2013 \"Assent is like this.\" \u2013 Afterwards, when he had completely closed his hand, and showed his fist, that, he said, was Comprehension. From which simile he also gave that state a new name, calling it \"katalepsis\" (\u03ba\u03b1\u03c4\u03ac\u03bb\u03b7\u03c8\u03b9\u03c2). But when he brought his left hand against his right, and with it took a firm and tight hold of his fist: \u2013 \"Knowledge\" \u2013 he said, was of that character; and that was what none but a wise person possessed.\nPhysics.\nThe universe, in Zeno's view, is God: a divine reasoning entity, where all the parts belong to the whole. Into this pantheistic system he incorporated the physics of Heraclitus; the universe contains a divine artisan-fire, which foresees everything, and extending throughout the universe, must produce everything:\nZeno, then, defines nature by saying that it is artistically working fire, which advances by fixed methods to creation. For he maintains that it is the main function of art to create and produce and that what the hand accomplishes in the productions of the arts we employ, is accomplished much more artistically by nature, that is, as I said, by artistically working fire, which is the master of the other arts.\nThis divine fire, or aether, is the basis for all activity in the universe, operating on otherwise passive matter, which neither increases nor diminishes itself. The primary substance in the universe comes from fire, passes through the stage of air, and then becomes water: the thicker portion becoming earth, and the thinner portion becoming air again, and then rarefying back into fire. Individual souls are part of the same fire as the world-soul of the universe. Following Heraclitus, Zeno adopted the view that the universe underwent regular cycles of formation and destruction.\nThe nature of the universe is such that it accomplishes what is right and prevents the opposite, and is identified with unconditional Fate, while allowing it the free-will attributed to it. According to Zeno's beliefs, \"[t]rue happiness\" can only be found by obeying natural laws and living in tune with the course of fate.\nEthics.\nLike the Cynics, Zeno recognised a single, sole and simple good, which is the only goal to strive for. \"Happiness is a good flow of life,\" said Zeno, and this can only be achieved through the use of right reason coinciding with the universal reason (\"Logos\"), which governs everything. A bad feeling (\"pathos\") \"is a disturbance of the mind repugnant to reason, and against Nature.\" This consistency of soul, out of which morally good actions spring, is virtue, true good can only consist in virtue.\nZeno deviated from the Cynics in saying that things that are morally adiaphora (indifferent) could nevertheless have value. Things have a relative value in proportion to how they aid the natural instinct for self-preservation. That which is to be preferred is a \"fitting action\" (\"kath\u00eakon\"/\u03ba\u03b1\u03b8\u1fc6\u03ba\u03bf\u03bd), a designation Zeno first introduced. Self-preservation, and the things that contribute towards it, has only a conditional value; it does not aid happiness, which depends only on moral actions.\nJust as virtue can only exist within the dominion of reason, so vice can only exist with the rejection of reason. Virtue is absolutely opposed to vice, the two cannot exist in the same thing together, and cannot be increased or decreased; no one moral action is more virtuous than another. All actions are either good or bad, since impulses and desires rest upon free consent, and hence even passive mental states or emotions that are not guided by reason are immoral, and produce immoral actions. Zeno distinguished four negative emotions: desire, fear, pleasure and sorrow (\"epithumia, phobos, h\u00eadon\u00ea, lup\u00ea\" / \u1f10\u03c0\u03b9\u03b8\u03c5\u03bc\u03af\u03b1, \u03c6\u03cc\u03b2\u03bf\u03c2, \u1f21\u03b4\u03bf\u03bd\u03ae, \u03bb\u03cd\u03c0\u03b7), and he was probably responsible for distinguishing the three corresponding positive emotions: will, caution, and joy (\"boul\u00easis, eulabeia, chara\" / \u03b2\u03bf\u03cd\u03bb\u03b7\u03c3\u03b9\u03c2, \u03b5\u1f50\u03bb\u03ac\u03b2\u03b5\u03b9\u03b1, \u03c7\u03b1\u03c1\u03ac), with no corresponding rational equivalent for pain. All errors must be rooted out, not merely set aside, and replaced with right reason.\nWorks.\nNone of Zeno's original writings have survived except as fragmentary quotations preserved by later writers. The most famous of his works was his \"Republic\", written in conscious imitation of, or opposition to, Plato's \"Republic\". Although it has not survived, more is known about it than any of his other works. It outlined Zeno's vision of the ideal Stoic society.\nA manuscript that was attributed to Zeno, matching a known title of one of Zeno's works, \u03a0\u03b5\u03c1\u1f76 \u03c6\u03cd\u03c3\u03b5\u03c9\u03c2 (\"On Nature),\" was discovered in 1949 in an Old Armenian translation. In 1956 it was translated into Russian and published with an extensive commentary. Subsequent philological investigation concluded that the author could not have been Zeno and was instead an anonymous Christian philosopher of the late sixth century or a little later, writing in the tradition of ancient philosophy, but doing so as a Christian. He is now known as Pseudo-Zeno. His work shows an integration of Christian and philosophical concepts, but in a very restrained way.\nThe titles of many of Zeno's writings are, however, known and are as follows:\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "37143", "revid": "10202399", "url": "https://en.wikipedia.org/wiki?curid=37143", "title": "Chrysippus", "text": "Greek Stoic philosopher (c.279\u2013c.206 BC)\nChrysippus of Soli (; , ; ) was a Greek Stoic philosopher. He was a native of Soli, Cilicia, but moved to Athens as a young man, where he became a pupil of the Stoic philosopher Cleanthes. When Cleanthes died, around 230 BC, Chrysippus became the third head of the Stoic school. A prolific writer, Chrysippus expanded the fundamental doctrines of Cleanthes' mentor Zeno of Citium, the founder and first head of the school, which earned him the title of the Second Founder of Stoicism.\nChrysippus excelled in logic, the theory of knowledge, ethics, and physics. He created an original system of propositional logic in order to better understand the workings of the universe and role of humanity within it. He adhered to a fatalistic view of fate, but nevertheless sought a role for personal agency in thought and action. Ethics, he thought, depended on understanding the nature of the universe, and he taught a therapy of extirpating the unruly passions which depress and crush the soul. He initiated the success of Stoicism as one of the most influential philosophical movements for centuries in the Greek and Roman world. The linguistic orientation of Chrysippus' work made it difficult for its students even within the Stoic school.\nOf his several written works, none have survived except as fragments. Segments of some of his works were discovered among the Herculaneum papyri.\nLife.\nPresumably of Phoenician descent, Chrysippus was the son of Apollonius of Tarsus, and he was born at Soli, Cilicia. He was slight in stature, and is reputed to have trained as a long-distance runner. While still young, he lost his substantial inherited property when it was confiscated to the king's treasury. Chrysippus moved to Athens, where he became the disciple of Cleanthes, who was then the head (\"scholarch\") of the Stoic school. He is believed to have attended the courses of Arcesilaus and his successor Lacydes, in the Platonic Academy.\nChrysippus threw himself eagerly into the study of the Stoic system. His reputation for learning among his contemporaries was considerable. He was noted for intellectual audacity and self-confidence and his reliance on his own ability was shown, among other things, in the request he is supposed to have made to Cleanthes: \"Give me the principles, and I will find the proofs myself.\" He succeeded Cleanthes as head of the Stoic school when Cleanthes died, in around 230 BC.\nChrysippus was a prolific writer. He is said to rarely have gone without writing 500 lines a day and he composed more than 705 works. His desire to be comprehensive meant that he would take both sides of an argument and his opponents accused him of filling his books with the quotations of others. He was considered diffuse and obscure in his utterances and careless in his style, but his abilities were highly regarded, and he came to be seen as a preeminent authority for the school.\nHe died during the 143rd Olympiad (208\u2013204 BC) at the age of 73. Diogenes La\u00ebrtius gives two different accounts of his death. In the first account, Chrysippus was seized with dizziness having drunk undiluted wine at a feast, and died soon after. In the second account, he was watching a donkey eat some figs and cried out: \"Now give the donkey a drink of pure wine to wash down the figs\", whereupon he died in a fit of laughter. His nephew Aristocreon erected a statue in his honour in the Kerameikos. Chrysippus was succeeded as head of the Stoic school by his pupil Zeno of Tarsus.\nOf his written works, none survived except as fragments quoted in the works of later authors like Cicero, Seneca, Galen, Plutarch, and others. In 2004, segments from \"Logical Questions\" and \"On Providence\" were discovered among the Herculaneum papyri. A third work by Chrysippus may also be among them.\nStudy.\nChrysippus had a long and successful career of resisting the attacks of the Academy and hoped not simply to defend Stoicism against the assaults of the past, but also against all possible attack in the future. He took the doctrines of Zeno and Cleanthes and crystallized them into what became the definitive system of Stoicism. He elaborated the physical doctrines of the Stoics and their theory of knowledge and he created much of their formal logic. In short, Chrysippus made the Stoic system what it was. It was said that \"without Chrysippus, there would have been no Stoa\".\nLogic.\nChrysippus wrote much on the subject of logic and created a system of propositional logic. Aristotle's term logic had been concerned with the interrelations of terms such as \"Socrates\" or \"man\" (\"all men are mortal, Socrates is a man, so Socrates is mortal\"). Stoic logic, on the other hand, was concerned with the interrelations of propositions such as \"it is day\" (\"if it is day, it is light: but it is day: so it is light\"). Though the earlier Megarian dialecticians \u2013\u00a0Diodorus Cronus and Philo\u00a0\u2013 had worked in this field and the pupils of Aristotle \u2013\u00a0Theophrastus and Eudemus\u00a0\u2013 had investigated hypothetical syllogisms, it was Chrysippus who developed these principles into a coherent system of propositional logic.\nPropositions.\nChrysippus defined a proposition as \"that which is capable of being denied or affirmed as it is in itself\" and gave examples of propositions such as \"it is day\" and \"Dion is walking.\" He distinguished between simple and non-simple propositions, which in modern terminology are known as atomic and molecular propositions. A simple proposition is an elementary statement such as \"it is day.\" Simple propositions are linked together to form non-simple propositions by the use of logical connectives. Chrysippus enumerated five kinds of molecular propositions according to the connective used:\nThus several types of molecular propositions, familiar to modern logic, were listed by Chrysippus, including the conjunction, the disjunction, and the conditional, and Chrysippus studied their criteria of truth closely.\nConditional propositions.\nThe first logicians to debate conditional statements were Diodorus Cronus and his pupil Philo. Writing five-hundred years later, Sextus Empiricus refers to a debate between Diodorus and Philo. Philo regarded all conditionals as true except those which with a correct antecedent had an incorrect consequent, and this meant a proposition such as \"if it is day, then I am talking,\" is true unless it is day and I fall silent. But Diodorus argued that a true conditional is one in which the antecedent clause could never lead to an untrue conclusion\u00a0\u2013 thus, because the proposition \"if it is day, then I am talking\" can be false, it is invalid. However, paradoxical propositions were still possible such as \"if atomic elements of things do not exist, atomic elements exists.\" Chrysippus adopted a much stricter view regarding conditional propositions, which made such paradoxes impossible: to him, a conditional is true if denial of the consequent is logically incompatible with the antecedent. This corresponds to the modern-day strict conditional.\nSyllogistic.\nChrysippus developed a syllogistic or system of deduction in which he made use of five types of basic arguments or argument forms called indemonstrable syllogisms, which played the role of axioms, and four inference rules, called \"themata\" by means of which complex syllogisms could be reduced to these axioms. The forms of the five indemonstrables were:\nOf the four inference rules (themata, \u03b8\u03ad\u03bc\u03b1\u03c4\u03b1), only two survived. One, the so-called first \"thema\", was a rule of antilogism. The other, the third \"thema\", was a cut rule by which chain syllogisms could be reduced to simple syllogisms. The purpose of Stoic syllogistic was not merely to create a formal system. It was also understood as the study of the operations of reason, the divine reason (\"logos\") which governs the universe, of which human beings are a part. The goal was to find valid rules of inference and forms of proof to help people find their way in life.\nAccording to Sextus Empiricus, Chrysippus held that dogs use disjunctive syllogism, such as when using scent to pick which path to run down. This was in contrast to a tradition since Aristotle, who saw reasoning (and reasoning deductively) as man's defining aspect.\nOther logical work.\nChrysippus analyzed speech and the handling of names and terms. He also devoted much effort in refuting fallacies and paradoxes. According to Diogenes La\u00ebrtius, Chrysippus wrote twelve works in 23 books on the liar paradox; seven works in 17 books on amphiboly; and another nine works in 26 books on other conundrums. In all, 28 works or 66 books were given over to puzzles or paradoxes.\nChrysippus is the first Stoic for whom the third of the four Stoic categories, i.e. the category \"somehow disposed\" is attested. In the surviving evidence, Chrysippus frequently makes use of the categories of \"substance\" and \"quality\", but makes little use of the other two Stoic categories (\"somehow disposed\" and \"somehow disposed in relation to something\"). It is not clear whether the categories had any special significance for Chrysippus, and a clear doctrine of categories may be the work of later Stoics.\nLater reception.\nChrysippus came to be renowned as one of the foremost logicians of ancient Greece. When Clement of Alexandria wanted to mention one who was master among logicians, as Homer was master among poets, it was Chrysippus, not Aristotle, he chose. Diogenes La\u00ebrtius wrote: \"If the gods use dialectic, they would use none other than that of Chrysippus.\" The logical work by Chrysippus came to be neglected and forgotten. Aristotle's logic prevailed, partly because it was seen as more practical, and partly because it was taken up by the Neoplatonists. As recently as the 19th century, Stoic logic was treated with contempt, a barren formulaic system, which was merely clothing the logic of Aristotle with new terminology. It was not until the 20th century, with the advances in logic, and the modern propositional calculus, that it became clear that Stoic logic constituted a significant achievement.\nEpistemology.\nFor the Stoics, truth is distinguished from error by the sage who possesses right reason. Chrysippus's theory of knowledge was empirical. The senses transmit messages from the external world, and their reports are controlled not by referring them to innate ideas, but by comparing them to previous reports stored in the mind. Zeno had defined impressions of sense as \"an impression in the soul\" and this was interpreted literally by Cleanthes, who compared the impression on the soul to the impression made by a seal on wax. Chrysippus preferred to regard it as an alteration or change in the soul; that is, the soul receives a modification from every external object that acts upon it, just as the air receives countless strokes when many people are speaking at once.\nIn the receipt of an impression, the soul is purely passive and the impression reveals not only its own existence, but that also of its cause\u00a0\u2013 just as light displays itself and the elements that are in it. The power to name the object resides in the understanding. First must come the impression, and the understanding\u00a0\u2013 having the power of utterance\u00a0\u2013 expresses in speech the affection it receives from the object. True presentations are distinguished from those that are false by the use of memory, classification and comparison. If the sense organ and the mind are healthy\u00a0\u2013 and provided that an external object can be really seen or heard\u00a0\u2013 the presentation, due to its clearness and distinctness, has the power to extort the assent that always lies in our power, to give or to withhold. In a context in which people are understood to be rational beings, reason is developed out of these notions.\nPhysics.\nChrysippus insisted on the organic unity of the universe, as well as the correlation and mutual interdependence of all of its parts. He said, the universe is \"the soul and guide of itself.\" Following Zeno, Chrysippus determined fiery breath or aether to be the primitive substance of the universe. Objects are made up of inert formless matter and an informing soul, \"pneuma\", provides form to the undifferentiated matter. The \"pneuma\" pervades all of substance and maintains the unity of the universe and constitutes the soul of the human being.\nThe classical elements change into one another by a process of condensation and rarefaction. Fire first becomes solidified into air; then air into water; and lastly, water into earth. The process of dissolution takes place in the reverse order: earth being rarefied into water, water into air and air into fire.\nThe human soul was divided by Chrysippus into eight faculties: the five senses, the power of reproduction, the power of speech, and the \"ruling part\" that is located in the chest rather than the head. Individual souls are perishable; but, according to the view originated by Chrysippus, the souls of wise people survive longer after their death. No individual soul can, however, survive beyond the periodic conflagration, when the universe is renewed.\nThere were no universals or abstract objects for Chrysippus, making him a kind of nominalist.\nFate.\nFor Chrysippus, all things happen according to fate: what seems to be accidental has always some hidden cause. The unity of the world consists in the chain-like dependence of cause upon cause. Nothing can take place without a sufficient cause. According to Chrysippus, every proposition is either true or false, and this must apply to future events as well:\nIf any motion exists without a cause, then not every proposition will be either true or false. For that which has not efficient causes is neither true nor false. But every proposition is either true or false. Therefore, there is no motion without a cause. And if this is so, then all effects owe their existence to prior causes. And if this is so, all things happen by fate. It follows therefore that whatever happens, happens by fate.\nThe Stoic view of fate is entirely based on a view of the universe as a whole. Individual things and persons only come into consideration as dependent parts of this whole. Everything is, in every respect, determined by this relation, and is consequently subject to the general order of the world.\nIf his opponents objected that, if everything is determined by destiny, there is no individual responsibility, since what has been once foreordained must happen, come what may, Chrysippus replied that there is a distinction to be made between simple and complex predestination. Becoming ill may be fated whatever happens but, if a person's recovery is linked to consulting a doctor, then consulting the doctor is fated to occur together with that person's recovery, and this becomes a complex fact. All human actions\u00a0\u2013 in fact, our destiny\u00a0\u2013 are decided by our relation to things, or as Chrysippus put it, events are \"co-fated\" to occur:\nThe non-destruction of one's coat, he says, is not fated simply, but co-fated with its being taken care of, and someone's being saved from his enemies is co-fated with his fleeing those enemies; and having children is co-fated with being willing to lie with a woman. ... For many things cannot occur without our being willing and indeed contributing a most strenuous eagerness and zeal for these things, since, he says, it was fated for these things to occur in conjunction with this personal effort. ... But it will be in our power, he says, with what is in our power being included in fate.\nThus our actions are predetermined, and are causally related to the overarching network of fate, but nevertheless the moral responsibility of how we respond to impressions remains our own. The one all-determining power is active everywhere, working in each particular being according to its nature, whether in rational or irrational creatures or in inorganic objects. Every action is brought about by the co-operation of causes depending on the nature of things and the character of the agent. Our actions would only be involuntary if they were produced by external causes alone, without any co-operation \u2013 on the part of our wills \u2013 with external causes. Virtue and vice are set down as things in our power, for which, consequently, we are responsible. Moral responsibility depends only on freedom of the will, and what emanates from our will is our own, no matter whether it is possible for us to act differently or not. This rather subtle position, which attempts to reconcile determinism with human responsibility, is known as soft-determinism, or as compatibilism.\nDivination.\nChrysippus also argued for the existence of fate based on divination, which he thought there was good evidence for. It would not be possible for diviners to predict the future if the future itself was accidental. Omens and portents, he believed, are the natural symptoms of certain occurrences. There must be countless indications of the course of providence, for the most part unobserved, the meaning of only a few having become known to humanity. To those who argued that divination was superfluous as all events are foreordained, he replied that both divination and our behaviour under the warnings which it affords are included in the chain of causation.\nGod.\nThe Stoics believed that the universe is God, and Chrysippus affirmed that \"the universe itself is God and the universal outpouring of its soul.\" It is the guiding principle of the universe, \"operating in mind and reason, together with the common nature of things and the totality which embraces all existence.\" Based on these beliefs, physicist and philosopher Max Bernhard Weinstein identified Chrysippus as a Pandeist.\nChrysippus sought to prove the existence of God, making use of a teleological argument:\nIf there is anything that humanity cannot produce, the being who produces it is better than humanity. But humanity cannot produce the things that are in the universe\u00a0\u2013 the heavenly bodies, etc. The being, therefore, who produces them is superior to humanity. But who is there that is superior to humanity, except God? Therefore, God exists.\nChrysippus spoke of God and gods interchangeably. He interpreted the gods of traditional Greek religion by viewing them as different aspects of the one reality. Cicero tells us that \"he further maintained that aether is that which people call Zeus, and that the air which permeates the seas is Poseidon, and that the earth is what is known by the name of Demeter, and he treated in similar style the names of the other gods.\" In addition, the universe exists for the benefit of the universal god:\nWe should infer in the case of a beautiful dwelling-place that it was built for its owners and not for mice; we ought, therefore, in the same way to regard the universe as the dwelling-place of the gods.\nTheodicy.\nIn response to the question of how evil could exist in a good universe, Chrysippus replied \"evil cannot be removed, nor is it well that it should be removed.\" Firstly, he argued, following Plato, that it was impossible for good to exist without evil, for justice could not be known without injustice, courage without cowardice, temperance without intemperance or wisdom without foolishness. Secondly, apparent evils exist as a consequent of nature's goodness, thus it was necessary for the human skull to be made from small and thin bones for reasons of utility, but this superior utility meant that the skull is vulnerable to blows. Thirdly, evils are distributed according to the rational will of Zeus, either to punish the wicked or because they are important to the world-order as a whole. Thus evil is good under disguise, and is ultimately conducive to the best. Chrysippus compared evil to the coarse jest in the comedy; for, just as the jest, though offensive in itself, improves the piece as a whole, \"so too you may criticize evil regarded by itself, yet allow that, taken with all else, it has its use.\"\nMathematics.\nChrysippus regarded bodies, surfaces, lines, places, the void and time as all being infinitely divisible. He determined one of the principal features of the infinite set: since a man and a finger have an infinite number of parts as do the universe and a man, it cannot be said that a man has more parts than his finger, nor that the universe has more parts than a man.\nChrysippus also responded to a problem first posed by Democritus. If a cone is divided by a plane parallel to its base, are the surfaces of the segments equal or unequal? If they are equal, then the cone becomes a cylinder; if they are unequal, then the surface of the cone must be stepped. The reply of Chrysippus was that the surfaces are both equal and unequal. Chrysippus was, in effect, negating the law of excluded middle with respect to the equal and unequal, and thus he may have anticipated an important principle of modern infinitesimal calculus, namely, the limit and the process of convergence towards a limit.\nChrysippus was notable for claiming that \"one\" is a number. One was not always considered a number by the ancient Greeks since they viewed one as that by which things are measured. Aristotle in his \"Metaphysics\" wrote, \"... a measure is not the things measured, but the measure or the One is the beginning of number.\" Chrysippus asserted that one had \"magnitude one\" (), although this was not generally accepted by the Greeks, and Iamblichus wrote that \"magnitude one\" was a contradiction in terms.\nEthics.\nChrysippus taught that ethics depended on physics. In his \"Physical Theses\", he stated: \"for there is no other or more appropriate way of approaching the subject of good and evil on the virtues or happiness than from the nature of all things and the administration of the universe.\" The goal of life, said Chrysippus, is to live in accordance with one's experience of the actual course of nature. A person's individual nature is part of the nature of the whole universe, and thus life should be lived in accordance with one's own human nature as well as that of the universe. Human nature is ethical, and humanity is akin to the Divine, emanating from the primal fire or aether, which, though material, is the embodiment of reason; and people should conduct themselves accordingly. People have freedom, and this freedom consists in emancipation from irrational desires (lust, riches, position in life, domination, etc.) and in subjecting the will to reason. Chrysippus laid the greatest stress on the worth and dignity of the individual, and on the power of will.\nThe Stoics admitted between the good and the bad a third class of things\u00a0\u2013 the indifferent (\"adiaphora\"). Of things morally indifferent, the best includes health, and riches, and honour, and the worst includes sickness and poverty. Chrysippus accepted that it was normal in ordinary usage to refer to the preferred indifferent things as \"good\", but the wise person, said Chrysippus, uses such things without requiring them. Practice and habit are necessary to render virtue perfect in the individual\u00a0\u2013 in other words, there is such a thing as moral progress, and character has to be built up.\n\"On Passions\".\nThe Stoics sought to be free of the unruly emotions, which they regarded as being contrary to nature. The passions or emotions (\"pathe\") are the disturbing element in right judgment. Chrysippus wrote a whole book, \"On Passions\" (), concerning the therapy of the emotions. The passions are like diseases which depress and crush the soul, thus he sought to eradicate them (\"apatheia\"). Wrong judgements turn into passions when they gather an impetus of their own, just as, when one has started running, it is difficult to stop. One cannot hope to eradicate the passions when one is in the heat of love or anger: this can only be done when one is calm. Therefore, one should prepare in advance, and deal with the passions in the mind as if they were present. By applying reason to passions such as greed, pride, or lust, one can understand the harm which they cause.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nWorks cited.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;\nFurther reading.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;\nExternal links.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "37145", "revid": "199747", "url": "https://en.wikipedia.org/wiki?curid=37145", "title": "Lucretius", "text": "1st-century BC Roman poet and philosopher\nTitus Lucretius Carus ( ; ; c.\u200999\u00a0\u2013 October 15, 55 BC) was a Roman poet and philosopher. His only known work is the philosophical poem \"De rerum natura\", a didactic work about the tenets and philosophy of Epicureanism, which usually is translated into English as \"On the Nature of Things\"\u2014and somewhat less often as \"On the Nature of the Universe\". \nVery little is known about Lucretius's life; the only certainty is that he was either a friend or client of Gaius Memmius, to whom the poem was addressed and dedicated. \"De rerum natura\" was a considerable influence on the Augustan poets, particularly Virgil (in his \"Aeneid\" and \"Georgics\", and to a lesser extent on the \"Eclogues\") and Horace. The work was almost lost during the Middle Ages, but was rediscovered in 1417 in a monastery in Germany by Poggio Bracciolini. It played an important role both in the development of atomism (Lucretius was an important influence on Pierre Gassendi) and the efforts of various figures of the Enlightenment era to construct a new Christian humanism.\nLife.\n&lt;templatestyles src=\"Template:Quote_box/styles.css\" /&gt;\nAnd now, good Memmius, receptive ears And keen intelligence detached from cares I pray you bring to true philosophy\n\"De rerum natura\" (tr. Melville) 1.50\n&lt;templatestyles src=\"Template:Quote_box/styles.css\" /&gt;\nIf I must speak, my noble Memmius, As nature's majesty now known demands\n\"De rerum natura\" (tr. Melville) 5.6\nVirtually nothing is known about the life of Lucretius, and there is insufficient basis for a confident assertion of the dates of Lucretius's birth or death in other sources. Another, yet briefer, note is found in the \"Chronicon\" of Donatus's pupil, Jerome. Writing four centuries after Lucretius's death, he enters under the 171st Olympiad: \"Titus Lucretius the poet is born.\" If Jerome is accurate about Lucretius's age (43) when Lucretius died (discussed below), then it may be concluded he was born in 99 or 98 BC. Less specific estimates place the birth of Lucretius in the 90s BC and his death in the 50s BC, in agreement with the poem's many allusions to the tumultuous state of political affairs in Rome and its civil strife.\nLucretius probably was a member of the aristocratic \"gens Lucretia\", and his work shows an intimate knowledge of the luxurious lifestyle in Rome. Lucretius's love of the countryside invites speculation that he inhabited family-owned rural estates, as did many wealthy Roman families, and he certainly was expensively educated with a mastery of Latin, Greek, literature, and philosophy.\nA brief biographical note is found in Aelius Donatus's \"Life of Virgil\", which seems to be derived from an earlier work by Suetonius. The note reads: \"The first years of his life Virgil spent in Cremona until the assumption of his \"toga virilis\" on his 17th birthday (when the same two men held the consulate as when he was born), and it so happened that on the very same day Lucretius the poet passed away.\" However, although Lucretius certainly lived and died around the time that Virgil and Cicero flourished, the information in this particular testimony is internally inconsistent: if Virgil was born in 70 BC, his 17th birthday would be in 53. The two consuls of 70 BC, Pompey and Crassus, stood together as consuls again in 55, not 53.\nAnother note regarding Lucretius's biography is found in Jerome's \"Chronicon\", where he contends that Lucretius \"was driven mad by a love potion, and when, during the intervals of his insanity, he had written a number of books, which were later emended by Cicero, he killed himself by his own hand in the 44th year of his life.\" The claim that he was driven mad by a love potion, although defended by such scholars as Reale and Catan, is often dismissed as the result of historical confusion, or anti-Epicurean bias. In some accounts the administration of the toxic aphrodisiac is attributed to his wife Lucilia. Regardless, Jerome's image of Lucretius as a lovesick, mad poet continued to have significant influence on modern scholarship until quite recently, although it now is accepted that such a report is inaccurate.\n\"De rerum natura\".\nHis poem \"De rerum natura\" (usually translated as \"On the Nature of Things\" or \"On the Nature of the Universe\") transmits the ideas of Epicureanism, which includes atomism and cosmology. Lucretius was the first writer known to introduce Roman readers to Epicurean philosophy. The poem, written in some 7,400 dactylic hexameters, is divided into six untitled books, and explores Epicurean physics through richly poetic language and metaphors. Lucretius presents the principles of atomism, the nature of the mind and soul, explanations of sensation and thought, the development of the world and its phenomena, and explains a variety of celestial and terrestrial phenomena. The universe described in the poem operates according to these physical principles, guided by \"fortuna\", \"chance\", and not the divine intervention of the traditional Roman deities and the religious explanations of the natural world.\nWithin this work, Lucretius makes reference to the cultural and technological development of humans in his use of available materials, tools, and weapons through prehistory to Lucretius's own time. He specifies the earliest weapons as hands, nails, and teeth. These were followed by stones, branches, and fire (once humans could kindle and control it). He then refers to \"tough iron\" and copper in that order, but goes on to say that copper was the primary means of tilling the soil and the basis of weaponry until, \"by slow degrees\", the iron sword became predominant (it still was in his day) and \"the bronze sickle fell into disrepute\" as iron ploughs were introduced. He had earlier envisaged a pre-technological, pre-literary kind of human whose life was lived \"in the fashion of wild beasts roaming at large\". From this beginning, he theorised, there followed the development in turn of crude huts, use and kindling of fire, clothing, language, family, and city-states. He believed that smelting of metal, and perhaps too, the firing of pottery, was discovered by accident: for example, the result of a forest fire. He does specify, however, that the use of copper followed the use of stones and branches and preceded the use of iron.\nLucretius seems to equate copper with bronze, an alloy of copper and tin that has much greater resilience than copper; both copper and bronze were superseded by iron during his millennium (1000 BC to 1 BC). He may have considered bronze to be a stronger variety of copper and not necessarily a wholly individual material. Lucretius is believed to be the first to put forward a theory of the successive uses of first wood and stone, then copper and bronze, and finally iron. Although his theory lay dormant for many centuries, it was revived in the nineteenth century and he has been credited with originating the concept of the three-age system that was formalised from 1834 by C. J. Thomsen.\nReception.\nIn a letter by Cicero to his brother Quintus in February 54 BC, Cicero said: \"The poems of Lucretius are as you write: they exhibit many flashes of genius, and yet show great mastership.\" In the work of another author in late Republican Rome, Virgil writes in the second book of his \"Georgics\", apparently referring to Lucretius, \"Happy is he who has discovered the causes of things and has cast beneath his feet all fears, unavoidable fate, and the din of the devouring Underworld.\"\nNatural philosophy.\nLucretius was an early thinker in what grew to become the study of evolution. He believed that nature experiments endlessly across the eons, and the organisms that adapt best to their environment have the best chance of surviving. Living organisms survived because of the commensurate relationship between their strength, speed, or intellect and the external dynamics of their environment. Prior to Charles Darwin's 1859 publication of \"On the Origin of Species\", the natural philosophy of Lucretius typified one of the foremost non-teleological and mechanistic accounts of the creation and evolution of life. In contrast to modern thought on the subject, he did not believe that new species evolved from previously existing ones. Lucretius challenged the assumption that humans are necessarily superior to animals, noting that mammalian mothers in the wild recognize and nurture their offspring as do human mothers.\nWhile Epicurus left open the possibility for free will by arguing for the uncertainty of the paths of atoms, Lucretius viewed the soul or mind as emerging from fortuitous arrangements of distinct particles.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nBibliography.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "37146", "revid": "43056943", "url": "https://en.wikipedia.org/wiki?curid=37146", "title": "New Sweden", "text": "Swedish colony in North America (1638\u20131655)\nNew Sweden () was a colony of the Swedish Empire between 1638 and 1655 along the lower reaches of the Delaware River in what is now Delaware, Maryland, New Jersey, and Pennsylvania. Established during the Thirty Years' War when Sweden was a great power, New Sweden formed part of the Swedish efforts to colonize the Americas.\nSettlements were established on both sides of the Delaware River. Fort Christina, located in what is now Wilmington, Delaware, was the first settlement, named after Christina, Queen of Sweden. The settlers were Swedes, Finns, and a number of Dutch. New Sweden was conquered by the Dutch Republic in 1655 and incorporated into the Dutch colony of New Netherland.\nHistory.\nBy the middle of the 17th century, Sweden had reached its greatest territorial extent, encompassing Finland and Estonia, as well as parts of present-day Russia, Poland, Germany, Norway and Latvia. It was one of the great powers of Europe during the \"stormaktstiden\" (\"Age of Greatness\" or \"Great Power Period\"). At the same time, other European nations were establishing colonies in the New World and building successful trading empires. Sweden sought to expand its own influence by creating a tobacco plantation and fur-trading colony, aiming to bypass French, English and Dutch merchants.\nThe Swedish South Company (also known as the Company of New Sweden) was founded in 1626 with a mandate to establish colonies between Florida and Newfoundland for the purposes of trade, particularly along the Delaware River. Its charter included Swedish, Dutch, and German stockholders. The directors of the company included Flemish/Dutch merchant Samuel Blommaert. The company sponsored 11 expeditions in 14 separate voyages to Delaware between 1638 and 1655; two were lost.\nThe first Swedish expedition to America sailed from the port of Gothenburg in late 1637, organized and overseen by Clas Larsson Fleming, a Swedish admiral from Finland. Blommaert assisted the fitting-out and appointed Peter Minuit (the former Governor of New Netherland) to lead the expedition. The expedition sailed into Delaware Bay aboard the \"Fogel Grip\" and \"Kalmar Nyckel\"; territory that was claimed by the Dutch. They passed Cape May and Cape Henlopen in late March 1638 and anchored on March 29 at a rocky point on the Minquas Kill that is known today as Swedes' Landing. They built a fort at the confluence of the Christina River and Brandywine Creek which they named Fort Christina after their Queen.\nIn the following years, the area was settled by roughly 600 Swedes and Finns, a number of Dutchmen, a few Germans, a Dane, and at least one Estonian. Minuit served as the first governor of the colony of New Sweden. He had been the third Director of New Netherland, and he knew that the Dutch claimed the area surrounding the Delaware River and its bay. The Dutch West India Company, however, had withdrawn its settlers from the area in order to concentrate on the settlement on Manhattan Island, leaving Fort Nassau on the east side of the Delaware River as the only Dutch outpost on the Delaware River.\nMinuit landed on the west bank of the river and met with the sachems of the Lenape and Susquehannock. They held a conclave in Minuit's cabin on the \"Kalmar Nyckel\", and he persuaded the Lenape to sign deeds which he had prepared to resolve any issue with the Dutch. The Swedes claimed that the purchase included land on both sides of the South (Delaware) River from the Schuylkill River down to Delaware Bay in what is now Pennsylvania, Delaware and Maryland. Lenape sachem Mattahoon later claimed that the purchase only included as much land as was contained within an area marked by \"six trees\", and the rest of the land occupied by the Swedes was stolen.\nThe Director of New Netherland, Willem Kieft, objected to the Swedish presence, but Minuit ignored him since he knew that the Dutch were militarily weak at the moment. Minuit completed Fort Christina, then sailed for Stockholm to bring a second group of settlers. He made a detour to the Caribbean to pick up a shipment of tobacco to sell in Europe in order to make the voyage profitable; however, he died on this voyage during a hurricane at St. Christopher in the Caribbean. The official duties of the governor of New Sweden were carried out by Captain M\u00e5ns Nilsson Kling, until a new governor was selected and arrived from Sweden two years later.\nThe colony expanded along the river under the leadership of Johan Bj\u00f6rnsson Printz, governor from 1643 to 1653. They established Fort Nya Elfsborg on the east bank of the Delaware near what is now Salem, New Jersey, and Fort Nya Gothenborg on Tinicum Island. Printz built his manor house, The Printzhof, at Fort Nya Gothenborg, and the Swedish colony prospered for a time. New Sweden established a strong trading relationship with the Susquehannock and supported them in their war against Maryland colonists.\nConquest of New Sweden.\nIn 1651, the Dutch West India Company abandoned Fort Nassau and established Fort Casimir on the west side of the Delaware River a few miles south of Fort Christina. In May 1654, soldiers from New Sweden led by Governor Johan Risingh captured Fort Casimir and renamed it Fort Trinity (\"Trefaldigheten\" in Swedish). In November 1654, the directors of the Dutch West India Company ordered the Director-General of New Netherland, Peter Stuyvesant, to \"drive\" the Swedes from the river.\nIn the summer of 1655, Stuyvesant sailed from New Amsterdam to Delaware Bay with 7 ships and 317 soldiers and quickly retook Fort Casimir (Fort Trinity). Stuyvesant then proceeded to besiege Fort Christina which surrendered on September 15, 1655. During the siege, the Dutch plundered houses and killed livestock in the vicinity of the fort. New Sweden was formally incorporated into New Netherland although the Swedish and Finnish settlers were allowed local autonomy. They retained their own militia, religion, court, and lands. This lasted until the English conquest of New Netherland in 1664 at the beginning of the Second Anglo-Dutch War. The conquest began on August 29, 1664, with the capture of New Amsterdam and ended with the capture of Fort Casimir in October.\nIn 1669, New Sweden was under English rule, but most of the population was still Swedish. A man named Marcus Jacobsson, posing as a member of the K\u00f6nigsmarck family, attempted to instigate a rebellion against the English to return New Sweden to Swedish rule. The rebellion, known as the Revolt of the Long Swede due to Jacobsson's height, failed. Jacobsson was sold into indentured servitude in Barbados and the families that had supported him were fined for their participation in the revolt.\nNew Sweden continued to exist unofficially, and some immigration and expansion continued. The first settlement at Wicaco began with a Swedish log blockhouse located on Society Hill in Philadelphia in 1669. It was later used as a church until about 1700, when Gloria Dei (Old Swedes') Church of Philadelphia was built on the site.\nHoarkill, New Amstel, and Upland.\nOn September 12, 1673, following the Dutch recapture of the Delaware region from the Third Anglo-Dutch War, Governor Anthony Colve's council erected three territorial courts\u2014Hoarkill, New Amstel, and Upland\u2014whose jurisdictions correspond to the modern counties of Sussex, New Castle and the extinct Upland (later partitioned between Pennsylvania and Delaware).\nThe Treaty of Westminster of 1674 ended the second period of Dutch control and required them to return all of New Netherland to the English on June 29, including the three counties which they created. After taking stock, the English declared on November 11 that settlements on the west side of the Delaware River and Delaware Bay were to be dependent on the Province of New York, including the three Counties. This declaration was followed by a declaration that renamed New Amstel as New Castle. The other counties retained their Dutch names.\nThe next step in the assimilation of New Sweden into New York was the extension of the Duke's laws into the region on September 22, 1676. This was followed by the partition of some Upland Counties to conform to the borders of Pennsylvania and Delaware, with most of the Delaware portion going to New Castle County on November 12, 1678. The remainder of Upland continued in place under the same name. On June 21, 1680, New Castle and Hoarkill Counties were partitioned to produce St. Jones County.\nOn March 4, 1681, what had been the colony of New Sweden was formally partitioned into the colonies of Delaware and Pennsylvania. The border was established 12 miles north of New Castle, and the northern limit of Pennsylvania was set at 42 degrees north latitude. The eastern limit was the border with New Jersey at the Delaware River, while the western limit was undefined. In 1682, Upland ceased to exist as the result of the reorganization of the Colony of Pennsylvania, with the Upland government becoming the government of Chester County, Pennsylvania.\nOn August 24, 1682, the Duke of York transferred the western Delaware River region to William Penn, including Delaware, thus transferring Deale County and St. Jones County from New York to Delaware. St. Jones County was renamed Kent County, Deale County was renamed Sussex County, and New Castle County retained its name.\nSwedish explorer and botanist Pehr Kalm visited the descendants of the early Swedish immigrants to New Sweden in the mid-18th century and documented their experiences with the Native American Indians who resided in those parts, in a book entitled \"Travels into North America\".\nSignificance and legacy.\nHistorian H. Arnold Barton has suggested that the greatest significance of New Sweden was the strong and lasting interest in America that the colony generated in Sweden, although major Swedish immigration did not occur until the late 19th century. From 1870 to 1910, more than one million Swedes arrived in America, settling particularly in Minnesota and other states of the Upper Midwest.\nTraces of New Sweden persist in the lower Delaware valley, including Holy Trinity Church in Wilmington, Delaware, Gloria Dei Church and St. James Kingsessing Church in Philadelphia, Trinity Episcopal Church in Swedesboro, New Jersey, and Christ Church in Swedesburg, Pennsylvania. All of those churches are commonly known as \"Old Swedes' Church\". The town of Kristina (now Christiana, Delaware), named after the Swedish queen Kristina, is one of the few settlements in the area retaining a Swedish name, and the town of Uppland survives as Upland, Pennsylvania. Swedesford Road is still found in Chester and Montgomery Counties, Pennsylvania, although Swedesford has long since become Norristown. Swedeland, Pennsylvania, is part of Upper Merion Township in Montgomery County. The American Swedish Historical Museum in South Philadelphia houses many exhibits, documents, and artifacts from the New Sweden colony.\nPerhaps the greatest contribution of New Sweden to the development of the New World is the log house building technique. The colonists of New Sweden brought with them the log cabin, which became such an icon of the American frontier that it is commonly thought of as an American structure. The C. A. Nothnagle Log House on Swedesboro-Paulsboro Road in Gibbstown, New Jersey, is one of the oldest surviving log houses in the United States. Cabin floor plans, such as the dogtrot can be traced to Finnish colonists in New Sweden, as can split-rail fences.\nFinnish influence.\nThe settlers came from all over the Swedish realm and many of them were Finnish-speaking. The proportion of Finns in New Sweden grew especially towards the end of the period of colonization. Finns composed 22 percent of the population during Swedish rule, and rose to about 50 percent after the colony came under Dutch rule. A contingent of 140 Finns arrived in 1664, and the ship \"Mercurius\" brought another 106 settlers in 1665, 92 of whom were listed as Finns. Their early presence is reflected in place names near the Delaware River such as Finland (Marcus Hook), Torne, Lapland, Finns Point, Mullica Hill, and Mullica River.\nThe exact number of Finns is difficult to ascertain, as the colonists' language was not recorded and Finnish names were distorted in the official records written in Swedish. Identifications of some surnames, such as \"Rambo\", \"Cock\", or \"Stille\", as derivations of East Finnish names (\"Romppainen\", \"Kokkoinen\", \"Hiljakainen\") have been criticized by the historian Kari Tarkiainen, as the bearers of these names had Swedish-sounding given names or patronymics. Also in some other cases, the identification is based on weak evidence. In some records, Finnish origin was explicitly indicated with the term \"Finn\".\nMany of the Finnish inhabitants of New Sweden did not come directly from Finland, but from the \"Finnskogen\" (\"Finn forests\") of central Sweden. These communities had formed in the late 1500s and early 1600s, when tens of thousands of Savo Finns migrated from Finland to sparsely populated forested regions, especially V\u00e4rmland and neighboring provinces. Their migration was encouraged during the reigns of Charles IX and Gustavus Adolphus. These Forest Finns practiced slash-and-burn agriculture, a method also used by the local Indigenous Lenape Indians along the Delaware River. By 1630s this practice had become a source of friction with Swedish authorities, who accused the Finns of destroying valuable timber resources.\nIn the early 1640s the Swedish Crown considered relocating some of the Forest Finns to New Sweden. In 1640 several Finns who had been sentenced for unlawful slash-and-burn farming in V\u00e4rmland petitioned to be sent to the colony. By 1643 provincial governors received orders to capture and send such forest-destroying Finns to Delaware. A small number of petty criminals from Finland were also sent to the colony, but forced migration never became extensive. By the late 1640s, Forest Finns had grown enthusiastic about opportunities in New Sweden, and hundreds of volunteers petitioned for permission to emigrate.\nFinnish language did not persist in Delaware, unlike Swedish, which was used in the church and remained in use until the 18th century.\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\nCitations.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "37147", "revid": "27015025", "url": "https://en.wikipedia.org/wiki?curid=37147", "title": "Tilburg", "text": "City in North Brabant, Netherlands\nTilburg () is a city and municipality in the Netherlands, in the southern province of North Brabant. With a population of 229,833 (January 2, 2024), it is the second-largest city or municipality in North Brabant after Eindhoven and the seventh-largest in the Netherlands.\nTilburg University is located in Tilburg, as are Avans University of Applied Sciences and Fontys University of Applied Sciences.\nThere are three railway stations within the municipality: Tilburg, Tilburg Universiteit and Tilburg Reeshof. The \"Spoorzone\" area around Tilburg Central station, once a Dutch Railways train maintenance yard, has been purchased by the city and is being transformed into an urban zone.\nHistory.\nLittle is known about the beginnings of Tilburg. The name \"Tilliburg\" first appeared in documents dating from AD 709, but after that there was no mention for several centuries. In the later Middle Ages, Tilburg referred to a region rather than a particular town or village; its population was largely in a couple of hamlets, one of which was known as \"Eastern Tilburg\" (\"Oost-Tilburg\"), which was later reflected in the name \"Oisterwijk\" (\"Eastern Quarter\"). This village centred around a small (probably wooden) castle or \"Motteburcht\" on an equally small hill, which became derelict and was torn down after a few centuries at most. Of this first \"Tilburg Castle\", nothing remained c.\u00a02000, except for a few remnants of its moat in the suburbs of Oisterwijk. In the 14th century, Tilburg was proclaimed a manor; together with Goirle, it acquired the title of \"The Manor of Tilburg and Goirle\".\nSuccessively, the manorial rights fell into the hands of several lords of noble lineage. They derived their income from taxes, fines and interest paid by the villagers.\nIn the 15th century, one of the lords of Tilburg, Jan van Haestrecht, built Tilburg Castle. \"That stone chamber at Hasselt\" is mentioned in several historical documents. In 1858, however, the castle was pulled down to make way for a factory, but the name lives on, in the city arms and logo. A replica of the foundations of the castle was restored in c.\u20091995 in its original location, after the factory was demolished. In 1803, Goirle was separated from Tilburg and on 18 April 1809, Tilburg was granted city status. In that year, it had about 9,000 inhabitants. In 2009 Tilburg hosted several festivities in celebration of 200 years as a city.\nWool capital of the Netherlands.\nTilburg grew around one of the so-called \"herd places\" or \"Frankish triangles\", triangular plots where a number of roads (usually sand roads) met. These herd places were collective pasturelands for flocks of sheep. Their shape is still reflected in the layout of many places in Tilburg. Many districts, including Korvel, Oerle, Broekhoven, Hasselt, Heikant, De Schans, and Heuvel, bear the names of these old hamlets. The poor farmers living in these hamlets soon decided not to sell the wool from their sheep but to weave it themselves, and for a long time, much of the space inside their small houses was occupied by a loom\u2014by the 17th century these numbered about 300. Enterprising people saw their chance. As so-called \"drapers\", they supplied the weavers with the raw materials for their \"home working\", and the first Tilburg \"mill houses\" came into existence. From then on, the wool industry underwent rapid growth, and in 1881 Tilburg had as many as 145 wool mills. Home weaving continued, however, until the early 20th century. Woollen textiles from Tilburg were known far and wide.\nAfter World War\u00a0II Tilburg retained its place as wool capital of the Netherlands, but in the 1960s the industry collapsed and by the 1980s the number of operating wool mills had declined. Present-day Tilburg industry consists of a wide variety of enterprises. The main economic sector has become transport and logistics with a variety of industry as a close second.\nUrban renewal.\nWhen the wool industry collapsed in the 1960s, Cees Becht was the mayor of Tilburg. While he was in office, many old buildings were destroyed, including some precious monuments. The Koningswei neighbourhood (King's Meadows) was demolished and replaced by Koningsplein (King's Square). The reason for demolishing the neighbourhood was to replace slum with modern building. The new development, however, has not been successful and many feel that the square feels abandoned most of the year. Considered even worse was the demolition of the old city hall. This classicistic-styled building was a national-registered monument, but even that did not stop Becht's plans to demolish it to build the nine-storey, modern-day, black complex. A part of the empty area was used to build the system of the inner \"Cityring\". Another building that was demolished was the old railway station, which was replaced due to \"Hoogspoor\" (literally: high rails), a project bringing the railway on viaducts to reduce traffic congestion around 1960. The century-old station building was replaced by the modern one. Because of all of this and some more parts of Tilburg, Cees Becht gained the dubious nickname \"Cees de Sloper\" (Cees the demolisher).\nModern history.\nIn the 1980s, many locations, formerly occupied by wool factories had been filled with small-scale housing projects. This mostly happened when Henk Letschert was mayor of Tilburg. The \"Heuvel\", one of the important squares, had its own lime tree until 27 April 1994, being chopped down for a bicycle parking basement. The felling led to many protests, because the tree was still healthy. After the Pieter Vreedeplein reconstruction, plans were made to plant a descendant of the original lime tree. Three were placed, only one of them survived. The last living tree was moved to another location again, but died shortly after. As of 23 November 2011, no more descendants have been placed. The current one is just another lime tree. In the 1990s Tilburg developed a modern skyline. Because of new policy three buildings were built, which are considered skyscrapers in the Netherlands. These are the Interpolis headquarters, the Westpoint Tower and StadsHeer. The Westpoint Tower has an altitude of and was the tallest residential tower in The Netherlands until the Montevideo in Rotterdam surpassed it. The 'StadsHeer' is the third one and is part of the 'Haestrechtkwartier' (Haestrecht quarter). The residential tower is nicknamed \"De Vogelkooikes\" (The Bird Cages) for its cubic balconies taped onto the building.\nKing William II.\nKing William\u00a0II (1792\u20131849) was fond of Tilburg. \"Here I can breathe freely and I feel happy\", he once said about the town. King William\u00a0II always supported Tilburg\u2014he provided money to improve the sheep breeding, built new farms and founded a cavalry barracks on St.\u00a0Joseph Street, now a monumental building of the City Archives. Although the King was always made welcome by the manufacturers he had befriended, he needed his own residence in Tilburg, and commissioned the construction of a palace, which would function as his country residence. Construction started in 1847 and was completed just days before William II died, in 1849. It is now part of Tilburg City Hall. In 1987 an obelisk was erected nearby, in memory of King William\u00a0II. It replaced the old \"needle\" dating from 1874, which was removed from the street in 1968. After its restoration, William\u00a0II's statue has got a place again in the heart of the city, where he felt happy among its inhabitants. The local football club Willem\u00a0II Tilburg was named after the king.\nTopography.\nTilburg Centrum.\nTilburg Centrum is the downtown of Tilburg, and is situated between (clockwise) the Spoorlaan, Heuvelring, Paleisring, Schouwburgring and Noordhoekring, which is the same as the order of the one-way roads around the district. The district has 6,572 inhabitants, and most of the shops, hotels, restaurants and cafes of the city. In 2008, the refurbished Pieter Vreedeplein was opened to the public, addressing a lack of shopping facilities as compared to similar-sized cities in the Netherlands. Two smaller cinemas were replaced by a bigger one on the Pieter Vreedeplein in 2007. Despite being called \"Centrum\", the district is some distance southeast of the geographical center. The district is connected by the Tilburg railway station.\nOud-Noord.\nOud-Noord is situated north of the railway that crosses Tilburg, and between the \"Ringbanen\" (ring roads around the city center). The district has 33,915 inhabitants. Contemporary arts museum \"De Pont\" is located within the district. When the railway marshalling yard belonging to the Nederlandse Spoorwegen became obsolete, a considerable stretch of the railway across the city, the \"Spoorzone\", became an urban renewal project. New premises for two courses run by Fontys University of Applied Sciences will be located here, as will Tilburg's new central library, replacing the library in Koningsplein. The railway yard is the largest area, though more areas along the railway will be reconstructed.\nOud-Zuid.\nOud-Zuid is a district south, and also west and east of downtown Tilburg. The district has 38,659 inhabitants. As of 2012[ [update]], all the 'skyscrapers' of Tilburg, higher than are located within the district. The \"Hart van Brabantlaan\" is almost surrounded by high buildings like Westpoint Tower and the StadsHeer as a small part of the urban renewal. This area along the railway is partly located in \"Oud-Zuid\". Many important locations in Tilburg are located within the district, just out of the center, such as 013 music venue and the Schouwburg built in 1961. Also the Koningsplein with the main library and the Piushaven are located within the district. Old herd places include Korvel, Broekhoven and Oerle.\nNoord.\nTilburg-Noord is located north of the Wilhelmina Canal. The district has 23,340 inhabitants. Tilburg-Noord was built in the period 1966\u20131974. Therefore, it has many apartment buildings up to 16 floors, drive-in houses, green strips and industrial development. The streets in this district are mostly named after musicians from the Renaissance up to pop artists from the 1960s. The main shopping center is Wagnerplein, while there's also the Verdiplein in Stokhasselt. The one at the Tartinistraat became defunct. Before the district was built, it was mainly an agricultural area attached to a few villages, including Heikant, which is still the name of the biggest neighbourhood. Heikant's former village square, including the old church, is still present. The northernmost part of the district is still agricultural with some forests. In this agricultural area, the blessed Peter Donders was born; there still stands a chapel and a procession park.\nOost.\nTilburg-oost consists of primarily industrial development. Residential neighbourhoods are in a small strip east of the Ringbaan Oost rather than the whole district, however, it is not considered as a part of the city center. The district only has 748 inhabitants.\nZuid.\nTilburg-Zuid is located between the A58 motorway and the Ringbaan Zuid, and is the southernmost district. Tilburg-zuid has 19,149 inhabitants. The district contains two neighbourhoods and many businesses. The football club Willem II is located within the district, as well as the ice-skating rink with a speed skating rink, the Ireen W\u00fcst IJsbaan, which is located here. The main campus of Fontys University of Applied Sciences is located in this district, as well as St. Elisabeth hospital and Leijpark, one of the largest public parks in the city.\nWest.\nTilburg-west was mostly built after WWII, and has 26,655 inhabitants. The district with its neighbourhoods consists mostly of small brick houses and apartment buildings, except for Zorgvlied, which contains more expensive, free-standing houses. The Westermarkt is the largest shopping center out of the inner city. Many higher educational buildings are standing here, like Tilburg University and Avans Hogeschool. Another place of many schools is along the Reitse Hoevenstraat with multiple secondary schools such as: Jozefmavo and Theresialyceum. The district is connected by train with the Tilburg Universiteit railway station, Previously known as \"Tilburg West station\" and has one of the two hospitals in Tilburg (TweeSteden ziekenhuis). The largest mosque of Tilburg, the Turkish S\u00fcleymaniye-Mosque built in 2001, stands in the southeastern corner of the district. West is surrounded by forests like \"Wandelbos\" and the \"Oude Warande\", located west of the university.\nReeshof.\nThe Reeshof is the westernmost district and the most recent expansion of the city of Tilburg proper. and has a population of 42,994 inhabitants. Because of this, the Reeshof became the largest district of Tilburg by population. The first houses were completed in 1980, in the neighbourhood Gesworen Hoek. As of 2012[ [update]], the last neighbourhood (Koolhoven Buiten) is under construction. The district is connected by the Tilburg Reeshof railway station and multiple roads that encircle the district plus the industrial development Vossenberg north of the Wilhelmina Canal. The Donge runs through the district, including greenspace with some Highland cattle grazing between the fences protecting the surrounding neighbourhoods. This small-scale nature project is called the Dongevallei, which literally means Donge Valley in English.\nDemographics.\nEthnic makeup.\nThe population of Tilburg was 222,601 on 1 July 2021. According to the Tilburg city council, the city will reach a population of 217,000 by 2025. Of these, 23.3% (47,964 people) are of foreign descent. People are classified as being of foreign descent when they were born outside of the Netherlands, or when at least one of their parents was born outside of the Netherlands.\nReligion and life stances.\nThe Tilburg agglomeration has the following religious makeup as of 2003:\nGeography.\nClimate.\nTilburg experiences an oceanic climate (K\u00f6ppen climate classification \"Cfb\") similar to almost all of the Netherlands. Thunderstorms occur in Western Brabant more often than anywhere else in the Netherlands, up to 31 days a year.\nEconomy.\nThe economy was concentrated on wool industry for centuries, however, since the 1960s, Tilburg has made more progress in having different kinds of industries, supported by the government to save the city from poverty after the decline of wool industry. Chemical company IFF has a factory in Tilburg. In the 1980s, the Japanese company Fujifilm came to Tilburg. Insurance companies like Interpolis and CZ are headquartered in Tilburg, as well. Iris Ohyama has its European offices in Tilburg. Since 2013, the electric car-producing company Tesla operates their main EU facility for assembly and distribution in Europe in the industrial area of Vossenberg north of the suburb \"De Reeshof\" in Tilburg.\nTilburg has a high concentration of transportation/distribution industries, specializing in value added logistics and services, due to being the geographical center of the Benelux countries and being located on the transport corridor between Antwerp / Rotterdam and the Ruhr area. The 'Waalwijk-Tilburg' region has been in the logistics hotspots top 3 within the Netherlands for years now and finished third in 2017.\nEducation.\nTilburg University.\nHigher education is of significant importance, with Tilburg University attracting scholars from all over the world. It has a student population of about 13,000 students, about 8 per cent of whom are international students. With well-facilitated Library, museums and city center with many pubs and cafes, this percentage has steadily increased over the past years. TiU offers both Dutch-taught and English-taught programmes.\nThe institution has gained a reputation in both research and education. In the field of economics, the Faculty of Economics and Business Administration ranked No. 1 in Europe for the second consecutive time in 2007 according to the Journal of the European Economic Association with regard to publications in top journals. In 2007 the Executive MBA program at the university's TiasNimbas Business School ranked # 11 in the world according to the \"Financial Times\".\nFontys School of Fine and Performing Arts.\nTilburg is also the location of the Fontys School of Fine and Performing Arts (Dutch: \"Fontys Hogeschool voor de Kunsten\" - \"FHK\"), part of the Fontys Hogescholen. The School originated from the merging of various educational institutions that had existed in different capacity in Tilburg before being united under the Fontys group, such as the Brabants Conservatorium, one of the nine conservatoires in the Netherlands, and the Academie voor Beeldende Vorming.\nFontys School of Arts offers various bachelor and master programmes in English and in Dutch, across different fields in music, visual arts, dance, theatre and performing arts\nThis institution is based in a building known as 'Kunstkluster', located in the centre of the city next to the Schouwburg and incorporating a Concert Hall.\nCulture and recreation.\nTilburg is a pilot city of the Council of Europe and the EU Intercultural Cities programme.\nBeverages.\nSchrobbel\u00e8r is a local liqueur. It has an alcoholic percentage of 21.5%, slightly lower than most bitters and has a relatively sweet flavour. The drink is sold in a stone jar and is drunk cold from its own glass, a high and tiny chalice glass, larger than a J\u00e4germeister glass. The drink originated in 1973 when Tilburgian entrepreneur Jan Wassing started experimenting with a drink with lower alcoholic percentage that was appropriate for his stomach. The result was successful. The drink is distilled now at Loven industrial area in Tilburg by the Eindhoven company Schrobbeler Ltd, without the \u00e8 on the last vowel. The drink is especially consumed at Carnival. The name is derived from the profession of 'Schrobbelaar', in the textile industry in Tilburg. The profession was unskilled and had a low wage.\nAnother known drink from Tilburg is Peerke's Nat, which has a higher alcoholic percentage than Schrobbel\u00e8r (25%) and was introduced at the beatification of Peter Donders (locally named Peerke). The drink is sold in bottles of 70 centiliters.\nThe Koningshoeven Brewery brews trappist beer. It was founded in 1884 at Koningshoeven Abbey.\nOpen air art.\nTilburg's open air art is mostly supported by KORT (Kunst in Open Ruimte Tilburg, Dutch for Art in Open Space Tilburg). One example is the turning house on the Hasseltrotonde, a roundabout, was erected in 2008.\nBesides being responsible for newer, modern art, KORT also gives information about older works of art, like the Willem II statue on the Heuvel.\nFestival city, music.\nThe city of Tilburg hosts many festivals, such as Incubate, \"Festival Mundial\" (world culture), Stranger Than Paranoia (jazz), Tilburg Students Festival, and Roadburn Festival. \"013\" is a modern pop-centre. Paradox is a club for experimental jazz and improvised music. Fontys University of Applied Sciences started a pop academy in the beginning of the 21st century, and students often perform on local stages.\nMuseums.\nTilburg has a renowned museum of contemporary art, De Pont, which houses works from artists such as Ai Weiwei, Anish Kapoor and Richard Serra. The museum, which opened in 1992, is housed in a former wool mill, an important piece of Tilburg's history. Anish Kapoor's artwork \"'Skymirror\"' is displayed on the square in front of the museum. Also in the field of contemporary arts is the SEA Foundation Tilburg, an internationally oriented art foundation, exhibition space and artist residence for artists, writers and curators. Tilburg also hosts a significant textile museum, offering not only a historical view in its former factory, but also a laboratory for design, production and development of textile as a material. It opened in 1958.\nThere is also the \"Natuurmuseum Brabant\" (Brabant Museum of Nature), dedicated to natural history. Originally named the Natural History Museum of Tilburg (\"Natuurhistorisch Museum Tilburg),\" the museum was established in 1935. The museum was originally housed in the former intendant residence of King Willem II, but moved in 1985 to its current location, a former technical school near the railway station. The collection consists of a large variety of stuffed animals, animals kept in formaldehyde, dried plants, stones, minerals, fossils and archaeological artfacts.\nThe Museum Scryption was a former museum in Tilburg with the main theme 'written communication'. It closed in 2011.\nTilburg City Museum.\nThe Tilburg City Museum (\"Stadsmuseum Tilburg)\" does not have a fixed location. It manages the city's heritage collection and creates exhibitions at diverse venues. For example, the Peerke Donders Pavilion and Vincent's Drawing Room.\nThe collection that \"Stadsmuseum Tilburg\" manages is part of Tilburg City Collection. In addition, it manages the Memory of Tilburg with more than 4400 stories.\nParks and Forests.\nParks and forests provide inhabitants from the Tilburg area with recreation. The \"Leijpark\" and \"Reeshofpark\" are the largest parks. Leijpark was famous for Festival Mundial and lies next to St. Elisabeth hospital and a monastery, the Cenakel. Reeshofpark was created in the late 1990s, including some restaurants opened in 2011. Some older parks include Wilhelminapark in Oud-Noord, are built on the square of the former herd place Veldhoven. Tilburg offers, in comparison to other top-ten cities in the Netherlands, the most forest area. In the municipality, Tilburg has the Wandelbos, a forest south of the similarly named neighbourhood in Tilburg-West, the Oude Warande, the Kaaistoep, a forest of 4.5\u00a0km2, and partially, Huis Ter Heide in the northwest of Tilburg, a 6.5\u00a0km2-sized natural redevelopment area. Out of the municipality, there's a national park called Loonse en Drunense Duinen which includes dunes of drift sand from the west coast.\nSports.\nThe local football team is Willem II, named in remembrance of King William II.\nTilburg Ten Miles is an annual road running competition held in Tilburg.\nStudents' sports like rowing and hockey are popular as well. Tilburg hosts three field hockey clubs that play in top national leagues.\nTilburg has an ice skating rink, including the indoor 400\u00a0m speed skating rink Ireen W\u00fcst IJsbaan. Within the speed skating rink there's an ice hockey field. The hockey team Tilburg Trappers dominated the Eredivisie (Dutch Premier League) for years before moving to the Oberliga, the third tier of ice hockey in Germany.\nIn 2024 bandy was introduced, including the founding of a new club, \"Bandy Vereniging Tilburg\". At the Women's Bandy World Championship in 2025, three players from Tilburg were part of the Dutch team.\nAttractions.\nTilburg is the closest major settlement to The Efteling, with the amusement park made famous by Python being located only 11km away from the city centre. A small fan owned miniature attraction called Mini Efteling can be found a short distance away from the actual park. Located within the city itself is Zoo De Oliemeulen, a small family run animal attraction housing over 1,000 different animals.\nTransport.\nTilburg has three railway stations: Tilburg (Centraal), Tilburg Universiteit and Tilburg Reeshof. The latter was built to connect the then-latest district of Tilburg, the Reeshof. Intercity trains only stop at Tilburg (centraal). The name of Tilburg Universiteit Station was Tilburg West from its construction in 1968 to December 2010, however, after 40 years, it was not the westernmost station anymore. A fourth railway station is planned for Berkel-Enschot, also in the municipality of Tilburg and getting more absorbed into Tilburg. In the past, until 1938, Berkel-Enschot had its own train station. Udenhout, lying further northeast in the municipality, also had its train station until 1938. Both stations are on the line to 's-Hertogenbosch.\nThe Tilburg city and local buses are operated by Arriva. The city experimented from 2005 to 2008 with free public transport for children and 55+ people. Before Arriva, the buses were operated by Veolia, and before that by BBA (abbreviation for Brabants(ch)e Buurtspoorwegen en Autobussen). The city is also served by an hourly bus service to the Belgian city of Turnhout, operated by De Lijn.\nTilburg has an extensive bicycle path network called Sternet-Routes. The first bicycle path of this network was built between the city center and the university in 1975. From the mid-1990s, multiple bicycle paths (rather than lanes along the road) have been built. Older lanes are paved by tiles, but newer paths are largely done with asphalt paving. For this network of bicycle paths, some new tunnels were built under the railway that crosses the city.\nTilburg is, at variance from other Dutch cities of a similar size, connected by only one national motorway, the A58 / E312 (to Breda and Eindhoven). An outer beltway, consisting of two provincial 2x2-roads and the A58, was finished in May 2012. Although the outer beltway is fully navigable, the Burgemeester Bechtweg, which was built initially as a two-lane (one per direction) road, was finished in 2013. Two other routes are of considerable importance for Tilburg: the A261/N261 to Waalwijk and the A65/N65 to 's-Hertogenbosch. Neither is a complete motorway, and both experience bottlenecks. Various plans exist to build both to higher standards, with the N261 improved in 2015 to remove all traffic lights beyond Tilburg.\nTwin towns \u2013 sister cities.\nTilburg is twinned with:\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "37149", "revid": "3397128", "url": "https://en.wikipedia.org/wiki?curid=37149", "title": "Cranial nerves", "text": "Nerves that emerge directly from the brain\nCranial nerves are nerves that emerge directly from the brain, including the brainstem. They relay information between the brain and various parts of the body, primarily to the head and neck regions and are responsible for special senses of vision, taste, smell, and hearing.\nThe cranial nerves emerge from the central nervous system above the level of the first vertebra of the vertebral column. Each cranial nerve is paired and is present on both sides. \nThere are conventionally twelve pairs of cranial nerves, which are described with Roman numerals I\u2013XII. Some considered there to be thirteen pairs of cranial nerves, including the non-paired cranial nerve zero. The numbering of the cranial nerves is based on the order in which they emerge from the brain and brainstem, from front to back.\nThe terminal nerves (0), olfactory nerves (I) and optic nerves (II) emerge from the cerebrum, and the remaining ten pairs arise from the brainstem, which is the lower part of the brain.\nThe cranial nerves are considered components of the peripheral nervous system (PNS), although on a structural level the olfactory (I), optic (II), and trigeminal (V) nerves are more accurately considered part of the central nervous system (CNS).\nThe cranial nerves are in contrast to spinal nerves, which emerge from segments of the spinal cord.\nAnatomy.\nMost typically, humans are considered to have twelve pairs of cranial nerves (I\u2013XII), with the terminal nerve (0) more recently canonized. The nerves are: the olfactory nerve (I), the optic nerve (II), oculomotor nerve (III), trochlear nerve (IV), trigeminal nerve (V), abducens nerve (VI), facial nerve (VII), vestibulocochlear nerve (VIII), glossopharyngeal nerve (IX), vagus nerve (X), accessory nerve (XI), and the hypoglossal nerve (XII).\nTerminology.\nCranial nerves are generally named according to their structure or function. For example, the olfactory nerve (I) supplies smell, and the facial nerve (VII) supplies the muscles of the face. Because Latin was the \"lingua franca\" of the study of anatomy when the nerves were first documented, recorded, and discussed, many nerves maintain Latin or Greek names, including the trochlear nerve (IV), named according to its structure, as it supplies a muscle that attaches to a pulley (). The trigeminal nerve (V) is named in accordance with its three components ( meaning triplets), and the vagus nerve (X) is named for its wandering course ().\nCranial nerves are numbered based on their position from front to back (rostral-caudal) of their position on the brain, as, when viewing the forebrain and brainstem from below, they are often visible in their numeric order. For example, the olfactory nerves (I) and optic nerves (II) arise from the base of the forebrain, and the other nerves, III to XII, arise from the brainstem.\nCranial nerves have paths within and outside the skull. The paths within the skull are called \"intracranial\" and the paths outside the skull are called \"extracranial\". There are many holes in the skull called \"foramina\" by which the nerves can exit the skull. All cranial nerves are \"paired\", which means they occur on both the right and left sides of the body. The muscle, skin, or additional function supplied by a nerve, on the same side of the body as the side it originates from, is an \"ipsilateral\" function. If the function is on the opposite side to the origin of the nerve, this is known as a \"contralateral\" function.\nIntracranial course.\nNuclei.\nGrossly, all cranial nerves have a nucleus. With the exception of the olfactory nerve (I) and optic nerve (II), all the nuclei are present in the brainstem.\nThe midbrain has the nuclei of the oculomotor nerve (III) and trochlear nerve (IV); the pons has the nuclei of the trigeminal nerve (V), abducens nerve (VI), facial nerve (VII) and vestibulocochlear nerve (VIII); and the medulla has the nuclei of the glossopharyngeal nerve (IX), vagus nerve (X), accessory nerve (XI) and hypoglossal nerve (XII). The olfactory nerve (I) emerges from the olfactory bulb, and depending slightly on division the optic nerve (II) is considered to emerge from the lateral geniculate nuclei.\nBecause each nerve may have several functions, the nerve fibres that make up the nerve may collect in more than one nucleus. For example, the trigeminal nerve (V), which has a sensory and a motor role, has at least four nuclei.\nExiting the brainstem.\nWith the exception of the olfactory nerve (I) and optic nerve (II), the cranial nerves emerge from the brainstem. The oculomotor nerve (III) and trochlear nerve (IV) emerge from the midbrain, the trigeminal (V), abducens (VI), facial (VII) and vestibulocochlear (VIII) from the pons, and the glossopharyngeal (IX), vagus (X), accessory (XI) and hypoglossal (XII) emerge from the medulla.\nThe olfactory nerve (I) and optic nerve (II) emerge separately. The olfactory nerves emerge from the olfactory bulbs on either side of the crista galli, a bony projection below the frontal lobe, and the optic nerves (II) emerge from the lateral colliculus, swellings on either side of the temporal lobes of the brain.\nGanglia.\nThe cranial nerves give rise to a number of ganglia, collections of the cell bodies of neurons in the nerves that are outside of the brain. These ganglia are both parasympathetic and sensory ganglia.\nThe sensory ganglia of the cranial nerves, directly correspond to the dorsal root ganglia of spinal nerves and are known as cranial nerve ganglia. Sensory ganglia exist for nerves with sensory function: V, VII, VIII, IX, X. There are also a number of parasympathetic cranial nerve ganglia. Sympathetic ganglia supplying the head and neck reside in the upper regions of the sympathetic trunk, and do not belong to the cranial nerves.\nThe ganglion of the sensory nerves, which are similar in structure to the dorsal root ganglion of the spinal cord, include:\nAdditional ganglia for nerves with parasympathetic function exist, and include the ciliary ganglion of the oculomotor nerve (III), the pterygopalatine ganglion of the maxillary nerve (V2), the submandibular ganglion of the lingual nerve, a branch of the facial nerve (VII), and the otic ganglion of the glossopharyngeal nerve (IX).\nExiting the skull and extracranial course.\nAfter emerging from the brain, the cranial nerves travel within the skull, and some must leave it in order to reach their destinations. Often the nerves pass through holes in the skull, called foramina, as they travel to their destinations. Other nerves pass through bony canals, longer pathways enclosed by bone. These foramina and canals may contain more than one cranial nerve and may also contain blood vessels.\nDevelopment.\nThe cranial nerves are formed from the contribution of two specialized embryonic cell populations, cranial neural crest and ectodermal placodes. The components of the sensory nervous system of the head are derived from the neural crest and from an embryonic cell population developing in close proximity, the cranial sensory placodes (the olfactory, lens, otic, trigeminal, epibranchial and paratympanic placodes). The dual origin cranial nerves are summarized in the following table:\nContributions of neural crest cells and placodes to ganglia and cranial nerves\nAbbreviations: CN, cranial nerve; m, purely motor nerve; mix, mixed nerve (sensory and motor); NC, neural crest; PA, pharyngeal (branchial) arch; r, rhombomere; s, purely sensory nerve. * There is no known ganglion of the accessory nerve. The cranial part of the accessory nerve sends occasional branches to the superior ganglion of the vagus nerve.\nFunction.\nThe cranial nerves provide motor and sensory supply mainly to the structures within the head and neck. The sensory supply includes both \"general\" sensation such as temperature and touch, and \"special\" senses such as taste, vision, smell, balance and hearing. The vagus nerve (X) provides sensory and autonomic (parasympathetic) supply to structures in the neck and also to most of the organs in the chest and abdomen.\nTerminal nerve (0).\nThe terminal nerve (0) may not have a role in humans, although it has been implicated in hormonal responses to smell, sexual response and mate selection.\nSmell (I).\nThe olfactory nerve (I) conveys information giving rise to the sense of smell.\nDamage to the olfactory nerve (I) can cause an inability to smell (anosmia), a distortion in the sense of smell (parosmia), or a distortion or lack of taste.\nVision (II).\nThe optic nerve (II) transmits visual information.\nDamage to the optic nerve (II) affects specific aspects of vision that depend on the location of the damage. A person may not be able to see objects on their left or right sides (homonymous hemianopsia), or may have difficulty seeing objects from their outer visual fields (bitemporal hemianopsia) if the optic chiasm is involved. Inflammation (optic neuritis) may impact the sharpness of vision or color detection\nEye movement (III, IV, VI).\nThe oculomotor nerve (III), trochlear nerve (IV) and abducens nerve (VI) coordinate eye movement. The oculomotor nerve (III) controls all muscles of the eye except for the superior oblique muscle controlled by the trochlear nerve (IV), and the lateral rectus muscle controlled by the abducens nerve (VI). This means the ability of the eye to look down and inwards is controlled by the trochlear nerve (IV), the ability to look outwards is controlled by the abducens nerve (VI), and all other movements are controlled by the oculomotor nerve (III)\nDamage to these nerves may affect the movement of the eye. Damage may result in double vision (diplopia) because the movements of the eyes are not synchronized. Abnormalities of visual movement may also be seen on examination, such as jittering (nystagmus).\nDamage to the oculomotor nerve (III) can cause double vision and inability to coordinate the movements of both eyes (strabismus), also eyelid drooping (ptosis) and pupil dilation (mydriasis). Lesions may also lead to inability to open the eye due to paralysis of the levator palpebrae muscle. Individuals suffering from a lesion to the oculomotor nerve, may compensate by tilting their heads to alleviate symptoms due to paralysis of one or more of the eye muscles it controls.\nDamage to the trochlear nerve (IV) can also cause double vision with the eye adducted and elevated. The result will be an eye which can not move downwards properly (especially downwards when in an inward position). This is due to impairment in the superior oblique muscle.\nDamage to the abducens nerve (VI) can also result in double vision. This is due to impairment in the lateral rectus muscle, supplied by the abducens nerve.\nTrigeminal nerve (V).\nThe trigeminal nerve (V) and its three main branches the ophthalmic (V1), maxillary (V2), and mandibular (V3) provide sensation to the skin of the face and also controls the muscles of chewing.\nDamage to the trigeminal nerve leads to loss of sensation in an affected area. Other conditions affecting the trigeminal nerve (V) include trigeminal neuralgia, herpes zoster, sinusitis pain, presence of a dental abscess, and cluster headaches.\nFacial expression (VII).\nThe facial nerve (VII) controls most muscles of facial expression, supplies the sensation of taste from the front two-thirds of the tongue, and controls the stapedius muscle. Most muscles are supplied by the cortex on the opposite side of the brain; the exception is the frontalis muscle of the forehead, in which the left and the right side of the muscle both receive inputs from both sides of the brain.\nDamage to the facial nerve (VII) may cause facial palsy. This is where a person is unable to move the muscles on one or both sides of their face. The most common cause of this is Bell's palsy, the ultimate cause of which is unknown. Patients with Bell's palsy often have a drooping mouth on the affected side and often have trouble chewing because the buccinator muscle is affected. The facial nerve is also the most commonly affected cranial nerve in blunt trauma.\nHearing and balance (VIII).\nThe vestibulocochlear nerve (VIII) supplies information relating to balance and hearing via its two branches, the vestibular and cochlear nerves. The vestibular part is responsible for supplying sensation from the vestibules and semicircular canal of the inner ear, including information about balance, and is an important component of the vestibuloocular reflex, which keeps the head stable and allows the eyes to track moving objects. The cochlear nerve transmits information from the cochlea, allowing sound to be heard.\nWhen damaged, the vestibular nerve may give rise to the sensation of spinning and dizziness (vertigo). Function of the vestibular nerve may be tested by putting cold and warm water in the ears and watching eye movements caloric stimulation. Damage to the vestibulocochlear nerve can also present as repetitive and involuntary eye movements (nystagmus), particularly when the eye is moving horizontally. Damage to the cochlear nerve will cause partial or complete deafness in the affected ear.\nOral sensation, taste, and salivation (IX).\nThe glossopharyngeal nerve (IX) supplies the stylopharyngeus muscle and provides sensation to the oropharynx and back of the tongue. The glossopharyngeal nerve also provides parasympathetic input to the parotid gland.\nDamage to the nerve may cause failure of the gag reflex; a failure may also be seen in damage to the vagus nerve (X).\nVagus nerve (X).\nThe vagus nerve (X) provides sensory and parasympathetic supply to structures in the neck and also to most of the organs in the chest and abdomen.\nLoss of function of the vagus nerve (X) will lead to a loss of parasympathetic supply to a very large number of structures. Major effects of damage to the vagus nerve may include a rise in blood pressure and heart rate. Isolated dysfunction of only the vagus nerve is rare, but \u2013 if the lesion is located above the point at which the vagus first branches off \u2013 can be indicated by a hoarse voice, due to dysfunction of one of its branches, the recurrent laryngeal nerve.\nDamage to this nerve may result in difficulties swallowing.\nShoulder elevation and head-turning (XI).\nThe accessory nerve (XI) supplies the sternocleidomastoid and trapezius muscles.\nDamage to the accessory nerve (XI) will lead to weakness in the trapezius muscle on the same side as the damage. The trapezius lifts the shoulder when shrugging, so the affected shoulder will not be able to shrug and the shoulder blade (scapula) will protrude into a winged position. Depending on the location of the lesion there may also be weakness present in the sternocleidomastoid muscle, which acts to turn the head so that the face points to the opposite side.\nTongue movement (XII).\nThe hypoglossal nerve (XII) supplies the intrinsic muscles of the tongue, controlling tongue movement. The hypoglossal nerve (XII) is unique in that it is supplied by the motor cortices of both hemispheres of the brain.\nDamage to the nerve may lead to fasciculations or wasting (atrophy) of the muscles of the tongue. This will lead to weakness of tongue movement on that side. When damaged and extended, the tongue will move towards the weaker or damaged side, as shown in the image. The fasciculations of the tongue are sometimes said to look like a \"bag of worms\". Damage to the nerve tract or nucleus will not lead to atrophy or fasciculations, but only weakness of the muscles on the same side as the damage.\nClinical significance.\nExamination.\nDoctors, neurologists and other medical professionals may conduct a cranial nerve examination as part of a neurological examination to examine the cranial nerves. This is a highly formalised series of steps involving specific tests for each nerve. Dysfunction of a nerve identified during testing may point to a problem with the nerve or of a part of the brain.\nA cranial nerve exam starts with observation of the patient, as some cranial nerve lesions may affect the symmetry of the eyes or face. Vision may be tested by examining the visual fields, or by examining the retina with an ophthalmoscope, using a process known as funduscopy. Visual field testing may be used to pin-point structural lesions in the optic nerve, or further along the visual pathways. Eye movement is tested and abnormalities such as nystagmus are observed for. The sensation of the face is tested, and patients are asked to perform different facial movements, such as puffing out of the cheeks. Hearing is checked by voice and tuning forks. The patient's uvula is examined. After performing a shrug and head turn, the patient's tongue function is assessed by various tongue movements.\nSmell is not routinely tested, but if there is suspicion of a change in the sense of smell, each nostril is tested with substances of known odors such as coffee or soap. Intensely smelling substances, for example ammonia, may lead to the activation of pain receptors of the trigeminal nerve (V) located in the nasal cavity and this can confound olfactory testing.\nDamage.\nCompression.\nNerves may be compressed because of increased intracranial pressure, a mass effect of an intracerebral haemorrhage, or tumour that presses against the nerves and interferes with the transmission of impulses along the nerve. Loss of function of a cranial nerve may sometimes be the first symptom of an intracranial or skull base cancer.\nAn increase in intracranial pressure may lead to impairment of the optic nerves (II) due to compression of the surrounding veins and capillaries, causing swelling of the eyeball (papilloedema). A cancer, such as an optic nerve glioma, may also impact the optic nerve (II). A pituitary tumour may compress the optic tracts or the optic chiasm of the optic nerve (II), leading to visual field loss. A pituitary tumour may also extend into the cavernous sinus, compressing the oculomotor nerve (III), trochlear nerve (IV) and abducens nerve (VI), leading to double-vision and strabismus. These nerves may also be affected by herniation of the temporal lobes of the brain through the falx cerebri.\nThe cause of trigeminal neuralgia, in which one side of the face is exquisitely painful, is thought to be compression of the nerve by an artery as the nerve emerges from the brain stem. An acoustic neuroma, particularly at the junction between the pons and medulla, may compress the facial nerve (VII) and vestibulocochlear nerve (VIII), leading to hearing and sensory loss on the affected side.\nStroke.\nOcclusion of blood vessels that supply the nerves or their nuclei, an ischemic stroke, may cause specific signs and symptoms relating to the damaged area. If there is a stroke of the midbrain, pons or medulla, various cranial nerves may be damaged, resulting in dysfunction and symptoms of . Thrombosis, such as a cavernous sinus thrombosis, refers to a clot (thrombus) affecting the venous drainage from the cavernous sinus, affects the optic (II), oculomotor (III), trochlear (IV), ophthalmic branch of the trigeminal nerve (V1) and the abducens nerve (VI).\nInflammation.\nInflammation of a cranial nerve can occur as a result of infection, such as viral causes like reactivated herpes simplex virus, or can occur spontaneously. Inflammation of the facial nerve (VII) may result in Bell's palsy.\nMultiple sclerosis, an inflammatory process resulting in a loss of the myelin sheathes which surround the cranial nerves, may cause a variety of shifting symptoms affecting multiple cranial nerves. Inflammation may also affect other cranial nerves. Other rarer inflammatory causes affecting the function of multiple cranial nerves include sarcoidosis, miliary tuberculosis, and inflammation of arteries, such as granulomatosis with polyangiitis.\nOther.\nTrauma to the skull, disease of bone, such as Paget's disease, and injury to nerves during surgery are other causes of nerve damage.\nHistory.\nThe Graeco-Roman anatomist Galen (AD 129\u2013210) named seven pairs of cranial nerves. Much later, in 1664, English anatomist Sir Thomas Willis suggested that there were actually 9 pairs of nerves. Finally, in 1778, German anatomist Samuel Soemmering named the 12 pairs of nerves that are generally accepted today. However, because many of the nerves emerge from the brain stem as rootlets, there is continual debate as to how many nerves there actually are, and how they should be grouped. For example, there is reason to consider both the olfactory (I) and optic (II) nerves to be brain tracts, rather than cranial nerves.\nOther animals.\nCranial nerves are also present in other vertebrates. Other amniotes (non-amphibian tetrapods) have cranial nerves similar to those of humans. In anamniotes (fishes and amphibians), the accessory nerve (XI) and hypoglossal nerve (XII) do not exist, with the accessory nerve (XI) being an integral part of the vagus nerve (X); the hypoglossal nerve (XII) is represented by a variable number of spinal nerves emerging from vertebral segments fused into the occiput. These two nerves only became discrete nerves in the ancestors of amniotes. The very small terminal nerve (nerve N or O) exists in humans but may not be functional. In other animals, it appears to be important to sexual receptivity based on perceptions of pheromones.\n* Brachial plexus\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nExternal links.\nhttps://"}
{"id": "37150", "revid": "40567980", "url": "https://en.wikipedia.org/wiki?curid=37150", "title": "White cane", "text": "Distinctive cane used in walking by the blind\nA white cane is a device used by many people who are blind or visually impaired. A white cane primarily allows its user to scan their surroundings for obstacles or orientation marks, but is also helpful for onlookers in identifying the user as blind or visually impaired and taking appropriate care.\nVariants.\nMobility canes are often made from aluminium, graphite-reinforced plastic or other fibre-reinforced plastic, and can come with a wide variety of tips depending upon user preference.\nWhite canes can be either collapsible or straight, with both versions having advantages and disadvantages. The National Federation of the Blind in the United States affirms that the lightness and greater length of the straight canes allows greater mobility and safety, though collapsible canes can be stored with more ease, giving them advantage in crowded areas such as classrooms and public events.\nHistory.\nBlind people have used canes as mobility tools for centuries.\nIn 1921 James Biggs, a photographer from Bristol who became blind after an accident and was uncomfortable with the amount of traffic around his home, painted his walking stick white to be more easily visible.\nIn 1931 in France, Guilly d'Herbemont launched a national white stick movement for blind people. On February 7, 1931, Guilly d'Herbemont symbolically gave the first two white canes to blind people, in the presence of several French ministers. 5,000 more white canes were later sent to blind French veterans from World War I and blind civilians.\nThe first special white cane ordinance was passed in December 1930 in Peoria, Illinois, after intervention of the Peoria Lions Club, granting blind pedestrians protections and the right-of-way while carrying a white cane.\nThe long cane was improved upon by World War II veterans rehabilitation specialist, Richard E. Hoover, at Valley Forge Army Hospital. In 1944, he took the Lions Club white cane (originally made of wood) and went around the hospital blindfolded for a week. During this time he developed what is now the standard method of \"long cane\" training or the Hoover Method. He is now called the \"Father of the Lightweight Long Cane Technique\". The basic technique is to swing the cane from the center of the body back and forth before the feet. The cane should be swept before the rear foot as the person steps. Before he taught other rehabilitators (or \"orientors\") his new technique, he had a special commission to have lightweight, long white canes made for the veterans of the European fronts.\nHoover's long cane technique was perfected at Edward Hines Jr. VA Hospital, which opened the Department of Veterans Affairs' first Blind Rehabilitation Center in 1948 and was staffed by former Valley Forge Army Hospital instructors. \nOn October 6, 1964, a joint resolution of the Congress, HR 753, was signed into law authorizing the President of the United States to proclaim October 15 of each year as \"White Cane Safety Day\". President Lyndon Johnson was the first to make this proclamation.\nLegislation about canes.\nWhile the white cane is commonly accepted as a \"symbol of blindness\", different countries still have different rules concerning what constitutes a \"cane for the blind\".\nIn the United Kingdom, the white cane indicates that the individual has a visual impairment but normal hearing; with red bands added, it indicates that the user is deafblind.\nIn the United States, laws vary from state to state, but in all cases, those carrying white canes are afforded the right-of-way when crossing a road. They are afforded the right to use their cane in any public place as well. In some cases, it is illegal for a non-blind person to use a white cane with the intent of being given right-of-way.\nIn November 2002, Argentina passed a law recognizing the use of green canes by people with low vision, stating that the nation would \"adopt from this law, the use of a green cane in the whole of Argentina as a means of orientation and mobility for people with low vision. It will have the same characteristics in weight, length, elastic grip and fluorescent ring as do white canes used by the blind.\"\nIn Germany, people carrying a white cane are exempted from the \"Vertrauensgrundsatz\" (trust principle), therefore meaning that other traffic participants should not rely on them to adhere to all traffic regulations and practices. Although there is no general duty to mark oneself as blind or otherwise disabled, a blind or visually impaired person involved in a traffic accident without having marked themselves may be held responsible for damages unless they prove that their lack of marking was not causal or otherwise related to the accident.\nChildren and canes.\nIn many countries, including the UK, a cane is not generally introduced to a child until they are between 7 and 10 years old. However, more recently canes have been started to be introduced as soon as a child learns to walk to aid development with great success.\nJoseph Cutter and Lilli Nielsen, pioneers in research on the development of blind and disabled children, have begun to introduce new research on mobility in blind infants in children. Cutter's book, \"Independent Movement and Travel in Blind Children\", recommends a cane to be introduced as early as possible, so that the blind child learns to use it and move around naturally and organically, the same way a sighted child learns to walk. A longer cane, between nose and chin height, is recommended to compensate for a child's more immature grasp and tendency to hold the handle of the cane by the side instead of out in front. Mature cane technique should not be expected from a child, and style and technique can be refined as the child gets older.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nExternal links.\n&lt;templatestyles src=\"Sister-inline/styles.css\"/&gt; Media related to at Wikimedia Commons"}
{"id": "37151", "revid": "1302984629", "url": "https://en.wikipedia.org/wiki?curid=37151", "title": "Association for the Taxation of Financial Transactions and for Citizens' Action", "text": "French tax advocacy group\nThe Association pour la Taxation des Transactions financi\u00e8res et pour l'Action Citoyenne (Association for the Taxation of financial Transactions and Citizens' Action, ATTAC) is an activist organisation originally created to promote the establishment of a tax on foreign exchange transactions.\nBackground.\nOriginally called \"Action for a Tobin Tax to Assist the Citizen\", ATTAC was a single-issue movement demanding the introduction of the so-called Tobin tax on currency speculation. ATTAC has enlarged its scope to a wide range of issues related to globalisation, and monitoring the decisions of the World Trade Organization (WTO), the Organisation for Economic Co-operation and Development (OECD,) and the International Monetary Fund (IMF). ATTAC representatives attend the meetings of the G8 with the goal of influencing policymakers' decisions. In 2007, ATTAC spokesmen criticised Germany for what it called the criminalisation of anti-G8 groups.\nAt the founding, ATTAC had specific statutory objectives based on the promotion of the Tobin tax. For example, ATTAC Luxembourg specifies in article 1 of its statutes that it: \n...aims to produce and communicate information, and to promote and carry out activities of all kinds for the recapture, by the citizens, of the power that the financial sector has on all aspects of political, economic, social and cultural life throughout the world. Such means include the taxation of transactions in foreign exchange markets (Tobin tax).\nATTAC refutes claims that it is an anti-globalisation movement, but it criticises the neoliberal ideology that it sees as dominating economic globalisation. It supports those globalisation policies that their representatives characterise as sustainable and socially just. One of ATTAC's slogans is \"The World is not for sale\", denouncing the \"merchandisation\" of society. Another slogan is \"Another world is possible\", pointing to an alternative globalisation in which people and not profit is in focus.\nJames Tobin opposing ATTAC.\nAttac was founded to promote the Tobin tax by the Keynesian economist James Tobin. Tobin has said that Attac has misused his name. He says he has nothing in common with their goals and supports free trade \u2014 \"everything that these movements are attacking. They're misusing my name.\"\nOrganisational history.\nIn December 1997, Ignacio Ramonet wrote an editorial in \"Le Monde diplomatique\" in which he advocated for the establishment of the Tobin tax and the creation of an organisation to pressure governments around the world to introduce the tax. ATTAC was created on 3 June 1998, during a constitutive assembly in France. While it was founded in France it now exists in over forty countries around the world. In France, politicians from the left are members of the association. In Luxembourg, Francois Bausch of the left Green party is the founding politician in the association's initial member list.\nATTAC functions on a principle of decentralisation: local associations organise meetings, conferences, and compose documents that become counter-arguments to the perceived neoliberal discourse. ATTAC aims to formalise the possibility of an alternative to the neoliberal society that is currently required of globalisation. ATTAC aspires to be a movement of popular education.\nViews on Attac and its members in different countries.\nFinland.\nCommunist Juhani Lohikoski, previously a chairman of Communist Youth League and Socialist League, served as the chairman of Finnish Attac for two terms (2002 \u2013 2004). Yrj\u00f6 Hakanen, chairman of the Communist Party of Finland, was a member of the board and a member of the founding committee. In March 2002 Aimo Kairamo, the long-time chief editor of the party organ of the Social Democrat Party, resigned from Attac and recommended the same decision for other social democrats because of the left-wing minority communists' leading positions. Soon also the social democrat foreign minister Erkki Tuomioja considered to follow Kairamo's example.\nSweden.\nResearcher Malin Gawell covers the birth and development of Attac Sweden in her doctoral thesis on activist entrepreneurship. She suggests that Attac in Sweden was formed by people seeking a new way of organising with flat hierarchy, and with the strongly sensed need of making a change as the driving force.\nFrom another perspective, Sydsvenskan newspaper suggested that the downturn of memberships in Swedish Attac after the hype in the beginning of 2001 may be due to its views on trade policies.\nIssues and activities.\nThe main issues covered by ATTAC today are:\nIn France, ATTAC associates with many other left-wing causes.\nNestl\u00e9gate.\nIn 2008, the Swiss multinational food and beverage company Nestl\u00e9 was hit by a scandal which was later called Nestl\u00e9gate by the media. Between the years 2003 and 2005, Nestl\u00e9 hired the external Security company Securitas AG to spy on the Swiss branch of Attac. Nestl\u00e9 started the monitoring when Attac Switzerland decided to work on a critical book about Nestl\u00e9.\nIn response to the Nestl\u00e9gate, Attac Switzerland filed a lawsuit against Nestl\u00e9. The lawsuit was decided in favour of Attac in January 2013, as the personal rights of the observed were violated. They received a compensation for damages of 3000 Swiss francs each (about 3230 USD at the date of the proclamation of sentence).\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "37153", "revid": "46450117", "url": "https://en.wikipedia.org/wiki?curid=37153", "title": "Supercomputer", "text": "Type of extremely powerful computer\nA supercomputer is a type of computer with a high level of performance as compared to a general-purpose computer. Supercomputers play an important role in the field of computational science, and are used for a wide range of computationally intensive tasks in various fields including quantum mechanics, weather forecasting, climate research, oil and gas exploration, molecular modeling (computing the structures and properties of chemical compounds, biological macromolecules, polymers, and crystals), and physical simulations (such as simulations of aerodynamics, of the early moments of the universe, and of nuclear weapons). They have been essential in the field of cryptanalysis.\nThe performance of a supercomputer is commonly measured in floating-point operations per second (FLOPS) instead of million instructions per second (MIPS). Since 2022, exascale supercomputers have existed which can perform over 1018\u00a0FLOPS. For comparison, a desktop computer has performance in the range of hundreds of gigaFLOPS (1011) to tens of teraFLOPS (1013). Since November 2017, all of the world's fastest 500 supercomputers run on Linux-based operating systems. Additional research is being conducted in the United States, the European Union, Taiwan, Japan, and China to build faster, more powerful and technologically superior exascale supercomputers.\nSupercomputers were introduced in the 1960s, and for several decades the fastest was made by Seymour Cray at Control Data Corporation (CDC), Cray Research and subsequent companies bearing his name or monogram. The first such machines were highly tuned conventional designs that ran more quickly than their more general-purpose contemporaries. Through the decade, increasing amounts of parallelism were added, with one to four processors being typical. In the 1970s, vector processors operating on large arrays of data came to dominate. A notable example is the highly successful Cray-1 of 1976. Vector computers remained the dominant design into the 1990s. From then until today, massively parallel supercomputers with tens of thousands of off-the-shelf processors became the norm.\nThe U.S. has long been a leader in the supercomputer field, initially through Cray's nearly uninterrupted dominance, and later through a variety of technology companies. Japan made significant advancements in the field during the 1980s and 1990s, while China has become increasingly active in supercomputing in recent years. As of November 2024[ [update]], Lawrence Livermore National Laboratory's El Capitan is the world's fastest supercomputer. The US has five of the top 10; Italy two, Japan, Finland, Switzerland have one each. In June 2018, all combined supercomputers on the TOP500 list broke the 1\u00a0exaFLOPS mark.\nHistory.\nIn 1960, UNIVAC built the Livermore Atomic Research Computer (LARC), today considered among the first supercomputers, for the US Navy Research and Development Center. It still used high-speed drum memory, rather than the newly emerging disk drive technology. Also, among the first supercomputers was the IBM 7030 Stretch. The IBM 7030 was built by IBM for the Los Alamos National Laboratory, which then in 1955 had requested a computer 100 times faster than any existing computer. The IBM 7030 used transistors, magnetic core memory, pipelined instructions, prefetched data through a memory controller and included pioneering random access disk drives. The IBM 7030 was completed in 1961 and despite not meeting the challenge of a hundredfold increase in performance, it was purchased by the Los Alamos National Laboratory. Customers in England and France also bought the computer, and it became the basis for the IBM 7950 Harvest, a supercomputer built for cryptanalysis.\nThe third pioneering supercomputer project in the early 1960s was the Atlas at the University of Manchester, built by a team led by Tom Kilburn. He designed the Atlas to have memory space for up to a million words of 48 bits, but because magnetic storage with such a capacity was unaffordable, the actual core memory of the Atlas was only 16,000 words, with a drum providing memory for a further 96,000 words. The Atlas Supervisor swapped data in the form of pages between the magnetic core and the drum. The Atlas operating system also introduced time-sharing to supercomputing, so that more than one program could be executed on the supercomputer at any one time. Atlas was a joint venture between Ferranti and Manchester University and was designed to operate at processing speeds approaching one\u00a0microsecond per instruction, about one\u00a0million instructions per second.\nThe CDC 6600, designed by Seymour Cray, was finished in 1964 and marked the transition from germanium to silicon transistors. Silicon transistors could run more quickly and the overheating problem was solved by introducing refrigeration to the supercomputer design. Thus, the CDC6600 became the fastest computer in the world. Given that the 6600 outperformed all the other contemporary computers by about 10 times, it was dubbed a \"supercomputer\" and defined the supercomputing market, when one hundred computers were sold at $8 million each.\nCray left CDC in 1972 to form his own company, Cray Research. Four years after leaving CDC, Cray delivered the 80\u00a0MHz Cray-1 in 1976, which became one of the most successful supercomputers in history. The Cray-2 was released in 1985. It had eight central processing units (CPUs), liquid cooling and the electronics coolant liquid Fluorinert was pumped through the supercomputer architecture. It reached 1.9\u00a0gigaFLOPS, making it the first supercomputer to break the gigaflop barrier.\nMassively parallel designs.\nThe only computer to seriously challenge the Cray-1's performance in the 1970s was the ILLIAC IV. This machine was the first realized example of a true massively parallel computer, in which many processors worked together to solve different parts of a single larger problem. In contrast with the vector systems, which were designed to run a single stream of data as quickly as possible, in this concept, the computer instead feeds separate parts of the data to entirely different processors and then recombines the results. The ILLIAC's design was finalized in 1966 with 256 processors and offer speed up to 1\u00a0GFLOPS, compared to the 1970s Cray-1's peak of 250\u00a0MFLOPS. However, development problems led to only 64 processors being built, and the system could never operate more quickly than about 200\u00a0MFLOPS while being much larger and more complex than the Cray. Another problem was that writing software for the system was difficult, and getting peak performance from it was a matter of serious effort.\nBut the partial success of the ILLIAC IV was widely seen as pointing the way to the future of supercomputing. Cray argued against this, famously quipping that \"If you were plowing a field, which would you rather use? Two strong oxen or 1024 chickens?\" But by the early 1980s, several teams were working on parallel designs with thousands of processors, notably the Connection Machine (CM) that developed from research at MIT. The CM-1 used as many as 65,536 simplified custom microprocessors connected together in a network to share data. Several updated versions followed; the CM-5 supercomputer is a massively parallel processing computer capable of many billions of arithmetic operations per second.\nIn 1982, Osaka University's LINKS-1 Computer Graphics System used a massively parallel processing architecture, with 514 microprocessors, including 257 Zilog Z8001 control processors and 257 iAPX 86/20 floating-point processors. It was mainly used for rendering realistic 3D computer graphics. Fujitsu's VPP500 from 1992 is unusual since, to achieve higher speeds, its processors used GaAs, a material normally reserved for microwave applications due to its toxicity. Fujitsu's Numerical Wind Tunnel supercomputer used 166 vector processors to gain the top spot in 1994 with a peak speed of 1.7\u00a0gigaFLOPS (GFLOPS) per processor. The Hitachi SR2201 obtained a peak performance of 600\u00a0GFLOPS in 1996 by using 2048 processors connected via a fast three-dimensional crossbar network. The Intel Paragon could have 1000 to 4000 Intel i860 processors in various configurations and was ranked the fastest in the world in 1993. The Paragon was a MIMD machine which connected processors via a high speed two-dimensional mesh, allowing processes to execute on separate nodes, communicating via the Message Passing Interface.\nSoftware development remained a problem, but the CM series sparked off considerable research into this issue. Similar designs using custom hardware were made by many companies, including the Evans &amp; Sutherland ES-1, MasPar, nCUBE, Intel iPSC and the Goodyear MPP. But by the mid-1990s, general-purpose CPU performance had improved so much in that a supercomputer could be built using them as the individual processing units, instead of using custom chips. By the turn of the 21st century, designs featuring tens of thousands of commodity CPUs were the norm, with later machines adding graphic units to the mix.\nIn 1998, David Bader developed the first Linux supercomputer using commodity parts. While at the University of New Mexico, Bader sought to build a supercomputer running Linux using consumer off-the-shelf parts and a high-speed low-latency interconnection network. The prototype utilized an Alta Technologies \"AltaCluster\" of eight dual, 333\u00a0MHz, Intel Pentium II computers running a modified Linux kernel. Bader ported a significant amount of software to provide Linux support for necessary components as well as code from members of the National Computational Science Alliance (NCSA) to ensure interoperability, as none of it had been run on Linux previously. Using the successful prototype design, he led the development of \"RoadRunner,\" the first Linux supercomputer for open use by the national science and engineering community via the National Science Foundation's National Technology Grid. RoadRunner was put into production use in April 1999. At the time of its deployment, it was considered one of the 100 fastest supercomputers in the world. Though Linux-based clusters using consumer-grade parts, such as Beowulf, existed prior to the development of Bader's prototype and RoadRunner, they lacked the scalability, bandwidth, and parallel computing capabilities to be considered \"true\" supercomputers.\nSystems with a massive number of processors generally take one of two paths. In the grid computing approach, the processing power of many computers, organized as distributed, diverse administrative domains, is opportunistically used whenever a computer is available. In another approach, many processors are used in proximity to each other, e.g. in a computer cluster. In such a centralized massively parallel system the speed and flexibility of the \"&lt;templatestyles src=\"Template:Visible anchor/styles.css\" /&gt;interconnect\" becomes very important and modern supercomputers have used various approaches ranging from enhanced Infiniband systems to three-dimensional torus interconnects. The use of multi-core processors combined with centralization is an emerging direction, e.g. as in the Cyclops64 system.\nAs the price, performance and energy efficiency of general-purpose graphics processing units (GPGPUs) have improved, a number of petaFLOPS supercomputers such as Tianhe-I and Nebulae have started to rely on them. However, other systems such as the K computer continue to use conventional processors such as SPARC-based designs and the overall applicability of GPGPUs in general-purpose high-performance computing applications has been the subject of debate, in that while a GPGPU may be tuned to score well on specific benchmarks, its overall applicability to everyday algorithms may be limited unless significant effort is spent to tune the application to it. However, GPUs are gaining ground, and in 2012 the Jaguar supercomputer was transformed into Titan by retrofitting CPUs with GPUs.\nHigh-performance computers have an expected life cycle of about three years before requiring an upgrade. The Gyoukou supercomputer is unique in that it uses both a massively parallel design and liquid immersion cooling.\nSpecial purpose supercomputers.\nA number of special-purpose systems have been designed, dedicated to a single problem. This allows the use of specially programmed FPGA chips or even custom ASICs, allowing better price/performance ratios by sacrificing generality. Examples of special-purpose supercomputers include Belle, Deep Blue, and Hydra for playing chess, Gravity Pipe for astrophysics, MDGRAPE-3 for protein structure prediction and molecular dynamics, and Deep Crack for breaking the DES cipher.\nEnergy usage and heat management.\nThroughout the decades, the management of heat density has remained a key issue for most centralized supercomputers. The large amount of heat generated by a system may also have other effects, e.g. reducing the lifetime of other system components. There have been diverse approaches to heat management, from pumping Fluorinert through the system, to a hybrid liquid-air cooling system or air cooling with normal air conditioning temperatures. A typical supercomputer consumes large amounts of electrical power, almost all of which is converted into heat, requiring cooling. For example, Tianhe-1A consumes 4.04\u00a0megawatts (MW) of electricity. The cost to power and cool the system can be significant, e.g. 4\u00a0MW at $0.10/kWh is $400 an hour or about $3.5 million per year.\nHeat management is a major issue in complex electronic devices and affects powerful computer systems in various ways. The thermal design power and CPU power dissipation issues in supercomputing surpass those of traditional computer cooling technologies. The supercomputing awards for green computing reflect this issue.\nThe packing of thousands of processors together inevitably generates significant amounts of heat density that need to be dealt with. The Cray-2 was liquid cooled, and used a Fluorinert \"cooling waterfall\" which was forced through the modules under pressure. However, the submerged liquid cooling approach was not practical for the multi-cabinet systems based on off-the-shelf processors, and in System X a special cooling system that combined air conditioning with liquid cooling was developed in conjunction with the Liebert company.\nIn the Blue Gene system, IBM deliberately used low power processors to deal with heat density. The IBM Power 775, released in 2011, has closely packed elements that require water cooling. The IBM Aquasar system uses hot water cooling to achieve energy efficiency, the water being used to heat buildings as well.\nThe energy efficiency of computer systems is generally measured in terms of \"FLOPS per watt\". In 2008, Roadrunner by IBM operated at 376\u00a0MFLOPS/W. In November 2010, the Blue Gene/Q reached 1,684\u00a0MFLOPS/W and in June 2011 the top two spots on the Green 500 list were occupied by Blue Gene machines in New York (one achieving 2097\u00a0MFLOPS/W) with the DEGIMA cluster in Nagasaki placing third with 1375\u00a0MFLOPS/W.\nBecause copper wires can transfer energy into a supercomputer with much higher power densities than forced air or circulating refrigerants can remove waste heat, the ability of the cooling systems to remove waste heat is a limiting factor. As of 2015[ [update]], many existing supercomputers have more infrastructure capacity than the actual peak demand of the machine\u00a0\u2013 designers generally conservatively design the power and cooling infrastructure to handle more than the theoretical peak electrical power consumed by the supercomputer. Designs for future supercomputers are power-limited\u00a0\u2013 the thermal design power of the supercomputer as a whole, the amount that the power and cooling infrastructure can handle, is somewhat more than the expected normal power consumption, but less than the theoretical peak power consumption of the electronic hardware.\nSoftware and system management.\nOperating systems.\nSince the end of the 20th century, supercomputer operating systems have undergone major transformations, based on the changes in supercomputer architecture. While early operating systems were custom tailored to each supercomputer to gain speed, the trend has been to move away from in-house operating systems to the adaptation of generic software such as Linux.\nSince modern massively parallel supercomputers typically separate computations from other services by using multiple types of nodes, they usually run different operating systems on different nodes, e.g. using a small and efficient lightweight kernel such as CNK or CNL on compute nodes, but a larger system such as a full Linux distribution on server and I/O nodes.\nWhile in a traditional multi-user computer system job scheduling is, in effect, a tasking problem for processing and peripheral resources, in a massively parallel system, the job management system needs to manage the allocation of both computational and communication resources, as well as gracefully deal with inevitable hardware failures when tens of thousands of processors are present.\nAlthough most modern supercomputers use Linux-based operating systems, each manufacturer has its own specific Linux distribution, and no industry standard exists, partly due to the fact that the differences in hardware architectures require changes to optimize the operating system to each hardware design.\nSoftware tools and message passing.\nThe parallel architectures of supercomputers often dictate the use of special programming techniques to exploit their speed. Software tools for distributed processing include standard APIs such as MPI and PVM, VTL, and open source software such as Beowulf.\nIn the most common scenario, environments such as PVM and MPI for loosely connected clusters and OpenMP for tightly coordinated shared memory machines are used. Significant effort is required to optimize an algorithm for the interconnect characteristics of the machine it will be run on; the aim is to prevent any of the CPUs from wasting time waiting on data from other nodes. GPGPUs have hundreds of processor cores and are programmed using programming models such as CUDA or OpenCL.\nMoreover, it is quite difficult to debug and test parallel programs. Special techniques need to be used for testing and debugging such applications.\nDistributed supercomputing.\nOpportunistic approaches.\nOpportunistic supercomputing is a form of networked grid computing whereby a \"super virtual computer\" of many loosely coupled volunteer computing machines performs very large computing tasks. Grid computing has been applied to a number of large-scale embarrassingly parallel problems that require supercomputing performance scales. However, basic grid and cloud computing approaches that rely on volunteer computing cannot handle traditional supercomputing tasks such as fluid dynamic simulations.\nThe fastest grid computing system is the volunteer computing project Folding@home (F@h). As of \u00a02020[ [update]], F@h reported 2.5\u00a0exaFLOPS of x86 processing power. Of this, over 100\u00a0PFLOPS are contributed by clients running on various GPUs, and the rest from various CPU systems.\nThe Berkeley Open Infrastructure for Network Computing (BOINC) platform hosts a number of volunteer computing projects. As of \u00a02017[ [update]], BOINC recorded a processing power of over 166\u00a0petaFLOPS through over 762\u00a0thousand active Computers (Hosts) on the network.\nAs of \u00a02016[ [update]], Great Internet Mersenne Prime Search's (GIMPS) distributed Mersenne Prime search achieved about 0.313\u00a0PFLOPS through over 1.3\u00a0million computers. The PrimeNet server has supported GIMPS's grid computing approach, one of the earliest volunteer computing projects, since 1997.\nQuasi-opportunistic approaches.\nQuasi-opportunistic supercomputing is a form of distributed computing whereby the \"super virtual computer\" of many networked geographically disperse computers performs computing tasks that demand huge processing power. Quasi-opportunistic supercomputing aims to provide a higher quality of service than opportunistic grid computing by achieving more control over the assignment of tasks to distributed resources and the use of intelligence about the availability and reliability of individual systems within the supercomputing network. However, quasi-opportunistic distributed execution of demanding parallel computing software in grids should be achieved through the implementation of grid-wise allocation agreements, co-allocation subsystems, communication topology-aware allocation mechanisms, fault tolerant message passing libraries and data pre-conditioning.\nHigh-performance computing clouds.\nCloud computing with its recent and rapid expansions and development have grabbed the attention of high-performance computing (HPC) users and developers in recent years. Cloud computing attempts to provide HPC-as-a-service exactly like other forms of services available in the cloud such as software as a service, platform as a service, and infrastructure as a service. HPC users may benefit from the cloud in different angles such as scalability, resources being on-demand, fast, and inexpensive. On the other hand, moving HPC applications have a set of challenges too. Good examples of such challenges are virtualization overhead in the cloud, multi-tenancy of resources, and network latency issues. Much research is currently being done to overcome these challenges and make HPC in the cloud a more realistic possibility.\nIn 2016, Penguin Computing, Parallel Works, R-HPC, Amazon Web Services, Univa, Silicon Graphics International, Rescale, Sabalcore, and Gomput started to offer HPC cloud computing. The Penguin On Demand (POD) cloud is a bare-metal compute model to execute code, but each user is given virtualized login node. POD computing nodes are connected via non-virtualized 10\u00a0Gbit/s Ethernet or QDR InfiniBand networks. User connectivity to the POD data center ranges from 50\u00a0Mbit/s to 1\u00a0Gbit/s. Citing Amazon's EC2 Elastic Compute Cloud, Penguin Computing argues that virtualization of compute nodes is not suitable for HPC. Penguin Computing has also criticized that HPC clouds may have allocated computing nodes to customers that are far apart, causing latency that impairs performance for some HPC applications.\nPerformance measurement.\nCapability versus capacity.\nSupercomputers generally aim for the maximum in capability computing rather than capacity computing. Capability computing is typically thought of as using the maximum computing power to solve a single large problem in the shortest amount of time. Often a capability system is able to solve a problem of a size or complexity that no other computer can, e.g. a very complex weather simulation application.\nCapacity computing, in contrast, is typically thought of as using efficient cost-effective computing power to solve a few somewhat large problems or many small problems. Architectures that lend themselves to supporting many users for routine everyday tasks may have a lot of capacity but are not typically considered supercomputers, given that they do not solve a single very complex problem.\nPerformance metrics.\nIn general, the speed of supercomputers is measured and benchmarked in FLOPS (floating-point operations per second), and not in terms of MIPS (million instructions per second), as is the case with general-purpose computers. These measurements are commonly used with an SI prefix such as tera-, combined into the shorthand TFLOPS (1012 FLOPS, pronounced \"teraflops\"), or peta-, combined into the shorthand PFLOPS (1015 FLOPS, pronounced \"petaflops\".) Petascale supercomputers can process one quadrillion (1015) (1000\u00a0trillion) FLOPS. Exascale is computing performance in the exaFLOPS (EFLOPS) range. An EFLOPS is one quintillion (1018) FLOPS (one million TFLOPS). However, the performance of a supercomputer can be severely impacted by fluctuation brought on by elements like system load, network traffic, and concurrent processes, as mentioned by Brehm and Bruhwiler (2015).\nNo single number can reflect the overall performance of a computer system, yet the goal of the Linpack benchmark is to approximate how fast the computer solves numerical problems and it is widely used in the industry. The FLOPS measurement is either quoted based on the theoretical floating point performance of a processor (derived from manufacturer's processor specifications and shown as \"Rpeak\" in the TOP500 lists), which is generally unachievable when running real workloads, or the achievable throughput, derived from the LINPACK benchmarks and shown as \"Rmax\" in the TOP500 list. The LINPACK benchmark typically performs LU decomposition of a large matrix. The LINPACK performance gives some indication of performance for some real-world problems, but does not necessarily match the processing requirements of many other supercomputer workloads, which for example may require more memory bandwidth, or may require better integer computing performance, or may need a high performance I/O system to achieve high levels of performance.\nThe TOP500 list.\nSince 1993, the fastest supercomputers have been ranked on the TOP500 list according to their LINPACK benchmark results. The list does not claim to be unbiased or definitive, but it is a widely cited current definition of the \"fastest\" supercomputer available at any given time.\nThis is a list of the computers which appeared at the top of the TOP500 list since June 1993, and the \"Peak speed\" is given as the \"Rmax\" rating. In 2018, Lenovo became the world's largest provider for the TOP500 supercomputers with 117 units produced.\nLegend:\nApplications.\nThe application of supercomputers has evolved significantly since the 1970s, expanding across numerous fields of scientific and governmental research. In the 1970s, the Cray-1 supercomputer was instrumental in early weather forecasting and aerodynamic research. The scope broadened in the 1980s with machines like the CDC Cyber facilitating probabilistic analyses and modeling radiation shielding. The 1990s saw supercomputers, such as the EFF DES cracker, applied to brute-force tasks like code-breaking.\nMoving into the 2000s, supercomputers became crucial for national security applications; the ASCI Q was used to conduct 3D nuclear test simulations, satisfying the requirements of the Nuclear Non-Proliferation Treaty without physical testing. The 2010s brought advancements in molecular dynamics simulations, exemplified by the use of the Tianhe-1A. The trend has continued into the 2020s, with supercomputers providing powerful tools for research into outbreak prevention and electrochemical reactions.\nBeyond these specific decades, supercomputers have been pivotal in major scientific endeavors. For instance, the IBM Blue Gene/P was used to simulate a network of artificial neurons equivalent to one percent of a human cerebral cortex (1.6 billion neurons and 9 trillion connections), and the same team later simulated the entirety of a rat's brain.\nModern applications are diverse and critical. Weather forecasting relies heavily on supercomputing power, with the National Oceanic and Atmospheric Administration utilizing these systems to process vast quantities of observational data to enhance forecast accuracy. The United States also employs supercomputers through the Advanced Simulation and Computing Program to maintain and simulate its nuclear stockpile.\nThe field has its challenges, as highlighted by IBM's abandonment of the ambitious Blue Waters petascale project in 2011. Despite such setbacks, the utility of supercomputers continues to grow. A recent example of their critical role occurred in early 2020, when supercomputers were rapidly deployed to run extensive simulations using numerous paralleled CPUs to identify compounds with the potential to stop the spread of COVID-19.\nDevelopment and trends.\nIn the 2010s, China, the United States, the European Union, and others competed to be the first to create a 1\u00a0exaFLOP (1018 or one quintillion FLOPS) supercomputer. Erik P. DeBenedictis of Sandia National Laboratories has theorized that a zettaFLOPS (1021 or one sextillion FLOPS) computer is required to accomplish full weather modeling, which could cover a two-week time span accurately. Such systems might be built around 2030.\nMany Monte Carlo simulations use the same algorithm to process a randomly generated data set; particularly, integro-differential equations describing physical transport processes, the random paths, collisions, and energy and momentum depositions of neutrons, photons, ions, electrons, etc. The next step for microprocessors may be into the third dimension; and specializing to Monte Carlo, the many layers could be identical, simplifying the design and manufacture process.\nThe cost of operating high performance supercomputers has risen, mainly due to increasing power consumption. In the mid-1990s a top 10 supercomputer required in the range of 100\u00a0kilowatts, in 2010 the top 10 supercomputers required between 1 and 2\u00a0megawatts. A 2010 study commissioned by DARPA identified power consumption as the most pervasive challenge in achieving Exascale computing. At the time a megawatt per year in energy consumption cost about 1\u00a0million dollars. Supercomputing facilities were constructed to efficiently remove the increasing amount of heat produced by modern multi-core central processing units. Based on the energy consumption of the Green 500 list of supercomputers between 2007 and 2011, a supercomputer with 1\u00a0exaFLOPS in 2011 would have required nearly 500\u00a0megawatts. Operating systems were developed for existing hardware to conserve energy whenever possible. CPU cores not in use during the execution of a parallelized application were put into low-power states, producing energy savings for some supercomputing applications.\nThe increasing cost of operating supercomputers has been a driving factor in a trend toward bundling of resources through a distributed supercomputer infrastructure. National supercomputing centers first emerged in the US, followed by Germany and Japan. The European Union launched the Partnership for Advanced Computing in Europe (PRACE) with the aim of creating a persistent pan-European supercomputer infrastructure with services to support scientists across the European Union in porting, scaling and optimizing supercomputing applications. Iceland built the world's first zero-emission supercomputer. Located at the Thor Data Center in Reykjav\u00edk, Iceland, this supercomputer relies on completely renewable sources for its power rather than fossil fuels. The colder climate also reduces the need for active cooling, making it one of the greenest facilities in the world of computers.\nFunding supercomputer hardware also became increasingly difficult. In the mid-1990s a top 10 supercomputer cost about 10\u00a0million euros, while in 2010 the top 10 supercomputers required an investment of between 40 and 50\u00a0million euros. In the 2000s national governments put in place different strategies to fund supercomputers. In the UK the national government funded supercomputers entirely and high performance computing was put under the control of a national funding agency. Germany developed a mixed funding model, pooling local state funding and federal funding.\nIn fiction.\nExamples of supercomputers in fiction include HAL 9000, Multivac, The Machine Stops, GLaDOS, SHODAN, The Evitable Conflict, Vulcan's Hammer, Colossus, WOPR, AM, and Deep Thought. A supercomputer from Thinking Machines was mentioned as the supercomputer used to sequence the DNA extracted from preserved parasites in the Jurassic Park series.\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "37154", "revid": "39076087", "url": "https://en.wikipedia.org/wiki?curid=37154", "title": "Coxsackie A virus", "text": "Virus that causes hand, foot and mouth disease\nCoxsackie A virus (CAV) is a cytolytic Coxsackievirus of the \"Picornaviridae\" family, an enterovirus (a group containing the polioviruses, coxsackieviruses, and echoviruses).\nViral structure and genome.\nCoxsackie A virus is a subgroup of enterovirus A, which are small, non-enveloped, positive-sense, single-stranded RNA viruses. Its protective, icosahedral capsid has an external portion that contains sixty copies of viral proteins (VP1,-2,-3) and an internal portion surrounding the RNA genome containing sixty copies of VP4 viral proteins. This capsid mediates cell entry and elicits the humoral immune responses. Enteroviruses have a depression encircling each fivefold axis (canyon), which is their binding site for immunoglobulin-like receptors. This binding can trigger viral expansion and release of its genome.\nA complete genome analysis of Coxsackie virus A2, A4, A5, and A10 strains isolated from individuals with hand-foot-mouth disease showed that natural recombination is frequent in the virus's evolution. Its strains in China were related to strains in Mongolia, Taiwan, likely to those that circulated in Europe, and form a distinct lineage from strains imported from Japan and South Korea.\nReplication cycle.\nReplication of the coxsackie virus happens through contributions of the host and virus components. The virus enters the cell where it is internalized into endoplasmic reticulum and the Golgi apparatus. After viral un-coating, viral RNA is released. Ribosomes on the rough endoplasmic reticulum translate the RNA into viral polyprotein. This polyprotein is processed into structural protein P1 and non-structural proteins P2 and P3. Via the virus-encoded proteinase, P1 is processed into the viral capsid subunit proteins VP0, -1, -3. The 5'-non-coding region contains sequences that control genome replication and translation, while the 3'-non-coding region contains polyA tail needed for virus infectivity.\nDiseases.\nThe most well known Coxsackie A disease is hand, foot and mouth disease (unrelated to foot-and-mouth disease), a common childhood illness which affects mostly children aged 5 or under, often produced by Coxsackie A16. In most individuals, infection is asymptomatic or causes only mild symptoms. In others, infection produces short-lived (7\u201310 days) fever and painful blisters in the mouth (a condition known as \"herpangina\"), on the palms and fingers of the hand, or on the soles of the feet. There can also be blisters in the throat, or on or above the tonsils. Adults can also be affected. The rash, which can appear several days after high temperature and painful sore throat, can be itchy and painful, especially on the hands/fingers and bottom of feet.\nOther diseases include acute haemorrhagic conjunctivitis (A24 specifically), herpangina, and aseptic meningitis (both Coxsackie A and B viruses). Coxsackievirus A7 is associated with neurological diseases and can cause paralytic poliomyelitis.\nSigns and symptoms.\nCoxsackie A virus leads to a number of diseases, however the most common signs and symptoms that appear with infection are fever and flu-like symptoms, mouth sores, and skin rashes. People who are infected may present with a mild fever and sore throat, and a general discomfort three to six days subsequent to exposure. Painful mouth sores (herpangina) may be present in the back of the mouth. These sores usually appear 24 hours after the flu like symptoms begin, and may blister, causing further discomfort when eating or drinking. A flat, red skin rash may appear, commonly accompanied by fluid filled blisters and scabbing. Rash will commonly present on the bottom of the feet, palm of hands, and other areas of the body, and persists upwards to ten days.\nWhen the symptoms are incredibly severe, some may require hospitalization due to dehydration caused by inability to swallow food or water without pain, or seizures and convulsions can occur due to high fever. Signs of dehydration include dry skin, unintentional weight loss, or decreased urine output/darkened urine and if present should refer to a health care provider for intervention. Other serious complications include inflammatory brain conditions, such as viral meningitis or encephalitis, which require medical intervention. A professional health care may need to monitor if the someone infected is immunocompromised, or the symptoms do not improve within ten days.\nThe diagnosis of this disease centers around the appearance and behavior of fever, rash and mouth sores. Outside of the symptoms, age is also taken into consideration as the most common age of infection is under five years of age. A healthcare professional may choose to confirm the diagnosis through collecting samples from mouth sores and skin blisters, or a stool sample may also be ordered to rule out any other causes.\nOutbreaks.\nSince 2008, coxsackievirus A6 (CVA6) has been associated with several worldwide outbreaks of hand, foot and mouth disease (HFMD). In Finland, the initial HFMD case caused by the CVA6 lead to its identification of being the responsible pathogen of the outbreaks in Europe, North America, and Asia. Coxsackievirus A16 (CVA16) has also been linked to HFMD.\nOutbreaks are more commonly seen amongst children (those seven and younger) compared to outbreaks rates amongst adults. Due to this, there are outbreaks within daycares, summer camps, and early autumn.\nPregnancy.\nSerious pregnancy complications due to hand, foot, and mouth disease are rare due to its limited data. However, HFMD is concerning if the mother contracts the virus at the end of her pregnancy. CVA16 infection has been associated with third trimester massive perivillous fibrin deposition leading to intrauterine death. It has also led to first trimester spontaneous abortions. Overall, there is limited information about the Coxsackievirus A strain in pregnant women.\nOn the other hand, there has been some reports of the Coxsackievirus B (CVB) with regards to pregnant women. Contraction of the CVB are not associated with a higher risk of spontaneous abortions. Although, complications at the end of the pregnancy carries an increased risk of stillbirth or HFMD in the child. There have been reports of congenital heart defects and urogenital anomalies within the newborns of women who seroconverted to CVB during pregnancy. CVB is responsible for up to half of all individuals with pediatric myocarditis. In the past, it has been stated that newborns who have contracted CVB have a 75% mortality from myocarditis.\nTransmission.\nThe modes of transmission of the Coxsackie virus is primarily through contact between people, respiratory droplets (fluid from coughing and sneezing), and contaminated surfaces. All age groups can become infected with the Coxsackie virus, however it occurs most frequently in young children under the age of 10 and in who have a weakened immune system.\nThe main ways the Coxsackie Virus spreads are:\nAlthough adults are less susceptible to infection, it is still possible for an adult to get infected with the Coxsackie virus. If a pregnant mother is infected, there is a 30-50% chance the infection will be passed on to the infant.\nPrevention.\nThere is no vaccine to reduce the chances of infection and spread. It is critical to use non-pharmacological interventions to reduce the spread and transmission of Coxsackie virus. The best and most effective strategy for prevention is adopting proper hand hygiene, avoiding contact with the infected, abstain from touching mucous membranes of the face, and sanitizing frequently touched surfaces.\nPrognosis.\nSome of those who are infected with the Coxsackie virus may have complications that may lead to more serious issues. Complications include stomatitis, meningitis, pulmonary edema, myocarditis, pneumonia, and possibly spontaneous abortions.\nTreatment.\nTreatment is dependent on the disease process initiated by the virus. There is no known cure or vaccine against this virus.\nMost Coxsackie A virus infections are mild and self-limiting meaning the infection has the ability to resolve on its own without requiring treatment. Symptoms of a Coxsackie A infection tend to dissipate on their own within 7\u201310 days. Treatment tends to focus on supportive care where the symptoms of the infection are targeted but not the virus itself. NSAIDs such as ibuprofen/naproxen and acetaminophen can be used to manage the flu-like symptoms, fever, and any other pain the infected individual may be feeling. Do not give a child aspirin as it may increase the risk of Reye syndrome. Fluids are recommended as to decrease the chances of dehydration. Mouth sores will make eating and drinking painful and may potentially lead to loss of appetite and refusing to eat in order to prevent mouth and throat pain. Severe dehydration may lead to hospitalization. Additionally, topical oral analgesic medications or salt water rinses can be used to help numb the sores and ease the throat pain. Since Coxsackie A is a viral infection, antibiotics will have no effect on the infection as they only work on bacterial infections.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "37156", "revid": "205121", "url": "https://en.wikipedia.org/wiki?curid=37156", "title": "US Constitution", "text": ""}
{"id": "37157", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=37157", "title": "G-77", "text": ""}
{"id": "37160", "revid": "3525933", "url": "https://en.wikipedia.org/wiki?curid=37160", "title": "Reinhard", "text": "Reinhard is a German, Austrian, Danish, and to a lesser extent Norwegian and Swedish surname (from Germanic \"ragin\", counsel, and \"hart\", strong), and a spelling variant of Reinhardt.\nSee also.\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n Name listThis page or section lists people that share the same given name or the same family name. "}
{"id": "37161", "revid": "636570", "url": "https://en.wikipedia.org/wiki?curid=37161", "title": "Fuel injection", "text": "Feature of internal combustion engines\nFuel injection is the introduction of fuel in an internal combustion engine, most commonly automotive engines, by the means of a fuel injector. This article focuses on fuel injection in reciprocating piston and Wankel rotary engines.\nAll compression-ignition engines (e.g. diesel engines), and many spark-ignition engines (i.e. petrol (gasoline) engines, such as Otto or Wankel), use fuel injection of one kind or another. Mass-produced diesel engines for passenger cars (such as the Mercedes-Benz OM 138) became available in the late 1930s and early 1940s, being the first fuel-injected engines for passenger car use. In passenger car petrol engines, fuel injection was introduced in the early 1950s and gradually gained prevalence until it had largely replaced carburettors by the early 1990s. The primary difference between carburetion and fuel injection is that fuel injection atomizes the fuel through a small nozzle under high pressure, while carburetion relies on suction created by intake air accelerated through a Venturi tube to draw fuel into the airstream.\nThe term \"fuel injection\" is vague and comprises various distinct systems with fundamentally different functional principles. The only thing all fuel injection systems have in common is the absence of carburetion. \nThere are two main functional principles of mixture formation systems for internal combustion engines: \"internal\" and \"external\". A fuel injection system that uses external mixture formation is called a manifold injection system. There exist two types of manifold injection systems: \"multi-point\" (or \"port\") and \"single-point\" (or \"throttle body\") injection. \nInternal mixture formation systems can be separated into several different varieties of \"direct\" and \"indirect\" injection, the most common being the \"common-rail\" injection, a variety of direct injection. The term \"electronic fuel injection\" refers to any fuel injection system controlled by an engine control unit.\nSystem functions.\nThe fundamental functions of a fuel injection system are described in the following sections. In some systems, a single component performs multiple functions.\nPressurising fuel.\nFuel injection is operated by spraying pressurised fuel into the engine. Therefore a device to pressurise the fuel is needed, such as a fuel pump.\nMetering fuel.\nThe system must determine the appropriate amount of fuel to be supplied and control the fuel flow to supply this amount.\nSeveral early mechanical injection systems used relatively sophisticated helix-controlled injection pump(s) that both metered fuel and created injection pressure. Since the 1980s, electronic systems have been used to control the metering of fuel. More recent systems use an electronic engine control unit which meters the fuel and controls the ignition timing and various other engine functions.\nInjecting fuel.\nThe fuel injector is effectively a spray nozzle that performs the final stage in the delivery of fuel into the engine. The injector is located in the combustion chamber, inlet manifold or\u00a0\u2013 less commonly\u00a0\u2013 the throttle body.\nFuel injectors which also control the metering are called \"injection valves\", while injectors that perform all three functions are called \"unit injectors\".\nDirect injection systems.\nDirect injection means that the fuel is injected into the main combustion chamber of each cylinder. As air and fuel are mixed only inside the combustion chamber, air alone is sucked into the engine during the intake stroke. The injection scheme is always intermittent (either sequential or cylinder-individual).\nFuel is injected directly into the combustion chamber either with a blast of air or hydraulically, with the former rendered obsolete in automotive engines in the early 20th century by the invention of the precombustion chamber. \nTypically, hydraulic direct injection systems spray fuel into the air inside the cylinder or combustion chamber. Direct injection can be achieved with a conventional helix-controlled injection pump, unit injectors, or a sophisticated common-rail injection system. The last is the most common system in modern automotive engines.\nDirect injection for petrol engines.\nDuring the 20th century, most petrol engines used either a carburettor or indirect fuel injection. Use of direct injection in petrol engines has become increasingly common in the 21st century.\nCommon-rail injection systems.\nIn a common-rail system, fuel from the fuel tank is supplied to a common header (called the accumulator), and then sent through tubing to the injectors, which inject it into the combustion chambers. The accumulator has a high-pressure relief valve to maintain pressure and return the excess fuel to the fuel tank. The fuel is sprayed with the help of a nozzle that is opened and closed with a solenoid-operated needle valve. Third-generation common-rail diesels use piezoelectric injectors for increased precision, with fuel pressures up to .\nThe types of common-rail systems include \"air-guided injection\" and \"spray-guided injection\".\nUnit injector systems.\nUsed by diesel engines, these systems include:\nHelix-controlled pump systems.\nThis injection method was previously used in many diesel engines. Types of systems include:\nOther systems.\nThe M-System, used in some diesel engines from the 1960s to the 1980s, sprayed the fuel onto the walls of the combustion chamber, as opposed to most other direct-injection systems which spray the fuel into the middle of the chamber.\nIndirect injection systems.\nManifold injection.\nManifold injection systems are common in petrol-fuelled engines such as the Otto engine and the Wankel engine. In a manifold injection system, air and fuel are mixed outside the combustion chamber so that a mixture of air and fuel is sucked into the engine. The main types of manifold injections systems are \"multi-point injection\" and \"single-point injection\". \nThese systems use either a \"continuous injection\" or an \"intermittent injection\" design. In a continuous injection system, fuel flows at all times from the fuel injectors, but at a variable flow rate. The most common automotive continuous injection system is the \nmulti-point Bosch K-Jetronic system, introduced in 1974 and used until the mid-1990s by various car manufacturers. Intermittent injection systems can be \"sequential\", in which injection is timed to coincide with each cylinder's intake stroke; \"batched\", in which fuel is injected to the cylinders in groups, without precise synchronization to any particular cylinder's intake stroke; \"simultaneous\", in which fuel is injected at the same time to all the cylinders; or \"cylinder-individual\", in which the engine control unit can adjust the injection for each cylinder individually.\nMulti-point injection.\nMulti-point injection (also called 'port injection') injects fuel into the intake ports just upstream of each cylinder's intake valve, rather than at a central point within an intake manifold. Typically, multi-point injected systems use multiple fuel injectors, but some systems, such as GM's central port injection system, use tubes with poppet valves fed by a central injector instead of multiple injectors.\nSingle-point injection.\nSingle-point injection (also called 'throttle-body injection') uses one injector in a throttle body mounted similarly to a carburettor on an intake manifold. As in a carburetted induction system, the fuel is mixed with the air before entering the intake manifold. Single-point injection was a relatively low-cost way for automakers to reduce exhaust emissions to comply with tightening regulations while providing better \"driveability\" (easy starting, smooth running, no engine stuttering) than could be obtained with a carburettor. Many of the carburettor's supporting components\u2014such as the air filter, intake manifold, and fuel line routing\u2014could be used with few or no changes. This postponed the redesign and tooling costs of these components. Single-point injection was used extensively on American-made passenger cars and light trucks during 1980\u20131995, and in some European cars in the early and mid-1990s.\nIn the US, the G10 engine in the 2000 Chevrolet Metro became the last engine available on an American-sold vehicle to use throttle body injection.\nDiesel engines.\nIn indirect-injected diesel engines (as well as Akroyd engines), there are two combustion chambers: the main combustion chamber, and a pre-chamber (also called an ante-chamber) that is connected to the main one. The fuel is injected only into the pre-chamber (where it begins to combust), and not directly into the main combustion chamber. Therefore, this principle is called \"indirect injection\". There exist several slightly different indirect injection systems that have similar characteristics.\nTypes of indirect injection used by diesel engines include:\nHistory.\n1870s\u20131930s: early systems.\nIn 1872, George Bailey Brayton obtained a patent on an internal combustion engine that used a pneumatic fuel injection system, also invented by Brayton: air-blast injection.413 In 1894, Rudolf Diesel copied Brayton's air-blast injection system for the diesel engine, but also improved it.414 He increased the air blast pressure from to .415 In the meantime, the first manifold injection system was designed by Johannes Spiel in 1884, while working at \"Hallesche Maschinenfabrik\" in Germany. \nIn 1891, the British Herbert-Akroyd oil engine became the first engine to use a pressurised fuel injection system. This design, called a hot-bulb engine used a 'jerk pump' to dispense fuel oil at high pressure to an injector. Another development in early diesel engines was the pre-combustion chamber, which was invented in 1919 by Prosper l'Orange to avoid the drawbacks of air-blast injection systems. The pre-combustion chamber made it feasible to produce engines in size suitable for automobiles and MAN Truck &amp; Bus presented the first direct-injected diesel engine for trucks in 1924. Higher pressure diesel injection pumps were introduced by Bosch in 1927.\nIn 1898, German company Deutz AG started producing four-stroke petrol stationary engines with manifold injection. The 1906 Antoinette 8V aircraft engine (the world's first V8 engine) was another early four-stroke engine that used manifold injection. The first petrol engine with direct-injection was a two-stroke aircraft engine designed by Otto Mader in 1916. Another early spark-ignition engine to use direct-injection was the 1925 Hesselman engine, designed by Swedish engineer Jonas Hesselman. This engine could run on a variety of fuels (such as oil, kerosene, petrol or diesel oil) and used a stratified charge principle whereby fuel is injected towards the end of the compression stroke, then ignited with a spark plug.\nThe Cummins \"Model H\" diesel truck engine was introduced in America in 1933. In 1936, the Mercedes-Benz OM 138 diesel engine (using a precombustion chamber) became one of the first fuel-injected engines used in a mass-production passenger car.\n1940s\u20131950s: WWII aircraft and early direct-injection petrol engines.\nDuring World War II, several petrol engines for aircraft used direct-injection systems, such as the European Junkers Jumo 210, Daimler-Benz DB 601, BMW 801, and the Shvetsov ASh-82FN (M-82FN). The German direct-injection systems were based on diesel injection systems used by Bosch, Deckel, Junkers and l'Orange. By around 1943, the Rolls-Royce Merlin and Wright R-3350 had switched from traditional carburettors to fuel-injection (called \"pressure carburettors\" at the time), however these engines used throttle body manifold injection, rather than the direct-injection systems of the German engines. From 1940, the Mitsubishi Kinsei 60 series engine used a direct-injection system, along with the related Mitsubishi Kasei engine from 1941. In 1943, a low-pressure fuel injection system was added to the Nakajima Homare Model 23 radial engine.\nThe first mass-produced petrol direct-injection system was developed by Bosch and initially used in small automotive two-stroke petrol engines. Introduced in the 1950 Goliath GP700 small saloon, it was also added to the Gutbrod Superior engine in 1952. This mechanically controlled system was essentially a specially lubricated high-pressure diesel direct-injection pump of the type that is governed by the vacuum behind an intake throttle valve. A Bosch mechanical direct-injection system was also used in the straight-eight used in the 1954 Mercedes-Benz W196 Formula One racing car. The first four-stroke direct-injection petrol engine for a passenger car was released the following year, in the Mercedes-Benz 300SL sports car. However the engine suffered lubrication problems due to petrol diluting the engine oil, and subsequent Mercedes-Benz engines switched to a manifold injection design. Likewise, most petrol injection systems prior to the 2000s used the less-expensive manifold injection design.\n1950s\u20131970s: manifold injection for petrol engines.\nThroughout the 1950s, several manufacturers introduced their manifold injection systems for petrol engines. Lucas Industries had begun developing a fuel injection system in 1941 and by 1956 it was used in the Jaguar racing cars. At the 1957 24 Hours of Le Mans, the 1st to 4th placed cars were Jaguar D-Type entries using a Lucas fuel injection system. Also in 1957, General Motors introduced the Rochester Ramjet option, consisting of a fuel injection system for the V8 engine in the Chevrolet Corvette. During the 1960s, fuel injection systems were also produced by Hilborn, SPICA and Kugelfischer.\nUp until this time, the fuel injection systems had used a mechanical control system. In 1957, the American Bendix Electrojector system was introduced, which used analogue electronics for the control system. The Electrojector was intended to be available for the Rambler Rebel mid-size car, however reliability problems meant that the fuel injection option was not offered. In 1958, the Chrysler 300D, DeSoto Adventurer, Dodge D-500 and Plymouth Fury offered the Electrojector system, becoming the first cars known to use an electronic fuel injection (EFI) system.\nThe Electrojector patents were subsequently sold to Bosch, who developed the Electrojector into the Bosch D-Jetronic. The D-Jetronic was produced from 1967-1976 and first used on the VW 1600TL/E. The system was a speed/density system, using engine speed and intake manifold air density to calculate the amount of fuel to be injected. In 1974, Bosch introduced the K-Jetronic system, which used a continuous flow of fuel from the injectors (rather than the pulsed flow of the D-Jetronic system). K-Jetronic was a mechanical injection system, using a plunger actuated by the intake manifold pressure which then controlled the fuel flow to the injectors. \nAlso in 1974, Bosch introduced the L-Jetronic system, a pulsed flow system which used an air flow meter to calculate the amount of fuel required. L-Jetronic was widely adopted on European cars during the 1970s and 1980s. As a system that uses electronically controlled fuel injectors which open and close to control the amount of fuel entering the engine, the L-Jetronic system uses the same basic principles as modern electronic fuel injection (EFI) systems.\n1980s\u2013present: digital electronics and common-rail injection.\nPrior to 1979, the electronics in fuel injection systems used analogue electronics for the control system. The Bosch Motronic multi-point fuel injection system (also amongst the first systems where the ignition system is controlled by the same device as the fuel injection system) was the first mass-produced system to use digital electronics. The Ford EEC-III single-point fuel injection system, introduced in 1980, was another early digital fuel injection system. These and other electronic manifold injection systems (using either port injection or throttle-body injection) became more widespread through the 1980s, and by the early 1990s they had replaced carburettors in most new petrol-engined cars sold in developed countries.\nThe aforementioned injection systems for petrol passenger car engines\u00a0\u2013 except for the 1954\u20131959 \"Mercedes-Benz 300\u00a0SL\"\u00a0\u2013 all used manifold injection (i.e. the injectors located at the intake ports or throttle body, instead of inside the combustion chamber). This began to change when the first mass-produced petrol direct injection system for passenger cars was a common rail system introduced in the 1997 Mitsubishi 6G74 V6 engine. The first common-rail system for a passenger car diesel engine was the Fiat Multijet straight-four engine, introduced in the 1999 \"Alfa Romeo 156 1.9 JTD\" model. Since the 2010s, many petrol engines have switched to direct-injection (sometimes in combination with separate manifold injectors for each cylinder). Similarly, many modern diesel engines use a common-rail design.\nStratified charge injection was used in several petrol engines in the early 2000s, such as the introduced in 2000. However, the stratified charge systems were largely no longer in use by the late 2010s, due to increased exhaust emissions of gasses and particulates, along with the increased cost and complexity of the systems.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "37162", "revid": "201064", "url": "https://en.wikipedia.org/wiki?curid=37162", "title": "Roland Freisler", "text": "German jurist (1893\u20131945)\nKarl Roland Freisler (30 October 1893 \u2013 3 February 1945) was a German jurist, judge, and politician who served as the State Secretary of the Reich Ministry of Justice from 1935 to 1942 and as president of the People's Court from 1942 to 1945. As a prominent ideologist of Nazism, he influenced as a jurist the Nazification of the German legal system. He was appointed president of the People's Court in 1942, overseeing the prosecution of political crimes as a judge. Freisler became known for his aggressive personality, his humiliation of defendants, and his frequent use of the death penalty in sentencing.\nA law student at Kiel University, Freisler joined the Imperial German Army on the outbreak of the First World War and saw action on the Eastern Front, where he was wounded and taken prisoner of war by the Imperial Russian Army. On his return to Germany, he completed his law studies at the University of Jena and was awarded a Doctorate of Law in 1922. Freisler joined the Nazi Party in 1925, upon which he began defending Party members in court for acts of political violence.\nAfter the Nazi seizure of power in 1933, Freisler was appointed State Secretary of the Prussian Ministry of Justice; two years later he became State Secretary in the unified Reich Ministry of Justice. Through his zealotry as well as his legal and verbal dexterity, he quickly established himself as the most feared judge in Nazi Germany and the personification of the Nazi ideology in domestic law. In 1942, representing Acting \"Reichsminister\" of Justice Franz Schlegelberger, Freisler attended the Wannsee Conference, the event which set the Holocaust in motion.\nIn August 1942, Freisler succeeded Otto Georg Thierack as president of the People's Court. He presided over the show trials of the White Rose resistance group and perpetrators of the 20 July plot and handed out over 5,000 death sentences in his three-year tenure. Freisler was killed in February 1945 during an American bombing raid on Berlin. Although the death penalty was abolished with the creation of the Federal Republic in 1949, Freisler's 1941 definition of murder in German law, as opposed to the less severe crime of manslaughter, survives in the Strafgesetzbuch \u00a7 211.\nEarly life.\nRoland Freisler was born on 30 October 1893 in Celle, Lower Saxony, the son of Julius Freisler (b. 1862 in Klantendorf, Moravia), an engineer and teacher, and Charlotte Auguste Florentine Schwerdtfeger (1863\u20131932). He was baptized as a Protestant on 13 December 1893. He had a younger brother, Oswald, who became a lawyer, and another brother who became a doctor. He attended the Wilhelmsgymnasium Kassel and received his \"Abitur\" in 1912, graduating at the top of his class.\nWorld War I.\nFreisler was attending law school at Kiel University upon the outbreak of World War I in 1914, which interrupted his studies. He saw active service in the German Imperial Army during the war after enlisting as a \"Fahnenjunker\" (officer cadet) in 1914 with the 167th Infantry Regiment (1st Upper Alsatian) in Kassel, and by 1915 he was a \"Leutnant\". While serving on the front-line with the 22nd Division, he was awarded the Iron Cross 2nd class, for heroism in action. In October 1915, he was wounded in action on the Eastern Front and taken as a prisoner of war by Imperial Russian forces.\nWhile a prisoner, Freisler learned to speak Russian and developed an interest in Marxism after the Russian Revolution had commenced. The Bolshevik provisional authority which took over responsibility for Freisler's prisoner of war camp made use of him as a camp \"Commissar\" (as he was described by them in his repatriated prisoner of war paperwork in 1918) administratively organizing the camp's food supplies from 1917 to 1918.\nAccording to historian Georg Franz\u2013Willig's \"Ursprung der Hitlerbewegung\" 3 volume set published by Sch\u00fctz, Pr. Oldendorf, 1974, the SPD newspaper \"Vorw\u00e4rts\" of 3 May 1924 ran an article entitled \"SPIRITUAL KINSHIP: JEWISH\u2013COMMUNIST, POPULAR REICHSTAG CANDIDATE\", in which it stated that Freisler had been \"until rather recently a member of the German Communist Party\" and that this was interesting because \"his grandmother was a full Jewess\". However, H.W. Koch states that there is no evidence that Freisler was of Jewish extraction, and that after the Russian Revolution the description \"Commissar\" was simply the title given to anyone employed in an administrative post in the prison camps and had no political connotations. He also states that Freisler was never a Communist, although in the early days of his Nazi Party career in the 1920s, he was on the movement's left wing.\nIn the late 1930s, during Joseph Stalin's Great Purge in the Soviet Union, Freisler attended the Moscow Trials to watch the proceedings against the condemned. Freisler later rejected any insinuation that he had ever co-operated with the Soviets, the ideological nemesis of Nazi Germany but rumours about his time as a \"Commissar\" with the \"Reds\" cast a shadow over his subsequent career as a political official in Germany.\nInterwar legal and political career.\nFreisler returned to Germany in 1919 to complete his law studies at the University of Jena, and he qualified as a Doctor of Law in 1922. In 1924, he began working as an \"Assessor\" in Kassel and also was elected as a city councillor for the V\u00f6lkisch-Social Bloc, an ultranationalist splinter party. He joined the Nazi Party in July 1925 (membership number 9,679) and immediately gained a position of authority within the organisation by using his legal training to defend Party members and \"Sturmabteilung\" (SA) men who were regularly facing prosecutions for acts of political violence. As an early Party member, or \"Alter K\u00e4mpfer\", he would later be awarded the Golden Party Badge. From late 1925 to September 1927, Freisler was the Deputy \"Gauleiter\" in \"Gau Hessen-Nassau Nord\" under Walter Schultz. He was also a member of the Party's National Socialist Motor Corps (NSKK), attaining the rank of NSKK-\"Brigadef\u00fchrer\" in 1942. As the Nazis changed from a fringe political beer hall and street fighting movement into a political party, Freisler was elected to the Hesse-Nassau provincial \"Landtag\", a position he held between 1930 and 1933. In 1931, he joined the Association of National Socialist German Legal Professionals, founded by fellow Nazi lawyer Hans Frank. He was elected to the Prussian Landtag in April 1932 serving until the Landtag was dissolved in October 1933. At the November 1933 German parliamentary election he was elected as a deputy of the \"Reichstag\", retaining his seat until his death. Reelected in 1936 and 1938, he represented electoral constituencies 19 (Hesse-Nassau), 13 (Schleswig-Holstein) and 35 (Mecklenburg), respectively.\nIn 1927, Karl Weinrich, a Nazi member of the Prussian \"Landtag\" along with Freisler, characterised his reputation in the rapidly expanding Nazi movement in the late 1920s: \"Rhetorically Freisler is equal to our best speakers, if not superior; particularly on the broad masses he has influence, but thinking people mostly reject him. Party Comrade Freisler is usable as only a speaker though and is unsuitable for any position of authority because of his unreliability and moodiness\".\nCareer in Nazi Germany.\nIn February 1933, after the Nazi seizure of power, Freisler was appointed Ministerial Director in the Prussian Ministry of Justice under Hans Kerrl. He was placed in charge of the personnel office and used his authority to force out Jewish members of the staff. By June, he was promoted to State Secretary in the Ministry. On 31 July, Prussian Minister president Hermann G\u00f6ring appointed him to the recently reconstituted Prussian State Council. On the founding of the Academy for German Law by Hans Frank in October 1933, Freisler was made a member. He was the chairman of its Criminal Law Committee, head of its department of scientific studies and editor of the Academy newspaper. When the Prussian Ministry of Justice was merged with the Reich Ministry of Justice on 1 April 1935, Freisler became the State Secretary in the unified Ministry, where he served until August 1942.\nFreisler's mastery of legal texts, mental agility, dramatic courtroom verbal dexterity and verbal force, in combination with his zealous conversion to Nazi ideology, made him the most feared judge in Nazi Germany, and the personification of Nazism in domestic law. Despite his talents and loyalty, Adolf Hitler never appointed him to any post beyond the legal system. That might have been because he was a lone figure, lacking support within the senior echelons of the Nazi hierarchy but he had also been politically compromised by his brother, Oswald Freisler, also a lawyer. Oswald had acted as a defence counsel against the regime's authority several times during the increasingly politically driven trials by which the Nazis sought to enforce their control of German society and he had the habit of wearing his Nazi Party membership badge in court whilst doing so. Propaganda minister Joseph Goebbels reproached Oswald Freisler and reported his actions to Adolf Hitler who ordered Freisler's expulsion from the Party. (Oswald Freisler died, allegedly by suicide, in 1939.)\nIn 1941, in a discussion at the \"F\u00fchrer Headquarters\" about whom to appoint to replace Franz G\u00fcrtner, the Reich Justice Minister, who had died, Goebbels suggested Roland Freisler as an option; Hitler's reply, referring to Freisler's alleged \"Red\" past, was: \"That old Bolshevik? No!\"\nContribution to the Nazification of the law.\nFreisler was a committed Nazi ideologist and used his legal skills to adapt its theories into practical law-making and judicature. He published a paper entitled \"Die rassebiologische Aufgabe bei der Neugestaltung des Jugendstrafrechts\" (\"The racial-biological task involved in the reform of juvenile criminal law\"). In this document he argued that \"racially foreign, racially degenerate, racially incurable or seriously defective juveniles\" should be sent to juvenile centres or correctional education centres and segregated from those who are \"German and racially valuable\".\nHe strongly advocated the creation of laws to punish \"Rassenschande\" (\"race defilement\", the Nazi term for sexual relations between \"Aryans\" and \"inferior races\"), to be classed as \"racial treason\". Freisler looked to racist laws in the United States states as a model for Nazi legislation to target Jews in Germany. Freisler considered Jim Crow racist legislation \"primitive\" for failing to provide a legal definition of the term black or negro person. While some more conservative Nazi lawyers objected to the lack of precision with which a person could be defined as a \"Jew,\" he argued that American judges were able to identify black people for purposes of laws in American states that prohibited \"miscegenation\" between black and white people and laws that otherwise codified racial segregation and German laws could similarly target Jews even if the term \"Jew\" could not be given a precise legal definition.\nIn 1933, he published a pamphlet calling for the legal prohibition of \"mixed-blood\" sexual intercourse, which met with expressions of public unease in the dying elements of the German free press and non-Nazi political classes and lacked public authorization from the policy of the Nazi Party, which had only just obtained dictatorial control of the state. It also led to a clash with his superior Franz G\u00fcrtner but Freisler's ideological views reflected things to come, as was shown by the enactment of the Nuremberg Laws within two years.\nIn October 1939, Freisler introduced the concept of 'precocious juvenile criminal' in the \"Juvenile Felons Decree\". This \"provided the legal basis for imposing the death penalty and penitentiary terms on juveniles for the first time in German legal history\". Between 1933 and 1945, the Reich's courts sentenced at least 72 German juveniles to death, among them 17-year-old Helmuth H\u00fcbener, found guilty of high treason for distributing anti-war leaflets in 1942.\nOn the outbreak of World War II, Freisler issued a legal \"Decree against National Parasites\" (September 1939) introducing the term \"perpetrator type\", which was used in combination with another Nazi ideological term, \"parasite\". The adoption of racial biological terminology into law portrayed juvenile criminality as \"parasitical\", implying the need for harsher sentences to remedy it. He justified the new concept with: \"in times of war, breaches of loyalty and baseness cannot find any leniency and must be met with the full force of the law\".\nOn 8 July 1940, the Justice Ministry received a written complaint from a senior local court judge protesting against the euthanasia killings of physically or mentally disabled individuals that had claimed his wards. Freisler met with him and explained that the ministry was establishing orderly procedures for the program with \"expert committees\" and \"grievance councils\" but he did not dispute the legality of the killings, arguing that the Nazi state had brought about a new concept of law. The judge continued to protest and some months later, after a second meeting with \"Reichsminister\" G\u00fcrtner reinforced Freisler's position, the judge was forced to retire.\nOn 31 October 1941, Freisler issued a directive that Jewish inmates must wear the identifying yellow badge in Reich prisons. He also worked closely with the \"Reichsstatthalter\" of Reichsgau Wartheland, Arthur Greiser, on standardizing penalties for Jews and Poles in the occupied eastern territories. They concluded that the death penalty or concentration camp imprisonment, imposed by special courts-martial, were the only acceptable punishments for these categories of individuals, even for minor offenses. These penal regulations came into force in December 1941, and also were applied to Jews who were transported into the eastern territories.\nWannsee Conference.\nOn 20 January 1942, Freisler, representing Acting \"Reichsminister\" of Justice Franz Schlegelberger, attended the Wannsee Conference of senior governmental officials in a villa on the southwestern outskirts of Berlin to provide expert legal advice for the planning of the destruction of European Jewry. The official minutes of the conference do not record any comments by Freisler; he had a history of antisemitism and knowledge of the regime's use of extrajudicial killing in other contexts and was certainly aware that the purpose of the meeting was to discuss the extermination of the Jews.\nPresidency of the People's Court.\nOn 20 August 1942, Hitler promoted Otto Georg Thierack to Reich Justice Minister, replacing the retiring Schlegelberger, and named Freisler to succeed Thierack as president of the People's Court (\"Volksgerichtshof\"). This court had jurisdiction over a broad array of political offences, including black marketeering, work slowdowns and defeatism. These actions were viewed by Freisler as \"Wehrkraftzersetzung\" (undermining defensive capability) and were punished severely, with many death sentences. The People's Court under Freisler's domination almost always sided with the prosecuting authority, to the point that being brought before it was tantamount to a capital charge. Its separate administrative existence beyond the ordinary judicial system, despite its trappings, rapidly turned it into an executive execution arm and psychological domestic terror weapon of the regime, in the tradition of a revolutionary tribunal rather than a court of law.\nHe chaired the First Senate of the People's Court wearing a blood scarlet judicial robe, in a hearing chamber bedecked with scarlet swastika-draped banners and a large black sculpted bust of Adolf Hitler's head upon a high pedestal behind his chair, opening each hearing session with the Nazi salute from the bench. He acted as prosecutor, judge and jury combined, and also as his own recorder, thereby controlling the record of the written grounds for the sentences that he passed.\nThe frequency of death sentences rose sharply under Freisler's rule. Approximately 90% of all cases that came before him ended in guilty verdicts. Between 1942 and 1945, more than 5,000 death sentences were decreed by him, 2,600 of these through the court's First Senate, which Freisler controlled. He was responsible in his three years on the court for as many death sentences as all other senate sessions of the court combined in the court's existence between 1934 and 1945.\nFreisler became known during this period for berating each member of the steady stream of defendants passing before him. He was known to be interested in Andrey Vyshinsky, the Chief Prosecutor of the Soviet purge trials and had attended those show trials to watch Vyshinsky's courtroom performances in a similar capacity in Moscow in 1938.\nWhite Rose show-trials.\nOn 18 February 1943, Sophie Scholl and Hans Scholl were captured by the Gestapo. Through questioning, it became clear that the two siblings were part of a resistance group called the White Rose that was attempting to sow discord in Germany by the use of mailing pamphlets urging passive resistance. A third resistance member, Christoph Probst was soon arrested. On 22 February 1943, Freisler was flown into Munich for the sole purpose of presiding over their trial. The verdict was as expected, guilty. Freisler sentenced the three to death by hanging but fearful of them being raised to martyrdom status if they were publicly killed, it was decided to execute them by guillotine instead.\nOn 19 April 1943, Freisler was flown back again to stand as judge over the second trial of the White Rose members. Out of the thirteen defendants, three were sentenced to death, nine were given prison sentences and one was acquitted.\n20 July Plot show-trials.\nIn August 1944, some of the arrested perpetrators of the 20 July Plot against Adolf Hitler were brought before Freisler for punishment. The proceedings were filmed to be shown to the German public in cinema newsreels, and show how Freisler ran his court. He would often alternate between questioning the defendants in an analytical manner, then suddenly launch into a tirade, even going so far as to shout insults at the accused from the bench. The shift from cold, clinical interrogation to fits of screaming rage was designed to psychologically disarm, torment and humiliate those on trial while discouraging any attempt on their part to defend or justify their actions. At one point, Freisler yelled at Field Marshal Erwin von Witzleben, who was trying to hold up his trousers after being purposely given old, oversized and beltless clothing: \"You dirty old man, why do you keep fiddling with your trousers?\"\nAnother instance is from Freisler's public appearance during the trial of the defendant Ulrich-Wilhelm Graf Schwerin von Schwanenfeld. The footage taken shows Freisler drowning out Schwerin's weak and muted testimony, prompted by his concern over the Wehrmacht's \"numerous murders in Poland\", by roaring at him in an exaggerated and theatrical manner, declaring \"Sie sind ja ein sch\u00e4biger Lump!\" (roughly, \"You really are a lousy piece of trash!\").\nNearly all of the accused were sentenced to death by hanging, with some of the sentences being carried out within two hours of the verdict being delivered.\nDeath.\nOn the morning of 3 February 1945, Freisler was conducting a Saturday session of the People's Court when United States Army Air Forces bombers attacked Berlin, led by the B-17 of Lieutenant Colonel Robert Rosenthal. Government and Nazi Party buildings were hit, including the Reich Chancellery, the Gestapo headquarters, the Party Chancellery and the People's Court. Hearing the air raid sirens, Freisler hastily adjourned the court and ordered that the prisoners be taken to an air raid shelter but he stayed behind to gather files before leaving.\nA bomb struck the court building at 11:08, causing a partial internal collapse and a masonry column came crashing down on Freisler, crushing and killing him instantly. A large portion of the courtroom also landed on Freisler's corpse. The flattened remains of Freisler were found beneath the rubble still clutching the files he had stopped to retrieve. Among the files was that of Fabian von Schlabrendorff, a bomb plotter who was on trial that day and facing execution.\nA differing account stated that Freisler \"was killed by a bomb fragment while trying to escape from his law court to the air-raid shelter\" and that he \"bled to death on the pavement outside the People's Court at Bellevuestrasse 15 in Berlin\". Schlabrendorff was \"standing near Freisler when the latter met his end\". Schlabrendorff was re-tried and acquitted, and survived the war, ultimately following Freisler as a judge, on the Federal Constitutional Court.\nA foreign correspondent reported, \"Apparently nobody regretted his death\". Luise Jodl, wife of General Alfred Jodl, recounted more than 25 years later that she had been working at the L\u00fctzow Hospital when Freisler's body was brought in, and that a worker commented, \"It is God's verdict\". According to Mrs. Jodl, \"Not one person said a word in reply\". His body was buried in the grave of his wife's family at the Waldfriedhof Dahlem Cemetery in Berlin. His name is not recorded on the gravestone.\nPersonal life.\nHe married Marion Russegger on 24 March 1928; the couple had two sons, Harald and Roland.\nFreisler in film and fiction.\nFreisler appears in fictional form in the Hans Fallada novel \"Every Man Dies Alone\" (1947). In 1943, he tried and handed down death penalties to Otto and Elise Hampel, who were both guillotined for distributing anti-Nazi postcards and whose true story inspired Fallada's novel.\nIn the novel \"Fatherland\" (1992) by Robert Harris, which takes place in an alternate 1964 in which Nazi Germany won World War II, Freisler is mentioned as having survived until winter 1954, when he is killed by a maniac with a knife on the steps of the Berlin People's Court. It is implied that his death was actually caused by the Gestapo, to ensure that the Wannsee Conference and the Holocaust remained a secret.\nFreisler has been portrayed by screen actors at least eight times, by Rainer Steffen in the German television film \"Die Wannseekonferenz\" (1984), by Roland Sch\u00e4fer in the Anglo-French-German film \"Reunion\" (1989), by Brian Cox in the British television film \"Witness Against Hitler\" (1996), by Owen Teale in the BBC\u2013HBO film \"Conspiracy\" (2001), by Andr\u00e9 Hennicke in the film \"Sophie Scholl \u2013 The Final Days\" (2005), by Helmut Stauss in the film \"Valkyrie\" (2008) by Karl Knaup in \"Rommel\" (2012, uncredited) and by Arnd Klawitter in the German television film \"Die Wannseekonferenz\" (2022).\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nBibliography.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "37165", "revid": "22651524", "url": "https://en.wikipedia.org/wiki?curid=37165", "title": "Grand Slam", "text": "Grand Slam or Grand slam may refer to:\n&lt;templatestyles src=\"Template:TOC_right/styles.css\" /&gt;\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "37166", "revid": "784330", "url": "https://en.wikipedia.org/wiki?curid=37166", "title": "Openlaw", "text": "Openlaw is a project at the Berkman Klein Center for Internet &amp; Society at Harvard Law School aimed at releasing case arguments under a copyleft license, in order to encourage public suggestions for improvement.\nBerkman lawyers specialise in cyberlaw\u2014hacking, copyright, encryption and so on\u2014and the centre has strong ties with the EFF and the open source software community. In 1998 faculty member Lawrence Lessig, now at Stanford Law School, was asked by online publisher Eldritch Press to mount a legal challenge to US copyright law. Eldritch takes books whose copyright has expired and publishes them on the Web, but legislation called the Sonny Bono Copyright Term Extension Act extended copyright from 50 to 70 years after the author's death, cutting off its supply of new material. Lessig invited law students at Harvard and elsewhere to help craft legal arguments challenging the new law on an online forum, which evolved into Openlaw.\nNormal law firms write arguments the way commercial software companies write code. Lawyers discuss a case behind closed doors, and although their final product is released in court, the discussions or \"source code\" that produced it remain secret. In contrast, Openlaw crafts its arguments in public and releases them under a copyleft license. \"We deliberately used free software as a model,\" said Wendy Seltzer, who took over Openlaw when Lessig moved to Stanford. Around 50 legal scholars worked on Eldritch's case, and Openlaw has taken other cases, too.\n\"The gains are much the same as for software,\" Seltzer says. \"Hundreds of people scrutinise the 'code' for bugs, and make suggestions how to fix it. And people will take underdeveloped parts of the argument, work on them, then patch them in.\" Armed with arguments crafted in this way, Openlaw took Eldritch's case\u2014deemed unwinnable at the outset\u2014right through the system to the Supreme Court. The case, \"Eldred v. Ashcroft\", lost in 2003.\nAmong the drawbacks to this approach: the arguments are made in public from the start, so Openlaw can't spring a surprise in court. Nor can it take on cases where confidentiality is important. But where there's a strong public interest element, open sourcing has big advantages. Citizens' rights groups, for example, have taken parts of Openlaw's legal arguments and used them elsewhere. \"People use them on letters to Congress, or put them on flyers,\" Seltzer says.\nFurther reading.\nThis modified article was originally written by New Scientist magazine (see https://www.newscientist.com/hottopics/copyleft/) and released under the copyleft license."}
{"id": "37167", "revid": "33145", "url": "https://en.wikipedia.org/wiki?curid=37167", "title": "Loris", "text": "Subfamily of primates\nLoris is the common name for the wet-nosed primates of the subfamily Lorinae (sometimes spelled Lorisinae) in the family Lorisidae. \"Loris\" is one genus in this subfamily and includes the slender lorises, \"Nycticebus\" is the genus containing the slow lorises, and \"Xanthonycticebus\" is the genus name of the pygmy slow loris.\nDescription.\nLorises are nocturnal and arboreal. They are found in tropical and woodland forests of India, Sri Lanka, and parts of southeast Asia. They resemble lemurs. A loris's locomotion is a slow and cautious climbing form of quadrupedalism. Some lorises are almost entirely insectivorous, while others also include fruits, gums, leaves, and slugs in their diet.\nLorises are strepsirrhines, most of which have a toothcomb, a special adaptation in their lower front teeth. This toothcomb is used for grooming their fur and even injecting their venom.\nFemale lorises practice \"infant parking\", leaving their infants in trees or bushes. Before they do this, they bathe their young with allergenic saliva that is acquired by licking patches on the insides of their elbows which produce a mild toxin that discourages most predators, though orangutans occasionally eat lorises.\nTaxonomic classification.\nThe family Lorisidae is found within the infraorder Lemuriformes and superfamily Lorisoidea, along with the family Galagidae, the galagos. This superfamily is a sister taxon of Lemuroidea, the lemurs. Within Lorinae, there are ten species (and several more subspecies) of lorises across three genera:\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nExternal links.\n&lt;templatestyles src=\"Sister-inline/styles.css\"/&gt; Data related to at Wikispecies"}
{"id": "37168", "revid": "49788739", "url": "https://en.wikipedia.org/wiki?curid=37168", "title": "Friday the 13th", "text": "Unlucky day in popular superstition\nFriday the 13th is considered an unlucky day in Western superstition. It occurs when the 13th day of the month in the Gregorian calendar falls on a Friday, which happens at least once every year but can occur up to three times in the same year.\nFor a month to have a Friday the 13th, the first day of the month must be a Sunday.\nOrigins.\nUnluckiness of 13.\nOne source mentioned for the unlucky reputation of the number 13 is a Norse myth about twelve gods having a dinner party in Valhalla. The trickster god Loki, who was not invited, arrived as the thirteenth guest, and arranged for H\u00f6\u00f0r, the god of darkness, to shoot Balder, the god of joy and gladness, with a mistletoe-tipped arrow. Balder died, triggering much suffering in the world, which caused the number 13 to be considered unlucky.\nChristian associations.\nThe superstition seems to relate to various things, like the story of Jesus's Last Supper and crucifixion in which there were thirteen individuals present in the Upper Room on the thirteenth of Nisan Maundy Thursday, the night before his death on Good Friday.\nIn conjunction with Friday.\nWhile there is evidence of both Friday and the number\u00a013 being considered unlucky, there is no record of the two items being referred to as especially unlucky in conjunction before the 19th\u00a0century.\nThe Knights Templar.\nSome cite the arrest of the Knights Templar on Friday, October 13, 1307, by officers of King Philip IV of France as the origin of the Friday the 13th superstition, but it is agreed the origins remain murky.\n19th century.\nIn France, Friday 13th might have been associated with misfortune as early as the first half of the 19th century. A character in the 1834 play \"Les Finesses des Gribouilles\" states, \"I was born on a Friday, December 13th, 1813 from which come all of my misfortunes\".\nAn early documented reference in English occurs in H. S. Edwards' biography of Gioachino Rossini, who died on Friday 13th of November 1868:\n\"Rossini was surrounded to the last by admiring friends; and if it be true that, like so many Italians, he regarded Fridays as an unlucky day and thirteen as an unlucky number, it is remarkable that on Friday 13th of November he passed away.\"\nDissemination.\nIt is possible that the publication in 1907 of T. W. Lawson's popular novel \"Friday, the Thirteenth\",\ncontributed to popularizing the superstition. In the novel, an unscrupulous broker takes advantage of the superstition to create a Wall Street panic on a Friday the 13th.\nOccurrence.\nDistribution.\nEach 400-year Gregorian solar cycle contains 146,097\u00a0days (with 97\u00a0leap days) or exactly 20,871\u00a0weeks. Each cycle contains the same pattern of days of the week and therefore the same pattern of Fridays that are on the 13th. The 13th\u00a0day of the month is very slightly more likely to be a Friday than any other day of the week.\nAny month that starts on a Sunday contains a Friday the 13th, and there is at least one Friday the 13th in every calendar year.\nThe months with a Friday the 13th are determined by the Dominical letter (G, F, GF, etc.) of the year. Years which begin on the same day of the week and are of the same type (i.e. common year or leap year), will have a Friday the 13th in the same months.\nThis sequence, given here for 1900\u20132099, follows a 28-year cycle from 1\u00a0March 1900 to 28\u00a0February 2100:\nFrequency.\nAlthough there is always at least one Friday the 13th per calendar year, it can be as long as 14\u00a0months between two Friday the 13ths. The longest period that occurs without a Friday the 13th is 14\u00a0months, either from July to September the following year being a common year starting on Tuesday (F) (e.g. 2018\u201319 and 2029-30), or from August to October the following year being a leap year starting on Saturday (BA) (e.g. 1999\u20132000 and 2027\u201328). The shortest period that occurs with a Friday the 13th is just one month, from February to March in a common year starting on Thursday (D) (e.g. 2015 and 2026).\nOn average, there is a Friday the 13th once every 212.35\u00a0days. Friday the 13ths occurs with an average frequency of 1.7218 per year or about 3477 since the year 1 CE.\nFrequency within a single year.\nThere can be no more than three Friday the 13ths in a single calendar year; either in February, March, and November in a common year starting on Thursday (such as 2015 or 2026) (D), or January, April, and July in a leap year starting on Sunday (such as 2012 or 2040) (AG).\nIn the 2000s, there were three Friday the 13ths in 2009, and two Friday the 13ths in 2001, 2002, 2006, and 2007. In the 2010s, there were three Friday the 13ths in 2012 and 2015, and two in 2013, 2017, 2018, and 2019. In the 2020s, there were two Friday the 13ths in 2020, 2023 and 2024. There will also be three Friday the 13ths in 2026, and two in 2029. The remaining years all have one Friday the 13th.\nFor the details see the table below; this table is for the Gregorian calendar and Jan/Feb for leap years:\nSocial influence.\nAccording to the Stress Management Center and Phobia Institute in Asheville, North Carolina, an estimated 17\u201321\u00a0million people in the United States are affected by \"Paraskevidekatriaphobia\" (fear of Friday the 13th), making it the most feared day and date in history. Some people are so paralyzed by fear that they avoid their normal routines in doing business, taking flights or even getting out of bed. It has been estimated that US$ 800\u2013900\u00a0million is lost in business on this day.\nDespite this, representatives for both Delta Air Lines and Continental Airlines (the latter now merged into United Airlines) have stated that their airlines do not suffer from any noticeable drop in travel on those Fridays.\nIn Finland, a consortium of governmental and nongovernmental organizations led by the Ministry of Social Affairs and Health promotes the National Accident Day (\"kansallinen tapaturmap\u00e4iv\u00e4\") to raise awareness about automotive safety, which always falls on a Friday the 13th.\nThe event is coordinated by the Finnish Red Cross and has been held since 1995.\nRate of accidents.\nA study by Scanlon, Luben, Scanlon, &amp; Singleton (1993)\nattracted attention from popular science literature,\nas it concluded that \"the risk of hospital admission as a result of a transport accident \"may\" be increased by as much as 52\u00a0percent on the 13th\";1584\nhowever, the authors clearly state that \"the numbers of admissions from accidents are too small to allow meaningful analysis\".1586\nSubsequent studies have disproved any correlation between Friday the 13th and the rate of accidents.\nOn 12\u00a0June 2008 the Dutch Centre for Insurance Statistics stated to the contrary, that \"fewer accidents and reports of fire and theft occur when the 13th of the month falls on a Friday than on other Fridays, because people are preventatively more careful or just stay home. Statistically speaking, driving is slightly safer on Friday the 13th, at least in the Netherlands; in the last two years, Dutch insurers received reports of an average 7,800\u00a0traffic accidents each Friday; but the average figure when the 13th fell on a Friday was just 7,500.\"\nTattoo Holiday.\nIn recent years, Friday the 13th has emerged as a holiday for tattoo parlors and enthusiasts; with many shops running for 24 hours, and offering flash tattoos featuring the number 13 in the design. Some claim that having a number 13 tattoo can be an antidote to bad luck. According to Oliver Peck: \"Bad luck would come your way, it would see the number 13, see that bad luck is already there, and it would pass on by.\"\nSimilar dates.\nSimilar dates are prevalent in many cultures, although it is unclear whether these similarities are in any way historically connected or only coincidental.\nTuesday the 13th in Hispanic and Greek culture.\nIn Hispanic countries, instead of Friday, Tuesday the 13th (\"martes trece\") is considered a day of bad luck.\nThe Greeks also consider Tuesday (and especially the 13th) an unlucky day. Tuesday is considered dominated by the influence of Ares, the god of war (or Mars, the Roman equivalent). The fall of Constantinople to the Fourth Crusade occurred on Tuesday 13\u00a0April 1204, and the Fall of Constantinople to the Ottomans happened on Tuesday 29\u00a0May 1453, events that strengthen the superstition about Tuesday. In addition, in Greek the name of the day is \"Triti\" (\u03a4\u03c1\u03af\u03c4\u03b7) meaning the third (day of the week), adding weight to the superstition, since bad luck is said to \"come in threes\".\nThere is a Tuesday the 13th in months that begin on a Thursday.\nFriday the 17th in Italy.\nIn Italian popular culture, Friday the 17th (and not the 13th) is considered a bad luck day.\nThe origin of this belief could be traced in the writing of the number\u00a017, in Roman numerals: XVII. By shuffling the digits of the number one can get the Latin \"v\u012bx\u012b\" (\"I have lived\", implying death at present), an omen of bad luck.\nIn fact, in Italy, 13 is generally considered a lucky number, although some people may consider 13 an unlucky number as well due to Americanization.\nThe 2000 parody film \"Shriek if You Know What I Did Last Friday the Thirteenth\" was released in Italy with the title \"Shriek \u2013 Hai impegni per venerd\u00ec 17?\" (\"Shriek \u2013 Do You Have Something to Do on Friday the 17th?\").\nThere is a Friday the 17th in months that begin on a Wednesday.\nFootnotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "37169", "revid": "212624", "url": "https://en.wikipedia.org/wiki?curid=37169", "title": "Friday 13th", "text": ""}
{"id": "37170", "revid": "50941089", "url": "https://en.wikipedia.org/wiki?curid=37170", "title": "Red slender loris", "text": "Species of primate\n&lt;templatestyles src=\"Template:Taxobox/core/styles.css\" /&gt;\nThe red slender loris (Loris tardigradus) is a small, nocturnal strepsirrhine primate native to the rainforests of Sri Lanka. This is No. 6 of the 10 focal species and No. 22 of the 100 EDGE mammal species worldwide considered the most evolutionarily distinct and globally endangered.\nTaxonomy.\nThe taxonomy of slender lorises in Sri Lanka has undergone significant revision. Early classifications recognized multiple subspecies. A pivotal review split them into two species: the small, red wet-zone endemic Loris tardigradus and the larger, greyish Loris lydekkerianus found in India and parts of Sri Lanka. Genetic and morphological studies later supported this split.\nRecent work refined the classification of the Red slender loris, identifying three subspecies based on morphology and geography: \"L. t. tardigradus\", \"L. t. nycticeboides\", and \"L. t. parvus\".\nSubspecies.\nNorthwestern Red Slender Loris (Loris tardigradus ssp. parvus) - CR.\nRestricted to the northwestern wet and intermediate zones north of the Kelani River in the Western Province. Localities include Henarathgoda, Meerigama Kanda, Pilikuttuwa, and Horagolla.\nSouthwestern Red Slender Loris (Loris tardigradus ssp. tardigradus) - EN.\nFound in the southwestern wet zone across the Matara, Ratnapura, Kalutara, and Kegalle districts. It occurs in fragmented forests including Sinharaja, Kanneliya, Peak Wilderness Sanctuary, and Yagirala.\nHighland Slender Loris (Loris tardigradus ssp. nycticeboides) - CR.\nConfined to high-altitude areas (above approximately 1,500 m) in the central highlands. Known sites are Horton Plains, Hakgala, and forests near Nuwara Eliya such as Conical Hill and Kandapola.\nDescription.\nThis small, slender primate is distinguished by large forward-facing eyes used for precise depth perception, long slender limbs, a well-developed index finger, the absence of tail, and large prominent ears, which are thin, rounded and hairless at the edges. The soft dense fur is reddish-brown color on the back, and the underside is whitish-grey with a sprinkling of silver hair. Its body length on average is , with an average weight of a mere . This loris has a four-way grip on each foot. The big toe opposes the other 4 toes for a pincer-like grip on branches and food. It has a dark face mask with central pale stripe, much like the slow lorises.\n\"L. tardigradus tardigradus\" is reddish brown in the back and creamy yellow below, while \"L. tardigradus nycticeboides\" is dark brown dorsally and very light brown in upperparts.\nDistribution.\nThis species is endemic to the rainforests of Sri Lanka. It is typically found in the southwestern wet zone and the adjacent central highlands. Its range spans the wet southwestern region, extending from Colombo through Kalutara, Ratnapura, Kegalle, Galle, and Matara, and possibly into the wetter parts of Hambantota District.\nThe species has been documented in numerous protected and proposed forest reserves across several provinces. \"Western Province\" (Maimbulakanda, Pilikuttuwa, Kalukele, Horagolla, Dikkele, Meerigama Kanda, Labugama Kalatuwawa, Indikada Mukalana, Koskanda, Bodinagala, Yagirala, and Madakada), \"Southern Province\" (Oliyagankele, Welihena, Masmullah, Kekanadura, Beraliya Mukalana, Dandeniya-Aparekka, Kanneliya, Polgahakanda, and Kottawa), \"Sabaragamuwa Province\" (Sinharaja World Heritage Site, Delwala, Peak Wilderness Sanctuary, Gilimale, Madampe, and Salgala), \"Central Province\" (Horton Plains National Park, Kikiliyamana, Hakgala Strict Nature Reserve, and Conical Hill).\nBehavior.\nThe red slender loris favors lowland rainforests (up to in altitude), tropical rainforests and inter-monsoon forests of the south western wet-zone of Sri Lanka. Masmullah Proposed Forest Reserve harbors one of few remaining red slender loris populations, and is considered a biodiversity hotspot. The most common plant species eaten was \"Humboldtia laurifolia\", occurring at 676 trees/ha, with overall density at 1077 trees/ha. \"Humboldtia laurifolia\" is vulnerable and has a mutualistic relationship with ants, providing abundant food for lorises. Reports from the 1960s suggest that it once also occurred in the coastal zone, however it is now thought to be extinct there.\nThe red slender loris differs from its close relative the gray slender loris in its frequent use of rapid arboreal locomotion. It forms small social groups, containing adults of both sexes as well as young animals. This species is among the most social of the nocturnal primates. During daylight hours the animals sleep in groups in branch tangles, or curled up on a branch with their heads between their legs. The groups also undertake mutual grooming and play at wrestling. The adults typically hunt separately during the night. They are primarily insectivorous but also eat bird eggs, berries, leaves, buds and occasionally invertebrates as well as geckos and lizards. They forage, and while doing so, ants may stick to the back of their hands. As this occurs, the red slender loris is able to consume these ants. To maximize protein and nutrient uptake they consume every part of their prey, including the scales and bones. They make nests out of leaves or find hollows of trees or a similar secure place to live in.\nReproduction.\nFemales are dominant. The female reaches her sexual maturity at 10 months and is receptive to the male twice a year. This species mates while hanging upside down from branches; individuals in captivity will not breed if no suitable branch is available. The gestation period is 166\u2013169 days, after which the female will bear 1\u20132 young which feed from her for 6\u20137 months. The lifespan of this species is believed to be around 15\u201318 years in the wild.\nThreats.\nThis slender loris is an endangered species. Habitat destruction is a major threat. It is widely trapped and killed for use in supposed remedies for eye diseases and is preyed upon by snakes, dogs, and some fish. Other threats include electrocution on live wires, road accidents and capture for the pet trade.\nConservation.\nThe red slender loris was identified as one of the top-10 \"focal species\" in 2007 by the Evolutionarily Distinct and Globally Endangered (EDGE) project.\nOne early success has been the rediscovery of the virtually unknown Horton Plains slender loris (\"Loris tardigradus nycticeboides\"). Originally documented in 1937, there have been only four known encounters in the past 72 years, and for more than 60 years until 2002 the sub-species had been believed to be extinct. The sub-species was rediscovered in 2002 by a team led by Anna Nekaris in Horton Plains National Park. The late 2009 capture by a team working under the Zoological Society of London's EDGE programme has resulted in the first detailed physical examination of the Horton Plains sub-species and the first-ever photographs of it. The limited available evidence suggests there may be only about 100 animals still existing, which would make it among the top five most-threatened primates worldwide.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "37171", "revid": "19295592", "url": "https://en.wikipedia.org/wiki?curid=37171", "title": "Cray-1", "text": "Supercomputer manufactured by Cray Research\nThe Cray-1 was a supercomputer designed, manufactured and marketed by Cray Research. Announced in 1975, the first Cray-1 system was installed at Los Alamos National Laboratory in 1976. Eventually, eighty Cray-1s were sold, making it one of the most successful supercomputers in history. It is perhaps best known for its unique shape, a relatively small C-shaped cabinet with a ring of benches around the outside covering the power supplies and the cooling system.\nThe Cray-1 was the first supercomputer to successfully implement the vector processor design. These systems improve the performance of math operations by arranging memory and registers to quickly perform a single operation on a large set of data. Previous systems like the CDC STAR-100 and ASC had implemented these concepts but did so in a way that seriously limited their performance. The Cray-1 addressed these problems and produced a machine that ran several times faster than any similar design.\nThe Cray-1's architect was Seymour Cray; the chief engineer was Cray Research co-founder Lester Davis. They would go on to design several new machines using the same basic concepts, and retained the performance crown into the 1990s.\nHistory.\nFrom 1968 to 1972, Seymour Cray of Control Data Corporation (CDC) worked on the CDC 8600, the successor to his earlier CDC 6600 and CDC 7600 designs. The 8600 was essentially made up of four 7600s in a box with an additional special mode that allowed them to operate lock-step in a SIMD fashion.\nJim Thornton, formerly Cray's engineering partner on earlier designs, had started a more radical project known as the CDC STAR-100. Unlike the 8600's brute-force approach to performance, the STAR took an entirely different route. The main processor of the STAR had lower performance than the 7600, but added hardware and instructions to speed up particularly common supercomputer tasks.\nBy 1972, the 8600 had reached a dead end; the machine was so incredibly complex that it was impossible to get one working properly. Even a single faulty component would render the machine non-operational. Cray went to William Norris, Control Data's CEO, saying that a redesign from scratch was needed. At the time, the company was in serious financial trouble, and with the STAR in the pipeline as well, Norris could not invest the money.\nAs a result, Cray left CDC and started Cray Research very close to the CDC lab. In the back yard of the land he purchased in Chippewa Falls, Wisconsin, Cray and a group of former CDC employees started looking for ideas. At first, the concept of building another supercomputer seemed impossible, but after Cray Research's Chief Technology Officer travelled to Wall Street and found a lineup of investors willing to back Cray, all that was needed was a design.\nFor four years Cray Research designed its first computer. In 1975 the 80\u00a0MHz Cray-1 was announced. The excitement was so high that a bidding war for the first machine broke out between Lawrence Livermore National Laboratory and Los Alamos National Laboratory, the latter eventually winning and receiving serial number 001 in 1976 for a six-month trial. The National Center for Atmospheric Research (NCAR) was the first official customer of Cray Research in 1977, paying US$8.86 million ($7.9 million plus $1 million for the disks) for serial number 3. The NCAR machine was decommissioned in 1989. The company expected to sell perhaps a dozen of the machines, and set the selling price accordingly, but ultimately over 80 Cray-1s of all types were sold, priced from $5M to $8M. The machine made Seymour Cray a celebrity and his company a success, lasting until the supercomputer crash in the early 1990s.\nBased on a recommendation by William Perry's study, the NSA purchased a Cray-1 for theoretical research in cryptanalysis. According to Budiansky, \"Though standard histories of Cray Research would persist for decades in stating that the company's first customer was Los Alamos National Laboratory, in fact it was NSA...\"\nThe 160\u00a0MFLOPS Cray-1 was succeeded in 1982 by the 800\u00a0MFLOPS Cray X-MP, the first Cray multi-processing computer. In 1985, the very advanced Cray-2, capable of 1.9\u00a0GFLOPS peak performance, succeeded the first two models but met a somewhat limited commercial success because of certain problems at producing sustained performance in real-world applications. A more conservatively designed evolutionary successor of the Cray-1 and X-MP models was therefore made by the name Cray Y-MP and launched in 1988.\nBy comparison, the processor in a typical 2013 smart device, such as a Google Nexus 10 or HTC One, performs at roughly 1 GFLOPS, while the A13 processor in a 2019 iPhone 11 performs at 154.9 GFLOPS, a mark supercomputers succeeding the Cray-1 would not reach until 1994.\nBackground.\nTypical scientific workloads consist of reading in large data sets, transforming them in some way and then writing them back out again. Normally the transformations being applied are identical across all of the data points in the set. For instance, the program might add 5 to every number in a set of a million numbers.\nIn simple computers the program would loop over all million numbers, adding five, thereby executing a million instructions saying codice_1. Internally the computer solves this instruction in several steps. First it reads the instruction from memory and decodes it, then it collects any additional information it needs, in this case the numbers b and c, and then finally runs the operation and stores the results. The end result is that the computer requires tens or hundreds of millions of cycles to carry out these operations.\nVector machines.\nIn the STAR, new instructions essentially wrote the loops for the user. The user told the machine where in memory the list of numbers was stored, then fed in a single instruction codice_2. At first glance it appears the savings are limited; in this case the machine fetches and decodes only a single instruction instead of 1,000,000, thereby saving 1,000,000 fetches and decodes, perhaps one-fourth of the overall time.\nThe real savings are not so obvious. Internally, the CPU of the computer is built up from a number of separate parts dedicated to a single task, for instance, adding a number, or fetching from memory. Normally, as the instruction flows through the machine, only one part is active at any given time. This means that each sequential step of the entire process must complete before a result can be saved. The addition of an instruction pipeline changes this. In such machines the CPU will \"look ahead\" and begin fetching succeeding instructions while the current instruction is still being processed. In this assembly line fashion any one instruction still requires as long to complete, but as soon as it finishes executing, the next instruction is right behind it, with most of the steps required for its execution already completed.\nVector processors use this technique with one additional trick. Because the data layout is in a known format\u00a0\u2014 a set of numbers arranged sequentially in memory\u00a0\u2014 the pipelines can be tuned to improve the performance of fetches. On the receipt of a vector instruction, special hardware sets up the memory access for the arrays and stuffs the data into the processor as fast as possible.\nCDC's approach in the STAR used what is today known as a \"memory-memory architecture\". This referred to the way the machine gathered data. It set up its pipeline to read from and write to memory directly. This allowed the STAR to use vectors of length not limited by the length of registers, making it highly flexible. Unfortunately, the pipeline had to be very long in order to allow it to have enough instructions in flight to make up for the slow memory. That meant the machine incurred a high cost when switching from processing vectors to performing operations on non-vector operands. Additionally, the low scalar performance of the machine meant that after the switch had taken place and the machine was running scalar instructions, the performance was quite poor. The result was rather disappointing real-world performance, something that could, perhaps, have been forecast by Amdahl's law.\nCray's approach.\nCray studied the failure of the STAR and learned from it. He decided that in addition to fast vector processing, his design would also require excellent all-around scalar performance. That way when the machine switched modes, it would still provide superior performance. Additionally he noticed that the workloads could be dramatically improved in most cases through the use of registers.\nJust as earlier machines had ignored the fact that most operations were being applied to many data points, the STAR ignored the fact that those same data points would be repeatedly operated on. Whereas the STAR would read and process the same memory five times to apply five vector operations on a set of data, it would be much faster to read the data into the CPU's registers once, and then apply the five operations. However, there were limitations with this approach. Registers were significantly more expensive in terms of circuitry, so only a limited number could be provided. This implied that Cray's design would have less flexibility in terms of vector sizes. Instead of reading any sized vector several times as in the STAR, the Cray-1 would have to read only a portion of the vector at a time, but it could then run several operations on that data prior to writing the results back to memory. Given typical workloads, Cray felt that the small cost incurred by being required to break large sequential memory accesses into segments was a cost well worth paying.\nSince the typical vector operation would involve loading a small set of data into the vector registers and then running several operations on it, the vector system of the new design had its own separate pipeline. For instance, the multiplication and addition units were implemented as separate hardware, so the results of one could be internally pipelined into the next, the instruction decode having already been handled in the machine's main pipeline. Cray referred to this concept as \"chaining\", as it allowed programmers to \"chain together\" several instructions and extract higher performance.\nPerformance.\nIn 1978, a team from the Argonne National Laboratory tested a variety of typical workloads on a Cray-1 as part of a proposal to purchase one for their use, replacing their IBM 370/195. They also planned on testing on the CDC STAR-100 and Burroughs Scientific Computer, but such tests, if they were performed, were not published. The tests were run on the Cray-1 at the National Center for Atmospheric Research (NCAR) in Boulder, Colorado. The only other Cray available at the time was the one at Los Alamos, but accessing this machine required Q clearance.\nThe tests were reported in two ways. The first was a minimum conversion needed to get the program running without errors, but making no attempt to take advantage of the Cray's vectorization. The second included a moderate set of updates to the code, often unwinding loops so they could be vectorized. Generally, the minimal conversions ran roughly the same speed as the 370 to about 2 times its performance (mostly due to a larger exponent range on the Cray), but vectorization led to further increases between 2.5 and 10 times. In one example program, which performed an internal fast Fourier transform, performance improved from the IBM's 47 milliseconds to 3.\nDescription.\nThe new machine was the first Cray design to use integrated circuits (ICs). Although ICs had been available since the 1960s, it was only in the early 1970s that they reached the performance necessary for high-speed applications. The Cray-1 used only four different IC types, an emitter-coupled logic (ECL) dual 5-4 NOR gate (one 5-input, and one 4-input, each with differential output), another slower MECL 10K 5-4 NOR gate used for address fanout, a 16\u00d74-bit high speed (6\u00a0ns) static RAM (SRAM) used for registers and a 1,024\u00d71-bit 48\u00a0ns SRAM used for the main memory. These integrated circuits were supplied by Fairchild Semiconductor and Motorola. In all, the Cray-1 contained about 200,000 gates.\nICs were mounted on large five-layer printed circuit boards, with up to 144 ICs per board. Boards were then mounted back to back for cooling (see below) and placed in twenty-four racks containing 72 double-boards. The typical module (distinct processing unit) required one or two boards. In all the machine contained 1,662 modules in 113 varieties.\nEach cable between the modules was a twisted pair, cut to a specific length in order to guarantee the signals arrived at precisely the right time and minimize electrical reflection. Each signal produced by the ECL circuitry was a differential pair, so the signals were balanced. This tended to make the demand on the power supply more constant and reduce switching noise. The load on the power supply was so evenly balanced that Cray boasted that the power supply was unregulated. To the power supply, the entire computer system looked like a simple resistor.\nThe high-performance ECL circuitry generated considerable heat, and Cray's designers spent as much effort on the design of the refrigeration system as they did on the rest of the mechanical design. In this case, each circuit board was paired with a second, placed back to back with a sheet of copper between them. The copper sheet conducted heat to the edges of the cage, where liquid Freon running in stainless steel pipes drew it away to the cooling unit below the machine. The first Cray-1 was delayed six months due to problems in the cooling system; lubricant that is normally mixed with the Freon to keep the compressor running would leak through the seals and eventually coat the boards with oil until they shorted out. New welding techniques had to be used to properly seal the tubing.\nIn order to bring maximum speed out of the machine, the entire chassis was bent into a large C-shape. Speed-dependent portions of the system were placed on the \"inside edge\" of the chassis, where the wire-lengths were shorter. This allowed the cycle time to be decreased to 12.5\u00a0ns (80\u00a0MHz), not as fast as the 8\u00a0ns 8600 he had given up on, but fast enough to beat CDC 7600 and the STAR. NCAR estimated that the overall throughput on the system was 4.5 times that of the CDC 7600.\nThe Cray-1 was built as a 64-bit system, a departure from the 7600/6600, which were 60-bit machines (a change was also planned for the 8600). Addressing was 24-bit, with a maximum of 1,048,576 64-bit words (1 megaword) of main memory, where each word also had eight parity bits for a total of 72 bits per word. Memory was spread across 16 interleaved memory banks, each with a 50\u00a0ns cycle time, allowing up to four words to be read per cycle. Smaller configurations could have 0.25 or 0.5 megawords of main memory. Maximum aggregate memory bandwidth was 638\u00a0Mbit/s.\nThe main register set consisted of eight 64-bit scalar (S) registers and eight 24-bit address (A) registers. These were backed by a set of sixty-four registers each for S and A temporary storage known as T and B respectively, which could not be seen by the functional units. The vector system added another eight 64-element by 64-bit vector (V) registers, as well as a vector length (VL) and vector mask (VM). Finally, the system also included a 64-bit real-time clock register and four 64-bit instruction buffers that held sixty-four 16-bit instructions each. The hardware was set up to allow the vector registers to be fed at one word per cycle, while the address and scalar registers required two cycles. In contrast, the entire 16-word instruction buffer could be filled in four cycles.\nThe Cray-1 had twelve pipelined functional units. The 24-bit address arithmetic was performed in an add unit and a multiply unit. The scalar portion of the system consisted of an add unit, a logical unit, a population count, a leading zero count unit and a shift unit. The vector portion consisted of add, logical and shift units. The floating point functional units were shared between the scalar and vector portions, and these consisted of add, multiply and reciprocal approximation units.\nThe system had limited parallelism. It could issue one instruction per clock cycle, for a theoretical performance of 80\u00a0MIPS, but with vector floating-point multiplication and addition occurring in parallel theoretical performance was 160\u00a0MFLOPS. (The reciprocal approximation unit could also operate in parallel, but did not deliver a true floating-point result - two additional multiplications were needed to achieve a full division.)\nSince the machine was designed to operate on large data sets, the design also dedicated considerable circuitry to I/O. Earlier Cray designs at CDC had included separate computers dedicated to this task, but this was no longer needed. Instead the Cray-1 included four six-channel controllers, each of which was given access to main memory once every four cycles. The channels were 16 bits wide and included three control bits and four bits for error correction, so the maximum transfer speed was one word per 100\u00a0ns, or 500 thousand words per second for the entire machine.\nThe initial model, the Cray-1A, weighed including the Freon refrigeration system. Configured with 1 million words of main memory, the machine and its power supplies consumed about 115\u00a0kW of power; cooling and storage likely more than doubled this figure. A Data General SuperNova S/200 minicomputer served as the maintenance control unit (MCU), which was used to feed the Cray Operating System into the system at boot time, to monitor the CPU during use, and optionally as a front-end computer. Most, if not all, Cray-1As were delivered using the follow-on Data General Eclipse as the MCU.\nThe reliability of the CRAY-1A was very low by today's standards. At the European Centre for Medium-Range Weather Forecasts, which was one of the first customers, the mean time between hardware faults was reported to be 96 hours in 1979. Seymour Cray deliberately made design decisions that sacrificed reliability for speed, but improved his later designs after being questioned on this matter. Similarly, the Cray Operating System (COS) was fairly rudimentary, hardly tested and updated weekly or even daily in the early days.\nCray-1S.\nThe Cray-1S, announced in 1979, was an improved Cray-1 that supported a larger main memory of 1, 2 or 4 million words. The larger main memory was made possible through the use of 4,096 x 1-bit bipolar RAM ICs with a 25\u00a0ns access time. The Data General minicomputers were optionally replaced with an in-house 16-bit design running at 80\u00a0MIPS. The I/O subsystem was separated from the main machine, connected to the main system via a 6\u00a0Mbit/s control channel and a 100\u00a0Mbit/s High Speed Data Channel. This separation made the 1S look like two \"half Crays\" separated by a few feet, which allowed the I/O system to be expanded as needed. Systems could be bought in a variety of configurations from the S/500 with no I/O and 0.5 million words of memory to the S/4400 with four I/O processors and 4 million words of memory.\nCray-1M.\nThe Cray-1M, announced in 1982, replaced the Cray-1S. It had a faster 12\u00a0ns cycle time and used less expensive MOS RAM in the main memory. The 1M was supplied in only three versions, the M/1200 with 1 million words in 8 banks, or the M/2200 and M/4200 with 2 or 4 million words in 16 banks. All of these machines included two, three or four I/O processors, and the system added an optional second High Speed Data Channel. Users could add a Solid-state Storage Device with 8 to 32 million words of MOS RAM.\nSoftware.\nIn 1978, the first standard software package for the Cray-1 was released, consisting of three main products:\nThe United States Department of Energy funded sites from Lawrence Livermore National Laboratory, Los Alamos Scientific Laboratory, Sandia National Laboratories and the National Science Foundation supercomputer centers (for high-energy physics) represented the second largest block with LLL's Cray Time Sharing System (CTSS). CTSS was written in a dynamic memory Fortran, first named LRLTRAN, which ran on CDC 7600s, renamed CVC (pronounced \"Civic\") when vectorization for the Cray-1 was added. Cray Research attempted to support these sites accordingly. These software choices had influences on later minisupercomputers, also known as \"crayettes\".\nNCAR has its own operating system (NCAROS).\nThe National Security Agency developed its own operating system (Folklore) and language (IMP with ports of Cray Pascal and C and Fortran 90 later)\nLibraries started with Cray Research's own offerings and Netlib.\nOther operating systems existed, but most languages tended to be Fortran or Fortran-based. Bell Laboratories, as proof of both portability concept and circuit design, moved the first C compiler to their Cray-1 (non-vectorizing). This act would later give CRI a six-month head start on the Cray-2 Unix port to ETA Systems' detriment, and Lucasfilm's first computer generated test film, \"The Adventures of Andr\u00e9 &amp; Wally B.\".\nApplication software generally tends to be either classified (\"e.g.\" nuclear code, cryptanalytic code) or proprietary (\"e.g.\" petroleum reservoir modeling). This was because little software was shared between customers and university customers. The few exceptions were climatological and meteorological programs until the NSF responded to the Japanese Fifth Generation Computer Systems project and created its supercomputer centers. Even then, little code was shared.\nPartly because Cray were interested in the publicity, they supported the development of Cray Blitz which won the fourth (1983) and fifth (1986) World Computer Chess Championship, as well as the 1983 and 1984 North American Computer Chess Championship. The program, Chess, that dominated in the 1970s ran on Control Data Corporation supercomputers.\nMuseums.\nCray-1s are on display at the following locations:\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "37175", "revid": "7903804", "url": "https://en.wikipedia.org/wiki?curid=37175", "title": "Lars Onsager", "text": "Norwegian-American physical chemist and theoretical physicist (1903-1976)\nLars Onsager (November 27, 1903 \u2013 October 5, 1976) was a Norwegian American physical chemist and theoretical physicist. He held the Gibbs Professorship of Theoretical Chemistry at Yale University. He was awarded the Nobel Prize in Chemistry in 1968.\nEducation and early life.\nLars Onsager was born in Kristiania (now Oslo), Norway. His father was a lawyer. After completing secondary school in Oslo, he attended the Norwegian Institute of Technology (NTH) in Trondheim, graduating as a chemical engineer in 1925. While there he worked through \"A Course of Modern Analysis\", which was instrumental in his later work.\nCareer and research.\nIn 1925 he arrived at a correction to the Debye-H\u00fcckel theory of electrolytic solutions, to specify Brownian movement of ions in solution, and during 1926 published it. He traveled to Z\u00fcrich, where Peter Debye was teaching, and confronted Debye, telling him his theory was wrong. He impressed Debye so much that he was invited to become Debye's assistant at the Eidgen\u00f6ssische Technische Hochschule (ETH), where he remained until 1928.\nJohns Hopkins University.\nIn 1928 he went to the United States to take a faculty position at the Johns Hopkins University in Baltimore, Maryland. At JHU he had to teach freshman classes in chemistry, and it quickly became apparent that, while he was a genius at developing theories in physical chemistry, he had little talent for teaching. He was dismissed by JHU after one semester.\nBrown University.\nOn leaving JHU, he accepted a position (involving the teaching of statistical mechanics to graduate students in chemistry) at Brown University in Providence, Rhode Island, where it became clear that he was no better at teaching advanced students than freshmen, but he made significant contributions to statistical mechanics and thermodynamics. His graduate student Raymond Fuoss worked under him and eventually joined him on the Yale chemistry faculty. His statistical mechanics course was nicknamed \"Sadistical Mechanics\" by the students.\nHis research at Brown was concerned mainly with the effects on diffusion of temperature gradients, and produced the Onsager reciprocal relations in statistical mechanics, a set of equations published in 1929 and, in an expanded form, in 1931, whose importance went unrecognized for many years. However, their value became apparent during the decades following World War II, and by 1968 they were considered important enough to gain Onsager that year's Nobel Prize in Chemistry.\nIn 1933, when the Great Depression limited Brown's ability to support a faculty member who was only useful as a researcher and not a teacher, he was let go by Brown. He traveled to Austria to visit electrochemist Hans Falkenhagen. He met Falkenhagen's sister-in-law, Margrethe Arledter. They were married on September 7, 1933, and had three sons and a daughter.\nYale University.\nAfter the trip to Europe, he was hired by Yale University, where he remained for most of the rest of his life, retiring in 1972.\nAt Yale, he had been hired as a postdoctoral fellow, but it was discovered that he had never received a Ph.D. While he had submitted an outline of his work in reciprocal relations to the Norwegian Institute of Technology, they had decided it was too incomplete to qualify as a doctoral dissertation. He was told that he could submit one of his published papers to the Yale faculty as a dissertation, but insisted on doing a new research project instead. His dissertation laid the mathematical background for his interpretation of deviations from Ohm's law in weak electrolytes. It dealt with the solutions of the Mathieu equation of period formula_1 and certain related functions and was beyond the comprehension of the chemistry and physics faculty. Only when some members of the mathematics department, including the chairman Einar Hille (who also liked \"A Course of Modern Analysis\"), insisted that the work was good enough that \"they\" would grant the doctorate if the chemistry department would not, was he granted a Ph.D. in chemistry in 1935.\nEven before the dissertation was finished, he was appointed assistant professor in 1934, and promoted to associate professor in 1940. He quickly showed at Yale the same traits he had at JHU and Brown: he produced brilliant theoretical research, but was incapable of giving a lecture at a level that a student (even a graduate student) could comprehend. He was also unable to direct the research of graduate students, except for the occasional outstanding one. His two courses on statistical mechanics were nicknamed \"Advanced Norwegian I\" and \"Advanced Norwegian II\" for being incomprehensible.\nDuring the late 1930s, Onsager researched the dipole theory of dielectrics, making improvements for another topic that had been studied by Peter Debye. However, when he submitted his paper to a journal that Debye edited in 1936, it was rejected. Debye would not accept Onsager's ideas until after World War II. During the 1940s, Onsager studied the statistical-mechanical theory of phase transitions in solids, deriving a mathematically elegant theory which was enthusiastically received. In what is widely considered a tour de force of mathematical physics, he obtained the exact solution for the two dimensional Ising model in zero field in 1944.\nIn 1960 he was awarded an honorary degree, doctor techn. honoris causa, at the Norwegian Institute of Technology, later part of Norwegian University of Science and Technology.\nIn 1945, Onsager was naturalized as an American citizen, and the same year he was awarded the title of \"J. Willard Gibbs Professor of Theoretical Chemistry\". This was particularly appropriate because Onsager, like Willard Gibbs, had been involved primarily in the application of mathematics to problems in physics and chemistry and, in a sense, could be considered to be continuing in the same areas Gibbs had pioneered.\nIn 1947, he was elected to the National Academy of Sciences, the American Academy of Arts and Sciences in 1949, and in 1950 he joined the ranks of Alpha Chi Sigma.\nAfter World War II, Onsager researched new topics of interest. \nHe proposed a theoretical explanation of the superfluid properties of liquid helium in 1949; two years later the physicist Richard Feynman independently proposed the same theory. He also worked on the theories of liquid crystals and the electrical properties of ice. While on a Fulbright scholarship to the University of Cambridge, he worked on the magnetic properties of metals. He developed important ideas on the quantization of magnetic flux in metals. He was awarded the Lorentz Medal in 1958, Willard Gibbs Award in 1962, and the Nobel Prize in Chemistry in 1968. He was elected a member of the American Philosophical Society in 1959 and a Foreign Member of the Royal Society (ForMemRS) in 1975.\nAfter Yale.\nIn 1972 Onsager retired from Yale and became emeritus. He then became a member of the Center for Theoretical Studies, University of Miami, and was appointed Distinguished University Professor of Physics. At the University of Miami he remained active in guiding and inspiring postdoctoral students as his teaching skills, although not his lecturing skills, had improved during the course of his career. He developed interests in semiconductor physics, biophysics and radiation chemistry. However, his death came before he could produce any breakthroughs comparable to those of his earlier years.\nResearch.\nExact solution of the 2D Ising model.\nTo solve the 2D Ising model, Onsager began by diagonalizing increasingly large transfer matrices. He said that it's because he had a lot of time during WWII. He began by computing the 2\u00a0\u00d7\u00a02 transfer matrix of the 1D Ising model, which is already solved by Ising himself. He then computed the transfer matrix of the \"Ising ladder\", meaning two 1D Ising models side-by-side, connected by links. The transfer matrix is then 4\u00a0\u00d7\u00a04. He repeated this for up to six 1D Ising models, resulting in transfer matrices of up to 64\u00a0\u00d7\u00a064. He diagonalized all of them and found that all the eigenvalues were of a special form, so he guessed that the algebra of the problem was an associative algebra (later called the Onsager algebra).\nThe solution involved generalized quaternion algebra and the theory of elliptic functions, which he learned from \"A Course of Modern Analysis\".\nPersonal life.\nHe remained in Florida until his death from an aneurysm in Coral Gables, Florida in 1976. Onsager was buried next to John Gamble Kirkwood at New Haven's Grove Street Cemetery. While Kirkwood's tombstone has a long list of awards and positions, including the American Chemical Society Award in Pure Chemistry, the Richards Medal, and the Lewis Award, Onsager's tombstone, in its original form, simply said \"Nobel Laureate\". When Onsager's wife Gretel died in 1991 and was buried there, his children added an asterisk after \"Nobel Laureate\" and \"*etc.\" in the lower right corner of the stone. He was identified as a Protestant.\nLegacy.\nThe Norwegian Institute of Technology established the Lars Onsager Lecture and The Lars Onsager Professorship in 1993 to award outstanding scientists in the scientific fields of Lars Onsager; Chemistry, Physics and Mathematics. The American Physical Society established Lars Onsager Prize in statistical physics in 1993. In 1997 his sons and daughter donated his scientific works and professional belongings to NTNU (before 1996 NTH) in Trondheim, Norway as his alma mater. These are now organized as \"The Lars Onsager Archive\" at the Gunnerus Library in Trondheim.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "37179", "revid": "41744047", "url": "https://en.wikipedia.org/wiki?curid=37179", "title": "Opencola", "text": ""}
{"id": "37183", "revid": "7611264", "url": "https://en.wikipedia.org/wiki?curid=37183", "title": "Novikov self-consistency principle", "text": "Time-travel paradoxes are impossible\nThe Novikov self-consistency principle, also known as the Novikov self-consistency conjecture and Larry Niven's law of conservation of history, is a principle developed by Russian physicist Igor Dmitriyevich Novikov in the mid-1980s. Novikov intended it to solve the problem of paradoxes in time travel, which is theoretically permitted in certain solutions of general relativity that contain what are known as closed timelike curves. The principle asserts that if an event exists that would cause a paradox or any \"change\" to the past whatsoever, then the probability of that event is zero. It would thus be impossible to create time paradoxes.\nHistory.\nPhysicists have long known that some solutions to the theory of general relativity contain closed timelike curves\u2014for example the G\u00f6del metric. Novikov discussed the possibility of closed timelike curves (CTCs) in books he wrote in 1975 and 1983, offering the opinion that only self-consistent trips back in time would be permitted. In a 1990 paper by Novikov and several others, \"Cauchy problem in spacetimes with closed timelike curves\", the authors state:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;The only type of causality violation that the authors would find unacceptable is that embodied in the science-fiction concept of going backward in time and killing one's younger self (\"changing the past\"). Some years ago one of us (Novikov) briefly considered the possibility that CTCs might exist and argued that they cannot entail this type of causality violation: events on a CTC are already guaranteed to be self-consistent, Novikov argued; they influence each other around a closed curve in a self-adjusted, cyclical, self-consistent way. The other authors recently have arrived at the same viewpoint.\nWe shall embody this viewpoint in a \"principle of self-consistency,\" which states that \"the only solutions to the laws of physics that can occur locally in the real Universe are those which are globally self-consistent.\" This principle allows one to build a local solution to the equations of physics only if that local solution can be extended to a part of a (not necessarily unique) global solution, which is well defined throughout the nonsingular regions of the space-time.\nAmong the co-authors of this 1990 paper were Kip Thorne, Mike Morris, and Ulvi Yurtsever, who in 1988 had stirred up renewed interest in the subject of time travel in general relativity with their paper \"Wormholes, Time Machines, and the Weak Energy Condition\", which showed that a new general relativity solution known as a traversable wormhole could lead to closed timelike curves, and unlike previous CTC-containing solutions, it did not require unrealistic conditions for the universe as a whole. After discussions with the lead author of the 1990 paper, John Friedman, they convinced themselves that time travel need not lead to unresolvable paradoxes, regardless of the object sent through the wormhole.\nBy way of response, physicist Joseph Polchinski wrote them a letter arguing that one could avoid the issue of free will by employing a potentially paradoxical thought experiment involving a billiard ball sent back in time through a wormhole. In Polchinski's scenario, the billiard ball is fired into the wormhole at an angle such that, if it continues along its path, it will exit in the past at just the right angle to collide with its earlier self, knocking it off track and preventing it from entering the wormhole in the first place. Thorne would refer to this scenario as \"Polchinski's paradox\" in 1994.\nUpon considering the scenario, Fernando Echeverria and Gunnar Klinkhammer, two students at Caltech (where Thorne taught), arrived at a solution to the problem, that lays out the same elements as the solution Feynman and Wheeler termed the \"glancing blow\" solution, to evade inconsistencies arising from causality loops. In the revised scenario, the ball from the future emerges at a different angle than the one that generates the paradox, and delivers its younger self a glancing blow instead of knocking it completely away from the wormhole. This blow alters its trajectory by just the right degree, meaning it will travel back in time with the angle required to deliver its younger self the necessary glancing blow. Echeverria and Klinkhammer actually found that there was more than one self-consistent solution, with slightly different angles for the glancing blow in each situation. Later analysis by Thorne and Robert Forward illustrated that for certain initial trajectories of the billiard ball, there could actually be an infinite number of self-consistent solutions.\nEcheverria, Klinkhammer, and Thorne published a paper discussing these results in 1991; in addition, they reported that they had tried to see if they could find \"any\" initial conditions for the billiard ball for which there were no self-consistent extensions, but were unable to do so. Thus, it is plausible that there exist self-consistent extensions for every possible initial trajectory, although this has not been proven. This only applies to initial conditions outside of the chronology-violating region of spacetime, which is bounded by a Cauchy horizon. This could mean that the Novikov self-consistency principle does not actually place any constraints on systems outside of the region of space-time where time travel is possible, only inside it.\nEven if self-consistent extensions can be found for arbitrary initial conditions outside the Cauchy horizon, the finding that there can be multiple distinct self-consistent extensions for the same initial condition\u2014indeed, Echeverria et al. found an infinite number of consistent extensions for every initial trajectory they analyzed\u2014can be seen as problematic, since classically there seems to be no way to decide which extension the laws of physics will choose. To get around this difficulty, Thorne and Klinkhammer analyzed the billiard ball scenario using quantum mechanics, performing a quantum-mechanical sum over histories (path integral) using only the consistent extensions, and found that this resulted in a well-defined probability for each consistent extension. The authors of \"Cauchy problem in spacetimes with closed timelike curves\" write:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;The simplest way to impose the principle of self-consistency in quantum mechanics (in a classical space-time) is by a sum-over-histories formulation in which one includes all those, and only those, histories that are self-consistent. It turns out that, at least formally (modulo such issues as the convergence of the sum), for every choice of the billiard ball's initial, nonrelativistic wave function before the Cauchy horizon, such a sum over histories produces unique, self-consistent probabilities for the outcomes of all sets of subsequent measurements. ... We suspect, more generally, that for any quantum system in a classical wormhole spacetime with a stable Cauchy horizon, the sum over all self-consistent histories will give unique, self-consistent probabilities for the outcomes of all sets of measurements that one might choose to make.\nAssumptions.\nThe Novikov consistency principle assumes certain conditions about what sort of time travel is possible. Specifically, it assumes either that there is only one timeline, or that any alternative timelines (such as those postulated by the many-worlds interpretation of quantum mechanics) are not accessible.\nGiven these assumptions, the constraint that time travel must not lead to inconsistent outcomes could be seen merely as a tautology, a self-evident truth that cannot possibly be false. However, the Novikov self-consistency principle is intended to go beyond just the statement that history must be consistent, making the additional nontrivial assumption that the universe obeys the same local laws of physics in situations involving time travel that it does in regions of space-time that lack closed timelike curves. This is clarified in the above-mentioned \"Cauchy problem in spacetimes with closed timelike curves\", where the authors write:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;That the principle of self-consistency is not totally tautological becomes clear when one considers the following alternative: The laws of physics might permit CTCs; and when CTCs occur, they might trigger new kinds of local physics which we have not previously met. ... The principle of self-consistency is intended to rule out such behavior. It insists that local physics is governed by the same types of physical laws as we deal with in the absence of CTCs: the laws that entail self-consistent single valuedness for the fields. In essence, the principle of self-consistency is a principle of no new physics. If one is inclined from the outset to ignore or discount the possibility of new physics, then one will regard self-consistency as a trivial principle.\nImplications for time travelers.\nThe assumptions of the self-consistency principle can be extended to hypothetical scenarios involving intelligent time travelers as well as unintelligent objects such as billiard balls. The authors of \"Cauchy problem in spacetimes with closed timelike curves\" commented on the issue in the paper's conclusion, writing:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;If CTCs are allowed, and if the above vision of theoretical physics' accommodation with them turns out to be more or less correct, then what will this imply about the philosophical notion of free will for humans and other intelligent beings? It certainly will imply that intelligent beings cannot change the past. Such change is incompatible with the principle of self-consistency. Consequently, any being who went through a wormhole and tried to change the past would be prevented by physical law from making the change; i.e., the \"free will\" of the being would be constrained. Although this constraint has a more global character than constraints on free will that follow from the standard, local laws of physics, it is not obvious to us that this constraint is more severe than those imposed by standard physical law.\nSimilarly, physicist and astronomer J. Craig Wheeler concludes that:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;According to the consistency conjecture, any complex interpersonal interactions must work themselves out self-consistently so that there is no paradox. That is the resolution. This means, if taken literally, that if time machines exist, there can be no free will. You cannot will yourself to kill your younger self if you travel back in time. You can coexist, take yourself out for a beer, celebrate your birthday together, but somehow circumstances will dictate that you cannot behave in a way that leads to a paradox in time. Novikov supports this point of view with another argument: physics already restricts your free will every day. You may will yourself to fly or to walk through a concrete wall, but gravity and condensed-matter physics dictate that you cannot. Why, Novikov asks, is the consistency restriction placed on a time traveler any different?\nTime-loop logic.\nTime-loop logic, coined by roboticist and futurist Hans Moravec, is a hypothetical system of computation that exploits the Novikov self-consistency principle to compute answers much faster than possible with the standard model of computational complexity using Turing machines. In this system, a computer sends a result of a computation backwards through time and relies upon the self-consistency principle to force the sent result to be correct, provided the machine can reliably receive information from the future and provided the algorithm and the underlying mechanism are formally correct. An incorrect result or no result can still be produced if the time travel mechanism or algorithm are not guaranteed to be accurate.\nA simple example is an iterative method algorithm. Moravec states:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;Make a computing box that accepts an input, which represents an approximate solution to some problem, and produces an output that is an improved approximation. Conventionally you would apply such a computation repeatedly a finite number of times, and then settle for the better, but still approximate, result. Given an appropriate negative delay something else is possible: [...] the result of each iteration of the function is brought back in time to serve as the \"first\" approximation. As soon as the machine is activated, a so-called \"fixed-point\" of F, an input which produces an identical output, usually signaling a perfect answer, appears (by an extraordinary coincidence!) immediately and steadily. [...] If the iteration does not converge, that is, if F has no fixed point, the computer outputs and inputs will shut down or hover in an unlikely intermediate state.\nQuantum computation with a negative delay.\nPhysicist David Deutsch showed in 1991 that this model of computation could solve NP problems in polynomial time, and Scott Aaronson later extended this result to show that the model could also be used to solve PSPACE problems in polynomial time. Deutsch shows that quantum computation with a negative delay\u2014backwards time travel\u2014produces only self-consistent solutions, and the chronology-violating region imposes constraints that are not apparent through classical reasoning. Researchers published in 2014 a simulation in which they claim to have validated Deutsch's model with photons. However, it was shown in an article by Tolksdorf and Verch that Deutsch's self-consistency condition can be fulfilled to arbitrary precision in any quantum system described according to relativistic quantum field theory even on spacetimes which do not admit closed timelike curves, casting doubts on whether Deutsch's model is really characteristic of quantum processes simulating closed timelike curves in the sense of general relativity.\nIn a later article,\nthe same authors show that Deutsch's CTC fixed point condition can also be fulfilled in any system\nsubject to the laws of classical statistical mechanics, even if it is not built up by quantum systems. The authors conclude that hence, \nDeutsch's condition is not specific to quantum physics, nor does it depend on the quantum nature of a physical system so that it can be fulfilled.\nIn consequence, Tolksdorf and Verch argue that Deutsch's condition is not sufficiently specific to allow statements about time travel scenarios or their hypothetical realization by quantum physics.\nLloyd's prescription.\nAn alternative proposal was later presented by Seth Lloyd based upon post-selection and path integrals. In particular, the path integral is over single-valued fields, leading to self-consistent histories.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "37184", "revid": "47187980", "url": "https://en.wikipedia.org/wiki?curid=37184", "title": "Colossus", "text": "Colossus, Colossos, or the plural Colossi or Colossuses, may refer to:\n&lt;templatestyles src=\"Template:TOC_right/styles.css\" /&gt;\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "37185", "revid": "10202399", "url": "https://en.wikipedia.org/wiki?curid=37185", "title": "Pope Leo VIII", "text": "Head of the Catholic Church from 964 to 965\nPope Leo VIII (c. 915\u00a0\u2013 1 March 965) was a Roman prelate who claimed the Holy See from 963 until 964 in opposition to John XII and Benedict V and again from 23 June 964 to his death. Today, he is considered by the Catholic Church to have been an antipope during the first period and the legitimate pope during the second. An appointee of Holy Roman Emperor Otto I, Leo VIII's pontificate occurred after the period known as the \"saeculum obscurum\".\nEarly life.\nBorn in Rome in the region around the \"Clivus Argentarius\", Leo came from an illustrious noble family. He was the son of John, who held the office of protonotary. Although a layperson, he was the \"protoscriniarius\" (or superintendent of the Roman public schools for scribes) in the papal court during the pontificate of John XII. In 963, he was included in a party that John sent to the newly crowned Holy Roman emperor, Otto I, who was besieging the deposed King Berengar II of Italy at the castle of St. Leo in Umbria. His instructions were to reassure the emperor that the pope was determined to correct the abuses of the papal court, as well as protest Otto's actions in demanding that cities in the Papal States take an oath of fidelity to the emperor instead of the pope.\nStruggle for papacy.\nBy the time Otto entered Rome to depose Pope John XII, Leo had been appointed protonotary to the Apostolic See. A synod convened by the emperor uncanonically deposed John XII (who had fled to Tibur). It proceeded to elect Leo VIII, who was the emperor's nominee, as pope on 4 December 963. Since Leo was still a layman, he was ordained as ostiarius, lector, acolyte, subdeacon, deacon and priest in the space of a day by Sico, the cardinal-bishop of Ostia, who then proceeded to consecrate him as bishop on 6 December 963. The deposed John, however, still had a large body of sympathisers within Rome; he offered large bribes to the Roman nobility if they would rise up and overthrow Otto and kill Leo, and so, in early January 964, the Roman people staged an uprising that was quickly put down by Otto's troops. Leo, hoping to reach out to the Roman nobility, persuaded Otto to release the hostages he had taken from the leading Roman families in exchange for their continued good behaviour. However, once Otto left Rome around 12 January 964, the Romans again rebelled, and caused Leo to flee Rome and take refuge with Otto sometime in February 964.\nJohn XII returned, and in February convened a synod which in turn deposed Leo on 26 February 964, with John excommunicating Leo in the process. Leo remained with Otto, and, with the death of John XII in May 964, the Romans elected Benedict V. Otto proceeded to besiege Rome, taking Leo with him, and when the Romans eventually surrendered to Otto, Leo was reinstalled in the Lateran Palace as Pope.\nThe former pope was brought before Leo with Benedict's clerical and lay supporters and clad in his pontifical robes. Benedict was asked how he dared to assume the chair of Saint Peter while Leo was still alive. Benedict responded, \"If I have sinned, have mercy on me.\" Having received a promise from the emperor that his life would be spared if he submitted, Benedict threw himself at Leo's feet and acknowledged his guilt. Brought before a synod convened by Leo, Benedict's episcopal ordination was revoked, his pallium was torn from him, and his pastoral staff was broken over him by Leo. However, through the intercession of Otto, Benedict was allowed to retain the rank of deacon. Then, after having the Roman nobility swear an oath over the Tomb of Saint Peter to obey and be faithful to Leo, Otto departed Rome in late June 964.\nPontificate.\nHaving been crowned by Otto, the remainder of Leo's pontificate was reasonably trouble-free. He issued numerous bulls, many of which detailed granting privileges to Otto and his successors. Some of the bulls were alleged to grant the Holy Roman emperors the right to choose their successors in the Kingdom of Italy and the right to nominate the pope, and all popes, archbishops, and bishops were to receive investiture from the emperor. In addition, Leo is also claimed to have relinquished to Otto all the territory of the Papal States that had been granted to the Apostolic See by Pepin the Short and Charlemagne. Although Leo certainly granted various concessions to his imperial patron, it is now believed that the \"investiture\" bulls associated with Leo were, if not completely fabricated during the Investiture Controversy, at the very least so tampered with that it is now largely impossible to reconstruct them in their original form.\nLeo VIII died on 1 March 965 and was succeeded by John XIII. The \"Liber Pontificalis\" described him as venerable, energetic, and honourable. He had some streets dedicated to him in and around the \"Clivus Argentarius\", including the \"descensus Leonis Prothi\".\nHistoriography.\nAlthough Leo was considered an antipope for many years, his current status is still confusing. The \"Annuario Pontificio\" makes the following point about the pontificate of Leo VIII:\n\"At this point, as again in the mid-eleventh century, we come across elections in which problems of harmonizing historical criteria and those of theology and canon law make it impossible to decide clearly which side possessed the legitimacy whose factual existence guarantees the unbroken lawful succession of the Successors of Saint Peter. The uncertainty that in some cases results has made it advisable to abandon the assignation of successive numbers in the list of the Popes.\"\nDue to Leo's uncanonical original election, it is now accepted that, at least until the deposition of Benedict V, he was almost certainly an antipope. The deposition of John XII was almost certainly invalid, as John did not acquiesce, so the election of Benedict V almost certainly was canonical. However, if Benedict did consent to his deposition as Liutprand of Cremona (who chronicled the events of this period) wrote, and if, as seems certain, no further protest was made against Leo's position, it has been the consensus of historians that he may be regarded as a true pope from July 964 to his death in 965. The fact that no one else attempted to claim the papacy during this time and that the next pope to assume the name Leo was consecrated Leo IX also seems to indicate that he is a true pope.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "37186", "revid": "4677237", "url": "https://en.wikipedia.org/wiki?curid=37186", "title": "Vagus nerve", "text": "Main nerve of the parasympathetic nervous system\nThe vagus nerve, also known as the tenth cranial nerve (CN X), plays a crucial role in the autonomic nervous system, which is responsible for regulating involuntary functions within the human body. This nerve carries both sensory and motor fibers and serves as a major pathway that connects the brain to various organs, including the heart, lungs, and digestive tract. As a key part of the parasympathetic nervous system, the vagus nerve helps regulate essential involuntary functions like heart rate, breathing, and digestion. By controlling these processes, the vagus nerve contributes to the body's \"rest and digest\" response, helping to calm the body after stress, lower heart rate, improve digestion, and maintain homeostasis. \nThere are two separate vagus nerves: the right vagus and the left vagus. In the neck, the right vagus nerve contains on average approximately 105,000 fibers, while the left vagus nerve has about 87,000 fibers, according to one source. Other sources report different figures, with around 25,000 fibers in the right vagus nerve and 23,000 fibers in the left.\nThe vagus nerve is the longest nerve of the autonomic nervous system in the human body, consisting of both sensory \u2013 the majority \u2013 and some motor fibers, both sympathetic and parasympathetic. The sensory fibers originate from the jugular and nodose ganglia, while the motor fibers are derived from neurons in the dorsal nucleus of the vagus and the nucleus ambiguus. Although historically the vagus nerve was also known as the pneumogastric nerve, reflecting its role in regulating both the lungs and digestive system, its role in regulating cardiac function is fundamental.\nStructure.\nUpon leaving the medulla oblongata between the olive and the inferior cerebellar peduncle, the vagus nerve extends through the jugular foramen, then passes into the carotid sheath between the internal carotid artery and the internal jugular vein down to the neck, chest, and abdomen, where it contributes to the innervation of the viscera, reaching all the way to the colon. Besides giving some output to various organs, the vagus nerve comprises between 80% and 90% of afferent nerve fibers conveying sensory information about the state of the body's organs to the central nervous system.\nThe right and left vagus nerves descend from the cranial vault through the jugular foramina, penetrating the carotid sheath between the internal and external carotid arteries, then passing posterolateral to the common carotid artery. The cell bodies of visceral afferent fibers of the vagus nerve are located bilaterally in the inferior ganglion of the vagus nerve (nodose ganglia).The vagus runs parallel to the common carotid artery and internal jugular vein inside the carotid sheath. \nRight Vagus Nerve: The right vagus nerve gives rise to the right recurrent laryngeal nerve, which hooks around the right subclavian artery and ascends into the neck between the trachea and esophagus. The right vagus then crosses anterior to the right subclavian artery, runs posterior to the superior vena cava, descends posterior to the right main bronchus, and contributes to cardiac, pulmonary, and esophageal plexuses. It forms the posterior vagal trunk at the lower part of the esophagus and passes through the diaphragm to enter the abdomen through the esophageal hiatus.\nLeft Vagus Nerve: The left vagus nerve enters the thorax between left common carotid artery and left subclavian artery and descends on the aortic arch. It gives rise to the left recurrent laryngeal nerve, which hooks around the aortic arch to the left of the ligamentum arteriosum and ascends between the trachea and esophagus. The left vagus further gives off thoracic cardiac branches, breaks up into the pulmonary plexus, continues into the esophageal plexus, and enters the abdomen as the anterior vagal trunk by way of the esophageal hiatus of the diaphragm.\nNuclei.\nThe vagus nerve includes axons which emerge from or converge onto four nuclei of the medulla:\nDevelopment.\nThe motor division of the glossopharyngeal nerve is derived from the basal plate of the embryonic medulla oblongata, while the sensory division originates from the cranial neural crest. The development of the vagus nerve begins early in embryonic life, around the third to fourth week of gestation. It forms from two key structures: neural crest cells, which contribute to its sensory components, and the neural tube, which forms its motor components in the brainstem (specifically in the medulla oblongata). By weeks 4 to 5, the vagus nerve begins to connect with the fourth and sixth pharyngeal arches, which give rise to muscles involved in swallowing and speaking. Around weeks 5 to 6, specialized nuclei in the brainstem develop to manage the nerve\u2019s motor and sensory functions. These centers are essential for regulating vital automatic processes like breathing, digestion, and heart rate. Between weeks 6 and 9, the vagus nerve extends its branches to various organs, including the heart, lungs, and gastrointestinal tract, as well as sensory areas like the ear and throat. As the fetus grows, the vagus nerve matures into a crucial part of the parasympathetic nervous system, helping maintain the body's internal balance. This process shows how a single nerve can become so important for multiple systems in the body.\nFunction.\nThe vagus nerve supplies motor parasympathetic fibers to all the organs (except the adrenal glands) from the neck down to the second segment of the transverse colon. The vagus also controls a few skeletal muscles, including:\nThis means that the vagus nerve is responsible for such varied tasks as heart rate, gastrointestinal peristalsis, sweating, and quite a few muscle movements in the mouth, including speech (via the recurrent laryngeal nerve). It also has some afferent fibers that innervate the inner (canal) portion of the outer ear (via the auricular branch, also known as Arnold's or Alderman's nerve) and part of the meninges. The vagus nerve is also responsible for regulating inflammation in the body, via the inflammatory reflex.\nEfferent vagus nerve fibers innervating the pharynx and back of the throat are responsible for the gag reflex. In addition, 5-HT3 receptor-mediated afferent vagus stimulation in the gut due to gastroenteritis is a cause of vomiting. Stimulation of the vagus nerve in the cervix uteri (as in some medical procedures) can lead to a vasovagal response.\nThe vagus nerve also plays a role in satiation following food consumption. Knocking out vagal nerve receptors has been shown to cause hyperphagia (greatly increased food intake). Neuroscientist Ivan De Araujo and colleagues have shown that the vagus nerve transmits reward signals from the body to the brain, potentially explaining how stimulation of the nerve leads to emotional changes.\nCardiac effects.\nParasympathetic innervation of the heart is partially controlled by the vagus nerve and is shared by the thoracic ganglia. Vagal and spinal ganglionic nerves mediate the lowering of the heart rate. The right vagus branch innervates the sinoatrial node. In healthy people, parasympathetic tone from these sources is well-matched to sympathetic tone. Hyperstimulation of parasympathetic influence promotes bradyarrhythmias. When hyperstimulated, the left vagal branch predisposes the heart to conduction block at the atrioventricular node.\nAt this location, neuroscientist Otto Loewi first demonstrated that nerves secrete substances called neurotransmitters, which have effects on receptors in target tissues. In his experiment, Loewi electrically stimulated the vagus nerve of a frog heart, which slowed the heart. Then he took the fluid from the heart and transferred it to a second frog heart without a vagus nerve. The second heart slowed without electrical stimulation. Loewi described the substance released by the vagus nerve as vagusstoff, which was later found to be acetylcholine. \nDrugs that inhibit the muscarinic receptors (anticholinergics) such as atropine and scopolamine, are called vagolytic because they inhibit the action of the vagus nerve on the heart, gastrointestinal tract, and other organs. Anticholinergic drugs increase heart rate and are used to treat bradycardia.\nUrogenital and hormonal effects.\nExcessive activation of the vagal nerve during emotional stress, which is a parasympathetic overcompensation for a strong sympathetic nervous system response associated with stress, can also cause vasovagal syncope due to a sudden drop in cardiac output, causing cerebral hypoperfusion. Vasovagal syncope affects young children and women more than other groups. It can also lead to temporary loss of bladder control under moments of extreme fear.\nResearch has shown that women having had complete spinal cord injury can experience orgasms through the vagus nerve, which can go from the uterus and cervix to the brain.\nInsulin signaling activates the adenosine triphosphate (ATP)-sensitive potassium (KATP) channels in the arcuate nucleus, decreases AgRP release, and through the vagus nerve, leads to decreased glucose production by the liver by decreasing gluconeogenic enzymes: phosphoenolpyruvate carboxykinase, glucose 6-phosphatase.\nClinical significance.\nStimulation.\nVagus nerve stimulation (VNS) therapy via a neurostimulator implanted in the chest has been used to control seizures in epilepsy patients and has been approved for treating drug-resistant clinical depression. Several noninvasive VNS devices that stimulate an afferent branch of the vagus nerve are available. GammaCore is recommended by The National Institute for Health and Care Excellence (NICE) for cluster headaches.\nVNS may also be achieved by one of the \"vagal maneuvers\": holding the breath for 20 to 60\u00a0seconds, dipping the face in cold water, coughing, humming or singing, or tensing the stomach muscles as if to bear down to have a bowel movement. Patients with supraventricular tachycardia, atrial fibrillation, and other illnesses may be trained to perform vagal maneuvers (or find one or more on their own).\nVagus nerve blocking (VBLOC) therapy is similar to VNS but used only during the day. In a six-month open-label trial involving three medical centers in Australia, Mexico, and Norway, vagus nerve blocking helped 31\u00a0obese participants lose an average of nearly 15\u00a0percent of their excess weight. As of 2008[ [update]], a yearlong double-blind, phase\u00a0II trial had begun.\nVagotomy.\nVagotomy (cutting of the vagus nerve) is a now obsolete therapy that was performed for peptic ulcer disease and now superseded by oral medications, including H2 antagonists, proton pump inhibitors and antibiotics. Vagotomy is currently being researched as a less invasive alternative weight-loss procedure to gastric bypass surgery. The procedure curbs the feeling of hunger and is sometimes performed in conjunction with putting bands on patients' stomachs, resulting in an average of 43% of excess weight loss at six months with diet and exercise.\nOne serious side effect of vagotomy is a vitamin B12 deficiency later in life \u2013 perhaps after about 10 years \u2013 that is similar to pernicious anemia. The vagus normally stimulates the stomach's parietal cells to secrete acid and intrinsic factor. Intrinsic factor is needed to absorb vitamin B12 from food. The vagotomy reduces this secretion and ultimately leads to deficiency, which, if left untreated, causes nerve damage, tiredness, dementia, paranoia, and ultimately death.\nResearchers from Aarhus University and Aarhus University Hospital have demonstrated that vagotomy prevents (halves the risk of) the development of Parkinson's disease, suggesting that Parkinson's disease begins in the gastrointestinal tract and spreads via the vagus nerve to the brain. Or giving further evidence to the theory that dysregulated environmental stimuli, such as that received by the vagus nerve from the gut, may have a negative effect on the dopamine reward system of the substantia nigra, thereby causing Parkinson's disease.\nVagus nerve pathology.\nThe sympathetic and parasympathetic components of the autonomic nervous system (ANS) control and regulate the function of various organs, glands, and involuntary muscles throughout the body (e.g., vocalization, swallowing, heart rate, respiration, gastric secretion, and intestinal motility). Hence, most of the signs and symptoms of vagus nerve dysfunction, apart from vocalisation, are vague and non specific. Laryngeal nerve palsy results in paralysis of an ipsilateral vocal cord and is used as a pointer to diseases affecting the vagus nerve from its origin down to termination of its branch of the laryngeal nerve.\nThe hypersensitivity of vagal afferent nerves causes refractory or idiopathic cough.\nArnold's nerve ear-cough reflex, though uncommon, is a manifestation of a vagal sensory neuropathy and this is the cause of a refractory chronic cough that can be treated with gabapentin. The cough is triggered by mechanical stimulation of the external auditory meatus and accompanied by other neuropathic features such as throat irritation (laryngeal paresthesia) and cough triggered by exposure to nontussive triggers such as cold air and eating (termed allotussia). These features suggest a neuropathic origin to the cough.\nPathology of the vagus nerve proximal to the laryngeal nerve typically presents with symptom hoarse voice and physical sign of paralysed vocal cords. Although a large proportion of these are the result of idiopathic vocal cord palsy but tumours especially lung cancers are next common cause. Tumours at the apex of right lung and at the hilum of the left lung are the most common oncological causes of vocal cord palsy. Less common tumours causing vocal cord palsy includes thyroid and proximal oesophageal malignancy.\nEtymology.\nThe Latin word \"vagus\" means literally \"wandering\" (the words \"vagrant\", \"vagabond\", \"vague\", and \"divagation\" come from the same root). Sometimes the right and left branches together are spoken of in the plural and are thus called vagi ( ). The vagus was also historically called the \"pneumogastric\" nerve since it innervates both the lungs and the stomach.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "37190", "revid": "30021739", "url": "https://en.wikipedia.org/wiki?curid=37190", "title": "Thomas Becket", "text": "Archbishop of Canterbury from 1162 to 1170\nThomas Becket (), also known as Saint Thomas of Canterbury, Thomas of London and later Thomas \u00e0 Becket (21 December 1119 or 1120\u00a0\u2013 29 December 1170), was an English cleric and statesman who served as Lord Chancellor from 1155 to 1162, and then as Archbishop of Canterbury from 1162 until his death in 1170. He is known for his conflict with King Henry II over the rights and privileges of the Church and was murdered by followers of the king in Canterbury Cathedral. He was canonised by Pope Alexander III two years after his death. He is venerated as a saint and martyr by the Catholic Church and the Anglican Communion.\nSources.\nThe main sources for the life of Becket are a number of biographies written by contemporaries. A few of these documents are by unknown writers, although traditional historiography has given them names. The known biographers are John of Salisbury, Edward Grim, Benedict of Peterborough, William of Canterbury, William fitz Stephen, Guernes of Pont-Sainte-Maxence, Robert of Cricklade, Alan of Tewkesbury, Benet of St Albans, and Herbert of Bosham. The other biographers, who remain anonymous, are generally given the pseudonyms of Anonymous I, Anonymous II (or Anonymous of Lambeth), and Anonymous III (or Lansdowne Anonymous).\nBesides these accounts, there are also two others that are likely contemporary that appear in the \"Quadrilogus II\" and the . Besides these biographies, there is also the mention of the events of Becket's life in the chronicles of the time. These include Robert of Torigni's work, Roger of Howden's and , Ralph de Diceto's works, William of Newburgh's , and Gervase of Canterbury's works. Another account appears in (\"Conquest of Ireland\", 1189) by Gerald of Wales.\nEarly life.\nBecket was born c. 1119 (or 1120 according to later tradition) at Cheapside, London, on 21 December, the feast day of Thomas the Apostle. He was the son of Gilbert and Matilda Beket. Gilbert's father was from Thierville in the lordship of Brionne in Normandy and was either a small landowner or a petty knight. Matilda was also of Norman descent \u2013 her family may have originated near Caen. Gilbert was perhaps related to Theobald of Bec, whose family was also from Thierville. Gilbert began his life as a merchant, perhaps in textiles, but by the 1120s he was living in London and was a property owner, living on the rental income from his properties. He also served as the sheriff of the city at some point. Becket's parents were buried in Old St Paul's Cathedral.\nOne of Becket's father's wealthy friends, Richer de L'Aigle, often invited Thomas to his estates in Sussex where Becket encountered hunting and hawking. According to Grim, Becket learned much from Richer, who was later a signatory of the Constitutions of Clarendon against him.\nAt age 10, Becket was sent as a student to Merton Priory south-west of the city in Surrey. He later attended a grammar school in London, perhaps the one at St Paul's Cathedral. He did not study any subjects beyond the trivium and quadrivium at these schools. Around age 20, he spent about a year in Paris, but he did not study canon or civil law at the time, and his Latin skill remained somewhat rudimentary. Some time after Becket began his schooling, his father suffered financial reverses, and Becket was forced to earn a living as a clerk; with the help of his father he secured a place in the business of a relative. Later Becket acquired a position in the household of Archbishop of Canterbury Theobald of Bec.\nTheobald entrusted him with several important missions to Rome and also sent him to Bologna and Auxerre to study canon law. In 1154, Theobald named Becket Archdeacon of Canterbury, and other ecclesiastical offices included benefices, prebends at Lincoln Cathedral and St Paul's Cathedral, and provost of Beverley. His efficiency in those posts led Theobald to recommend him to King Henry II for the vacant post of Lord Chancellor, to which Becket was appointed in January 1155.\nAs chancellor, Becket enforced the king's traditional sources of revenue that were exacted from all landowners, including churches and bishoprics. King Henry sent his son Henry to live in Becket's household, it being the custom then for noble children to be fostered out to other noble houses.\nPrimacy.\nBecket was nominated as Archbishop of Canterbury in 1162, several months after the death of Theobald. His election was confirmed on 23 May 1162 by a royal council of bishops and noblemen. Henry may have hoped that Becket would continue to put royal government first rather than the church, but the famed transformation of Becket into an ascetic occurred at this time.\nBecket was ordained a priest on 2 June 1162 at Canterbury, and on 3 June he was consecrated as archbishop by Henry of Blois, the Bishop of Winchester and the other suffragan bishops of Canterbury.\nA rift grew between Henry and Becket as Becket resigned his chancellorship and sought to recover and extend the rights of the archbishopric. This led to a series of conflicts with the king, including one over the jurisdiction of secular courts over English clergymen, which accelerated antipathy between Becket and the king. Attempts by Henry to influence other bishops against Becket began in Westminster Abbey in October 1163, where the king sought approval of the traditional rights of royal government in regard to the church. This led to the Constitutions of Clarendon in 1164, where Becket was officially asked to agree to the king's rights or face political repercussions.\nConstitutions of Clarendon.\nKing Henry II presided over assemblies of most of the higher English clergy at Clarendon Palace on 30 January 1164. In 16 constitutions he sought less clerical independence and weaker connections with Rome. He used his skills to induce their consent and apparently succeeded with all but Becket. Finally, even Becket expressed willingness to agree to the substance of the Constitutions of Clarendon, but he still refused formally to sign the documents. Henry summoned Becket to appear before a great council at Northampton Castle on 8 October 1164, to answer allegations of contempt of royal authority and malfeasance in the chancellor's office. Convicted on the charges, Becket stormed out of the trial and fled to the Continent.\nHenry pursued the fugitive archbishop with a series of edicts, targeting Becket and all Becket's friends and supporters, but King Louis VII of France offered Becket protection. He spent nearly two years in the Cistercian abbey of Pontigny until Henry's threats against the order obliged him to return to Sens. Becket fought back by threatening excommunication and an interdict against the king and bishops and the kingdom, but Pope Alexander III, though sympathising with him in theory, favoured a more diplomatic approach. Papal legates were sent in 1167 with authority to act as arbitrators. In 1170, Alexander sent delegates to impose a solution to the dispute. At that point, Henry offered a compromise that would allow Thomas to return to England from exile.\nAssassination.\nOn 14 June 1170 Roger de Pont L'\u00c9v\u00eaque, Archbishop of York, was at Westminster Abbey with Gilbert Foliot, Bishop of London, and Josceline de Bohon, Bishop of Salisbury, to crown the heir apparent, Henry the Young King. This breached Canterbury's privilege of coronation, and in November 1170 Becket excommunicated all three.\nOn hearing reports of Becket's actions, Henry II is said to have uttered words interpreted by his men as wishing Becket killed. The exact wording is in doubt, and several versions were reported. The most commonly quoted, as invented in 1740 and handed down by oral tradition, is \"Will no one rid me of this turbulent priest?\", but according to historian Simon Schama this is incorrect: he accepts the account of the contemporary biographer Grim, writing in Latin, who gives, \"What miserable drones and traitors have I nourished and brought up in my household, who let their lord be treated with such shameful contempt by a low-born cleric?\" Many other variants have found their way into popular culture.\nRegardless of what Henry said, it was interpreted as a royal command. Four knights\u2014Reginald FitzUrse, Hugh de Morville, William de Tracy and Richard le Breton\u2014set out to confront Becket. On 29 December 1170 they arrived at Canterbury. According to accounts by the monk Gervase of Canterbury and eyewitness Grim, the knights placed their weapons under a tree outside the cathedral and hid their armour under cloaks before entering to challenge Becket. The knights told Becket he was to go to Winchester to give an account of his actions, but Becket refused. When he refused their demands to submit to the king's will, they retrieved their weapons and rushed back inside. Becket, meanwhile, proceeded to the Cathedral for vespers. The other monks tried to bolt themselves in for safety, but Becket said to them, \"It is not right to make a fortress out of the house of prayer!\", ordering them to reopen the doors.\nThe four knights, wielding drawn swords, ran into the room crying, \"Where is Thomas Becket, traitor to the king and country?\" They found Becket in a spot near a door to the monastic cloister, the stairs into the crypt, and the stairs leading up into the quire of the cathedral, where the monks were chanting vespers. On seeing them Becket said, \"I am no traitor and I am ready to die.\" One knight grabbed him and tried to pull him outside, but Becket grabbed onto a pillar and bowed his head to make peace with God.\nSeveral contemporary accounts of what happened next exist; of particular note is that of Grim, who was wounded in the attack. This is part of his account:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;\nAfter Becket's death.\nAfter his death, the monks prepared Becket's body for burial. According to some accounts, it was found that Becket had worn a hairshirt under his archbishop's garments \u2013 a sign of penance. Soon after, the faithful throughout Europe began venerating Becket as a martyr, and on 21 February 1173 \u2013 little more than two years after his death \u2013 he was canonised by Pope Alexander III in St Peter's Church, Segni. In 1173, Becket's sister Mary was appointed abbess of Barking as reparation for the murder of her brother. On 12 July 1174, amidst the Revolt of 1173\u20131174, Henry humbled himself in public penance at Becket's tomb and at St Dunstan's Church, Canterbury, which became a popular pilgrimage site.\nBecket's assassins fled north to de Morville's Knaresborough Castle for about a year. De Morville also held property in Cumbria, and this too may have provided a hiding place, as the men prepared for a longer stay in the separate kingdom of Scotland. They were not arrested and Henry did not confiscate their lands, but he did not help them when they sought his advice in August 1171. Pope Alexander excommunicated all four. Seeking forgiveness, the assassins travelled to Rome, where Alexander ordered them to serve as knights in the Holy Lands for a period of 14 years.\nThis sentence also inspired the Knights of Saint Thomas, incorporated in 1191 at Acre and which was to be modelled on the Teutonic Knights. This was the only military order native to England (with chapters in Acre, London, Kilkenny, and Nicosia), just as the Gilbertine Order was the only monastic order native to England. \nThe monks were afraid Becket's body might be stolen, and so his remains were placed beneath the floor of the eastern crypt of the cathedral. A stone cover over it had two holes where pilgrims could insert their heads and kiss the tomb, as illustrated in the \"Miracle Windows\" of the Trinity Chapel. A guard chamber (now the Wax Chamber) had a clear view of the grave. In 1220 Becket's bones were moved to a gold-plated, bejewelled shrine behind the high altar in the recently built Trinity Chapel. The golden casket was placed on a pink marble base with prayer niches raised on three steps. Canterbury's religious history had always brought many pilgrims, and after Becket's death the numbers rapidly rose.\nCult in the Middle Ages.\nIn Dublin, the Abbey of St Thomas the Martyr was built in 1177 for the Augustines. In Scotland, King William the Lion ordered the building of Arbroath Abbey in 1178. On completion in 1197 the new foundation was dedicated to Becket, whom the king had known personally while at the English court as a young man.\nThe translation of Becket's body occurred on 7 July 1220, the 50th jubilee year of his death, and was \"one of the great symbolic events in the life of the medieval English Church\", attended by King Henry III, the papal legate, Archbishop of Canterbury Stephen Langton, and many dignitaries and magnates, both secular and ecclesiastical. A \"major new feast day was instituted, commemorating the translation... celebrated each July almost everywhere in England and in many French churches.\" It was suppressed in 1536 with the Reformation. The shrine was destroyed in 1538 during the dissolution of the monasteries on orders from King Henry VIII. He also destroyed Becket's bones and ordered all mention of his name obliterated.\nA cult began, which included drinking of \"water of Saint Thomas\", a mix of water and the remains of the martyr's blood miraculously multiplied. The procedure was frowned upon by the more orthodox, due to the similarities with the eucharist of the blood of Jesus. The saint's fame quickly spread through the Norman world. The first holy image of Becket is thought to be a mosaic icon still visible in Monreale Cathedral in Sicily, created shortly after his death. Becket's cousins obtained refuge at the Sicilian court during their exile, and King William II of Sicily wed a daughter of Henry II. Marsala Cathedral in western Sicily is dedicated to Becket. Over 45 medieval chasse reliquaries decorated in champlev\u00e9 enamel showing similar scenes from Becket's life survive, including the Becket Casket, constructed to hold relics of him at Peterborough Abbey and now housed in London's Victoria and Albert Museum.\nAs the scion of a mercantile dynasty of later centuries, Mercers, Becket was much regarded as a Londoner by citizens and adopted as London's co-patron saint with Saint Paul: both appear on the seals of the city and of the Lord Mayor. The Bridge House Estates seal has only a Becket image, while his martyrdom is shown on the reverse.\nExplanatory notes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\nFootnotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nBibliography.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "37192", "revid": "0", "url": "https://en.wikipedia.org/wiki?curid=37192", "title": "Arthur of Britain", "text": ""}
{"id": "37194", "revid": "40192293", "url": "https://en.wikipedia.org/wiki?curid=37194", "title": "Jonathan Edwards", "text": "Jonathan Edwards may refer to:\n&lt;templatestyles src=\"Template:TOC_right/styles.css\" /&gt;\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "37195", "revid": "49836368", "url": "https://en.wikipedia.org/wiki?curid=37195", "title": "Tok Pisin language", "text": ""}
{"id": "37196", "revid": "28146993", "url": "https://en.wikipedia.org/wiki?curid=37196", "title": "Causality", "text": "How one process influences another\nCausality is an influence by which one event, process, state, or object (\"a\" \"cause\") contributes to the production of another event, process, state, or object (an \"effect\") where the cause is at least partly responsible for the effect, and the effect is at least partly dependent on the cause. The cause of something may also be described as the reason for the event or process.\nIn general, a process can have multiple causes, which are also said to be \"causal factors\" for it, and all lie in its past. An effect can in turn be a cause of, or causal factor for, many other effects, which all lie in its future. Thus, the distinction between cause and effect either follows from or else provides the distinction between past and future. While the former viewpoint is more prevalent in physics, some writers have held that causality is metaphysically prior to notions of time and space. Causality is an abstraction that indicates how the world progresses. As such, it is a basic concept, and one might expect it to be more apt as an explanation of other concepts of progression than something to be explained by yet more fundamental ideas. The concept is like those of agency and efficacy. For this reason, a leap of intuition may be needed to grasp it. Accordingly, causality is implicit in the structure of ordinary language, as well as explicit in the language of scientific causal notation.\nIn English studies of Aristotelian philosophy, the word \"cause\" is used as a specialized technical term, the translation of Aristotle's term \u03b1\u1f30\u03c4\u03af\u03b1, by which Aristotle meant \"explanation\" or \"answer to a 'why' question\". Aristotle categorized the four types of answers as material, formal, efficient, and final \"causes\". In this case, the \"cause\" is the explanans for the explanandum, and failure to recognize that different kinds of \"cause\" are being considered can lead to futile debate. Of Aristotle's four explanatory modes, the one nearest to the concerns of the present article is the \"efficient\" one.\nDavid Hume, as part of his opposition to rationalism, argued that pure reason alone cannot prove the reality of efficient causality; instead, he appealed to custom and mental habit, observing that all human knowledge derives solely from experience.\nThe topic of causality remains a staple in contemporary philosophy.\n&lt;templatestyles src=\"Template:TOC limit/styles.css\" /&gt;\nConcept.\nMetaphysics.\nThe nature of cause and effect is a concern of the subject known as metaphysics. Kant thought that time and space were notions prior to human understanding of the progress or evolution of the world, and he also recognized the priority of causality. But he did not have the understanding that came with knowledge of Minkowski geometry and the special theory of relativity, that the notion of causality can be used as a prior foundation from which to construct notions of time and space.\nOntology.\nA general metaphysical question about cause and effect is: \"what kind of entity can be a cause, and what kind of entity can be an effect?\"\nOne viewpoint on this question is that cause and effect are of one and the same kind of entity, causality being an asymmetric relation between them. That is to say, it would make good sense grammatically to say either \"\"A\" is the cause and \"B\" the effect\" or \"\"B\" is the cause and \"A\" the effect\", though only one of those two can be actually true. In this view, one opinion, proposed as a metaphysical principle in process philosophy, is that every cause and every effect is respectively some process, event, becoming, or happening. An example is 'his tripping over the step was the cause, and his breaking his ankle the effect'. Another view is that causes and effects are 'states of affairs', with the exact natures of those entities being more loosely defined than in process philosophy.\nAnother viewpoint on this question is the more classical one, that a cause and its effect can be of different kinds of entity. For example, in Aristotle's efficient causal explanation, an action can be a cause while an enduring object is its effect. For example, the generative actions of his parents can be regarded as the efficient cause, with Socrates being the effect, Socrates being regarded as an enduring object, in philosophical tradition called a 'substance', as distinct from an action.\nEpistemology.\nSince causality is a subtle metaphysical notion, considerable intellectual effort, along with exhibition of evidence, is needed to establish knowledge of it in particular empirical circumstances. According to David Hume, the human mind is unable to perceive causal relations directly. On this ground, the scholar distinguished between the regularity view of causality and the counterfactual notion. According to the counterfactual view, \"X\" causes \"Y\" if and only if, without \"X, Y\" would not exist. Hume interpreted the latter as an ontological view, i.e., as a description of the nature of causality; but, given the limitations of the human mind, advised using the former (stating, roughly, that \"X\" causes \"Y\" if and only if the two events are spatiotemporally conjoined, and \"X\" precedes \"Y\") as an epistemic definition of causality. We need an epistemic concept of causality in order to distinguish between causal and noncausal relations. The contemporary philosophical literature on causality can be divided into five major approaches to causality. These include the (mentioned above) regularity, probabilistic, counterfactual, mechanistic, and manipulationist views. The five approaches can be shown to be reductive, i.e., they define causality in terms of relations of other types. According to this reading, they define causality in terms of, respectively, empirical regularities (constant conjunctions of events), changes in conditional probabilities, counterfactual conditions, mechanisms underlying causal relations, and invariance under intervention.\nGeometrical significance.\nCausality has the properties of antecedence and contiguity. These are topological, and are ingredients for space-time geometry. As developed by Alfred Robb, these properties allow the derivation of the notions of time and space. Max Jammer writes \"the Einstein postulate ... opens the way to a straightforward construction of the causal topology ... of Minkowski space.\" Causal efficacy propagates no faster than light.\nThus, the notion of causality is metaphysically prior to the notions of time and space. In practical terms, this is because use of the relation of causality is necessary for the interpretation of empirical experiments. Interpretation of experiments is needed to establish the physical and geometrical notions of time and space.\nVolition.\nThe deterministic world-view holds that the history of the universe can be exhaustively represented as a progression of events following one after the other as cause and effect. Incompatibilism holds that determinism is incompatible with free will, so if determinism is true, \"free will\" does not exist. Compatibilism, on the other hand, holds that determinism is compatible with, or even necessary for, free will.\nNecessary and sufficient causes.\nCauses may sometimes be distinguished into two types: necessary and sufficient. A third type of causation, which requires neither necessity nor sufficiency, but which contributes to the effect, is called a \"contributory cause\".\nJ. L. Mackie argues that usual talk of \"cause\" in fact refers to INUS conditions (insufficient but non-redundant parts of a condition which is itself unnecessary but sufficient for the occurrence of the effect). An example is a short circuit as a cause for a house burning down. Consider the collection of events: the short circuit, the proximity of flammable material, and the absence of firefighters. Together these are unnecessary but sufficient to the house's burning down (since many other collections of events certainly could have led to the house burning down, for example shooting the house with a flamethrower in the presence of oxygen and so forth). Within this collection, the short circuit is an insufficient (since the short circuit by itself would not have caused the fire) but non-redundant (because the fire would not have happened without it, everything else being equal) part of a condition which is itself unnecessary but sufficient for the occurrence of the effect. So, the short circuit is an INUS condition for the occurrence of the house burning down. \nHowever, Mackie's INUS account succumbs to the problem of joint effects of a common cause: it incorrectly identifies one effect of a common cause as an instantiated INUS condition for another effect of the same common cause, even though the two effects are not causally related. Modern regularity theories aim to overcome this problem using so-called non-redundant regularities. \nContrasted with conditionals.\nConditional statements are \"not\" statements of causality. An important distinction is that statements of causality require the antecedent to precede or coincide with the consequent in time, whereas conditional statements do not require this temporal order. Confusion commonly arises since many different statements in English may be presented using \"If ..., then ...\" form (and, arguably, because this form is far more commonly used to make a statement of causality). The two types of statements are distinct, however.\nFor example, all of the following statements are true when interpreting \"If ..., then ...\" as the material conditional:\nThe first is true since both the antecedent and the consequent are true. The second is true in sentential logic and indeterminate in natural language, regardless of the consequent statement that follows, because the antecedent is false.\nThe ordinary indicative conditional has somewhat more structure than the material conditional. For instance, although the first is the closest, neither of the preceding two statements seems true as an ordinary indicative reading. But the sentence:\nintuitively seems to be true, even though there is no straightforward causal relation in this hypothetical situation between Shakespeare's not writing Macbeth and someone else's actually writing it.\nAnother sort of conditional, the counterfactual conditional, has a stronger connection with causality, yet even counterfactual statements are not all examples of causality. Consider the following two statements:\nIn the first case, it would be incorrect to say that A's being a triangle \"caused\" it to have three sides, since the relationship between triangularity and three-sidedness is that of definition. The property of having three sides actually determines A's state as a triangle. Nonetheless, even when interpreted counterfactually, the first statement is true. An early version of Aristotle's \"four cause\" theory is described as recognizing \"essential cause\". In this version of the theory, that the closed polygon has three sides is said to be the \"essential cause\" of its being a triangle. This use of the word 'cause' is of course now far obsolete. Nevertheless, it is within the scope of ordinary language to say that it is essential to a triangle that it has three sides.\nA full grasp of the concept of conditionals is important to understanding the literature on causality. In everyday language, loose conditional statements are often enough made, and need to be interpreted carefully.\nQuestionable cause.\nFallacies of questionable cause, also known as causal fallacies, \"non-causa pro causa\" (Latin for \"non-cause for cause\"), or false cause, are informal fallacies where a cause is incorrectly identified.\nTheories.\nCounterfactual theories.\nCounterfactual theories define causation in terms of a counterfactual relation, and can often be seen as \"floating\" their account of causality on top of an account of the logic of counterfactual conditionals. Counterfactual theories reduce facts about causation to facts about what would have been true under counterfactual circumstances. The idea is that causal relations can be framed in the form of \"Had C not occurred, E would not have occurred.\" This approach can be traced back to David Hume's definition of the causal relation as that \"where, if the first object had not been, the second never had existed.\" More full-fledged analysis of causation in terms of counterfactual conditionals only came in the 20th century after development of the possible world semantics for the evaluation of counterfactual conditionals. In his 1973 paper \"Causation,\" David Lewis proposed the following definition of the notion of \"causal dependence\":\nAn event E \"causally depends\" on C if, and only if, (i) if C had occurred, then E would have occurred, and (ii) if C had not occurred, then E would not have occurred.\nCausation is then analyzed in terms of counterfactual dependence. That is, C causes E if and only if there exists a sequence of events C, D1, D2, ... Dk, E such that each event in the sequence counterfactually depends on the previous. This chain of causal dependence may be called a \"mechanism\".\nNote that the analysis does not purport to explain how we make causal judgements or how we reason about causation, but rather to give a metaphysical account of what it is for there to be a causal relation between some pair of events. If correct, the analysis has the power to explain certain features of causation. Knowing that causation is a matter of counterfactual dependence, we may reflect on the nature of counterfactual dependence to account for the nature of causation. For example, in his paper \"Counterfactual Dependence and Time's Arrow,\" Lewis sought to account for the time-directedness of counterfactual dependence in terms of the semantics of the counterfactual conditional. If correct, this theory can serve to explain a fundamental part of our experience, which is that we can causally affect the future but not the past.\nOne challenge for the counterfactual account is overdetermination, whereby an effect has multiple causes. For instance, suppose Alice and Bob both throw bricks at a window and it breaks. If Alice hadn't thrown the brick, then it still would have broken, suggesting that Alice wasn't a cause; however, intuitively, Alice did cause the window to break. The Halpern-Pearl definitions of causality take account of examples like these. The first and third Halpern-Pearl conditions are easiest to understand: AC1 requires that Alice threw the brick and the window broke in the actual work. AC3 requires that Alice throwing the brick is a minimal cause (cf. blowing a kiss and throwing a brick). Taking the \"updated\" version of AC2(a), the basic idea is that we have to find a set of variables and settings thereof such that preventing Alice from throwing a brick also stops the window from breaking. One way to do this is to stop Bob from throwing the brick. Finally, for AC2(b), we have to hold things as per AC2(a) and show that Alice throwing the brick breaks the window. (The full definition is a little more involved, involving checking all subsets of variables.)\nProbabilistic causation.\nInterpreting causation as a deterministic relation means that if \"A\" causes \"B\", then \"A\" must \"always\" be followed by \"B\". In this sense, war does not cause deaths, nor does smoking cause cancer or emphysema. As a result, many turn to a notion of probabilistic causation. Informally, \"A\" (\"The person is a smoker\") probabilistically causes \"B\" (\"The person has now or will have cancer at some time in the future\"), if the information that \"A\" occurred increases the likelihood of \"B\"s occurrence. Formally, P{\"B\"|\"A\"}\u2265 P{\"B\"} where P{\"B\"|\"A\"} is the conditional probability that \"B\" will occur given the information that \"A\" occurred, and P{\"B\"} is the probability that \"B\" will occur having no knowledge whether \"A\" did or did not occur. This intuitive condition is not adequate as a definition for probabilistic causation because of its being too general and thus not meeting our intuitive notion of cause and effect. For example, if \"A\" denotes the event \"The person is a smoker,\" \"B\" denotes the event \"The person now has or will have cancer at some time in the future\" and \"C\" denotes the event \"The person now has or will have emphysema some time in the future,\" then the following three relationships hold: P{\"B\"|\"A\"} \u2265 P{\"B\"}, P{\"C\"|\"A\"} \u2265 P{\"C\"} and P{\"B\"|\"C\"} \u2265 P{\"B\"}. The last relationship states that knowing that the person has emphysema increases the likelihood that he will have cancer. The reason for this is that having the information that the person has emphysema increases the likelihood that the person is a smoker, thus indirectly increasing the likelihood that the person will have cancer. However, we would not want to conclude that having emphysema causes cancer. Thus, we need additional conditions such as temporal relationship of \"A\" to \"B\" and a rational explanation as to the mechanism of action. It is hard to quantify this last requirement and thus different authors prefer somewhat different definitions.\nCausal calculus.\nWhen experimental interventions are infeasible or illegal, the derivation of a cause-and-effect relationship from observational studies must rest on some qualitative theoretical assumptions, for example, that symptoms do not cause diseases, usually expressed in the form of missing arrows in causal graphs such as Bayesian networks or path diagrams. The theory underlying these derivations relies on the distinction between \"conditional probabilities\", as in formula_1, and \"interventional probabilities\", as in formula_2. The former reads: \"the probability of finding cancer in a person known to smoke, having started, unforced by the experimenter, to do so at an unspecified time in the past\", while the latter reads: \"the probability of finding cancer in a person forced by the experimenter to smoke at a specified time in the past\". The former is a statistical notion that can be estimated by observation with negligible intervention by the experimenter, while the latter is a causal notion which is estimated in an experiment with an important controlled randomized intervention. It is specifically characteristic of quantal phenomena that observations defined by incompatible variables always involve important intervention by the experimenter, as described quantitatively by the observer effect. In classical thermodynamics, processes are initiated by interventions called thermodynamic operations. In other branches of science, for example astronomy, the experimenter can often observe with negligible intervention.\nThe theory of \"causal calculus\" (also known as do-calculus, Judea Pearl's Causal Calculus, Calculus of\nActions) permits one to infer interventional probabilities from conditional probabilities in causal Bayesian networks with unmeasured variables. One very practical result of this theory is the characterization of confounding variables, namely, a sufficient set of variables that, if adjusted for, would yield the correct causal effect between variables of interest. It can be shown that a sufficient set for estimating the causal effect of formula_3 on formula_4 is any set of non-descendants of formula_3 that formula_6-separate formula_3 from formula_4 after removing all arrows emanating from formula_3. This criterion, called \"backdoor\", provides a mathematical definition of \"confounding\" and helps researchers identify accessible sets of variables worthy of measurement.\nStructure learning.\nWhile derivations in causal calculus rely on the structure of the causal graph, parts of the causal structure can, under certain assumptions, be learned from statistical data. The basic idea goes back to Sewall Wright's 1921 work on path analysis. A \"recovery\" algorithm was developed by Rebane and Pearl (1987) which rests on Wright's distinction between the three possible types of causal substructures allowed in a directed acyclic graph (DAG):\nType 1 and type 2 represent the same statistical dependencies (i.e., formula_3 and formula_14 are independent given formula_4) and are, therefore, indistinguishable within purely cross-sectional data. Type 3, however, can be uniquely identified, since formula_3 and formula_14 are marginally independent and all other pairs are dependent. Thus, while the \"skeletons\" (the graphs stripped of arrows) of these three triplets are identical, the directionality of the arrows is partially identifiable. The same distinction applies when formula_3 and formula_14 have common ancestors, except that one must first condition on those ancestors. Algorithms have been developed to systematically determine the skeleton of the underlying graph and, then, orient all arrows whose directionality is dictated by the conditional independencies observed.\nAlternative methods of structure learning search through the \"many\" possible causal structures among the variables, and remove ones which are strongly incompatible with the observed correlations. In general this leaves a set of possible causal relations, which should then be tested by analyzing time series data or, preferably, designing appropriately controlled experiments. In contrast with Bayesian Networks, path analysis (and its generalization, structural equation modeling), serve better to estimate a known causal effect or to test a causal model than to generate causal hypotheses.\nFor nonexperimental data, causal direction can often be inferred if information about time is available. This is because (according to many, though not all, theories) causes must precede their effects temporally. This can be determined by statistical time series models, for instance, or with a statistical test based on the idea of Granger causality, or by direct experimental manipulation. The use of temporal data can permit statistical tests of a pre-existing theory of causal direction. For instance, our degree of confidence in the direction and nature of causality is much greater when supported by cross-correlations, ARIMA models, or cross-spectral analysis using vector time series data than by cross-sectional data.\nDerivation theories.\nNobel laureate Herbert A. Simon and philosopher Nicholas Rescher claim that the asymmetry of the causal relation is unrelated to the asymmetry of any mode of implication that contraposes. Rather, a causal relation is not a relation between values of variables, but a function of one variable (the cause) on to another (the effect). So, given a system of equations, and a set of variables appearing in these equations, we can introduce an asymmetric relation among individual equations and variables that corresponds perfectly to our commonsense notion of a causal ordering. The system of equations must have certain properties, most importantly, if some values are chosen arbitrarily, the remaining values will be determined uniquely through a path of serial discovery that is perfectly causal. They postulate the inherent serialization of such a system of equations may correctly capture causation in all empirical fields, including physics and economics.\nManipulation theories.\nSome theorists have equated causality with manipulability. Under these theories, \"x\" causes \"y\" only in the case that one can change \"x\" in order to change \"y\". This coincides with commonsense notions of causations, since often we ask causal questions in order to change some feature of the world. For instance, we are interested in knowing the causes of crime so that we might find ways of reducing it.\nThese theories have been criticized on two primary grounds. First, theorists complain that these accounts are circular. Attempting to reduce causal claims to manipulation requires that manipulation is more basic than causal interaction. But describing manipulations in non-causal terms has provided a substantial difficulty.\nThe second criticism centers around concerns of anthropocentrism. It seems to many people that causality is some existing relationship in the world that we can harness for our desires. If causality is identified with our manipulation, then this intuition is lost. In this sense, it makes humans overly central to interactions in the world.\nSome attempts to defend manipulability theories are recent accounts that do not claim to reduce causality to manipulation. These accounts use manipulation as a sign or feature in causation without claiming that manipulation is more fundamental than causation.\nProcess theories.\nSome theorists are interested in distinguishing between causal processes and non-causal processes (Russell 1948; Salmon 1984). These theorists often want to distinguish between a process and a pseudo-process. As an example, a ball moving through the air (a process) is contrasted with the motion of a shadow (a pseudo-process). The former is causal in nature while the latter is not.\nSalmon (1984) claims that causal processes can be identified by their ability to transmit an alteration over space and time. An alteration of the ball (a mark by a pen, perhaps) is carried with it as the ball goes through the air. On the other hand, an alteration of the shadow (insofar as it is possible) will not be transmitted by the shadow as it moves along.\nThese theorists claim that the important concept for understanding causality is not causal relationships or causal interactions, but rather identifying causal processes. The former notions can then be defined in terms of causal processes.\nA subgroup of the process theories is the mechanistic view on causality. It states that causal relations supervene on mechanisms. While the notion of mechanism is understood differently, the definition put forward by the group of philosophers referred to as the 'New Mechanists' dominate the literature.\nFields.\nScience.\nFor the scientific investigation of efficient causality, the cause and effect are each best conceived of as temporally transient processes.\nWithin the conceptual frame of the scientific method, an investigator sets up several distinct and contrasting temporally transient material processes that have the structure of experiments, and records candidate material responses, normally intending to determine causality in the physical world. For instance, one may want to know whether a high intake of carrots causes humans to develop the bubonic plague. The quantity of carrot intake is a process that is varied from occasion to occasion. The occurrence or non-occurrence of subsequent bubonic plague is recorded. To establish causality, the experiment must fulfill certain criteria, only one example of which is mentioned here. For example, instances of the hypothesized cause must be set up to occur at a time when the hypothesized effect is relatively unlikely in the absence of the hypothesized cause; such unlikelihood is to be established by empirical evidence. A mere observation of a correlation is not nearly adequate to establish causality. In nearly all cases, establishment of causality relies on repetition of experiments and probabilistic reasoning. Hardly ever is causality established more firmly than as more or less probable. It is most convenient for establishment of causality if the contrasting material states of affairs are precisely matched, except for only one variable factor, perhaps measured by a real number.\nPhysics.\nOne has to be careful in the use of the word cause in physics. Properly speaking, the hypothesized cause and the hypothesized effect are each temporally transient processes. For example, force is a useful concept for the explanation of acceleration, but force is not by itself a cause. More is needed. For example, a temporally transient process might be characterized by a definite change of force at a definite time. Such a process can be regarded as a cause. Causality is not inherently implied in equations of motion, but postulated as an additional constraint that needs to be satisfied (i.e. a cause always precedes its effect). This constraint has mathematical implications such as the Kramers-Kronig relations.\nCausality is one of the most fundamental and essential notions of physics. Causal efficacy cannot 'propagate' faster than light. Otherwise, reference coordinate systems could be constructed (using the Lorentz transform of special relativity) in which an observer would see an effect precede its cause (i.e. the postulate of causality would be violated).\nCausal notions appear in the context of the flow of mass-energy. Any actual process has causal efficacy that can propagate no faster than light. In contrast, an abstraction has no causal efficacy. Its mathematical expression does not propagate in the ordinary sense of the word, though it may refer to virtual or nominal 'velocities' with magnitudes greater than that of light. For example, wave packets are mathematical objects that have group velocity and phase velocity. The energy of a wave packet travels at the group velocity (under normal circumstances); since energy has causal efficacy, the group velocity cannot be faster than the speed of light. The phase of a wave packet travels at the phase velocity; since phase is not causal, the phase velocity of a wave packet can be faster than light.\nCausal notions are important in general relativity to the extent that the existence of an arrow of time demands that the universe's semi-Riemannian manifold be orientable, so that \"future\" and \"past\" are globally definable quantities.\nEngineering.\nA causal system is a system with output and internal states that depends only on the current and previous input values. A system that has \"some\" dependence on input values from the future (in addition to possible past or current input values) is termed an acausal system, and a system that depends \"solely\" on future input values is an anticausal system. Acausal filters, for example, can only exist as postprocessing filters, because these filters can extract future values from a memory buffer or a file.\nWe have to be very careful with causality in physics and engineering. Cellier, Elmqvist, and Otter describe causality forming the basis of physics as a misconception, because physics is essentially acausal. In their article they cite a simple example: \"The relationship between voltage across and current through an electrical resistor can be described by Ohm's law: V = IR, yet, whether it is the current flowing through the resistor that causes a voltage drop, or whether it is the difference between the electrical potentials on the two wires that causes current to flow is, from a physical perspective, a meaningless question\". In fact, if we explain cause-effect using the law, we need two explanations to describe an electrical resistor: as a voltage-drop-causer or as a current-flow-causer. There is no physical experiment in the world that can distinguish between action and reaction.\nBiology, medicine and epidemiology.\nAustin Bradford Hill built upon the work of Hume and Popper and suggested in his paper \"The Environment and Disease: Association or Causation?\" that aspects of an association such as strength, consistency, specificity, and temporality be considered in attempting to distinguish causal from noncausal associations in the epidemiological situation. (See Bradford Hill criteria.) He did not note however, that temporality is the only necessary criterion among those aspects. Directed acyclic graphs (DAGs) are increasingly used in epidemiology to help enlighten causal thinking.\nPsychology.\nPsychologists take an empirical approach to causality, investigating how people and non-human animals detect or infer causation from sensory information, prior experience and innate knowledge.\nAttribution:\nAttribution theory is the theory concerning how people explain individual occurrences of causation. Attribution can be external (assigning causality to an outside agent or force\u2014claiming that some outside thing motivated the event) or internal (assigning causality to factors within the person\u2014taking personal responsibility or accountability for one's actions and claiming that the person was directly responsible for the event). Taking causation one step further, the type of attribution a person provides influences their future behavior.\nThe intention behind the cause or the effect can be covered by the subject of action. See also accident; blame; intent; and responsibility.\nWhereas David Hume argued that causes are inferred from non-causal observations, Immanuel Kant claimed that people have innate assumptions about causes. Within psychology, Patricia Cheng attempted to reconcile the Humean and Kantian views. According to her power PC theory, people filter observations of events through an intuition that causes have the power to generate (or prevent) their effects, thereby inferring specific cause-effect relations.\nOur view of causation depends on what we consider to be the relevant events. Another way to view the statement, \"Lightning causes thunder\" is to see both lightning and thunder as two perceptions of the same event, viz., an electric discharge that we perceive first visually and then aurally.\nDavid Sobel and Alison Gopnik from the Psychology Department of UC Berkeley designed a device known as \"the blicket detector\" which would turn on when an object was placed on it. Their research suggests that \"even young children will easily and swiftly learn about a new causal power of an object and spontaneously use that information in classifying and naming the object.\"\nSome researchers such as Anjan Chatterjee at the University of Pennsylvania and Jonathan Fugelsang at the University of Waterloo are using neuroscience techniques to investigate the neural and psychological underpinnings of causal launching events in which one object causes another object to move. Both temporal and spatial factors can be manipulated.\nSee Causal Reasoning (Psychology) for more information.\nStatistics and economics.\nStatistics and economics usually employ pre-existing data or experimental data to infer causality by regression methods. The body of statistical techniques involves substantial use of regression analysis. Typically a linear relationship such as\nformula_20\nis postulated, in which formula_21 is the \"i\"th observation of the dependent variable (hypothesized to be the caused variable), formula_22 for \"j\"=1...,\"k\" is the \"i\"th observation on the \"j\"th independent variable (hypothesized to be a causative variable), and formula_23 is the error term for the \"i\"th observation (containing the combined effects of all other causative variables, which must be uncorrelated with the included independent variables). If there is reason to believe that none of the formula_24s is caused by \"y\", then estimates of the coefficients formula_25 are obtained. If the null hypothesis that formula_26 is rejected, then the alternative hypothesis that formula_27 and equivalently that formula_24 causes \"y\" cannot be rejected. On the other hand, if the null hypothesis that formula_26 cannot be rejected, then equivalently the hypothesis of no causal effect of formula_24 on \"y\" cannot be rejected. Here the notion of causality is one of contributory causality as discussed above: If the true value formula_31, then a change in formula_24 will result in a change in \"y\" \"unless\" some other causative variable(s), either included in the regression or implicit in the error term, change in such a way as to exactly offset its effect; thus a change in formula_24 is \"not sufficient\" to change \"y\". Likewise, a change in formula_24 is \"not necessary\" to change \"y\", because a change in \"y\" could be caused by something implicit in the error term (or by some other causative explanatory variable included in the model).\nThe above way of testing for causality requires belief that there is no reverse causation, in which \"y\" would cause formula_24. This belief can be established in one of several ways. First, the variable formula_24 may be a non-economic variable: for example, if rainfall amount formula_24 is hypothesized to affect the futures price \"y\" of some agricultural commodity, it is impossible that in fact the futures price affects rainfall amount (provided that cloud seeding is never attempted). Second, the instrumental variables technique may be employed to remove any reverse causation by introducing a role for other variables (instruments) that are known to be unaffected by the dependent variable. Third, the principle that effects cannot precede causes can be invoked, by including on the right side of the regression only variables that precede in time the dependent variable; this principle is invoked, for example, in testing for Granger causality and in its multivariate analog, vector autoregression, both of which control for lagged values of the dependent variable while testing for causal effects of lagged independent variables.\nRegression analysis controls for other relevant variables by including them as regressors (explanatory variables). This helps to avoid false inferences of causality due to the presence of a third, underlying, variable that influences both the potentially causative variable and the potentially caused variable: its effect on the potentially caused variable is captured by directly including it in the regression, so that effect will not be picked up as an indirect effect through the potentially causative variable of interest. Given the above procedures, coincidental (as opposed to causal) correlation can be probabilistically rejected if data samples are large and if regression results pass cross-validation tests showing that the correlations hold even for data that were not used in the regression. Asserting with certitude that a common-cause is absent and the regression represents the true causal structure is \"in principle\" impossible.\nThe problem of omitted variable bias, however, has to be balanced against the risk of inserting Causal colliders, in which the addition of a new variable formula_38 induces a correlation between formula_24 and formula_40 via Berkson's paradox.\nApart from constructing statistical models of observational and experimental data, economists use axiomatic (mathematical) models to infer and represent causal mechanisms. Highly abstract theoretical models that isolate and idealize one mechanism dominate microeconomics. In macroeconomics, economists use broad mathematical models that are calibrated on historical data. A subgroup of calibrated models, dynamic stochastic general equilibrium (DSGE) models are employed to represent (in a simplified way) the whole economy and simulate changes in fiscal and monetary policy.\nStatistical and economic analyses often rely on regression methods applied to observational or pre\u2011existing data to infer causal relationships. Experimental designs, in contrast, establish causality by systematically manipulating independent variables under controlled conditions. Experiments therefore provide stronger internal validity because causal mechanisms are demonstrated directly rather than inferred from patterns in observational data.\nManagement.\nFor quality control in manufacturing in the 1960s, Kaoru Ishikawa developed a cause and effect diagram, known as an Ishikawa diagram or fishbone diagram. The diagram categorizes causes, such as into the six main categories shown here. These categories are then sub-divided. Ishikawa's method identifies \"causes\" in brainstorming sessions conducted among various groups involved in the manufacturing process. These groups can then be labeled as categories in the diagrams. The use of these diagrams has now spread beyond quality control, and they are used in other areas of management and in design and engineering. Ishikawa diagrams have been criticized for failing to make the distinction between necessary conditions and sufficient conditions. It seems that Ishikawa was not even aware of this distinction.\nHumanities.\nHistory.\nIn the discussion of history, events are sometimes considered as if in some way being agents that can then bring about other historical events. Thus, the combination of poor harvests, the hardships of the peasants, high taxes, lack of representation of the people, and kingly ineptitude are among the \"causes\" of the French Revolution. This is a somewhat Platonic and Hegelian view that reifies causes as ontological entities. In Aristotelian terminology, this use approximates to the case of the \"efficient\" cause.\nSome philosophers of history such as Arthur Danto have claimed that \"explanations in history and elsewhere\" describe \"not simply an event\u2014something that happens\u2014but a change\". Like many practicing historians, they treat causes as intersecting actions and sets of actions which bring about \"larger changes\", in Danto's words: to decide \"what are the elements which persist through a change\" is \"rather simple\" when treating an individual's \"shift in attitude\", but \"it is considerably more complex and metaphysically challenging when we are interested in such a change as, say, the break-up of feudalism or the emergence of nationalism\".\nMuch of the historical debate about causes has focused on the relationship between communicative and other actions, between singular and repeated ones, and between actions, structures of action or group and institutional contexts and wider sets of conditions. John Gaddis has distinguished between exceptional and general causes (following Marc Bloch) and between \"routine\" and \"distinctive links\" in causal relationships: \"in accounting for what happened at Hiroshima on August 6, 1945, we attach greater importance to the fact that President Truman ordered the dropping of an atomic bomb than to the decision of the Army Air Force to carry out his orders.\" He has also pointed to the difference between immediate, intermediate and distant causes. For his part, Christopher Lloyd puts forward four \"general concepts of causation\" used in history: the \"metaphysical idealist concept, which asserts that the phenomena of the universe are products of or emanations from an omnipotent being or such final cause\"; \"the empiricist (or Humean) regularity concept, which is based on the idea of causation being a matter of constant conjunctions of events\"; \"the functional/teleological/consequential concept\", which is \"goal-directed, so that goals are causes\"; and the \"realist, structurist and dispositional approach, which sees relational structures and internal dispositions as the causes of phenomena\".\nLaw.\nAccording to law and jurisprudence, legal cause must be demonstrated to hold a defendant liable for a crime or a tort (i.e. a civil wrong such as negligence or trespass). It must be proven that causality, or a \"sufficient causal link\" relates the defendant's actions to the criminal event or damage in question. Causation is also an essential legal element that must be proven to qualify for remedy measures under international trade law.\nHistory.\nHindu philosophy.\nVedic period (c.\u20091750\u2013500 BCE) literature contains early discussions of karma. Karma is the belief held by Hinduism and other Indian religions that a person's actions cause certain effects in the current life and/or in future life, positively or negatively. The various philosophical schools (darshanas) provide different accounts of the subject. A doctrine of satkaryavada affirms that the effect inheres in the cause in some way. The effect is thus either a real or apparent modification of the cause. A doctrine of asatkaryavada affirms that the effect does not inhere in the cause, but is a new arising. In Brahma Samhita, Brahma describes Krishna as the prime cause of all causes.\nhttp:// identifies five causes for any action (knowing which it can be perfected): the body, the individual soul, the senses, the efforts and the supersoul.\nAccording to Monier-Williams, in the Ny\u0101ya causation theory from Sutra I.2.I,2 in the Vaisheshika philosophy, from causal non-existence is effectual non-existence; but, not effectual non-existence from causal non-existence. A cause precedes an effect. With a threads and cloth metaphors, three causes are:\nMonier-Williams also proposed that Aristotle's and the Nyaya's causality are considered conditional aggregates necessary to man's productive work.\nBuddhist philosophy.\nKarma is the causality principle focusing on 1) causes, 2) actions, 3) effects, where it is the mind's phenomena that guide the actions that the actor performs. Buddhism trains the actor's actions for continued and uncontrived virtuous outcomes aimed at reducing suffering. This follows the Subject\u2013verb\u2013object structure.\nThe general or universal definition of pratityasamutpada (or \"dependent origination\" or \"dependent arising\" or \"interdependent co-arising\") is that everything arises in dependence upon multiple causes and conditions; nothing exists as a singular, independent entity. A traditional example in Buddhist texts is of three sticks standing upright and leaning against each other and supporting each other. If one stick is taken away, the other two will fall to the ground.\nCausality in the Chittamatrin Buddhist school approach, Asanga's (c.\u2009400 CE) mind-only Buddhist school, asserts that objects cause consciousness in the mind's image. Because causes precede effects, which must be different entities, then subject and object are different. For this school, there are no objects which are entities external to a perceiving consciousness. The Chittamatrin and the Yogachara Svatantrika schools accept that there are no objects external to the observer's causality. This largely follows the Nikayas approach.\nThe Vaibhashika (c.\u2009500 CE) is an early Buddhist school which favors direct object contact and accepts simultaneous cause and effects. This is based in the consciousness example which says, intentions and feelings are mutually accompanying mental factors that support each other like poles in tripod. In contrast, simultaneous cause and effect rejectors say that if the effect already exists, then it cannot effect the same way again. How past, present and future are accepted is a basis for various Buddhist school's causality viewpoints.\nAll the classic Buddhist schools teach karma. \"The law of karma is a special instance of the law of cause and effect, according to which all our actions of body, speech, and mind are causes and all our experiences are their effects.\"\nWestern philosophy.\nAristotelian.\nAristotle identified four kinds of answer or explanatory mode to various \"Why?\" questions. He thought that, for any given topic, all four kinds of explanatory mode were important, each in its own right. As a result of traditional specialized philosophical peculiarities of language, with translations between ancient Greek, Latin, and English, the word 'cause' is nowadays in specialized philosophical writings used to label Aristotle's four kinds. In ordinary language, the word 'cause' has a variety of meanings, the most common of which refers to efficient causation, which is the topic of the present article.\nOf Aristotle's four kinds or explanatory modes, only one, the 'efficient cause' is a cause as defined in the leading paragraph of this present article. The other three explanatory modes might be rendered material composition, structure and dynamics, and, again, criterion of completion. The word that Aristotle used was \u03b1\u1f30\u03c4\u03af\u03b1. For the present purpose, that Greek word would be better translated as \"explanation\" than as \"cause\" as those words are most often used in current English. Another translation of Aristotle is that he meant \"the four Becauses\" as four kinds of answer to \"why\" questions.\nAristotle assumed efficient causality as referring to a basic fact of experience, not explicable by, or reducible to, anything more fundamental or basic.\nIn some works of Aristotle, the four causes are listed as (1) the essential cause, (2) the logical ground, (3) the moving cause, and (4) the final cause. In this listing, a statement of essential cause is a demonstration that an indicated object conforms to a definition of the word that refers to it. A statement of logical ground is an argument as to why an object statement is true. These are further examples of the idea that a \"cause\" in general in the context of Aristotle's usage is an \"explanation\".\nThe word \"efficient\" used here can also be translated from Aristotle as \"moving\" or \"initiating\".\nEfficient causation was connected with Aristotelian physics, which recognized the four elements (earth, air, fire, water), and added the fifth element (aether). Water and earth by their intrinsic property \"gravitas\" or heaviness intrinsically fall toward, whereas air and fire by their intrinsic property \"levitas\" or lightness intrinsically rise away from, Earth's center\u2014the motionless center of the universe\u2014in a straight line while accelerating during the substance's approach to its natural place.\nAs air remained on Earth, however, and did not escape Earth while eventually achieving infinite speed\u2014an absurdity\u2014Aristotle inferred that the universe is finite in size and contains an invisible substance that holds planet Earth and its atmosphere, the sublunary sphere, centered in the universe. And since celestial bodies exhibit perpetual, unaccelerated motion orbiting planet Earth in unchanging relations, Aristotle inferred that the fifth element, \"aither\", that fills space and composes celestial bodies intrinsically moves in perpetual circles, the only constant motion between two points. (An object traveling a straight line from point \"A\" to \"B\" and back must stop at either point before returning to the other.)\nLeft to itself, a thing exhibits \"natural motion\", but can\u2014according to Aristotelian metaphysics\u2014exhibit \"enforced motion\" imparted by an efficient cause. The form of plants endows plants with the processes nutrition and reproduction, the form of animals adds locomotion, and the form of humankind adds reason atop these. A rock normally exhibits \"natural motion\"\u2014explained by the rock's material cause of being composed of the element earth\u2014but a living thing can lift the rock, an \"enforced motion\" diverting the rock from its natural place and natural motion. As a further kind of explanation, Aristotle identified the final cause, specifying a purpose or criterion of completion in light of which something should be understood.\nAristotle himself explained,\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;\"Cause\" means\n(a) in one sense, that as the result of whose presence something comes into being\u2014e.g., the bronze of a statue and the silver of a cup, and the classes which contain these [i.e., the material cause];\n(b) in another sense, the form or pattern; that is, the essential formula and the classes which contain it\u2014e.g. the ratio 2:1 and number in general is the cause of the octave\u2014and the parts of the formula [i.e., the formal cause].\n(c) The source of the first beginning of change or rest; e.g. the man who plans is a cause, and the father is the cause of the child, and in general that which produces is the cause of that which is produced, and that which changes of that which is changed [i.e., the efficient cause].\n(d) The same as \"end\"; i.e. the final cause; e.g., as the \"end\" of walking is health. For why does a man walk? \"To be healthy\", we say, and by saying this we consider that we have supplied the cause [the final cause].\n(e) All those means towards the end which arise at the instigation of something else, as, e.g., fat-reducing, purging, drugs, and instruments are causes of health; for they all have the end as their object, although they differ from each other as being some instruments, others actions [i.e., necessary conditions].\nAristotle further discerned two modes of causation: proper (prior) causation and accidental (chance) causation. All causes, proper and accidental, can be spoken as potential or as actual, particular or generic. The same language refers to the effects of causes, so that generic effects are assigned to generic causes, particular effects to particular causes, and actual effects to operating causes.\nAverting infinite regress, Aristotle inferred the first mover\u2014an unmoved mover. The first mover's motion, too, must have been caused, but, being an unmoved mover, must have moved only toward a particular goal or desire.\nPyrrhonism.\nWhile the plausibility of causality was accepted in Pyrrhonism, it was equally accepted that it was plausible that nothing was the cause of anything.\nMiddle Ages.\nIn line with Aristotelian cosmology, Thomas Aquinas posed a hierarchy prioritizing Aristotle's four causes: \"final &gt; efficient &gt; material &gt; formal\". Aquinas sought to identify the first efficient cause\u2014now simply \"first cause\"\u2014as everyone would agree, said Aquinas, to call it \"God\". Later in the Middle Ages, many scholars conceded that the first cause was God, but explained that many earthly events occur within God's design or plan, and thereby scholars sought freedom to investigate the numerous \"secondary causes\".\nAfter the Middle Ages.\nFor Aristotelian philosophy before Aquinas, the word \"cause\" had a broad meaning. It meant 'answer to a why question' or 'explanation', and Aristotelian scholars recognized four kinds of such answers. With the end of the Middle Ages, in many philosophical usages, the meaning of the word 'cause' narrowed. It often lost that broad meaning, and was restricted to just one of the four kinds. For authors such as Niccol\u00f2 Machiavelli, in the field of political thinking, and Francis Bacon, concerning science more generally, Aristotle's moving cause was the focus of their interest. A widely used modern definition of causality in this newly narrowed sense was assumed by David Hume. He undertook an epistemological and metaphysical investigation of the notion of moving cause. He denied that we can ever perceive cause and effect, except by developing a habit or custom of mind where we come to associate two types of object or event, always contiguous and occurring one after the other. In Part III, section XV of his book \"A Treatise of Human Nature\", Hume expanded this to a list of eight ways of judging whether two things might be cause and effect. The first three:\nAnd then additionally there are three connected criteria which come from our experience and which are \"the source of most of our philosophical reasonings\": \nAnd then two more: \nIn 1949, physicist Max Born distinguished determination from causality. For him, determination meant that actual events are so linked by laws of nature that certainly reliable predictions and retrodictions can be made from sufficient present data about them. He describes two kinds of causation: nomic or generic causation and singular causation. Nomic causality means that cause and effect are linked by more or less certain or probabilistic general laws covering many possible or potential instances; this can be recognized as a probabilized version of Hume's criterion 3. An occasion of singular causation is a particular occurrence of a definite complex of events that are physically linked by antecedence and contiguity, which may be recognized as criteria 1 and 2.\nSee also.\n&lt;templatestyles src=\"Col-begin/styles.css\"/&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nFurther reading.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "37197", "revid": "28481209", "url": "https://en.wikipedia.org/wiki?curid=37197", "title": "Isotope separation", "text": "Concentrating specific isotopes of a chemical element\nIsotope separation is the process of concentrating specific isotopes of a chemical element by removing other isotopes. The use of the nuclides produced is varied. The largest variety is used in research (e.g. in chemistry where atoms of \"marker\" nuclide are used to figure out reaction mechanisms). By tonnage, separating natural uranium into enriched uranium and depleted uranium is the largest application. This process is crucial in the manufacture of uranium fuel for nuclear power plants and is also required for the creation of uranium-based nuclear weapons (unless uranium-233 is used). Plutonium-based weapons use plutonium produced in a nuclear reactor, which must be operated in such a way as to produce plutonium already of suitable isotopic mix or \"grade\".\nWhile chemical elements can be purified through chemical processes, isotopes of the same element have nearly identical chemical properties which makes this type of separation impractical, except for separation of deuterium.\nTechniques.\nThere are three types of isotope separation techniques:\nThe third type of separation is still experimental; practical separation techniques all depend in some way on the atomic mass. It is therefore generally easier to separate isotopes with a larger relative mass difference. For example, deuterium has twice the mass of ordinary (light) hydrogen and it is generally easier to purify it than to separate uranium-235 from the more common uranium-238. On the other extreme, separation of fissile plutonium-239 from the common impurity plutonium-240, while desirable in that it would allow the creation of gun-type fission weapons from plutonium, is generally agreed to be impractical.\nEnrichment cascades.\nAll large-scale isotope separation schemes employ a number of similar stages which produce successively higher concentrations of the desired isotope. Each stage enriches the product of the previous step further before being sent to the next stage. Similarly, the tailings from each stage are returned to the previous stage for further processing. This creates a sequential enriching system called a cascade. There are two important factors that characterize the performance of a cascade. The first is the separation factor, which is a number greater than 1. The second is the number of required stages to get the desired purity.\nCommercial materials.\nTo date, large-scale commercial isotope separation of only three elements has occurred. In each case, the rarer of the two most common isotopes of an element has been concentrated for use in nuclear technology:\nSome isotopically purified elements are used in smaller quantities for specialist applications, especially in the semiconductor industry, where purified silicon is used to improve crystal structure and thermal conductivity, and carbon with greater isotopic purity to make diamonds with greater thermal conductivity.\nIsotope separation is an important process for both peaceful and military nuclear technology, and therefore the capability that a nation has for isotope separation is of extreme interest to the intelligence community.\nAlternatives.\nThe only alternative to isotope separation is to manufacture the required isotope in its pure form. This may be done by irradiation of a suitable target, but care is needed in target selection and other factors to ensure that only the required isotope of the element of interest is produced. Isotopes of other elements are not so great a problem as they can be removed by chemical means.\nThis is particularly relevant in the preparation of high-grade plutonium-239 for use in weapons. It is not practical to separate Pu-239 from Pu-240 or Pu-241. Fissile Pu-239 is produced following neutron capture by uranium-238, but further neutron capture will produce Pu-240 which is less fissile and worse, is a fairly strong neutron emitter, and Pu-241 which decays to Am-241, a strong alpha emitter that poses self-heating and radiotoxicity problems. Therefore, the uranium targets used to produce military plutonium must be irradiated for only a short time, to minimise the production of these unwanted isotopes. Conversely, blending plutonium with Pu-240 renders it less suitable for nuclear weapons.\nIf the desired goal is not an atom bomb but running a nuclear power plant, the alternative to enrichment of uranium for use in a light-water reactor is the use of a neutron moderator with a lower neutron absorption cross section than protium. Options include heavy water as used in CANDU type reactors or graphite as used in magnox or RBMK reactors. Obtaining heavy water however also requires isotope separation, in this case of hydrogen isotopes, which is easier due to the bigger variation in atomic weight. Both magnox and RBMK reactors had undesirable properties when run with natural uranium, which ultimately led to the replacement of this fuel with low enriched uranium, negating the advantage of foregoing enrichment. Pressurized heavy-water reactors such as the CANDU are still in active use and India which has limited domestic uranium resources and been under a partial nuclear embargo ever since it became an atom bomb state in particular relies on heavy water moderated reactors for its nuclear power. A big downside of heavy water reactors is the enormous upfront cost of the heavy water.\nMethodology.\nDiffusion.\nOften done with gases, but also with liquids, the diffusion method relies on the fact that in thermal equilibrium, two isotopes with the same energy will have different average velocities. The lighter atoms (or the molecules containing them) will travel more quickly through a membrane, whose pore diameters are not larger than the mean free path length (Knudsen flow). The speed ratio is equal to the inverse square root of the mass ratio, so the amount of separation is small. For example for 235UF6 versus 238UF6 it is 1.0043. Hence many cascaded stages are needed to obtain high purity. This method is expensive due to the work needed to push gas through a membrane and the many stages necessary, each requiring recompression of the gas.\nThe first large-scale separation of uranium isotopes was achieved by the United States in large gaseous diffusion separation plants at Clinton Engineering Works, which were established as part of the Manhattan Project. These used uranium hexafluoride gas as the process fluid. Nickel powder and electro-deposited nickel mesh diffusion barriers were pioneered by Edward Adler and Edward Norris. Due to the high energy consumption, enrichment of uranium by diffusion was gradually replaced by more efficient methods. \nThe last diffusion plant closed in 2013. The Paducah Gaseous Diffusion Plant was a US government effort to generate highly enriched uranium to power military reactors and create nuclear bombs which led to the establishment of the facility in 1952. Paducah's enrichment was initially kept to low levels, and the facility operated as a \"feed facility\" for other defence facilities that processed the enriched uranium at Oak Ridge National Laboratory in Oak Ridge, Tennessee, and Portsmouth Gaseous Diffusion Plant in Piketon, Ohio. The goal of Paducah and its sister facility in Piketon was adjusted in the 1960s when they started to enrich uranium for use in commercial nuclear reactors to produce energy.\nCentrifugal.\nCentrifugal schemes rapidly rotate the material allowing the heavier isotopes to go closer to an outer radial wall. This is often done in gaseous form using a Zippe-type centrifuge. Centrifuging plasma can separate isotopes as well as separating ranges of elements for radioactive waste reduction, nuclear reprocessing, and other purposes. The process is called \"plasma mass separation\"; the devices are called \"plasma mass filter\" or \"plasma centrifuge\" (not to be confused with medical centrifuges).\nThe centrifugal separation of isotopes was first suggested by Aston and Lindemann in 1919 and the first successful experiments were reported by Beams and Haynes on isotopes of chlorine in 1936. However attempts to use the technology during the Manhattan Project were unproductive. In modern times it is the main method used throughout the world to enrich uranium and as a result remains a fairly secretive process, hindering a more widespread uptake of the technology. In general a feed of UF6 gas is connected to a cylinder that is rotated at high speed. Near the outer edge of the cylinder heavier gas molecules containing U-238 collect, while molecules containing U-235 concentrate at the centre and are then fed to another cascade stage. \nUse of gaseous centrifugal technology to enrich isotopes is desirable as power consumption is greatly reduced when compared to more conventional techniques such as diffusion plants since fewer cascade steps are required to reach similar degrees of separation. As well as requiring less energy to achieve the same separation, far smaller scale plants are possible, making them an economic possibility for a small nation attempting to produce a nuclear weapon. Pakistan is believed to have used this method in developing its nuclear weapons.\nVortex tubes were used by South Africa in their Helikon vortex separation process. The gas is injected tangentially into a chamber with special geometry that further increases its rotation to a very high rate, causing the isotopes to separate. The method is simple because vortex tubes have no moving parts, but energy intensive, about 50 times greater than gas centrifuges. A similar process, known as \"jet nozzle\" was created in Germany, with a demonstration plant built in Brazil, and they went as far as developing a site to fuel the country's nuclear plants.\nElectromagnetic.\nElectromagnetic separation is mass spectrometry on a large scale, so it is sometimes referred to as mass spectrometry. It uses the fact that charged particles are deflected in a magnetic field and the amount of deflection depends upon the particle's mass. It is very expensive for the quantity produced, as it has an extremely low throughput, but it can allow very high purities to be achieved. This method is often used for processing small amounts of pure isotopes for research or specific use (such as isotopic tracers) but is impractical for industrial use.\nAt Oak Ridge National Laboratory and at the University of California, Berkeley, Ernest O. Lawrence developed electromagnetic separation for much of the uranium used in the first atomic bombs. Devices using his principle are named calutrons. After the war the method was largely abandoned as impractical. It had only been undertaken (along with diffusion and other technologies) to guarantee there would be enough material for use, whatever the cost. Its main eventual contribution to the war effort was to further concentrate material from the gaseous diffusion plants to higher levels of purity.\nLaser.\nIn this method a laser is tuned to a wavelength which excites only one isotope of the material and ionizes those atoms preferentially. For atoms, the resonant absorption of light for an isotope depends on\nallowing finely tuned lasers to interact with only one isotope. After the atom is ionized it can be removed from the sample by applying an electric field. This method is often abbreviated as AVLIS (atomic vapor laser isotope separation). This method has only been developed as laser technology has improved in the 1970s to 1980s. Attempts to develop it to an industrial scale for uranium enrichment were successively given up in the 1990s \"due to never ending technical difficulties\" and because centrifuges have reached technical maturity in the meantime. However, it is a major concern to those in the field of nuclear proliferation, because it may be cheaper and more easily hidden than other methods of isotope separation. Tunable lasers used in AVLIS include the dye laser and more recently diode lasers.\nA second method of laser separation is known as molecular laser isotope separation (MLIS). In this method, an infrared laser is directed at uranium hexafluoride gas (if enrichment of uranium is desired), exciting molecules that contain a U-235 atom. A second laser, either also in the IR (infrared multiphoton dissociation) or in the UV, frees a fluorine atom, leaving uranium pentafluoride which then precipitates out of the gas. Cascading the MLIS stages is more difficult than with other methods because the UF5 must be fluorinated back to UF6 before being introduced into the next MLIS stage. But with light elements, the isotope selectivity is usually good enough that cascading is not required.\nSeveral alternative MLIS schemes have been developed. For example, one uses a first laser in the near-infrared or visible region, where a selectivity of over 20:1 can be obtained in a single stage. This method is called OP-IRMPD (Overtone Pre-excitation\u2014IR Multiple Photon Dissociation). But due to the small absorption probability in the overtones, too many photons remain unused, so that the method did not reach industrial feasibility. Also some other MLIS methods suffer from wasting of the expensive photons.\nFinally, the 'Separation of isotopes by laser excitation' (SILEX) process, developed by Silex Systems in Australia, has been licensed to General Electric for the development of a pilot enrichment plant. For uranium, it uses a cold molecular beam with UF6 in a carrier gas, in which the 235UF6 is selectively excited by an infrared laser near 16\u00a0\u03bcm. In contrast to the excited molecules, the nonexcited heavier isotopic molecules tends to form clusters with the carrier gas, and these clusters stay closer to the axis of the molecular beam, so that they can pass a skimmer and are thus separated from the excited lighter isotope.\nQuite recently yet another scheme has been proposed for the deuterium separation using Trojan wavepackets in circularly polarized electromagnetic field. The process of Trojan wave packet formation by the adiabatic-rapid passage depends in ultra-sensitive way on the reduced electron and nucleus mass which with the same field frequency further leads to excitation of Trojan or anti-Trojan wavepacket depending on the kind of the isotope. Those and their giant, rotating electric dipole moments are then formula_1-shifted in phase and the beam of such atoms splits in the gradient of the electric field in the analogy to Stern\u2013Gerlach experiment.\nChemical methods.\nAlthough isotopes of a single element are normally described as having the same chemical properties, this is not strictly true. In particular, reaction rates are very slightly affected by atomic mass.\nTechniques using this are most effective for light atoms such as hydrogen. Lighter isotopes tend to react or evaporate more quickly than heavy isotopes, allowing them to be separated. This is how heavy water is produced commercially, see Girdler sulfide process for details. Lighter isotopes also disassociate more rapidly under an electric field. This process in a large cascade was used at the heavy water production plant at Rjukan.\nOne candidate for the largest kinetic isotopic effect ever measured at room temperature, 305, may eventually be used for the separation of tritium (T). The effects for the oxidation of tritiated formate anions to HTO were measured as:\nDistillation.\nIsotopes of hydrogen, carbon, oxygen, and nitrogen can be enriched by distilling suitable light compounds over long columns. The separation factor is the ratio of vapor pressures of two isotopic molecules. In equilibrium such a separation results at each theoretical plate of the column and is multiplied by the same factor in the next step (at the next plate). Because the elementary separation factor is small, a large number of such plates is needed. This requires total column heights of 20 to 300 m.\nThe lower vapor pressure of the heavier molecule is due to its higher energy of vaporization, which in turn results from its lower energy of zero-point vibration in the intermolecular potential. As expected from formulas for vapor pressure, the ratio becomes more favorable at lower temperatures (lower pressures). The vapor pressure ratio for H2O to D2O is 1.055 at 50\u00a0\u00b0C (123 mbar) and 1.026 at 100\u00a0\u00b0C (1013 mbar). For 12CO to 13CO it is 1.007 near the normal boiling point (81.6\u00a0K), and 1.003 for 12CH4 to 13CH4 near 111.7\u00a0K (boiling point).\nThe 13C enrichment by (cryogenic) distillation was developed in the late 1960s by scientists at Los Alamos National Laboratory. It is still the preferred method for13C enrichment. Deuterium enrichment by water distillation is only done, if it was preenriched by a process (chemical exchange) with lower energy demand. Beginning with the low natural abundance (0.015% D) would require evaporation of too large quantities of water.\nSeparative work unit.\nSeparative work unit (SWU) is a complex unit which is a function of the amount of uranium processed and the degree to which it is enriched, \"i.e.\" the extent of increase in the concentration of the U-235 isotope relative to the remainder.\nThe unit is strictly: kilogram separative work unit, and it measures the quantity of separative work (indicative of energy used in enrichment) when feed and product quantities are expressed in kilograms. The effort expended in separating a mass \"F\" of feed of assay \"xf\" into a mass \"P\" of product assay xp and waste of mass \"W\" and assay \"xw\" is expressed in terms of the number of separative work units needed, given by the expression SWU = \"WV\"(\"xw\") + \"PV\"(\"xp\") - \"FV\"(\"xf\"), where \"V\"(\"x\") is the \"value function,\" defined as \"V\"(\"x\") = (1 - 2\"x\") ln ((1 - \"x\") /\"x\").\nSeparative work is expressed in SWUs, kg SW, or kg UTA (from the German \"Urantrennarbeit\" )\nIf, for example, for 100 kilograms (220 pounds) of natural uranium, it takes about 60 SWU to produce 10 kilograms (22 pounds) of uranium enriched in U-235 content to 4.5%.\nIsotope separators for research.\nRadioactive beams of specific isotopes are widely used in the fields of experimental physics, biology and materials science. The production and formation of these radioactive atoms into an ionic beam for study is an entire field of research carried out at many laboratories throughout the world. The first isotope separator was developed at the Copenhagen Cyclotron by Bohr and coworkers using the principle of electromagnetic separation. Today, there are many laboratories around the world that supply beams of radioactive ions for use. \nArguably the principal Isotope Separator On Line (ISOL) is ISOLDE at CERN, which is a joint European facility spread across the Franco-Swiss border near the city of Geneva. This laboratory uses mainly proton spallation of uranium carbide targets to produce a wide range of radioactive fission fragments that are not found naturally on earth. During spallation (bombardment with high energy protons), a uranium carbide target is heated to several thousand degrees so that radioactive atoms produced in the nuclear reaction are released. \nOnce out of the target, the vapour of radioactive atoms travels to an ionizer cavity. This ionizer cavity is a thin tube made of a refractory metal with a high work function allowing for collisions with the walls to liberate a single electron from a free atom (surface ionization effect). Once ionized, the radioactive species are accelerated by an electrostatic field and injected into an electromagnetic separator. As ions entering the separator are of approximately equal energy, those ions with a smaller mass will be deflected by the magnetic field by a greater amount than those with a heavier mass. This differing radius of curvature allows for isobaric purification to take place. \nOnce purified isobarically, the ion beam is then sent to the individual experiments. In order to increase the purity of the isobaric beam, laser ionization can take place inside the ionizer cavity to selectively ionize a single element chain of interest. At CERN, this device is called the Resonance Ionization Laser Ion Source (RILIS). Currently over 60% of all experiments opt to use the RILIS to increase the purity of radioactive beams.\nBeam production capability.\nAs the production of radioactive atoms by the ISOL technique depends on the free atom chemistry of the element to be studied, there are certain beams which cannot be produced by simple proton bombardment of thick actinide targets. Refractory metals such as tungsten and rhenium do not emerge from the target even at high temperatures due to their low vapour pressure. In order to produce these types of beams, a thin target is required. The Ion Guide Isotope Separator On Line (IGISOL) technique was developed in 1981 at the University of Jyv\u00e4skyl\u00e4 cyclotron laboratory in Finland. In this technique, a thin uranium target is bombarded with protons and nuclear reaction products recoil out of the target in a charged state. The recoils are stopped in a gas cell and then exit through a small hole in the side of the cell where they are accelerated electrostatically and injected into a mass separator. This method of production and extraction takes place on a shorter timescale compared to the standard ISOL technique and isotopes with short half-lives (sub millisecond) can be studied using an IGISOL. An IGISOL has also been combined with a laser ion source at the Leuven Isotope Separator On Line (LISOL) in Belgium. Thin target sources generally provide significantly lower quantities of radioactive ions than thick target sources and this is their main drawback.\nAs experimental nuclear physics progresses, it is becoming more and more important to study the most exotic of radioactive nuclei. In order to do so, more inventive techniques are required to create nuclei with extreme proton/neutron ratios. An alternative to the ISOL techniques described here is that of fragmentation beams, where the radioactive ions are produced by fragmentation reactions on a fast beam of stable ions impinging on a thin target (usually of beryllium atoms). This technique is used, for example, at the Facility for Rare Isotope Beams (FRIB) at Michigan State University and at the Radioactive Isotope Beam Factory (RIBF) at RIKEN, in Japan.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "37201", "revid": "27015025", "url": "https://en.wikipedia.org/wiki?curid=37201", "title": "Larry Gelbart", "text": "American comedy writer and playwright (1928\u20132009)\nLarry Simon Gelbart (February 25, 1928 \u2013 September 11, 2009) was an American television writer, playwright, screenwriter, director and author, most famous as a creator and producer of the television series \"M*A*S*H\", and as co-writer of the Broadway musicals \"A Funny Thing Happened on the Way to the Forum\" and \"City of Angels\".\nBiography.\nEarly life.\nGelbart was born in Chicago, Illinois, to Jewish immigrants Harry Gelbart, \"a barber since his half of a childhood in Latvia,\" and Frieda Sturner, from what is now D\u0105browa G\u00f3rnicza (Poland), who migrated to the United States. Larry Gelbart had a sister, Marcia Gelbart Walkenstein.\nHis family later moved to Los Angeles and he attended Fairfax High School. Drafted into the U.S. Army near the end of World War II, Gelbart worked for the Armed Forces Radio Service in Los Angeles. Attaining the rank of sergeant, Gelbart was honorably discharged after serving 1 year and 11 days. Those last 11 days prevented Gelbart from being drafted for service during the Korean War.\nTelevision.\nGelbart began as a writer at the age of sixteen for Danny Thomas's radio show after his father, who was Thomas's barber, showed Thomas some jokes Gelbart had written. During the 1940s Gelbart also wrote for Jack Paar and Bob Hope. In the 1950s, his most important work in television involved writing for Red Buttons, Sid Caesar on \"Caesar's Hour\", and in Celeste Holm's \"Honestly, Celeste!\", as well as with writers Mel Tolkin, Michael Stewart, Selma Diamond, Neil Simon, Mel Brooks, Carl Reiner and Woody Allen on two Caesar specials.\nIn 1972, Gelbart was one of the main forces behind the creation of the television series \"M*A*S*H\", writing the pilot (for which he received a \"Developed for Television by __\" credit); then producing, often writing and occasionally directing the series for its first four seasons, from 1972 to 1976. \"M*A*S*H\" earned Gelbart a Peabody Award and an Emmy for Outstanding Comedy Series and went on to considerable commercial and critical success.\nFilms.\nGelbart's best known screen work is perhaps the screenplay for 1982's \"Tootsie\", which he co-wrote with Murray Schisgal. He was nominated for an Academy Award for that script, and also was Oscar-nominated for his adapted screenplay for 1977's \"Oh, God!\" starring John Denver and George Burns. On his relationship with actor Dustin Hoffman in \"Tootsie\", Gelbart is reported to have said, \"Never work with an Oscar-winner who is shorter than the statue\". He later retracted this statement, saying it was just a joke.\nHe collaborated with Burt Shevelove on the screenplay for the 1966 British film \"The Wrong Box\". Gelbart also co-wrote the golden-era film spoof \"Movie Movie\" (1978) starring George C. Scott in dual roles, the racy comedy \"Blame It on Rio\" (1984) starring Michael Caine and the 2000 remake of \"Bedazzled\" with Elizabeth Hurley and Brendan Fraser. His script for \"Rough Cut\" (1980), a caper film starring Burt Reynolds, Lesley-Anne Down and David Niven, was credited under the pseudonym Francis Burns.\nGelbart-scripted films for television included \"Barbarians at the Gate\" (1993), a true story about the battle for control of the RJR Nabisco corporation starring James Garner that was based on the best-selling book of that name; the original comedy \"Weapons of Mass Distraction\" (1997) starring Ben Kingsley and Gabriel Byrne as rival media moguls; and \"And Starring Pancho Villa as Himself\" (2003) starring Antonio Banderas as the Mexican revolutionary leader.\nBroadway.\nGelbart co-wrote the long-running Broadway musical farce \"A Funny Thing Happened on the Way to the Forum\" with Burt Shevelove and Stephen Sondheim in 1962. After the show received poor reviews and box-office returns during its previews in Washington, D.C., rewrites and restaging helped; it was a smash Broadway hit and ran for 964 performances. Its book won a Tony Award. In a 1991 published edition of the musical, Gelbart wrote \"it remains for me the best piece of work I've been lucky enough to see my name on.\" A film version starring Zero Mostel and directed by Richard Lester, was released in 1966. Gelbart was critical of the movie, as most of his and Shevelove's libretto was largely rewritten.\nGelbart's other Broadway credits include the musical \"City of Angels\", which won him the Drama Desk Award for Outstanding Book of a Musical, the Tony Award for Best Book of a Musical, and an Edgar Award and an off-Broadway musical, \"In The Beginning\", a satirical take on the Bible, with music and lyrics by Maury Yeston. He also wrote the Iran-Contra satire \"Mastergate\", as well as \"Sly Fox\" and a musical adaptation of the Preston Sturges movie \"Hail the Conquering Hero\", whose grueling development inspired Gelbart to utter what evolved into the classic quip, \"If Hitler is alive, I hope he's out of town with a musical.\"\nMemoirs.\nIn 1997, Gelbart published his memoir, \"Laughing Matters: On Writing M*A*S*H, Tootsie, Oh, God! and a Few Other Funny Things\".\nBlogger.\nGelbart was a contributing blogger at \"The Huffington Post\", and also was a regular participant on the alt.tv.mash Usenet newsgroup as \"Elsig\". (His user name was derived from El for Larry, si for Simon and g for Gelbart.)\nHonors.\nIn 1995, a Golden Palm Star on the Palm Springs, California, Walk of Stars was dedicated to him.\nHe won a Tony Award for the book of \"A Funny Thing Happened on the Way to the Forum.\"\nHe won a Tony Award for the book of \"City of Angels\".\nHe won an Emmy Award for Outstanding Comedy Series in 1974 for \"M*A*S*H\".\nIn 2002, Gelbart was inducted into the American Theatre Hall of Fame.\nIn 2008, he was inducted into the Television Hall of Fame.\nDeath.\nGelbart was diagnosed with cancer in June and died at his Beverly Hills home on September 11, 2009, aged 81. His wife of 53 years, Pat Gelbart, said that after being married for so long, \"we finished each other's sentences.\" She declined to specify the type of cancer he had. He was interred at the Hillside Memorial Park Cemetery in Culver City, California.\nWriting credits.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\n\"M*A*S*H\" episodes.\nThe following is a list of \"M*A*S*H\" episodes (42 Total) written and/or directed by Gelbart.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "37206", "revid": "35498457", "url": "https://en.wikipedia.org/wiki?curid=37206", "title": "Robert Watson-Watt", "text": "Scottish physicist and pioneer of direction-finding and radar technology (1892\u20131973)\nSir Robert Alexander Watson-Watt (13 April 1892 \u2013 5 December 1973) was a British radio engineer and pioneer of radio direction finding and radar technology.\nWatt began his career in radio physics with a job at the Met Office, where he began looking for accurate ways to track thunderstorms using the radio waves given off by lightning. This led to the 1920s development of a system later known as high-frequency direction finding (HFDF or \"huff-duff\"). Although well publicized at the time, the system's enormous military potential was not developed until the late 1930s. Huff-duff allowed operators to determine the location of an enemy radio transmitter in seconds and it became a major part of the network of systems that helped defeat the threat of German U-boats during World War II. It is estimated that huff-duff was used in about a quarter of all attacks on U-boats.\nIn 1935, Watt was asked to comment on reports of a German death ray based on radio. Watt and his assistant Arnold Frederic Wilkins quickly determined it was not possible, but Wilkins suggested using radio signals to locate aircraft at long distances. This led to a February 1935 demonstration where signals from a BBC short-wave transmitter were bounced off a Handley Page Heyford aircraft. Watt led the development of a practical version of this device, which entered service in 1938 under the code name Chain Home. This system provided the vital advance information that helped the Royal Air Force in the Battle of Britain.\nAfter the success of his invention, Watson Watt was sent to the U.S. in 1941 to advise on air defence after Japan's attack on Pearl Harbor. He returned and continued to lead radar development for the War Office and Ministry of Supply. He was elected a Fellow of the Royal Society in 1941, was given a knighthood in 1942 and was awarded the US Medal for Merit in 1946.\nEarly years.\nWatson-Watt was born in Brechin, Angus, Scotland, on 13 April 1892. He claimed to be a descendant of James Watt, the famous engineer and inventor of the practical steam engine, but no evidence of any family relationship has been found. After attending Damacre Primary School and Brechin High School, he was accepted at University College, Dundee (then part of the University of St Andrews and which became Queen's College, Dundee in 1954 and then the University of Dundee in 1967). Watson-Watt had a successful time as a student, winning the Carnelley Prize for Chemistry and a class medal for Ordinary Natural Philosophy in 1910.\nHe graduated with a BSc in engineering in 1912, and was offered an assistantship by Professor William Peddie, the holder of the Chair of Physics at University College, Dundee from 1907 to 1942. It was Peddie who encouraged Watson-Watt to study radio, or \"wireless telegraphy\" as it was then known, and who took him through what was effectively a postgraduate class on the physics of radio frequency oscillators and wave propagation. At the start of the Great War Watson-Watt was working as an assistant in the college's Engineering Department.\nEarly experiments.\nIn 1916, Watson-Watt wanted a job with the War Office, but nothing obvious was available in communications. Instead, he joined the Meteorological Office, which was interested in his ideas on the use of radio for the detection of thunderstorms. Lightning gives off a radio signal as it ionizes the air, and his goal was to detect this signal to warn pilots of approaching thunderstorms. The signal occurs across a wide range of frequencies and could be easily detected and amplified by naval longwave sets. In fact, lightning was a major problem for communications at these common wavelengths.\nHis early experiments were successful in detecting the signal and he quickly proved to be able to do so at ranges up to 2,500\u00a0km (1500 miles). Location was determined by rotating a loop antenna to maximize (or minimise) the signal, thus \"pointing\" to the storm. The strikes were so fleeting that it was very difficult to turn the antenna in time to positively locate one. Instead, the operator would listen to many strikes and develop a rough average location.\nAt first, he worked at the Wireless Station of Air Ministry Meteorological Office in Aldershot, Hampshire. In 1924 when the War Department gave notice that they wished to reclaim their Aldershot site, he moved to Ditton Park near Slough, Buckinghamshire. The National Physical Laboratory (NPL) was already using this site and had two main devices that would prove pivotal to his work.\nThe first was an Adcock antenna, an arrangement of four masts that allowed the direction of a signal to be detected through phase differences. Using pairs of these antennas positioned at right angles, one could make a simultaneous measurement of the lightning's direction on two axes. Displaying the fleeting signals was a problem. This was solved by the second device, the WE-224 oscilloscope, recently acquired from Bell Labs. By feeding the signals from the two antennae into the X and Y channels of the oscilloscope, a single strike caused the appearance of a line on the display, indicating the direction of the strike. The scope's relatively \"slow\" phosphor only allowed the signal to be read long after the strike had occurred. Watt's new system was being used in 1926 and was the topic of an extensive paper by Watson-Watt and Herd.\nThe Met and NPL radio teams were amalgamated in 1927 to form the Radio Research Station with Watson-Watt as director. Continuing research throughout, the teams had become interested in the causes of \"static\" radio signals and found that much could be explained by distant signals located over the horizon being reflected off the upper atmosphere. This was the first direct indication of the reality of the Heaviside layer, proposed earlier, but at this time largely dismissed by engineers. To determine the altitude of the layer, Watt, Appleton and others developed the 'squegger' to develop a 'time base' display, which would cause the oscilloscope's dot to move smoothly across the display at very high speed. By timing the squegger so that the dot arrived at the far end of the display at the same time as expected signals reflected off the Heaviside layer, the altitude of the layer could be determined. This time-base circuit was key to the development of radar. After a further reorganization in 1933, Watt became Superintendent of the Radio Department of NPL in Teddington.\nRADAR.\nThe air defence problem.\nDuring the First World War, the Germans had used Zeppelins as long-range bombers over Britain and defences had struggled to counter the threat. Since that time, aircraft capabilities had improved considerably and the prospect of widespread aerial bombardment of civilian areas was causing the government anxiety. Heavy bombers were now able to approach at altitudes that anti-aircraft guns of the day were unable to reach. With enemy airfields across the English Channel potentially only 20 minutes' flying-time away, bombers would have dropped their bombs and be returning to base before any intercepting fighters could get to altitude. The only answer seemed to be to have standing patrols of fighters in the air, but with the limited cruising time of a fighter, this would require a huge air force. An alternative solution was urgently needed and, in 1934, the Air Ministry set up a committee, the CSSAD (Committee for the Scientific Survey of Air Defence), chaired by Sir Henry Tizard to find ways to improve air defence in the UK.\nRumours that Nazi Germany had developed a \"death ray\" that was capable of destroying towns, cities and people using radio waves, were given attention in January 1935 by Harry Wimperis, Director of Scientific Research at the Air Ministry. He asked Watson-Watt about the possibility of building their version of a death-ray, specifically to be used against aircraft. Watson-Watt quickly returned a calculation carried out by his young colleague, Arnold Wilkins, showing that such a device was impossible to construct, and fears of a Nazi version soon vanished. He also mentioned in the same report a suggestion that was originally made to him by Wilkins, who had recently heard of aircraft disturbing shortwave communications, that radio waves might be capable of detecting aircraft, \"Meanwhile, attention is being turned to the still difficult, but less unpromising, problem of radio detection and numerical considerations on the method of detection by reflected radio waves will be submitted when required\". Wilkins's idea, checked by Watt, was promptly presented by Tizard to the CSSAD on 28 January 1935.\nAircraft detection and location.\nOn 12 February 1935, Watson-Watt sent the secret memo of the proposed system to the Air Ministry, \"Detection and location of aircraft by radio methods\". Although not as exciting as a death-ray, the concept clearly had potential, but the Air Ministry, before giving funding, asked for a demonstration proving that radio waves could be reflected by an aircraft. This was ready by 26 February and consisted of two receiving antennae located about away from one of the BBC's shortwave broadcast stations at Daventry. The two antennae were phased such that signals travelling directly from the station cancelled themselves out, but signals arriving from other angles were admitted, thereby deflecting the trace on a CRT indicator (passive radar). Such was the secrecy of this test that only three people witnessed it: Watson-Watt, his colleague Arnold Wilkins, and a single member of the committee, A. P. Rowe. The demonstration was a success: on several occasions, the receiver showed a clear return signal from a Handley Page Heyford bomber flown around the site. Prime Minister Stanley Baldwin was kept quietly informed of radar progress. On 2 April 1935, Watson-Watt received a patent on a radio device for detecting and locating an aircraft.\nIn mid-May 1935, Wilkins left the Radio Research Station with a small party, including Edward George Bowen, to start further research at Orford Ness, an isolated peninsula on the Suffolk coast of the North Sea. By June, they were detecting aircraft at a distance of , which was enough for scientists and engineers to stop all work on competing sound-based detection systems. By the end of the year, the range was up to , at which point, plans were made in December to set up five stations covering the approaches to London.\nOne of these stations was to be located on the coast near Orford Ness, and Bawdsey Manor was selected to become the main centre for all radar research. To put a radar defence in place as quickly as possible, Watson-Watt and his team created devices using existing components, rather than creating new components for the project, and the team did not take additional time to refine and improve the devices. So long as the prototype radars were in workable condition, they were put into production. They conducted \"full scale\" tests of a fixed radar radio tower system, attempting to detect an incoming bomber by radio signals for interception by a fighter. The tests were a complete failure, with the fighter only seeing the bomber after it had passed its target. The problem was not the radar but the flow of information from trackers from the Observer Corps to the fighters, which took many steps and was very slow. Henry Tizard, Patrick Blackett, and Hugh Dowding immediately set to work on this problem, designing a 'command and control air defence reporting system' with several layers of reporting that were eventually sent to a single large room for mapping. Observers watching the maps would then tell the fighters what to do via direct communications.\nBy 1937, the first three stations were ready, and the associated system was put to the test. The results were encouraging, and the government immediately commissioned construction of 17 additional stations. This became Chain Home, the array of fixed radar towers on the east and south coasts of England. By the start of World War II, 19 were ready for the Battle of Britain, and by the end of the war, over 50 had been built. The Germans were aware of the construction of Chain Home but were not sure of its purpose. They tested their theories with a flight of the Zeppelin LZ 130 but concluded the stations were a new long-range naval communications system.\nAs early as 1936, it was realized that the \"Luftwaffe\" would turn to night bombing if the day campaign did not go well. Watson-Watt had put another of the staff from the Radio Research Station, Edward Bowen, in charge of developing a radar that could be carried by a fighter. Night-time visual detection of a bomber was good to about 300\u00a0yards (metres) and the existing Chain Home systems simply did not have the accuracy needed to get the fighters that close. Bowen decided that an airborne radar should not exceed 90 kg (200 lb) in weight or 8\u00a0ft\u00b3 (230 L) in volume and should require no more than 500 watts of power. To reduce the drag of the antennae, the operating wavelength could not be much greater than one metre, difficult for the day's electronics. However, aircraft interception (AI) radar was perfected by 1940 and was instrumental in eventually ending The Blitz of 1941. Watson-Watt justified his choice of a non-optimal frequency for his radar, with his oft-quoted \"cult of the imperfect\", which he stated as \"Give them the third-best to go on with; the second-best comes too late; the best never comes\".\nCivil Service trade union activities.\nBetween 1934 and 1936, Watson-Watt was president of the Institution of Professional Civil Servants, now a part of Prospect, the \"union for professionals\". The union speculates that at this time he was involved in campaigning for an improvement in pay for Air Ministry staff.\nContribution to Second World War.\nIn his \"English History 1914\u20131945\", the historian A. J. P. Taylor paid the highest of praise to Watson-Watt, Sir Henry Tizard and their associates who developed radar, crediting them with being fundamental to victory in the Second World War.\nIn July 1938, Watson-Watt left Bawdsey Manor and took up the post of Director of Communications Development (DCD-RAE). In 1939, Sir George Lee took over the job of DCD and Watson-Watt became Scientific Advisor on Telecommunications (SAT) to the Ministry of Aircraft Production, travelling to the US in 1941 to advise them on the severe inadequacies of their air defence, illustrated by the Pearl Harbor attack. He was knighted by George VI in 1942 and received the US Medal for Merit in 1946.\nTen years after his knighthood, Watson-Watt was awarded \u00a350,000 by the UK government for his contributions in the development of radar. He established a practice as a consulting engineer. In the 1950s, he moved to Canada and later he lived in the US, where he published \"Three Steps to Victory\" in 1958. Around 1958, he appeared as a mystery challenger on the American television programme \"To Tell The Truth\". In 1956, Watson-Watt reportedly was pulled over for speeding in Canada by a radar gun-toting policeman. His remark was, \"Had I known what you were going to do with it I would never have invented it!\". He wrote an ironic poem (\"A Rough Justice\") afterwards,\n&lt;poem&gt;\nPity Sir Robert Watson-Watt,\nstrange target of this radar plot\nAnd thus, with others I can mention,\nthe victim of his own invention.\nHis magical all-seeing eye\nenabled cloud-bound planes to fly\nbut now by some ironic twist\nit spots the speeding motorist\nand bites, no doubt with legal wit,\nthe hand that once created it.\n...&lt;/poem&gt;\nLegacy.\nOn 3 September 2014, a statue of Sir Robert Watson-Watt was unveiled in Brechin by the Princess Royal. One day later, the BBC Two drama \"Castles in the Sky\", aired with Eddie Izzard in the role of Watson Watt.\nA collection of some of the correspondence and papers of Watson-Watt is held by the National Library of Scotland. A collection of papers relating to Watson-Watt is also held by Archive Services at the University of Dundee.\nA briefing facility at RAF Boulmer has been named the Watson-Watt auditorium in his honour.\nBusiness and financial life.\nWatson-Watt had a problematic business and financial life.\nFamily life.\nRobert Watson-Watt was married three times. He had a daughter with his second wife.\nWatson-Watt was married on 20 July 1916 in Hammersmith, London to Margaret Robertson (d.1988), the daughter of a draughtsman. In 1952 they divorced and he remarried. His second wife was a Canadian widow, Jean Wilkinson, who later died in 1964. He returned to Scotland in the 1960s, after the closure of his Canadian engineering business. \nIn 1966, at the age of 74, he married Dame Katherine Trefusis Forbes. She was 67 years old at the time and had also played a significant role in the Battle of Britain as the founding Air Commander of the Women's Auxiliary Air Force, which supplied the radar-room operatives. The couple lived in London during the winter, and at \"The Observatory\", Trefusis Forbes' summer home, in Pitlochry, Perthshire, during the warmer months.\nWatson-Watt died in 1973, aged 81 in Inverness, two years after his third wife. They are buried together in the churchyard of the Episcopal Church of the Holy Trinity in Pitlochry, Scotland.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "37207", "revid": "20468248", "url": "https://en.wikipedia.org/wiki?curid=37207", "title": "Nuclear engineering", "text": "Applied science\nNuclear engineering is the engineering discipline concerned with designing and applying systems that utilize the energy released by nuclear processes.\nThe most prominent application of nuclear engineering is the generation of electricity. Worldwide, some 439 nuclear reactors in 31 countries generate 10 percent of the world's energy through nuclear fission. In the future, it is expected that nuclear fusion will add another nuclear means of generating energy. Both reactions make use of the nuclear binding energy released when atomic nucleons are either separated (fission) or brought together (fusion). The energy available is given by the binding energy curve, and the amount generated is much greater than that generated through chemical reactions. Fission of 1 gram of uranium yields as much energy as burning 3 tons of coal or 600 gallons of fuel oil, without adding carbon dioxide to the atmosphere.\nHistory.\nNuclear engineering was born in 1938, with the discovery of nuclear fission. The first \"artificial\" nuclear reactor, CP-1, was designed by a team of physicists who were concerned that Nazi Germany might also be seeking to build a bomb based on nuclear fission. (The earliest known nuclear reaction on Earth occurred naturally, 1.7 billion years ago, in Oklo, Gabon, Africa.) The second artificial nuclear reactor, the X-10 Graphite Reactor, was also a part of the Manhattan Project, as were the plutonium-producing reactors of the Hanford Engineer Works. \nThe first nuclear reactor to generate electricity was Experimental Breeder Reactor I (EBR-I), which did so near Arco, Idaho, in 1951. EBR-I was a standalone facility, not connected to a grid, but a later Idaho research reactor in the BORAX series did briefly supply power to the town of Arco in 1955.\nThe first commercial nuclear power plant, built to be connected to an electrical grid, is the Obninsk Nuclear Power Plant, which began operation in 1954. The second is the Shippingport Atomic Power Station, which produced electricity in 1957.\nFor a chronology, from the discovery of uranium to the current era, see https:// or History of Nuclear Power. Also see https://, https://, and https://. \nSee List of Commercial Nuclear Reactors for a comprehensive listing of nuclear power reactors and https:// for worldwide and country-level statistics on nuclear power generation.\nSub-disciplines.\nNuclear engineers work in such areas as the following:\nMany chemical, electrical and mechanical and other types of engineers also work in the nuclear industry, as do many scientists and support staff. In the U.S., nearly 100,000 people directly work in the nuclear industry. Including secondary sector jobs, the number of people supported by the U.S. nuclear industry is 475,000.\nEmployment.\nIn the United States, nuclear engineers are employed as follows:\nJob prospects for nuclear engineers worldwide are not available, but the IAEA estimates that nuclear energy capacity will grow by 40% (an additional 514 GW(e) ) to 2.5 times current capacity (an additional 950 GW(e)) by 2050. Countries with existing nuclear energy capacity and those actively exploring nuclear energy are listed in the following.\nEducation.\nOrganizations that provide study and training in nuclear engineering include the following:\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "37208", "revid": "50500138", "url": "https://en.wikipedia.org/wiki?curid=37208", "title": "Landslide", "text": "Natural hazard involving ground movement\nLandslides, also known as landslips, rockslips or rockslides, are several forms of mass wasting that may include a wide range of ground movements, such as rockfalls, mudflows, shallow or deep-seated slope failures and debris flows. Landslides occur in a variety of environments, characterized by either steep or gentle slope gradients, from mountain ranges to coastal cliffs or even underwater, in which case they are called submarine landslides.\nGravity is the primary driving force for a landslide to occur, but there are other factors affecting slope stability that produce specific conditions that make a slope prone to failure. In many cases, the landslide is triggered by a specific event (such as heavy rainfall, an earthquake, a slope cut to build a road, and many others), although this is not always identifiable.\nLandslides are frequently made worse by human development (such as urban sprawl) and resource exploitation (such as mining and deforestation). Land degradation frequently leads to less stabilization of soil by vegetation. Additionally, global warming caused by climate change and other human impact on the environment, can increase the frequency of natural events (such as extreme weather) which trigger landslides. Landslide mitigation describes the policy and practices for reducing the risk of human impacts of landslides, reducing the risk of natural disaster.\nCauses.\nLandslides occur when the slope (or a portion of it) undergoes some processes that change its condition from stable to unstable. This is essentially due to a decrease in the shear strength of the slope material, an increase in the shear stress borne by the material, or a combination of the two. A change in the stability of a slope can be caused by a number of factors, acting together or alone. Natural causes of landslides include:\nLandslides are aggravated by human activities, such as:\nTypes.\nHungr-Leroueil-Picarelli classification.\nIn traditional usage, the term landslide has at one time or another been used to cover almost all forms of mass movement of rocks and regolith at the Earth's surface. In 1978, geologist David Varnes noted this imprecise usage and proposed a new, much tighter scheme for the classification of mass movements and subsidence processes. This scheme was later modified by Cruden and Varnes in 1996, and refined by Hutchinson (1988), Hungr et al. (2001), and finally by Hungr, Leroueil and Picarelli (2014). The classification resulting from the latest update is provided below.\nUnder this classification, six types of movement are recognized. Each type can be seen both in rock and in soil. A fall is a movement of isolated blocks or chunks of soil in free-fall. The term topple refers to blocks coming away by rotation from a vertical face. A slide is the movement of a body of material that generally remains intact while moving over one or several inclined surfaces or thin layers of material (also called shear zones) in which large deformations are concentrated. Slides are also sub-classified by the form of the surface(s) or shear zone(s) on which movement happens. The planes may be broadly parallel to the surface (\"planar slides\") or spoon-shaped (\"rotational slides\"). Slides can occur catastrophically, but movement on the surface can also be gradual and progressive. Spreads are a form of subsidence, in which a layer of material cracks, opens up, and expands laterally. Flows are the movement of fluidised material, which can be both dry or rich in water (such as in mud flows). Flows can move imperceptibly for years, or accelerate rapidly and cause disasters. Slope deformations are slow, distributed movements that can affect entire mountain slopes or portions of it. Some landslides are complex in the sense that they feature different movement types in different portions of the moving body, or they evolve from one movement type to another over time. For example, a landslide can initiate as a rock fall or topple and then, as the blocks disintegrate upon the impact, transform into a debris slide or flow. An avalanching effect can also be present, in which the moving mass entrains additional material along its path.\nFlows.\nSlope material that becomes saturated with water may produce a debris flow or mud flow. However, also dry debris can exhibit flow-like movement. Flowing debris or mud may pick up trees, houses and cars, and block bridges and rivers causing flooding along its path. This phenomenon is particularly hazardous in alpine areas, where narrow gorges and steep valleys are conducive of faster flows. Debris and mud flows may initiate on the slopes or result from the fluidization of landslide material as it gains speed or incorporates further debris and water along its path. River blockages as the flow reaches a main stream can generate temporary dams. As the impoundments fail, a domino effect may be created, with a remarkable growth in the volume of the flowing mass, and in its destructive power.\nAn earthflow is the downslope movement of mostly fine-grained material. Earthflows can move at speeds within a very wide range, from as low as 1\u00a0mm/yr to many km/h. Though these are a lot like mudflows, overall they are more slow-moving and are covered with solid material carried along by the flow from within. Clay, fine sand and silt, and fine-grained, pyroclastic material are all susceptible to earthflows. These flows are usually controlled by the pore water pressures within the mass, which should be high enough to produce a low shearing resistance. On the slopes, some earthflow may be recognized by their elongated shape, with one or more lobes at their toes. As these lobes spread out, drainage of the mass increases and the margins dry out, lowering the overall velocity of the flow. This process also causes the flow to thicken. Earthflows occur more often during periods of high precipitation, which saturates the ground and builds up water pressures. However, earthflows that keep advancing also during dry seasons are not uncommon. Fissures may develop during the movement of clayey materials, which facilitate the intrusion of water into the moving mass and produce faster responses to precipitation.\nA rock avalanche, sometimes referred to as sturzstrom, is a large and fast-moving landslide of the flow type. It is rarer than other types of landslides but it is often very destructive. It exhibits typically a long runout, flowing very far over a low-angle, flat, or even slightly uphill terrain. The mechanisms favoring the long runout can be different, but they typically result in the weakening of the sliding mass as the speed increases. The causes of this weakening are not completely understood. Especially for the largest landslides, it may involve the very quick heating of the shear zone due to friction, which may even cause the water that is present to vaporize and build up a large pressure, producing a sort of hovercraft effect. In some cases, the very high temperature may even cause some of the minerals to melt. During the movement, the rock in the shear zone may also be finely ground, producing a nanometer-size mineral powder that may act as a lubricant, reducing the resistance to motion and promoting larger speeds and longer runouts. The weakening mechanisms in large rock avalanches are similar to those occurring in seismic faults.\nSlides.\nSlides can occur in any rock or soil material and are characterized by the movement of a mass over a planar or curvilinear surface or shear zone.\nA debris slide is a type of slide characterized by the chaotic movement of material mixed with water and/or ice. It is usually triggered by the saturation of thickly vegetated slopes which results in an incoherent mixture of broken timber, smaller vegetation and other debris. Debris flows and avalanches differ from debris slides because their movement is fluid-like and generally much more rapid. This is usually a result of lower shear resistances and steeper slopes. Typically, debris slides start with the detachment of large rock fragments high on the slopes, which break apart as they descend.\nClay and silt slides are usually slow but can experience episodic acceleration in response to heavy rainfall or rapid snowmelt. They are often seen on gentle slopes and move over planar surfaces, such as over the underlying bedrock. Failure surfaces can also form within the clay or silt layer itself, and they usually have concave shapes, resulting in rotational slides.\nShallow and deep-seated landslides.\nSlope failure mechanisms often contain large uncertainties and could be significantly affected by heterogeneity of soil properties. A landslide in which the sliding surface is located within the soil mantle or weathered bedrock (typically to a depth from few decimeters to some meters) is called a shallow landslide. Debris slides and debris flows are usually shallow. Shallow landslides can often happen in areas that have slopes with high permeable soils on top of low permeable soils. The low permeable soil traps the water in the shallower soil generating high water pressures. As the top soil is filled with water, it can become unstable and slide downslope.\nDeep-seated landslides are those in which the sliding surface is mostly deeply located, for instance well below the maximum rooting depth of trees. They usually involve deep regolith, weathered rock, and/or bedrock and include large slope failures associated with translational, rotational, or complex movements. They tend to form along a plane of weakness such as a fault or bedding plane. They can be visually identified by concave scarps at the top and steep areas at the toe. Deep-seated landslides also shape landscapes over geological timescales and produce sediment that strongly alters the course of fluvial streams.\nRelated phenomena.\nResulting tsunamis.\nLandslides that occur undersea, or have impact into water e.g. significant rockfall or volcanic collapse into the sea, can generate tsunamis. Massive landslides can also generate megatsunamis, which are usually hundreds of meters high. In 1958, one such tsunami occurred in Lituya Bay in Alaska.\nLandslide prediction mapping.\nLandslide hazard analysis and mapping can provide useful information for catastrophic loss reduction, and assist in the development of guidelines for sustainable land-use planning. The analysis is used to identify the factors that are related to landslides, estimate the relative contribution of factors causing slope failures, establish a relation between the factors and landslides, and to predict the landslide hazard in the future based on such a relationship. The factors that have been used for landslide hazard analysis can usually be grouped into geomorphology, geology, land use/land cover, and hydrogeology. Since many factors are considered for landslide hazard mapping, geographic information system (GIS) is an appropriate tool because it has functions of collection, storage, manipulation, display, and analysis of large amounts of spatially referenced data which can be handled fast and effectively. Cardenas reported evidence on the exhaustive use of GIS in conjunction of uncertainty modelling tools for landslide mapping. Remote sensing techniques are also highly employed for landslide hazard assessment and analysis. Before and after aerial photographs and satellite imagery are used to gather landslide characteristics, like distribution and classification, and factors like slope, lithology, and land use/land cover to be used to help predict future events. Before and after imagery also helps to reveal how the landscape changed after an event, what may have triggered the landslide, and shows the process of regeneration and recovery.\nUsing satellite imagery in combination with GIS and on-the-ground studies, it is possible to generate maps of likely occurrences of future landslides. Such maps should show the locations of previous events as well as clearly indicate the probable locations of future events. In general, to predict landslides, one must assume that their occurrence is determined by certain geologic factors, and that future landslides will occur under the same conditions as past events. Therefore, it is necessary to establish a relationship between the geomorphologic conditions in which the past events took place and the expected future conditions.\nNatural disasters are a dramatic example of people living in conflict with the environment. Early predictions and warnings are essential for the reduction of property damage and loss of life. Because landslides occur frequently and can represent some of the most destructive forces on earth, it is imperative to have a good understanding as to what causes them and how people can either help prevent them from occurring or simply avoid them when they do occur. Sustainable land management and development is also an essential key to reducing the negative impacts felt by landslides.\nGIS offers a superior method for landslide analysis because it allows one to capture, store, manipulate, analyze, and display large amounts of data quickly and effectively. Because so many variables are involved, it is important to be able to overlay the many layers of data to develop a full and accurate portrayal of what is taking place on the Earth's surface. Researchers need to know which variables are the most important factors that trigger landslides in any given location. Using GIS, extremely detailed maps can be generated to show past events and likely future events which have the potential to save lives, property, and money.\nSince the '90s, GIS have been also successfully used in conjunction to decision support systems, to show on a map real-time risk evaluations based on monitoring data gathered in the area of the Val Pola disaster (Italy).\nExtraterrestrial landslides.\nEvidence of past landslides has been detected on many bodies in the Solar System, but since most observations are made by probes that only observe for a limited time and most bodies in the Solar System appear to be geologically inactive not many landslides are known to have happened in recent times. Both Venus and Mars have been subject to long-term mapping by orbiting satellites, and examples of landslides have been observed on both planets.\nLandslide monitoring.\nThe monitoring of landslides is essential for estimating the dangerous situations, making it possible to issue alerts on time, to avoid loses of lives and property, and to have proper planning and reducing measures in place. Currently, there exist different type of techniques aimed to monitor landslides:\nSeismic techniques.\n\u2022Geophones and accelerometers: Detect seismic vibrations or movements that might indicate slope instability.\nClimate-change impact on landslides.\nClimate-change impact on temperature, both average rainfall and rainfall extremes, and evapotranspiration may affect landslide distribution, frequency and intensity. However, this impact shows strong variability in different areas. Therefore, the effects of climate change on landslides need to be studied on a regional scale. Climate change can have both positive and negative impacts on landslides. Temperature rise may increase evapotranspiration, leading to a reduction in soil moisture and stimulate vegetation growth, also due to a CO2 increase in the atmosphere. Both effects may reduce landslides in some conditions. On the other side, temperature rise causes an increase of landslides due to\nSince the average precipitation is expected to decrease or increase regionally, rainfall induced landslides may change accordingly, due to changes in infiltration, groundwater levels and river bank erosion. Weather extremes are expected to increase due to climate change including heavy precipitation. This yields negative effects on landslides due to focused infiltration in soil and rock and an increase of runoff events, which may trigger debris flows.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "37210", "revid": "11292982", "url": "https://en.wikipedia.org/wiki?curid=37210", "title": "Shichi Narabe", "text": ""}
{"id": "37211", "revid": "2387872", "url": "https://en.wikipedia.org/wiki?curid=37211", "title": "William I of the Netherlands", "text": "King of the Netherlands from 1815 to 1840\nWilliam I (Willem Frederik; 24 August 1772 \u2013 12 December 1843) was King of the Netherlands and Grand Duke of Luxembourg from 1815 until his abdication in 1840. \nBorn as the son of William V, Prince of Orange, the last stadtholder of the Dutch Republic, and Wilhelmina of Prussia, William experienced significant political upheavals early in life. He fought against the French invasion during the Flanders campaign, and after the Batavian Revolution in 1795, his family went into exile. He briefly ruled the Principality of Nassau-Orange-Fulda before Napoleon's French troops' occupation forced him out of power. Following the defeat of Napoleon in 1814, William was invited back to the Netherlands, where he proclaimed himself Sovereign Prince of the United Netherlands. \nIn 1815, William raised the Netherlands to a kingdom and concurrently became the grand duke of Luxembourg. His reign saw the adoption of a new constitution, which granted him extensive powers. He was a strong proponent of economic development, founding several universities and promoting trade. However, his efforts to impose the Reformed faith and the Dutch language in the mostly Catholic and partly French-speaking southern provinces, combined with economic grievances, sparked the Belgian Revolution in 1830. Unable to suppress the rebellion, William ultimately accepted Belgian independence in 1839 under the Treaty of London.\nWilliam's later years were marked by dissatisfaction with constitutional changes and personal reasons, leading to his abdication in 1840 in favor of his son, King William II. He spent his final years in Berlin, where he died in 1843.\nLife.\nWilliam was the son of William V, Prince of Orange, the last stadtholder of the Dutch Republic, and Wilhelmina of Prussia. During the Flanders campaign, he commanded the Dutch troops and fought against the French invasion. The family went into exile in London in 1795 following the Batavian Revolution. As compensation for the loss of his father's possessions in the Low Countries, William was appointed ruler of the newly created Principality of Nassau-Orange-Fulda in 1803. When Napoleon invaded Germany in 1806, William fought on the Prussian side and was deposed upon French victory. With the death of his father in 1806, he became Prince of Orange and ruler of the Principality of Orange-Nassau, which he also lost the same year after the dissolution of the Holy Roman Empire and subsequent creation of the Confederation of the Rhine. He spent the following years in exile in Prussia. In 1813, following Napoleon's defeat at Leipzig, the Orange-Nassau territories were restored to William; he also accepted the offer to become Sovereign Prince of the United Netherlands.\nWilliam proclaimed himself king of the Netherlands in 1815. In the same year, he concluded a treaty with King Frederick William III in which he ceded the Orange-Nassau to Prussia in exchange for becoming the new grand duke of Luxembourg. As king, he adopted a new constitution, presided over strong economic and industrial progress, promoted trade and founded the universities of Leuven, Ghent and Li\u00e8ge. The imposition of the Reformed faith and the Dutch language, as well as feelings of economic inequity, caused widespread resentment in the southern provinces and led to the outbreak of the Belgian Revolution in 1830. William failed to crush the rebellion and in 1839 he accepted the independence of Belgium in accordance with the Treaty of London.\nWilliam's disapproval of changes to the constitution, the loss of Belgium and his intention to marry Henrietta d'Oultremont, a Roman Catholic, led to his decision to abdicate in 1840. His eldest son acceded to the throne as King William II. William died in 1843 in Berlin at the age of 71.\nPrince of Orange.\nKing William I's parents were the last stadtholder William V, Prince of Orange of the Dutch Republic, and his wife Wilhelmina of Prussia. Until 1806, William was formally known as William VI, Prince of Orange-Nassau, and between 1806 and 1813 also as Prince of Orange. In Berlin on 1 October 1791, William married his maternal first cousin (Frederica Louisa) Wilhelmina of Prussia, born in Potsdam. She was the daughter of King Frederick William II of Prussia. After Wilhelmina died in 1837, William married Countess Henrietta d'Oultremont (28 February 1792, in Maastricht \u2013 26 October 1864, in ), created countess of Nassau, on 17 February 1841, also in Berlin.\nYouth and early military career.\nAs eldest son of the William V, Prince of Orange, William was informally referred to as \"Erfprins\" (Hereditary Prince) by contemporaries from his birth until the death of his father in 1806 to distinguish him from William V.\nLike his younger brother Prince Frederick of Orange-Nassau he was tutored by the Swiss mathematician Leonhard Euler and the Dutch historian Herman Tollius. They were both tutored in the military arts by General Frederick Stamford. After the Patriot revolt had been suppressed in 1787, he in 1788\u201389 attended the military academy in Brunswick which was considered an excellent military school, together with his brother. In 1790 he visited a number of foreign courts like the one in Nassau and the Prussian capital Berlin, where he first met his future wife.\nWilliam subsequently studied briefly at the University of Leiden. In 1790 he was appointed a general of infantry in the Dutch States Army of which his father was Captain general, and he was made a member of the Council of State of the Netherlands. In November 1791 he took his new bride to The Hague.\nAfter the National Convention of the French Republic had declared war on the Dutch Republic in February 1793, William was appointed commander-in-chief of the \"veldleger\" (mobile army) of the States Army (his father remained the nominal head of the armed forces). As such he commanded the troops that took part in the Flanders Campaign of 1793\u201395. He took part in the Battles of Veurne and Menin (where his brother was wounded) in 1793, and commanded during the Siege of Landrecies (1794), whose fortress surrendered to him. In May 1794 he had replaced general Kaunitz as commander of the combined Austro-Dutch forces on the instigation of Emperor Francis II who apparently had a high opinion of him. William was victorious at the Battles of Gosselies and Lambusart and proved to be an able commander, but the French armies ultimately proved too strong, and the general allied leadership too inept. Despite a well-executed attack by William on the French left, the allied army under Coburg was finally defeated at the Battle of Fleurus. The French first entered Dutch Brabant which they dominated after the Battle of Boxtel. When in the winter of 1794\u201395 the rivers in the Rhine delta froze over, the French breached the southern Hollandic Water Line and the situation became militarily untenable. In many places Dutch revolutionaries took over the local government. After the Batavian Revolution in Amsterdam on 18 January 1795 the stadtholder decided to flee to England, and his sons accompanied him. (On this last day in Holland his father relieved William honorably of his commands). The next day the Batavian Republic was proclaimed.\nExile.\nSoon after the departure to England the hereditary prince went back to the continent, where his brother was assembling former members of the States Army in Osnabr\u00fcck for a planned invasion into the Batavian Republic in the summer of 1795. However, the neutral Prussian government forbade this.\nIn 1799, William landed in modern-day North Holland as part of the Anglo-Russian invasion of Holland. He was instrumental in fomenting a mutiny among the crews of a Batavian Navy squadron under Rear-Admiral Samuel Story, resulting in the squadron surrendering without a fight to the Royal Navy, which accepted the surrender in the name of the stadtholder. Not all the local Dutch population, however, was pleased with the arrival of the prince. One local Orangist was even executed. The hoped-for popular uprising failed to materialise. After several minor battles the hereditary prince was forced to leave the country again after the Convention of Alkmaar. The mutineers of the Batavian fleet, with their ships, and a large number of deserters from the Batavian army accompanied the retreating British troops to Britain. There William formed the King's Dutch Brigade with these troops, a military unit in British service, that swore oaths of allegiance to the British king, but also to the States General, defunct since 1795, \"whenever those would be reconstituted.\" This brigade trained on the Isle of Wight in 1800 and was stationed in Ireland for a time.\nWhen peace was concluded between Britain and the French Republic under First Consul Napoleon Bonaparte the Orange exiles were at their nadir. The Dutch Brigade was dissolved on 12 July 1802. Many members of the brigade went home to the Batavian Republic, thanks to an amnesty. The surrendered ships of the Batavian Navy were not returned, due to an agreement between the stadtholder and the British government of 11 March 1800. Instead the stadtholder was allowed to sell them to the Royal Navy for an appreciable sum.\nThe stadtholder, feeling discontented with the British, left for Germany. The hereditary prince, having a more flexible mind, went to visit Napoleon at St. Cloud in 1802. He apparently charmed the First Consul, and was charmed by him. Napoleon raised hopes for William that he might have an important role in a reformed Batavian Republic. Meanwhile, William's brother-in-law Frederick William III of Prussia, neutral at the time, promoted a Franco-Prussian convention of 23 May 1802, in addition to the Treaty of Amiens, that gave the House of Orange a few abbatial domains in Germany, that were combined to the Principality of Nassau-Orange-Fulda by way of indemnification for its losses in the Batavian Republic. The stadtholder gave this principality immediately to his son.\nWhen war broke out between the French Empire and Prussia in 1806, William supported his Prussian relatives, though he was nominally a French vassal. He received command of a Prussian division which took part in the Battle of Jena\u2013Auerstedt. The Prussians lost that battle and William was forced to surrender his troops rather ignominiously at Erfurt the day after the battle. He was made a prisoner of war, but was paroled soon. Napoleon punished him for his betrayal, however, by taking away his principality. As a parolee, William was not allowed to take part in the hostilities anymore. After the Peace of Tilsit William received a pension from France in compensation.\nIn the same year, 1806, his father, the Prince of Orange died, and William not only inherited the title, but also his father's claims on the inheritance embodied in the Nassau lands. This would become important a few years later, when developments in Germany coincided to make William the F\u00fcrst (Prince) of a diverse assembly of Nassau lands that had belonged to other branches of the House of Nassau.\nBut before this came about, in 1809 tensions between Austria and France became intense, resulting in the War of the Fifth Coalition. William did not hesitate to join the Austrian army as a \"Feldmarschalleutnant\" (major-general) in May 1809 As a member of the staff of the Austrian supreme commander, Archduke Charles he took part in the Battle of Wagram, where he was wounded in the leg.\nTsar Alexander I of Russia played a central role in the restoration of the Netherlands. Prince William VI (as he was now known), who had been living in exile in Prussia, met with Alexander I in March 1813. Alexander promised to support William and help restore an independent Netherlands with William as king. Russian troops in the Netherlands participated with their Prussian allies in restoring the dynasty. Dynastic considerations of marriage between the royal houses of Great Britain and the Netherlands, assured British approval.\nReturn.\nAfter Napoleon's defeat at Leipzig (October 1813), the French troops retreated to France from all over Europe. The Netherlands had been annexed to the French Empire by Napoleon in 1810. But now city after city was evacuated by the French occupation troops. In the ensuing power vacuum a number of former Orangist politicians and former Patriots formed a provisional government in November 1813. Although a large number of the members of the provisional government had helped drive out William V 18 years earlier, it was taken for granted that his son would have to head any new government. They also agreed it would be better in the long term for the Dutch to restore him themselves, rather than have the Great Powers impose him on the country. The Dutch population were pleased with the departure of the French, who had ruined the Dutch economy, and this time welcomed the prince.\nAfter having been invited by the Triumvirate of 1813, on 30 November 1813 William disembarked from and landed at Scheveningen beach, only a few yards from the place where he had left the country with his father 18 years before, and on 6 December the provisional government offered him the title of king. William refused, instead proclaiming himself \"Sovereign Prince of the Netherlands\". He also wanted the rights of the people to be guaranteed by \"a wise constitution\".\nThe constitution offered William extensive, nearly absolute powers: ministers were only responsible to him, while a unicameral parliament (the States General) exercised only limited power. He was inaugurated as sovereign prince in the New Church in Amsterdam on 30 March 1814. In August 1814, he was appointed Governor-General of the former Austrian Netherlands and the Prince-Bishopric of Li\u00e8ge (more or less modern-day Belgium) by the Allied Powers who occupied that country, ruling them on behalf of Prussia. He was also made Grand Duke of Luxembourg, having received that territory in return for trading his hereditary German lands to Prussia and the Duke of Nassau. The Great Powers had already agreed via the secret Eight Articles of London to unite the Low Countries into a single kingdom, it was believed that this would help keep France in check. With the de facto addition of the Austrian Netherlands and Luxembourg to his realm, William had fulfilled his family's three-century dream of uniting the Low Countries.\nKing of the Netherlands.\nFeeling threatened by Napoleon, who had escaped from Elba, William proclaimed the Netherlands a kingdom on 16 March 1815 at the urging of the powers gathered at the Congress of Vienna. His son, the future king William II, fought as a commander at the Battle of Waterloo. After Napoleon had been sent into exile, William adopted a new constitution which included many features of its predecessor, including the near-autocratic powers vested in the crown. He was formally confirmed as hereditary ruler of what was known as the United Kingdom of the Netherlands at the Congress of Vienna.\nPrincipal changes.\nThe States General was divided into two chambers. The \"Eerste Kamer\" (First Chamber or Senate or House of Lords) was appointed by the king. The \"Tweede Kamer\" (Second Chamber or House of Representatives or House of Commons) was elected by the Provincial States, which were in turn chosen by census suffrage. The 110 seats were divided equally between the north and the south, although the population of the north (2 million) was significantly less than that of the south (3.5 million). The States General's primary function was to approve the king's laws and decrees. The constitution contained many present-day Dutch political institutions; however, their functions and composition have changed greatly over the years.\nThe constitution was accepted in the north, but not in the south. The under-representation of the south was one of the causes of the Belgian Revolution. Referendum turnout was low, in the southern provinces, but William interpreted all abstentions to be \"yes\" votes. He prepared a lavish inauguration for himself in Brussels, where he gave the people copper coins (leading to his first nickname, \"the Copper King\").\nThe spearhead of King William's policies was economic progress. As he founded many trade institutions, his second nickname was \"the King-Merchant\". In 1822, he founded the Algemeene Nederlandsche Maatschappij ter Begunstiging van de Volksvlijt, which would become one of the most important institutions of Belgium after its independence. Industry flourished, especially in the South. In 1817, he also founded three universities in the southern provinces, such as a new University of Leuven, the University of Ghent and the University of Li\u00e8ge. The northern provinces, meanwhile, were the centre of trade. This, in combination with the colonies (Dutch East Indies, Surinam, Cura\u00e7ao and Dependencies, and the Dutch Gold Coast) created great wealth for the kingdom. However, the money flowed into the hands of Dutch directors. Only a few Belgians managed to profit from the economic growth. Feelings of economic inequity were another cause of the Belgian uprising.\nWilliam was also determined to create a unified people, even though the north and the south had drifted far apart culturally and economically since the south was reconquered by Spain after the Act of Abjuration of 1581. The north was commercial, Protestant and entirely Dutch-speaking; the south was industrial, Roman Catholic and divided between Dutch and French-speakers. To that end, he concluded his family's longtime factional song, \"Wilhelmus,\" was no longer suitable for the times. The song referenced the desire of the founder of his house, William the Silent, to be rewarded with a \"realm\" for fighting against Spain. Having fulfilled his family's quest to unite that \"realm,\" William wanted to show he represented the entire nation, not just a faction. He was also well aware that the \"Wilhelmus\" had Calvinist connotations, and would not have much appeal in the south. A new song, \"Wien Ne\u00earlands Bloed\" (Those With Dutch Blood), won a contest for a new anthem, and remained in place through 1932.\nOfficially, a separation of church and state existed in the kingdom. However, William himself was a strong supporter of the Reformed Church. This led to resentment among the people in the mostly Catholic south. William had also devised controversial language and school policies. Dutch was imposed as the official language in (the Dutch-speaking region of) Flanders; this angered French-speaking aristocrats and industrial workers. Schools throughout the kingdom were required to instruct students in the Reformed faith and the Dutch language. Many in the south feared that the king sought to extinguish Catholicism and the French language.\nRevolt of the Southern Provinces.\nIn August 1830 Daniel Auber's opera \"La muette de Portici\", about the repression of Neapolitans, was staged in Brussels. Performances of this opera seemed to crystallize a sense of nationalism and \"Hollandophobia\" in Brussels, and spread to the rest of the south. Rioting ensued, chiefly aimed at the kingdom's unpopular justice minister, Cornelis Felix van Maanen, who lived in Brussels. An infuriated William responded by sending troops to repress the riots. However, the riots had spread to other southern cities. The riots quickly became popular uprisings. An independent state of Belgium emerged out of the 1830 Revolution.\nThe next year, William sent his sons William, the Prince of Orange, and Prince Frederick to invade the new state. Although initially victorious in this Ten Days' Campaign, the Royal Netherlands Army was forced to retreat after the threat of French intervention. Some support for the Orange dynasty (chiefly among the Flemish) persisted for years, but the Dutch never regained control over Belgium. William nevertheless continued the war for eight years. His economic successes became overshadowed by a perceived mismanagement of the war effort. High costs of the war came to burden the Dutch economy, fueling public resentment. In 1839, William was forced to end the war. The United Kingdom of the Netherlands was dissolved by the Treaty of London (1839) and the northern part continued as the Kingdom of the Netherlands. It was not renamed, however, as the \"United-\" prefix had never been part of its official name, but rather was retrospectively added by historians for descriptive purposes.\nConstitutional changes and abdication in later life.\nConstitutional changes were initiated in 1840 because the terms which involved the United Kingdom of the Netherlands had to be removed. These constitutional changes also included the introduction of judicial ministerial responsibility. Although the policies remained uncontrolled by parliament, the prerogative was controllable now. The very conservative William could not live with these constitutional changes. This, the disappointment about the loss of Belgium, and his intention to marry Henrietta d'Oultremont (paradoxically both \"Belgian\" and Roman Catholic) made him wish to abdicate. He fulfilled this intent on 7 October 1840 and his eldest son acceded to the throne as King William II. William I died in 1843 in Berlin at the age of 71.\nChildren.\nWith his wife Wilhelmina, King William I had six children:\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "37213", "revid": "1808194", "url": "https://en.wikipedia.org/wiki?curid=37213", "title": "Goldie OGilt", "text": ""}
{"id": "37214", "revid": "1808194", "url": "https://en.wikipedia.org/wiki?curid=37214", "title": "Calisota", "text": ""}
{"id": "37215", "revid": "35936988", "url": "https://en.wikipedia.org/wiki?curid=37215", "title": "Duckburg", "text": ""}
{"id": "37216", "revid": "11292982", "url": "https://en.wikipedia.org/wiki?curid=37216", "title": "Standard Temperature and Pressure", "text": ""}
{"id": "37217", "revid": "30292728", "url": "https://en.wikipedia.org/wiki?curid=37217", "title": "Ducktales", "text": ""}
{"id": "37219", "revid": "44870320", "url": "https://en.wikipedia.org/wiki?curid=37219", "title": "Billung", "text": "Dynasty of Saxon noblemen\nThe House of Billung was a dynasty of Saxon noblemen in the 9th through 12th centuries.\nThe first known member of the house was Count Wichmann, mentioned as a Billung in 811. Oda, the wife of Count Liudolf, oldest known member of the Liudolfing House, was also a Billung as was Matilda of Ringelheim. \nIn the 10th century, the property of the family was centered in the Bardengau around L\u00fcneburg and they controlled the march named after them. In the middle of the 10th century, when the Saxon dukes of the House of Liudolfing had also become German kings, King Otto the Great entrusted more and more of his ducal authority to Hermann Billung. For five generations, the House of Billung ruled the Duchy of Saxony.\nThe house submerged into the Welf and Ascania dynasties when Duke Magnus died in 1106 without sons; the family's property was divided between his two daughters. His daughter Wulfhilde married Henry IX, Duke of Bavaria, a member of the House of Welf; his daughter Eilika married Otto, Count of Ballenstedt, a member of the House of Ascania. As a consequence, for the following decades control of Saxony was contested between the Welfs and Ascanians.\nThe Billung dukes of Saxony were:\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "37220", "revid": "50981011", "url": "https://en.wikipedia.org/wiki?curid=37220", "title": "Infection", "text": "Invasion of an organism's body by pathogenic agents\nMedical condition&lt;templatestyles src=\"Template:Infobox/styles-images.css\" /&gt;\nAn infection is the invasion of tissues by pathogens, their multiplication, and the reaction of host tissues to the infectious agent and the toxins they produce. An infectious disease, also known as a transmissible disease or communicable disease, is an illness resulting from an infection.\nInfections can be caused by a wide range of pathogens, most prominently bacteria and viruses. Hosts can fight infections using their immune systems. Mammalian hosts react to infections with an innate response, often involving inflammation, followed by an adaptive response.\nTreatment for infections depends on the type of pathogen involved. Common medications include:\nInfectious diseases remain a significant global health concern, causing approximately 9.2 million deaths in 2013 (17% of all deaths). The branch of medicine that focuses on infections is referred to as infectious diseases.\n&lt;templatestyles src=\"Template:TOC limit/styles.css\" /&gt;\nTypes.\nInfections are caused by infectious agents (pathogens) including:\nSigns and symptoms.\nThe signs and symptoms of an infection depend on the type of disease. Some signs of infection affect the whole body generally, such as fatigue, loss of appetite, weight loss, fevers, night sweats, chills, aches and pains. Others are specific to individual body parts, such as skin rashes, coughing, or a runny nose.\nIn certain cases, infectious diseases may be asymptomatic for much or even all of their course in a given host. In the latter case, the disease may only be defined as a \"disease\" (which by definition means an illness) in hosts who secondarily become ill after contact with an asymptomatic carrier. An infection is not synonymous with an infectious disease, as some infections do not cause illness in a host.\nBacterial or viral.\nAs bacterial and viral infections can both cause the same kinds of symptoms, it can be difficult to distinguish which is the cause of a specific infection. Distinguishing the two is important, since viral infections cannot be cured by antibiotics whereas bacterial infections can.\nPathophysiology.\nThere is a general chain of events that applies to infections, sometimes called the chain of infection or transmission chain. The chain of events involves several steps\u00a0\u2013 which include the infectious agent, reservoir, entering a susceptible host, exit and transmission to new hosts. Each of the links must be present in a chronological order for an infection to develop. Understanding these steps helps health care workers target the infection and prevent it from occurring in the first place.\nColonization.\nInfection begins when an organism successfully enters the body, grows and multiplies. This is referred to as colonization. Most humans are not easily infected. Those with compromised or weakened immune systems have an increased susceptibility to chronic or persistent infections. Individuals who have a suppressed immune system are particularly susceptible to opportunistic infections. Entrance to the host at host\u2013pathogen interface, generally occurs through the mucosa in orifices like the oral cavity, nose, eyes, genitalia, anus, or the microbe can enter through open wounds. While a few organisms can grow at the initial site of entry, many migrate and cause systemic infection in different organs. Some pathogens grow within the host cells (intracellular) whereas others grow freely in bodily fluids.\nWound colonization refers to non-replicating microorganisms within the wound, while in infected wounds, replicating organisms exist and tissue is injured. All multicellular organisms are colonized to some degree by extrinsic organisms, and the vast majority of these exist in either a mutualistic or commensal relationship with the host. An example of the former is the anaerobic bacteria species, which colonizes the mammalian colon, and an example of the latter are the various species of staphylococcus that exist on human skin. Neither of these colonizations are considered infections. The difference between an infection and a colonization is often only a matter of circumstance. Non-pathogenic organisms can become pathogenic given specific conditions, and even the most virulent organism requires certain circumstances to cause a compromising infection. Some colonizing bacteria, such as \"Corynebacteria sp.\" and \"Viridans streptococci\", prevent the adhesion and colonization of pathogenic bacteria and thus have a symbiotic relationship with the host, preventing infection and speeding wound healing.\nThe variables involved in the outcome of a host becoming inoculated by a pathogen and the ultimate outcome include:\nAs an example, several staphylococcal species remain harmless on the skin, but, when present in a normally sterile space, such as in the capsule of a joint or the peritoneum, multiply without resistance and cause harm.\nAn interesting fact that gas chromatography\u2013mass spectrometry, 16S ribosomal RNA analysis, omics, and other advanced technologies have made more apparent to humans in recent decades is that microbial colonization is very common even in environments that humans think of as being nearly sterile. Because it is normal to have bacterial colonization, it is difficult to know which chronic wounds can be classified as infected and how much risk of progression exists. Despite the huge number of wounds seen in clinical practice, there are limited quality data for evaluated symptoms and signs. A review of chronic wounds in the Journal of the American Medical Association's \"Rational Clinical Examination Series\" quantified the importance of increased pain as an indicator of infection. The review showed that the most useful finding is an increase in the level of pain [likelihood ratio (LR) range, 11\u201320] makes infection much more likely, but the absence of pain (negative likelihood ratio range, 0.64\u20130.88) does not rule out infection (summary LR 0.64\u20130.88).\nDisease.\nDisease can arise if the host's protective immune mechanisms are compromised and the organism inflicts damage on the host. Microorganisms can cause tissue damage by releasing a variety of toxins or destructive enzymes. For example, \"Clostridium tetani\" releases a toxin that paralyzes muscles, and staphylococcus releases toxins that produce shock and sepsis. Not all infectious agents cause disease in all hosts. For example, less than 5% of individuals infected with polio develop disease. On the other hand, some infectious agents are highly virulent. The prion causing mad cow disease and Creutzfeldt\u2013Jakob disease invariably kills all animals and people that are infected.\nPersistent infections occur because the body is unable to clear the organism after the initial infection. Persistent infections are characterized by the continual presence of the infectious organism, often as latent infection with occasional recurrent relapses of active infection. There are some viruses that can maintain a persistent infection by infecting different cells of the body. Some viruses once acquired never leave the body. A typical example is the herpes virus, which tends to hide in nerves and become reactivated when specific circumstances arise.\nPersistent infections cause millions of deaths globally each year. Chronic infections by parasites account for a high morbidity and mortality in many underdeveloped countries.\nTransmission.\nFor infecting organisms to survive and repeat the infection cycle in other hosts, they (or their progeny) must leave an existing reservoir and cause infection elsewhere. Infection transmission can take place via many potential routes:\nThe relationship between \"virulence versus transmissibility\" is complex; with studies showing no clear relationships between the two. There is still a small number of evidence that partially suggests a link between virulence and transmissibility.\nDiagnosis.\nDiagnosis of infectious disease sometimes involves identifying an infectious agent either directly or indirectly. In practice most minor infectious diseases such as warts, cutaneous abscesses, respiratory system infections and diarrheal diseases are diagnosed by their clinical presentation and treated without knowledge of the specific causative agent. Conclusions about the cause of the disease are based upon the likelihood that a patient came in contact with a particular agent, the presence of a microbe in a community, and other epidemiological considerations. Given sufficient effort, all known infectious agents can be specifically identified.\nDiagnosis of infectious disease is nearly always initiated by medical history and physical examination. More detailed identification techniques involve the culture of infectious agents isolated from a patient. Culture allows identification of infectious organisms by examining their microscopic features, by detecting the presence of substances produced by pathogens, and by directly identifying an organism by its genotype.\nMany infectious organisms are identified without culture and microscopy. This is especially true for viruses, which cannot grow in culture. For some suspected pathogens, doctors may conduct tests that examine a patient's blood or other body fluids for antigens or antibodies that indicate presence of a specific pathogen that the doctor suspects.\nOther techniques (such as X-rays, CAT scans, PET scans or NMR) are used to produce images of internal abnormalities resulting from the growth of an infectious agent. The images are useful in detection of, for example, a bone abscess or a spongiform encephalopathy produced by a prion.\nThe benefits of identification, however, are often greatly outweighed by the cost, as often there is no specific treatment, the cause is obvious, or the outcome of an infection is likely to be benign.\nSymptomatic diagnostics.\nThe diagnosis is aided by the presenting symptoms in any individual with an infectious disease, yet it usually needs additional diagnostic techniques to confirm the suspicion. Some signs are specifically characteristic and indicative of a disease and are called pathognomonic signs; but these are rare. Not all infections are symptomatic.\nIn children the presence of cyanosis, rapid breathing, poor peripheral perfusion, or a petechial rash increases the risk of a serious infection by greater than 5 fold. Other important indicators include parental concern, clinical instinct, and temperature greater than 40\u00a0\u00b0C.\nMicrobial culture.\nMany diagnostic approaches depend on microbiological culture to isolate a pathogen from the appropriate clinical specimen. In a microbial culture, a growth medium is provided for a specific agent. A sample taken from potentially diseased tissue or fluid is then tested for the presence of an infectious agent able to grow within that medium. Many pathogenic bacteria are easily grown on nutrient agar, a form of solid medium that supplies carbohydrates and proteins necessary for growth, along with copious amounts of water. A single bacterium will grow into a visible mound on the surface of the plate called a colony, which may be separated from other colonies or melded together into a \"lawn\". The size, color, shape and form of a colony is characteristic of the bacterial species, its specific genetic makeup (its strain), and the environment that supports its growth. Other ingredients are often added to the plate to aid in identification. Plates may contain substances that permit the growth of some bacteria and not others, or that change color in response to certain bacteria and not others. Bacteriological plates such as these are commonly used in the clinical identification of infectious bacterium. Microbial culture may also be used in the identification of viruses: the medium, in this case, being cells grown in culture that the virus can infect, and then alter or kill. In the case of viral identification, a region of dead cells results from viral growth, and is called a \"plaque\". Eukaryotic parasites may also be grown in culture as a means of identifying a particular agent.\nIn the absence of suitable plate culture techniques, some microbes require culture within live animals. Bacteria such as \"Mycobacterium leprae\" and \"Treponema pallidum\" can be grown in animals, although serological and microscopic techniques make the use of live animals unnecessary. Viruses are also usually identified using alternatives to growth in culture or animals. Some viruses may be grown in embryonated eggs. Another useful identification method is Xenodiagnosis, or the use of a vector to support the growth of an infectious agent. Chagas disease is the most significant example, because it is difficult to directly demonstrate the presence of the causative agent, \"Trypanosoma cruzi\" in a patient, which therefore makes it difficult to definitively make a diagnosis. In this case, xenodiagnosis involves the use of the vector of the Chagas agent \"T. cruzi\", an uninfected triatomine bug, which takes a blood meal from a person suspected of having been infected. The bug is later inspected for growth of \"T. cruzi\" within its gut.\nMicroscopy.\nAnother principal tool in the diagnosis of infectious disease is microscopy. Virtually all of the culture techniques discussed above rely, at some point, on microscopic examination for definitive identification of the infectious agent. Microscopy may be carried out with simple instruments, such as the compound light microscope, or with instruments as complex as an electron microscope. Samples obtained from patients may be viewed directly under the light microscope, and can often rapidly lead to identification. Microscopy is often also used in conjunction with biochemical staining techniques, and can be made exquisitely specific when used in combination with antibody based techniques. For example, the use of antibodies made artificially fluorescent (fluorescently labeled antibodies) can be directed to bind to and identify a specific antigens present on a pathogen. A fluorescence microscope is then used to detect fluorescently labeled antibodies bound to internalized antigens within clinical samples or cultured cells. This technique is especially useful in the diagnosis of viral diseases, where the light microscope is incapable of identifying a virus directly.\nOther microscopic procedures may also aid in identifying infectious agents. Almost all cells readily stain with a number of basic dyes due to the electrostatic attraction between negatively charged cellular molecules and the positive charge on the dye. A cell is normally transparent under a microscope, and using a stain increases the contrast of a cell with its background. Staining a cell with a dye such as Giemsa stain or crystal violet allows a microscopist to describe its size, shape, internal and external components and its associations with other cells. The response of bacteria to different staining procedures is used in the taxonomic classification of microbes as well. Two methods, the Gram stain and the acid-fast stain, are the standard approaches used to classify bacteria and to diagnosis of disease. The Gram stain identifies the bacterial groups Bacillota and Actinomycetota, both of which contain many significant human pathogens. The acid-fast staining procedure identifies the Actinomycetota genera \"Mycobacterium\" and \"Nocardia\".\nBiochemical tests.\nBiochemical tests used in the identification of infectious agents include the detection of metabolic or enzymatic products characteristic of a particular infectious agent. Since bacteria ferment carbohydrates in patterns characteristic of their genus and species, the detection of fermentation products is commonly used in bacterial identification. Acids, alcohols and gases are usually detected in these tests when bacteria are grown in selective liquid or solid media.\nThe isolation of enzymes from infected tissue can also provide the basis of a biochemical diagnosis of an infectious disease. For example, humans can make neither RNA replicases nor reverse transcriptase, and the presence of these enzymes are characteristic., of specific types of viral infections. The ability of the viral protein hemagglutinin to bind red blood cells together into a detectable matrix may also be characterized as a biochemical test for viral infection, although strictly speaking hemagglutinin is not an \"enzyme\" and has no metabolic function.\nSerological methods are highly sensitive, specific and often extremely rapid tests used to identify microorganisms. These tests are based upon the ability of an antibody to bind specifically to an antigen. The antigen, usually a protein or carbohydrate made by an infectious agent, is bound by the antibody. This binding then sets off a chain of events that can be visibly obvious in various ways, dependent upon the test. For example, \"Strep throat\" is often diagnosed within minutes, and is based on the appearance of antigens made by the causative agent, \"S. pyogenes\", that is retrieved from a patient's throat with a cotton swab. Serological tests, if available, are usually the preferred route of identification, however the tests are costly to develop and the reagents used in the test often require refrigeration. Some serological methods are extremely costly, although when commonly used, such as with the \"strep test\", they can be inexpensive.\nComplex serological techniques have been developed into what are known as immunoassays. Immunoassays can use the basic antibody \u2013 antigen binding as the basis to produce an electro-magnetic or particle radiation signal, which can be detected by some form of instrumentation. Signal of unknowns can be compared to that of standards allowing quantitation of the target antigen. To aid in the diagnosis of infectious diseases, immunoassays can detect or measure antigens from either infectious agents or proteins generated by an infected organism in response to a foreign agent. For example, immunoassay A may detect the presence of a surface protein from a virus particle. Immunoassay B on the other hand may detect or measure antibodies produced by an organism's immune system that are made to neutralize and allow the destruction of the virus.\nInstrumentation can be used to read extremely small signals created by secondary reactions linked to the antibody \u2013 antigen binding. Instrumentation can control sampling, reagent use, reaction times, signal detection, calculation of results, and data management to yield a cost-effective automated process for diagnosis of infectious disease.\nPCR-based diagnostics.\nTechnologies based upon the polymerase chain reaction (PCR) method will become nearly ubiquitous gold standards of diagnostics of the near future, for several reasons. First, the catalog of infectious agents has grown to the point that virtually all of the significant infectious agents of the human population have been identified. Second, an infectious agent must grow within the human body to cause disease; essentially it must amplify its own nucleic acids to cause a disease. This amplification of nucleic acid in infected tissue offers an opportunity to detect the infectious agent by using PCR. Third, the essential tools for directing PCR, primers, are derived from the genomes of infectious agents, and with time those genomes will be known if they are not already.\nThus, the technological ability to detect any infectious agent rapidly and specifically is currently available. The only remaining blockades to the use of PCR as a standard tool of diagnosis are in its cost and application, neither of which is insurmountable. The diagnosis of a few diseases will not benefit from the development of PCR methods, such as some of the clostridial diseases (tetanus and botulism). These diseases are fundamentally biological poisonings by relatively small numbers of infectious bacteria that produce extremely potent neurotoxins. A significant proliferation of the infectious agent does not occur, this limits the ability of PCR to detect the presence of any bacteria.\nMetagenomic sequencing.\nGiven the wide range of bacterial, viral, fungal, protozoal, and helminthic pathogens that cause debilitating and life-threatening illnesses, the ability to quickly identify the cause of infection is important yet often challenging. For example, more than half of cases of encephalitis, a severe illness affecting the brain, remain undiagnosed, despite extensive testing using the standard of care (microbiological culture) and state-of-the-art clinical laboratory methods. Metagenomic sequencing-based diagnostic tests are currently being developed for clinical use and show promise as a sensitive, specific, and rapid way to diagnose infection using a single all-encompassing test. This test is similar to current PCR tests; however, an untargeted whole genome amplification is used rather than primers for a specific infectious agent. This amplification step is followed by next-generation sequencing or third-generation sequencing, alignment comparisons, and taxonomic classification using large databases of thousands of pathogen and commensal reference genomes. Simultaneously, antimicrobial resistance genes within pathogen and plasmid genomes are sequenced and aligned to the taxonomically classified pathogen genomes to generate an antimicrobial resistance profile \u2013 analogous to antibiotic sensitivity testing \u2013 to facilitate antimicrobial stewardship and allow for the optimization of treatment using the most effective drugs for a patient's infection.\nMetagenomic sequencing could prove especially useful for diagnosis when the patient is immunocompromised. An ever-wider array of infectious agents can cause serious harm to individuals with immunosuppression, so clinical screening must often be broader. Additionally, the expression of symptoms is often atypical, making a clinical diagnosis based on presentation more difficult. Thirdly, diagnostic methods that rely on the detection of antibodies are more likely to fail. A rapid, sensitive, specific, and untargeted test for all known human pathogens that detects the presence of the organism's DNA rather than antibodies is therefore highly desirable.\nIndication of tests.\nThere is usually an indication for a specific identification of an infectious agent only when such identification can aid in the treatment or prevention of the disease, or to advance knowledge of the course of an illness prior to the development of effective therapeutic or preventative measures. For example, in the early 1980s, prior to the appearance of AZT for the treatment of AIDS, the course of the disease was closely followed by monitoring the composition of patient blood samples, even though the outcome would not offer the patient any further treatment options. In part, these studies on the appearance of HIV in specific communities permitted the advancement of hypotheses as to the route of transmission of the virus. By understanding how the disease was transmitted, resources could be targeted to the communities at greatest risk in campaigns aimed at reducing the number of new infections. The specific serological diagnostic identification, and later genotypic or molecular identification, of HIV also enabled the development of hypotheses as to the temporal and geographical origins of the virus, as well as a myriad of other hypothesis. The development of molecular diagnostic tools have enabled physicians and researchers to monitor the efficacy of treatment with anti-retroviral drugs. Molecular diagnostics are now commonly used to identify HIV in healthy people long before the onset of illness and have been used to demonstrate the existence of people who are genetically resistant to HIV infection. Thus, while there still is no cure for AIDS, there is great therapeutic and predictive benefit to identifying the virus and monitoring the virus levels within the blood of infected individuals, both for the patient and for the community at large.\nClassification.\nSubclinical versus clinical (latent versus apparent).\nSymptomatic infections are \"apparent\" and \"clinical\", whereas an infection that is active but does not produce noticeable symptoms may be called \"inapparent,\" \"silent,\" \"subclinical\", or occult. An infection that is inactive or dormant is called a \"latent infection\". An example of a latent bacterial infection is latent tuberculosis. Some viral infections can also be latent, examples of latent viral infections are any of those from the \"Herpesviridae\" family.\nThe word \"infection\" can denote any presence of a particular pathogen at all (no matter how little) but also is often used in a sense implying a \"clinically apparent\" infection (in other words, a case of infectious disease). This fact occasionally creates some ambiguity or prompts some usage discussion; to get around this it is common for health professionals to speak of \"colonization\" (rather than \"infection\") when they mean that some of the pathogens are present but that no clinically apparent infection (no disease) is present.\nCourse of infection.\nDifferent terms are used to describe how and where infections present over time. In an \"acute\" infection, symptoms develop rapidly; its course can either be rapid or protracted. In \"chronic\" infection, symptoms usually develop gradually over weeks or months and are slow to resolve. In \"subacute\" infections, symptoms take longer to develop than in acute infections but arise more quickly than those of chronic infections. A \"focal\" infection is an initial site of infection from which organisms travel via the bloodstream to another area of the body.\nPrimary versus opportunistic.\nAmong the many varieties of microorganisms, relatively few cause disease in otherwise healthy individuals. Infectious disease results from the interplay between those few pathogens and the defenses of the hosts they infect. The appearance and severity of disease resulting from any pathogen depend upon the ability of that pathogen to damage the host as well as the ability of the host to resist the pathogen. However, a host's immune system can also cause damage to the host itself in an attempt to control the infection. Clinicians, therefore, classify infectious microorganisms or microbes according to the status of host defenses \u2013 either as \"primary pathogens\" or as \"opportunistic pathogens\".\nPrimary pathogens.\nPrimary pathogens cause disease as a result of their presence or activity within the normal, healthy host, and their intrinsic virulence (the severity of the disease they cause) is, in part, a necessary consequence of their need to reproduce and spread. Many of the most common primary pathogens of humans only infect humans, however, many serious diseases are caused by organisms acquired from the environment or that infect non-human hosts.\nOpportunistic pathogens.\nOpportunistic pathogens can cause an infectious disease in a host with depressed resistance (immunodeficiency) or if they have unusual access to the inside of the body (for example, via trauma). Opportunistic infection may be caused by microbes ordinarily in contact with the host, such as pathogenic bacteria or fungi in the gastrointestinal or the upper respiratory tract, and they may also result from (otherwise innocuous) microbes acquired from other hosts (as in \"Clostridioides difficile\" colitis) or from the environment as a result of traumatic introduction (as in surgical wound infections or compound fractures). An opportunistic disease requires impairment of host defenses, which may occur as a result of genetic defects (such as chronic granulomatous disease), exposure to antimicrobial drugs or immunosuppressive chemicals (as might occur following poisoning or cancer chemotherapy), exposure to ionizing radiation, or as a result of an infectious disease with immunosuppressive activity (such as with measles, malaria or HIV disease). Primary pathogens may also cause more severe disease in a host with depressed resistance than would normally occur in an immunosufficient host.\nSecondary infection.\nWhile a primary infection can practically be viewed as the root cause of an individual's current health problem, a secondary infection is a sequela or complication of that root cause. For example, an infection due to a burn or penetrating trauma (the root cause) is a secondary infection. Primary pathogens often cause primary infection and often cause secondary infection. Usually, opportunistic infections are viewed as secondary infections (because immunodeficiency or injury was the predisposing factor).\nOther types of infection.\nOther types of infection consist of mixed, iatrogenic, nosocomial, and community-acquired infection. A mixed infection is an infection that is caused by two or more pathogens. An example of this is appendicitis, which is caused by \"Bacteroides fragilis\" and \"Escherichia coli\". The second is an iatrogenic infection. This type of infection is one that is transmitted from a health care worker to a patient. A nosocomial infection is also one that occurs in a health care setting. Nosocomial infections are those that are acquired during a hospital stay. Lastly, a community-acquired infection is one in which the infection is acquired from a whole community.\nInfectious or not.\nOne manner of proving that a given disease is infectious, is to satisfy Koch's postulates (first proposed by Robert Koch), which require that first, the infectious agent be identifiable only in patients who have the disease, and not in healthy controls, and second, that patients who contract the infectious agent also develop the disease. These postulates were first used in the discovery that Mycobacteria species cause tuberculosis.\nHowever, Koch's postulates cannot usually be tested in modern practice for ethical reasons. Proving them would require experimental infection of a healthy individual with a pathogen produced as a pure culture. Conversely, even clearly infectious diseases do not always meet the infectious criteria; for example, \"Treponema pallidum\", the causative spirochete of syphilis, cannot be cultured \"in vitro\" \u2013 however the organism can be cultured in rabbit testes. It is less clear that a pure culture comes from an animal source serving as host than it is when derived from microbes derived from plate culture.\nEpidemiology, or the study and analysis of who, why and where disease occurs, and what determines whether various populations have a disease, is another important tool used to understand infectious disease. Epidemiologists may determine differences among groups within a population, such as whether certain age groups have a greater or lesser rate of infection; whether groups living in different neighborhoods are more likely to be infected; and by other factors, such as gender and race. Researchers also may assess whether a disease outbreak is sporadic, or just an occasional occurrence; endemic, with a steady level of regular cases occurring in a region; epidemic, with a fast arising, and unusually high number of cases in a region; or pandemic, which is a global epidemic. If the cause of the infectious disease is unknown, epidemiology can be used to assist with tracking down the sources of infection.\nContagiousness.\nInfectious diseases are sometimes called contagious diseases when they are easily transmitted by contact with an ill person or their secretions (e.g., influenza). Thus, a contagious disease is a subset of infectious disease that is especially infective or easily transmitted. All contagious diseases are infectious, but not vice versa. Other types of infectious, transmissible, or communicable diseases with more specialized routes of infection, such as vector transmission or sexual transmission, are usually not regarded as \"contagious\", and often do not require medical isolation (sometimes loosely called quarantine) of those affected. However, this specialized connotation of the word \"contagious\" and \"contagious disease\" (easy transmissibility) is not always respected in popular use.\nInfectious diseases are commonly transmitted from person to person through direct contact. The types of direct contact are through person to person and droplet spread. Indirect contact such as airborne transmission, contaminated objects, food and drinking water, animal person contact, animal reservoirs, insect bites, and environmental reservoirs are another way infectious diseases are transmitted. The basic reproduction number of an infectious disease measures how easily it spreads through direct or indirect contact.\nBy anatomic location.\nInfections can be classified by the anatomic location or organ system infected, including:\nIn addition, locations of inflammation where infection is the most common cause include pneumonia, meningitis and salpingitis.\nPrevention.\nTechniques like hand washing, wearing gowns, and wearing face masks can help prevent infections from being passed from one person to another. Aseptic technique was introduced in medicine and surgery in the late 19th century and greatly reduced the incidence of infections caused by surgery. Frequent hand washing remains the most important defense against the spread of unwanted organisms. There are other forms of prevention such as avoiding the use of illicit drugs, using a condom, wearing gloves, and having a healthy lifestyle with a balanced diet and regular exercise. Cooking foods well and avoiding foods that have been left outside for a long time is also important.\nAntimicrobial substances used to prevent transmission of infections include:\nOne of the ways to prevent or slow down the transmission of infectious diseases is to recognize the different characteristics of various diseases. Some critical disease characteristics that should be evaluated include virulence, distance traveled by those affected, and level of contagiousness. The human strains of Ebola virus, for example, incapacitate those infected extremely quickly and kill them soon after. As a result, those affected by this disease do not have the opportunity to travel very far from the initial infection zone. Also, this virus must spread through skin lesions or permeable membranes such as the eye. Thus, the initial stage of Ebola is not very contagious since its victims experience only internal hemorrhaging. As a result of the above features, the spread of Ebola is very rapid and usually stays within a relatively confined geographical area. In contrast, the human immunodeficiency virus (HIV) kills its victims very slowly by attacking their immune system. As a result, many of its victims transmit the virus to other individuals before even realizing that they are carrying the disease. Also, the relatively low virulence allows its victims to travel long distances, increasing the likelihood of an epidemic.\nAnother effective way to decrease the transmission rate of infectious diseases is to recognize the effects of small-world networks. In epidemics, there are often extensive interactions within hubs or groups of infected individuals and other interactions within discrete hubs of susceptible individuals. Despite the low interaction between discrete hubs, the disease can jump and spread in a susceptible hub via a single or few interactions with an infected hub. Thus, infection rates in small-world networks can be reduced somewhat if interactions between individuals within infected hubs are eliminated (Figure 1). However, infection rates can be drastically reduced if the main focus is on the prevention of transmission jumps between hubs. The use of needle exchange programs in areas with a high density of drug users with HIV is an example of the successful implementation of this treatment method. Another example is the use of ring culling or vaccination of potentially susceptible livestock in adjacent farms to prevent the spread of the foot-and-mouth virus in 2001.\nA general method to prevent transmission of vector-borne pathogens is pest control.\nIn cases where infection is merely suspected, individuals may be quarantined until the incubation period has passed and the disease manifests itself or the person remains healthy. Groups may undergo quarantine, or in the case of communities, a cordon sanitaire may be imposed to prevent infection from spreading beyond the community, or in the case of protective sequestration, into a community. Public health authorities may implement other forms of social distancing, such as school closings, lockdowns or temporary restrictions (e.g. circuit breakers) to control an epidemic.\nImmunity.\nInfection with most pathogens does not result in death of the host and the offending organism is ultimately cleared after the symptoms of the disease have waned. This process requires immune mechanisms to kill or inactivate the inoculum of the pathogen. Specific acquired immunity against infectious diseases may be mediated by antibodies and/or T lymphocytes. Immunity mediated by these two factors may be manifested by:\nThe immune system response to a microorganism often causes symptoms such as a high fever and inflammation, and has the potential to be more devastating than direct damage caused by a microbe.\nResistance to infection (immunity) may be acquired following a disease, by asymptomatic carriage of the pathogen, by harboring an organism with a similar structure (crossreacting), or by vaccination. Knowledge of the protective antigens and specific acquired host immune factors is more complete for primary pathogens than for opportunistic pathogens. There is also the phenomenon of herd immunity which offers a measure of protection to those otherwise vulnerable people when a large enough proportion of the population has acquired immunity from certain infections.\nImmune resistance to an infectious disease requires a critical level of either antigen-specific antibodies and/or T cells when the host encounters the pathogen. Some individuals develop natural serum antibodies to the surface polysaccharides of some agents although they have had little or no contact with the agent, these natural antibodies confer specific protection to adults and are passively transmitted to newborns.\nHost genetic factors.\nThe organism that is the target of an infecting action of a specific infectious agent is called the host. The host harbouring an agent that is in a mature or sexually active stage phase is called the definitive host. The intermediate host comes in contact during the larvae stage. A host can be anything living and can attain to asexual and sexual reproduction.\nThe clearance of the pathogens, either treatment-induced or spontaneous, it can be influenced by the genetic variants carried by the individual patients. For instance, for genotype 1 hepatitis C treated with Pegylated interferon-alpha-2a or Pegylated interferon-alpha-2b (brand names Pegasys or PEG-Intron) combined with ribavirin, it has been shown that genetic polymorphisms near the human IL28B gene, encoding interferon lambda 3, are associated with significant differences in the treatment-induced clearance of the virus. This finding, originally reported in \"Nature\", showed that genotype 1 hepatitis C patients carrying certain genetic variant alleles near the IL28B gene are more possibly to achieve sustained virological response after the treatment than others. Later report from \"Nature\" demonstrated that the same genetic variants are also associated with the natural clearance of the genotype 1 hepatitis C virus.\nTreatments.\nWhen infection attacks the body, \"anti-infective\" drugs can suppress the infection. Several broad types of anti-infective drugs exist, depending on the type of organism targeted; they include antibacterial (antibiotic; including antitubercular), antiviral, antifungal and antiparasitic (including antiprotozoal and antihelminthic) agents. Depending on the severity and the type of infection, the antibiotic may be given by mouth or by injection, or may be applied topically. Severe infections of the brain are usually treated with intravenous antibiotics. Sometimes, multiple antibiotics are used in case there is resistance to one antibiotic. Antibiotics only work for bacteria and do not affect viruses. Antibiotics work by slowing down the multiplication of bacteria or killing the bacteria. The most common classes of antibiotics used in medicine include penicillin, cephalosporins, aminoglycosides, macrolides, quinolones and tetracyclines.\nNot all infections require treatment, and for many self-limiting infections the treatment may cause more side-effects than benefits. Antimicrobial stewardship is the concept that healthcare providers should treat an infection with an antimicrobial that specifically works well for the target pathogen for the shortest amount of time and to only treat when there is a known or highly suspected pathogen that will respond to the medication.\nSusceptibility to infection.\nPandemics such as COVID-19 show that people dramatically differ in their susceptibility to infection. This may be because of general health, age, or their immune status, e.g. when they have been infected previously. However, it also has become clear that there are genetic factors which determine susceptibility to infection. For instance, up to 40% of SARS-CoV-2 infections may be asymptomatic, suggesting that many people are naturally protected from disease. Large genetic studies have defined risk factors for severe SARS-CoV-2 infections, and genome sequences from 659 patients with severe COVID-19 revealed genetic variants that appear to be associated with life-threatening disease. One gene identified in these studies is type I interferon (IFN). Autoantibodies against type I IFNs were found in up to 13.7% of patients with life-threatening COVID-19, indicating that a complex interaction between genetics and the immune system is important for natural resistance to Covid.\nSimilarly, mutations in the ERAP2 gene, encoding endoplasmic reticulum aminopeptidase 2, seem to increase the susceptibility to the plague, the disease caused by an infection with the bacteria \"Yersinia pestis\". People who inherited two copies of a complete variant of the gene were twice as likely to have survived the plague as those who inherited two copies of a truncated variant.\nSusceptibility also determined the epidemiology of infection, given that different populations have different genetic and environmental conditions that affect infections.\nEpidemiology.\nAn estimated 1,680 million people died of infectious diseases in the 20th century and about 10 million in 2010.\nThe World Health Organization collects information on global deaths by International Classification of Disease (ICD) code categories. The following table lists the top infectious disease by number of deaths in 2002. 1993 data is included for comparison.\nThe top three single agent/disease killers are HIV/AIDS, TB and malaria. While the number of deaths due to nearly every disease have decreased, deaths due to HIV/AIDS have increased fourfold. Childhood diseases include pertussis, poliomyelitis, diphtheria, measles and tetanus. Children also make up a large percentage of lower respiratory and diarrheal deaths. In 2012, approximately 3.1 million people have died due to lower respiratory infections, making it the number 4 leading cause of death in the world.\nHistoric pandemics.\nWith their potential for unpredictable and explosive impacts, infectious diseases have been major actors in human history. A pandemic (or global epidemic) is a disease that affects people over an extensive geographical area. For example:\nEmerging diseases.\nIn most cases, microorganisms live in harmony with their hosts via mutual or commensal interactions. Diseases can emerge when existing parasites become pathogenic or when new pathogenic parasites enter a new host.\nSeveral human activities have led to the emergence of zoonotic human pathogens, including viruses, bacteria, protozoa, and rickettsia, and spread of vector-borne diseases, see also globalization and disease and wildlife disease:\nGerm theory of disease.\nIn Antiquity, the Greek historian Thucydides (c.\u2009460 \u2013 c.\u2009400 BCE) was the first person to write, in his account of the plague of Athens, that diseases could spread from an infected person to others. In his \"On the Different Types of Fever\" (c.\u2009175 AD), the Greco-Roman physician Galen speculated that plagues were spread by \"certain seeds of plague\", which were present in the air. In the Sushruta Samhita, the ancient Indian physician Sushruta theorized: \"Leprosy, fever, consumption, diseases of the eye, and other infectious diseases spread from one person to another by sexual union, physical contact, eating together, sleeping together, sitting together, and the use of same clothes, garlands and pastes.\" This book has been dated to about the sixth century BC.\nA basic form of contagion theory was proposed by Persian physician Ibn Sina (known as Avicenna in Europe) in \"The Canon of Medicine\" (1025), which later became the most authoritative medical textbook in Europe up until the 16th century. In Book IV of the \"Canon\", Ibn Sina discussed epidemics, outlining the classical miasma theory and attempting to blend it with his own early contagion theory. He mentioned that people can transmit disease to others by breath, noted contagion with tuberculosis, and discussed the transmission of disease through water and dirt. The concept of invisible contagion was later discussed by several Islamic scholars in the Ayyubid Sultanate who referred to them as \"najasat\" (\"impure substances\"). The fiqh scholar Ibn al-Haj al-Abdari (c.\u20091250\u20131336), while discussing Islamic diet and hygiene, gave warnings about how contagion can contaminate water, food, and garments, and could spread through the water supply, and may have implied contagion to be unseen particles.\nWhen the Black Death bubonic plague reached Al-Andalus in the 14th century, the Arab physicians Ibn Khatima (c.\u20091369) and Ibn al-Khatib (1313\u20131374) hypothesised that infectious diseases were caused by \"minute bodies\" and described how they can be transmitted through garments, vessels and earrings. Ideas of contagion became more popular in Europe during the Renaissance, particularly through the writing of the Italian physician Girolamo Fracastoro. Anton van Leeuwenhoek (1632\u20131723) advanced the science of microscopy by being the first to observe microorganisms, allowing for easy visualization of bacteria.\nIn the mid-19th century John Snow and William Budd did important work demonstrating the contagiousness of typhoid and cholera through contaminated water. Both are credited with decreasing epidemics of cholera in their towns by implementing measures to prevent contamination of water. Louis Pasteur proved beyond doubt that certain diseases are caused by infectious agents, and developed a vaccine for rabies. Robert Koch provided the study of infectious diseases with a scientific basis known as Koch's postulates. Edward Jenner, Jonas Salk and Albert Sabin developed effective vaccines for smallpox and polio, which would later result in the eradication and near-eradication of these diseases, respectively. Alexander Fleming discovered the world's first antibiotic, penicillin, which Florey and Chain then developed. Gerhard Domagk developed sulphonamides, the first broad spectrum synthetic antibacterial drugs.\nMedical specialists.\nThe medical treatment of infectious diseases falls into the medical field of Infectious Disease and in some cases the study of propagation pertains to the field of Epidemiology. Generally, infections are initially diagnosed by primary care physicians or internal medicine specialists. For example, an \"uncomplicated\" pneumonia will generally be treated by the internist or the pulmonologist (lung physician). The work of the infectious diseases specialist therefore entails working with both patients and general practitioners, as well as laboratory scientists, immunologists, bacteriologists and other specialists.\nAn infectious disease team may be alerted when:\nSociety and culture.\nSeveral studies have reported associations between pathogen load in an area and human behavior. Higher pathogen load is associated with decreased size of ethnic and religious groups in an area. This may be due high pathogen load favoring avoidance of other groups, which may reduce pathogen transmission, or a high pathogen load preventing the creation of large settlements and armies that enforce a common culture. Higher pathogen load is also associated with more restricted sexual behavior, which may reduce pathogen transmission. It also associated with higher preferences for health and attractiveness in mates. Higher fertility rates and shorter or less parental care per child is another association that may be a compensation for the higher mortality rate. There is also an association with polygyny which may be due to higher pathogen load, making selecting males with a high genetic resistance increasingly important. Higher pathogen load is also associated with more collectivism and less individualism, which may limit contacts with outside groups and infections. There are alternative explanations for at least some of the associations although some of these explanations may in turn ultimately be due to pathogen load. Thus, polygyny may also be due to a lower male: female ratio in these areas but this may ultimately be due to male infants having increased mortality from infectious diseases. Another example is that poor socioeconomic factors may ultimately in part be due to high pathogen load preventing economic development.\nFossil record.\nEvidence of infection in fossil remains is a subject of interest for paleopathologists, scientists who study occurrences of injuries and illness in extinct life forms. Signs of infection have been discovered in the bones of carnivorous dinosaurs. When present, however, these infections seem to tend to be confined to only small regions of the body. A skull attributed to the early carnivorous dinosaur \"Herrerasaurus ischigualastensis\" exhibits pit-like wounds surrounded by swollen and porous bone. The unusual texture of the bone around the wounds suggests they were affected by a short-lived, non-lethal infection. Scientists who studied the skull speculated that the bite marks were received in a fight with another \"Herrerasaurus\". Other carnivorous dinosaurs with documented evidence of infection include \"Acrocanthosaurus\", \"Allosaurus\", \"Tyrannosaurus\" and a tyrannosaur from the Kirtland Formation. The infections from both tyrannosaurs were received by being bitten during a fight, like the \"Herrerasaurus\" specimen.\nOuter space.\nA 2006 Space Shuttle experiment found that \"Salmonella typhimurium\", a bacterium that can cause food poisoning, became more virulent when cultivated in space. On April 29, 2013, scientists in Rensselaer Polytechnic Institute, funded by NASA, reported that, during spaceflight on the International Space Station, microbes seem to adapt to the space environment in ways \"not observed on Earth\" and in ways that \"can lead to increases in growth and virulence\". More recently, in 2017, bacteria were found to be more resistant to antibiotics and to thrive in the near-weightlessness of space. Microorganisms have been observed to survive the vacuum of outer space.\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "37221", "revid": "13286072", "url": "https://en.wikipedia.org/wiki?curid=37221", "title": "North Coast Athletic Conference", "text": "NCAA Division III athletic conference\nThe North Coast Athletic Conference (NCAC) is an intercollegiate athletic conference that competes in the National Collegiate Athletic Association (NCAA) Division III which is composed of colleges located in Ohio and Indiana. It sponsors 23 sports, 11 for men and 12 for women.\nHistory.\nThe formation of the NCAC was announced at joint news conferences in Cleveland, Columbus and Pittsburgh in February 1983. Allegheny College, Case Western Reserve University (CWRU), Denison University, Kenyon College, Oberlin College, Ohio Wesleyan University, and The College of Wooster were charter members in 1984, the same year that NCAC athletic conference play began. The conference offered 10 women's sports, the most offered by a conference at that time.\nIn 1988, Earlham College and Wittenberg College accepted invitations to join the NCAC, pushing conference membership to nine schools in three states. The two schools would begin play in the fall of 1989. In 1998, Hiram College, and Wabash College accepted invitations to join the NCAC, pushing conference membership to 10 schools in three states, which both schools began play in the fall of 1999. Case Western Reserve, a charter member of the NCAC, announced that it would leave the NCAC following the 1998\u201399 academic year. The Spartans would compete on a full-time basis in the University Athletic Association (UAA) after more than a decade of joint conference membership affiliation.\nEarlham announced it would depart the NCAC for the Heartland Collegiate Athletic Conference (HCAC), beginning with the 2010\u201311 season. DePauw University became the 10th member of the NCAC beginning in the 2011\u201312 season.\nAllegheny left the NCAC after the 2021\u201322 school year to return to its former home of the Presidents' Athletic Conference (PAC). Allegheny and Earlham remain single-sport NCAC members in field hockey. Later in 2022, Transylvania University and Washington &amp; Jefferson College were announced as single-sport NCAC members for field hockey, beginning with the 2023 season. \nThe most recent changes to the NCAC membership were announced in 2024. First, on January 18, John Carroll University announced it was leaving the Ohio Athletic Conference to join the NCAC. Then on April 23, Hiram announced it would leave the NCAC in 2025 to return to the PAC, which it had left in 1989.\nDiversity, equity, and inclusion (DEI) initiatives.\nIn 2019, the NCAC was one of the first NCAA conferences to participate in the organization's LGBTQ OneTeam Program, which launched in fall 2019. Two facilitators from the NCAC \u2013 Seth Hayes of Denison University and Rhea Debussy of Kenyon College \u2013 were among the first 30 facilitators for this NCAA Division III program. In 2021, the NCAA announced that two NCAC staff members \u2013 Kate Costanzo of Allegheny College and Rhea Debussy of Kenyon College \u2013 were finalists for the NCAA Division III LGBTQ Administrator/Coach/Staff of the Year Award.\nMember schools.\nCurrent members.\nThe NCAC currently has nine full members, all private schools. \n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nAffiliate members.\nThe NCAC has four affiliate members, all are private schools.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nFuture affiliate member.\nThe NCAC will have one future affiliate member, a private school.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nFormer members.\nThe NCAC has three former full members, all private schools.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nMembership timeline.\n\u00a0Full member (all sports)\u00a0\n\u00a0Full member (non-football)\u00a0\n\u00a0Associate member (football)\u00a0\n\u00a0Associate member (sport)\u00a0\nSports.\nConference Sports\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "37222", "revid": "125972", "url": "https://en.wikipedia.org/wiki?curid=37222", "title": "Philip Pullman", "text": "English author (born 1946)\nSir Philip Nicholas Outram Pullman (born 19 October 1946) is an English writer. He is best known for the fantasy trilogy \"His Dark Materials\". The first volume, \"Northern Lights\" (1995), won the Carnegie Medal and later the \"Carnegie of Carnegies\". The third volume, \"The Amber Spyglass\" (2000), won the Whitbread Award. In 2003, \"His Dark Materials\" ranked third in the BBC's The Big Read, a poll of 200 top novels voted by the British public. In 2017, he started a companion trilogy, \"The Book of Dust\". As of 2025, the books in the two trilogies plus related short stories have sold more than 49 million copies in total.\nIn 2008, \"The Times\" named Pullman one of the \"50 greatest British writers since 1945\". In a 2004 BBC poll, he was named the eleventh most influential person in British culture. He was knighted in the 2019 New Year Honours for services to literature. Michael Morpurgo said: \u201cThe range and depth of his imagination and of his learning certainly make him the Tolkien of our day.\u201d\nEarly life and education.\nPhilip Pullman was born in Norwich, England, the son of Audrey Evelyn Pullman (n\u00e9e Merrifield) and Royal Air Force pilot Alfred Outram Pullman. The family travelled with his father's job, including to Southern Rhodesia, though most of his formative years were spent in Llanbedr in Ardudwy, Wales.\nIn 1954, when Pullman was seven, his father, an RAF pilot, was killed in a plane crash in Kenya, being posthumously awarded the Distinguished Flying Cross (DFC). In an exchange with a journalist in 2008, Pullman said that, as a boy, he saw his father as \"a hero, steeped in glamour, killed in action defending his country\", and who had been \"training pilots\". Pullman was then presented with a report from \"The London Gazette\" of 1954 \"which carried the official RAF news of the day [and] said that the medal was given for 'gallant and distinguished service' during the Mau Mau uprising.\" Responding to that new information, Pullman wrote: \"My father probably doesn't come out of this with very much credit, judged by the standards of modern liberal progressive thought\", and he accepted the revelation as \"a serious challenge to his childhood memory.\" His mother remarried the following year and they moved to North Wales. He remembers her reading him \"Just So Stories\": \"Kipling\u2019s rhythms must have got into my memory\". His favorite childhood book was Erich K\u00e4stner\u2019s \"Emil and the Three Twins\", \"which was the sequel to his great \"Emil and the Detectives\". It was only much later that I realised why that book had such a deep effect on me: like mine, Emil\u2019s mother had been widowed, and he didn\u2019t want her to marry again. I had no idea of the parallel then.\" Pullman discovered comics, including Superman and Batman, and continues to enjoy the medium, citing Herg\u00e9's \"Adventures of Tintin\" as an influence.xxviii\nHe attended Taverham Hall School and Eaton House and, from 1957, was educated at Ysgol Ardudwy in Harlech, Gwynedd, spending time in Norfolk with his grandfather, a clergyman. When he was \"twelve or thirteen\" he heard older students reciting T. S. Eliot's \"Journey of the Magi\": \"It intoxicated me. That was one of the moments I realized poetry was going to be very important to me. It had a physical effect on me.\" Poetry taught him that words have \"weight and colour and taste and shape as well as meaning.\" A few years later, Pullman discovered John Milton's \"Paradise Lost\", which would become a major influence on \"His Dark Materials\": \"I found, in that classroom so long ago, that it had the power to stir a physical response: my heart beat faster, the hair on my head stirred, my skin bristled. Ever since then, that has been my test for poetry, just as it was for A. E. Housman, who dared not think of a line of poetry while he was shaving, in case he cut himself.\" Other influences include Homer, Virgil and Dante.\nAs a teenager, he discovered Donald Allen\u2019s \"The New American Poetry 1945-1960\": \"This 1960 anthology burst into my life when I was 16 and changed the course of everything for me. Allen Ginsberg's 'Howl' was part of it; I had no idea poetry could do anything like that.\" Ginsberg led him to William Blake: \"My mind and my body reacted to certain lines from the \"Songs of Innocence and of Experience\", from \"The Marriage of Heaven and Hell\", from 'Auguries of Innocence', from \"Europe\", from \"America\" with the joyful immediacy of a flame leaping to meet a gas jet. What these things meant I didn\u2019t quite know then, and I\u2019m not sure I fully know now. There was no sober period of reflection, consideration, comparison, analysis: I didn\u2019t have to work anything out. I knew they were true in the way I knew that I was alive.\" Influenced by Bob Dylan, he wrote poems and songs (\"Thank God none of them were recorded.\")\nFrom 1965, Pullman attended Exeter College, Oxford, receiving a Third Class BA in 1968. In an interview with \"The Oxford Student\", he noted that he \"did not really enjoy the English course\", and that \"I thought I was doing quite well until I came out with my third class degree and then I realised that I wasn't \u2013 it was the year they stopped giving fourth class degrees otherwise I'd have got one of those\".\nPullman married Judith Speller in 1970 and they have two sons. At the time of his marriage he began teaching children aged 9 to 13 at Bishop Kirk Middle School in Summertown, North Oxford, where he also wrote school plays. He recalls retelling classics for his students: \"My real purpose in telling them stories was to practice telling stories. And I practiced on the greatest model of storytelling we\u2019ve got, which is \"The Iliad\" and \"The Odyssey\".\nWriting.\nHis debut novel, \"The Haunted Storm\" (1972) was joint-winner of the New English Library's Young Writer's Award, but he refuses to discuss it. He followed it with \"Galatea\", (1978) an adult fantasy. \"Kirkus Reviews\" wrote: \"Pullman is not without ideas or talent; both shine often enough through this grandiose muddle to make one wonder what he'll do next.\" His school plays inspired his first children's book, \"Count Karlstein\" (1982). He stopped teaching shortly after the publication of \"The Ruby in the Smoke\" (1985), a Victorian mystery and the first book in the Sally Lockhart quartet, followed by \"The Shadow in the North\" (1986), \"The Tiger in the Well\" (1990) and \"The Tin Princess\" (1994). He collaborated with David Mostyn on \"Spring-Heeled Jack\" (1989), a combination of graphic novel and text based on a penny dreadful character. \"Publishers Weekly\" wrote: \"this waggish, innovative story of a courageous trio is sure to engage even the most reluctant reader.\" He wrote the realist novel \"The Broken Bridge\" (1990), \"a love letter to that landscape of North Wales.\"\nBetween 1988 and 1996, Pullman taught part-time at Westminster College, Oxford, continuing to write children's stories. He began \"His Dark Materials\" in about 1993. The first book, \"Northern Lights\", was published in 1995 (as \"The Golden Compass\" in the U.S. in 1996). While working on the trilogy, he wrote \"The Firework-Maker's Daughter\" (1995), \"Clockwork, or All Wound Up\" (1996) and \"I Was a Rat! or, The Scarlet Slippers\" (1999), which he called fairy tales. \"The Firework-Maker's Daughter\" won the Gold Nestl\u00e9 Smarties Book Prize. The trilogy continued with \"The Subtle Knife\" (1997) and \"The Amber Spyglass\" (2000).\nPullman has been writing full-time since 1996. He continues to deliver talks and writes occasionally for \"The Guardian\", including writing and lecturing about education, in which he is often critical of unimaginative education policies. He was awarded a CBE in the New Year's Honours list in 2004. That year, he was elected President of the Blake Society and guest-edited \"The Mays Literary Anthology\", a collection of new writing from students at the Universities of Oxford and Cambridge. He returned to fairy tales with \"The Scarecrow and His Servant\" (2004), which won the Silver Smarties Prize.\nIn 2008, he started working on \"The Book of Dust\", a companion trilogy to \"His Dark Materials\", and \"The Adventures of John Blake\", a story for the British children's comic \"The DFC\", with artist John Aggs.\nIn 2012, during a break from writing \"The Book of Dust\", Pullman was asked by Penguin Classics to curate 50 of Grimms' classic fairytales, from their compendium of over 200 stories. \"They are not all of the same quality\", said Pullman. \"Some are easily much better than others. And some are obvious classics. You can't do a selected Grimms' without Rumpelstiltskin, Cinderella and so on.\" In 2017, a collection of his lectures and essays were published as \"Daemon Voices: On Stories and Storytelling\".\n\"His Dark Materials\".\n\"His Dark Materials\" is a trilogy consisting of \"Northern Lights\" (titled \"The Golden Compass\" in North America), \"The Subtle Knife\" and \"The Amber Spyglass\". \nThe trilogy's title comes from Book II of \"Paradise Lost\". \n\"Northern Lights\" takes place in a parallel universe where peoples' souls are embodied in animals called \"d\u00e6mons\". Pullman was influenced by Socrates's daimon, described in Plato's \"Apology of Socrates\". The trilogy centres around Lyra Belacqua, a girl initially growing up in Jordan College, Oxford. \"Northern Lights\" won both the annual Carnegie Medal and the Guardian Children's Fiction Prize, a similar award that authors may not win twice. \"The Subtle Knife\" introduces Will Parry, a boy from our universe. \"The Amber Spyglass\" moves across several universes. It was awarded both 2001 Whitbread Prize for best children's book and the Whitbread Book of the Year prize in January 2002, the first children's book so honoured. In 2003, it placed third in the BBC's Big Read poll. Pullman has written three companion pieces to the trilogy: \"Lyra's Oxford\" (2003), \"Once Upon a Time in the North\" (2008) and \"Serpentine\" (2020). He refers to another, which will expand the character Will Parry, as the \"green book\".\nPullman has narrated unabridged audiobooks of the three novels in the \"His Dark Materials\" trilogy; with a full cast.\n\"The Book of Dust\".\n\"The Book of Dust\" includes characters and events from \"His Dark Materials\". Pullman has said that the new series is neither sequel, nor prequel, but an \"equel\".\n\"La Belle Sauvage\", the first volume of \"The Book of Dust\", was published by Penguin Random House Children's and David Fickling in the UK and by Random House Children's in the US in 2017. A sequel, \"The Secret Commonwealth\", was published in October 2019. It includes a character named after Nur Huda el-Wahabi, a 16-year-old victim of London's Grenfell Tower fire. As part of the charity auction Authors for Grenfell Tower, Pullman offered the highest bidder a chance to name a character in the upcoming trilogy. Ultimately, he raised \u00a332,400. The third and final book in the trilogy, \"The Rose Field\", was released on 23 October 2025.\nStyle, themes and influences.\nIn an epigraph to \"The Amber Spyglass\", Pullman wrote that: \"My principle when researching a novel is 'Read like a butterfly, write like a bee', and if this story contains any honey, it is entirely because of the quality of the nectar that I found in the work of better writers. But there are three debts that need to be acknowledged above the rest. One is to the essay 'On the Marionette Theatre' by Heinrich von Kleist, which I first read in translation by Idris Parry in the \"Times Literary Supplement\" in 1978. The other is to John Milton's \"Paradise Lost\". The third is to the works of William Blake.\" He credits his teacher Enid Jones with \"showing him that responsibility and delight can coexist.\"\nChristina Patterson writes that \"\"The Firework-Maker's Daughter\" is both an adventure story and an extended metaphor for the making of art. \"Clockwork\" is a gothic fantasy with a sinister twist, which draws heavily on German Romanticism. It's also a philosophical parable, playing with notions of free will, cause and effect. \"I Was a Rat!\" is a rollicking romp about Roger the rat-boy, but - as the title implies - it's also a brilliant parody of the sleazier reaches of journalism.\" Pullman says \"What I hope is that the stories I write will entertain both the young readers and the older ones. What I don't want to do is to write the sort of book that has silly slapstick for children and clever stuff for the grown-ups. I want them all to enjoy the same bits for the same reason - but maybe see different things in it.\nHe has incorporated science into his writing. \"His Dark Materials\" draws on the many worlds interpretation of quantum theory and the concept of dark matter. \"I found dark matter a very helpful metaphor, and I had my fingers crossed since 1993 that they wouldn't discover what it was before I finished. And they still haven't.\" Another theme is the nature of consciousness. He says that \"Science is clearly a field where the imagination can be triumphant\" but that there are things that lie beyond it: \"I think a lot of the things that science is either dubious about or skeptical about or refuses to have anything to do with are these qualities which are so well expressed in literature or music or poetry or the visual arts. The sort of gung-ho triumphalist proponents of science will say 'That's because we haven't got there yet. We'll measure it, we'll do it one day. ... I'd point out, we've got there already. You read it in Shelley and Keats and Shakespeare, you hear it in Stravinsky and Debussy.\"\nCampaigns and views.\nPullman has been a vocal campaigner on a number of issues related to books and politics.\nViews on fantasy.\nIn a lecture at the Sea of Faith conference, Pullman said that \"the writers we call the greatest of all \u2013 Shakespeare, Tolstoy, Proust, George Eliot herself, are those who have created the most lifelike simulacra of real human beings in real human situations. In fact the more profound and powerful the imagination, the closer to reality are the forms it dreams up.\" He said he wanted to write fantasy realistically, or write fantastic characters with psychological depth: \"Because when I thought about it, there was no reason why fantasy shouldn't be realistic, in a psychological sense \u2013 and it was the lack of that sort of realism that I objected to in the work of the big Tolkien and all the little Tolkiens.\" He says David Lindsay's \"A Voyage to Arcturus\" \"shows that fantasy is capable of saying big and important things.\" He concludes that fantasy is \"a great vehicle when it serves the purposes of realism, and a lot of old cobblers when it doesn't.\" Pullman says that he sees \"His Dark Materials\" as \"stark realism\", not fantasy. He has praised fantasy authors like Alan Garner.\nViews on children's literature.\nPullman believes that children deserve quality literature, and that there isn't a clear demarcation between children's and adult literature. In a talk at the Royal Society of Literature, he quoted C. S. Lewis in \"On Three Ways of Writing for Children\": \"I now like hock, which I am sure I should not have liked as a child. But I still like lemon-squash. I call this growth or development because I have been enriched: where I formerly had only one pleasure, I now have two.\" Pullman said that: \"It would be nice to think that normal human curiosity would let us open our minds to experience from every quarter, to listen to every storyteller in the marketplace. It would be nice too, occasionally, to read a review of an adult book that said, 'This book is so interesting, and so clearly and beautifully written, that children would enjoy it as well.'\" \nHe is an admirer of Philippa Pearce; when Pullman's \"Northern Lights\" won the Carnegie of Carnegies, Pearce's \"Tom's Midnight Garden\" was the runner-up. Pullman said: \"Personally, I feel they got the initials right but not the name. I don't know if the result would be the same in a hundred years' time; maybe Philippa Pearce would win then.\" In 2011, Pullman gave the Philippa Pearce Lecture.\nHe is also an admirer of Leon Garfield, \"someone who put the best of his imagination into everything he wrote\", particularly praising \"The Pleasure Garden\". \nIn a lecture, he said that \"one of the things we need to do for children is introduce them to the pleasures of the subtle and complex. One way to do that, of course, is to let them see us enjoying it, and then forbid them to touch it, on the grounds that it's too grown-up for them, their minds aren't ready to cope with it, it's too strong, it'll drive them mad with strange and uncontrollable desires. If that doesn't make them want to try it, nothing will.\"\nViews on poetry.\nHe writes: \u201cThe experience of reading poetry aloud when you don\u2019t fully understand it is a curious and complicated one. It\u2019s like suddenly discovering that you can play the organ. Rolling swells and peals of sound, powerful rhythms and rich harmonies are at your command; and as you utter them you begin to realise that the sound you\u2019re releasing from the words as you speak is part of the reason they\u2019re there. The sound is part of the meaning and that part only comes alive when you speak it. ...\nWe need to remind ourselves of this, especially if we have anything to do with education. I have come across teachers and student teachers whose job was to teach poetry, but who thought that poetry was only a fancy way of dressing up simple statements to make them look complicated, and that their task was to help their pupils translate the stuff into ordinary English. ... No one had told such people that poetry is in fact enchantment; that it has the form it does because that very form casts a spell; and that when they thought they were bothered and bewildered, they were in fact being bewitched, and if they let themselves accept the enchantment and enjoy it, they would eventually understand much more about the poem.\u201d Of Elizabeth Bishop he writes: \u201dHow simple some great poetry can seem - as simple as water, and as necessary.\u201d\nViews on fairy tales.\nHe disagrees with Richard Dawkins that fairy tales would lead children to believe in magic. Citing evidence by Gordon Wells, he writes of the importance of reading to children: \u201cMy guess is that the kind of stories children are offered has far less effect on their development than whether they are given stories at all; and that children whose parents take the trouble to sit and read with them \u2013 and talk about the stories, not in a lecturing sort of way but genuinely conversing, in the way that Wells describes \u2013will grow up to be much more fluent and confident not only with language but with pretty well any kind of intellectual activity, including science. And children who are deprived of this contact, this interaction, the world of stories, are not likely to flourish at all. What sort of evidence that is, I don\u2019t know, but I believe it.\u201d\nViews on monarchy.\nIn 2002, to coincide with the Golden Jubilee of Queen Elizabeth II, Pullman was interviewed for a feature in \"The Guardian\" on notable republicans. According to Pullman, \"The present system is unsustainable, because it is cruel. No individual and no family should be subject to the pressures of publicity and expectation that have beset the Windsors.\" Expressing sympathy for the young Prince William, Pullman added, \"we can't have a quiet, sensible, unobtrusive sort of monarchy because of the mistakes the Windsors have made, and because of the disgusting and unredeemable nature of the tabloid press; so we shall have to have a republic. The one thing to avoid is a political president. Let's have a well-respected figure from some other walk of life, and leave politics to the prime minister and parliament.\" In 2010, \"The Atlantic\" described Pullman's Jesus in \"The Good Man Jesus and the Scoundrel Christ\" as \"a proper republican in the Pullman sense of the word: instinctively fraternal and anti-institutional, spreading his rough-and-ready enlightenments across the horizontal axis.\"\nAge and gender labelling of books.\nIn 2008, Pullman led a campaign against the introduction of age bands on the covers of children's books, saying: \"It's based on a one-dimensional view of growth, which regards growing older as moving along a line like a monkey climbing a stick: now you're seven, so you read these books; and now you're nine so you read these.\" More than 1,200 authors, booksellers, illustrators, librarians and teachers joined the campaign; Pullman's own publisher, Scholastic, agreed to his request not to put the age bands on his book covers. Joel Rickett, deputy editor of \"The Bookseller\", said: \"The steps taken by Mr Pullman and other authors have taken the industry by surprise and I think these proposals are now in the balance.\"\nIn 2014, Pullman supported the Let Books Be Books campaign to stop children's books being labelled as \"for girls\" or \"for boys\", saying: \"I'm against anything, from age-ranging to pinking and blueing, whose effect is to shut the door in the face of children who might enjoy coming in. No publisher should announce on the cover of any book the sort of readers the book would prefer. Let the readers decide for themselves.\"\nCivil liberties.\nPullman has a strong commitment to traditional British civil liberties and is noted for his criticism of growing state authority and government encroachment into everyday life. In February 2009, he was the keynote speaker at the Convention on Modern Liberty in London and wrote an extended piece in \"The Times\" condemning the Labour government for its attacks on basic civil rights. Later, he and other authors threatened to stop visiting schools in protest at new laws requiring them to be vetted to work with youngsters\u2014though officials claimed that the laws had been misinterpreted.\nPublic jury.\nIn July 2011, Pullman was one of the lead campaigners signing a declaration that called for a 1,000-strong \"public jury\", selected at random, to draw up a \"public interest first\" test to ensure that power was taken away from \"remote interest groups\". The declaration was also signed by 56 academics, writers, trade unionists and politicians from the Labour Party, the Liberal Democrats and the Green Party.\nLibrary closures.\nIn October 2011, Pullman backed a campaign to stop 600 library closures in England, calling it a \"war against stupidity\". London Borough of Brent claimed that it was closing half of its libraries to fulfil its \"exciting plans\" to improve its library service. Pullman said: \"All the time, you see, the council had been longing to improve the library service, and the only thing standing in the way was \u2013 the libraries.\"\nSpeaking at a conference organised by The Library Campaign and Voices for the Library, he added: The book is second only to the wheel as the best piece of technology human beings have ever invented. A book symbolises the whole intellectual history of mankind; it's the greatest weapon ever devised in the war against stupidity. Beware of anyone who tries to make books harder to get at. And that is exactly what these closures are going to do \u2013 oh, not intentionally, except in a few cases; very few people are stupid intentionally; but that will be the effect. Books will be harder to get at. Stupidity will gain a little ground.\nEbook library loans.\nIn advance of becoming president of the Society of Authors in August 2013, Pullman led a call for authors to be fairly paid for ebook library loans. Under arrangements in force at the time, authors were paid 6p per library loan by the government for physical books, but nothing for ebook loans. In addition, the Society found that publishers had possibly been inadvertently underpaying authors for ebook loans. Altogether, this may have resulted in authors losing up to two-thirds of the income they would have received on the sale and loan of a physical book. Addressing this issue, Pullman said: New media and new forms of buying and lending are all very interesting, for all kinds of reasons, but one principle remains unchanged: authors must be paid fairly for their work. Any arrangement that doesn't acknowledge that principle is a bad one, and needs to be changed. That is our whole argument.\nWilliam Blake's cottage and memorial stone.\nAs a long-time enthusiast of William Blake, and president of the Blake Society, Pullman led a campaign in 2014 to buy the Sussex cottage where the poet lived between 1800 and 1803, saying: Surely it isn't beyond the resources of a nation that can spend enormous amounts of money on acts of folly and unnecessary warfare, a nation that likes to boast about its literary heritage, to find the money to pay for a proper memorial and a centre for the study of this great poet and artist. Not least because this is the place where he wrote the words now often sung as an alternative (and better) national anthem, the poem known as Jerusalem: \"And did those feet in ancient time\". Blake's feet walked in Felpham. Let's not let this opportunity pass by.\nAs president of the Blake Society, on 11 August 2018, Pullman inaugurated Blake's new memorial gravestone on the site of his grave in Bunhill Fields, following a long campaign by the society.\nBoycott of Brexit 50p coin.\nIn January 2020, Pullman called for literate people to boycott the newly minted Brexit 50p coin due to the omission of the Oxford comma in its slogan \"Peace, prosperity and friendship with all nations\". The viewpoint was supported by some, while lexicographer Susie Dent indicated it was optional and Baroness Bakewell said she had been \"taught that it was wrong to use the comma in such circumstances\".\nPresidency of the Society of Authors.\nIn 2013, Pullman was elected President of the Society of Authors \u2013 the \"ultimate honour\" awarded by the British writers' body, and a position first held by Alfred, Lord Tennyson. In January 2016, Pullman resigned as patron of the Oxford Literary Festival in support of the Society of Authors' campaign for writers to be paid fees at festivals and drew attention to the poor remuneration of writers.\nOn 10 August 2021, Pullman tweeted a response to what he wrongly thought was criticism of Kate Clanchy's teaching memoir \"Some Kids I Taught and What They Taught Me\". His tweet said that those who condemn a book without reading it would be at home with \"Boko Haram and the Taliban.\" Pullman later deleted the tweet and apologised. On the 11th of August The Society of Authors put out a statement and an interview with Chair of Management Committee Joanne Harris which were described by \"The Guardian\" as the society \"distancing\" itself from Pullman. Pullman resigned his presidency, later stating that the management committee urging him to apologise for something he hadn't done had been a factor in his decision to stand down. He later criticised Harris for her \"facetious and flippant\" public comments and stated that the Society of Authors had become a \"vehicle for gesture politics\" and called for external review and reform of the organisation.\nPerspective on religion.\nPullman speaks of replacing The Kingdom of Heaven with \u201cThe Republic of Heaven\u201d:\nWe have to realize that our human nature demands meaning and joy just as Jane Eyre demanded love and kindness (\u201cYou think we can live without them, but we cannot live so\u201d); to accept that this meaning and joy will involve a passionate love of the physical world, \"this\" world, of food and drink and sex and music and laughter, and not a suspicion and hatred of it; to understand that it will both grow out of and add to the achievements of the human mind such as science and art. ... In the republic, we\u2019re connected in a moral way to one another, to other human beings. We have responsibilities to them, and they to us. We\u2019re not isolated units of self-interest in a world where there is no such thing as society; we cannot live so.\u201d\nHe writes of a myth for the republic:\u201cOf course, there are two kinds of \"why\", and our story must deal with both. There\u2019s the one that asks \"What brought us here?\" and the other that asks \"What are we here for?\" One looks back, and the other looks forward, perhaps. And in offering an answer to the first why, a republican myth must accept the overwhelmingly powerful evidence for evolution by natural selection. The neo-Darwinians tell us that the processes of life are blind and automatic; there has been no purpose in our coming here. Well, I think a republican response to that would be: \"there is now.\" We are conscious, and conscious of our own consciousness. We might have arrived at this point by a series of accidents, but from now on we have to take charge of our fate. Now we are here, now we are conscious, we make a difference. Our presence changes everything. So a myth of the republic of Heaven would explain what our true purpose is. Our purpose is to understand and to help others to understand, to explore, to speculate, to imagine. And that purpose has a moral force.\u201d\nIn \"The Chronicles of Narnia\", he objects to Susan's exclusion from Narnia because of her interest in \"lipstick and nylons and invitations.\": \u201cIn other words, normal human development, which includes a growing awareness of your body and its effect on the opposite sex, is something from which Lewis\u2019s narrative, and what he would like us to think is the Kingdom of Heaven, turns with horror. The ending of \"The Last Battle\" makes this position even clearer. 'The term is over: the holidays have begun,' says Aslan to the children, having just let them know that 'there \"was\" a real railway accident...Your father and mother and all of you are \u2014 as you used to call it in the Shadowlands \u2014 dead.' Using Narnia as our moral compass, we can take it as axiomatic that in the republic of Heaven, people do not regard life in this world as so worthless and contemptible that they leave it with pleasure and relief, and a railway accident is not an end-of-term treat.\u201d He adds, \u201cIt's not the presence of Christian doctrine I object to so much as the absence of Christian virtue. The highest virtue - we have on the authority of the New Testament itself - is love, and yet you find not a trace of that in the books.\u201d\nAlthough Pullman has stated he is \"a Church of England atheist, and a 1662 \"Book of Common Prayer\" atheist, because that's the tradition I was brought up in\", he has also said he is technically an agnostic. He has singled out elements of Christianity for criticism: \"if there is a God, and he is as the Christians describe him, then he deserves to be put down and rebelled against.\" He has also acknowledged that the same could be said of all religions.\nPullman has also referred to himself as knowingly \"of the Devil's party\", a reference to William Blake's revisionist view of Milton in \"The Marriage of Heaven and Hell\". \nPullman is a supporter of Humanists UK and an Honorary Associate of the National Secular Society. In 2011, he was given a services to Humanism award by the British Humanist Association for his contribution as a longstanding supporter.\nOn 15 September 2010, Pullman, along with 54 other public figures (including Stephen Fry, Professor Richard Dawkins, Terry Pratchett, Jonathan Miller and Ken Follett), signed an open letter published in \"The Guardian\" stating their opposition to Pope Benedict XVI being given \"the honour of a state visit\" to the UK; the letter argued that the Pope had led and condoned global abuses of human rights, leading a state which has \"resisted signing many major human rights treaties and has formed its own treaties (\"concordats\") with many states which negatively affect the human rights of citizens of those states\".\nLaura Miller described Pullman as one of England's most outspoken atheists. He has characterised atheist totalitarian regimes as religions.\nAlan Jacobs (of Wheaton College) said that in \"His Dark Materials\" Pullman replaced the theist world-view of John Milton's \"Paradise Lost\" with a Rousseauist one.\nThe books in the series have been criticised for their attitude to religion, especially Catholicism, by the Catholic League for Religious and Civil Rights and Focus on the Family. Writing in the \"Catholic Herald\" in 1999, Leonie Caldecott cited Pullman's work as an example of fiction \"far more worthy of the bonfire than Harry [Potter]\" on the grounds that \"[by] co-opting Catholic terminology and playing with Judaeo-Christian theological concepts, Pullman is effectively removing, among a mass audience of a highly impressionable age, some of the building blocks for future evangelisation\". Pullman was flattered and asked his publisher to include quotes from Caldecott's article in his next book. In 2002, the \"Catholic Herald\" published an article by Sarah Johnson that compared Pullman to a \"playground bully\" whose work \"attacks a religious minority\". The following year, after Benedict Allen's reference to the criticism during the BBC TV series \"The Big Read\", the \"Catholic Herald\" republished both articles and Caldecott claimed her \"bonfire\" comment was a joke and accused Pullman and his supporters of quoting her out of context. In a longer article for \"Touchstone\" magazine earlier in 2003, Caldecott had also described Pullman's work as \"axe-grinding\" and \"a kind of Luciferian enterprise\".\nColumnist Peter Hitchens, in a 2002 article for \"The Mail on Sunday\", accused Pullman of \"killing god\" and described him as \"the most dangerous author in Britain\" because he said in an interview: \"I'm trying to undermine the basis of Christian belief.\" Pullman responded by posting Hitchens' article on his study wall. In that interview, which was for a February 2001 article in \"The Washington Post\", Pullman acknowledged that a controversy would be likely to boost sales, but continued: \"I'm not in the business of offending people. I find the books upholding certain values that I think are important, such as life is immensely valuable and this world is an extraordinarily beautiful place. We should do what we can to increase the amount of wisdom in the world.\" Hitchens also views the \"His Dark Materials\" series as a direct rebuttal of C. S. Lewis's \"The Chronicles of Narnia\"; Hitchens' brother Christopher Hitchens, author of \"God Is Not Great\", praised \"His Dark Materials\" as a fresh alternative to Lewis, J. R. R. Tolkien and J. K. Rowling, describing the author as one \"whose books have begun to dissolve the frontier between adult and juvenile fiction\". However, he was more critical of \"The Good Man Jesus and the Scoundrel Christ\", accusing Pullman of being a \"Protestant atheist\" for supporting the teachings of Christ but being critical of organised religion.\nPullman has found support from some Christians, most notably Rowan Williams, the former Archbishop of Canterbury, who argued that Pullman's attacks focus on the constraints and dangers of dogmatism and the use of religion to oppress, not on Christianity itself. Williams recommended \"His Dark Materials\" for discussion in religious education classes, and said that \"to see large school-parties in the audience of the Pullman plays at the National Theatre is vastly encouraging\". Pullman and Williams took part in a National Theatre platform debate a few days later to discuss myth, religious experience and its representation in the arts.\nDonna Freitas, professor of religion at Boston University, argued that challenges to traditional images of God should be welcomed as part of a \"lively dialogue about faith\". The Christian writers Kurt Bruner and Jim Ware \"also uncover spiritual themes within the books\". Pullman's contribution to the \"Canongate Myth\" series, \"The Good Man Jesus and the Scoundrel Christ\", was described by Mike Collett-White as \"a far more direct exploration of the foundations of Christianity and the church as well as an examination of the fascination and power of storytelling\".\nIn a 2017 interview with \"The Times\", Pullman said: \"The place religion has in our lives is a permanent one.\" He concluded that there was \"no point in condemning [religion]\", and mused that it is part of the human mind to ask philosophical questions such as the purpose of life. He reiterated that it was useless to \"become censorious about [religion], to say there is no God\". He also mentioned that \"The Book of Dust\" is based on the \"extreme danger of putting power into the hands of those who believe in some absolute creed, whether that is Christianity or Islam or Marxism\". He says \"The Bible is the most wonderful book \u2013 I wouldn\u2019t be without it. It\u2019s a library of all kinds of stories: poetry, history, mythology, crazy ravings. It\u2019s got it all. Not much humour in it, though.\"\nPersonal life.\nIn October 2009, he became a patron of the Palestine Festival of Literature. He is also a patron of the Shakespeare Schools Festival, a charity that enables school children across the UK to perform Shakespeare in professional theatres.\nA lifelong fan of Norwich City, Pullman penned the foreword to the club's official history, published in 2020. He is an art lover, and wrote an appreciation of \u00c9douard Manet\u2019s \"A Bar at the Folies-Berg\u00e8re\". He is a devotee of Johann Sebastian Bach, and appeared in John Eliot Gardiner\u2019s documentary \"Bach: A Passionate Life\". He muses \u201cSuch splendour and wonderfulness would, on its own, convince me that there was a God if I felt inclined to take that conclusion.\u201d\nHe is an admirer of MacDonald Harris, \"someone who attends to every aspect of the words they're using, not least their weight, their rhythm and their colour.\" He is a fan of Norman Lindsay's \"The Magic Pudding\", which he calls \"the funniest children's book ever written\". \nHe says his favorite book is probably Robert Burton's \"The Anatomy of Melancholy\", describing it as \"a funny book about depression written in a very prolix, ornate style.\" Among contemporary authors, he admires John le Carr\u00e9: \"compared to him, I\u2019m just a \"child.\"\u201d\nAwards and honours.\nHe was a joint-winner of the New English Library's Young Writer's Award in 1972.\nPullman was awarded an Honorary Doctorate in Letters from the University of East Anglia in 2003.\nIn 2005, Pullman won the annual Astrid Lindgren Memorial Award from the Swedish Arts Council, recognising his career contribution to \"children's and young adult literature in the broadest sense\". According to the presentation, \"Pullman radically injects new life into fantasy by introducing a variety of alternative worlds and by allowing good and evil to become ambiguous.\" In every genre, \"he combines storytelling and psychological insight of the highest order.\"\nIn 2006, he was one of five finalists for the biennial, international Hans Christian Andersen Medal, and he was the British nominee again in 2012.\nPullman was awarded an honorary degree from the University of Dundee in June 2007. He received the degree at the inaugural Dundee Literary Festival, where he was a featured author.\nOn 23 November 2007, Pullman was made an honorary professor at Bangor University.\nOn 24 June 2009, Pullman was awarded the degree of D.Litt. (Doctor of Letters), \"honoris causa\", by the University of Oxford at the Enc\u00e6nia ceremony in the Sheldonian Theatre.\nPullman was named a Knight Bachelor in the 2019 New Year's Honours list. In March 2019, the charity Action for Children's Art presented Pullman with their annual J. M. Barrie Award to mark a \"lifetime's achievement in delighting children\".\n\"Northern Lights\", was published in 1995 (entitled \"The Golden Compass\" in the U.S., 1996). Pullman won both the annual Carnegie Medal and the Guardian Children's Fiction Prize, a similar award that authors may not win twice.\nIn 2001 he was elected a Fellow of the Royal Society of Literature. He was awarded a CBE in the New Year's Honours list in 2004. In 2004, he was elected President of the Blake Society. In 2013, he was awarded a Honorary Doctorate by the University of Bath.\nBibliography.\nChildren's short stories.\nNovellas:\nCollections:\nVisual Artwork.\nFor the \"Lantern Slides\" editions of the His Dark Materials books, Pullman contributed one illustration for each chapter of the trilogy. Initially the publisher inquired if there shouldn't be a decorative element at the head of each chapter, to which Pullman agreed, but campaigned for a unique image for each chapter rather than repeating the same. Pullman reports, \"I then asked if I could draw them. 'But you\u2019re not an artist', he (his publisher) pointed out. I said I could draw, and he challenged me to prove it. So I went away and had a go. I knew the illustrations would be reproduced very small, but after some experimenting I found a way of doing it that involved solid blacks that wouldn\u2019t get lost in the printing.\"\nIn 2023, these illustrations were published as fine art limited-edition letterpress prints by Electric Works in San Francisco, California. Editions were limited to 100 of eighteen different individual images. In addition, a broadside comprising all 76 chapters of the entire trilogy was also published in an edition of 250.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "37223", "revid": "16886228", "url": "https://en.wikipedia.org/wiki?curid=37223", "title": "His Dark Materials", "text": "Novel trilogy by Philip Pullman\nHis Dark Materials is a trilogy of fantasy novels by Philip Pullman consisting of \"Northern Lights\" (1995; published as \"The Golden Compass\" in North America), \"The Subtle Knife\" (1997), and \"The Amber Spyglass\" (2000). It follows the coming of age of two children, Lyra Belacqua and Will Parry, as they wander through a series of parallel universes. The novels have won a number of awards, including the Carnegie Medal in 1995 for \"Northern Lights\" and the 2001 Whitbread Book of the Year for \"The Amber Spyglass\". In 2003, the trilogy was ranked third on the BBC's The Big Read poll.\nAlthough \"His Dark Materials\" has been marketed as young adult fiction, and the central characters are children, Pullman wrote with no target audience in mind. The fantasy elements include witches and armoured polar bears; the trilogy also alludes to concepts from physics, philosophy, and theology. It functions in part as a retelling and inversion of John Milton's epic \"Paradise Lost\", with Pullman commending humanity for what Milton saw as its most tragic failing, original sin. The trilogy has attracted controversy for its criticism of religion. By 2024, more than 22 million copies of the novels had been sold in 50 countries, and they had been translated into 40 languages.\nThe books have been dramatised several times. BBC Radio 4 produced a three-part full-cast dramatisation in 2003 as did RT\u00c9 the same year. The London Royal National Theatre staged a two-part adaptation of the trilogy in 2003\u20132004. New Line Cinema released a film adaptation of \"Northern Lights\", \"The Golden Compass\", in 2007. A BBC commissioned television series, based on the trilogy and produced by Bad Wolf, was broadcast by both the BBC and HBO between November 2019 and February 2023.\nPullman followed the trilogy with four short works set in the \"Northern Lights\" universe: \"Lyra's Oxford\", (2003); \"Once Upon a Time in the North\", (2008); \"The Collectors\" (2014); and the latest \"Serpentine\", (2020). A new trilogy, also set in the same universe as \"Northern Lights\", titled \"The Book of Dust\", was published beginning 19 October 2017 with the release of the first novel \"La Belle Sauvage\"; the second book, \"The Secret Commonwealth\", was released in October 2019; the final novel, \"The Rose Field\", was published on 23 October 2025.\nSetting.\nThe trilogy takes place across a multiverse, moving between many parallel worlds. In \"Northern Lights\", the story takes place in a world with some similarities to our own: dress-style resembles that of the UK's Edwardian era; the technology does not include cars or fixed-wing aircraft, but zeppelins feature as a mode of transport.\nThe dominant religion has parallels with Christianity. The Church (governed by the \"Magisterium\", the same name as the authority of the Catholic Church) exerts a strong control over society and has some of the appearance and organisation of the Catholic Church, but one in which the centre of power had been moved from Rome to Geneva, moved there by Pullman's fictional \"Pope John Calvin\" (Geneva was the home of the historical John Calvin).\nIn \"The Subtle Knife\", the story moves between our own world, the world of the first novel, and a third world containing the city of Citt\u00e0gazze. In \"The Amber Spyglass\", several other worlds appear alongside those three.\nTitles.\nThe title of the series comes from 17th-century poet John Milton's \"Paradise Lost\":\n&lt;poem&gt;\nInto this wilde Abyss,\nThe Womb of nature and perhaps her Grave,\nOf neither Sea, nor Shore, nor Air, nor Fire,\nBut all these in their pregnant causes mixt \nConfus'dly, and which thus must ever fight,\nUnless th' Almighty Maker them ordain\nHis dark materials to create more Worlds,\nInto this wilde Abyss the warie fiend\nStood on the brink of Hell and look'd a while,\nPondering his Voyage; for no narrow frith\nHe had to cross.\n&lt;/poem&gt;\n\u2014 \"Paradise Lost\", Book 2, lines 910\u2013920\nPullman chose this particular phrase from Milton because it echoed the dark matter of astrophysics.\nPullman earlier proposed to name the series \"The Golden Compasses\", also a reference to \"Paradise Lost\", where it denotes the pair of compasses with which God set the bounds of all creation:\n&lt;poem&gt;\nThen staid the fervid wheels, and in his hand\nHe took the golden compasses, prepared\nIn God's eternal store, to circumscribe\nThis universe, and all created things:\nOne foot he centred, and the other turned\nRound through the vast profundity obscure...\n&lt;/poem&gt;\n\u2014 \"Paradise Lost\", Book 7, lines 224\u2013229\nAlthough Pullman did not intend it as such, the American publishers interpreted this title as a reference to the alethiometer, a compass-like device that features prominently in the books. Pullman eventually settled on the titles \"His Dark Materials\" for the series and \"Northern Lights\" for the first book, but the American publishers disliked the latter title and chose to use \"The Golden Compass\" instead.\nPlot.\n\"Northern Lights\" (or \"The Golden Compass\").\nIn Jordan College, Oxford, 11-year-old Lyra Belacqua and her d\u00e6mon Pantalaimon witness the Master attempt to poison Lord Asriel, Lyra's rebellious and adventuring uncle. She warns Asriel, then spies on his lecture about Dust, mysterious elementary particles. Lyra's friend Roger is kidnapped by child abductors known as Gobblers. Lyra is adopted by a charming socialite, Mrs Coulter. The Master secretly entrusts Lyra with an alethiometer, a truth-telling device. Lyra discovers that Mrs Coulter is the leader of the Gobblers, and that it is a project secretly funded by the Church. Lyra flees to the Gyptians, canal-faring nomads, whose children have also been abducted. They reveal to Lyra that Asriel and Mrs Coulter are actually her parents.\nThe Gyptians form an expedition to the Arctic with Lyra to rescue the children. Lyra recruits Iorek Byrnison, an armoured bear, and his human aeronaut friend, Lee Scoresby. She also learns that Lord Asriel has been exiled, guarded by the bears on Svalbard.\nNear Bolvangar, the Gobbler research station, Lyra finds an abandoned child who has been cut from his d\u00e6mon; the Gobblers are experimenting on children by severing the bond between human and d\u00e6mon, a procedure called intercision.\nLyra is captured and taken to Bolvangar, where she is reunited with Roger. Mrs Coulter tells Lyra that the intercision prevents the onset of troubling adult emotions. Lyra and the children are rescued by Scoresby, Iorek, the Gyptians, and Serafina Pekkala's flying witch clan. Lyra falls out of Scoresby's balloon and is taken by the panserbj\u00f8rne to the castle of their usurping king, Iofur Raknison. She tricks Iofur into fighting Iorek, who arrives with the others to rescue Lyra. Iorek kills Iofur and takes his place as the rightful king.\nLyra, Iorek, and Roger travel to Svalbard, where Asriel has continued his Dust research in exile. He tells Lyra that the Church believes Dust is the basis of sin, and plans to visit the other universes and destroy its source. He severs Roger from his d\u00e6mon, killing him and releasing enough energy to create an opening to a parallel universe. Lyra resolves to stop Asriel and discover the source of Dust for herself.\n\"The Subtle Knife\".\nLyra journeys through Asriel's opening between worlds to Citt\u00e0gazze, a city whose denizens discovered a way to travel between worlds. Citt\u00e0gazze's reckless use of the technology has released Spectres which destroy adult souls but to which children are immune, rendering the world empty of adults. Here Lyra meets and befriends Will Parry, a twelve-year-old boy from our world's Oxford. Will, who recently killed a man to protect his ailing mother, has stumbled into Citt\u00e0gazze in an effort to locate his long-lost father. Venturing into Will's (our) world, Lyra meets Dr. Mary Malone, a physicist who is researching dark matter, which is analogous to Dust in Lyra's world. Lyra encourages Dr. Malone's attempts to communicate with the particles, and when the physicist does they tell her to travel into the Citt\u00e0gazze world. Lyra's alethiometer is stolen by Lord Boreal, alias Sir Charles Latrom, an ally of Mrs Coulter who has found a way to Will's Oxford and established a home there.\nWill becomes the bearer of the Subtle Knife, a tool forged three hundred years before by Citt\u00e0gazze's scientists from the same alloy used to make the guillotine in Bolvangar. One edge of the knife can divide subatomic particles and form subtle divisions in space, creating portals between worlds; the other edge easily cuts through any form of matter. Using the knife's portal-creating powers, Will and Lyra are able to retrieve her alethiometer from Latrom's mansion in Will's world.\nMeanwhile, in Lyra's world, Lee Scoresby seeks out the Arctic explorer Stanislaus Grumman, who years before entered Lyra's world through a portal in Alaska. Scoresby finds him living as a shaman under the name Jopari and he turns out to be Will's father, John Parry. Parry insists on being taken through the opening into the Citt\u00e0gazze world in Scoresby's balloon, since he has foreseen that he should meet the wielder of the Subtle Knife there. In that world, Scoresby dies defending Parry from the forces of the Church, while Parry succeeds in reuniting with his son moments before being murdered by Juta Kamainen, a witch whose love John had once rejected. After his father's death, Will discovers that Lyra has been kidnapped by Mrs Coulter, and he is approached by two angels requesting his aid.\n\"The Amber Spyglass\".\nAt the beginning of \"The Amber Spyglass,\" Lyra has been kidnapped by her mother, Mrs Coulter, an agent of the Magisterium who has learned of the prophecy identifying Lyra as the next Eve. A pair of angels, Balthamos and Baruch, tell Will that he must travel with them to give the Subtle Knife to Lyra's father, Lord Asriel, as a weapon against The Authority. Will ignores the angels; with the help of a local girl named Ama, the Bear King Iorek Byrnison, and Lord Asriel's Gallivespian spies, the Chevalier Tialys and the Lady Salmakia, he rescues Lyra from the cave where her mother has hidden her from the Magisterium, which has become determined to kill her before she yields to temptation and sin like the original Eve.\nWill, Lyra, Tialys and Salmakia journey to the Land of the Dead, temporarily parting with their d\u00e6mons to release the ghosts from their captivity. Mary Malone, a scientist from Will's world interested in \"shadows\" (or Dust in Lyra's world), travels to a land populated by strange sentient creatures called Mulefa. There, she comes to understand the true nature of Dust, which is both created by and nourishes life that has become self-aware. Lord Asriel and the reformed Mrs Coulter work to destroy the Authority's Regent Metatron. They succeed, but themselves suffer annihilation in the process by pulling Metatron into the abyss.\nThe Authority himself dies of his own frailty when Will and Lyra free him from the crystal prison wherein Metatron had trapped him, able to do so because an attack by cliff-ghasts kills or drives away the prison's protectors. When Will and Lyra emerge from the land of the dead, they find their d\u00e6mons.\nThe book ends with Will and Lyra falling in love but realising they cannot live together in the same world, because all windows \u2013 except one from the underworld to the world of the Mulefa \u2013 must be closed to prevent the loss of Dust. Additionally, with every window opening, a Spectre is created, so Will must never use the knife again. They must also be apart because both of them can only live full lives in their native worlds. During the return, Mary Malone learns how to see her own d\u00e6mon, who takes the form of a black Alpine chough. Lyra loses her ability to intuitively read the alethiometer and determines to learn how to use her conscious mind to achieve the same effect.\nCharacters.\nAll humans in Lyra's world, including witches, have a d\u00e6mon. It is the physical manifestation of a person's 'inner being', soul or spirit. It takes the form of a creature (moth, bird, dog, monkey, snake, etc.) and is usually the opposite sex to its human counterpart. The d\u00e6mons of children have the ability to change form - from one creature to another - but during a child's puberty, their d\u00e6mon \"settles\" into a permanent form, which reflects the person's personality. When a person dies, the d\u00e6mon dies too. Armoured bears, cliff ghasts, and other creatures do not have d\u00e6mons. An armoured bear's armour is his soul.\nD\u00e6mons.\nOne distinctive aspect of Pullman's story is the presence of \"d\u00e6mons\" (pronounced \"demon\"). In the birth-universe of the story's protagonist Lyra Belacqua, a human individual's inner-self manifests itself throughout life as an animal-shaped \"d\u00e6mon\" that almost always stays near its human counterpart. During the childhood of its associated human, a d\u00e6mon can change its animal shape at will, but with the onset of adolescence it settles into a fixed, final animal form.\nInfluences.\nPullman has identified three major literary influences on \"His Dark Materials\": the essay \"On the Marionette Theatre\" by Heinrich von Kleist, the works of William Blake, and, most important, John Milton's \"Paradise Lost\", from which the trilogy derives its title. In his introduction, he adapts a famous description of Milton by Blake to quip that he (Pullman) \"is of the Devil's party and \"does\" know it\".\nCritics have compared the trilogy with C. S. Lewis's \"The Chronicles of Narnia\", which Pullman despises, and also with such fantasy books as \"Bridge to Terabithia\" by Katherine Paterson and \"A Wrinkle in Time\" by Madeleine L'Engle.\nAwards and recognition.\nThe first volume, \"Northern Lights\", won the Carnegie Medal for children's fiction in the UK in 1995. In 2007, the judges of the CILIP Carnegie Medal for children's literature selected it as one of the ten most important children's novels of the previous 70 years. In an online June 2007 poll, it was voted the best Carnegie Medal winner in the 70-year history of the award, the Carnegie of Carnegies. \"The Amber Spyglass\" won the 2001 Whitbread Book of the Year award, the first time that such an award has been bestowed on a book from their \"children's literature\" category.\nThe trilogy came third in the 2003 BBC's \"Big Read\", a national poll of viewers' favourite books, after \"The Lord of the Rings\" and \"Pride and Prejudice\".\nOn 19 May 2005, Pullman attended the British Library in London to receive formal congratulations for his work from culture secretary Tessa Jowell \"on behalf of the government\". On 25 May 2005, Pullman received the Swedish government's Astrid Lindgren Memorial Award for children's and youth literature (sharing it with Japanese illustrator Ry\u014dji Arai). Swedes regard this prize as second only to the Nobel Prize in Literature; it has a value of 5 million Swedish Kronor or approximately \u00a3385,000. In 2008, \"The Observer\" cites \"Northern Lights\" as one of the 100 best novels. \"Time\" magazine in the US included \"Northern Lights\" (\"The Golden Compass\") in its list of the 100 Best Young-Adult Books of All Time. In November 2019, the BBC listed \"His Dark Materials\" on its list of the 100 most influential novels.\nChristian opposition.\n\"His Dark Materials\" has occasioned controversy, primarily among some Christian groups.\nCynthia Grenier, in the \"Catholic Culture\", said: \"In the world of Pullman, God Himself (the Authority) is a merciless tyrant. His Church is an instrument of oppression, and true heroism consists of overthrowing both\". William A. Donohue of the Catholic League described Pullman's trilogy as \"atheism for kids\". Pullman said of Donohue's call for a boycott, \"Why don't we trust readers? [...] Oh, it causes me to shake my head with sorrow that such nitwits could be loose in the world\".\nIn a November 2002 interview, Pullman was asked to respond to the \"Catholic Herald\" calling his books \"the stuff of nightmares\" and \"worthy of the bonfire\". He replied: \"My response to that was to ask the publishers to print it in the next book, which they did! I think it's comical, it's just laughable\". The original remark in\" Catholic Herald \"(which was \"there are numerous candidates that seem to me to be far more worthy of the bonfire than Harry Potter\") was written in the context of parents in South Carolina pressing their Board of Education to ban the \"Harry Potter\" books.\nPullman expressed surprise over what he considered to be a relatively low level of criticism for \"His Dark Materials\" on religious grounds, saying \"I've been surprised by how little criticism I've got. Harry Potter's been taking all the flak... Meanwhile, I've been flying under the radar, saying things that are far more subversive than anything poor old Harry has said. My books are about killing God\". Others support this interpretation, arguing that the series, while clearly anticlerical, is also anti-theological because the death of God is represented as a fundamentally unimportant question.\nPullman found support from some other Christians, most notably from Rowan Williams, the former archbishop of Canterbury (spiritual head of the Anglican Communion), who argued that Pullman's attacks focus on the constraints and dangers of dogmatism and the use of religion to oppress, not on Christianity itself. Williams also recommended the \"His Dark Materials\" series of books for inclusion and discussion in Religious Education classes, and stated that \"To see large school-parties in the audience of the Pullman plays at the National Theatre is vastly encouraging\". Pullman and Williams took part in a National Theatre platform debate a few days later to discuss myth, religious experience, and its representation in the arts.\nRelated works.\n\"Lyra's Oxford\".\nThe 2003 novella \"Lyra's Oxford\" takes place two years after the timeline of \"The Amber Spyglass\". A witch who seeks revenge for her son's death in the war against the Authority draws Lyra, now 15, into a trap. Birds mysteriously rescue her and Pan, and she makes the acquaintance of an alchemist, formerly the witch's lover.\n\"Once Upon a Time in the North\".\nThis 2008 novella serves as a prequel to \"His Dark Materials\" and focuses on the Texan aeronaut Lee Scoresby as a young man. After winning his hot-air balloon, Scoresby heads to the North, landing on the Arctic island Novy Odense, where he is pulled into a conflict between the oil tycoon Larsen Manganese, the corrupt mayoral candidate Ivan Poliakov, and his longtime enemy from the Dakota Country, Pierre McConville. The story tells of Lee and Iorek's first meeting and of how they overcame these enemies.\n\"The Collectors\".\nA short story originally released exclusively as an audiobook by Audible in December 2014, narrated by actor Bill Nighy. The story refers to the early life of Mrs Coulter and is set in the senior common room of an Oxford college. The story was released by Penguin Books as a physical book in September 2022.\n\"The Book of Dust\".\n\"The Book of Dust\" is a second trilogy of novels set before, during and after \"His Dark Materials\". The first book, \"La Belle Sauvage\", was published on 19 October 2017. The second book, \"The Secret Commonwealth\", was published on 3 October 2019. The third and final book, \"The Rose Field\", was published on 23 October 2025.\n\"Serpentine\".\nA novella that was released in October 2020. Set after the events of \"The Amber Spyglass\" and before \"The Secret Commonwealth\", Lyra and Pantalaimon journey back to the far North to meet with the Consul of Witches.\n\"The Imagination Chamber\".\nIn January 2022, Pullman announced the release of the book \"The Imagination Chamber: Cosmic Rays from Lyra's Universe\", which would include new scenes set during the events of \"His Dark Materials\" and \"The Book of Dust\". It was published on 28 April 2022.\nAdaptations.\nRadio.\nBBC Radio 4 broadcast a radio play adaptation of \"His Dark Materials\" in 3 episodes, each lasting 2\u00bd hours. The books were adapted and dramatised by Lavinia Murray and directed by both David Hunter and Janet Whitaker, with the music composed by Billy Cowie. It was first broadcast over three consecutive weeks in from 4 January 2003, and then re-broadcast in both 2008-9 and in 2017, and was released by the BBC as box-sets on CD and audio cassette. The streaming rights are held by Audible.\nThe cast included: \nBoth Stamp and Fearon had roles in the subsequent BBC One television adaptation of 2019\u21922022, with Stamp as the previous elderly bearer of \u00c6sah\u00e6ttr, \"the Subtle Knife\", Giacomo Paradisi in Torre degli Angeli of Citt\u00e0gazze, and Fearon as Will Parry's school boxing coach Mr Hanway.\nAlso in 2003, an Irish radio dramatisation of \"Northern Lights\" was made by RT\u00c9 (Raidi\u00f3 Teilif\u00eds \u00c9ireann).\nTheatre.\nNicholas Hytner directed a theatrical version of the books as a two-part, six-hour performance for London's Royal National Theatre in December 2003, running until March 2004. It starred Anna Maxwell-Martin as Lyra, Dominic Cooper as Will, Timothy Dalton as Lord Asriel, Patricia Hodge as Mrs Coulter and Niamh Cusack as Serafina Pekkala, with d\u00e6mon puppets designed by Michael Curry. The play was successful and was revived (with a different cast and a revised script) for a second run between November 2004 and April 2005. It has since been staged by several other theatres in the UK and elsewhere.\nA new production was staged at Birmingham Repertory Theatre in March and April 2009, directed by Rachel Kavanaugh and Sarah Esdaile and starring Amy McAllister as Lyra. This version toured the UK and included a performance in Pullman's hometown of Oxford; Pullman made a cameo appearance.\nFilm.\nNew Line Cinema released a film adaptation, titled \"The Golden Compass\", on 7 December 2007. Directed by Chris Weitz, the production had a mixed reception, and though worldwide sales were strong, its U.S. earnings were not as high as the studio had hoped.\nThe filmmakers obscured the explicitly Biblical character of the Authority to avoid offending viewers. Weitz declared that he would not do the same for the planned sequels. \"Whereas \"The Golden Compass\" had to be introduced to the public carefully\", he said, \"the religious themes in the second and third books can't be minimised without destroying the spirit of these books. ...I will not be involved with any 'watering down' of books two and three, since what I have been working towards the whole time in the first film is to be able to deliver on the second and third\".\n\"The Golden Compass\" film stars Dakota Blue Richards as Lyra, Nicole Kidman as Mrs Coulter, and Daniel Craig as Lord Asriel. Eva Green plays Serafina Pekkala, Ian McKellen voices Iorek Byrnison, and Freddie Highmore voices Pantalaimon. While Sam Elliott blamed the Catholic Church's opposition for forcing the cancellation of any adaptations of the rest of the trilogy, \"The Guardian\"'s film critic Stuart Heritage believed disappointing reviews may have been the real reason.\nTelevision.\nIn November 2015, the BBC commissioned a television adaptation of \"His Dark Materials\". The eight-part adaptation had a planned premiere date in 2017. By July 2018, Dafne Keen had been provisionally cast as Lyra Belacqua, Ruth Wilson as Marisa Coulter, James McAvoy as Lord Asriel, Lin-Manuel Miranda as Lee Scoresby and Clarke Peters as the Master of Jordan College. The series received its premiere in London on 15 October 2019. Broadcast began on BBC One in the United Kingdom and in Ireland on 3 November and on HBO in the United States on 4 November 2019. In 2020 the second series of \"His Dark Materials\" began streaming on BBC One in the United Kingdom on 8 November and on HBO Max in the United States on 16 November. The third and final eight-episode series premiered first on HBO on 5 December 2022, and on 18 December 2022 in the UK.\nAudiobooks.\nRandom House produced unabridged audiobooks of each \"His Dark Materials\" novel, read by Pullman, with parts read by actors including Jo Wyatt, Steven Webb, Peter England, Stephen Thorne and Douglas Blackwell. Penguin Audio has produced subsequent audiobook versions of the trilogy, read by Ruth Wilson.\nGraphic novels.\nA series of graphic novels was produced by French writer Stephane Melchior, the first one adapting \"Northern Lights\" and released in 2017 and illustrated by Clement Obrerie. The second volume, adapting \"The Subtle Knife\" was released in 2020 and illustrated by Thomas Gilbert.\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt; \nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nFurther reading.\nBooks\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;\nArticles\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
