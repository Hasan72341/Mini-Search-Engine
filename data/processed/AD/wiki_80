{"id": "40688", "revid": "46869372", "url": "https://en.wikipedia.org/wiki?curid=40688", "title": "Baud", "text": "Symbol rate measurement in telecommunications\nIn telecommunications and electronics, baud (; symbol: Bd) is a common unit of measurement of symbol rate, which is one of the components that determine the speed of communication over a data channel.\nIt is the unit for symbol rate or modulation rate in symbols per second or pulses per second. It is the number of distinct symbol changes (signalling events) made to the transmission medium per second in a digitally modulated signal or a bd rate line code.\nBaud is related to \"gross bit rate\", which can be expressed in bits per second (bit/s). If there are precisely two symbols in the system (typically 0 and 1), then baud and bits per second are equivalent.\nNaming.\nThe baud unit is named after \u00c9mile Baudot, the inventor of the Baudot code for telegraphy, and is represented according to the rules for SI units. \nThat is, the first letter of its symbol is uppercase (Bd), but when the unit is spelled out, it should be written in lowercase (baud) except when it begins a sentence or is capitalized for another reason, such as in title case.\nIt was defined by the CCITT (now the ITU-T) in November 1926. The earlier standard had been the number of words per minute, which was a less robust measure since word length can vary.\nDefinitions.\nThe symbol duration time, also known as the unit interval, can be directly measured as the time between transitions by looking at an eye diagram of the signal on an oscilloscope. The symbol duration time \"T\"s can be calculated as:\nformula_1\nwhere \"f\"s is the symbol rate.\nThere is also a chance of miscommunication, which leads to ambiguity.\nExample: Communication at the baud rate \"1000\u00a0Bd\" means communication by means of sending \"1000 symbols per second\". In the case of a modem, this corresponds to \"1000 tones per second\"; similarly, in the case of a line code, this corresponds to \"1000 pulses per second\". The symbol duration time is \" second\" (that is, \"1 millisecond\").\nThe baud is scaled using standard metric prefixes, so that, for example\nRelationship to gross bit rate.\nThe symbol rate is related to gross bit rate expressed in bit/s. The term baud has sometimes incorrectly been used to mean bit rate, since these rates are the same in old modems as well as in the simplest digital communication links using only one bit per symbol, such that binary digit 0 is represented by one symbol, and binary digit 1 by another symbol. In more advanced modems and data transmission techniques, a symbol may have more than two states, so it may represent more than one bit. A bit (binary digit) always represents one of two states.\nIf N bits are conveyed per symbol, and the gross bit rate is R, inclusive of channel coding overhead, the symbol rate \"f\"s can be calculated as\nformula_2\nBy taking information per pulse \"N\" in bit/pulse to be the base-2-logarithm of the number of distinct messages \"M\" that could be sent, Hartley constructed a measure of the gross bit rate \"R\" as\nformula_3 where formula_4\nHere, the formula_5 denotes the ceiling function of formula_6, where formula_6 is taken to be any real number greater than zero, then the ceiling function rounds up to the nearest natural number (e.g. formula_8).\nIn that case, \"M\" = 2\"N\" different symbols are used. In a modem, these may be time-limited sine wave tones with unique combinations of amplitude, phase or frequency. For example, in a 64QAM modem, \"M\" = 64, and so the bit rate is \"N\" = log2(64) = 6 times the baud rate. In a line code, these may be \"M\" different voltage levels.\nThe ratio is not necessarily an integer; in 4B3T coding, the bit rate is of the baud rate. (A typical basic rate interface with a 160 kbit/s raw data rate operates at 120\u00a0kBd.) \nCodes with many symbols, and thus a bit rate higher than the symbol rate, are most useful on channels such as telephone lines with a limited bandwidth but a high signal-to-noise ratio within that bandwidth. In other applications, the bit rate is less than the symbol rate. Eight-to-fourteen modulation as used on audio CDs has bit rate of the baud rate.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "40689", "revid": "11435369", "url": "https://en.wikipedia.org/wiki?curid=40689", "title": "Acknowledgement", "text": ""}
{"id": "40690", "revid": "49190362", "url": "https://en.wikipedia.org/wiki?curid=40690", "title": "Acoustic coupler", "text": "Device for coupling electrical signals by sound\nIn telecommunications, an acoustic coupler is an interface device for coupling electrical signals by acoustical means\u2014usually into and out of a telephone.\nThe link is achieved through converting electric signals from the phone line to sound and reconverting sound to electric signals needed for the end terminal, such as a teletypewriter, and back, rather than through direct electrical connection.\nAcoustic couplers can be considered an early method of acoustic data transmission.\nHistory and applications.\nPrior to its breakup in 1984, Bell System's legal monopoly over telephony in the United States allowed the company to impose strict rules on how consumers could access their network. Customers were prohibited from connecting equipment not made or sold by Bell to the network. The same set-up was operative in nearly all countries, where the telephone companies were nationally owned. In many households, telephones were hard-wired to wall terminals before connectors like RJ11 and BS 6312 became standardized.\nThe situation was similar in other countries. In Australia, until 1975 the PMG, a Government monopoly, owned all telephone wiring and equipment in user premises and prohibited attachment of third party devices, and while most handsets were connected by 600 series connectors, these were rare in Australia so imported equipment could not be directly connected in any case, despite the general electrical compatibility.\nIt was not until a landmark U.S. court ruling regarding the Hush-A-Phone in 1956 that the use of a phone attachment (by a third party vendor) was allowed for the first time; though AT&amp;T's right to regulate any device connected to the telephone system was upheld by the courts, they were instructed to cease interference towards Hush-A-Phone users. A second court decision in 1968 regarding the Carterfone further allowed \"any device not harmful to the system\" to be connected directly to the AT&amp;T network. This decision enabled the proliferation of later innovations like answering machines, fax machines, and modems.\nWhen inventors began developing devices to send non-voice signals over a telephone line, the need for a workaround for the Bell restrictions was apparent. As early as 1937, telefax machines used by newspapers were using some kind of couplers, possibly acoustic but more likely magnetic for single-directional communication. Multiplexed bidirectional telephone coupling was not needed by these early fax machines.\nRobert Weitbrecht created a workaround for the Bell restrictions in 1963. He developed a coupling device that converted sound from the ear piece of the telephone handset to electrical signals, and converted the electrical pulses coming from the teletypewriter to sound that goes into the mouth piece of the telephone handset. His acoustic coupler is known as the Weitbrecht Modem.\nThe Weitbrecht Modem inspired other engineers to develop other modems to work with 8-bit ASCII terminals at a faster rate. Such modems or couplers were developed around 1966 by John van Geen at the Stanford Research Institute (now SRI International), that mimicked handset operations. An early commercial model was built by Livermore Data Systems in 1968. One would dial the computer system (which would have telephone company datasets) on one's phone, and when the connection was established, place the handset into the acoustic modem. \nSince the handsets were all supplied by the telephone company, most had the same shape, simplifying the physical interface. A microphone and a speaker inside the modem box would pick up and transmit the signaling tones, and circuitry would convert those audio frequency-shift keying encoded binary signals for an RS232 output socket. With luck one could get 300 baud (~bits/second) transmission rates, but 150 baud was more typical. \nThat speed was sufficient for typewriter-based terminals, as the IBM 2741, running at 134.5 baud, or a teleprinter, running at 110 baud.\nThe practical upper limit for acoustic-coupled modems was 1200 baud, first made available in 1973 by Vadic and 1977 by AT&amp;T. 1200 baud endpoints became widespread in 1985 with the advent of the Hayes Smartmodem 1200A, though it used an RJ11 jack and was not an acoustic coupler. Such devices facilitated the creation of dial-up bulletin board systems, a forerunner of modern internet chat rooms, message boards, and e-mail.\nDesign.\nUsually, a telephone handset was placed into a cradle that had been engineered to fit closely (by the use of rubber seals) around its microphone and earpiece. A modem would modulate a loudspeaker in the cup attached to the handset's microphone, and sound from the speaker in the telephone handset's earpiece would be picked up by a microphone in the cup attached to that. In this way signals could be passed in both directions. Despite the use of seals, acoustic couplers were sensitive to external noise and depended on the then-widespread standardization of the size and shape of handsets.\nOnce direct electrical connections to telephone networks were made legal, they rapidly became the preferred method of attaching modems, and the use of acoustic couplers dwindled. Acoustic couplers were still used until at least the late 1990s by people travelling in areas of the world where electrical connection to the telephone network was illegal or impractical. Many models of Telecommunications Device for the Deaf (TDD) still have a built-in acoustic coupler, which allow more universal use with pay phones and for emergency calls by deaf people.\nPopular culture.\nAn acoustic coupler (a Novation CAT 300 baud model) is prominently shown early in the 1983 film \"WarGames\", when character David Lightman (portrayed by actor Matthew Broderick) places a telephone handset into the cradle of a prop acoustic modem to accentuate the act of using telephone lines for interconnection to the developing computer networks of the period\u2014in this case, a military command computer. The earliest major motion picture depicting an acoustic coupler was probably the 1968 Steve McQueen film \"Bullitt\".\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "40692", "revid": "43007828", "url": "https://en.wikipedia.org/wiki?curid=40692", "title": "Active laser medium", "text": "Source of optical gain in a laser\nThe active laser medium (also called a gain medium or lasing medium) is the source of optical gain within a laser. The gain results from the stimulated emission of photons through electronic or molecular transitions to a lower energy state from a higher energy state previously populated by a pump source.\nExamples of active laser media include:\nIn order to fire a laser, the active gain medium must be changed into a state in which population inversion occurs. The preparation of this state requires an external energy source and is known as laser pumping. Pumping may be achieved with electrical currents (e.g. semiconductors, or gases via high-voltage discharges) or with light, generated by discharge lamps or by other lasers (semiconductor lasers). More exotic gain media can be pumped by chemical reactions, nuclear fission, or with high-energy electron beams.\nExample of a model of gain medium.\nThe simplest model of optical gain in real systems includes just two, energetically well separated, groups of sub-levels. Within each sub-level group, fast transitions ensure that thermal equilibrium is reached quickly. Stimulated emissions between upper and lower groups, essential for gain, require the upper levels to be more populated than the corresponding lower ones. This situation is called population-inversion. It is more readily achieved if unstimulated transition rates between the two groups are slow, i.e. the upper levels are metastable. Population inversions are more easily produced when only the lowest sublevels are occupied, requiring either low temperatures or well energetically split groups. \nIn the case of amplification of optical signals, the lasing frequency is called \"signal frequency.\" If the externally provided energy required for the signal's amplification is optical, it would necessarily be at the same or higher \"pump\" frequency. \nCross-sections.\nThe simple medium can be characterized with effective cross-sections of absorption and emission at frequencies formula_1 and formula_2.\nThe relative concentrations can be defined as formula_7 and formula_8.\nThe rate of transitions of an active center from the ground state to the excited state can be expressed like this: formula_9.\nWhile the rate of transitions back to the ground state can be expressed like: formula_10,\nwhere formula_11 and formula_12 are effective cross-sections of absorption at the frequencies of the signal and the pump, formula_13 and formula_14 are the same for stimulated emission, and\nformula_15 is rate of the spontaneous decay of the upper level.\nThen, the kinetic equation for relative populations can be written as follows:\nformula_16\nThe dynamic saturation intensities can be defined:\nformula_17,\nformula_18.\nThe absorption at strong signal:\nformula_19.\nThe gain at strong pump:\nformula_20,\nwhere formula_21\nis determinant of cross-section.\nGain never exceeds value formula_22, and absorption never exceeds value formula_23.\nAt given intensities formula_24, formula_25 of pump and signal, the gain and absorption\ncan be expressed as follows:\nformula_26,\nformula_27,\nwhere \nformula_28, \nformula_29, \nformula_30, \nformula_31 .\nIdentities.\nThe following identities take place:\nformula_32, formula_33\nThe state of gain medium can be characterized with a single parameter, such as population of the upper level, gain or absorption.\nEfficiency of the gain medium.\nThe efficiency of a gain medium can be defined as\nformula_34.\nWithin the same model, the efficiency can be expressed as follows:\nformula_35.\nFor efficient operation, both intensities\u2014pump and signal\u2014should exceed their saturation intensities:\nformula_36, and formula_37.\nThe estimates above are valid for a medium uniformly filled with pump and signal light. Spatial hole burning may slightly reduce the efficiency because some regions are pumped well, but the pump is not efficiently withdrawn by the signal in the nodes of the interference of counter-propagating waves."}
{"id": "40693", "revid": "39166520", "url": "https://en.wikipedia.org/wiki?curid=40693", "title": "Adaptive communications", "text": "Adaptive communications can mean any communications system, or portion thereof, that automatically uses feedback information obtained from the system itself or from the signals carried by the system to modify dynamically one or more of the system operational parameters to improve system performance or to resist degradation. \nThe modification of a system parameter may be discrete, as in hard-switched diversity reception, or may be continuous, as in a \npredetection combining algorithm.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "40694", "revid": "7903804", "url": "https://en.wikipedia.org/wiki?curid=40694", "title": "Adaptive predictive coding", "text": "Adaptive predictive coding (APC) is a narrowband analog-to-digital conversion that uses a one-level or multilevel sampling system in which the value of the signal at each sampling instant is predicted according to a linear function of the past values of the quantized signals.\nAPC is related to linear predictive coding (LPC) in that both use adaptive predictors. However, APC uses fewer prediction coefficients, thus requiring a higher sampling rate than LPC.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "40695", "revid": "4145", "url": "https://en.wikipedia.org/wiki?curid=40695", "title": "Adder\u2013subtractor", "text": "Computing circuit\nIn digital circuits, an adder\u2013subtractor is a circuit that is capable of adding or subtracting numbers (in particular, binary). Below is a circuit that adds \"or\" subtracts depending on a control signal. It is also possible to construct a circuit that performs both addition and subtraction at the same time.\nConstruction.\nHaving an \"n\"-bit adder for \"A\" and \"B\", then \"S\" = \"A\" + \"B\". Then, assume the numbers are in two's complement. Then to perform \"B\" \u2212 \"A\", two's complement theory says to invert each bit of \"A\" with a NOT gate then add one. This yields \"S\" = \"B\" + \"A\" + 1, which is easy to do with a slightly modified adder.\nBy preceding each \"A\" input bit on the adder with a 2-to-1 multiplexer where:\nthat has control input \"D\" that is also connected to the initial carry, then the modified adder performs\nThis works because when \"D\" = 1 the \"A\" input to the adder is really \"A\" and the carry in is 1. Adding \"B\" to \"A\" and 1 yields the desired subtraction of \"B\" \u2212 \"A\".\nA way you can mark number \"A\" as positive or negative without using a multiplexer on each bit is to use an XOR gate to precede each bit instead. \nThis produces the same truth table for the bit arriving at the adder as the multiplexer solution does since the XOR gate output will be what the input bit is when \"D\" = 0 and the inverted input bit when \"D\" = 1.\nRole in the arithmetic logic unit.\nAdders are a part of the core of an arithmetic logic unit (ALU). The control unit decides which operations an ALU should perform (based on the op code being executed) and sets the ALU operation. The \"D\" input to the adder\u2013subtractor above would be one such control line from the control unit.\nThe adder\u2013subtractor above could easily be extended to include more functions. For example, a 2-to-1 multiplexer could be introduced on each \"Bi\" that would switch between zero and \"Bi\"; this could be used (in conjunction with \"D\" = 1) to yield the two's complement of \"A\" since \u2212\"A\" = \"A\" + 1.\nA further step would be to change the 2-to-1 multiplex on \"A\" to a 4-to-1 with the third input being zero, then replicating this on \"Bi\" thus yielding the following output functions:\nBy adding more logic in front of the adder, a single adder can be converted into much more than just an adder\u2014an ALU.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "40696", "revid": "24465790", "url": "https://en.wikipedia.org/wiki?curid=40696", "title": "Address (disambiguation)", "text": "Geographic Location.\nAn address is a collection of information used to give the location of a building or a plot of land.\nMovies.\nAddress or The Address may also refer to:\n&lt;templatestyles src=\"Template:TOC_right/styles.css\" /&gt;\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "40699", "revid": "16416757", "url": "https://en.wikipedia.org/wiki?curid=40699", "title": "Adjacent-channel interference", "text": "Interference caused by a signal in an adjacent channel\nAdjacent-channel interference (ACI) is interference caused by extraneous power from a signal in an adjacent channel. ACI may be caused by inadequate filtering (such as incomplete filtering of unwanted modulation products in FM systems), improper tuning or poor frequency control (in the reference channel, the interfering channel or both).\nACI is distinguished from crosstalk.\nOrigin.\nThe adjacent-channel interference which receiver A experiences from a transmitter B is the sum of the power that B emits into A's channel\u2014known as the \"unwanted emission\", and represented by the ACLR (Adjacent Channel Leakage Ratio)\u2014and the power that A picks up from B's channel, which is represented by the ACS (Adjacent Channel Selectivity). B emitting power into A's channel is called adjacent-channel leakage (unwanted emissions). It occurs for two reasons. First, because RF filters require a roll-off, and do not eliminate a signal completely. Second, due to intermodulation in B's amplifiers, which cause the transmitted spectrum to spread beyond what was intended. Therefore, B emits some power in the adjacent channel which is picked up by A. A receives some emissions from B's channel due to the roll off of A's selectivity filters. Selectivity filters are designed to \"select\" a channel. Similarly, B's signal suffers intermodulation distortion passing through A's RF input amplifiers, leaking more power into adjacent frequencies.\nAvoidance procedure.\nBroadcast regulators frequently manage the broadcast spectrum in order to minimize adjacent-channel interference. For example, in North America, FM radio stations in a single region cannot be licensed on adjacent frequencies \u2014 that is, if a station is licensed on 99.5\u00a0MHz in a city, the first-adjacent frequencies of 99.3\u00a0MHz and 99.7\u00a0MHz cannot be used anywhere within a certain distance of that station's transmitter, and the second-adjacent frequencies of 99.1\u00a0MHz and 99.9\u00a0MHz are restricted to specialized usages such as low-power stations. Similar restrictions formerly applied to third-adjacent frequencies as well (i.e. 98.9\u00a0MHz and 100.1\u00a0MHz in the example above), but these are no longer observed.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "40700", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=40700", "title": "Advanced Data Communication Control Procedures", "text": "Computer networking protocol\nIn telecommunications, Advanced Data Communication Control Procedures (or Protocol) (ADCCP) is a bit-oriented data link layer protocol developed by the American National Standards Institute. It is functionally equivalent to the ISO High-Level Data Link Control (HDLC) protocol.\nAlthough the ISO and ANSI standards writers coordinated their work, so the differences between the standards are mainly editorial, there is one meaningful difference: ADCCP's definition of the basic subset required to implement balanced asynchronous mode includes the RSET frame, while HDLC makes it optional.\nOne major difference between the two is the unnumbered (U) format. When extended (7-bit) sequence numbers are used, I and S frames have two-byte control fields. Like early versions of HDLC, ADCCP specifies a 2-byte control field format with the P/F flag duplicated. Later HDLC specifications, in particular ISO/IEC 13239, changed that to specify that U frames have 1-byte control fields in all cases. \nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "40701", "revid": "44141905", "url": "https://en.wikipedia.org/wiki?curid=40701", "title": "Aerial insert", "text": "Segment of cabling\nIn telecommunications an aerial insert is a segment of cabling that rises from ground to a point above ground, followed by an overhead run, e.g. on poles, followed by a drop back into the ground. An aerial insert is used in places where it is not possible or practical to place a cable underground. Aerial inserts might be encountered in crossing deep ditches, canals, rivers, or subway lines.\nAerial inserts can be found for the same reason also in underground power transmission lines. However for this purpose, the aerial insert is in most cases realized as classical overhead line. It is also possible that a powerline consists of multiple underground and overhead sections of different length.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "40702", "revid": "48618112", "url": "https://en.wikipedia.org/wiki?curid=40702", "title": "Aeronautical Emergency Communications System Plan", "text": "In telecommunications, the Aeronautical Emergency Communications System Plan (AECS) provides for the operation of aeronautical communications stations, on a voluntary, organized basis, to provide the President and the Federal Government, as well as multiple heads of state and local governments, or their designated representatives, and the aeronautical industry with an expeditious means of communications during an emergency.\nReferences.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "40703", "revid": "7903804", "url": "https://en.wikipedia.org/wiki?curid=40703", "title": "AIOD leads", "text": "In land-line telephony, AIOD leads are Terminal equipment leads used solely to transmit automatic identified outward dialing (AIOD) data from a PBX to the public switched telephone network or to switched service networks (\"e.g.,\" EPSCS), so that a vendor can provide a detailed monthly bill identifying long distance calling usage by individual PBX stations, tie trunks, or the attendant console. It resembles common channel signalling in that the AIOD leads provide data for all trunks, but is used only for billing, thus resembling automatic number identification.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "40704", "revid": "27823944", "url": "https://en.wikipedia.org/wiki?curid=40704", "title": "Airborne radio relay", "text": "Airborne radio relay is a technique employing aircraft fitted with radio relay stations for the purpose of increasing the range, flexibility, or physical security of communications systems. The aircraft may be manned or unmanned aerial vehicles.\nUse in Vietnam.\nOne of the first uses of airborne radio relay was by the United States Army's 1st Cavalry Division in the Battle of Ia Drang during the Vietnam War, which employed the technique to improve communications with commanders at headquarters. The action of war had shifted to the borders of Laos and Cambodia, where the hilly terrain made the monetary and human cost of seizing and holding high ground, and airlifting and installing radio relay equipment prohibitive. In 1968, the Department of the Army provided four specially-equipped relay aircraft to the Division, which proved invaluable throughout the country, in particular, during the 1st Cavalry Division's relief of Khe Sanh in 1968.\nThe use of airborne radio relay was a great success, although two problems arose during the Vietnam War. The first was the limitations of the aircraft used as relays. The 1st Cavalry Division had originally used C-7 Caribous as the relay aircraft, but when these planes were turned over to the Air Force, the equipment was installed in single-engine Otter aircraft, which were too underpowered to carry the heavy equipment required for relay. Eventually, the 1st Signal Brigade was provided with six specially-equipped U-21 aircraft for use in relay operations. The second problem was that of radio frequency interference: the limited frequency spectrum in use for combat radios meant that relay aircraft often interfered with the communication of ground units when their frequencies were overridden by the airborne units. The Army eventually assigned certain frequencies for airborne relay only, although this further limited the frequencies available to ground units.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "40706", "revid": "3488865", "url": "https://en.wikipedia.org/wiki?curid=40706", "title": "Alarm sensor", "text": "In telecommunications, the term alarm sensor has the following meanings:\n\"Note:\" Alarm sensors may also be redundant or chained, such as when one alarm sensor is used to protect the housing, cabling, or power protected by another alarm sensor."}
{"id": "40707", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=40707", "title": "A-law algorithm", "text": "Audio companding in communications\nAn A-law algorithm is a standard companding algorithm, used in European 8-bit PCM digital communications systems to optimize, i.e. modify, the dynamic range of an analog signal for digitizing. It is one of the two companding algorithms in the G.711 standard from ITU-T, the other being the similar \u03bc-law, used in North America and Japan.\nFor a given input formula_1, the equation for A-law encoding is as follows:\nformula_2\nwhere formula_3 is the compression parameter. In Europe, formula_4.\nA-law expansion is given by the inverse function:\nformula_5\nThe reason for this encoding is that the wide dynamic range of speech does not lend itself well to efficient linear digital encoding. A-law encoding effectively reduces the dynamic range of the signal, thereby increasing the coding efficiency and resulting in a signal-to-distortion ratio that is superior to that obtained by linear encoding for a given number of bits.\nComparison to \u03bc-law.\nThe \u03bc-law algorithm provides a slightly larger dynamic range than the A-law at the cost of worse proportional distortion for small signals. By convention, A-law is used for an international connection if at least one country uses it."}
{"id": "40708", "revid": "44979714", "url": "https://en.wikipedia.org/wiki?curid=40708", "title": "Allan variance", "text": "Measure of frequency stability in clocks and oscillators\nThe Allan variance (AVAR), also known as two-sample variance, is a measure of frequency stability in clocks, oscillators and amplifiers. It is named after David W. Allan and expressed mathematically as formula_1.\nThe Allan deviation (ADEV), also known as sigma-tau, is the square root of the Allan variance, formula_2.\nThe \"M-sample variance\" is a measure of frequency stability using \"M\" samples, time \"T\" between measurements and observation time formula_3. \"M\"-sample variance is expressed as\nformula_4\nThe Allan variance is intended to estimate stability due to noise processes and not that of systematic errors or imperfections such as frequency drift or temperature effects. The Allan variance and Allan deviation describe frequency stability. See also the section Interpretation of value below.\nThere are also different adaptations or alterations of Allan variance, notably the modified Allan variance MAVAR or MVAR, the total variance, and the Hadamard variance. There also exist time-stability variants such as time deviation (TDEV) or time variance (TVAR). Allan variance and its variants have proven useful outside the scope of timekeeping and are a set of improved statistical tools to use whenever the noise processes are not unconditionally stable, thus a derivative exists.\nThe general \"M\"-sample variance remains important, since it allows dead time in measurements, and bias functions allow conversion into Allan variance values. Nevertheless, for most applications the special case of 2-sample, or \"Allan variance\" with formula_5 is of greatest interest.\nBackground.\nWhen investigating the stability of crystal oscillators and atomic clocks, it was found that they did not have a phase noise consisting only of white noise, but also of flicker frequency noise. These noise forms become a challenge for traditional statistical tools such as standard deviation, as the estimator will not converge. The noise is thus said to be divergent. Early efforts in analyzing the stability included both theoretical analysis and practical measurements.\nAn important side consequence of having these types of noise was that, since the various methods of measurements did not agree with each other, the key aspect of repeatability of a measurement could not be achieved. This limits the possibility to compare sources and make meaningful specifications to require from suppliers. Essentially all forms of scientific and commercial uses were then limited to dedicated measurements, which hopefully would capture the need for that application.\nTo address these problems, David Allan introduced the \"M\"-sample variance and (indirectly) the two-sample variance. While the two-sample variance did not completely allow all types of noise to be distinguished, it provided a means to meaningfully separate many noise-forms for time-series of phase or frequency measurements between two or more oscillators. Allan provided a method to convert between any \"M\"-sample variance to any \"N\"-sample variance via the common 2-sample variance, thus making all \"M\"-sample variances comparable. The conversion mechanism also proved that \"M\"-sample variance does not converge for large \"M\", thus making them less useful. IEEE later identified the 2-sample variance as the preferred measure.\nAn early concern was related to time- and frequency-measurement instruments that had a dead time between measurements. Such a series of measurements did not form a continuous observation of the signal and thus introduced a systematic bias into the measurement. Great care was spent in estimating these biases. The introduction of zero-dead-time counters removed the need, but the bias-analysis tools have proved useful.\nAnother early aspect of concern was related to how the bandwidth of the measurement instrument would influence the measurement, such that it needed to be noted. It was later found that by algorithmically changing the observation formula_3, only low formula_3 values would be affected, while higher values would be unaffected. The change of formula_3 is done by letting it be an integer multiple formula_9 of the measurement timebase formula_10:\nformula_11\nThe physics of crystal oscillators were analyzed by D. B. Leeson, and the result is now referred to as Leeson's equation. The feedback in the oscillator will make the white noise and flicker noise of the feedback amplifier and crystal become the power-law noises of formula_12 white frequency noise and formula_13 flicker frequency noise respectively. These noise forms have the effect that the standard variance estimator does not converge when processing time-error samples. The mechanics of the feedback oscillators was unknown when the work on oscillator stability started, but was presented by Leeson at the same time as the set of statistical tools was made available by David W. Allan. For a more thorough presentation on the Leeson effect, see modern phase-noise literature.\nInterpretation of value.\nAllan variance is defined as one half of the time average of the squares of the differences between successive readings of the frequency deviation sampled over the sampling period. The Allan variance depends on the time period used between samples, therefore, it is a function of the sample period, commonly denoted as \"\u03c4\", likewise the distribution being measured, and is displayed as a graph rather than a single number. A low Allan variance is a characteristic of a clock with good stability over the measured period.\nAllan deviation is widely used for plots (conventionally in log\u2013log format) and presentation of numbers. It is preferred, as it gives the relative amplitude stability, allowing ease of comparison with other sources of errors.\nAn Allan deviation of 1.3\u00d710\u22129 at observation time 1 s (i.e. \"\u03c4\" = 1 s) should be interpreted as there being an instability in frequency between two observations 1\u00a0second apart with a relative root mean square (RMS) value of 1.3\u00d710\u22129. For a 10\u00a0MHz clock, this would be equivalent to 13\u00a0mHz RMS movement. If the phase stability of an oscillator is needed, then the time deviation variants should be consulted and used.\nOne may convert the Allan variance and other time-domain variances into frequency-domain measures of time (phase) and frequency stability.\nFormulations.\n\"M\"-sample variance.\nGiven a time-series formula_14, for any positive real numbers formula_15, define the real number sequenceformula_16Then the formula_17-sample variance is defined (here in a modernized notation form) as the Bessel-corrected variance of the sequence formula_18:formula_19The interpretation of the symbols is as follows:\nDead-time can be accounted for by letting the time formula_26 be different from that of formula_3.\nAllan variance.\nThe Allan variance is defined as\nformula_30\nwhere formula_31 and formula_32 denotes the expectation operator.\nThe condition formula_33 means the samples are taken with no dead-time between them.\nAllan deviation.\nJust as with standard deviation and variance, the Allan deviation is defined as the square root of the Allan variance:\nformula_34\nSupporting definitions.\nOscillator model.\nThe oscillator being analysed is assumed to follow the basic model of\n formula_35\nThe oscillator is assumed to have a nominal frequency of formula_36, given in cycles per second (SI unit: hertz). The nominal angular frequency formula_37 (in radians per second) is given by\n formula_38\nThe total phase can be separated into a perfectly cyclic component formula_39, along with a fluctuating component formula_40:\n formula_41\nTime error.\nThe time-error function \"x\"(\"t\") is the difference between expected nominal time and actual normal time:\n formula_42\nFor measured values a time-error series TE(\"t\") is defined from the reference time function \"T\"ref(\"t\") as\n formula_43\nFrequency function.\nThe frequency function formula_44 is the frequency over time, defined as\n formula_45\nFractional frequency.\nThe fractional frequency \"y\"(\"t\") is the normalized difference between the frequency formula_44 and the nominal frequency formula_36:\nformula_48\nAverage fractional frequency.\nThe average fractional frequency is defined as\nformula_49\nwhere the average is taken over observation time \"\u03c4\", the \"y\"(\"t\") is the fractional-frequency error at time \"t\", and \"\u03c4\" is the observation time.\nSince \"y\"(\"t\") is the derivative of \"x\"(\"t\"), we can without loss of generality rewrite it as\nformula_50\nEstimators.\nThis definition is based on the statistical expected value, integrating over infinite time. The real-world situation does not allow for such time-series, in which case a statistical estimator needs to be used in its place. A number of different estimators will be presented and discussed.\nFixed \"\u03c4\" estimators.\nA first simple estimator would be to directly translate the definition into\nformula_51\nor for the time series:\nformula_52\nThese formulas, however, only provide the calculation for the \"\u03c4\" = \"\u03c4\"0 case. To calculate for a different value of \"\u03c4\", a new time-series needs to be provided.\nNon-overlapped variable \u03c4 estimators.\nTaking the time-series and skipping past \"n\"\u00a0\u2212\u00a01 samples, a new (shorter) time-series would occur with \"\u03c4\"0 as the time between the adjacent samples, for which the Allan variance could be calculated with the simple estimators. These could be modified to introduce the new variable \"n\" such that no new time-series would have to be generated, but rather the original time series could be reused for various values of \"n\". The estimators become\nformula_53\nwith formula_54,\nand for the time series:\nformula_55\nwith formula_56.\nThese estimators have a significant drawback in that they will drop a significant amount of sample data, as only 1/\"n\" of the available samples is being used.\nOverlapped variable \"\u03c4\" estimators.\nA technique presented by J. J. Snyder provided an improved tool, as measurements were overlapped in \"n\" overlapped series out of the original series. The overlapping Allan variance estimator was introduced by Howe, Allan and Barnes. This can be shown to be equivalent to averaging the time or normalized frequency samples in blocks of \"n\" samples prior to processing. The resulting predictor becomes\nformula_57\nor for the time series:\nformula_58\nThe overlapping estimators have far superior performance over the non-overlapping estimators, as \"n\" rises and the time-series is of moderate length. The overlapped estimators have been accepted as the preferred Allan variance estimators in IEEE, ITU-T and ETSI standards for comparable measurements such as needed for telecommunication qualification.\nModified Allan variance.\nIn order to address the inability to separate white phase modulation from flicker phase modulation using traditional Allan variance estimators, an algorithmic filtering reduces the bandwidth by \"n\". This filtering provides a modification to the definition and estimators and it now identifies as a separate class of variance called modified Allan variance. The modified Allan variance measure is a frequency stability measure, just as is the Allan variance.\nTime stability estimators.\nA time stability (\u03c3\"x\") statistical measure, which is often called the time deviation (TDEV), can be calculated from the modified Allan deviation (MDEV). The TDEV is based on the MDEV instead of the original Allan deviation, because the MDEV can discriminate between white and flicker phase modulation (PM). The following is the time variance estimation based on the modified Allan variance:\nformula_59\nand similarly for modified Allan deviation to time deviation:\nformula_60\nThe TDEV is normalized so that it is equal to the classical deviation for white PM for time constant \"\u03c4\" =\u00a0\"\u03c4\"0. To understand the normalization scale factor between the statistical measures, the following is the relevant statistical rule: For independent random variables \"X\" and \"Y\", the variance (\u03c3\"z\"2) of a sum or difference (\"z\" = \"x\" \u2212 \"y\") is the sum square of their variances (\u03c3\"z\"2 = \u03c3\"x\"2 + \u03c3\"y\"2). The variance of the sum or difference (\"y\" = \"x\"2\"\u03c4\" \u2212 \"x\"\"\u03c4\") of two independent samples of a random variable is twice the variance of the random variable (\u03c3\"y\"2 = 2\u03c3\"x\"2). The MDEV is the second difference of independent phase measurements (\"x\") that have a variance (\u03c3\"x\"2). Since the calculation is the double difference, which requires three independent phase measurements (\"x\"2\"\u03c4\" \u2212 2\"x\"\"\u03c4\" + \"x\"), the modified Allan variance (MVAR) is three times the variances of the phase measurements.\nOther estimators.\nFurther developments have produced improved estimation methods for the same stability measure, the variance/deviation of frequency, but these are known by separate names such as the Hadamard variance, modified Hadamard variance, the total variance, modified total variance and the Theo variance. These distinguish themselves in better use of statistics for improved confidence bounds or ability to handle linear frequency drift.\nConfidence intervals and equivalent degrees of freedom.\nStatistical estimators will calculate an estimated value on the sample series used. The estimates may deviate from the true value and the range of values which for some probability will contain the true value is referred to as the confidence interval. The confidence interval depends on the number of observations in the sample series, the dominant noise type, and the estimator being used. The width is also dependent on the statistical certainty for which the confidence interval values forms a bounded range, thus the statistical certainty that the true value is within that range of values. For variable-\"\u03c4\" estimators, the \"\u03c4\"0 multiple \"n\" is also a variable.\nConfidence interval.\nThe confidence interval can be established using chi-squared distribution with df degrees of freedom by using the distribution of the sample variance:\nformula_61\nwhere \"s\"\"2\" is the sample variance of our estimate, \"\u03c3\"2 is the true variance value, df is the degrees of freedom for the estimator, and \"\u03c7\"2 is calculated based on the inverse cumulative density distribution of a \"\u03c7\"2 with df degrees of freedom. For a 90% probability, covering the range from the 5% to the 95% range on the probability curve, the upper and lower limits can be found using the inequality\nformula_62\nwhich after rearrangement for the true variance becomes\nformula_63\nEffective degrees of freedom.\nThe degrees of freedom represents the number of free variables capable of contributing to the estimate. Depending on the estimator and noise type, the effective degrees of freedom varies. Estimator formulas depending on \"N\" (number of total sample points) and \"n\" (integer multiple of \"\u03c4\"0) has been found empirically:\nPower-law noise.\nThe Allan variance will treat various power-law noise types differently, conveniently allowing them to be identified and their strength estimated. As a convention, the measurement system width (high corner frequency) is denoted \"f\"\"H\".\nAs found in and in modern forms.\nThe Allan variance is unable to distinguish between WPM and FPM, but is able to resolve the other power-law noise types. In order to distinguish WPM and FPM, the modified Allan variance needs to be employed.\nThe above formulas assume that\nformula_64\nand thus that the bandwidth of the observation time is much lower than the instruments bandwidth. When this condition is not met, all noise forms depend on the instrument's bandwidth.\n\"\u03b1\"\u2013\"\u03bc\" mapping.\nThe detailed mapping of a phase modulation of the form\nformula_65\nwhere\nformula_66\nor frequency modulation of the form\nformula_67\ninto the Allan variance of the form\nformula_68\ncan be significantly simplified by providing a mapping between \"\u03b1\" and \"\u03bc\". A mapping between \"\u03b1\" and \"K\"\"\u03b1\" is also presented for convenience:\nGeneral conversion from phase noise.\nA signal with spectral phase noise formula_69 with units rad2/Hz can be converted to Allan Variance by\n formula_70\nLinear response.\nWhile Allan variance is intended to be used to distinguish noise forms, it will depend on some but not all linear responses to time. They are given in the table:\nThus, linear drift will contribute to output result. When measuring a real system, the linear drift or other drift mechanism may need to be estimated and removed from the time-series prior to calculating the Allan variance.\nTime and frequency filter properties.\nIn analyzing the properties of Allan variance and friends, it has proven useful to consider the filter properties on the normalize frequency. Starting with the definition for Allan variance for\nformula_71\nwhere\nformula_72\nReplacing the time series of formula_73 with the Fourier-transformed variant formula_74 the Allan variance can be expressed in the frequency domain as\nformula_75\nThus the transfer function for Allan variance is\nformula_76\nBias functions.\nThe \"M\"-sample variance, and the defined special case Allan variance, will experience systematic bias depending on different number of samples \"M\" and different relationship between \"T\" and \"\u03c4\". In order to address these biases the bias-functions \"B\"1 and \"B\"2 has been defined and allows conversion between different \"M\" and \"T\" values.\nThese bias functions are not sufficient for handling the bias resulting from concatenating \"M\" samples to the \"M\u03c4\"0 observation time over the \"MT\"0 with the dead-time distributed among the \"M\" measurement blocks rather than at the end of the measurement. This rendered the need for the \"B\"3 bias.\nThe bias functions are evaluated for a particular \u03bc value, so the \u03b1\u2013\u03bc mapping needs to be done for the dominant noise form as found using noise identification. Alternatively, the \u03bc value of the dominant noise form may be inferred from the measurements using the bias functions.\n\"B\"1 bias function.\nThe \"B\"1 bias function relates the \"M\"-sample variance with the 2-sample variance, keeping the time between measurements \"T\" and time for each measurements \"\u03c4\" constant. It is defined as\nformula_77\nwhere\nformula_78\nThe bias function becomes after analysis\nformula_79\n\"B\"2 bias function.\nThe \"B\"2 bias function relates the 2-sample variance for sample time \"T\" with the 2-sample variance (Allan variance), keeping the number of samples \"N\" = 2 and the observation time \"\u03c4\" constant. It is defined as\nformula_80\nwhere\nformula_78\nThe bias function becomes after analysis\nformula_82\n\"B\"3 bias function.\nThe \"B\"3 bias function relates the 2-sample variance for sample time \"MT\"0 and observation time \"M\u03c4\"0 with the 2-sample variance (Allan variance) and is defined as\nformula_83\nwhere\nformula_84\nformula_85\nThe \"B\"3 bias function is useful to adjust non-overlapping and overlapping variable \"\u03c4\" estimator values based on dead-time measurements of observation time \"\u03c4\"0 and time between observations \"T\"0 to normal dead-time estimates.\nThe bias function becomes after analysis (for the \"N\"\u00a0=\u00a02 case)\n formula_86\nwhere\n formula_87\n\"\u03c4\" bias function.\nWhile formally not formulated, it has been indirectly inferred as a consequence of the \"\u03b1\"\u2013\"\u03bc\" mapping. When comparing two Allan variance measure for different \"\u03c4\", assuming same dominant noise in the form of same \u03bc coefficient, a bias can be defined as\nformula_88\nThe bias function becomes after analysis\nformula_89\nConversion between values.\nIn order to convert from one set of measurements to another the \"B\"1, \"B\"2 and \u03c4 bias functions can be assembled. First the \"B\"1 function converts the (\"N\"1,\u202f\"T\"1,\u202f\"\u03c4\"1) value into (2,\u202f\"T\"1,\u202f\"\u03c4\"1), from which the \"B\"2 function converts into a (2,\u202f\"\u03c4\"1,\u202f\"\u03c4\"1) value, thus the Allan variance at \"\u03c4\"1. The Allan variance measure can be converted using the \u03c4 bias function from \"\u03c4\"1 to \"\u03c4\"2, from which then the (2,\u202f\"T\"2,\u202f\"\u03c4\"2) using \"B\"2 and then finally using \"B\"1 into the (\"N\"2,\u202f\"T\"2,\u202f\"\u03c4\"2) variance. The complete conversion becomes\nformula_90\nwhere\nformula_91\nformula_92\nSimilarly, for concatenated measurements using \"M\" sections, the logical extension becomes\nformula_93\nMeasurement issues.\nWhen making measurements to calculate Allan variance or Allan deviation, a number of issues may cause the measurements to degenerate. Covered here are the effects specific to Allan variance, where results would be biased.\nMeasurement bandwidth limits.\nA measurement system is expected to have a bandwidth at or below that of the Nyquist rate, as described within the Shannon\u2013Hartley theorem. As can be seen in the power-law noise formulas, the white and flicker noise modulations both depend on the upper corner frequency formula_94 (these systems are assumed to be low-pass filtered only). Considering the frequency filter property, it can be clearly seen that low-frequency noise has greater impact on the result. For relatively flat phase-modulation noise types (e.g. WPM and FPM), the filtering has relevance, whereas for noise types with greater slope the upper frequency limit becomes of less importance, assuming that the measurement system bandwidth is wide relative the formula_3 as given by\nformula_96\nWhen this assumption is not met, the effective bandwidth formula_94 needs to be notated alongside the measurement. The interested should consult NBS TN394.\nIf, however, one adjust the bandwidth of the estimator by using integer multiples of the sample time formula_98, then the system bandwidth impact can be reduced to insignificant levels. For telecommunication needs, such methods have been required in order to ensure comparability of measurements and allow some freedom for vendors to do different implementations. The ITU-T Rec. G.813 for the TDEV measurement.\nIt can be recommended that the first formula_10 multiples be ignored, such that the majority of the detected noise is well within the passband of the measurement systems bandwidth.\nFurther developments on the Allan variance was performed to let the hardware bandwidth be reduced by software means. This development of a software bandwidth allowed addressing the remaining noise, and the method is now referred to modified Allan variance. This bandwidth reduction technique should not be confused with the enhanced variant of modified Allan variance, which also changes a smoothing filter bandwidth.\nDead time in measurements.\nMany measurement instruments of time and frequency have the stages of arming time, time-base time, processing time and may then re-trigger the arming. The arming time is from the time the arming is triggered to when the start event occurs on the start channel. The time-base then ensures that minimal amount of time goes prior to accepting an event on the stop channel as the stop event. The number of events and time elapsed between the start event and stop event is recorded and presented during the processing time. When the processing occurs (also known as the dwell time), the instrument is usually unable to do another measurement. After the processing has occurred, an instrument in continuous mode triggers the arm circuit again. The time between the stop event and the following start event becomes dead time, during which the signal is not being observed. Such dead time introduces systematic measurement biases, which needs to be compensated for in order to get proper results. For such measurement systems will the time \"T\" denote the time between the adjacent start events (and thus measurements), while formula_3 denote the time-base length, i.e. the nominal length between the start and stop event of any measurement.\nDead-time effects on measurements have such an impact on the produced result that much study of the field have been done in order to quantify its properties properly. The introduction of zero-dead-time counters removed the need for this analysis. A zero-dead-time counter has the property that the stop event of one measurement is also being used as the start event of the following event. Such counters create a series of event and time timestamp pairs, one for each channel spaced by the time-base. Such measurements have also proved useful in order forms of time-series analysis.\nMeasurements being performed with dead time can be corrected using the bias function \"B\"1, \"B\"2 and \"B\"3. Thus, dead time as such is not prohibiting the access to the Allan variance, but it makes it more problematic. The dead time must be known, such that the time between samples \"T\" can be established.\nMeasurement length and effective use of samples.\nStudying the effect on the confidence intervals that the length \"N\" of the sample series have and the effect of the variable \"\u03c4\" parameter \"n,\" the confidence intervals may become very large since the effective degree of freedom may become small for some combination of \"N\" and \"n\" for the dominant noise form (for that \"\u03c4\").\nThe effect may be that the estimated value may be much smaller or much greater than the real value, which may lead to false conclusions of the result.\nIt is recommended that:\nDominant noise type.\nA large number of conversion constants, bias corrections and confidence intervals depends on the dominant noise type. For proper interpretation shall the dominant noise type for the particular \"\u03c4\" of interest be identified through noise identification. Failing to identify the dominant noise type will produce biased values. Some of these biases may be of several order of magnitude, so it may be of large significance.\nLinear drift.\nSystematic effects on the signal is only partly cancelled. Phase and frequency offset is cancelled, but linear drift or other high-degree forms of polynomial phase curves will not be cancelled and thus form a measurement limitation. Curve fitting and removal of systematic offset could be employed. Often removal of linear drift can be sufficient. Use of linear-drift estimators such as the Hadamard variance could also be employed. A linear drift removal could be employed using a moment-based estimator.\nMeasurement instrument estimator bias.\nTraditional instruments provided only the measurement of single events or event pairs. The introduction of the improved statistical tool of overlapping measurements by J. J. Snyder allowed much improved resolution in frequency readouts, breaking the traditional digits/time-base balance. While such methods is useful for their intended purpose, using such smoothed measurements for Allan variance calculations would give a false impression of high resolution, but for longer \"\u03c4\" the effect is gradually removed, and the lower-\"\u03c4\" region of the measurement has biased values. This bias is providing lower values than it should, so it is an overoptimistic (assuming that low numbers is what one wishes) bias, reducing the usability of the measurement rather than improving it. Such smart algorithms can usually be disabled or otherwise circumvented by using time-stamp mode, which is much preferred if available.\nPractical measurements.\nWhile several approaches to measurement of Allan variance can be devised, a simple example may illustrate how measurements can be performed.\nMeasurement.\nAll measurements of Allan variance will in effect be the comparison of two different clocks. Consider a reference clock and a device under test (DUT), and both having a common nominal frequency of 10\u00a0MHz. A time-interval counter is being used to measure the time between the rising edge of the reference (channel A) and the rising edge of the device under test.\nIn order to provide evenly spaced measurements, the reference clock will be divided down to form the measurement rate, triggering the time-interval counter (ARM input). This rate can be 1\u00a0Hz (using the 1 PPS output of a reference clock), but other rates like 10\u00a0Hz and 100\u00a0Hz can also be used. The speed of which the time-interval counter can complete the measurement, output the result and prepare itself for the next arm will limit the trigger frequency.\nA computer is then useful to record the series of time differences being observed.\nPost-processing.\nThe recorded time-series require post-processing to unwrap the wrapped phase, such that a continuous phase error is being provided. If necessary, logging and measurement mistakes should also be fixed. Drift estimation and drift removal should be performed, the drift mechanism needs to be identified and understood for the sources. Drift limitations in measurements can be severe, so letting the oscillators become stabilized, by long enough time being powered on, is necessary.\nThe Allan variance can then be calculated using the estimators given, and for practical purposes the overlapping estimator should be used due to its superior use of data over the non-overlapping estimator. Other estimators such as total or Theo variance estimators could also be used if bias corrections is applied such that they provide Allan variance-compatible results.\nTo form the classical plots, the Allan deviation (square root of Allan variance) is plotted in log\u2013log format against the observation interval\u00a0\"\u03c4\".\nEquipment and software.\nThe time-interval counter is typically an off-the-shelf counter commercially available. Limiting factors involve single-shot resolution, trigger jitter, speed of measurements and stability of reference clock. The computer collection and post-processing can be done using existing commercial or public-domain software. Highly advanced solutions exists, which will provide measurement and computation in one box.\nResearch history.\nThe field of frequency stability has been studied for a long time. However, during the 1960s it was found that coherent definitions were lacking. A NASA-IEEE Symposium on Short-Term Stability in November 1964 resulted in the special February 1966 issue of the IEEE Proceedings on Frequency Stability.\nThe NASA-IEEE Symposium brought together many fields and uses of short- and long-term stability, with papers from many different contributors. The articles and panel discussions concur on the existence of the frequency flicker noise and the wish to achieve a common definition for both short-term and long-term stability.\nImportant papers, including those of David Allan, James A. Barnes, L. S. Cutler and C. L. Searle and D. B. Leeson, appeared in the IEEE Proceedings on Frequency Stability and helped shape the field.\nDavid Allan's article analyses the classical \"M\"-sample variance of frequency, tackling the issue of dead-time between measurements along with an initial bias function. Although Allan's initial bias function assumes no dead-time, his formulas do include dead-time calculations. His article analyses the case of M frequency samples (called N in the article) and variance estimators. It provides the now standard \u03b1\u2013\u03bc mapping, clearly building on James Barnes' work in the same issue.\nThe 2-sample variance case is a special case of the \"M\"-sample variance, which produces an average of the frequency derivative. Allan implicitly uses the 2-sample variance as a base case, since for arbitrary chosen \"M\", values may be transferred via the 2-sample variance to the \"M\"-sample variance. No preference was clearly stated for the 2-sample variance, even if the tools were provided. However, this article laid the foundation for using the 2-sample variance as a way of comparing other \"M\"-sample variances.\nJames Barnes significantly extended the work on bias functions, introducing the modern \"B\"1 and \"B\"2 bias functions. Curiously enough, it refers to the \"M\"-sample variance as \"Allan variance\", while referring to Allan's article \"Statistics of Atomic Frequency Standards\". With these modern bias functions, full conversion among \"M\"-sample variance measures of various \"M\", \"T\" and \"\u03c4\" values could be performed, by conversion through the 2-sample variance.\nJames Barnes and David Allan further extended the bias functions with the \"B\"3 function to handle the concatenated samples estimator bias. This was necessary to handle the new use of concatenated sample observations with dead-time in between.\nIn 1970, the IEEE Technical Committee on Frequency and Time, within the IEEE Group on Instrumentation &amp; Measurements, provided a summary of the field, published as NBS Technical Notice 394. This paper was first in a line of more educational and practical papers helping fellow engineers grasp the field. This paper recommended the 2-sample variance with \"T\" = \"\u03c4\", referring to it as Allan variance (now without the quotes). The choice of such parametrisation allows good handling of some noise forms and getting comparable measurements; it is essentially the least common denominator with the aid of the bias functions \"B\"1 and \"B\"2.\nJ. J. Snyder proposed an improved method for frequency or variance estimation, using sample statistics for frequency counters. To get more effective degrees of freedom out of the available dataset, the trick is to use overlapping observation periods. This provides a \u221a\"n\" improvement, and was incorporated in the overlapping Allan variance estimator. Variable-\u03c4 software processing was also incorporated. This development improved the classical Allan variance estimators, likewise providing a direct inspiration for the work on modified Allan variance.\nHowe, Allan and Barnes presented the analysis of confidence intervals, degrees of freedom, and the established estimators.\nEducational and practical resources.\nThe field of time and frequency and its use of Allan variance, Allan deviation and friends is a field involving many aspects, for which both understanding of concepts and practical measurements and post-processing requires care and understanding. Thus, there is a realm of educational material stretching about 40 years available. Since these reflect the developments in the research of their time, they focus on teaching different aspect over time, in which case a survey of available resources may be a suitable way of finding the right resource.\nThe first meaningful summary is the NBS Technical Note 394 \"Characterization of Frequency Stability\". This is the product of the Technical Committee on Frequency and Time of the IEEE Group on Instrumentation &amp; Measurement. It gives the first overview of the field, stating the problems, defining the basic supporting definitions and getting into Allan variance, the bias functions \"B\"1 and \"B\"2, the conversion of time-domain measures. This is useful, as it is among the first references to tabulate the Allan variance for the five basic noise types.\nA classical reference is the NBS Monograph 140 from 1974, which in chapter 8 has \"Statistics of Time and Frequency Data Analysis\". This is the extended variant of NBS Technical Note 394 and adds essentially in measurement techniques and practical processing of values.\nAn important addition will be the \"Properties of signal sources and measurement methods\". It covers the effective use of data, confidence intervals, effective degree of freedom, likewise introducing the overlapping Allan variance estimator. It is a highly recommended reading for those topics.\nThe IEEE standard 1139 \"Standard definitions of Physical Quantities for Fundamental Frequency and Time Metrology\" is beyond that of a standard a comprehensive reference and educational resource.\nA modern book aimed towards telecommunication is Stefano Bregni \"Synchronisation of Digital Telecommunication Networks\". This summarises not only the field, but also much of his research in the field up to that point. It aims to include both classical measures and telecommunication-specific measures such as MTIE. It is a handy companion when looking at measurements related to telecommunication standards.\nThe NIST Special Publication 1065 \"Handbook of Frequency Stability Analysis\" of W. J. Riley is a recommended reading for anyone wanting to pursue the field. It is rich of references and also covers a wide range of measures, biases and related functions that a modern analyst should have available. Further it describes the overall processing needed for a modern tool.\nUses.\nAllan variance is used as a measure of frequency stability in a variety of precision oscillators, such as crystal oscillators, atomic clocks and frequency-stabilized lasers over a period of a second or more. Short-term stability (under a second) is typically expressed as phase noise. The Allan variance is also used to characterize the bias stability of gyroscopes, including fiber optic gyroscopes, hemispherical resonator gyroscopes and MEMS gyroscopes and accelerometers.\n50th anniversary.\nIn 2016, https:// is going to be publishing a \"Special Issue to celebrate the 50th anniversary of the Allan Variance (1966\u20132016)\". A guest editor for that issue will be David's former colleague at NIST, Judah Levine, who is the most recent recipient of the I. I. Rabi Award.\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "40709", "revid": "15936215", "url": "https://en.wikipedia.org/wiki?curid=40709", "title": "Alphabet transliteration", "text": ""}
{"id": "40712", "revid": "28481209", "url": "https://en.wikipedia.org/wiki?curid=40712", "title": "Ambient noise level", "text": "Background sound pressure level\nIn atmospheric sounding and noise pollution, ambient noise level (sometimes called background noise level, reference sound level, or room noise level) is the background sound pressure level at a given location, normally specified as a reference level to study a new intrusive sound source.\nAmbient sound levels are often measured in order to map sound conditions over a spatial regime to understand their variation with locale. In this case the product of the investigation is a sound level contour map. Alternatively ambient noise levels may be measured to provide a reference point for analyzing an intrusive sound to a given environment. For example, sometimes aircraft noise is studied by measuring ambient sound without presence of any overflights, and then studying the noise addition by measurement or computer simulation of overflight events. Or roadway noise is measured as ambient sound, prior to introducing a hypothetical noise barrier intended to reduce that ambient noise level. \nAmbient noise level is measured with a sound level meter. It is usually measured in dB relative to a reference pressure of 0.00002 Pa, \"i.e.,\" 20 \u03bcPa (micropascals) in SI units. This is because 20 \u03bcPa is the faintest sound the human ear can detect. A pascal is a newton per square meter. The centimeter-gram-second system of units, the reference sound pressure for measuring ambient noise level is 0.0002 dyn/cm2, or 0.00002 N/m2. Most frequently ambient noise levels are measured using a frequency weighting filter, the most common being the A-weighting scale, such that resulting measurements are denoted dB(A), or decibels on the A-weighting scale.\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "40713", "revid": "28903366", "url": "https://en.wikipedia.org/wiki?curid=40713", "title": "Amplitude distortion", "text": "Amplitude distortion is distortion occurring in a system, subsystem, or device when the output amplitude is not a linear function of the input amplitude under specified conditions. \nGenerally, output is a linear function of input only for a fixed portion of the transfer characteristics. In this region, Ic=\u03b2Ib where Ic is collector current and Ib is base current, following linear relation y=mx.\nForms.\nWhen output is not in this portion, two forms of amplitude distortion might arise:\nDue to the additional outputs, this form of distortion is definitely unwanted in audio, radio and telecommunication amplifiers, and it occurs for more than two waves as well.\nIn a narrowband system such as a radio communication system, unwanted outputs such as X-Y and 2X+Y will be remote from the wanted band and so be ignored by the system. In contrast, 2X-Y and 2Y-X will be close to the wanted signals. These so-called third order distortion products (third order as m+n = 3) tend to dominate the non-linear distortion of narrowband systems.\nMeasurement.\nAmplitude distortion is measured with the system operating under steady-state conditions with a sinusoidal input signal. When other frequencies are present, the term \"amplitude\" refers to that of the fundamental only."}
{"id": "40714", "revid": "3121467", "url": "https://en.wikipedia.org/wiki?curid=40714", "title": "Analog decoding", "text": ""}
{"id": "40715", "revid": "140084", "url": "https://en.wikipedia.org/wiki?curid=40715", "title": "Analog (signal)", "text": ""}
{"id": "40716", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=40716", "title": "Angular misalignment loss", "text": "In waveguide design and construction, angular misalignment loss is power loss caused by the deviation from optimum angular alignment of the axes of source-to-waveguide, waveguide-to-waveguide, or waveguide-to-detector. The waveguide may be dielectric (an optical fiber) or metallic. Angular misalignment loss does not include lateral offset loss and longitudinal offset loss.\nSource: from Federal Standard 1037C\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "40717", "revid": "6727347", "url": "https://en.wikipedia.org/wiki?curid=40717", "title": "Antenna blind cone", "text": "Place where radio signals cannot be received over their transmitter\nIn telecommunications, antenna blind cone (sometimes called a cone of silence or antenna blind spot) is the volume of space, usually approximately conical with its vertex at the antenna, that cannot be scanned by an antenna because of limitations of the antenna radiation pattern and mount.\nThe concept was encountered as early as the 1950s in low-frequency radio ranges, when it was used to determine when an aircraft was directly over a station. As the signal may not completely fade away, the aircraft's position could be confirmed by listening for a station location, or \"Z\", marker.\nAn Air Route Surveillance Radar (ARSR) is an example of an antenna blind cone. The horizontal radiation pattern of an ARSR antenna is very narrow, and the vertical radiation pattern is fan-shaped, reaching approximately 70\u00b0 of elevation above the horizontal plane. As the fan antenna is rotated about a vertical axis, it can illuminate targets only if they are 70\u00b0 or less from the horizontal plane. Above that elevation, they are in the antenna blind cone.\nThe antenna blind cone is also referred to as the \"cone of silence\", especially in America. This term is also used for weather radars. NEXRAD radars make two-dimensional scans at varying angles ranging from 0.5\u00b0 above level to 19.5\u00b0 above level (during a significant weather event). These levels become much closer to the ground, and closer to each other, as they get closer to the radar site, rendering them of little use for the three-dimensional profiling such multi-level scanning is meant to provide. Thus, a weather event located very close to and/or directly overhead of the radar site will be mostly situated in the \"cone of silence.\" This is part of the reason why most U.S. weather radars partially overlap each other's territories.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "40718", "revid": "11292982", "url": "https://en.wikipedia.org/wiki?curid=40718", "title": "Antenna effective area", "text": ""}
{"id": "40719", "revid": "139104", "url": "https://en.wikipedia.org/wiki?curid=40719", "title": "Antenna height above average terrain", "text": ""}
{"id": "40720", "revid": "7611264", "url": "https://en.wikipedia.org/wiki?curid=40720", "title": "Noise temperature (antenna)", "text": "In radio frequency (RF) applications such as radio, radar and telecommunications, noise temperature of an antenna is a measure of the noise power density contributed by the antenna to the overall RF receiver system. It is defined as \"the temperature of a resistor having an available thermal noise power per unit bandwidth equal to that at the antenna's output at a specified frequency\". In other words, antenna noise temperature is a parameter that describes how much noise an antenna produces in a given environment. This temperature is not the physical temperature of the antenna. Moreover, an antenna does not have an intrinsic \"antenna temperature\" associated with it; rather the temperature depends on its gain pattern, pointing direction, and the thermal environment that it is placed in.\nMathematics.\nIn RF applications, noise power is defined using the relationship \"P\"noise = \"kTB\", where \"k\" is the Boltzmann constant, \"T\" is the noise temperature, and \"B\" is the noise bandwidth. Typically the noise bandwidth is determined by the bandwidth of the intermediate frequency (IF) filter of the radio receiver. Thus, we can define the noise temperature as:\n formula_1\nBecause \"k\" is a constant, we can effectively think of \"T\" as noise power spectral density (with unit W/Hz) normalized by \"k\".\nAntenna noise is only one of the contributors to the overall noise temperature of an RF receiver system, so it is typically subscripted, such as \"T\"A. It is added directly to the effective noise temperature of the receiver to obtain the overall system noise temperature:\n formula_2\nSources of antenna noise.\nAntenna noise temperature has contributions from many sources, including:\nGalactic noise is high below 1000\u00a0MHz. At around 150\u00a0MHz, it is approximately 1000\u00a0K. At 2500\u00a0MHz, it has leveled off to around 10\u00a0K .\nEarth has an accepted standard temperature of 288 K.\nThe level of the Sun's contribution depends on the solar flux. It is given by\n formula_3\nwhere formula_4 is the solar flux,\n formula_5 is the wavelength,\nand formula_6 is the logarithmic gain of the antenna in decibels.\nThe antenna noise temperature depends on antenna coupling to all noise sources in its environment as well as on noise generated within the antenna. That is, in a directional antenna, the portion of the noise source that the antenna's main and side lobes intersect contribute proportionally.\nFor example, a satellite antenna may not receive noise contribution from the Earth in its main lobe, but sidelobes will contribute a portion of the 288\u00a0K Earth noise to its overall noise temperature."}
{"id": "40721", "revid": "11994166", "url": "https://en.wikipedia.org/wiki?curid=40721", "title": "Aperture-to-medium coupling loss", "text": "Concept in antenna theory\nIn telecommunications, aperture-to-medium coupling loss is the difference between the theoretical antenna gain of a very large antenna, such as the antennas in beyond-the-horizon microwave links, and the gain that can be realized in practice. \n\"Note 1:\" Aperture-to-medium coupling loss is related to the ratio of the scatter angle to the antenna beamwidth. \n\"Note 2:\" The \"very large antennas\" are referred to in wavelengths; thus, this loss can apply to line-of-sight systems also."}
{"id": "40722", "revid": "4625892", "url": "https://en.wikipedia.org/wiki?curid=40722", "title": "Apparent power", "text": ""}
{"id": "40724", "revid": "40192293", "url": "https://en.wikipedia.org/wiki?curid=40724", "title": "Arithmetic overflow", "text": ""}
{"id": "40725", "revid": "49941710", "url": "https://en.wikipedia.org/wiki?curid=40725", "title": "Arithmetic shift", "text": "Shift operator in computer programming\nIn computer programming, an arithmetic shift is a shift operator, sometimes termed a signed shift (though it is not restricted to signed operands). The two basic types are the arithmetic left shift and the arithmetic right shift. For binary numbers it is a bitwise operation that shifts all of the bits of its operand; every bit in the operand is simply moved a given number of bit positions, and the vacant bit-positions are filled in. Instead of being filled with all 0s, as in logical shift, when shifting to the right, the leftmost bit (usually the sign bit in signed integer representations) is replicated to fill in all the vacant positions (this is a kind of sign extension).\nSome authors prefer the terms \"sticky right-shift\" and \"zero-fill right-shift\" for arithmetic and logical shifts respectively.\nArithmetic shifts can be useful as efficient ways to perform multiplication or division of signed integers by powers of two. Shifting left by \"n\" bits on a signed or unsigned binary number has the effect of multiplying it by 2\"n\". Shifting right by \"n\" bits on a two's complement \"signed\" binary number has the effect of dividing it by 2\"n\", but it always rounds down (towards negative infinity). This is different from the way rounding is usually done in signed integer division (which rounds towards 0). This discrepancy has led to bugs in a number of compilers.\nFor example, in the x86 instruction set, the SAR instruction (arithmetic right shift) divides a signed number by a power of two, rounding towards negative infinity. However, the IDIV instruction (signed divide) divides a signed number, rounding towards zero. So a SAR instruction cannot be substituted for an IDIV by power of two instruction nor vice versa.\nFormal definition.\nThe formal definition of an arithmetic shift, from Federal Standard 1037C is that it is:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;A shift, applied to the representation of a number in a fixed radix numeration system and in a fixed-point representation system, and in which only the characters representing the fixed-point part of the number are moved. An arithmetic shift is usually equivalent to multiplying the number by a positive or a negative integral power of the radix, except for the effect of any rounding; compare the logical shift with the arithmetic shift, especially in the case of floating-point representation.\nAn important word in the FS 1073C definition is \"usually\".\nNon-equivalence of arithmetic right shift and division.\nHowever, arithmetic \"right\" shifts are major traps for the unwary, specifically in treating rounding of negative integers. For example, in the usual two's complement representation of negative integers, \u22121 is represented as all 1's. For an 8-bit signed integer this is 1111\u00a01111. An arithmetic right-shift by 1 (or 2, 3, ..., 7) yields 1111\u00a01111 again, which is still \u22121. This corresponds to rounding down (towards negative infinity), but is not the usual convention for division.\nIt is frequently stated that arithmetic right shifts are equivalent to division by a (positive, integral) power of the radix (e.g., a division by a power of 2 for binary numbers), and hence that division by a power of the radix can be optimized by implementing it as an arithmetic right shift. (A shifter is much simpler than a divider. On most processors, shift instructions will execute faster than division instructions.) Large number of 1960s and 1970s programming handbooks, manuals, and other specifications from companies and institutions such as DEC, IBM, Data General, and ANSI make such incorrect statements.\nLogical right shifts are equivalent to division by a power of the radix (usually 2) only for positive or unsigned numbers. Arithmetic right shifts are equivalent to logical right shifts for positive signed numbers. Arithmetic right shifts for negative numbers in N's complement (usually two's complement) is roughly equivalent to division by a power of the radix (usually 2), where for odd numbers rounding downwards is applied (not towards 0 as usually expected).\nArithmetic right shifts for negative numbers are equivalent to division using rounding towards 0 in ones' complement representation of signed numbers as was used by some historic computers, but this is no longer in general use.\nHandling the issue in programming languages.\nThe (1999) ISO standard for the programming language C defines the right shift operator in terms of divisions by powers of 2. Because of the above-stated non-equivalence, the standard explicitly excludes from that definition the right shifts of signed numbers that have negative values. It does not specify the behaviour of the right shift operator in such circumstances, but instead requires each individual C compiler to define the behaviour of shifting negative values right.\nLike C, C++ had an implementation-defined right shift for signed integers until C++20. Starting in the C++20 standard, right shift of a signed integer is defined to be an arithmetic shift.\nApplications.\nIn applications where consistent rounding down is desired, arithmetic right shifts for signed values are useful. An example is in downscaling raster coordinates by a power of two, which maintains even spacing. For example, right shift by 1 sends 0, 1, 2, 3, 4, 5, ... to 0, 0, 1, 1, 2, 2, ..., and \u22121, \u22122, \u22123, \u22124, ... to \u22121, \u22121, \u22122, \u22122, ..., maintaining even spacing as \u22122, \u22122, \u22121, \u22121, 0, 0, 1, 1, 2, 2, ... In contrast, integer division with rounding towards zero sends \u22121, 0, and 1 all to 0 (3 points instead of 2), yielding \u22122, \u22121, \u22121, 0, 0, 0, 1, 1, 2, 2, ... instead, which is irregular at 0.\nReferences.\nCross-reference.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nSources used.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "40726", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=40726", "title": "Automatic repeat request", "text": "Error-control method for data transmission\nAutomatic repeat request (ARQ), also known as automatic repeat query, is an error-control method for data transmission that uses acknowledgements (messages sent by the receiver indicating that it has correctly received a message) and timeouts (specified periods of time allowed to elapse before an acknowledgment is to be received) If the sender does not receive an acknowledgment before the timeout, it re-transmits the message until it receives an acknowledgment or exceeds a predefined number of retransmissions. \nARQ is used to achieve reliable data transmission over an unreliable communication channel. ARQ is appropriate if the communication channel has varying or unknown capacity. \nVariations of ARQ protocols include Stop-and-wait ARQ, Go-Back-N ARQ, and Selective Repeat ARQ. All three protocols usually use some form of sliding window protocol to help the sender determine which (if any) packets need to be retransmitted. These protocols reside in the data link or transport layers (layers 2 and 4) of the OSI model.\nExamples.\nThe Transmission Control Protocol uses a variant of Go-Back-N ARQ to ensure reliable transmission of data over the Internet Protocol, which does not provide guaranteed delivery of packets; with Selective Acknowledgement (SACK), it uses Selective Repeat ARQ.\nIEEE 802.11 wireless networking uses ARQ retransmissions at the data-link layer.\nITU-T G.hn uses hybrid ARQ, a mixture of high-rate forward error correction (FEC) and ARQ. It is a high-speed local area network standard that can operate at data rates up to 1\u00a0Gbit/s over existing home wiring (power lines, phone lines and coaxial cables). G.hn uses CRC-32C for Error Detection, LDPC for FEC and selective repeat for ARQ.\nARQ systems are widely used on shortwave radio to ensure reliable delivery of data such as for telegrams. These systems came in forms called ARQ-E and ARQ-M, which also included the ability to multiplex two or four channels.\nA number of patents exist for the use of ARQ in live video contribution environments. In these high throughput environments negative acknowledgements are used to drive down overheads.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "40727", "revid": "1461430", "url": "https://en.wikipedia.org/wiki?curid=40727", "title": "Articulation score", "text": "In telecommunications, an articulation score (AS) is a subjective measure of the intelligibility of a voice system in terms of the percentage of words correctly understood over a channel perturbed by interference.\nArticulation scores have been experimentally obtained as functions of varying word content, bandwidth, audio signal-to-noise ratio and the experience of the talkers and listeners involved.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "40728", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=40728", "title": "Artificial transmission line", "text": "In telecommunications, an artificial transmission line is a two-port electrical network that has the characteristic impedance, transmission time delay, phase shift, or other parameter(s) of a real transmission line. It can be used to simulate a real transmission line in one or more of these respects.\nEarly artificial lines were used in telephony research and took the form of a cascade of lattice phase equalisers to provide the necessary delay. The lattice phase circuit was invented by Otto Zobel in the 1920s.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "40729", "revid": "1479481", "url": "https://en.wikipedia.org/wiki?curid=40729", "title": "Associative storage", "text": ""}
{"id": "40730", "revid": "2378565", "url": "https://en.wikipedia.org/wiki?curid=40730", "title": "Asynchronous communications system", "text": ""}
{"id": "40731", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=40731", "title": "Asynchronous operation", "text": "In telecommunications, asynchronous operation or asynchronous working is where a sequence of operations is executed such that the operations are executed out of time coincidence with any event. It can also be an operation that occurs without a regular or predictable time relationship to a specified event; e.g., the calling of an error diagnostic routine that may receive control at any time during the execution of a computer program.\nSources.\nFrom Federal Standard 1037C and from MIL-STD-188"}
{"id": "40732", "revid": "27823944", "url": "https://en.wikipedia.org/wiki?curid=40732", "title": "Atmospheric duct", "text": "Horizontal layer that propagates electromagnetic radiation\nIn telecommunications, an atmospheric duct is a horizontal layer in the lower atmosphere in which the vertical refractive index gradients are such that radio signals (and light rays) are guided or ducted, tend to follow the curvature of the Earth, and experience less attenuation in the ducts than they would if the ducts were not present. The duct acts as an atmospheric dielectric waveguide and limits the spread of the wavefront to only the horizontal dimension.\nAtmospheric ducting is a mode of propagation of electromagnetic radiation, usually in the lower layers of Earth\u2019s atmosphere, where the waves are bent by atmospheric refraction. In over-the-horizon radar, ducting causes part of the radiated and target-reflection energy of a radar system to be guided over distances far greater than the normal radar range. It also causes long-distance propagation of radio signals in bands that would normally be limited to line of sight.\nNormally radio \"ground waves\" propagate along the surface as creeping waves. That is, they are only diffracted around the curvature of the earth. This is one reason that early long-distance radio communication used long wavelengths. The best known exception is that HF (3\u201330\u00a0MHz.) waves are reflected by the ionosphere.\nThe reduced refractive index due to lower densities at the higher altitudes in the Earth's atmosphere bends the signals back toward the Earth. Signals in a higher refractive index layer, \"i.e.,\" duct, tend to remain in that layer because of the reflection and refraction encountered at the boundary with a lower refractive index material. In some weather conditions, such as inversion layers, density changes so rapidly that waves are guided around the curvature of the earth at constant altitude.\nPhenomena of atmospheric optics related to atmospheric ducting include the green flash, Fata Morgana, superior mirage, mock mirage of astronomical objects and the Novaya Zemlya effect.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "40733", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=40733", "title": "Attack time", "text": "In professional audio and telecommunications, attack time is the time between the instant that a signal at the input of a device or circuit exceeds the activation threshold of the device or circuit and the instant that the device or circuit reacts in a specified manner, or to a specified degree, to the input. Attack time occurs in devices such as clippers, peak limiters, compressors, and voxes.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "40734", "revid": "50789094", "url": "https://en.wikipedia.org/wiki?curid=40734", "title": "ARJ", "text": "Compression software\nARJ (Archived by Robert Jung) is a software tool designed in 1991 by Robert K. Jung for creating high-efficiency compressed file archives. ARJ is currently on version 2.86 for MS-DOS and 3.31 for Microsoft Windows and supports 16-bit, 32-bit and 64-bit Intel architectures.\nARJ was one of many file compression utilities for MS-DOS and Microsoft Windows during the early and mid-1990s. Parts of ARJ were covered by https:// (expired). ARJ is well-documented and includes over 150 command line switches.\nFile format support in other software.\nARJ archives can be unpacked with various tools other than the ARJ software. There exists a free software re-implementation of the tool. A number of software utilities, including 7-Zip, Total Commander, Zipeg, and WinRAR can also unpack .arj files. For macOS, standalone utilities, such as DeArj and UnArjMac, are available.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "40735", "revid": "46512407", "url": "https://en.wikipedia.org/wiki?curid=40735", "title": "Attenuation", "text": "Gradual loss of flux intensity through a medium\nIn physics, attenuation is the gradual loss of flux intensity through a medium. For instance, dark glasses attenuate sunlight, lead attenuates X-rays, and water and air attenuate both light and sound at variable attenuation rates. \nHearing protectors help reduce acoustic flux from flowing into the ears. This phenomenon is called acoustic attenuation and is measured in decibels (dBs).\nIn electrical engineering and telecommunications, attenuation affects the propagation of waves and signals in electrical circuits, in optical fibers, and in air. Electrical attenuators and optical attenuators are commonly manufactured components in this field.\nBackground.\nIn many cases, attenuation is an exponential function of the path length through the medium. In optics and in chemical spectroscopy, this is known as the Beer\u2013Lambert law. In engineering, attenuation is usually measured in units of decibels per unit length of medium (dB/cm, dB/km, etc.) and is represented by the attenuation coefficient of the medium in question. Attenuation also occurs in earthquakes; when the seismic waves move farther away from the hypocenter, they grow smaller as they are attenuated by the ground.\nUltrasound.\nOne area of research in which attenuation plays a prominent role is in ultrasound physics. Attenuation in ultrasound is the reduction in amplitude of the ultrasound beam as a function of distance through the imaging medium. Accounting for attenuation effects in ultrasound is important because a reduced signal amplitude can affect the quality of the image produced. By knowing the attenuation that an ultrasound beam experiences traveling through a medium, one can adjust the input signal amplitude to compensate for any loss of energy at the desired imaging depth.\nWave equations which take acoustic attenuation into account can be written on a fractional derivative form.\nIn homogeneous media, the main physical properties contributing to sound attenuation are viscosity and thermal conductivity.\nAttenuation coefficient.\nAttenuation coefficients are used to quantify different media according to how strongly the transmitted ultrasound amplitude decreases as a function of frequency. The attenuation coefficient (formula_1) can be used to determine total attenuation in dB in the medium using the following formula:\n formula_2\nAttenuation is linearly dependent on the medium length and attenuation coefficient, as well as \u2013 approximately \u2013 the frequency of the incident ultrasound beam for biological tissue (while for simpler media, such as air, the relationship is quadratic). Attenuation coefficients vary widely for different media. In biomedical ultrasound imaging however, biological materials and water are the most commonly used media. The attenuation coefficients of common biological materials at a frequency of 1\u00a0MHz are listed below:\nThere are two general ways of acoustic energy losses: absorption and scattering.\nUltrasound propagation through homogeneous media is associated only with absorption and can be characterized with absorption coefficient only. Propagation through heterogeneous media requires taking into account scattering.\nLight attenuation in water.\nShortwave radiation emitted from the Sun have wavelengths in the visible spectrum of light that range from 360\u00a0nm (violet) to 750\u00a0nm (red). When the Sun's radiation reaches the sea surface, the shortwave radiation is attenuated by the water, and the intensity of light decreases exponentially with water depth. The intensity of light at depth can be calculated using the Beer-Lambert Law.\nIn clear mid-ocean waters, visible light is absorbed most strongly at the longest wavelengths. Thus, red, orange, and yellow wavelengths are totally absorbed at shallower depths, while blue and violet wavelengths reach deeper in the water column. Because the blue and violet wavelengths are absorbed least compared to the other wavelengths, open-ocean waters appear deep blue to the eye.\nNear the shore, coastal water contains more phytoplankton than the very clear mid-ocean waters. Chlorophyll-a pigments in the phytoplankton absorb light, and the plants themselves scatter light, making coastal waters less clear than mid-ocean waters. Chlorophyll-a absorbs light most strongly in the shortest wavelengths (blue and violet) of the visible spectrum. In coastal waters where high concentrations of phytoplankton occur, the green wavelength reaches the deepest in the water column and the color of water appears blue-green or green.\nSeismic.\nThe energy with which an earthquake affects a location depends on the running distance. The attenuation in the signal of ground motion intensity plays an important role in the assessment of possible strong groundshaking. A seismic wave loses energy as it propagates through the earth (seismic attenuation). This phenomenon is tied into the dispersion of the seismic energy with the distance. There are two types of dissipated energy:\nIn porous fluid\u2014saturated sedimentary rocks such as sandstones, intrinsic attenuation of seismic waves is primarily caused by the wave-induced flow of the pore fluid relative to the solid frame.\nElectromagnetic.\nAttenuation decreases the intensity of electromagnetic radiation due to absorption or scattering of photons. Attenuation does not include the decrease in intensity due to inverse-square law geometric spreading. Therefore, calculation of the total change in intensity involves both the inverse-square law and an estimation of attenuation over the path.\nThe primary causes of attenuation in matter are the photoelectric effect, Compton scattering, and, for photon energies of above 1.022 MeV, pair production.\nCoaxial and general RF cables.\nThe attenuation of RF cables is defined by:\n formula_3\nwhere formula_4 is the input power into a 100\u00a0m long cable terminated with the nominal value of its characteristic impedance, and\nformula_5 is the output power at the far end of this cable.\nAttenuation in a coaxial cable is a function of the materials and the construction.\nRadiography.\nThe beam of X-ray is attenuated when photons are absorbed when the x-ray beam passes through the tissue. Interaction with matter varies between high energy photons and low energy photons. Photons travelling at higher energy are more capable of travelling through a tissue specimen as they have less chances of interacting with matter. This is mainly due to the photoelectric effect which states that \"the probability of photoelectric absorption is approximately proportional to (Z/E)3, where Z is the atomic number of the tissue atom and E is the photon energy. In context of this, an increase in photon energy (E) will result in a rapid decrease in the interaction with matter.\nIn CT imaging, attenuation describes the density or darkness of the image.\nOptics.\nAttenuation in fiber optics, also known as transmission loss, is the reduction in intensity of the light beam (or signal) with respect to distance travelled through a transmission medium. Attenuation coefficients in fiber optics usually use units of dB/km through the medium due to the relatively high quality of transparency of modern optical transmission. The medium is typically a fiber of silica glass that confines the incident light beam to the inside. Attenuation is an important factor limiting the transmission of a digital signal across large distances. Thus, much research has gone into both limiting the attenuation and maximizing the amplification of the optical signal.\nEmpirical research has shown that attenuation in optical fiber is caused primarily by both scattering and absorption.\nAttenuation in fiber optics can be quantified using the following equation:\n formula_6\nLight scattering.\nThe propagation of light through the core of an optical fiber is based on total internal reflection of the lightwave. Rough and irregular surfaces, even at the molecular level of the glass, can cause light rays to be reflected in many random directions. This type of reflection is referred to as \"diffuse reflection\", and it is typically characterized by wide variety of reflection angles. Most objects that can be seen with the naked eye are visible due to diffuse reflection. Another term commonly used for this type of reflection is \"light scattering\". Light scattering from the surfaces of objects is our primary mechanism of physical observation.\nLight scattering from many common surfaces can be modelled by reflectance.\nLight scattering depends on the wavelength of the light being scattered. Thus, limits to spatial scales of visibility arise, depending on the frequency of the incident lightwave and the physical dimension (or spatial scale) of the scattering center, which is typically in the form of some specific microstructural feature. For example, since visible light has a wavelength scale on the order of one micrometer, scattering centers will have dimensions on a similar spatial scale.\nThus, attenuation results from the incoherent scattering of light at internal surfaces and interfaces. In (poly)crystalline materials such as metals and ceramics, in addition to pores, most of the internal surfaces or interfaces are in the form of grain boundaries that separate tiny regions of crystalline order. It has recently been shown that, when the size of the scattering center (or grain boundary) is reduced below the size of the wavelength of the light being scattered, the scattering no longer occurs to any significant extent. This phenomenon has given rise to the production of transparent ceramic materials.\nLikewise, the scattering of light in optical quality glass fiber is caused by molecular-level irregularities (compositional fluctuations) in the glass structure. Indeed, one emerging school of thought is that a glass is simply the limiting case of a polycrystalline solid. Within this framework, \"domains\" exhibiting various degrees of short-range order become the building-blocks of both metals and alloys, as well as glasses and ceramics. Distributed both between and within these domains are microstructural defects that will provide the most ideal locations for the occurrence of light scattering. This same phenomenon is seen as one of the limiting factors in the transparency of IR missile domes.\nUV-Vis-IR absorption.\nIn addition to light scattering, attenuation or signal loss can also occur due to selective absorption of specific wavelengths, in a manner similar to that responsible for the appearance of color. Primary material considerations include both electrons and molecules as follows:\nThe selective absorption of infrared (IR) light by a particular material occurs because the selected frequency of the light wave matches the frequency (or an integral multiple of the frequency) at which the particles of that material vibrate. Since different atoms and molecules have different natural frequencies of vibration, they will selectively absorb different frequencies (or portions of the spectrum) of infrared (IR) light.\nApplications.\nIn optical fibers, attenuation is the rate at which the signal light decreases in intensity. For this reason, glass fiber (which has a low attenuation) is used for long-distance fiber optic cables; plastic fiber has a higher attenuation and, hence, shorter range. There also exist optical attenuators that decrease the signal in a fiber optic cable intentionally.\nAttenuation of light is also important in physical oceanography. This same effect is an important consideration in weather radar, as raindrops absorb a part of the emitted beam that is more or less significant, depending on the wavelength used.\nDue to the damaging effects of high-energy photons, it is necessary to know how much energy is deposited in tissue during diagnostic treatments involving such radiation. In addition, gamma radiation is used in cancer treatments where it is important to know how much energy will be deposited in healthy and in tumorous tissue.\nIn computer graphics attenuation defines the local or global influence of light sources and force fields.\nRadio.\nAttenuation is an important consideration in the modern world of wireless telecommunications. Attenuation limits the range of radio signals and is affected by the materials a signal must travel through (e.g., air, wood, concrete, rain). See the article on path loss for more information on signal loss in wireless communication.\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "40736", "revid": "6727347", "url": "https://en.wikipedia.org/wiki?curid=40736", "title": "Attenuation constant", "text": ""}
{"id": "40737", "revid": "22041646", "url": "https://en.wikipedia.org/wiki?curid=40737", "title": "Attenuator", "text": "Attenuator could mean:\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "40738", "revid": "24565488", "url": "https://en.wikipedia.org/wiki?curid=40738", "title": "Attribute", "text": "Attribute may refer to:\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "40739", "revid": "8066546", "url": "https://en.wikipedia.org/wiki?curid=40739", "title": "Audible ringing tone", "text": ""}
{"id": "40740", "revid": "276448", "url": "https://en.wikipedia.org/wiki?curid=40740", "title": "Audio response unit", "text": ""}
{"id": "40741", "revid": "34573757", "url": "https://en.wikipedia.org/wiki?curid=40741", "title": "Audit (telecommunication)", "text": "In telecommunications, an audit is one of:\nThe simplest audits consist of comparing current telecommunications billing and usage to the underlying rate structure whether that is dictated by contract, tariff, or price list. Complex audits utilize software applications, direct bargaining with service providers and activity reports that include detail down to an individual employee's usage.\nAuditing methods and consultants.\nIn business, companies with significant telecommunications costs or a telecommunications focus normally either conduct audits internally or hire a consultant. No matter the method, typical audits encompass one or more of the following:\nTelecom audits can encompass every communications service that a business expends its budget on. Audits may focus on mobile phones and devices, Internet service or land line telephony, or they may encompass all three.\nCurrent methods.\nInternal Audit: A business accounting department will generally only conduct a telecom bill review in the months that trigger a red flag due to the sudden spike in the expense of a communications service. This is usually contained to the one telecom service provider who triggered the red flag and the rest of the invoices are left unexamined because they fall within the considered norm of a small monthly cost increase.\nContingency Fee Based Audit: \nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "40742", "revid": "47468357", "url": "https://en.wikipedia.org/wiki?curid=40742", "title": "Audit trail", "text": "Record of activities\nAn audit trail (also called audit log) is a security-relevant chronological record, set of records, and/or destination and source of records that provide documentary evidence of the sequence of activities that have affected at any time a specific operation, procedure, event, or device. Audit records typically result from activities such as financial transactions, scientific research and health care data transactions, or communications by individual people, systems, accounts, or other entities.\nThe process that creates an audit trail is typically required to always run in a privileged mode, so it can access and supervise all actions from all users; a normal user should not be allowed to stop/change it. Furthermore, for the same reason, the trail file or database table with a trail should not be accessible to normal users. Another way of handling this issue is through the use of a role-based security model in the software. The software can operate with the closed-looped controls, or as a 'closed system', as required by many companies when using audit trail functionality.\nIndustry uses.\nIn telecommunications, the term means a record of both completed and attempted accesses and service, or data forming a logical path linking a sequence of events, used to trace the transactions that have affected the contents of a record. \nIn information or communications security, information audit means a chronological record of system activities to enable the reconstruction and examination of the sequence of events and/or changes in an event. Information put away or transmitted in paired structure that might be depended upon in court. An audit trail is a progression of records of computer data about a working framework, an application, or client exercises. Computer frameworks may have a few audit trails each gave to a specific sort of action. Related to proper apparatuses and systems, audit trails can help with distinguishing security infringement, execution issues and application issues. Routine log audits and investigation are valuable for distinguishing security episodes, approach infringement, fake movement, and operational issues soon after they have happened, and for giving information valuable to settling such issues. Audit logs can likewise be valuable for performing forensic investigation, supporting the associations inside examinations, setting up baselines, and distinguishing operational patterns and long run issues.\nIn nursing research, it refers to the act of maintaining a running log or journal of decisions relating to a research project, thus making clear the steps taken and changes made to the original protocol.\nIn accounting, it refers to documentation of detailed transactions supporting summary ledger entries. This documentation may be on paper or on electronic records.\nIn finance, it refers to an order (any firm indication of a willingness to buy or sell a security) tracking system, or consolidated audit trail, with respect to the trading of securities, that would capture order event information for orders in securities from the time of the receipt of an order, and further documenting the life of the order through the process of routing, modification, cancellation, and execution (in whole or in part) of the order.\nIn online proofing, it pertains to the version history of a piece of artwork, design, photograph, video, or web design proof in a project.\nIn clinical research, server based systems such as clinical trial management systems (CTMS) require audit trails. Anything regulatory or QA/QC related also requires audit trails.\nIn pharmaceutical manufacturing, it is a Good Manufacturing Practice regulatory requirement software generate audit trails, but not all software have audit trail functionality built-in. The first 'generic' audit trail generating software came out late 2021 and but later the effective date was moved to 1st April, 2023. The software is called Audit Trail Control, capable of fulfilling regulatory requirements for any software used in pharmaceutical manufacturing.\nIn voting, a voter-verified paper audit trail is a method of providing feedback to voters using a ballotless voting system.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "40743", "revid": "41235142", "url": "https://en.wikipedia.org/wiki?curid=40743", "title": "Aurora (disambiguation)", "text": "An aurora is a natural light display in the sky on Earth seen predominantly in the high latitudes.\nAurora may also refer to:\n&lt;templatestyles src=\"Template:TOC_right/styles.css\" /&gt;\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "40744", "revid": "920919554", "url": "https://en.wikipedia.org/wiki?curid=40744", "title": "Authenticate", "text": ""}
{"id": "40745", "revid": "50498344", "url": "https://en.wikipedia.org/wiki?curid=40745", "title": "Authenticator", "text": "Means to confirm a user's identity\nAn authenticator is a means used to confirm a user's identity, that is, to perform digital authentication. A person authenticates to a computer system or application by demonstrating that he or she has possession and control of an authenticator. In the simplest case, the authenticator is a common password.\nUsing the terminology of the NIST Digital Identity Guidelines, the party to be authenticated is called the \"claimant\" while the party verifying the identity of the claimant is called the \"verifier\". When the claimant successfully demonstrates possession and control of one or more authenticators to the verifier through an established authentication protocol, the verifier is able to infer the claimant's identity.\nClassification.\nAuthenticators may be characterized in terms of secrets, factors, and physical forms.\nAuthenticator secrets.\nEvery authenticator is associated with at least one secret that the claimant uses to demonstrate possession and control of the authenticator. Since an attacker could use this secret to impersonate the user, an authenticator secret must be protected from theft or loss.\nThe type of secret is an important characteristic of the authenticator. There are three basic types of authenticator secret: a memorized secret and two types of cryptographic keys, either a symmetric key or a private key.\nMemorized secret.\nA memorized secret is intended to be memorized by the user. A well-known example of a memorized secret is the common password, also called a passcode, a passphrase, or a personal identification number (PIN).\nAn authenticator secret known to both the claimant and the verifier is called a shared secret. For example, a memorized secret may or may not be shared. A symmetric key is shared by definition. A private key is not shared.\nAn important type of secret that is both memorized and shared is the password. In the special case of a password, the authenticator is the secret.\nCryptographic key.\nA cryptographic authenticator is one that uses a cryptographic key. Depending on the key material, a cryptographic authenticator may use symmetric-key cryptography or public-key cryptography. Both avoid memorized secrets, and in the case of public-key cryptography, there are no shared secrets as well, which is an important distinction.\nExamples of cryptographic authenticators include OATH authenticators and FIDO authenticators. The name OATH is an acronym from the words \"Open AuTHentication\" while FIDO stands for Fast IDentity Online. Both are the results of an industry-wide collaboration to develop an open reference architecture using open standards to promote the adoption of strong authentication. \nBy way of counterexample, a password authenticator is not a cryptographic authenticator. See the #Examples section for details.\nSymmetric key.\nA symmetric key is a shared secret used to perform symmetric-key cryptography. The claimant stores their copy of the shared key in a dedicated hardware-based authenticator or a software-based authenticator implemented on a smartphone. The verifier holds a copy of the symmetric key.\nPublic-private key pair.\nA public-private key pair is used to perform public-key cryptography. The public key is known to (and trusted by) the verifier while the corresponding private key is bound securely to the authenticator. In the case of a dedicated hardware-based authenticator, the private key never leaves the confines of the authenticator.\nAuthenticator factors and forms.\nAn authenticator is something unique or distinctive to a user (\"something that one has\"), is activated by either a PIN (\"something that one knows\"), or is a biometric (\"something that is unique to oneself\"). An authenticator that provides only one of these factors is called a single-factor authenticator whereas a multi-factor authenticator incorporates two or more factors. A multi-factor authenticator is one way to achieve multi-factor authentication. A combination of two or more single-factor authenticators is not a multi-factor authentication, yet may be suitable in certain conditions.\nAuthenticators may take a variety of physical forms (except for a memorized secret, which is intangible). One can, for example, hold an authenticator in one's hand or wear one on the face, wrist, or finger.\nIt is convenient to describe an authenticator in terms of its hardware and software components. An authenticator is hardware-based or software-based depending on whether the secret is stored in hardware or software, respectively.\nAn important type of hardware-based authenticator is called a security key, also called a security token (not to be confused with access tokens, session tokens, or other types of security tokens). A security key stores its secret in hardware, which prevents the secret from being exported. A security key is also resistant to malware since the secret is at no time accessible to software running on the host machine.\nA software-based authenticator (sometimes called a software token) may be implemented on a general-purpose electronic device such as a laptop, a tablet computer, or a smartphone. For example, a software-based authenticator implemented as a mobile app on the claimant's smartphone is a type of phone-based authenticator. To prevent access to the secret, a software-based authenticator may use a processor's trusted execution environment or a Trusted Platform Module (TPM) on the client device.\nA platform authenticator is built into a particular client device platform, that is, it is implemented on device. In contrast, a roaming authenticator is a cross-platform authenticator that is implemented off device. A roaming authenticator connects to a device platform via a transport protocol such as USB.\nExamples.\nThe following sections describe narrow classes of authenticators. For a more comprehensive classification, see the NIST Digital Identity Guidelines.\nSingle-factor authenticators.\nTo use an authenticator, the claimant must explicitly indicate their intent to authenticate. For example, each of the following gestures is sufficient to establish intent:\nThe latter is called a test of user presence (TUP). To activate a single-factor authenticator (\"something that one has\"), the claimant may be required to perform a TUP, which avoids unintended operation of the authenticator.\nA password is a secret that is intended to be memorized by the claimant and shared with the verifier. Password authentication is the process whereby the claimant demonstrates knowledge of the password by transmitting it over the network to the verifier. If the transmitted password agrees with the previously shared secret, user authentication is successful.\nOATH OTP.\nOne-time passwords (OTPs) have been used since the 1980s. In 2004, an Open Authentication Reference Architecture for the secure generation of OTPs was announced at the annual RSA Conference. The Initiative for Open Authentication (OATH) launched a year later. Two IETF standards grew out of this work, the HMAC-based One-time Password (HOTP) algorithm and the Time-based One-time Password (TOTP) algorithm specified by RFC\u00a04226 and RFC\u00a06238, respectively. By OATH OTP, we mean either HOTP or TOTP. OATH certifies conformance with the HOTP and TOTP standards.\nA traditional password (\"something that one knows\") is often combined with a one-time password (\"something that one has\") to provide two-factor authentication. Both the password and the OTP are transmitted over the network to the verifier. If the password agrees with the previously shared secret, and the verifier can confirm the value of the OTP, user authentication is successful.\nOne-time passwords are generated on demand by a dedicated OATH OTP authenticator that encapsulates a secret that was previously shared with the verifier. Using the authenticator, the claimant generates an OTP using a cryptographic method. The verifier also generates an OTP using the same cryptographic method. If the two OTP values match, the verifier can conclude that the claimant possesses the shared secret.\nA well-known example of an OATH authenticator is Google Authenticator, a phone-based authenticator that implements both HOTP and TOTP.\nMobile Push.\nA mobile push authenticator is essentially a native app running on the claimant's mobile phone. The app uses public-key cryptography to respond to push notifications. In other words, a mobile push authenticator is a single-factor cryptographic software authenticator. A mobile push authenticator (\"something that one has\") is usually combined with a password (\"something that one knows\") to provide two-factor authentication. Unlike one-time passwords, mobile push does not require a shared secret beyond the password.\nAfter the claimant authenticates with a password, the verifier makes an out-of-band authentication request to a trusted third party that manages a public-key infrastructure on behalf of the verifier. The trusted third party sends a push notification to the claimant's mobile phone. The claimant demonstrates possession and control of the authenticator by pressing a button in the user interface, after which the authenticator responds with a digitally signed assertion. The trusted third party verifies the signature on the assertion and returns an authentication response to the verifier.\nThe proprietary mobile push authentication protocol runs on an out-of-band secondary channel, which provides flexible deployment options. Since the protocol requires an open network path to the claimant's mobile phone, if no such path is available (due to network issues, e.g.), the authentication process can not proceed.\nFIDO U2F.\nA FIDO Universal 2nd Factor (U2F) authenticator (\"something that one has\") is a single-factor cryptographic authenticator that is intended to be used in conjunction with an ordinary web password. Since the authenticator relies on public-key cryptography, U2F does not require an additional shared secret beyond the password.\nTo access a U2F authenticator, the claimant is required to perform a test of user presence (TUP), which helps prevent unauthorized access to the authenticator's functionality. In practice, a TUP consists of a simple button push.\nA U2F authenticator interoperates with a conforming web user agent that implements the U2F JavaScript API. A U2F authenticator necessarily implements the CTAP1/U2F protocol, one of the two protocols specified in the FIDO Client to Authenticator Protocol.\nUnlike mobile push authentication, the U2F authentication protocol runs entirely on the front channel. Two round trips are required. The first round trip is ordinary password authentication. After the claimant authenticates with a password, the verifier sends a challenge to a conforming browser, which communicates with the U2F authenticator via a custom JavaScript API. After the claimant performs the TUP, the authenticator signs the challenge and returns the signed assertion to the verifier via the browser.\nMulti-factor authenticators.\nTo use a multi-factor authenticator, the claimant performs full user verification. The multi-factor authenticator (\"something that one has\") is activated by a PIN (\"something that one knows\"), or a biometric (\"something that is unique to oneself\"; e.g. fingerprint, face or voice recognition\"), or some other verification technique.\nATM card.\nTo withdraw cash from an automated teller machine (ATM), a bank customer inserts an ATM card into a cash machine and types a Personal Identification Number (PIN). The input PIN is compared to the PIN stored on the card's chip. If the two match, the ATM withdrawal can proceed.\nNote that an ATM withdrawal involves a memorized secret (i.e., a PIN) but the true value of the secret is not known to the ATM in advance. The machine blindly passes the input PIN to the card, which compares the customer's input to the secret PIN stored on the card's chip. If the two match, the card reports success to the ATM and the transaction continues.\nAn ATM card is an example of a multi-factor authenticator. The card itself is \"something that one has\" while the PIN stored on the card's chip is presumably \"something that one knows\". Presenting the card to the ATM and demonstrating knowledge of the PIN is a kind of multi-factor authentication.\nSecure Shell.\nSecure Shell (SSH) is a client-server protocol that uses public-key cryptography to create a secure channel over the network. In contrast to a traditional password, an SSH key is a cryptographic authenticator. The primary authenticator secret is the SSH private key, which is used by the client to digitally sign a message. The corresponding public key is used by the server to verify the message signature, which confirms that the claimant has possession and control of the private key.\nTo avoid theft, the SSH private key (\"something that one has\") may be encrypted using a passphrase (\"something that one knows\"). To initiate a two-factor authentication process, the claimant supplies the passphrase to the client system.\nLike a password, the SSH passphrase is a memorized secret but that is where the similarity ends. Whereas a password is a shared secret that is transmitted over the network, the SSH passphrase is not shared, and moreover, use of the passphrase is strictly confined to the client system. Authentication via SSH is an example of passwordless authentication since it avoids the transmission of a shared secret over the network. In fact, SSH authentication does not require a shared secret at all.\nFIDO2.\nThe FIDO U2F protocol standard became the starting point for the FIDO2 Project, a joint effort between the World Wide Web Consortium (W3C) and the FIDO Alliance. Project deliverables include the W3C Web Authentication (WebAuthn) standard and the FIDO Client to Authenticator Protocol (CTAP). Together WebAuthn and CTAP provide a strong authentication solution for the web.\nA FIDO2 authenticator, also called a WebAuthn authenticator, uses public-key cryptography to interoperate with a WebAuthn client, that is, a conforming web user agent that implements the WebAuthn JavaScript API. The authenticator may be a platform authenticator, a roaming authenticator, or some combination of the two. For example, a FIDO2 authenticator that implements the CTAP2 protocol is a roaming authenticator that communicates with a WebAuthn client via one or more of the following transport options: USB, near-field communication (NFC), or Bluetooth Low Energy (BLE). Concrete examples of FIDO2 platform authenticators include Windows Hello and the Android operating system.\nA FIDO2 authenticator may be used in either single-factor mode or multi-factor mode. In single-factor mode, the authenticator is activated by a simple test of user presence (e.g., a button push). In multi-factor mode, the authenticator (\"something that one has\") is activated by either a PIN (\"something that one knows\") or a biometric (\"something that is unique to oneself\").\nSecurity code.\nFirst and foremost, strong authentication begins with multi-factor authentication. The best thing one can do to protect a personal online account is to enable multi-factor authentication. There are two ways to achieve multi-factor authentication:\nIn practice, a common approach is to combine a password authenticator (\"something that one knows\") with some other authenticator (\"something that one has\") such as a cryptographic authenticator.\nGenerally speaking, a cryptographic authenticator is preferred over an authenticator that does not use cryptographic methods. All else being equal, a cryptographic authenticator that uses public-key cryptography is better than one that uses symmetric-key cryptography since the latter requires shared keys (which may be stolen or misused).\nAgain all else being equal, a hardware-based authenticator is better than a software-based authenticator since the authenticator secret is presumably better protected in hardware. This preference is reflected in the NIST requirements outlined in the next section.\nNIST authenticator assurance levels.\nNIST defines three levels of assurance with respect to authenticators. The highest authenticator assurance level (AAL3) requires multi-factor authentication using either a multi-factor authenticator or an appropriate combination of single-factor authenticators. At AAL3, at least one of the authenticators must be a cryptographic hardware-based authenticator. Given these basic requirements, possible authenticator combinations used at AAL3 include:\nSee the NIST Digital Identity Guidelines for further discussion of authenticator assurance levels.\nRestricted authenticators.\nLike authenticator assurance levels, the notion of a restricted authenticator is a NIST concept. The term refers to an authenticator with a demonstrated inability to resist attacks, which puts the reliability of the authenticator in doubt. Federal agencies mitigate the use a restricted authenticator by offering subscribers an alternative authenticator that is not restricted and by developing a migration plan in the event that a restricted authenticator is prohibited from use at some point in the future.\nCurrently, the use of the public switched telephone network is restricted by NIST. In particular, the out-of-band transmission of one-time passwords (OTPs) via recorded voice messages or SMS messages is restricted. Moreover, if an agency chooses to use voice- or SMS-based OTPs, that agency must verify that the OTP is being transmitted to a phone and not an IP address since Voice over IP (VoIP) accounts are not routinely protected with multi-factor authentication.\nComparison.\nIt is convenient to use passwords as a basis for comparison since it is widely understood how to use a password. On computer systems, passwords have been used since at least the early 1960s. More generally, passwords have been used since ancient times.\nIn 2012, Bonneau et al. evaluated two decades of proposals to replace passwords by systematically comparing web passwords to 35 competing authentication schemes in terms of their usability, deployability, and security. (The cited technical report is an extended version of the peer-reviewed paper by the same name.) They found that most schemes do better than passwords on security while \"every\" scheme does worse than passwords on deployability. In terms of usability, some schemes do better and some schemes do worse than passwords.\nGoogle used the evaluation framework of Bonneau et al. to compare security keys to passwords and one-time passwords. They concluded that security keys are more usable and deployable than one-time passwords, and more secure than both passwords and one-time passwords.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "40746", "revid": "1306352", "url": "https://en.wikipedia.org/wiki?curid=40746", "title": "Automated information system", "text": ""}
{"id": "40747", "revid": "1306352", "url": "https://en.wikipedia.org/wiki?curid=40747", "title": "Automated information systems security", "text": ""}
{"id": "40748", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=40748", "title": "Automatic callback", "text": "Telephone system feature\nIn telecommunications, an automatic callback is a computer telephony calling feature that permits a user, when encountering a busy condition or other condition where the called individual is unavailable, to instruct the system to retain the called number and to establish the call when there is an available line or when the called number is no longer busy. Automatic callback may be implemented in the terminal, in the telephone exchange, or shared between them. Automatic callback is not the same as camp-on.\nUsing callback on popular business telephone systems.\nComdial Digitech, DSU, Impact \nPlace an intercom call and press CAMP. Your phone will disconnect from the attempted call. When the phone you rang is available, your phone will ring with five ring bursts. Press intercom to ring the other phone. To use with calls made in the voice-announce mode, press intercom before the camp button. To cancel, press intercom and dial \"#6\".\nComdial ExecuTech System 2000\nMake an intercom call. At the busy signal, dial \"*6\". Hang up. When the desired extension becomes idle, the calling telephone receives five tone bursts. To answer callback rings, lift the handset. The called telephone will ring. To cancel auto call back before it rings, press \"ITCM\", dial \"#6\" and hang up. \nComdial Digital Impression\nWhen you reach a station that is busy or does not answer, press CAMP. When the phone you wish to reach becomes idle, your phone will ring with five short tones. Press ITCM to cause the other phone to ring. To cancel the callback, press ITCM and dial \"#6\". If the extension you call in voice announce mode is not answered, press ITCM before pressing CAMP.\nDatabase Systems Corp. PACER Phone System\nCustom callback is integrated into the CRM application that signals the phone system to redial a number on a particular date and time. Call is automatically assigned to the original agent or assigned to a hunt group associated with a particular campaign.\nExecutone Encore CX\nPress CALLBK when you hear the busy tone. Answer the callback by lifting the handset or pressing MON. \nInter-Tel Eclipse IDS Integrated Operator Terminal\nPress the Call Back key at the busy signal. Press the RLS key. When your line is free and the extension you called is idle, your extension will ring. When the calls rings back to you, press the RLS key.\nInter-Tel Eclipse2\nAssociate Display and Basic Digital Phone\nPress \"6\" at the busy signal and hang up. Your phone will ring when the extension if available. Press \"6\" again to cancel before you get your callback.\nIsoetec Digital Systems Display/Data Phone\nPress \"Cb.\" Soft key at the busy signal. Replace the handset or press \"HF\". Wait for the double tone. When the extension is no longer busy, it will automatically call you back.\nIsoetec IDS M Series Telephones\nWhen you hear the busy signal, press the \"CALL BACK\" key. Hang up. When you are signaled, lift the handset or press the \"HF\" key. Press the blinking \"CALL BACK\" key. \nPCS Digital Telephone \nPress \"cbck\" at the busy signal. When a station is available, pick up the handset. To cancel, press \"del\".\nVodavi StarPlus Phone System\nPress the pre-programmed CALL BACK button. Hang up. When the busy station becomes available, you will be signaled.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "40749", "revid": "27823944", "url": "https://en.wikipedia.org/wiki?curid=40749", "title": "Automatic call distributor", "text": "Device that directs incoming phone calls\nAn automated call distribution system, commonly known as automatic call distributor or automatic call dispatcher (ACD), is a telephony device that answers and distributes incoming calls to a specific group of terminals or agents within an organization. ACDs direct calls based on parameters that may include the caller's telephone number, the number they dialed, the time of day or a response to an automated voice prompt. Advanced ACD systems may use digital technologies such as computer telephony integration (CTI), computer-supported telecommunications applications (CSTA) or IVR as input to determine the route to a person or voice announcement that will serve the caller. Experts claim that \"the invention of ACD technology made the concept of a call centre possible.\"\nBackground.\nA Private Branch Exchange (PBX) is a telephone exchange device that acts as a switchboard to route phone calls within an organisation. This technology developed into Automated Call Distribution systems using computer technology to automatically connect incoming calls to recipients based on programmable logic.\nAlthough ACDs appeared in the 1950s, one of the first large and separate ACDs was a modified 5XB switch used by the New York Telephone Company in the early 1970s to distribute calls among hundreds of 4-1-1 information operators. Robert Hirvela developed and received a patent for technology that was used to create the Rockwell Galaxy Automatic Call Distributor, which was used by Continental Airlines for more than 20 years. Since then, ACDs have integrated incoming call management and voice messaging software into its capabilities.\nApplication.\nACD systems route incoming calls to people according to defined rules that may include, for example, the time of day, the day of the week, the geographic location of the caller and the availability of people to respond. The rules should aim to route the call to a person qualified to address the caller's needs. Routing can use caller ID, automatic number identification, interactive voice response or dialed number identification services to determine how calls are handled. ACD systems are often found in offices that handle large volumes of incoming phone calls from callers who require assistance at the earliest opportunity, but have no need to talk to a specific person: e.g., customer service representatives or emergency services dispatch centers.\nThere are several contact routing strategies that can be set up within an algorithm based on a company's needs. Skills-based routing is determined by an operator's knowledge to handle a caller's inquiry. Virtual contact centers can also be used to aggregate the skill sets of agents to help multiple vendors, where all real-time and statistical information can be shared amongst the contact center sites. An additional function for these external routing applications is to enable Computer telephony integration (CTI), which improves efficiency for call center agents by matching incoming phone calls with relevant data via screen pop.\nDistribution methods.\nMethods for distributing incoming calls from a queue include\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "40750", "revid": "40107148", "url": "https://en.wikipedia.org/wiki?curid=40750", "title": "Automatic calling", "text": ""}
{"id": "40751", "revid": "16333418", "url": "https://en.wikipedia.org/wiki?curid=40751", "title": "Automatic data processing", "text": "Automatic data processing (ADP) may refer to:\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "40752", "revid": "196471", "url": "https://en.wikipedia.org/wiki?curid=40752", "title": "Automatic data processing equipment", "text": ""}
{"id": "40753", "revid": "63286", "url": "https://en.wikipedia.org/wiki?curid=40753", "title": "Automatic link establishment", "text": "De facto standard in radio communications\nAutomatic Link Establishment, commonly known as ALE, is the worldwide \"de facto\" standard for digitally initiating and sustaining HF radio communications. ALE is a feature in an HF communications radio transceiver system that enables the radio station to make contact, or initiate a circuit, between itself and another HF radio station or network of stations. The purpose is to provide a reliable rapid method of calling and connecting during constantly changing HF ionospheric propagation, reception interference, and shared spectrum use of busy or congested HF channels.\nMechanism.\nA standalone ALE radio combines an HF SSB radio transceiver with an internal microprocessor and MFSK modem. It is programmed with a unique ALE address, similar to a phone number (or on newer generations, a username). When not actively in contact with another station, the HF SSB transceiver constantly scans through a list of HF frequencies called \"channels\", listening for any ALE signals transmitted by other radio stations. It decodes calls and soundings sent by other stations and uses the bit error rate to store a quality score for that frequency and sender-address.\nTo reach a specific station, the caller enters the ALE Address. On many ALE radios this is similar to dialing a phone number. The ALE controller selects the best available idle channel for that destination address. After confirming the channel is indeed idle, it then sends a brief selective calling signal identifying the intended recipient. When the distant scanning station detects ALE activity, it stops scanning and stays on that channel until it can confirm whether or not the call is for it. The two stations' ALE controllers automatically handshake to confirm that a link of sufficient quality has been established, then notify the operators that the link is up. If the callee fails to respond or the handshaking fails, the originating ALE node usually selects another frequency either at random or by making a guess of varying sophistication.\nUpon successful linking, the receiving station generally emits an audible alarm and shows a visual alert to the operator, thus indicating the incoming call. It also indicates the callsign or other identifying information of the linked station, similar to Caller ID. The operator then un-mutes the radio and answers the call then can talk in a regular conversation or negotiates a data link using voice or the ALE built-in short text message format. Alternatively, digital data can be exchanged via a built-in or external modem (such as a STANAG 5066 or MIL-STD-188-110B serial tone modem) depending on needs and availability. The ALE built-in text messaging facility can be used to transfer short text messages as an \"orderwire\" to allow operators to coordinate external equipment such as phone patches or non-embedded digital links, or for short tactical messages. \nCommon applications.\nAn ALE radio system enables connection for voice conversation, alerting, data exchange, texting, instant messaging, email, file transfer, image, geo-position tracking, or telemetry. With a radio operator initiating a call, the process normally takes a few minutes for the ALE to pick an HF frequency that is optimum for both sides of the communication link. It signals the operators audibly and visually on both ends, so they can begin communicating with each other immediately. In this respect, the longstanding need in HF radio for repetitive calling on pre-determined time schedules or tedious monitoring static is eliminated. It is useful as a tool for finding optimum channels to communicate between stations in real-time. In modern HF communications, ALE has largely replaced HF prediction charts, propagation beacons, chirp sounders, propagation prediction software, and traditional radio operator educated guesswork. ALE is most commonly used for hooking up operators for voice contacts on SSB (single-sideband modulation), HF internet connectivity for email, SMS phone texting or text messaging, real-time chat via HF text, Geo Position Reporting, and file transfer. High Frequency Internet Protocol or HFIP may be used with ALE for internet access via HF.\nTechniques.\nThe essence of ALE techniques is the use of automatic channel selection, scanning receivers, selective calling, handshaking, and robust burst modems. An ALE node decodes all received ALE signals heard on the channel(s) it monitors. It uses the fact that all ALE messages use forward error correction (FEC) redundancy. By noting how much error-correction occurred in each received and decoded message, an ALE node can detect the \"quality\" of the path between the sending station and itself. This information is coupled with the ALE address of the sending node and the channel the message was received on, and stored in the node's Link Quality Analysis (LQA) memory. When a call is initiated, the LQA lookup table is searched for matches involving the target ALE address and the best historic channel is used to call the target station. This reduces the likelihood that the call has to be repeated on alternate frequencies. Once the target station has heard the call and responded, a bell or other signalling device will notify both operators that a link has been established. At this point, the operators may coordinate further communication via orderwire text messages, voice, or other means. If further digital communication is desired, it may take place via external data modems or via optional modems built into the ALE terminal.\nThis unusual usage of FEC redundancy is the primary innovation that differentiates ALE from previous selective calling systems which either decoded a call or failed to decode due to noise or interference. A binary outcome of \"Good enough\" or not gave no way of automatically choosing between two channels, both of which are currently good enough for minimum communications. The redundancy-based scoring inherent in ALE thus allows for selecting the \"best\" available channel and (in more advanced ALE nodes) using all decoded traffic over some time window to sort channels into a list of decreasing probability-to-contact, significantly reducing co-channel interference to other users as well as dramatically decreasing the time needed to successfully link with the target node.\nTechniques used in the ALE standard include automatic signaling, automatic station identification (sounding), polling, message store-and-forward, linking protection and anti-spoofing to prevent hostile denial of service by ending the channel scanning process. Optional ALE functions include polling and the exchange of orderwire commands and messages. The orderwire message, known as AMD (Automatic Message Display), is the most commonly used text transfer method of ALE, and the only universal method that all ALE controllers have in common for displaying text. It is common for vendors to offer extensions to AMD for various non-standard features, although dependency on these extensions undermines interoperability. As in all interoperability scenarios, care should be taken to determine if this is acceptable before using such extensions.\nHistory and precedents.\nALE evolved from older HF radio selective calling technology. It combined existing channel-scanning selective calling concepts with microprocessors (enabling FEC decoding and quality scoring decisions), burst transmissions (minimizing co-channel interference), and transponding (allowing unattended operation and incoming-call signalling). Early ALE systems were developed in the late 1970s and early 1980s by several radio manufacturers. The first ALE-family controller units were external rack mounted controllers connected to control military radios, and were rarely interoperable across vendors.\nVarious methods and proprietary digital signaling protocols were used by different manufacturers in first generation ALE, leading to incompatibility. Later, a cooperative effort among manufacturers and the US government resulted in a second generation of ALE that included the features of first generation systems, while improving performance. The second generation 2G ALE system standard in 1986, MIL-STD-188-141A, was adopted in FED-STD-1045 for US federal entities. In the 1980s, military and other entities of the US government began installing early ALE units, using ALE controller products built primarily by US companies. The primary application during the first 10 years of ALE use was government and military radio systems, and the limited customer base combined with the necessity to adhere to MILSPEC standards kept prices extremely high. Over time, demand for ALE capabilities spread and by the late 1990s, most new government HF radios purchased were designed to meet at least the minimum ALE interoperability standard, making them eligible for use with standard ALE node gear. Radios implementing at least minimum ALE node functionality as an option internal to the radio became more common and significantly more affordable. As the standards were adopted by other governments worldwide, more manufacturers produced competitively priced HF radios to meet this demand. The need to interoperate with government organizations prompted many non-government organizations (NGOs) to at least partially adopt ALE standards for communication. As non-military experience spread and prices came down, other civilian entities started using 2G ALE. By the year 2000, there were enough civilian and government organizations worldwide using ALE that it became a de facto HF interoperability standard for situations where a priori channel and address coordination is possible.\nIn the late 1990s, a third generation 3G ALE with significantly improved capability and performance was included in MIL-STD-188-141B, retaining backward compatibility with 2G ALE, and was adopted in NATO STANAG 4538. Civilian and non-government adoption rates are much lower than 2G ALE due to the extreme cost as compared to surplus or entry-level 2G gear as well as the significantly increased system and planning complexity necessary to realize the benefits inherent in the 3G specification. For many militaries, whose needs for maximized intra-organizational capability and capacity always strain existing systems, the additional cost and complexity of 3G are less problematic.\nReliability.\nALE enables rapid unscheduled communication and message passing without requiring complex message centers, multiple radios and antennas, or highly trained operators. With the removal of these potential sources of failure, the tactical communication process becomes much more robust and reliable. The effects extend beyond mere force multiplication of existing communications methods; units such as helicopters, when outfitted with ALE radios, can now reliably communicate in situations where the crew are too busy to operate a traditional non-line of sight radio. This ability to enable tactical communication in conditions where dedicated trained operators and hardware are inappropriate is often considered to be the true improvement offered by ALE.\nALE is a critical path toward increased interoperability between organizations. By enabling a station to participate nearly simultaneously in many different HF networks, ALE allows for convenient cross-organization message passing and monitoring without requiring dedicated separate equipment and operators for each partner organization. This dramatically reduces staffing and equipment considerations, while enabling small mobile or portable stations to participate in multiple networks and subnetworks. The result is increased resilience, decreased fragility, increased ability to communicate information effectively, and the ability to rapidly add to or replace communication points as the situation demands.\nWhen combined with Near Vertical Incidence Skywave (NVIS) techniques and sufficient channels spread across the spectrum, an ALE node can provide greater than 95% success linking on the first call, nearly on par with SATCOM systems. This is significantly more reliable than cellphone infrastructure during disasters or wars yet is mostly immune to such considerations itself.\nStandards and protocols.\nGlobal standards for ALE are based on the original US MIL-STD 188-141A and FED-1045, known as 2nd Generation (2G) ALE. 2G ALE uses non-synchronised scanning of channels, and it takes several seconds to half a minute to repeatedly scan through an entire list of channels looking for calls. Thus it requires sufficient duration of transmission time for calls to connect or link with another station that is unsynchronised with its calling signal. The vast majority of ALE systems in use in the world at the present time are 2G ALE.\n2G technical characteristics.\nThe more common 2G ALE signal waveform is designed to be compatible with standard 3\u00a0kHz SSB narrowband voice channel transceivers. The modulation method is 8ary Frequency Shift Keying or 8FSK, also sometimes called Multi Frequency Shift Keying MFSK, with eight orthogonal tones between 750 and 2500\u00a0Hz. Each tone is 8 ms long, resulting in a transmitted over-the-air symbol rate of 125 baud or 125 symbols per second, with a raw data rate of 375 bits per second. The ALE data is formatted in 24-bit frames, which consist of a 3-bit preamble followed by three ASCII characters, each seven bits long. The received signal is usually decoded using digital signal processing techniques that are capable of recovering the 8FSK signal at a negative decibel signal-to-noise ratio (i.e., the signal may be recovered even when it is below the noise level). The over-the-air layers of the protocol involve the use of forward error correction, redundancy, and handshaking transponding similar to those used in ARQ techniques.\n3G technical characteristics.\nNewer standards of ALE, called 3rd Generation or 3G ALE, use accurate time synchronization (via a defined time-synch protocol as well as the option of GPS-locked clocks) to achieve faster and more dependable linking. Through synchronization, the calling time to achieve a link may be reduced to less than 10 seconds. The 3G ALE modem signal also provides better robustness and can work in channel conditions that are less favorable than 2G ALE. Dwell groups, limited callsigns, and shorter burst transmissions enable more rapid intervals of scanning. All stations in the same group scan and receive each channel at precisely the same time window. Although 3G ALE is more reliable and has significantly enhanced channel-time efficiency, the existence of a large installed base of 2G ALE radio systems and the wide availability of moderately priced (often military surplus) equipment, has made 2G the baseline standard for global interoperability.\nBasis for HF interoperability communications.\nInteroperability is a critical issue for the disparate entities which use radiocommunications to fulfill the needs of organizations. Largely due to the ubiquity of 2G ALE, it became the primary method for providing interoperability on HF between governmental and non-governmental disaster relief and emergency communications entities, and amateur radio volunteers. With digital techniques increasingly employed in communications equipment, a universal digital calling standard was needed, and ALE filled the gap. Nearly every major HF radio manufacturer in the world builds ALE radios to the 2G standard to meet the high demand that new installations of HF radio systems conform to this standard protocol. Disparate entities that historically used incompatible radio methods were then able to call and converse with each other using the common 2G ALE platform. Some manufacturers and organizations have used the AMD feature of ALE to expand the performance and connectivity. In some cases, this has been successful, and in other cases, the use of proprietary preamble or embedded commands has led to interoperability problems.\nTactical communication and resource management.\nALE serves as a convenient method of beyond line of sight communication. Originally developed to support military requirements, ALE is useful to many organizations who find themselves managing widely located units. United States Immigration and Customs Enforcement and United States Coast Guard are two members of the Customs Over the Horizon Enforcement Network (COTHEN), a MIL-STD 188-141A ALE network. All U.S. armed forces operate multiple similar networks. Similarly, shortwave utility listeners have documented frequency and callsign lists for many nations' military and guard units, as well as networks operated by oil exploration and production companies and public utilities in many countries.\nEmergency / disaster relief or extraordinary situation response communications.\nALE radio communication systems for both HF regional area networks and HF interoperability communications are in service among emergency and disaster relief agencies as well as military and guard forces. Extraordinary response agencies and organizations use ALE to respond to situations in the world where conventional communications may have been temporarily overloaded or damaged. In many cases, it is in place as alternative back-channel for organizations that may have to respond to situations or scenarios involving the loss of conventional communications. Earthquakes, storms, volcanic eruptions, and power or communication infrastructure failures are typical situations in which organizations may deem ALE necessary to operations. ALE networks are common among organizations engaged in extraordinary situation response such as: natural and man-made disasters, transportation, power, or telecommunication network failures, war, peacekeeping, or stability operations. Organizations known to use ALE for Emergency management, disaster relief, ordinary communication or extraordinary situation response include: Red Cross, FEMA, Disaster Medical Assistance Teams, NATO, Federal Bureau of Investigation, United Nations, AT&amp;T, Civil Air Patrol, SHARES, State of California Emergency Management Agency (CalEMA), other US States' Offices of Emergency Services or Emergency Management Agencies, and Amateur Radio Emergency Service (ARES).\nInternational HF telecommunications for disaster relief.\nThe International Telecommunication Union (ITU), in response to the need for interoperation in international disaster response spurred largely by humanitarian relief, included ALE in its Telecommunications for Disaster Relief recommendations. The increasing need for instant connectivity for logistical and tactical disaster relief response communications, such as the 2004 Indian Ocean earthquake tsunami led to ITU actions of encouragement to countries around the world toward loosening restrictions on such communications and equipment border transit during catastrophic disasters. The IARU Global Amateur Radio Emergency Communications Conferences (GAREC) and IARU Global Simulated Emergency Tests have included ALE.\nUse in amateur radio.\nAmateur radio operators began sporadic ALE operation on a limited basis in the early to mid-1990s, with commercial ALE radios and ALE controllers. In 2000, the first widely available software ALE controller for the Personal Computer, \"PCALE\", became available, and hams started to set up stations based on it. In 2001, the first organized and coordinated global ALE nets for International Amateur Radio began. In August 2005, ham radio operators supporting communications for emergency Red Cross shelters used ALE for Disaster Relief operations during the Hurricane Katrina disaster. After the event, hams developed more permanent ALE emergency/disaster relief networks, including internet connectivity, with a focus on interoperation between organizations. The amateur radio HFLink Automatic Link Establishment system uses an open net protocol to enable all amateur radio operators and amateur radio nets worldwide to participate in ALE and share the same ALE channels legally and interoperably. Amateur radio operators may use it to call each other for voice or data communications.\nAmateur radio interoperability adaptations.\nAmateur radio operators commonly provide local, regional, national, and international emergency / disaster relief communications. The need for interoperability on HF led to the adoption of ALE open networks by hams. Amateur radio adapted 2G ALE techniques, by using the common denominators of the 2G ALE protocol, with a limited subset of features found in the majority of all ALE radios and controllers. Each amateur radio ALE station uses the operator's call sign as the address, also known as the ALE Address, in the ALE radio controller. The lowest common denominator technique enables any manufacturer's ALE radios or software to be used for HF interoperability communications and networking. Known as Ham-Friendly ALE, the amateur radio ALE standard is used to establish radio communications, through a combination of active ALE on internationally recognized automatic data frequencies, and passive ALE scanning on voice channels. In this technique, active ALE frequencies include pseudorandom periodic polite station identification, while passive ALE frequencies are silently scanned for selective calling. ALE systems include Listen Before Transmit as a standard function, and in most cases this feature provides better busy channel detection of voice and data signals than the human ear. Ham-Friendly ALE technique is also known as 2.5G ALE, because it maintains 2G ALE compatibility while employing some of the adaptive channel management features of 3G ALE, but without the accurate GPS time synchronization of 3G ALE.\nDisaster relief HF network.\nHot standby ALE nets are in constant operation 24/7/365 for International Emergency and Disaster Relief communications. The Ham Radio Global ALE High Frequency Network, which began service in June 2007, is the world's largest intentionally open ALE network. It is a free open network staffed by volunteers, and used by amateur radio operators supporting disaster relief organizations.\nInternational coordination.\nInternational amateur radio ALE High Frequency channels are frequency coordinated with all Regions of the International Amateur Radio Union (IARU entity of ITU), for international, regional, national, and local use in the Amateur Radio Service. All Amateur Radio ALE channels use \"USB\" Upper Sideband standard. Different rules, regulations, and bandplans of the region and local country of operation apply to use of various channels. Some channels may not be available in every country. Primary or global channels are in common with most countries and regions.\nInternational channels.\n\"This listing is current as of February 2020.\"\nFrequency table notes: \nAutomatic Link Establishment ALE channel frequencies in the Amateur Radio Service are internationally coordinated with selective calling Selcall channels for interoperability purposes. \nNet is the ALE net address or Selcall net name.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "40754", "revid": "97297", "url": "https://en.wikipedia.org/wiki?curid=40754", "title": "Automatic message exchange", "text": "Automatic message exchange (AME): In an adaptive high-frequency (HF) radio network, an automated process allowing the transfer of a message from message injection to addressee reception, without human intervention. Through the use of machine-addressable transport guidance information, \"i.e.,\" the message header, the message is automatically routed through an on-line direct connection through single or multiple transmission media."}
{"id": "40755", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=40755", "title": "Automatic redial", "text": "Telephone service feature\nIn telecommunications, an automatic redial is a service feature that allows the user to dial, by depressing a single key or a few keys, the most recent telephone number dialed at that instrument.\n\"Note:\" Automatic redial is often associated with the telephone instrument, but may be provided by a PBX, or by the central office. \"Synonym\" last number redial. \"Contrast with\" automatic calling unit.\nOften one must subscribe to a caller ID for use of this function on a landline.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "40756", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=40756", "title": "Automatic sounding", "text": "In telecommunications, automatic sounding is the testing of selected channels for quality by providing a very brief identifying transmission that may be used by other stations to evaluate connectivity, and availability, and to identify known working channels for immediate or later use for communications or calling. They are often used to maintain connectivity in digital communications high frequency radio networks.\nAutomatic soundings are primarily intended to increase the efficiency of the automatic link establishment (ALE) function, thereby increasing system throughput. \nIn ALE, the sounding information consists of a heavily error-corrected short message identifying the sender. Recipients decode it and use the bit error rate to calculate and store a (channel, node, quality) tuple. As ionospheric conditions and mobile-node locations change, these quality tuples will shift. The stored data can be used to maximize the chance that the best channel to link with a given partner will be chosen first.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "40757", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=40757", "title": "Automatic switching system", "text": "Telephone exchange equipment\nIn data communications, an automatic switching system is a switching system in which all the operations required to execute the three phases of information-transfer transactions are automatically executed in response to signals from a user end-instrument. \nIn an automatic switching system, the information-transfer transaction is performed without human intervention, except for initiation of the access phase and the disengagement phase by a user. \nIn telephony, it refers to a telephone exchange in which all the operations required to set up, supervise, and release connections required for telephone calls are automatically performed in response to signals from a calling device. This distinction lost importance as manual switching declined during the 20th century.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "40758", "revid": "1274242953", "url": "https://en.wikipedia.org/wiki?curid=40758", "title": "Auxiliary power", "text": "Auxiliary power is electric power that is provided by an alternate source and that serves as backup for the primary power source at the station main bus or prescribed sub-bus.\nAn offline unit provides electrical isolation between the primary power source and the critical technical load whereas an online unit does not.\nA Class A power source is a primary power source, i.e., a source that assures an essentially continuous supply of power.\nTypes of auxiliary power services include Class B, a standby power plant to cover extended outages of the order of days; Class C, a 10-to-60-second quick-start unit to cover short-term outages of the order of hours; and Class D, an uninterruptible non-break unit using stored energy to provide continuous power within specified voltage and frequency tolerances.\nUses/Implementations.\nMany uses and implementations of auxiliary power are experimented with to increase its efficiency. One such experimentation was to find a better way to operate a diesel engine with fuel cell based auxiliary power units. \u00a0The method of which is to separate hydrogen-rich gas from the diesel fuel to generate electricity separately in an auxiliary power unit. \u00a0With this process, an effective reduction in emissions can be achieved by lowering the consumed volume of gas per hour. However, upon \u00a0power demands reaching 60% a sharp decrease in performance occurs, which can be solved by using a diesel or kerosine fuel with a maximum CO concentration of 1.5%.\nThere are a variety of other implementations of auxiliary power units in energy systems. This explains for how a significant portion of emissions come from commercial vehicles. \u00a0Diesel engines operating within densely populated areas, running within an ineffective range in order to power their auxiliary systems, such as refrigeration, are contributors to a large portion of the emissions from automobiles. \u00a0Using a model with a diesel powered four-stroke engine on a truck with 100% load capacity driving a combination of typical urban and city road cycles, emissions and auxiliary power demand were recorded. Then by using the calculated auxiliary power demand, a source was developed to support the demand for the auxiliary systems in the form of a PEM fuel cell. \u00a0The end product of the PEM fuel cell was able to support the auxiliary systems of the truck using a maximum of 5\u00a0kW of power. This input was able to sustain the cooling chamber, cabin air-conditioning, radio unit, etc. The introduction of this fuel cell also contributed to a 9% reduction in diesel fuel consumption and 9.6% reduction in CO2 emissions.\nLegal Requirements for Industries.\nThe United States Environmental Protection Agency has set out rules and guidelines for how auxiliary and supplemental power sources (ASPS) that provide secondary power to wastewater treatment plants in case of a blackout. \u00a0ASPS should be able to supply enough power to run the plant effectively, and be available for start-up in a short period of time in case of emergency. Types of ASPS necessary for adequate power generation include: internal combustion engines, microturbines, solar cells, fuel cells, and wind turbines. \u00a0ASPS technology is required to be reliable enough to start up quickly, and run for extended periods of time, (i.e. 48 hours or more) with sufficient fuel.\nEfficiency.\nAs previously affirmed, auxiliary power units are commonly used to improve the efficiency of electrical system. The use of auxiliary power units for range extended electric automobiles has been shown to improve the control of energy flow and distribution throughout the system, improving its overall efficiency.\nFor closed systems with extreme power consumption such as tankers and other vessels at sea, the use and quality of auxiliary power systems have a great impact on the efficiency of the overall system. \u00a0The different uses of auxiliary power for an array of ships and ship activities and how these different power schemes change the overall efficiency and/emissions of the ship's system. Studies have indicated that while ships travel between ports within the same bay, total ship exhaust emissions are due to primarily their auxiliary boiler and auxiliary engine power systems, due to the time and speed required to transit the port waters with the large berth of the vessel. \u00a0Findings also lead to the conclusion that the power output capabilities of auxiliary engines at a certain point do not increase with the size of the vessel, or the vessel's installed main engine power. There are a great many factors such as machinery variables, power schemes, and size and power of vessels, that there are too many factors to take into account in order to portray an accurate representation of the ratio between main power and auxiliary power output. More surveys and studies should be done in order to achieve this more accurate result.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "40759", "revid": "11292982", "url": "https://en.wikipedia.org/wiki?curid=40759", "title": "Auxiliary storage", "text": ""}
{"id": "40760", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=40760", "title": "Availability", "text": "Term in reliability engineering\nIn reliability engineering, the term availability has the following meanings:\nNormally high availability systems might be specified as 99.98%, 99.999% or 99.9996%. The converse, unavailability, is 1 minus the availability.\nRepresentation.\nThe simplest representation of availability (\"A\") is a ratio of the expected value of the uptime of a system to the aggregate of the expected values of up and down time (that results in the \"total amount of time\" \"C\" of the observation window)\n formula_1\nAnother equation for availability (\"A\") is a ratio of the Mean Time To Failure (MTTF) and Mean Time Between Failure (MTBF), or\n formula_2\nIf we define the status function formula_3 as\n formula_4\ntherefore, the availability \"A\"(\"t\") at time \"t\"\u00a0&gt;\u00a00 is represented by\n formula_5\nAverage availability must be defined on an interval of the real line. If we consider an arbitrary constant formula_6, then average availability is represented as\n formula_7\nLimiting (or steady-state) availability is represented by\n formula_8\nLimiting average availability is also defined on an interval formula_9 as,\n formula_10\nAvailability is the probability that an item will be in an operable and committable state at the start of a mission when the mission is called for at a random time, and is generally defined as uptime divided by total time (uptime plus downtime).\nSeries vs Parallel components.\nLet's say a series component is composed of components A, B and C. Then following formula applies: \nAvailability of series component = (availability of component A) x (availability of component B) x (availability of component C) \nTherefore, combined availability of multiple components in a series is always lower than the availability of individual components. \nOn the other hand, following formula applies to parallel components:\nAvailability of parallel components = 1 - (1 - availability of component A) X (1 - availability of component B) X (1 - availability of component C) \nIn corollary, if you have N parallel components each having X availability, then:\nAvailability of parallel components = 1 - (1 - X)^ N \nUsing parallel components can exponentially increase the availability of overall system. For example if each of your hosts has only 50% availability, by using 10 of hosts in parallel, you can achieve 99.9023% availability. \nNote that redundancy doesn\u2019t always lead to higher availability. In fact, redundancy increases complexity which in turn reduces availability. According to Marc Brooker, to take advantage of redundancy, ensure that:\nMethods and techniques to model availability.\nReliability Block Diagrams or Fault Tree Analysis are developed to calculate availability of a system or a functional failure condition within a system including many factors like:\nFurthermore, these methods are capable to identify the most critical items and failure modes or events that impact availability.\nDefinitions within systems engineering.\nAvailability, inherent (Ai) \nThe probability that an item will operate satisfactorily at a given point in time when used under stated conditions in an ideal support environment. It excludes logistics time, waiting or administrative downtime, and preventive maintenance downtime. It includes corrective maintenance downtime. \nInherent availability is generally derived from analysis of an engineering design:\nIt is based on quantities under control of the designer.\nAvailability, achieved (Aa) \nThe probability that an item will operate satisfactorily at a given \npoint in time when used under stated conditions in an ideal support environment (i.e., that personnel, tools, spares, etc. are instantaneously available). It excludes logistics time and waiting or administrative downtime. \nIt includes active preventive and corrective maintenance downtime.\nAvailability, operational (Ao) \nThe probability that an item will operate satisfactorily at a given point in time when used in an actual or realistic operating and support environment. It includes logistics time, ready time, and waiting or administrative downtime, and both preventive and corrective maintenance downtime. This value is equal to the mean time between failure (MTBF) divided by the mean time between failure plus the mean downtime (MDT). This measure extends the definition of availability to elements controlled by the logisticians and mission planners such as quantity and proximity of spares, tools and manpower to the hardware item.\nRefer to Systems engineering for more details\nBasic example.\nIf we are using equipment which has a mean time to failure (MTTF) of 81.5 years and mean time to repair (MTTR) of 1 hour:\n MTTF in hours = 81.5\u00a0\u00d7\u00a0365\u00a0\u00d7\u00a024\u00a0=\u00a0713940 (This is a reliability parameter and often has a high level of uncertainty!)\n Inherent availability (Ai) = 713940 / (713940+1) = 713940 / 713941 = 99.999860%\n Inherent unavailability = 1 / 713940 = 0.000140%\nOutage due to equipment in hours per year = 1/rate = 1/MTTF = 0.01235 hours per year.\nLiterature.\nAvailability is well established in the literature of stochastic modeling and optimal maintenance. Barlow and Proschan [1975] define availability of a repairable system as \"the probability that the system is operating at a specified time t.\" Blanchard [1998] gives a qualitative definition of availability as \"a measure of the degree of a system which is in the operable and committable state at the start of mission when the mission is called for at an unknown random point in time.\" This definition comes from the MIL-STD-721. Lie, Hwang, and Tillman [1977] developed a complete survey along with a systematic classification of availability.\nAvailability measures are classified by either the time interval of interest or the mechanisms for the system downtime. If the time interval of interest is the primary concern, we consider instantaneous, limiting, average, and limiting average availability. The aforementioned definitions are developed in Barlow and Proschan [1975], Lie, Hwang, and Tillman [1977], and Nachlas [1998]. The second primary classification for availability is contingent on the various mechanisms for downtime such as the inherent availability, achieved availability, and operational availability. (Blanchard [1998], Lie, Hwang, and Tillman [1977]). Mi [1998] gives some comparison results of availability considering inherent availability.\nAvailability considered in maintenance modeling can be found in Barlow and Proschan [1975] for replacement models, Fawzi and Hawkes [1991] for an R-out-of-N system with spares and repairs, Fawzi and Hawkes [1990] for a series system with replacement and repair, Iyer [1992] for imperfect repair models, Murdock [1995] for age replacement preventive maintenance models, Nachlas [1998, 1989] for preventive maintenance models, and Wang and Pham [1996] for imperfect maintenance models. A very comprehensive recent book is by Trivedi and Bobbio [2017].\nApplications.\nAvailability factor is used extensively in power plant engineering. For example, the North American Electric Reliability Corporation implemented the Generating Availability Data System in 1982.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "40762", "revid": "30292728", "url": "https://en.wikipedia.org/wiki?curid=40762", "title": "Backbone (disambiguation)", "text": "Backbone may refer to:\n&lt;templatestyles src=\"Template:TOC_right/styles.css\" /&gt;\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists anatomy associated with the title ."}
{"id": "40763", "revid": "45789152", "url": "https://en.wikipedia.org/wiki?curid=40763", "title": "Backscattering", "text": ""}
{"id": "40764", "revid": "4052843", "url": "https://en.wikipedia.org/wiki?curid=40764", "title": "Taxation", "text": ""}
{"id": "40765", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=40765", "title": "Back-to-back connection", "text": "A back-to-back connection is the direct connection of the output of one device to the input of a similar or related device.\nTelecommunications.\nIn telecommunications, a back-to-back connection can be formed by connecting a transmitter directly to a receiver without a transmission line in between. This is used for equipment measurements and testing purposes. The back-to-back connection eliminates the effects of the transmission medium.\nIn some cases, the output of a receiving device is instead connected to the input of a transmitting device.\nPower transmission.\nA back-to-back connection for electric power transmission is a high-voltage direct-current (HVDC) system with both ends in the same switchyard. This is used to couple asynchronously operated power grids or for connecting power grids of different frequencies where no DC transmission line is necessary.\nElectronics.\nIn electronics, a back-to-back connection is the connection of two identical or similar components in series with the opposite polarity. This is used to convert polarised components to non-polar use. Common examples include:\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "40766", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=40766", "title": "Backward channel", "text": "In a data transmission circuit a backward channel is the channel that passes data in a direction opposite to that of its associated forward channel. The backward channel is usually used for transmission of request, supervisory, acknowledgement, or error-control signals. The direction of flow of these signals is opposite to that in which user information is being transferred. The backward-channel bandwidth is usually less than that of the primary channel, that is, the forward (user information) channel. For example, ADSL's upstream channel, considered a backward channel for some types of analysis, typically has a bandwidth less than one-fourth of the downstream channel.\nIn data transmission, it is a secondary channel in which the direction of transmission is constrained to be opposite to that of the primary, \"i.e.\", the forward (user-information) channel. The direction of transmission in the backward channel is restricted by the control interchange circuit that controls the direction of transmission in the primary channel.\nReferences.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "40767", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=40767", "title": "Balanced line", "text": "Electrical circuit with two conductors of equal impedance\nIn telecommunications and professional audio, a balanced line or balanced signal pair is an electrical circuit consisting of two conductors of the same type, both of which have equal impedances along their lengths, to ground, and to other circuits. The primary advantage of the balanced line format is good rejection of common-mode noise and interference when fed to a differential device such as a transformer or differential amplifier.\nAs prevalent in sound recording and reproduction, balanced lines are referred to as balanced audio.\nA common form of balanced line is twin-lead, used for radio frequency communications. Also common is twisted pair, used for traditional telephone, professional audio, or for data communications. They are to be contrasted to unbalanced lines, such as coaxial cable, which is designed to have its return conductor connected to ground, or circuits whose return conductor actually is ground (see earth-return telegraph). Balanced and unbalanced circuits can be interfaced using a device called a balun.\nCircuits driving balanced lines must themselves be balanced to maintain the benefits of balance. This may be achieved by transformer coupling (repeating coils) or by merely balancing the impedance in each conductor.\nLines carrying symmetric signals (those with equal amplitudes but opposite polarities on each leg) are often incorrectly referred to as \"balanced\", but this is actually differential signalling. Balanced lines and differential signalling are often used together, but they are not the same thing. Differential signalling does not make a line balanced, nor does noise rejection in balanced cables require differential signalling.\nExplanation.\nTransmission of a signal over a balanced line reduces the influence of noise or interference due to external stray electric fields. Any external signal sources tend to induce only a common-mode signal on the line, and the balanced impedances to ground minimizes differential pickup due to stray electric fields. The conductors are sometimes twisted together to ensure that each conductor is equally exposed to any external magnetic fields that could induce unwanted noise.\nSome balanced lines also have electrostatic shielding to reduce the amount of noise introduced. The cable is often wrapped in foil, copper wire, or a copper braid. This shield provides immunity to RF interference but does not provide immunity to magnetic fields.\nSome balanced lines use 4-conductor star quad cable to provide immunity to magnetic fields. The geometry of the cable ensures that magnetic fields will cause equal interference of both legs of the balanced circuit. This balanced interference is a common-mode signal that can easily be removed by a transformer or balanced differential receiver.\nA balanced line allows a differential receiver to reduce the noise on a connection by rejecting common-mode interference. The lines have the same impedance to ground, so the interfering fields or currents induce the same voltage in both wires. Since the receiver responds only to the difference between the wires, it is not influenced by the induced noise voltage. If a balanced line is used in an unbalanced circuit, with different impedances from each conductor to ground, currents induced in the separate conductors will cause different voltage drops to ground, thus creating a voltage differential, making the line more susceptible to noise. Examples of twisted pairs include category 5 cable.\nCompared to unbalanced lines, balanced lines reduce the amount of noise per distance, allowing a longer cable run to be practical. This is because electromagnetic interference will affect both signals the same way. Similarities between the two signals are automatically removed at the end of the transmission path when one signal is subtracted from the other.\nTelephone systems.\nThe first application for balanced lines was for telephone lines. Interference that was of little consequence on a telegraph system (which is in essence digital) could be very disturbing for a telephone user. The initial format was to take two single-wire unbalanced telegraph lines and use them as a pair. This proved insufficient, however, with the growth of electric power transmission which tended to use the same routes. A telephone line running alongside a power line for many miles will inevitably have more interference induced in one leg than the other since one of them will be nearer to the power line. This issue was addressed by swapping the positions of the two legs every few hundred yards with a cross-over, thus ensuring that both legs had equal interference induced and allowing common-mode rejection to do its work. As the telephone system grew, it became preferable to use cable rather than open wires to save space, and also to avoid poor performance during bad weather. The cable construction used for balanced telephone cables was twisted pair; however, this did not become widespread until repeater amplifiers became available. For an unamplified telephone line, a twisted pair cable could only manage a maximum distance of 30\u00a0km. Open wires, on the other hand, with their lower capacitance, had been used for enormous distances\u2014the longest was the 1500\u00a0km from New York to Chicago built in 1893. Loading coils were used to improve the distance achievable with cable but the problem was not finally overcome until amplifiers started to be installed in 1912. Twisted pair balanced lines are still widely used for local loops, the lines that connect each subscriber's premises to their respective exchange.\nTelephone trunk lines, and especially frequency division multiplexing carrier systems, are usually 4-wire circuits rather than 2-wire circuits (or at least they were before fibre-optic became widespread) and require a different kind of cable. This format requires the conductors to be arranged in two pairs, one pair for the sending (go) signal and the other for the return signal. The greatest source of interference on this kind of transmission is usually the crosstalk between the go and return circuits themselves. The most common cable format is star quad, where the diagonally opposite conductors form the pairs. This geometry gives maximum common-mode rejection between the two pairs. An alternative format is DM (Dieselhorst-Martin) quad which consists of two twisted pairs with the twisting at different pitches.\nAudio systems.\nAn example of balanced lines is the connection of microphones to a mixer in professional systems. Classically, both dynamic and condenser microphones used transformers to provide a differential-mode signal. While transformers are still used in the large majority of modern dynamic microphones, more recent condenser microphones are more likely to use electronic drive circuitry. Each leg, irrespective of any signal, should have an identical impedance to ground. Pair cable (or a pair-derivative such as star quad) is used to maintain the balanced impedances and close twisting of the cores ensures that any interference is common to both conductors. Providing that the receiving end (usually a mixing console) does not disturb the line balance, and is able to ignore common-mode (noise) signals, and can extract differential ones, then the system will have excellent immunity to induced interference.\nTypical professional audio sources, such as microphones, have three-pin XLR connectors. One connects to the shield or chassis ground, while the other two are for the signal conductors. The signal wires can carry two copies of the same signal with opposite polarity (differential signalling) but need not do so. They are often termed \"hot\" and \"cold,\" and the AES14-1992(r2004) Standard [and EIA Standard RS-297-A] suggest that the pin that carries the positive signal that results from a positive air pressure on a transducer will be deemed 'hot'. Pin 2 has been designated as the 'hot' pin, and that designation serves useful for keeping a consistent polarity in the rest of the system. Since these conductors travel the same path from source to destination, the assumption is that any interference is induced upon both conductors equally. The appliance receiving the signals compares the difference between the two signals (often with disregard to electrical ground) allowing the appliance to ignore any induced electrical noise. Any induced noise would be present in equal amounts and in identical polarity on each of the balanced signal conductors, so the two signals\u2019 difference from each other would be unchanged. The successful rejection of induced noise from the desired signal depends in part on the balanced signal conductors receiving the same amount and type of interference. This typically leads to twisted, braided, or co-jacketed cables for use in balanced signal transmission.\nBalanced and differential.\nMany explanations of balanced lines assume symmetric signals (i.e. signals equal in magnitude but of opposite polarity) but this can lead to confusion of the two concepts\u2014signal symmetry and balanced lines are quite independent of each other. Essential in a balanced line is identical impedances in the two conductors in the driver, line and receiver (impedance balancing). These conditions ensure that external noise affects each leg of the line equally and thus appears as a common-mode signal that is rejected by the receiver. There are balanced drive circuits that have excellent common-mode impedance balancing between the legs but do \"not\" provide symmetric signals. Symmetric differential signals concern headroom and are not necessary for interference rejection.\nBaluns.\nInterfacing balanced and unbalanced lines requires a balun. For example, baluns can be used to send line level audio or E-carrier level 1 signals over coaxial cable (which is unbalanced) through of balanced category 5 cable by using a pair of baluns at each end of the CAT5 run. As the signal travels through the balanced line, noise is induced and added to the signal. As the CAT5 line is carefully impedance balanced, the noise induces equal (common-mode) voltages in both conductors. At the receiving end, the balun responds only to the difference in voltage between the two conductors, thus rejecting the noise picked up along the way and leaving the original signal intact.\nA once common application of a radio frequency balun was found at the antenna terminals of a television receiver. Typically a 300-ohm balanced twin lead antenna input could only be connected to a coaxial cable from a cable TV system through a balun.\nCharacteristic impedance.\nThe characteristic impedance formula_1 of a transmission line is an important parameter at higher frequencies of operation. For a parallel 2-wire transmission line,\nformula_2\nwhere formula_3 is half the distance between the wire centres, formula_4 is the wire radius and formula_5, formula_6 are respectively the permeability and permittivity of the surrounding medium. A commonly used approximation that is valid when the wire separation is much larger than the wire radius and in the absence of magnetic materials is\nformula_7\nwhere formula_8 is the relative permittivity of the surrounding medium.\nElectric power lines.\nIn electric power transmission, the three conductors used for three-phase power transmission are referred to as a balanced line since the instantaneous sum of the three line voltages is nominally zero. However, \"balance\" in this field is referring to the symmetry of the source and load: it has nothing to do with the impedance balance of the line itself, the sense of the meaning in telecommunications.\nFor the transmission of single-phase electric power as used for railway electrification, two conductors are used to carry in-phase and out-of-phase voltages such that the line is balanced.\nBipolar HVDC lines at which each pole is operated with the same voltage toward ground are also balanced lines.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "40768", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=40768", "title": "Balance return loss", "text": "Telecommunication theory\nIn telecommunications, balance return loss is one of two things:\nReferences.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "40769", "revid": "7903804", "url": "https://en.wikipedia.org/wiki?curid=40769", "title": "Balancing network", "text": "In a hybrid set, hybrid coil, or resistance hybrid, balancing network is a circuit used to match, \"i.e.\", to balance, the impedance of a uniform transmission line, (e.g., a twisted metallic pair, coaxial cable, etc.) over a selected range of frequencies. A balancing network is required to ensure isolation between the two ports of the four-wire side of the hybrid.\nA balancing network can also be a device used between a balanced device or line and an unbalanced device or line for the purpose of transforming from balanced to unbalanced or from unbalanced to balanced.\nSource: from Federal Standard 1037C and from MIL-STD-188\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "40771", "revid": "23919242", "url": "https://en.wikipedia.org/wiki?curid=40771", "title": "Bandwidth compression", "text": "In telecommunications, the term bandwidth compression has the following meanings: \nBandwidth compression implies a reduction in normal bandwidth of an information-carrying signal without reducing the information content of the signal. This can be accomplished with lossless data compression techniques. For more information read the Increasing speeds section in the Modem article. Bandwidth Compression is a core feature of WAN Optimization appliances to improve bandwidth efficiency.\nDefinition and types.\nThe concept encompasses a wide range of engineering methods and algorithms that aim to minimize the volume of data transmitted or stored, either by eliminating redundancies or by reducing the precision of information where acceptable. These techniques are categorized broadly into lossless and lossy methods, depending on whether the original data can be perfectly reconstructed. While lossless methods are essential in contexts that require full data fidelity, such as financial records or command-and-control systems, lossy approaches are more suitable for applications like video streaming or voice communication, where perceptual quality can be maintained despite some data loss.\nImportance and development.\nMoreover, with the proliferation of wireless sensor networks (WSNs) and the Internet of things (IoT), bandwidth compression has become vital for maintaining low-power operation and scalable network deployment. In such systems, transmitting raw data is often infeasible due to energy and bandwidth limitations. Therefore, advanced compression algorithms are integrated into sensor nodes to preprocess and reduce the amount of data that needs to be sent over the network.\nAs modern networks move toward higher data rates and greater device density, bandwidth compression continues to evolve alongside emerging technologies such as edge computing, AI-assisted compression, and semantic communication models. These advances promise to further improve transmission efficiency by adapting compression behavior in real time based on context, content, and channel conditions. Bandwidth compression plays a critical role in modern communication systems, particularly as demand for data-intensive services continues to increase. It is not only a means to optimize transmission efficiency but also a strategic response to the limitations of physical infrastructure and spectrum availability. Bandwidth compression techniques are designed to maximize the effective use of available bandwidth, which is especially crucial in mobile communications, satellite links, and embedded systems where resources are highly constrained.\nLossless compression techniques.\nLossless compression refers to methods that reduce the data size without any loss of information. Common techniques include Huffman coding, LZW, and Arithmetic coding, which are crucial in systems requiring full data fidelity, such as medical imaging or satellite telemetry. In constrained environments like NB-IoT and EC-GSM networks, these algorithms are employed to optimize energy use and transmission efficiency.\nLossy compression techniques.\nLossy compression methods allow for partial loss of data to achieve higher compression ratios. Widely used in multimedia applications, techniques such as the Discrete Cosine Transform and wavelet transforms are essential to standards like JPEG and JPEG 2000. These methods reduce bandwidth demands in applications where slight degradation in quality is acceptable.\nAdaptive and Intelligent Compression.\nAdaptive and intelligent compression techniques utilize machine learning and context-awareness to dynamically adjust compression strategies based on the nature of the data and communication environment. These methods improve efficiency by predicting the most suitable compression parameters or algorithms in real-time, reducing redundancy while maintaining acceptable quality or fidelity.\nIn IoT and 5G/6G systems, intelligent compression mechanisms leverage edge computing and federated learning to adapt to localized data patterns, achieving better energy efficiency and reduced latency. For example, in multimedia streaming or remote monitoring, these systems may detect changes in user behavior or environmental context to optimize bitrate and avoid unnecessary data transmission.\nFurthermore, semantic-aware compression\u2014where data is interpreted and filtered based on meaning rather than raw values\u2014is an emerging trend. It enables systems to prioritize transmission of more relevant or time-sensitive information, significantly enhancing bandwidth efficiency in mission-critical applications.\nApplications in wireless sensor networks.\nWireless sensor networks (WSNs), which typically operate under stringent power and bandwidth constraints, benefit significantly from bandwidth compression techniques. Recent studies propose rate-distortion optimized methods to compress sensor readings, thereby extending battery life and network lifespan. Such approaches also help reduce transmission congestion in real-time environmental monitoring and smart infrastructure systems."}
{"id": "40772", "revid": "31541467", "url": "https://en.wikipedia.org/wiki?curid=40772", "title": "Barrage jamming", "text": "Electronic warfare technique\nBarrage jamming is an electronic warfare technique that attempts to blind (\"jam\") radar systems by filling the display with noise, rendering the broadcaster's \"blip\" invisible on the display, and often those in the nearby area as well. \"Barrage\" refers to systems that send signals in many bands of frequencies compared to the bandwidth of any single radar. This allows the jammer to jam multiple radars at once, and reduces or eliminates the need for adjustments to respond to any single radar.\nEarly radar systems typically operated on a single frequency, and could only change that frequency by changing internal electronics. Against these radars, it was possible to use conventional radio sets to send out signals on the same band, causing the radar display to be filled with noise whenever the antenna was pointed in the general direction of the jammer. However, given that each individual radar would be operating on different frequencies, this \"spot jamming\" technique required multiple radio sets in order to jam more than one radar at a time, and true wide-band barrage jamming was very difficult.\nEarly barrage jammers in World War II used photomultiplier tubes to amplify a wideband noise source, An improved approach appeared with the introduction of the carcinotron in the early 1950s, a vacuum tube that generates microwaves whose frequency can be adjusted across a very wide band simply by changing the input voltage. A single carcinotron could be swept through the entire bandwidth of any potential radar network, jamming all of the radars in such rapid sequence that it appeared to be constant noise on all frequencies at all times. A downside to this approach is that the signal only spends a brief period of time at any one radar's frequency; depending on the scanning rate, the radar may only be jammed during certain periods, but if the rate is increased to offset this, the amount of noise in any one pulse period is reduced. More complex jammers can scan only the bands it sees being used, improving its effectiveness.\nBarrage jamming was extremely effective against 1950s radars, to the point where there was some belief that the carcinotron might render ground-based radars useless, especially in the early warning radar role. By the 1960s a number of techniques had been introduced to combat barrage jamming. Frequency agile radars, which change their frequency from pulse to pulse, force the jammer spread its signal across the entire bandwidth, ensuring the signal is diluted. Combining this with extremely powerful signals and highly focused antennas allowed new radars to overpower the jammers, \"burning through\" the jamming. Simple techniques, like turning off the receivers when the antenna was pointed close to the jammer, allowed the radar to continue tracking other targets. The use of phased array antennas and signal processing techniques that reduced sidelobes also improved performance.\nBarrage jammers also have the disadvantage that they are very easy to detect using a wideband receiver. This can be used to track the jammer using a variety of techniques. A well developed instance of this was deployed by the RAF in their RX12874 network, which could track jammer-carrying aircraft with accuracy equal to a radar. More generally, a barrage jammer's signal is so easy to receive that it makes an excellent early warning signal on its own.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "40773", "revid": "63286", "url": "https://en.wikipedia.org/wiki?curid=40773", "title": "Baseband", "text": "Range of frequencies occupied by an unmodulated signal\nIn telecommunications and signal processing, baseband is the range of frequencies occupied by a signal that has not been modulated to higher frequencies. Baseband signals typically originate from transducers, converting some other variable into an electrical signal. For example, the electronic output of a microphone is a baseband signal that is analogous to the applied voice audio. In conventional analog radio broadcasting, the baseband audio signal is used to modulate an RF carrier signal of a much higher frequency.\nA baseband signal may have frequency components going all the way down to the DC bias, or at least it will have a high ratio bandwidth. A modulated baseband signal is called a passband signal. This occupies a higher range of frequencies and has a lower ratio and fractional bandwidth.\nVarious uses.\nBaseband signal.\nA \"baseband signal\" or \"lowpass signal\" is a signal that can include frequencies that are very near zero, by comparison with its highest frequency (for example, a sound waveform can be considered as a baseband signal, whereas a radio signal or any other modulated signal is not).\nA \"baseband bandwidth\" is equal to the highest frequency of a signal or system, or an upper bound on such frequencies, for example the upper cut-off frequency of a low-pass filter. By contrast, passband bandwidth is the difference between a highest frequency and a nonzero lowest frequency.\nBaseband channel.\nA \"baseband channel\" or \"lowpass channel\" (or \"system\", or \"network\") is a communication channel that can transfer frequencies that are very near zero. Examples are serial cables and local area networks (LANs), as opposed to passband channels such as radio frequency channels and passband filtered wires of the analog telephone network. Frequency division multiplexing (FDM) allows an analog telephone wire to carry a baseband telephone call, concurrently as one or several carrier-modulated telephone calls.\nDigital baseband transmission.\nDigital baseband transmission, also known as line coding, aims at transferring a digital bit stream over baseband channel, typically an unfiltered wire, contrary to passband transmission, also known as \"carrier-modulated\" transmission. Passband transmission makes communication possible over a bandpass filtered channel, such as the telephone network local-loop or a band-limited wireless channel.\nBaseband transmission in Ethernet.\nThe word \"BASE\" in Ethernet physical layer standards, for example 10BASE5, 100BASE-TX and 1000BASE-SX, implies baseband digital transmission (i.e. that a line code and an unfiltered wire are used).\nBaseband processor.\nA baseband processor also known as BP or BBP is used to process the down-converted digital signal to retrieve essential data for a wireless digital system. The baseband processing block in GNSS receivers is responsible for providing observable data: that is, code pseudo-ranges and carrier phase measurements, as well as navigation data.\nEquivalent baseband signal.\nAn \"equivalent baseband signal\" or \"equivalent lowpass signal\" is a complex valued representation of the modulated physical signal (the so-called passband signal or RF signal). It is a concept within analog and digital modulation methods for (passband) signals with constant or varying carrier frequency (for example ASK, PSK QAM, and FSK). The equivalent baseband signal is formula_1 where formula_2 is the inphase signal, formula_3 the quadrature phase signal, and formula_4 the imaginary unit. This signal is sometimes called \"IQ data\". In a digital modulation method, the formula_2 and formula_3 signals of each modulation symbol are evident from the constellation diagram. The frequency spectrum of this signal includes negative as well as positive frequencies. The physical passband signal corresponds to \nformula_7\nwhere formula_8 is the carrier angular frequency in rad/s.\nModulation.\nA signal at baseband is often used to modulate a higher frequency carrier signal in order that it may be transmitted via radio. Modulation results in shifting the signal up to much higher frequencies (radio frequencies, or RF) than it originally spanned. A key consequence of the usual double-sideband amplitude modulation (AM) is that the range of frequencies the signal spans (its spectral bandwidth) is doubled. Thus, the RF bandwidth of a signal (measured from the lowest frequency as opposed to 0\u00a0Hz) is twice its baseband bandwidth. Steps may be taken to reduce this effect, such as single-sideband modulation. Conversely, some transmission schemes such as frequency modulation use even more bandwidth.\nThe figure below shows AM modulation:\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "40774", "revid": "107723", "url": "https://en.wikipedia.org/wiki?curid=40774", "title": "Base communications", "text": ""}
{"id": "40775", "revid": "1951353", "url": "https://en.wikipedia.org/wiki?curid=40775", "title": "Basic exchange telecommunications radio service", "text": "Radio service\nIn telecommunications, a basic exchange telecommunications radio service (BETRS) is a commercial service that can extend telephone service to rural areas by replacing the local loop with radio communications. In the BETRS, non-government ultra high frequency (UHF) and very high frequency (VHF) common carrier and the private radio service frequencies are shared. BETRS technology was developed in the mid-1980s and allows up to four subscribers to use a single radio channel pair, simultaneously, without interfering with one another.\nIn the US, this service may operate in the paired 152/158 and 454/459 MHz bands and on 10 channel blocks in the 816-820/861-865\u00a0MHz bands. BETRS may be licensed only to state-certified carriers in the area where the service is provided and is considered a part of the public switched telephone network (PSTN) by state regulators.\nRegulation of this service currently resides in parts 1 and 22 of the Code of Federal Regulations (CFR), Subtitle 47 on Telecommunications, and may be researched or ordered through the Government Printing Office (GPO).\nExternal links.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "40777", "revid": "10202399", "url": "https://en.wikipedia.org/wiki?curid=40777", "title": "Basic service element", "text": "In telecommunications, a basic service element (BSE) is: \nBSEs constitute optional capabilities to which the customer may subscribe or decline to subscribe."}
{"id": "40778", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=40778", "title": "Basic serving arrangement", "text": "Telecommunications arrangement\nIn telecommunications, the term basic serving arrangement (BSA) has the following meanings: "}
{"id": "40779", "revid": "49655340", "url": "https://en.wikipedia.org/wiki?curid=40779", "title": "BCH code", "text": "Error correction code\nIn coding theory, the Bose\u2013Chaudhuri\u2013Hocquenghem codes (BCH codes) form a class of cyclic error-correcting codes that are constructed using polynomials over a finite field (also called a \"Galois field\"). BCH codes were invented in 1959 by French mathematician Alexis Hocquenghem, and independently in 1960 by Raj Chandra Bose and D. K. Ray-Chaudhuri. The name \"Bose\u2013Chaudhuri\u2013Hocquenghem\" (and the acronym \"BCH\") arises from the initials of the inventors' surnames (mistakenly, in the case of Ray-Chaudhuri).\nOne of the key features of BCH codes is that during code design, there is a precise control over the number of symbol errors correctable by the code. In particular, it is possible to design binary BCH codes that can correct multiple bit errors. Another advantage of BCH codes is the ease with which they can be decoded, namely, via an algebraic method known as syndrome decoding. This simplifies the design of the decoder for these codes, using small low-power electronic hardware.\nBCH codes are used in applications such as satellite communications, compact disc players, DVDs, disk drives, USB flash drives, solid-state drives, and two-dimensional bar codes.\nDefinition and illustration.\nPrimitive narrow-sense BCH codes.\nGiven a prime number q and prime power \"q\"\"m\" with positive integers m and d such that \"d\" \u2264 \"q\"\"m\" \u2212 1, a primitive narrow-sense BCH code over the finite field (or Galois field) GF(\"q\") with code length \"n\" \n \"q\"\"m\" \u2212 1 and distance at least d is constructed by the following method.\nLet \u03b1 be a primitive element of GF(\"q\"\"m\").\nFor any positive integer i, let \"m\"\"i\"(\"x\") be the minimal polynomial with coefficients in GF(\"q\") of \u03b1\"i\".\nThe generator polynomial of the BCH code is defined as the least common multiple \"g\"(\"x\") \n lcm(\"m\"1(\"x\"),\u2026,\"m\"\"d\" \u2212 1(\"x\")).\nIt can be seen that \"g\"(\"x\") is a polynomial with coefficients in GF(\"q\") and divides \"x\"\"n\" \u2212 1.\nTherefore, the polynomial code defined by \"g\"(\"x\") is a cyclic code.\nExample.\nLet \"q\" \n 2 and \"m\" \n 4 (therefore \"n\" \n 15). We will consider different values of d for GF(16) \n GF(24) based on the reducing polynomial \"z\"4 + \"z\" + 1, using primitive element \"\u03b1\"(\"z\") \n \"z\". There are fourteen minimum polynomials \"m\"\"i\"(\"x\") with coefficients in GF(2) satisfying\nformula_1\nThe minimal polynomials are\nformula_2\nThe BCH code with formula_3 has the generator polynomial\nformula_4\nIt has minimal Hamming distance at least 3 and corrects up to one error. Since the generator polynomial is of degree 4, this code has 11 data bits and 4 checksum bits. It is also denoted as: (15, 11) BCH code.\nThe BCH code with formula_5 has the generator polynomial\nformula_6\nIt has minimal Hamming distance at least 5 and corrects up to two errors. Since the generator polynomial is of degree 8, this code has 7 data bits and 8 checksum bits. It is also denoted as: (15, 7) BCH code.\nThe BCH code with formula_7 has the generator polynomial\nformula_8\nIt has minimal Hamming distance at least 7 and corrects up to three errors. Since the generator polynomial is of degree 10, this code has 5 data bits and 10 checksum bits. It is also denoted as: (15, 5) BCH code. (This particular generator polynomial has a real-world application, in the \"format information\" of the QR code.)\nThe BCH code with formula_9 and higher has the generator polynomial\nformula_10\nThis code has minimal Hamming distance 15 and corrects 7 errors. It has 1 data bit and 14 checksum bits. It is also denoted as: (15, 1) BCH code. In fact, this code has only two codewords: 000000000000000 and 111111111111111 (a trivial repetition code).\nGeneral BCH codes.\nGeneral BCH codes differ from primitive narrow-sense BCH codes in two respects.\nFirst, the requirement that formula_11 be a primitive element of formula_12 can be relaxed. By relaxing this requirement, the code length changes from formula_13 to formula_14 the order of the element formula_15\nSecond, the consecutive roots of the generator polynomial may run from formula_16 instead of formula_17\nDefinition. Fix a finite field formula_18 where formula_19 is a prime power. Choose positive integers formula_20 such that formula_21 formula_22 and formula_23 is the multiplicative order of formula_19 modulo formula_25\nAs before, let formula_11 be a primitive formula_27th root of unity in formula_28 and let formula_29 be the minimal polynomial over formula_30 of formula_31 for all formula_32\nThe generator polynomial of the BCH code is defined as the least common multiple formula_33\nNote: if formula_34 as in the simplified definition, then formula_35 is 1, and the order of formula_19 modulo formula_27 is formula_38\nTherefore, the simplified definition is indeed a special case of the general one.\nSpecial cases.\nThe generator polynomial formula_41 of a BCH code has coefficients from formula_42\nIn general, a cyclic code over formula_43 with formula_41 as the generator polynomial is called a BCH code over formula_45\nThe BCH code over formula_12 and generator polynomial formula_41 with successive powers of formula_11 as roots is one type of Reed\u2013Solomon code where the decoder (syndromes) alphabet is the same as the channel (data and generator polynomial) alphabet, all elements of formula_12 . The other type of Reed Solomon code is an which is not a BCH code.\nProperties.\nThe generator polynomial of a BCH code has degree at most formula_50. Moreover, if formula_51 and formula_39, the generator polynomial has degree at most formula_53.\nA BCH code has minimal Hamming distance at least formula_54.\nA BCH code is cyclic.\nEncoding.\nBecause any polynomial that is a multiple of the generator polynomial is a valid BCH codeword, BCH encoding is merely the process of finding some polynomial that has the generator as a factor.\nThe BCH code itself is not prescriptive about the meaning of the coefficients of the polynomial; conceptually, a BCH decoding algorithm's sole concern is to find the valid codeword with the minimal Hamming distance to the received codeword. Therefore, the BCH code may be implemented either as a systematic code or not, depending on how the implementor chooses to embed the message in the encoded polynomial.\nNon-systematic encoding: The message as a factor.\nThe most straightforward way to find a polynomial that is a multiple of the generator is to compute the product of some arbitrary polynomial and the generator. In this case, the arbitrary polynomial can be chosen using the symbols of the message as coefficients.\nformula_55\nAs an example, consider the generator polynomial formula_56, chosen for use in the (31, 21) binary BCH code used by POCSAG and others. To encode the 21-bit message {101101110111101111101}, we first represent it as a polynomial over formula_57:\nformula_58\nThen, compute (also over formula_57):\nformula_60\nThus, the transmitted codeword is {1100111010010111101011101110101}.\nThe receiver can use these bits as coefficients in formula_61 and, after error-correction to ensure a valid codeword, can recompute formula_62\nSystematic encoding: The message as a prefix.\nA systematic code is one in which the message appears verbatim somewhere within the codeword. Therefore, systematic BCH encoding involves first embedding the message polynomial within the codeword polynomial, and then adjusting the coefficients of the remaining (non-message) terms to ensure that formula_61 is divisible by formula_41.\nThis encoding method leverages the fact that subtracting the remainder from a dividend results in a multiple of the divisor. Hence, if we take our message polynomial formula_65 as before and multiply it by formula_66 (to \"shift\" the message out of the way of the remainder), we can then use Euclidean division of polynomials to yield:\nformula_67\nHere, we see that formula_68 is a valid codeword. As formula_69 is always of degree less than formula_70 (which is the degree of formula_41), we can safely subtract it from formula_72 without altering any of the message coefficients, hence we have our formula_61 as\nformula_74\nOver formula_57 (i.e. with binary BCH codes), this process is indistinguishable from appending a cyclic redundancy check, and if a systematic binary BCH code is used only for error-detection purposes, we see that BCH codes are just a generalization of the mathematics of cyclic redundancy checks.\nThe advantage to the systematic coding is that the receiver can recover the original message by discarding everything after the first formula_76 coefficients, after performing error correction.\nDecoding.\nThere are many algorithms for decoding BCH codes. The most common ones follow this general outline:\nDuring some of these steps, the decoding algorithm may determine that the received vector has too many errors and cannot be corrected. For example, if an appropriate value of \"t\" is not found, then the correction would fail. In a truncated (not primitive) code, an error location may be out of range. If the received vector has more errors than the code can correct, the decoder may unknowingly produce an apparently valid message that is not the one that was sent.\nCalculate the syndromes.\nThe received vector formula_77 is the sum of the correct codeword formula_78 and an unknown error vector formula_79 The syndrome values are formed by considering formula_77 as a polynomial and evaluating it at formula_81 Thus the syndromes are\nformula_82\nfor formula_83 to formula_84\nSince formula_85 are the zeros of formula_86 of which formula_87 is a multiple, formula_88 Examining the syndrome values thus isolates the error vector so one can begin to solve for it.\nIf there is no error, formula_89 for all formula_90 If the syndromes are all zero, then the decoding is done.\nCalculate the error location polynomial.\nIf there are nonzero syndromes, then there are errors. The decoder needs to figure out how many errors and the location of those errors.\nIf there is a single error, write this as formula_91 where formula_92 is the location of the error and formula_93 is its magnitude. Then the first two syndromes are\nformula_94\nso together they allow us to calculate formula_93 and provide some information about formula_92 (completely determining it in the case of Reed\u2013Solomon codes).\nIf there are two or more errors,\nformula_97\nIt is not immediately obvious how to begin solving the resulting syndromes for the unknowns formula_98 and formula_99\nThe first step is finding, compatible with computed syndromes and with minimal possible formula_100 locator polynomial:\nformula_101\nThree popular algorithms for this task are:\nPeterson\u2013Gorenstein\u2013Zierler algorithm.\nPeterson's algorithm is the step 2 of the generalized BCH decoding procedure. Peterson's algorithm is used to calculate the error locator polynomial coefficients formula_102 of a polynomial\n formula_103\nNow the procedure of the Peterson\u2013Gorenstein\u2013Zierler algorithm. Expect we have at least 2\"t\" syndromes \"s\"\"c\", \u2026, \"s\"\"c\"+2\"t\"\u22121. Let \"v\"\u00a0=\u00a0\"t\".\nFactor error locator polynomial.\nNow that you have the formula_104 polynomial, its roots can be found in the form formula_105 by brute force for example using the Chien search algorithm. The exponential\npowers of the primitive element formula_11 will yield the positions where errors occur in the received word; hence the name 'error locator' polynomial.\nThe zeros of \u039b(\"x\") are \"\u03b1\"\u2212\"i\"1, \u2026, \"\u03b1\"\u2212\"i\"\"v\".\nCalculate error values.\nOnce the error locations are known, the next step is to determine the error values at those locations. The error values are then used to correct the received values at those locations to recover the original codeword.\nFor the case of binary BCH, (with all characters readable) this is trivial; just flip the bits for the received word at these positions, and we have the corrected code word. In the more general case, the error weights formula_107 can be determined by solving the linear system\n formula_108\nForney algorithm.\nHowever, there is a more efficient method known as the Forney algorithm.\nLet\nformula_109\nformula_110\nAnd the error evaluator polynomial\nformula_111\nFinally:\nformula_112\nwhere\nformula_113\nThan if syndromes could be explained by an error word, which could be nonzero only on positions formula_114, then error values are\nformula_115\nFor narrow-sense BCH codes, \"c\" = 1, so the expression simplifies to:\nformula_116\nExplanation of Forney algorithm computation.\nIt is based on Lagrange interpolation and techniques of generating functions.\nConsider formula_117 and for the sake of simplicity suppose formula_118 for formula_119 and formula_120 for formula_121 Then\nformula_122\nformula_123\nWe want to compute unknowns formula_124 and we could simplify the context by removing the formula_125 terms. This leads to the error evaluator polynomial\nformula_126\nThanks to formula_127 we have\nformula_128\nThanks to formula_129 (the Lagrange interpolation trick) the sum degenerates to only one summand for formula_130\nformula_131\nTo get formula_98 we just should get rid of the product. We could compute the product directly from already computed roots formula_133 of formula_134 but we could use simpler form.\nAs formal derivative\nformula_135\nwe get again only one summand in\nformula_136\nSo finally\nformula_137\nThis formula is advantageous when one computes the formal derivative of formula_129 form\nformula_139\nyielding:\nformula_112\nwhere\nformula_141\nDecoding based on extended Euclidean algorithm.\nAn alternate process of finding both the polynomial \u039b and the error locator polynomial is based on Yasuo Sugiyama's adaptation of the Extended Euclidean algorithm. Correction of unreadable characters could be incorporated to the algorithm easily as well.\nLet formula_142 be positions of unreadable characters. One creates polynomial localising these positions formula_143\nSet values on unreadable positions to 0 and compute the syndromes.\nAs we have already defined for the Forney formula let formula_144\nLet us run extended Euclidean algorithm for locating least common divisor of polynomials formula_145 and formula_146\nThe goal is not to find the least common divisor, but a polynomial formula_69 of degree at most formula_148 and polynomials formula_149 such that formula_150\nLow degree of formula_69 guarantees, that formula_152 would satisfy extended (by formula_153) defining conditions for formula_154\nDefining formula_155 and using formula_156 on the place of formula_104 in the Fourney formula will give us error values.\nThe main advantage of the algorithm is that it meanwhile computes formula_158 required in the Forney formula.\nExplanation of the decoding process.\nThe goal is to find a codeword which differs from the received word minimally as possible on readable positions. When expressing the received word as a sum of nearest codeword and error word, we are trying to find error word with minimal number of non-zeros on readable positions. Syndrom formula_159 restricts error word by condition\nformula_160\nWe could write these conditions separately or we could create polynomial\nformula_161\nand compare coefficients near powers formula_162 to formula_163\nformula_164\nSuppose there is unreadable letter on position formula_165 we could replace set of syndromes formula_166 by set of syndromes formula_167 defined by equation formula_168 Suppose for an error word all restrictions by original set formula_166 of syndromes hold,\nthan\nformula_170\nNew set of syndromes restricts error vector\nformula_171\nthe same way the original set of syndromes restricted the error vector formula_172 Except the coordinate formula_165 where we have formula_174 an formula_175 is zero, if formula_176 For the goal of locating error positions we could change the set of syndromes in the similar way to reflect all unreadable characters. This shortens the set of syndromes by formula_177\nIn polynomial formulation, the replacement of syndromes set formula_166 by syndromes set formula_167 leads to\nformula_180\nTherefore,\nformula_181\nAfter replacement of formula_182 by formula_145, one would require equation for coefficients near powers formula_184\nOne could consider looking for error positions from the point of view of eliminating influence of given positions similarly as for unreadable characters. If we found formula_185 positions such that eliminating their influence leads to obtaining set of syndromes consisting of all zeros, than there exists error vector with errors only on these coordinates.\nIf formula_104 denotes the polynomial eliminating the influence of these coordinates, we obtain\nformula_187\nIn Euclidean algorithm, we try to correct at most formula_188 errors (on readable positions), because with bigger error count there could be more codewords in the same distance from the received word. Therefore, for formula_104 we are looking for, the equation must hold for coefficients near powers starting from\nformula_190\nIn Forney formula, formula_104 could be multiplied by a scalar giving the same result.\nIt could happen that the Euclidean algorithm finds formula_104 of degree higher than formula_188 having number of different roots equal to its degree, where the Fourney formula would be able to correct errors in all its roots, anyway correcting such many errors could be risky (especially with no other restrictions on received word). Usually after getting formula_104 of higher degree, we decide not to correct the errors. Correction could fail in the case formula_104 has roots with higher multiplicity or the number of roots is smaller than its degree. Fail could be detected as well by Forney formula returning error outside the transmitted alphabet.\nCorrect the errors.\nUsing the error values and error location, correct the errors and form a corrected code vector by subtracting error values at error locations.\nDecoding examples.\nDecoding of binary code without unreadable characters.\nConsider a BCH code in GF(24) with formula_196 and formula_197. (This is used in QR codes.) Let the message to be transmitted be [1 1 0 1 1], or in polynomial notation, formula_198\nThe \"checksum\" symbols are calculated by dividing formula_199 by formula_41 and taking the remainder, resulting in formula_201 or [ 1 0 0 0 0 1 0 1 0 0 ]. These are appended to the message, so the transmitted codeword is [ 1 1 0 1 1 1 0 0 0 0 1 0 1 0 0 ].\nNow, imagine that there are two bit-errors in the transmission, so the received codeword is [ 1 0 0 1 1 1 0 0 0 1 1 0 1 0 0 ]. In polynomial notation:\nformula_202\nIn order to correct the errors, first calculate the syndromes. Taking formula_203 we have formula_204 formula_205 formula_206 formula_207 formula_208 and formula_209\nNext, apply the Peterson procedure by row-reducing the following augmented matrix.\nformula_210\nDue to the zero row, \"S\"3\u00d73 is singular, which is no surprise since only two errors were introduced into the codeword.\nHowever, the upper-left corner of the matrix is identical to [\"S\"2\u00d72 ], which gives rise to the solution formula_211 formula_212\nThe resulting error locator polynomial is formula_213 which has zeros at formula_214 and formula_215\nThe exponents of formula_11 correspond to the error locations.\nThere is no need to calculate the error values in this example, as the only possible value is 1.\nDecoding with unreadable characters.\nSuppose the same scenario, but the received word has two unreadable characters [ 1 0 0 ? 1 1 ? 0 0 1 1 0 1 0 0 ]. We replace the unreadable characters by zeros while creating the polynomial reflecting their positions formula_217 We compute the syndromes formula_218 and formula_219 (Using log notation which is independent on GF(24) isomorphisms. For computation checking we can use the same representation for addition as was used in previous example. Hexadecimal description of the powers of formula_11 are consecutively 1,2,4,8,3,6,C,B,5,A,7,E,F,D,9 with the addition based on bitwise xor.)\nLet us make syndrome polynomial\nformula_221\ncompute\nformula_222\nRun the extended Euclidean algorithm:\nformula_223\nWe have reached polynomial of degree at most 3, and as\nformula_224\nwe get\nformula_225\nTherefore,\nformula_226\nLet formula_227 Don't worry that formula_228 Find by brute force a root of formula_154 The roots are formula_230 and formula_231 (after finding for example formula_232 we can divide formula_129 by corresponding monom formula_234 and the root of resulting monom could be found easily).\nLet\nformula_235\nLet us look for error values using formula\nformula_236\nwhere formula_133 are roots of formula_238 formula_239 We get\nformula_240\nFact, that formula_241 should not be surprising.\nCorrected code is therefore [ 1 1 0 1 1 1 0 0 0 0 1 0 1 0 0].\nDecoding with unreadable characters with a small number of errors.\nLet us show the algorithm behaviour for the case with small number of errors. Let the received word is [ 1 0 0 ? 1 1 ? 0 0 0 1 0 1 0 0 ].\nAgain, replace the unreadable characters by zeros while creating the polynomial reflecting their positions formula_242\nCompute the syndromes formula_243 and formula_244\nCreate syndrome polynomial\nformula_245\nLet us run the extended Euclidean algorithm:\nformula_246\nWe have reached polynomial of degree at most 3, and as\n formula_247\nwe get\n formula_248\nTherefore,\n formula_249\nLet formula_250 Don't worry that formula_251 The root of formula_104 is formula_253\nLet \nformula_254\nLet us look for error values using formula formula_255 where formula_133 are roots of polynomial formula_238\n formula_258\nWe get\nformula_259\nThe fact that formula_260 should not be surprising.\nCorrected code is therefore [ 1 1 0 1 1 1 0 0 0 0 1 0 1 0 0].\nCitations.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "40780", "revid": "8273231", "url": "https://en.wikipedia.org/wiki?curid=40780", "title": "Beam diameter", "text": "Width of an electromagnetic beam\nThe beam diameter or beam width of an electromagnetic beam is the diameter along any specified line that is perpendicular to the beam axis and intersects it. Since beams typically do not have sharp edges, the diameter can be defined in many different ways. Five definitions of the beam width are in common use: D4\u03c3, 10/90 or 20/80 knife-edge, 1/e2, FWHM, and D86. The beam width can be measured in units of length at a particular plane perpendicular to the beam axis, but it can also refer to the angular diameter (i.e., angular width), which is the angle subtended by the beam at the source. The angular width is also called the beam divergence.\nBeam diameter is usually used to characterize electromagnetic beams in the optical regime, and occasionally in the microwave regime, that is, cases in which the aperture from which the beam emerges is very large with respect to the wavelength.\nBeam diameter usually refers to a beam of circular cross section, but not necessarily so. A beam may, for example, have an elliptical cross section, in which case the orientation of the beam diameter must be specified, for example with respect to the major or minor axis of the elliptical cross section. The term \"beam width\" may be preferred in applications where the beam does not have circular symmetry.\nDefinitions.\nSiegman lists seven different measures of beam width, with some of the practical difficulties of defining beam width. Commonly used definitions include:\nRayleigh beamwidth.\nThe angle between the maximum peak of radiated power and the first null (no power radiated in this direction) is called the Rayleigh beamwidth. This is well-defined for some beam profiles, for example, the Airy diffraction pattern of a uniformly-lit aperture, but is undefined for an ideal Gaussian beam.\nFull width at half maximum.\nThe simplest way to define the width of a beam is to choose two diametrically opposite points at which the irradiance is a specified fraction of the beam's peak irradiance, and take the distance between them as a measure of the beam's width. An obvious choice for this fraction is (\u22123 dB), in which case the diameter obtained is the full width of the beam at half its maximum intensity (FWHM). This is also called the \"half-power beam width\" (HPBW).\n1/e2 width.\nThe 1/e2 width is the distance between the two points where the intensity falls to 1/e2 = 0.135 times the maximum value. If there are more than two points that are 1/e2 times the maximum value, then the two points closest to the maximum are chosen. The 1/e2 width is important in the mathematics of Gaussian beams, in which the intensity profile is described by formula_1\nThe American National Standard Z136.1-2007 for Safe Use of Lasers (p.\u00a06) defines the beam diameter as the distance between diametrically opposed points in that cross-section of a beam where the power per unit area is 1/e (0.368) times that of the peak power per unit area. This is the beam diameter definition that is used for computing the maximum permissible exposure to a laser beam. The Federal Aviation Administration also uses the 1/e definition for laser safety calculations in FAA Order JO 7400.2, Para. 29-1-5d.\nMeasurements of the 1/e2 width only depend on three points on the marginal distribution, unlike D4\u03c3 and knife-edge widths that depend on the integral of the marginal distribution. 1/e2 width measurements are noisier than D4\u03c3 width measurements. For multimodal marginal distributions (a beam profile with multiple peaks), the 1/e2 width usually does not yield a meaningful value and can grossly underestimate the inherent width of the beam. For multimodal distributions, the D4\u03c3 width is a better choice. For an ideal single-mode Gaussian beam, the D4\u03c3, D86 and 1/e2 width measurements would give the same value.\nFor a Gaussian beam, the relationship between the 1/e2 width and the full width at half maximum is formula_2 where formula_3 is the full width of the beam at 1/e2.\nD4\u03c3 or second-moment width.\nThe D4\u03c3 width of a beam in the horizontal or vertical direction is 4 times \u03c3, where \u03c3 is the standard deviation of the horizontal or vertical marginal distribution respectively. Mathematically, the D4\u03c3 beam width in the \"x\" dimension for the beam profile formula_4 is expressed as\nformula_5\nwhere\nformula_6\nis the centroid of the beam profile in the \"x\" direction.\nWhen a beam is measured with a laser beam profiler, the wings of the beam profile influence the D4\u03c3 value more than the center of the profile, since the wings are weighted by the square of its distance, \"x\"2, from the center of the beam. If the beam does not fill more than a third of the beam profiler's sensor area, then there will be a significant number of pixels at the edges of the sensor that register a small baseline value (the background value). If the baseline value is large or if it is not subtracted out of the image, then the computed D4\u03c3 value will be larger than the actual value because the baseline value near the edges of the sensor are weighted in the D4\u03c3 integral by \"x\"2. Therefore, baseline subtraction is necessary for accurate D4\u03c3 measurements. The baseline is easily measured by recording the average value for each pixel when the sensor is not illuminated. The D4\u03c3 width, unlike the FWHM and 1/e2 widths, is meaningful for multimodal marginal distributions \u2014 that is, beam profiles with multiple peaks \u2014 but requires careful subtraction of the baseline for accurate results. The D4\u03c3 is the ISO international standard definition for beam width.\nKnife-edge width.\nBefore the advent of the CCD beam profiler, the beam width was estimated using the knife-edge technique: slice a laser beam with a razor and measure the power of the clipped beam as a function of the razor position. The measured curve is the integral of the marginal distribution, and starts at the total beam power and decreases monotonically to zero power. The width of the beam is defined as the distance between the points of the measured curve that are 10% and 90% (or 20% and 80%) of the maximum value. If the baseline value is small or subtracted out, the knife-edge beam width always corresponds to 60%, in the case of 20/80, or 80%, in the case of 10/90, of the total beam power no matter what the beam profile. On the other hand, the D4\u03c3, 1/e2, and FWHM widths encompass fractions of power that are beam-shape dependent. Therefore, the 10/90 or 20/80 knife-edge width is a useful metric when the user wishes to be sure that the width encompasses a fixed fraction of total beam power. Most CCD beam profiler's software can compute the knife-edge width numerically.\nFusing knife-edge method with imaging.\nThe main drawback of the knife-edge technique is that the measured value is displayed only on the scanning direction, minimizing the amount of relevant beam information. To overcome this drawback, an innovative technology offered commercially allows multiple directions beam scanning to create an image like beam representation.\nBy mechanically moving the knife edge across the beam, the amount of energy impinging the detector area is determined by the obstruction. The profile is then measured from the knife-edge velocity and its relation to the detector's energy reading. Unlike other systems, a unique scanning technique uses several different oriented knife-edges to sweep across the beam. By using tomographic reconstruction, mathematical processes reconstruct the laser beam size in different orientations to an image similar to the one produced by CCD cameras. The main advantage of this scanning method is that it is free from pixel size limitations (as in CCD cameras) and allows beam reconstructions with wavelengths not usable with existing CCD technology. Reconstruction is possible for beams in deep UV to far IR.\nD86 width.\nThe D86 width is defined as the diameter of the circle that is centered at the centroid of the beam profile and contains 86% of the beam power. The solution for D86 is found by computing the area of increasingly larger circles around the centroid until the area contains 0.86 of the total power. Unlike the previous beam width definitions, the D86 width is not derived from marginal distributions. The percentage of 86, rather than 50, 80, or 90, is chosen because a circular Gaussian beam profile integrated down to 1/e2 of its peak value contains 86% of its total power. The D86 width is often used in applications that are concerned with knowing exactly how much power is in a given area. For example, applications of high-energy laser weapons and lidars require precise knowledge of how much transmitted power actually illuminates the target.\nISO11146 beam width for elliptic beams.\nThe definition given before holds for stigmatic (circular symmetric) beams only. For astigmatic beams, however, a more rigorous definition of the beam width has to be used:\nformula_7\nand\nformula_8\nThis definition also incorporates information about \"x\"\u2013\"y\" correlation formula_9, but for circular symmetric beams, both definitions are the same.\nSome new symbols appeared within the formulas, which are the first- and second-order moments:\nformula_10\nformula_11\nformula_12\nformula_13\nformula_14\nthe beam power \nformula_15\nand \nformula_16\nUsing this general definition, also the beam azimuthal angle formula_17 can be expressed. It is the angle between the beam directions of minimal and maximal elongations, known as principal axes, and the laboratory system, being the formula_18 and formula_19 axes of the detector and given by \nformula_20\nMeasurement.\nInternational standard ISO 11146-1:2005 specifies methods for measuring beam widths (diameters), divergence angles and beam propagation ratios of laser beams (if the beam is stigmatic) and for general astigmatic beams ISO 11146-2 is applicable. The D4\u03c3 beam width is the ISO standard definition and the measurement of the M2 beam quality parameter requires the measurement of the D4\u03c3 widths.\nThe other definitions provide complementary information to the D4\u03c3. The D4\u03c3 and knife-edge widths are sensitive to the baseline value, whereas the 1/e2 and FWHM widths are not. The fraction of total beam power encompassed by the beam width depends on which definition is used.\nThe width of laser beams can be measured by capturing an image on a camera, or by using a laser beam profiler.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "40781", "revid": "6727347", "url": "https://en.wikipedia.org/wiki?curid=40781", "title": "Beam divergence", "text": "How much a beam expands as it travels\nIn electromagnetics, especially in optics, beam divergence is an angular measure of the increase in beam diameter or radius with distance from the optical aperture or antenna aperture from which the beam emerges. The term is relevant only in the \"far field\", away from any focus of the beam. Practically speaking, however, the far field can commence physically close to the radiating aperture, depending on aperture diameter and the operating wavelength.\nBeam divergence is often used to characterize electromagnetic beams in the optical regime, for cases in which the aperture from which the beam emerges is very large with respect to the wavelength. However, it is also used in the radio frequency (RF) band for cases in which the antenna is very large relative to a wavelength.\nBeam divergence usually refers to a beam of circular cross section, but not necessarily so. A beam may, for example, have an elliptical cross section, in which case the orientation of the beam divergence must be specified, for example with respect to the major or minor axis of the elliptical cross section.\nThe divergence of a beam can be calculated if one knows the beam diameter at two separate points far from any focus (\"Di\", \"Df\"), and the distance (\"l\") between these points. The beam divergence, formula_1, is given by \nformula_2\nIf a collimated beam is focused with a lens, the diameter formula_3 of the beam in the rear focal plane of the lens is related to the divergence of the initial beam by \nformula_4\nwhere \"f\" is the focal length of the lens. Note that this measurement is valid only when the beam size is measured at the rear focal plane of the lens, i.e. where the focus would lie for a truly collimated beam, and not at the actual focus of the beam, which would occur behind the rear focal plane for a divergent beam.\nLike all electromagnetic beams, lasers are subject to divergence, which is measured in milliradians (mrad) or degrees. For many applications, a lower-divergence beam is preferable. Neglecting divergence due to poor beam quality, the divergence of a laser beam is proportional to its wavelength and inversely proportional to the diameter of the beam at its narrowest point. For example, an ultraviolet laser that emits at a wavelength of 308\u00a0nm will have a lower divergence than an infrared laser at 808\u00a0nm, if both have the same minimum beam diameter. The divergence of good-quality laser beams is modeled using the mathematics of Gaussian beams.\nGaussian laser beams are said to be diffraction limited when their radial beam divergence formula_5 is close to the minimum possible value, which is given by \nformula_6\nwhere formula_7 is the laser wavelength and formula_8 is the radius of the beam at its narrowest point, which is called the \"beam waist\". This type of beam divergence is observed from optimized laser cavities. Information on the diffraction-limited divergence of a coherent beam is inherently given by the N-slit interferometric equation.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "40782", "revid": "11677590", "url": "https://en.wikipedia.org/wiki?curid=40782", "title": "Beam steering", "text": "Changing the direction of the main lobe of a radiation pattern\nBeam steering is a technique for changing the direction of the main lobe of a radiation pattern. Beam tilt is used in radio to aim the main lobe of the vertical plane radiation pattern of an antenna below (or above) the horizontal plane.\nRadio and radar.\nIn radio and radar systems, beam steering may be accomplished by switching the antenna elements or by changing the relative phases of the RF signals driving the elements. As a result, this directs the transmit signal towards an intended receiver. In recent days, beam steering is playing a significant role in 5G communication because of quasi-optic nature of 5G frequencies.\nThe simplest method \"beam tilt\" is mechanical beam tilt, where the antenna is physically mounted in such a manner as to lower the angle of the signal on one side. However, this also raises it on the other side, making it useful in only very limited situations.\nMore common is electrical beam tilt, where the phasing between antenna elements is tweaked to make the signal go down (usually) in all directions. This is extremely useful when the antenna is at a very high point, and the edge of the signal is likely to miss the target (broadcast audience, cellphone users, etc.) entirely.\nWith electrical tilting, front and back lobes tilt in the same direction. For example, an electrical downtilt will make both the front lobe and the back lobe tilt down. This is the property used in the above example where the signal is pointed down in all directions. On the contrary, mechanical downtilting will make the front lobe tilt down and the back lobe tilt up. In almost all practical cases, antennas are only tilted down \u2013 though tilting up is technically possible.\nThe use of purely electrical tilt with no mechanical tilt is an attractive choice for aesthetic reasons which are very important for operators seeking acceptance of integrated antennas in visible locations.\nIn GSM and UMTS cellular networks, mechanical tilt is almost always fixed whereas electrical tilt can be controlled using remote actuators and position sensors, thus reducing operating expenses. Remote electrical tilt is abbreviated as RET and it is part of the Antenna Interface Standards Group's open specification for the control interface of antenna devices.\nOccasionally, mechanical and electrical tilt will be used together in order to create greater beam tilt in one direction than the other, mainly to accommodate unusual terrain. Along with null fill, beam tilt is the essential parameter controlling the focus of radio communications, and together they can create almost infinite combinations of 3-D radiation patterns for any situation.\nBeam tilt optimization.\nBeam tilt optimization is a network optimization technique used in mobile networks aiming at controlling the inclination of the vertical tilt angle of the antenna in order to optimize a set of network performance indicators.\nDifferent studies in beam tilt optimization focus on Coverage-Capacity Optimization (CCO), for which the goal is to control the beam tilt in order to jointly optimize the radio coverage and capacity in the network cells and reduce interference from neighbouring cells.\nThere exists mainly two types of approaches to beam tilt optimization:\nAcoustics.\nIn acoustics, beam steering is used to direct the audio from loudspeakers to a specific location in the listening area. This is done by changing the magnitude and phase of two or more loudspeakers installed in a column where the combined sound is added and cancelled at the required position. Commercially, this type of loudspeaker arrangement is known as a line array. This technique has been around for many years but since the emergence of modern digital signal processing (DSP) technology there are now many commercially available products on the market. Beam steering and directivity Control using DSP was pioneered in the early 1990s by Duran Audio who launched a technology called DDC (Digital Directivity Control).\nOptical system.\nIn optical systems, beam steering may be accomplished by changing the refractive index of the medium through which the beam is transmitted or by the use of mirrors, prisms, lenses, or rotating diffraction gratings. Examples of optical beam steering approaches include mechanical mirror-based gimbals or beam-director units, galvanometer mechanisms that rotate mirrors, Risley prisms, phased-array optics, and microelectromechanical systems using micro-mirrors.\nBeam Steering Applications and Emerging Techniques.\nThe scope of beam-steering technologies has broadened significantly with innovations that serve both traditional applications and emerging demands in fields such as satellite communication, radar, and 5G networks. Traditional methods like parabolic reflectors and phased arrays are now complemented by Reflectarray (RA) and Transmitarray (TA) antennas. These designs serve as high-gain, planar alternatives with advantages in cost, efficiency, and scalability, meeting modern requirements for compact and lightweight systems. One of the latest approaches in beam steering involves Near-Field Meta-Steering (NFMS), which uses phase-gradient metasurfaces placed in close proximity to a feed antenna. This method achieves 3D beam steering by employing compact structures that allow wide-angle control over both elevation and azimuth, proving highly effective for systems where space and profile height are restricted. \nBeam steering has also found essential applications in high-speed, interference-free communication for defense and civilian markets. Satellite-based communication systems, for example, require dual-band beam-steering capabilities to handle uplink and downlink data streams simultaneously. The development of beam-steering antennas for Satellite Communication on the Move (SOTM) systems highlights the need for antennas that are not only efficient but also lightweight, low-profile, and cost-effective. Challenges remain, including addressing cost constraints and achieving higher scanning speeds and wider bandwidths.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nSource: from Federal Standard 1037C"}
{"id": "40783", "revid": "11292982", "url": "https://en.wikipedia.org/wiki?curid=40783", "title": "Beamwidth", "text": ""}
{"id": "40784", "revid": "3653754", "url": "https://en.wikipedia.org/wiki?curid=40784", "title": "B8ZS", "text": ""}
{"id": "40785", "revid": "48549243", "url": "https://en.wikipedia.org/wiki?curid=40785", "title": "Bel", "text": "Bel may refer to:\n&lt;templatestyles src=\"Template:TOC_right/styles.css\" /&gt;\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "40786", "revid": "15881234", "url": "https://en.wikipedia.org/wiki?curid=40786", "title": "Bias", "text": "Inclination for or against\nBias is a disproportionate weight \"in favor of\" or \"against\" an idea or thing, usually in a way that is inaccurate, closed-minded, prejudicial, or unfair. Biases can be innate or learned. People may develop biases for or against an individual, a group, or a belief. In science and engineering, a bias is a systematic error. Statistical bias results from an unfair sampling of a population, or from an estimation process that does not give accurate results on average.\nEtymology.\nThe word appears to derive from Old Proven\u00e7al into Old French \"biais\", \"sideways, askance, against the grain\". Whence comes French \"biais\", \"a slant, a slope, an oblique\".\nIt seems to have entered English via the game of bowls, where it referred to balls made with a greater weight on one side. Then, it expanded to the figurative use, \"a one-sided tendency of the mind\", and, at first especially in law, \"undue propensity or prejudice\". or ballast, used to lower the centre of gravity of a ship to increase stability and to keep the ship from one side.\nTypes.\nCognitive biases.\nA cognitive bias is a repeating or basic misstep in thinking, assessing, recollecting, or other cognitive processes. That is, a pattern of deviation from standards in judgment, whereby inferences may be created unreasonably. People create their own \"subjective social reality\" from their own perceptions, their view of the world may dictate their behaviour. Thus, cognitive biases may sometimes lead to perceptual distortion, inaccurate judgment, illogical interpretation, or what is broadly called irrationality. However some cognitive biases are taken to be adaptive, and thus may lead to success in the appropriate situation. Furthermore, cognitive biases as an example through education may allow faster choice selection when speedier outcomes for a task are more valuable than precision. Other cognitive biases are a \"by-product\" of human processing limitations, coming about because of an absence of appropriate mental mechanisms, or just from human limitations in information processing.\nAnchoring.\nAnchoring is a psychological heuristic that describes the propensity to rely on the first piece of information encountered when making decisions. According to this heuristic, individuals begin with an implicitly suggested reference point (the \"anchor\") and make adjustments to it to reach their estimate. For example, the initial price offered for a used car sets the standard for the rest of the negotiations, so that prices lower than the initial price seem more reasonable even if they are still higher than what the car is worth.\nApophenia.\nApophenia, also known as patternicity, or agenticity, is the human tendency to perceive meaningful patterns within random data. Apophenia is well documented as a rationalization for gambling. Gamblers may imagine that they see patterns in the numbers which appear in lotteries, card games, or roulette wheels. One manifestation of this is known as the \"gambler's fallacy\".\nPareidolia is the visual or auditory form of apophenia. It has been suggested that pareidolia combined with hierophany may have helped ancient societies organize chaos and make the world intelligible.\nAttribution bias.\nAn attribution bias can happen when individuals assess or attempt to discover explanations behind their own and others' behaviors. People make attributions about the causes of their own and others' behaviors; but these attributions do not necessarily precisely reflect reality. Rather than operating as objective perceivers, individuals are inclined to perceptual slips that prompt biased understandings of their social world. When judging others we tend to assume their actions are the result of internal factors such as personality, whereas we tend to assume our own actions arise because of the necessity of external circumstances. There are a wide range of sorts of attribution biases, such as the ultimate attribution error, fundamental attribution error, actor-observer bias, and self-serving bias.\nExamples of attribution bias:\nConfirmation bias.\nConfirmation bias is the tendency to search for, interpret, favor, and recall information in a way that confirms one's beliefs or hypotheses while giving disproportionately less attention to information that contradicts it. The effect is stronger for emotionally charged issues and for deeply entrenched beliefs. People also tend to interpret ambiguous evidence as supporting their existing position. Biased search, interpretation and memory have been invoked to explain attitude polarization (when a disagreement becomes more extreme even though the different parties are exposed to the same evidence), belief perseverance (when beliefs persist after the evidence for them is shown to be false), the irrational primacy effect (a greater reliance on information encountered early in a series) and illusory correlation (when people falsely perceive an association between two events or situations). Confirmation biases contribute to overconfidence in personal beliefs and can maintain or strengthen beliefs in the face of contrary evidence. Poor decisions due to these biases have been found in political and organizational contexts.\nFraming.\nFraming involves the social construction of social phenomena by mass media sources, political or social movements, political leaders, and so on. It is an influence over how people organize, perceive, and communicate about reality. It can be positive or negative, depending on the audience and what kind of information is being presented. For political purposes, framing often presents facts in such a way that implicates a problem that is in need of a solution. Members of political parties attempt to frame issues in a way that makes a solution favoring their own political leaning appear as the most appropriate course of action for the situation at hand. As understood in social theory, framing is a schema of interpretation, a collection of anecdotes and stereotypes, that individuals rely on to understand and respond to events. People use filters to make sense of the world, the choices they then make are influenced by their creation of a frame.\nCultural bias is the related phenomenon of interpreting and judging phenomena by standards inherent to one's own culture. Numerous such biases exist, concerning cultural norms for color, location of body parts, mate selection, concepts of justice, linguistic and logical validity, acceptability of evidence, and taboos. Ordinary people may tend to imagine other people as basically the same, not significantly more or less valuable, probably attached emotionally to different groups and different land.\nHalo effect and horn effect.\nThe halo effect and the horn effect are when an observer's overall impression of a person, organization, brand, or product influences their feelings about specifics of that entity's character or properties.\nThe name halo effect is based on the concept of the saint's halo, and is a specific type of confirmation bias, wherein positive sentiments in one area cause questionable or unknown characteristics to be seen positively. If the observer likes one aspect of something, they will have a positive predisposition toward everything about it. A person's appearance has been found to produce a halo effect. The halo effect is also present in the field of brand marketing, affecting perception of companies and non-governmental organizations (NGOs).\nThe opposite of the halo is the horn effect, when \"individuals believe (that negative) traits are inter-connected.\" The term horn effect refers to Devil's horns. It works in a negative direction: if the observer dislikes one aspect of something, they will have a negative predisposition towards other aspects.\nSelf-serving bias.\nSelf-serving bias is the tendency for cognitive or perceptual processes to be distorted by the individual's need to maintain and enhance self-esteem. It is the propensity to credit accomplishment to our own capacities and endeavors, yet attribute failure to outside factors, to dismiss the legitimacy of negative criticism, concentrate on positive qualities and accomplishments yet disregard flaws and failures. Studies have demonstrated that this bias can affect behavior in the workplace, in interpersonal relationships, playing sports, and in consumer decisions.\nStatus quo bias.\nStatus quo bias is an emotional bias; a preference for the current state of affairs. The current baseline (or status quo) is taken as a reference point, and any change from that baseline is perceived as a loss.\nStatus quo bias should be distinguished from a rational preference for the status quo ante, as when the current state of affairs is objectively superior to the available alternatives, or when imperfect information is a significant problem. A large body of evidence, however, shows that status quo bias frequently affects human decision-making.\nConflicts of interest.\nA conflict of interest is when a person or association has intersecting interests (financial, personal, etc.) which could potentially corrupt. The potential conflict is autonomous of actual improper actions, it can be found and intentionally defused before corruption, or the appearance of corruption, happens. \"A conflict of interest is a set of circumstances that creates a risk that professional judgement or actions regarding a primary interest will be unduly influenced by a secondary interest.\" It exists if the circumstances are sensibly accepted to present a hazard that choices made may be unduly affected by auxiliary interests.\nCorruption.\nA conflict of interest arises when a decision-maker participates in a corrupt act that seeks to influence the outcome in favor of a specific individual, organization, or entity in a decision-making process. For example, attempts to solicit a bribe or kickback in exchange for favoring a party creates a conflict of interest. A perceived conflict of interest may also arise in an individual who is offered such a payment, even if it is declined, particularly in situations where the attempt to bribe is not reported.\nLaws restricting whether a monetary transaction is appropriate can differ between jurisdictions based upon their criminal laws. For example, some nations criminalize the receipt of political campaign contributions in the form of cash, while other nations permit cash donations provided that donors otherwise adhere to election law.\nFavoritism.\nFavoritism, sometimes known as in-group favoritism, or in-group bias, refers to a pattern of favoring members of one's in-group over out-group members. This can be expressed in evaluation of others, in allocation of resources, and in many other ways. This has been researched by psychologists, especially social psychologists, and linked to group conflict and prejudice. Cronyism is favoritism of long-standing friends, especially by appointing them to positions of authority, regardless of their qualifications. Nepotism is favoritism granted to relatives.\nLobbying.\nLobbying is the attempt to influence choices made by administrators, frequently lawmakers or individuals from administrative agencies. Lobbyists may be among a legislator's constituencies, or not; they may engage in lobbying as a business, or not. Lobbying is often spoken of with contempt, the implication is that people with inordinate socioeconomic power are corrupting the law in order to serve their own interests. When people who have a duty to act on behalf of others, such as elected officials with a duty to serve their constituents' interests or more broadly the common good, stand to benefit by shaping the law to serve the interests of some private parties, there is a conflict of interest. This can lead to all sides in a debate looking to sway the issue by means of lobbyists.\nRegulatory issues.\nSelf-regulation is the process whereby an organization monitors its own adherence to legal, ethical, or safety standards, rather than have an outside, independent agency such as a third party entity monitor and enforce those standards. Self-regulation of any group can create a conflict of interest. If any organization, such as a corporation or government bureaucracy, is asked to eliminate unethical behavior within their own group, it may be in their interest in the short run to eliminate the appearance of unethical behavior, rather than the behavior itself.\nRegulatory capture is a form of political corruption that can occur when a regulatory agency, created to act in the public interest, instead advances the commercial or political concerns of special interest groups that dominate the industry or sector it is charged with regulating. Regulatory capture occurs because groups or individuals with a high-stakes interest in the outcome of policy or regulatory decisions can be expected to focus their resources and energies in attempting to gain the policy outcomes they prefer, while members of the public, each with only a tiny individual stake in the outcome, will ignore it altogether. Regulatory capture is a risk to which a regulatory agency is exposed by its very nature.\nShilling.\nShilling is deliberately giving spectators the feeling that one is an energetic autonomous client of a vendor for whom one is working. The effectiveness of shilling relies on crowd psychology to encourage other onlookers or audience members to purchase the goods or services (or accept the ideas being marketed). Shilling is illegal in some places, but legal in others. An example of shilling is paid reviews that give the impression of being autonomous opinions.\nStatistical biases.\nStatistical bias is a systematic tendency in the process of data collection, which results in lopsided, misleading results. This can occur in any of a number of ways, in the way the sample is selected, or in the way data are collected. It is a property of a statistical technique or of its results whereby the expected value of the results differs from the true underlying quantitative parameter being estimated.\nForecast bias.\nA forecast bias is when there are consistent differences between results and the forecasts of those quantities; that is: forecasts may have an overall tendency to be too high or too low.\nObserver-expectancy effect.\nThe observer-expectancy effect is when a researcher's expectations cause them to subconsciously influence the people participating in an experiment. It is usually controlled using a double-blind system, and was an important reason for the development of double-blind experiments.\nReporting bias and social desirability bias.\nIn epidemiology and empirical research, reporting bias is defined as \"selective revealing or suppression of information\" of undesirable behavior by subjects or researchers.\nIt refers to a tendency to under-report unexpected or undesirable experimental results, while being more trusting of expected or desirable results. This can propagate, as each instance reinforces the status quo, and later experimenters justify their own reporting bias by observing that previous experimenters reported different results.\nSocial desirability bias is a bias within social science research where survey respondents can tend to answer questions in a manner that will be viewed positively by others. It can take the form of over-reporting laudable behavior, or under-reporting undesirable behavior. This bias interferes with the interpretation of average tendencies as well as individual differences. The inclination represents a major issue with self-report questionnaires; of special concern are self-reports of abilities, personalities, sexual behavior, and drug use.\nSelection bias.\nSelection bias is the conscious or unconscious bias introduced into a study by the way individuals, groups or data are selected for analysis, if such a way means that true randomization is not achieved, thereby ensuring that the sample obtained is not representative of the population intended to be analyzed. This results in a sample that may be significantly different from the overall population.\nPrejudices.\nBias and prejudice are usually considered to be closely related. Prejudice is prejudgment, or forming an opinion before becoming aware of the relevant facts of a case. The word is often used to refer to preconceived, usually unfavorable, judgments toward people or a person because of gender, political opinion, social class, age, disability, religion, sexuality, race/ethnicity, language, nationality, or other personal characteristics. Prejudice can also refer to unfounded beliefs and may include \"any unreasonable attitude that is unusually resistant to rational influence\".\nAgeism.\nAgeism is the stereotyping and/or discrimination against individuals or groups on the basis of their age. It can be used in reference to prejudicial attitudes towards older people, or towards younger people.\nClassism.\nClassism is discrimination on the basis of social class. It includes attitudes that benefit the upper class at the expense of the lower class, or vice versa.\nLookism.\nLookism is stereotypes, prejudice, and discrimination on the basis of physical attractiveness, or more generally to people whose appearance matches cultural preferences. Many people make automatic judgments of others based on their physical appearance that influence how they respond to those people.\nRacism.\nRacism consists of ideologies based on a desire to dominate or a belief in the inferiority of another race. It may also hold that members of different races should be treated differently.\nContextual biases.\nBiases in academia.\nAcademic bias.\nAcademic bias is the bias or perceived bias of scholars allowing their beliefs to shape their research and the scientific community. Claims of bias are often linked to claims by conservatives of pervasive bias against political conservatives and religious Christians. Some have argued that these claims are based upon anecdotal evidence which would not reliably indicate systematic bias, and have suggested that this divide is due to self-selection of conservatives choosing not to pursue academic careers.\nThere is some evidence that perception of classroom bias may be rooted in issues of sexuality, race, class and sex as much or more than in religion.\nExperimenter bias.\nIn science research, experimenter bias occurs when experimenter expectancies regarding study results bias the research outcome. Examples of experimenter bias include conscious or unconscious influences on subject behavior including creation of demand characteristics that influence subjects, and altered or selective recording of experimental results themselves. It can also involve asking leading probes and not neutrally redirecting the subject back to the task when they ask for validation or questions.\nFunding bias.\nFunding bias refers to the tendency of a scientific study to support the interests of the study's financial sponsor. This phenomenon is recognized sufficiently that researchers undertake studies to examine bias in past published studies. It can be caused by any or all of: a conscious or subconscious sense of obligation of researchers towards their employers, misconduct or malpractice, publication bias, or reporting bias.\nFull text on net bias.\nFull text on net (or FUTON) bias is a tendency of scholars to cite academic journals with open access\u2014that is, journals that make their full text available on the internet without charge\u2014in their own writing as compared with toll access publications. Scholars can more easily discover and access articles that have their full text on the internet, which increases authors' likelihood of reading, quoting, and citing these articles, this may increase the impact factor of open access journals relative to journals without open access.\nThe related bias, no abstract available bias (NAA bias) is scholars' tendency to cite journal articles that have an abstract available online more readily than articles that do not.\nPublication bias.\nPublication bias is a type of bias with regard to what academic research is likely to be published because of a tendency among researchers and journal editors to prefer some outcomes rather than others (e.g., results showing a significant finding), which leads to a problematic bias in the published literature. This can propagate further as literature reviews of claims about support for a hypothesis will themselves be biased if the original literature is contaminated by publication bias. Studies with significant results often do not appear to be superior to studies with a null result with respect to quality of design. However, statistically significant results have been shown to be three times more likely to be published compared to papers with null results.\nBiases in law enforcement.\nDriving while black.\nDriving while black refers to the racial profiling of African American drivers. The phrase implies that a motorist might be pulled over by a police officer, questioned, and searched, because of a racial bias.\nRacial profiling.\nRacial profiling, or ethnic profiling, is the act of suspecting or targeting a person of a certain race on the basis of racially observed characteristics or behavior, rather than on individual suspicion. Racial profiling is commonly referred to regarding its use by law enforcement, and its leading to discrimination against minorities.\nVictim blaming.\nVictim blaming occurs when the victim of a wrongful act is held at fault for the harm that befell them. The study of victimology seeks to mitigate the perception of victims as responsible.\nBiases in media.\nMedia bias is the bias or perceived bias of journalists and news producers within the mass media in the selection of events, the stories that are reported, and how they are covered. The term generally implies a pervasive or widespread bias violating the standards of journalism, rather than the perspective of an individual journalist or article. The level of media bias in different nations is debated. There are also watchdog groups that report on media bias.\nPractical limitations to media neutrality include the inability of journalists to report all available stories and facts, the requirement that selected facts be linked into a coherent narrative, government influence including overt and covert censorship, the influence of the owners of the news source, concentration of media ownership, the selection of staff, the preferences of an intended audience, and pressure from advertisers.\nBias has been a feature of the mass media since its birth with the invention of the printing press. The expense of early printing equipment restricted media production to a limited number of people. Historians have found that publishers often served the interests of powerful social groups.\nAgenda setting.\nAgenda setting describes the capacity of the media to focus on particular stories, if a news item is covered frequently and prominently, the audience will regard the issue as more important. That is, its salience will increase.\nGatekeeping.\nGatekeeping is the way in which information and news are filtered to the public, by each person or corporation along the way. It is the \"process of culling and crafting countless bits of information into the limited number of messages that reach people every day, and it is the center of the media's role in modern public life. [...] This process determines not only which information is selected, but also what the content and nature of the messages, such as news, will be.\"\nSensationalism.\nSensationalism is when events and topics in news stories and pieces are overhyped to present skewed impressions of events, which may cause a misrepresentation of the truth of a story. Sensationalism may involve reporting about insignificant matters and events, or the presentation of newsworthy topics in a trivial or tabloid manner contrary to the standards of professional journalism.\nOther contexts.\nEducational bias.\nBias in education refers to real or perceived bias in the educational system. The content of school textbooks is often the issue of debate, as their target audience is young people, and the term \"whitewashing\" is used to refer to selective removal of critical or damaging evidence or comment. Religious bias in textbooks is observed in countries where religion plays a dominant role. There can be many forms of educational bias. Some overlooked aspects, occurring especially with the pedagogical circles of public and private schools\u2014sources that are unrelated to fiduciary or mercantile impoverishment which may be unduly magnified\u2014include teacher bias as well as a general bias against women who are going into STEM research.\nInductive bias.\nInductive bias occurs within the field of machine learning. In machine learning one seeks to develop algorithms that are able to \"learn\" to anticipate a particular output. To accomplish this, the learning algorithm is given training cases that show the expected connection. Then the learner is tested with new examples. Without further assumptions, this problem cannot be solved exactly as unknown situations may not be predictable. The inductive bias of the learning algorithm is the set of assumptions that the learner uses to predict outputs given inputs that it has not encountered. It may bias the learner towards the correct solution, the incorrect, or be correct some of the time. A classical example of an inductive bias is Occam's Razor, which assumes that the simplest consistent hypothesis is the best.\nInsider trading.\nInsider trading is the trading of a public company's stock or other securities (such as bonds or stock options) by individuals with access to non-public information about the company. In various countries, trading based on insider information is illegal because it is seen as unfair to other investors who do not have access to the information as the investor with insider information could potentially make far larger profits that a typical investor could make.\nMatch fixing.\nIn organized sports, match fixing occurs when a match is played to a completely or partially pre-determined result, violating the rules of the game and often the law. There is a variety of reasons for this, but the most common is in exchange for a payoff from gamblers. Players might also intentionally perform poorly to get an advantage in the future (such as a better draft pick, or an easier opponent in a playoff), or to rig a handicap system. Match-fixing generally refers to fixing the final result of the game. Another form of match-fixing, known as spot-fixing, involves fixing small events within a match which can be gambled upon, but which are unlikely to prove decisive in determining the final result of the game.\nImplicit bias.\nAn implicit bias, or implicit stereotype, is the unconscious attribution of particular qualities to a member of a certain social group.\nImplicit stereotypes are shaped by experience and based on learned associations between particular qualities and social categories, including race and/or gender. Individuals' perceptions and behaviors can be influenced by the implicit stereotypes they hold, even if they are unaware/unintentionally hold such stereotypes. Implicit bias is an aspect of implicit social cognition: the phenomenon that perceptions, attitudes, and stereotypes operate without conscious intention. For example, researchers may have implicit bias when designing survey questions and as a result, the questions do not produce accurate results or fail to encourage survey participation. The existence of implicit bias is supported by a variety of scientific articles in psychological literature. Implicit stereotype was first defined by psychologists Mahzarin Banaji and Anthony Greenwald in 1995.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "40788", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=40788", "title": "Bias distortion", "text": "In telecommunications, the term bias distortion has the following meanings: \nBias distortion is expressed in percent of the system-specified unit interval.\nReferences.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "40789", "revid": "50463899", "url": "https://en.wikipedia.org/wiki?curid=40789", "title": "Bilateral synchronization", "text": "Telephony synchronization technique\nBilateral Synchronization in Telecommunications.\nIn telecommunications, bilateral synchronization (or bilateral control) refers to a synchronization control system used between two exchanges (A and B). In this system, the clock at exchange A controls the data received at exchange B, while the clock at exchange B controls the data received at exchange A. This two-way synchronization ensures that data transmission between both exchanges remains aligned, minimizing errors caused by timing mismatches.\nThis type of synchronization is commonly implemented by deriving the timing from the incoming bitstream. The bitstream, which contains data bits transmitted between the exchanges, serves as the reference from which the timing information is extracted. By using this method, bilateral synchronization helps maintain consistent timing and prevents data corruption during transmission.\nTechnical Overview.\nBilateral synchronization plays a crucial role in ensuring the reliable transfer of data between different nodes in a telecommunications network. It ensures that each exchange accurately interprets the data stream it receives, based on the timing reference from the opposite exchange.\nProcess.\nBy keeping both clocks in sync, bilateral synchronization ensures that data packets are properly timed for accurate reception and processing at both ends of the communication chain.\nApplications.\nBilateral synchronization is critical in various telecommunications systems, including:\nBilateral Synchronization vs. Other Synchronization Techniques.\nBilateral synchronization is often compared to other synchronization methods, such as plesiochronous and synchronous systems.\nBilateral Synchronization in Practice.\nBilateral synchronization is particularly effective in systems where there is a need for precision timing between two points, without the complexity of a centralized synchronization model. It allows for flexibility and adaptability in systems that handle large volumes of data, such as in the telecommunications backbone, where data streams from multiple sources need to be kept in sync across various nodes.\nChallenges.\nWhile bilateral synchronization can offer advantages, it also poses challenges, including:\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "40790", "revid": "1883085", "url": "https://en.wikipedia.org/wiki?curid=40790", "title": "Billboard antenna", "text": ""}
{"id": "40791", "revid": "11952314", "url": "https://en.wikipedia.org/wiki?curid=40791", "title": "Binary notation", "text": ""}
{"id": "40792", "revid": "7903804", "url": "https://en.wikipedia.org/wiki?curid=40792", "title": "Bipolar signal", "text": "In telecommunications, a bipolar signal is a signal which may assume either of two polarities, neither of which is zero. \nA bipolar signal may have a two-state non-return-to-zero (NRZ) or a three-state return-to-zero (RZ) binary coding scheme. \nA bipolar signal is usually symmetrical with respect to zero amplitude, \"i.e.\" the absolute values of the positive and negative signal states are nominally equal. Contrast with unipolar encoding where one state is zero amplitude.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "40793", "revid": "36828769", "url": "https://en.wikipedia.org/wiki?curid=40793", "title": "Bit-count integrity", "text": "In telecommunications, the term bit-count integrity (BCI) has the following meanings: \n\"Note:\" Bit-count integrity is not the same as bit integrity, which requires that the delivered bits correspond exactly with the original bits.\nSources.\nfrom Federal Standard 1037C and from MIL-STD-188"}
{"id": "40794", "revid": "10202399", "url": "https://en.wikipedia.org/wiki?curid=40794", "title": "Bit error rate", "text": "Number of incorrect bits transmitted\nIn digital transmission, the number of bit errors is the number of received bits of a data stream over a communication channel that have been altered due to noise, interference, distortion or bit synchronization errors.\nThe bit error rate (BER) is the number of bit errors per unit time. The bit error ratio (also BER) is the number of bit errors divided by the total number of transferred bits during a studied time interval. Bit error ratio is a unitless performance measure, often expressed as a percentage.\nThe bit error probability \"pe\" is the expected value of the bit error ratio. The bit error ratio can be considered as an approximate estimate of the bit error probability. This estimate is accurate for a long time interval and a high number of bit errors.\nExample.\nAs an example, assume this transmitted bit sequence:\n1 1 0 0 0 1 0 1 1 \nand the following received bit sequence:\n0 1 0 1 0 1 0 0 1,\nThe number of bit errors (the underlined bits) is, in this case, 3. The BER is 3 incorrect bits divided by 9 transferred bits, resulting in a BER of 0.333 or 33.3%.\nPacket error ratio.\nThe packet error ratio (PER) is the number of incorrectly received data packets divided by the total number of received packets. A packet is declared incorrect if at least one bit is erroneous. The expectation value of the PER is denoted packet error probability \"pp\", which for a data packet length of \"N\" bits can be expressed as\nformula_1,\nassuming that the bit errors are independent of each other. For small bit error probabilities and large data packets, this is approximately\nformula_2\nSimilar measurements can be carried out for the transmission of frames, blocks, or symbols.\nThe above expression can be rearranged to express the corresponding BER (\"pe\") as a function of the PER (\"pp\") and the data packet length \"N\" in bits:\nformula_3\nFactors affecting the BER.\nIn a communication system, the receiver side BER may be affected by transmission channel noise, interference, distortion, bit synchronization problems, attenuation, wireless multipath fading, etc.\nThe BER may be improved by choosing a strong signal strength (unless this causes cross-talk and more bit errors), by choosing a slow and robust modulation scheme or line coding scheme, and by applying channel coding schemes such as redundant forward error correction codes.\nThe \"transmission BER\" is the number of detected bits that are incorrect before error correction, divided by the total number of transferred bits (including redundant error codes). The \"information BER\", approximately equal to the decoding error probability, is the number of decoded bits that remain incorrect after the error correction, divided by the total number of decoded bits (the useful information). Normally the transmission BER is larger than the information BER. The information BER is affected by the strength of the forward error correction code.\nAnalysis of the BER.\nThe BER may be evaluated using stochastic (Monte Carlo) computer simulations. If a simple transmission channel model and data source model is assumed, the BER may also be calculated analytically. An example of such a data source model is the Bernoulli source.\nExamples of simple channel models used in information theory are:\nA worst-case scenario is a completely random channel, where noise totally dominates over the useful signal. This results in a transmission BER of 50% (provided that a Bernoulli binary data source and a binary symmetrical channel are assumed, see below).\nIn a noisy channel, the BER is often expressed as a function of the normalized carrier-to-noise ratio measure denoted Eb/N0, (energy per bit to noise power spectral density ratio), or Es/N0 (energy per modulation symbol to noise spectral density).\nFor example, in the case of BPSK modulation and AWGN channel, the BER as function of the Eb/N0 is given by:\nformula_4,\nwhere\nformula_5. \nPeople usually plot the BER curves to describe the performance of a digital communication system. In optical communication, BER(dB) vs. Received Power(dBm) is usually used; while in wireless communication, BER(dB) vs. SNR(dB) is used.\nMeasuring the bit error ratio helps people choose the appropriate forward error correction codes. Since most such codes correct only bit-flips, but not bit-insertions or bit-deletions, the Hamming distance metric is the appropriate way to measure the number of bit errors. Many FEC coders also continuously measure the current BER.\nA more general way of measuring the number of bit errors is the Levenshtein distance.\nThe Levenshtein distance measurement is more appropriate for measuring raw channel performance before frame synchronization, and when using error correction codes designed to correct bit-insertions and bit-deletions, such as Marker Codes and Watermark Codes.\nMathematical draft.\nThe BER is the likelihood of a bit misinterpretation due to electrical noise formula_6. Considering a bipolar NRZ transmission, we have\nformula_7 for a \"1\" and formula_8 for a \"0\". Each of formula_9 and formula_10 has a period of formula_11.\nKnowing that the noise has a bilateral spectral density formula_12,\nformula_9 is formula_14\nand formula_10 is formula_16.\nReturning to BER, we have the likelihood of a bit misinterpretation formula_17.\nformula_18 and formula_19\nwhere formula_20 is the threshold of decision, set to 0 when formula_21.\nWe can use the average energy of the signal formula_22 to find the final expression :\nformula_23\nBit error rate test.\nBERT or bit error rate test is a testing method for digital communication circuits that uses predetermined stress patterns consisting of a sequence of logical ones and zeros generated by a test pattern generator.\nA BERT typically consists of a test pattern generator and a receiver that can be set to the same pattern. They can be used in pairs, with one at either end of a transmission link, or singularly at one end with a loopback at the remote end. BERTs are typically stand-alone specialised instruments, but can be personal computer\u2013based. In use, the number of errors, if any, are counted and presented as a ratio such as 1 in 1,000,000, or 1 in 1e06.\nBit error rate tester.\nA bit error rate tester (BERT), also known as a \"bit error ratio tester\" or \"bit error rate test solution\" (BERTs) is electronic test equipment used to test the quality of signal transmission of single components or complete systems.\nThe main building blocks of a BERT are:\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "40795", "revid": "612852", "url": "https://en.wikipedia.org/wiki?curid=40795", "title": "Bit inversion", "text": "Computing term meaning changing the state of a bit\nIn computing and telecommunications, bit inversion refers to the changing of the state of a bit or binary number to the opposite state, for example changing a 0 bit to 1 or vice versa. It is often represented with a tilde (~). It also refers to the changing of a \"state representing a given bit\" to the opposite state.\nUsage in computing.\nMany popular programming languages implement bit inversion as an operation. For example, in JavaScript, bit inversion is known as a 'bitwise NOT' and is implemented as seen below:\nvar a = 2;\nvar b = ~ a;\nIn this example, a is a 32-bit signed integer and in binary would be 00000000000000000000000000000010. Variable b is the bit inversion of variable a and equals 11111111111111111111111111111101 (\u22123 in decimal).\nIn Python:\n\u00bb&gt; a = 2\n\u00bb&gt; b = ~ a\n\u00bb&gt; b\n-3\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "40796", "revid": "6127189", "url": "https://en.wikipedia.org/wiki?curid=40796", "title": "Bit pairing", "text": "In telecommunications, bit pairing is the practice of establishing, within a code set, a number of subsets that have an identical bit representation except for the state of a specified bit. \n\"Note:\" An example of bit pairing occurs in the International Alphabet No. 5 and the American Standard Code for Information Interchange (ASCII), where the upper case letters are related to their respective lower case letters by the state of bit six.\nReferences.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "40797", "revid": "8066546", "url": "https://en.wikipedia.org/wiki?curid=40797", "title": "Bit robbing", "text": ""}
{"id": "40798", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=40798", "title": "Bit-sequence independence", "text": "Telecommunication term\nIn telecommunications, bit-sequence independence is a characteristic of some digital data transmission as well as \"digital transmission\" systems that impose no restrictions on, or modification of, the transmitted bit sequence.\nBit-sequence-independent protocols are in contrast to protocols that reserve certain bit sequences for special meanings, such as the flag sequence, 01111110, for HDLC, SDLC, and ADCCP protocols.\nBit-sequence-independence allows only line codes that have the same number of transitions per bit, otherwise, the line code is dependent on the bit sequence and, therefore, bit-sequence dependent.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "40799", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=40799", "title": "Bit slip", "text": "Loss or gain of bits in digital transmission\nIn digital transmission, bit slip is the loss or gain of a bit or bits, caused by clock drift\u00a0\u2013 variations in the respective clock rates of the transmitting and receiving devices.\nOne cause of bit slip is overflow of a receive buffer that occurs when the transmitter's clock rate exceeds that of the receiver. This causes one or more bits to be dropped for lack of storage capacity.\nOne way to maintain timing between transmitting and receiving devices is to employ an asynchronous protocol such as start-stop. Alternatively, bit slip can be prevented by using a self-clocking signal (such as a signal modulated using OQPSK) or using a line coding such as Manchester encoding.\nAnother cause is \"losing count\", as on a hard drive: if a hard drive encounters a long string of 0s, without any 1s (or a string of 1s without 0s), it may lose track of the frame between fields, and suffer bit slip. When a pulse of N consecutive zero bits are sent, clock drift may cause the hardware to apparently detect N-1 zero bits or N+1 zero bits\u00a0\u2013 both kinds of errors are called bit slip. Thus one prevents long strings without change via such devices as run length limited codes.\nMany communication systems use linear-feedback shift register scrambling to prevent long strings of 0s (or other symbol),\nincluding VSAT, 1000BASE-T, , etc.\nWhile a scrambler makes the \"losing count\" type of bit slip error occur far less often,\nwhen bit slip errors do occur (perhaps for other reasons), \nscramblers have the property of expanding small errors that add or lose a single bit into a much longer burst of errors.\nThe optimized cipher feedback mode (OCFB), the statistical self-synchronization mode, and the \"one-bit CFB mode\" also expand small bit-slip errors into a longer burst of errors, but eventually recover and produce the correct decrypted plaintext.\nA bit-slip error when using any other block cipher mode of operation generally results in complete corruption of the rest of the message.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "40800", "revid": "11292982", "url": "https://en.wikipedia.org/wiki?curid=40800", "title": "Bits per second", "text": ""}
{"id": "40801", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=40801", "title": "Bit-stream transmission", "text": "In telecommunications, the term bit-stream transmission has the following meanings:\n1. In bit-oriented systems, the transmission of bit strings.\n2. In character-oriented systems, the transmission of bit streams that represent characters.\nIn bit-stream transmission, the bits usually occur at fixed time intervals, start and stop signals are not used, and the bit patterns follow each other in sequence without interruption.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "40802", "revid": "50179043", "url": "https://en.wikipedia.org/wiki?curid=40802", "title": "Bit stuffing", "text": "Practice in data transfers\nIn data transmission and telecommunications, bit stuffing (also known\u2014uncommonly\u2014as positive justification) is the insertion of non-information bits into data. Stuffed bits should not be confused with overhead bits.\nBit stuffing refers to adding extra bits that do not contain any actual information into the data stream. These stuffed bits are different from overhead bits. Overhead bits are also non-data bits, but they are required for transmission, such as those used in headers, checksums, and other control information.\nBit stuffing is used for various purposes, such as for bringing bit streams that do not necessarily have the same or rationally related bit rates up to a common rate, or to fill buffers or frames. The location of the stuffing bits is communicated to the receiving end of the data link, where these extra bits are removed to return the bit streams to their original bit rates or form. Bit stuffing may be used to synchronize several channels before multiplexing or to rate-match two single channels to each other.\nAnother use of bit stuffing is for run length limited coding: to limit the number of consecutive bits of the same value in the data to be transmitted. A bit of the opposite value is inserted after the maximum allowed number of consecutive bits. Since this is a general rule the receiver doesn't need extra information about the location of the stuffing bits in order to do the de-stuffing. This is done to create additional signal transitions to ensure reliable reception or to escape special reserved code words such as frame sync sequences when the data happens to contain them. This technique is particularly useful in situations where data frames are transmitted over unreliable channels, such as wireless networks or noisy copper wires.\nBit stuffing does not ensure that the payload is intact (\"i.e.\" not corrupted by transmission errors); it is merely a way of attempting to ensure that the transmission starts and ends at the correct places. Error detection and correction techniques are used to check the frame for corruption after its delivery and, if necessary, the frame will be re-sent.\nZero-bit insertion.\nThe NRZI coding scheme transmits a 0 bit as a signal transition, and a 1 bit as no change. In this case, bit stuffing is most easily described as the insertion of a 0 bit after a long run of 1 bits.\nIt was popularized by IBM's SDLC (later renamed HDLC), and is also used in Low- and full-speed USB. \nAfter a long sequence of 1 bits there would be no transitions in the transmitted data, and it would be possible for the transmitter and receiver clocks to lose synchronisation. By inserting a 0 after five (SDLC) or six (USB) consecutive 1 bits the transmitter guarantees a maximum of six (SDLC) or seven (USB) bit times between transitions. The receiver can synchronise its clock against the transitions to ensure proper data recovery.\nIn SDLC the transmitted bit sequence \"01111110\" containing six adjacent 1 bits is the Flag byte. Bit stuffing ensures that this pattern can never occur in normal data, so it can be used as a marker for the beginning and end of the frame without any possibility of being confused with normal data.\nThe main disadvantage of bit-stuffing is that the code rate is unpredictable; it depends on the data being transmitted.\n\"Source: from Federal Standard 1037C in support of MIL-STD-188\"\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "40803", "revid": "50171615", "url": "https://en.wikipedia.org/wiki?curid=40803", "title": "Bit-synchronous operation", "text": "Digital communication using a clock-synchronized bit stream\nBit-synchronous operation is a type of digital communication in which the data circuit-terminating equipment (DCE), data terminal equipment (DTE), and transmitting circuits are all operated in bit synchronism with a clock signal.\nIn bit-synchronous operation, clock timing is usually delivered at twice the modulation rate, and one bit is transmitted or received during each clock cycle.\nBit-synchronous operation is sometimes erroneously referred to as digital synchronization.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "40805", "revid": "44106950", "url": "https://en.wikipedia.org/wiki?curid=40805", "title": "Black noise", "text": ""}
{"id": "40807", "revid": "1221525481", "url": "https://en.wikipedia.org/wiki?curid=40807", "title": "Blind transmission", "text": "Telecommunications transmission without receipt\nA blind transmission, in telecommunications, is a transmission made without obtaining a receipt, or acknowledgment of reception, from the intended receiving station. Blind transmissions may occur or be necessary when security constraints, such as radio silence, are imposed, when technical difficulties with a sender's receiver or receiver's transmitter occur, or when lack of time precludes the delay caused by waiting for receipts.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "40808", "revid": "50789340", "url": "https://en.wikipedia.org/wiki?curid=40808", "title": "Block", "text": "Block or blocked may refer to:\n&lt;templatestyles src=\"Template:TOC_right/styles.css\" /&gt;\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "40809", "revid": "28481209", "url": "https://en.wikipedia.org/wiki?curid=40809", "title": "Block check character", "text": "In telecommunications, a block check character (BCC) is a character added to a transmission block to facilitate error detection.\nIn longitudinal redundancy checking and cyclic redundancy checking, block check characters are computed for, and added to, each message block transmitted. This block check character is compared with a second block check character computed by the receiver to determine whether the transmission is error free.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "40810", "revid": "44062", "url": "https://en.wikipedia.org/wiki?curid=40810", "title": "Blocking", "text": "Blocking may refer to:\n&lt;templatestyles src=\"Template:TOC_right/styles.css\" /&gt;\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "40811", "revid": "252195", "url": "https://en.wikipedia.org/wiki?curid=40811", "title": "Block transfer attempt", "text": ""}
{"id": "40812", "revid": "40893", "url": "https://en.wikipedia.org/wiki?curid=40812", "title": "Bonding", "text": "Bonding may refer to:\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "40813", "revid": "22041646", "url": "https://en.wikipedia.org/wiki?curid=40813", "title": "Bootstrap", "text": ""}
{"id": "40814", "revid": "44318890", "url": "https://en.wikipedia.org/wiki?curid=40814", "title": "Branch", "text": "Structural part of trees and plants\nA branch, also called a ramus in botany, is a stem that grows off from another stem, or when structures like veins in leaves are divided into smaller veins.\nHistory and etymology.\nIn Old English, there are numerous words for branch, including , , , and .\nThere are also numerous descriptive words, such as (that is, something that has bled, or 'bloomed', out), (literally 'little bough'), (literally 'on growth'), and (literally 'offspringing'). Numerous other words for twigs and boughs abound, including , which still survives as the \"-toe\" in \"mistletoe\".\nLatin words for branch are or . The latter term is an affix found in other modern words such as \"cladodont\" (prehistoric sharks with branched teeth), \"cladode\" (flattened leaf-like branches), or \"cladogram\" (a branched diagram showing relations among organisms).\nWoody branches.\nLarge branches are known as boughs and small branches are known as twigs. The term \"twig\" usually refers to a terminus, while \"bough\" refers only to branches coming directly from the trunk.\nDue to a broad range of species of trees, branches and twigs can be found in many different shapes and sizes. While branches can be nearly horizontal, vertical, or diagonal, the majority of trees have upwardly diagonal branches. A number of mathematical properties are associated with tree branchings; they are natural examples of fractal patterns in nature, and, as observed by Leonardo da Vinci, their cross-sectional areas closely follow the da Vinci branching rule.\nSpecific terms.\nA bough can also be called a limb or arm, and though these are arguably metaphors, both are widely accepted synonyms for bough. A crotch or fork is an area where a trunk splits into two or more boughs. A twig is frequently referred to as a sprig as well, especially when it has been plucked. Other words for twig include branchlet, spray, and surcle, as well as the technical terms surculus and ramulus. Branches found under larger branches can be called underbranches.\nSome branches from specific trees have their own names, such as osiers and withes or withies, which come from willows. Often trees have certain words which, in English, are naturally collocated, such as holly and mistletoe, which usually employ the phrase \"sprig of\" (as in, a \"sprig of mistletoe\"). Similarly, the branch of a cherry tree is generally referred to as a \"cherry branch\", while other such formations (i.e., \"acacia branch\" or \"orange branch\") carry no such alliance. A good example of this versatility is oak, which could be referred to as variously an \"oak branch\", an \"oaken branch\", a \"branch of oak\", or the \"branch of an oak tree\".\nOnce a branch has been cut or in any other way removed from its source, it is most commonly referred to as a stick, and a stick employed for some purpose (such as walking, spanking, or beating) is often called a rod. Thin, flexible sticks are called switches, wands, shrags, or vimina (singular vimen).\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "40815", "revid": "45235143", "url": "https://en.wikipedia.org/wiki?curid=40815", "title": "Brewster's angle", "text": "Angle of incidence for which all reflected light will be polarized\nBrewster's angle (also known as the polarization angle) is the angle of incidence at which light with a particular polarization is perfectly transmitted through a transparent dielectric surface, with \"no reflection\". When \"unpolarized\" light is incident at this angle, the light that is reflected from the surface is perfectly polarized. The angle is named after the Scottish physicist Sir David Brewster (1781\u20131868).\nExplanation.\nWhen light encounters a boundary between two media with different refractive indices, some of it is usually reflected as shown in the figure above. The fraction that is reflected is described by the Fresnel equations, and depends on the incoming light's polarization and angle of incidence.\nThe Fresnel equations predict that light with the \"p\" polarization (electric field polarized in the same plane as the incident ray and the surface normal at the point of incidence) will not be reflected if the angle of incidence is \nformula_1\nwhere \"n\"1 is the refractive index of the initial medium through which the light propagates (the \"incident medium\"), and \"n\"2 is the index of the other medium. This equation is known as Brewster's law, and the angle defined by it is Brewster's angle.\nThe physical mechanism for this can be qualitatively understood from the manner in which electric dipoles in the media respond to \"p\"-polarized light. One can imagine that light incident on the surface is absorbed, and then re-radiated by oscillating electric dipoles at the interface between the two media. The polarization of freely propagating light is always perpendicular to the direction in which the light is travelling. The dipoles that produce the transmitted (refracted) light oscillate in the polarization direction of that light. These same oscillating dipoles also generate the reflected light. However, dipoles do not radiate any energy in the direction of the dipole moment. If the refracted light is \"p\"-polarized and propagates exactly perpendicular to the direction in which the light is predicted to be specularly reflected, the dipoles point along the specular reflection direction and therefore no light can be reflected. (See diagram, above)\nWith simple geometry this condition can be expressed as\nformula_2\nwhere \"\u03b8\"1 is the angle of reflection (or incidence) and \"\u03b8\"2 is the angle of refraction.\nUsing Snell's law,\nformula_3\none can calculate the incident angle \"\u03b8\"1 = \"\u03b8\"B at which no light is reflected:\nformula_4\nSolving for \"\u03b8\"B gives\nformula_5\nThe physical explanation of why the transmitted ray should be at formula_6 to the reflected ray can be difficult to grasp, but the Brewster angle result also follows simply from the Fresnel equations for reflectivity, which state that for p-polarized light\nformula_7\nThe reflection goes to zero when \nformula_8\nWe can now use Snell's Law to eliminate formula_9 as follows: we multiply Snell by formula_10 and square both sides; multiply the zero-reflection condition just obtained by formula_11 and square both sides; and add the equations. This produces\nformula_12\nWe finally divide both sides by formula_13, collect terms and rearrange to produce formula_14, from which the desired result follows (which then allows reverse proof that formula_15).\nFor a glass medium (\"n\"2 \u2248 1.5) in air (\"n\"1 \u2248 1), Brewster's angle for visible light is approximately 56\u00b0, while for an air-water interface (\"n\"2 \u2248 1.33), it is approximately 53\u00b0. Since the refractive index for a given medium changes depending on the wavelength of light, Brewster's angle will also vary with wavelength.\nThe phenomenon of light being polarized by reflection from a surface at a particular angle was first observed by \u00c9tienne-Louis Malus in 1808. He attempted to relate the polarizing angle to the refractive index of the material, but was frustrated by the inconsistent quality of glasses available at that time. In 1815, Brewster experimented with higher-quality materials and showed that this angle was a function of the refractive index, defining Brewster's law.\nBrewster's angle is often referred to as the \"polarizing angle\", because light that reflects from a surface at this angle is entirely polarized perpendicular to the plane of incidence (\"\"s\"-polarized\"). A glass plate or a stack of plates placed at Brewster's angle in a light beam can, thus, be used as a polarizer. The concept of a polarizing angle can be extended to the concept of a Brewster wavenumber to cover planar interfaces between two linear bianisotropic materials. In the case of reflection at Brewster's angle, the reflected and refracted rays are mutually perpendicular.\nFor magnetic materials, Brewster's angle can exist for only one of the incident wave polarizations, as determined by the relative strengths of the dielectric permittivity and magnetic permeability. This has implications for the existence of generalized Brewster angles for dielectric metasurfaces.\nApplications.\nWhile at the Brewster angle there is \"no\" reflection of the \"p\" polarization, at yet greater angles the reflection coefficient of the \"p\" polarization is always less than that of the \"s\" polarization, almost up to 90\u00b0 incidence where the reflectivity of each rises towards unity. Thus reflected light from horizontal surfaces (such as the surface of a road) at a distance much greater than one's height (so that the incidence angle of specularly reflected light is near, or usually well beyond the Brewster angle) is strongly \"s\"-polarized. Polarized sunglasses use a sheet of polarizing material to block horizontally-polarized light and thus reduce glare in such situations. These are most effective with smooth surfaces where specular reflection (thus from light whose angle of incidence is the same as the angle of reflection defined by the angle observed from) is dominant, but even diffuse reflections from roads for instance, are also significantly reduced.\nPhotographers also use polarizing filters to remove reflections from water so that they can photograph objects beneath the surface. Using a polarizing camera attachment which can be rotated, such a filter can be adjusted to reduce reflections from objects other than horizontal surfaces, such as seen in the accompanying photograph (right) where the \"s\" polarization (approximately vertical) has been eliminated using such a filter.\nWhen recording a classical hologram, the bright reference beam is typically arranged to strike the film in the \"p\" polarization at Brewster's angle. By thus eliminating reflection of the reference beam at the transparent back surface of the holographic film, unwanted interference effects in the resulting hologram are avoided.\nEntrance windows or prisms with their surfaces at the Brewster angle are commonly used in optics and laser physics in particular. The polarized laser light enters the prism at Brewster's angle without any reflective losses.\nIn surface science, Brewster angle microscopes are used to image layers of particles or molecules at air-liquid interfaces. Using illumination by a laser at Brewster's angle to the interface and observation at the angle of reflection, the uniform liquid does not reflect, appearing black in the image. However any molecular layers or artifacts at the surface, whose refractive index or physical structure contrasts with the liquid, allows for some reflection against that black background which is captured by a camera.\nBrewster windows.\nGas lasers using an external cavity (reflection by one or both mirrors \"outside\" the gain medium) generally seal the tube using windows tilted at Brewster's angle. This prevents light in the intended polarization from being lost through reflection (and reducing the round-trip gain of the laser) which is critical in lasers having a low round-trip gain. On the other hand, it \"does\" remove \"s\" polarized light, increasing the round trip loss for that polarization, and ensuring the laser only oscillates in one linear polarization, as is usually desired. And many sealed-tube lasers (which do not even need windows) have a glass plate inserted within the tube at the Brewster angle, simply for the purpose of allowing lasing in only one polarization.\nPseudo-Brewster's angle.\nWhen the reflecting surface is absorbing, reflectivity at parallel polarization (\"p\") goes through a non-zero minimum at the so-called pseudo-Brewster's angle.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "40816", "revid": "17698045", "url": "https://en.wikipedia.org/wiki?curid=40816", "title": "Bridge-to-bridge station", "text": ""}
{"id": "40817", "revid": "33839581", "url": "https://en.wikipedia.org/wiki?curid=40817", "title": "Bridging loss", "text": "Bridging loss is the loss, at a given frequency, that results when an impedance is connected across a transmission line. It is expressed as the ratio, in decibels, of the signal power delivered to a given point in a system downstream from the bridging point prior to bridging, to the signal power delivered to the given point after bridging. The term is introduced because return loss is not applicable to the high-impedance input conditions. The term is also used in telephone practice and is synonymous with the insertion loss that results from bridging an impedance across a circuit.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nSource: from Federal Standard 1037C and from MIL-STD-188"}
{"id": "40818", "revid": "1305866960", "url": "https://en.wikipedia.org/wiki?curid=40818", "title": "Brightness", "text": "Perception of light level\nBrightness is an attribute of visual perception in which a source appears to be radiating/reflecting light. In other words, brightness is the perception dictated by the luminance of a visual target. The perception is not linear to luminance, and relies on the context of the viewing environment (for example, see White's illusion).\nBrightness is a subjective sensation of an object being observed and one of the color appearance parameters of many color appearance models, typically denoted as formula_1. Brightness refers to how much light \"appears to shine\" from something. This is a different perception than lightness, which is how light something appears \"compared to\" a similarly lit white object.\nThe adjective \"bright\" derives from an Old English \"beorht\" with the same meaning via metathesis giving Middle English \"briht\". The word is from a Proto-Germanic , ultimately from a PIE root with a closely related meaning, \"white, bright\".\n\"Brightness\" was formerly used as a synonym for the photometric term \"luminance\" and (incorrectly) for the radiometric term \"radiance\". As defined by the US \"Federal Glossary of Telecommunication Terms\" (FS-1037C), \"brightness\" should now be used only for non-quantitative references to physiological sensations and perceptions of light.\nBrightness is an antonym of \"dimness\" or \"dullness\".\nWith regard to stars, brightness is quantified as apparent magnitude and absolute magnitude.\nTwo pictograms resembling the Sun with rays are used to represent the settings of luminance in display devices. They have been encoded in Unicode since version\u00a06.0 (October 2010) in the Miscellaneous Symbols and Pictographs block under U+1505 as \"low brightness symbol\" (\ud83d\udd05) and U+1F506 as \"high brightness symbol\" (\ud83d\udd06).\nThe United States Federal Trade Commission (FTC) has assigned an unconventional meaning to brightness when applied to lamps. When appearing on light bulb packages, brightness means luminous flux, while in other contexts it means luminance. Luminous flux is the total amount of light coming from a source, such as a lighting device. Luminance, the original meaning of brightness, is the amount of light per solid angle coming from an area, such as the sky. The table below shows the standard ways of indicating the amount of light.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nExternal links.\n&lt;templatestyles src=\"Sister-inline/styles.css\"/&gt; Media related to at Wikimedia Commons"}
{"id": "40819", "revid": "247726985", "url": "https://en.wikipedia.org/wiki?curid=40819", "title": "B6ZS", "text": ""}
{"id": "40820", "revid": "247727227", "url": "https://en.wikipedia.org/wiki?curid=40820", "title": "B3ZS", "text": ""}
{"id": "40821", "revid": "33839581", "url": "https://en.wikipedia.org/wiki?curid=40821", "title": "Buffer", "text": "Buffer may refer to:\n&lt;templatestyles src=\"Template:TOC_right/styles.css\" /&gt;\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "40822", "revid": "48588462", "url": "https://en.wikipedia.org/wiki?curid=40822", "title": "Burst switching", "text": "Technique in packet-switched networks\nIn a packet switched network, burst switching is a capability in which each network switch extracts routing instructions from an incoming packet header to establish and maintain the appropriate switch connection for the duration of the packet, following which the connection is automatically released. \nIn concept, burst switching is similar to connectionless mode transmission, but differs in that burst switching implies an intent to establish the switch connection in near real time, so that only minimum buffering is required at the node switch.\nA variant of burst switching used in optical networks is optical burst switching.\nApplications.\nExamples of technology using burst switching include:\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "40823", "revid": "4145", "url": "https://en.wikipedia.org/wiki?curid=40823", "title": "Burst transmission", "text": "Technique in telecommunications\nIn telecommunications, a burst transmission or data burst is the broadcast of a relatively high-bandwidth transmission over a short period. \nBurst transmission can be intentional, broadcasting a compressed message at a very high data signaling rate within a very short transmission time.\nIn the 1980s, the term \"data burst\" (and \"info burst\") was used for a technique used by some United Kingdom and South African TV programmes to transmit large amounts of primarily textual information. They would display multiple pages of text in rapid succession, usually at the end of the programme; viewers would videotape it and then read it later by playing it back using the pause button after each page.\nData bursts can occur naturally, such as when the download of data from the internet briefly experiences higher speeds. It can also occur in a computer network where data transmission is interrupted at intervals. Burst transmission enables communications between data terminal equipment (DTEs) and a data network operating at dissimilar data signaling rates.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "40824", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=40824", "title": "Busy hour", "text": ""}
{"id": "40825", "revid": "327592", "url": "https://en.wikipedia.org/wiki?curid=40825", "title": "Busy signal (disambiguation)", "text": "A busy signal is information communicated to a user or apparatus attempting a connection, indicating the requested connection cannot be completed.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "40827", "revid": "1286781637", "url": "https://en.wikipedia.org/wiki?curid=40827", "title": "Bypass", "text": "Bypass may refer to:\n&lt;templatestyles src=\"Template:TOC_right/styles.css\" /&gt;\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "40828", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=40828", "title": "Cable television relay service station", "text": "Retransmission facility\nIn telecommunications, a cable television relay service station (CARS) is a fixed or mobile station used for the transmission of television and related audio signals, signals of standard and FM broadcast stations, signals of instructional television fixed stations, and cablecasting from the point of reception to a terminal point from which the signals are distributed to the public.\nSource: from Federal Standard 1037C and from the Code of Federal Regulations, Telecommunications Parts 0-199"}
{"id": "40829", "revid": "48998989", "url": "https://en.wikipedia.org/wiki?curid=40829", "title": "Call", "text": "Call or Calls may refer to:\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "40830", "revid": "42342156", "url": "https://en.wikipedia.org/wiki?curid=40830", "title": "Call collision", "text": "In the field of telecommunications, a call collision, commonly referred to as a glare, can occur in two situations:\nGlare can sometimes be experienced when attempting to make an outgoing call on a private branch exchange (PBX) system but getting connected to an incoming call instead. This occurrence can also happen in residential settings if an outgoing call is initiated at the precise moment when an incoming call is about to start ringing.\nTo mitigate the risk of glare, Multi-line hunting techniques are employed. These techniques involve selecting circuits in the opposite preference order, ensuring that the highest numbered line, which is typically the last choice for incoming calls, becomes the first choice for outgoing calls. The following example illustrates the sequence:\nWith PRI circuits, the channel selection sequence is specified when the circuit is provisioned. Common practice is to have the PBX use descending channel selection, and the carrier to use ascending. Glare is not common on PRI circuits because the signaling is so fast, however it is not impossible (especially if there are subtle differences in the timers at either end, and the circuit is being used at near-capacity). The users will not experience a connection to an unexpected call (as would be the case with analog circuits), because glare causes protocol errors that generally prevent any sort of successful connection. Instead, one or both of the call attempts might fail, and ideally an error would appear in the logs (this depends on the logging capabilities of the systems at either end of the circuit). Glare is quite rare on PRI circuits, and can be difficult to troubleshoot.\nFor old, analog PBX trunks, glare can be reduced by using ground start signaling, which offers a better answer and disconnect supervision. IE: Nortel BSP discouraged using loop start trunks for this and other reasons. Long Distance exchanges in the 1950s and 60s incorporated Glare Detectors to alleviate the problem.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "40832", "revid": "41294494", "url": "https://en.wikipedia.org/wiki?curid=40832", "title": "Call duration", "text": "In telecommunications, the term call duration has the following meanings: \n&lt;templatestyles src=\"Citation/styles.css\"/&gt;\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\nIndex of articles associated with the same name\nThis includes a list of related items that share the same name (or similar names). &lt;br&gt; If an [ internal link] incorrectly led you here, you may wish to change the link to point directly to the intended article."}
{"id": "40833", "revid": "583020", "url": "https://en.wikipedia.org/wiki?curid=40833", "title": "Called-party camp-on", "text": ""}
{"id": "40836", "revid": "27823944", "url": "https://en.wikipedia.org/wiki?curid=40836", "title": "Call processing", "text": "In telecommunications, the term call processing has the following meanings: \n\"Volume Call Processing\" is the handling of calls when there are far more incoming calls than can be answered by an individual or group of attendants."}
{"id": "40837", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=40837", "title": "Call-second", "text": "In telecommunications, a call-second is a unit used to measure communications traffic density, equivalent to one call with a duration of one second. \nTraffic is measured independent of users. For example, one user making two 75-second calls is equivalent to two users each making one 75-second call, as each case produces 150 call-seconds of traffic. \nA CCS (centacall-second) is often used to describe 100 call-seconds, so 3600 call-seconds = 36 CCS = 1 call-hour. \nIn a communication network, a trunk (link) can carry numerous concurrent calls by means of multiplexing. Hence a particular number of call-seconds can be carried in infinitely many ways as calls are established and cleared over time. For example, one call-hour could be one call for an hour or two (possibly concurrent) calls for half an hour each. Call-seconds give a measure of the average number of concurrent calls.\nOffered load is defined as the traffic density per unit time, measured in erlangs. An erlang is defined as one call-hour per hour, or 3,600 call-seconds per hour.\nHence, if one CCS is measured over a one-hour period, the offered load is 1/36 erlangs."}
{"id": "40838", "revid": "6289403", "url": "https://en.wikipedia.org/wiki?curid=40838", "title": "Call set-up time", "text": ""}
{"id": "40839", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=40839", "title": "Call-sign allocation plan", "text": "Table of allocation of international call sign series\nIn telecommunications, call-sign allocation plan is the table of allocation of international call sign series contained in the current edition of the \"International Telecommunication Union (ITU) \"Radio Regulations\".\"\n\"Note:\" In the table of allocation, the first two characters of each call sign (whether two letters or one number and one letter, in that order) identify the nationality of the station. In certain instances where the complete alphabetical block is allocated to a single nation, the first letter is sufficient for national identity. Individual assignments are made by appropriate national assignment authorities from the national allocation.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "40840", "revid": "10841396", "url": "https://en.wikipedia.org/wiki?curid=40840", "title": "Call tracing", "text": "In telecommunications, call tracing is a procedure that permits an entitled user to be informed about the routing of data for an established connection, identifying the entire route from the origin to the destination.\nThere are two types of call tracing. Permanent call tracing permits tracing of all calls. On-demand call tracing permits tracing, upon request, of a specific call, provided that the called party dials a designated code immediately after the call to be traced is disconnected.\nReferences.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "40841", "revid": "37290531", "url": "https://en.wikipedia.org/wiki?curid=40841", "title": "Camp-on busy signal", "text": "Telecommunications term\nIn telecommunications, the term camp-on busy signal has the following meanings: \n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "40842", "revid": "49117425", "url": "https://en.wikipedia.org/wiki?curid=40842", "title": "Cancel character", "text": "Either of two control codes used to delete or rescind preceding data or characters\nIn telecommunications and character encoding, the term cancel character refers to a control character which may be either of: \nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "40843", "revid": "3125232", "url": "https://en.wikipedia.org/wiki?curid=40843", "title": "Capacitive coupling", "text": "Transfer of energy between circuits\nCapacitive coupling is the transfer of energy within an electrical network or between distant networks by means of displacement current between circuit(s)nodes, induced by the electric field. This coupling can have an intentional or accidental effect.\nIn its simplest implementation, capacitive coupling is achieved by placing a capacitor between two nodes. Where analysis of many points in a circuit is carried out, the capacitance at each point and between points can be described in a matrix form.\nUse in analog circuits.\nIn analog circuits, a coupling capacitor is used to connect two circuits such that only the AC signal from the first circuit can pass through to the next while DC is blocked. This technique helps to isolate the DC bias settings of the two coupled circuits. Capacitive coupling is also known as \"AC coupling\" and the capacitor used for the purpose is also known as a \"DC-blocking capacitor\".\nA coupling capacitor's ability to prevent a DC load from interfering with an AC source is particularly useful in Class A amplifier circuits by preventing a 0 volt input being passed to a transistor with additional resistor biasing; creating continuous amplification.\nCapacitive coupling decreases the low frequency gain of a system containing capacitively coupled units. Each coupling capacitor along with the input electrical impedance of the next stage forms a high-pass filter and the sequence of filters results in a cumulative filter with a cutoff frequency that may be higher than those of each individual filter.\nCoupling capacitors can also introduce nonlinear distortion at low frequencies. This is not an issue at high frequencies because the voltage across the capacitor stays very close to zero. However, if a signal passing through the coupling capacitance has a frequency that is low relative to the RC cutoff frequency, voltages can develop across the capacitor, which for some capacitor types results in changes of capacitance, leading to distortion. This is avoided by choosing capacitor types that have low \"voltage coefficient\", and by using large values that put the cutoff frequency far lower than the frequencies of the signal.\nUse in digital circuits.\nAC coupling is also widely used in digital circuits to transmit digital signals with a zero DC component, known as DC-balanced signals. DC-balanced waveforms are useful in communications systems, since they can be used over AC-coupled electrical connections to avoid voltage imbalance problems and charge accumulation between connected systems or components.\nFor this reason, most modern line codes are designed to produce DC-balanced waveforms. The most common classes of DC-balanced line codes are constant-weight codes and paired-disparity codes.\nGimmick loop.\nAgimmick loop is a simple type of capacitive coupler: two closely spaced strands of wire. It provides capacitive coupling of a few picofarads between two nodes. Usually the wires are twisted together.\nParasitic capacitive coupling.\nCapacitive coupling is often unintended, such as the capacitance between two wires or PCB traces that are next to each other. One signal may capacitively couple with another and cause what appears to be noise. To reduce coupling, wires or traces are often separated as much as possible, or ground lines or ground planes are run in between signals that might affect each other, so that the lines capacitively couple to ground rather than each other. Prototypes of high-frequency (tens of megahertz) or high-gain analog circuits often use circuits that are built over a ground plane to control unwanted coupling. If a high-gain amplifier's output capacitively couples to its input it may become an electronic oscillator.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "40844", "revid": "1309155304", "url": "https://en.wikipedia.org/wiki?curid=40844", "title": "Capture effect", "text": "FM radio reception phenomenon\nIn a radio receiver, the capture effect is a phenomenon associated with reception in which only the stronger of two or more signals received within the bandwidth of the receiver passband will be demodulated. The Capture effect therefore enables frequency reuse of the same frequency by imposing a sufficient distance separation, e.g. used in AM communication in the AM(R)S (Aeronautical mobile (R) service), or between FM-BC transmitter for the capture take effect. Alternatively the capture effect enables two frequency ILS-Localizer (ILS-LOC) and ILS-Glide-Path (ILS-GP) to operate at airports in presence of strong reflections, e.g. due to terrain and buildings. \nFM phenomenon.\nThe capture effect is defined as the complete suppression of the weaker signal at the receiver's limiter (if present) where the weaker signal is not amplified, but attenuated. When both signals are nearly equal in strength or are fading independently, the receiver may rapidly switch from one to another and exhibit flutter.\nThe capture effect can occur at the signal limiter, or in the demodulation stage for circuits that do not require a signal limiter. Some types of radio receiver circuits have a stronger capture effect than others. The measurement of how well a receiver rejects a second signal on the same frequency is called its capture ratio. It is measured as the lowest ratio of the power of two signals that will result in the suppression of the weaker signal. \nThe capture effect phenomenon was first documented in 1938 by General Electric engineers conducting test transmissions. Two experimental FM stations, located 15 miles (24 km) apart in Albany and Schenectady, New York, were configured to transmit on the same frequency, in order to study how this would affect reception. It was determined that, for most of the path between the two stations, only one of the signals could be heard, with the complete elimination of the other. It was concluded that this effect occurred whenever the stronger signal was about twice as strong as the weaker one. This was significantly different than the case with amplitude modulation signals, where the general standard for broadcasting stations was that to avoid objectionable interference the stronger signal had to be about twenty times that of the weaker one. The capture effect thus allowed co-channel FM broadcasting stations to be located somewhat closer to each other than AM ones, without causing mutual interference.\nAM capture effect use.\nWhen AM (Amplitude Modulation) transmitter share the same center frequency, the weaker signal will introduce distortions in form of beat frequencies or both signals interfere completely with each other. If only a carrier devoid of modulation is received or added in a receiver, e.g. using a BFO (Beat Frequency Oscillator), a tone with the frequency offset between the two carrier frequencies will be heard. \nBy introducing a frequency offset between the carrier frequencies in the magnitude of at least the sum of the highest modulation frequencies employed by both transmitter, will eliminate generation of beat frequencies, e.g. highest modulation frequency is 2.4 kHz therefore the min. frequency offset between both carrier is 4.8 kHz (=2 x 2.4 kHz). \nThe capture effect is actively employed in Europe for aeronautical VHF-communication in the band 118 MHz to 137 MHz to provide coverage for aircraft flying under ATC (Air Traffic Control) in large ATC sectors that cannot be covered by a single transmitter site. For ATC large sectors up to five transmitter sites are required to provide continuous coverage within the area of a large ATC sector. Transmitter are strategically placed therefore ensuring that not all transmitter will be received at any point in space simultaneously.\nOperation of instrument landing system localizer (ILS-LLZ) and instrument landing system glide path (ILS-GP) at airports is often impossible due to strong reflections, e.g. on terrain and buildings. The solution lead to the development of two-frequency ILS-LOC and ILS-GP. Two frequency ILS systems use an additional carrier frequencies with a frequency offset of at least 5 kHz and max. 14 kHz, both signals centered above and below the nominal center frequency of a 50 kHz wide ILS-LLZ-channel. No. 3.1.3.2.1 Both signals are radiated with two independent antenna patterns that overlap only in part. The course signal of an ILS-LLZ has a higher EIRP to provide a range of up to 25 NM and a lower EIRP for the clearance of an ILS-Localizer signal to provide a range between 10 NM to 17 NM. For difficult terrain and weather impact 2f ILS-GP are employed. Two frequency ILS-LOC and ILS-GP are an ICAO standard precision approach system and are standardized for worldwide use in ICAO Annex 10. \nAM receiver will demodulate any carriers and their side bands that are strong enough for demodulation within the receiver passband resulting in an audio mix, if their carrier frequencies are not separated by the sum of the highest modulation frequencies of employed in both transmitter. If only a carrier devoid of modulation is received or added in a receiver, e.g. using a BFO (Beat Frequency Oscillator) the resulting tone is the frequency offset between the two carrier frequencies. \nDigital modulation.\nFor digital modulation schemes it has been shown that for properly implemented on-off keying/amplitude-shift keying systems, co-channel rejection can be better than for frequency-shift keying systems.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "40845", "revid": "42056547", "url": "https://en.wikipedia.org/wiki?curid=40845", "title": "Carrier", "text": "Carrier may refer to:\n&lt;templatestyles src=\"Template:TOC_right/styles.css\" /&gt;\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "40846", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=40846", "title": "Carrier-sense multiple access with collision avoidance", "text": "Computer network multiple access method\nCarrier-sense multiple access with collision avoidance (CSMA/CA) in computer networking, is a link layer multiple access method in which carrier sensing is used. Under CSMA/CA, nodes attempt to avoid collisions by beginning transmission only after the channel is sensed to have no traffic. When they do transmit, nodes transmit frames in their entirety.\nThis technique is primarily used in wireless networks, where the alternative with collision detection CSMA/CD is not possible due to wireless transmitters de-sensing (turning off) their receivers during packet transmission.\nCSMA/CA is unreliable due to the hidden node problem.\nDetails.\nCollision avoidance is used to improve the performance of the CSMA method by attempting to divide the channel somewhat equally among all transmitting nodes within the collision domain.\n* Request to Send/Clear to Send (RTS/CTS) may optionally be used at this point to mediate access to the shared medium. This goes some way to alleviating the problem of hidden nodes because, for instance, in a wireless network, the Access Point only issues a \"Clear to Send\" to one node at a time. However, wireless 802.11 implementations do not typically implement RTS/CTS for all transmissions; they may turn it off completely, or at least not use it for small packets (the overhead of RTS, CTS and transmission is too great for small data transfers).\n* Transmission: if the medium was identified as being clear \"or\" the node received a CTS to explicitly indicate it can send, it sends the frame in its entirety. Unlike CSMA/CD, it is very challenging for a wireless node to listen at the same time as it transmits (its transmission will dwarf any attempt to listen). Continuing the wireless example, the node awaits receipt of an acknowledgement packet from the Access Point to indicate the packet was received and checksummed correctly. If such acknowledgement does not arrive in a timely manner, it assumes the packet collided with some other transmission, causing the node to enter a period of binary exponential backoff prior to attempting to re-transmit.\nAlthough CSMA/CA has been used in a variety of wired communication systems, it is particularly beneficial in a wireless LAN due to a common problem of multiple stations being able to see the Access Point, but not each other. This is due to differences in transmit power, and receive sensitivity, as well as distance, and location with respect to the AP. This will cause a station to not be able to 'hear' another station's broadcast. This is the so-called 'hidden node', or 'hidden station' problem. Devices utilizing 802.11 based standards can enjoy the benefits of collision avoidance (RTS / CTS handshake, also Point coordination function), although they do not do so by default. By default they use a Carrier sensing mechanism called \"exponential backoff\" (or Distributed coordination function), that relies upon a station attempting to 'listen' for another station's broadcast before sending. CA, or PCF relies upon the AP (or the 'receiver' for Ad hoc networks) granting a station the exclusive right to transmit for a given period of time after requesting it (Request to Send / Clear to Send).\nCSMA-CA requires a determination of whether a channel is 'idle', even when incompatible standards and overlapping transmission frequencies are used. Per the standards, for 802.11/Wi-Fi transmitters on the same channel, transmitters must take turns to transmit if they can detect each other even 3\u00a0dB above the noise floor (the thermal noise floor is around -101\u00a0dBm for 20\u00a0MHz channels). On the other hand, transmitters will ignore transmitters with incompatible standards or on overlapping channels if the received signal strength from them is below a threshold Pth which, for non Wi-Fi 6 systems, is between -76 and -80\u00a0dBm.\nIEEE 802.11 RTS/CTS Exchange.\nCSMA/CA can optionally be supplemented by the exchange of a Request to Send (RTS) packet sent by the sender S, and a Clear to Send (CTS) packet sent by the intended receiver R. Thus alerting all nodes within range of the sender, receiver or both, to not transmit for the duration of the main transmission. This is known as the IEEE 802.11 RTS/CTS exchange. Implementation of RTS/CTS helps to partially solve the hidden node problem that is often found in wireless networking.\nPerformance.\nCSMA/CA performance is based largely upon the modulation technique used to transmit the data between nodes. Studies show that under ideal propagation conditions (simulations), direct-sequence spread spectrum (DSSS) provides the highest throughput for all nodes on a network when used in conjunction with CSMA/CA and the IEEE 802.11 RTS/CTS exchange under light network load conditions. Frequency hopping spread spectrum (FHSS) follows distantly behind DSSS with regard to throughput with a greater throughput once network load becomes substantially heavy. However, the throughput is generally the same under real world conditions due to radio propagation factors.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "40847", "revid": "35498457", "url": "https://en.wikipedia.org/wiki?curid=40847", "title": "Carrier-sense multiple access with collision detection", "text": "Media access control method used most notably in early Ethernet\nCarrier-sense multiple access with collision detection (CSMA/CD) is a medium access control (MAC) method used most notably in early Ethernet technology for local area networking. It uses carrier-sensing to defer transmissions until no other stations are transmitting. This is used in combination with collision detection in which a transmitting station detects collisions by sensing transmissions from other stations while it is transmitting a frame. When this collision condition is detected, the station stops transmitting that frame, transmits a jam signal, and then waits for a random time interval before trying to resend the frame.\nCSMA/CD is a modification of pure carrier-sense multiple access (CSMA). CSMA/CD is used to improve CSMA performance by terminating transmission as soon as a collision is detected, thus shortening the time required before a retry can be attempted.\nWith the growing popularity of Ethernet switches in the 1990s, IEEE 802.3 deprecated Ethernet repeaters in 2011, making CSMA/CD and half-duplex operation less common and less important.\nProcedure.\nThe following procedure is used to initiate a transmission. The procedure is complete when the frame is transmitted successfully or a collision is detected during transmission.\nThe following procedure is used to resolve a detected collision. The procedure is complete when retransmission is initiated or the retransmission is aborted due to numerous collisions.\nMethods for collision detection are media dependent. On a shared, electrical bus such as 10BASE5 or 10BASE2, collisions can be detected by comparing transmitted data with received data or by recognizing a higher than normal signal amplitude on the bus. On all other media, a carrier sensed on the receive channel while transmitting triggers a collision event. Repeaters or hubs detect collisions on their own and propagate jam signals.\nThe collision recovery procedure can be likened to what happens at a dinner party, where all the guests talk to each other through a common medium (the air). Before speaking, each guest politely waits for the current speaker to finish. If two guests start speaking at the same time, both stop and wait for short, random periods of time (in Ethernet, this time is measured in microseconds). The hope is that by each choosing a random period of time, both guests will not choose the same time to try to speak again, thus avoiding another collision.\nJam signal.\nThe jam signal or jamming signal is a signal that carries a 32-bit binary pattern sent by a data station to inform the other transmitting stations of the collision and that they must not transmit.\nThe maximum jam-time is calculated as follows: The maximum allowed diameter of an Ethernet installation is limited to 232 bits. This makes a round-trip-time of 464 bits. As the slot time in Ethernet is 512 bits, the difference between slot time and round-trip-time is 48 bits (6 bytes), which is the maximum \"jam-time\".\nThis in turn means: A station noting a collision has occurred is sending a 4 to 6 byte long pattern composed of 16 1-0 bit combinations.\nThe purpose of this is to ensure that any other node which may currently be receiving a frame will receive the jam signal in place of the correct 32-bit MAC CRC; this causes the other receivers to discard the frame due to a CRC error.\nLate collision.\nA late collision is a type of collision that happens further into the packet than is allowed for by the protocol standard in question. In 10-megabit-per-second shared-medium Ethernet, if a collision error occurs after the first 512 bits of data are transmitted by the transmitting station, a late collision is said to have occurred. Importantly, late collisions are not re-sent by the NIC, unlike collisions occurring before the first 64 octets; it is left for the upper layers of the protocol stack to determine that there was loss of data.\nAs a correctly set up CSMA/CD network link should not have late collisions, the usual possible causes are full-duplex/half-duplex mismatch, exceeded Ethernet cable length limits, or defective hardware such as incorrect cabling, non-compliant number of hubs in the network, or a bad NIC.\nLocal collision.\nA local collision is a collision that occurs at the NIC, as opposed to on the wire. A NIC cannot detect local collisions without attempting to send information.\nOn UTP cable, a local collision is detected on the local segment only when a station detects a signal on the RX pair at the same time it is sending on the TX pair. Since the two signals are on different pairs, there is no characteristic change in the signal. Collisions are only recognized on UTP when the station is operating in half-duplex. The only functional difference between half and full-duplex operation in this regard is whether or not the transmit and receive pairs are permitted to be used simultaneously.\nRemote collision.\nA remote collision, in CSMA/CD computer networks over half-duplex media (10BASE5 or 10BASE2), is a collision that occurs when a frame shorter than the minimum length is transmitted. This frame may cause a collision at the remote end which cannot be detected by the transmitter, so the frame is not resent on the physical layer. Due to interference on the medium, its data is corrupted and frame check sequence fails, requiring recovery at a higher layer, if possible.\nChannel capture effect.\nThe channel capture effect is a phenomenon where one user of a shared medium \"captures\" the medium for a significant time. During this period (usually 16 frames), other users are denied use of the medium. This effect was first seen in networks using CSMA/CD on Ethernet. Because of this effect, the most data-intense connection dominates the multiple-access wireless channel. This happens in Ethernet links because of the way nodes \"back off\" from the link and attempt to re-access it. In the Ethernet protocol, when a communication collision happens (when two users of the medium try to send at the same time), each user waits for a random period of time before re-accessing the link. However, a user will wait (\"back off\") for a random amount of time proportional to the number of times it has successively tried to access the link. The channel capture effect happens when one user continues to \"win\" the link.\nFor example, user A and user B both try to access a quiet link at the same time. Since they detect a collision, user A waits for a random time between 0 and 1 time units and so does user B. Let's say user A chooses a lower back-off time. User A then begins to use the link and B allows it to finish sending its frame. If user A still has more to send, then user A and user B will cause another data collision. A will once again choose a random back-off time between 0 and 1, but user B will choose a back-off time between 0 and 3 \u2013 because this is B's second time colliding in a row. Chances are A will \"win\" this one again. If this continues, A will most likely win all the collision battles, and after 16 collisions (the number of tries before a user backs down for an extended period of time), user A will have \"captured\" the channel.\nThe ability of one node to capture the entire medium is decreased as the number of nodes increases. This is because as the number of nodes increases, there is a higher probability that one of the \"other\" nodes will have a lower back-off time than the capturing node.\nThe channel capture effect creates a situation where one station is able to transmit while others are continually backing off, thus leading to a situation of short-term unfairness. Yet, the situation is long-term fair because every station has the opportunity to \"capture\" the medium once one station is done transmitting. The efficiency of the channel is increased when one node has captured the channel.\nA negative side effect of the capture effect would be the idle time created due to stations backing off. Once one station is finished transmitting on the medium, large idle times are present because all other stations were continually backing off. In some instances, back-off can occur for so long that some stations actually discard packets because maximum attempt limits have been reached.\nApplications.\nCSMA/CD was used in now-obsolete shared-medium Ethernet variants (10BASE5, 10BASE2), and in the early versions of twisted-pair Ethernet, which used repeater hubs. Modern Ethernet networks, built with switches and full-duplex connections, no longer need to use CSMA/CD, because each Ethernet segment, or collision domain, is now isolated. CSMA/CD is still supported for backwards compatibility and for half-duplex connections. The IEEE 802.3 standard, which defines all Ethernet variants, for historical reasons still bore the title \"Carrier sense multiple access with collision detection (CSMA/CD) access method and physical layer specifications\" until 802.3-2008, which uses new name \"IEEE Standard for Ethernet\".\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "40848", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=40848", "title": "Carrier shift", "text": "Telecommunications term\nIn telecommunications, the term carrier shift has the following meanings: \n\"Note 1:\" The carrier shift results in a change in carrier power. \n\"Note 2:\" The carrier shift may be a shift to a higher or to a lower frequency. \n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "40849", "revid": "76", "url": "https://en.wikipedia.org/wiki?curid=40849", "title": "Carrier system", "text": "Type of telecommunications system\nA carrier system is a transmission system that transmits information, such as the voice signals of a telephone call and the video signals of television, by modulation of one or multiple carrier signals above the principal voice frequency or data rate.\nCarrier systems typically transmit multiple channels of communication simultaneously over the shared medium using various forms of multiplexing. Prominent multiplexing methods of the carrier signal are time-division multiplexing (TDM) and frequency-division multiplexing (FDM). A cable television system is an example of frequency-division multiplexing. Many television programs are carried simultaneously on the same coaxial cable by sending each at a different frequency. Multiple layers of multiplexing may ultimately be performed upon a given input signal. For example, in the public switched telephone network, many telephone calls are sent over shared trunk lines by time-division multiplexing. For long-distance calls several of these channels may be sent over a communications satellite link by frequency-division multiplexing. At a given receiving node, specific channels may be demultiplexed individually.\nHistory.\nCarrier systems increase economic efficiency by carrying more traffic on comparable cost of communication infrastructure. 19th century telephone systems, operating by direct baseband transmission, could only carry one telephone call on each wire pair, hence routes with heavy traffic needed many wire pairs.\nIn the 1920s, frequency-division multiplexing could carry several circuits on the same balanced wires, and by the 1930s L-carrier and similar systems carried hundreds of calls simultaneously on coaxial cables. The capacity of these systems increased in the middle of the century.\nIn the 1950s, research began into further increasing the throughput of terminal equipment by using digital signals with time-division multiplexing (TDM). This work led to T-carrier, E-carrier and other similar digital systems.\nDue to the shorter repeater spacings required by digital systems, long-distance transmission still used FDM until the late 1970s when optical fiber was improved to the point that digital connections became the cheapest ones for all distances, short and long. By the end of the century, analog connections between and within telephone exchanges became rare.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "40850", "revid": "11292982", "url": "https://en.wikipedia.org/wiki?curid=40850", "title": "Carrier-to-receiver noise density", "text": ""}
{"id": "40851", "revid": "5450285", "url": "https://en.wikipedia.org/wiki?curid=40851", "title": "Carson bandwidth rule", "text": "Rule in telecommunications\nIn telecommunications, the Carson's bandwidth rule defines the approximate bandwidth requirements of communications system components for a carrier signal that is frequency modulated by a continuous or broad spectrum of frequencies rather than a single frequency. Carson's rule does not apply well when the modulating signal contains discontinuities, such as a square wave. Carson's rule originates from John Renshaw Carson's 1922 paper.\nCarson's bandwidth rule is expressed by the relation:\nformula_1\nwhere:\nformula_2 is the bandwidth requirement;\nformula_3 is the peak frequency deviation;\nformula_4 is the highest frequency in the modulating signal.\nFor example, a typical VHF/UHF two-way radio signal using FM mode, with 5 kHz peak deviation, and a maximum audio frequency of 3\u00a0kHz, would require an approximate bandwidth of 2 \u00d7 (5 kHz + 3 kHz)= 16 kHz.\nStandard broadcast stereo FM, with a peak deviation of 75\u00a0kHz, has a highest modulating frequency (which combines L + R and L \u2212 R) of 53\u00a0kHz (assuming no RDS or other subcarriers). Most of the energy therefore falls within an approximate bandwidth of 2 \u00d7 (75 + 53)= 256 kHz. (Geographically close FM broadcast transmitters are almost always assigned nominal center frequencies at least 400\u00a0kHz apart).\nCarson's bandwidth rule is often applied to transmitters, antennas, optical sources, receivers, photodetectors, and other communications system components.\nAny frequency modulated signal will have an \"infinite\" number of sidebands and hence an infinite bandwidth but, in practice, all significant sideband energy (98% or more) is concentrated within the bandwidth defined by Carson's rule. It is a useful approximation, but setting the arbitrary definition of occupied bandwidth at 98% of the power still means that the power outside the band is about formula_5 less than the carrier inside, therefore Carson's Rule is of little use in spectrum planning."}
{"id": "40852", "revid": "29615425", "url": "https://en.wikipedia.org/wiki?curid=40852", "title": "CASE", "text": ""}
{"id": "40853", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=40853", "title": "Cassegrain antenna", "text": "Type of parabolic antenna with a convex secondary reflector\nIn telecommunications and radar, a Cassegrain antenna is a parabolic antenna in which the feed antenna is mounted at or behind the surface of the concave main parabolic reflector dish and is aimed at a smaller convex secondary reflector suspended in front of the primary reflector. The beam of radio waves from the feed illuminates the secondary reflector, which reflects it back to the main reflector dish, which reflects it forward again to form the desired beam. The Cassegrain design is widely used in parabolic antennas, particularly in large antennas such as those in satellite ground stations, radio telescopes, and communication satellites.\nGeometry.\nThe primary reflector is a paraboloid, while the shape of the convex secondary reflector is a hyperboloid. The geometrical condition for radiating a collimated, plane wave beam is that the feed antenna is located at the far focus of the hyperboloid, while the focus of the primary reflector coincides with the near focus of the hyperboloid. Usually the secondary reflector and the feed antenna are located on the central axis of the dish. However, in \"offset Cassegrain\" configurations, the primary dish reflector is asymmetric, and its focus, and the secondary reflector, are located to one side of the dish, so that the secondary reflector does not partially obstruct the beam.\nAdvantages.\nThis design is an alternative to the most common parabolic antenna design, called \"front feed\" or \"prime focus\", in which the feed antenna itself is mounted suspended in front of the dish at the focus, pointed back toward the dish. The Cassegrain is a more complex design, but in certain applications it has advantages over front feed that can justify its increased complexity:\nA disadvantage of the Cassegrain is that the feed horn(s) must have a narrower beamwidth (higher gain) to focus its radiation on the smaller secondary reflector, instead of the wider primary reflector as in front-fed dishes. The angular width the secondary reflector subtends at the feed horn is typically 10\u201315\u00b0, as opposed to 120\u2013180\u00b0 the main reflector subtends in a front-fed dish. Therefore, the feed horn must be longer for a given wavelength.\nBeam waveguide antenna.\nA beam waveguide antenna is a type of complicated Cassegrain antenna with a long radio wave path to allow the feed electronics to be located at ground level. It is used in very large steerable radio telescopes and satellite ground antennas, where the feed electronics are too complicated and bulky, or requires too much maintenance and alterations, to locate on the dish; for example those using cryogenically cooled amplifiers. The beam of incoming radio waves from the secondary reflector is reflected by additional mirrors in a long twisting path through the axes of the altazimuth mount, so the antenna can be steered without interrupting the beam, and then down through the antenna tower to a feed building at ground level.\nHistory.\nThe Cassegrain antenna design was adapted from the Cassegrain telescope, a type of reflecting telescope developed around 1672 and attributed to French Province England priest Laurent Cassegrain. The first Cassegrain antenna was invented and patented by Cochrane and Whitehead at Elliot Bros in Borehamwood, England, in 1952. The patent, British Patent Number 700868, was subsequently challenged in court, but prevailed. The Voyager 1 spacecraft launched in 1977 is, as of September 2024[ [update]], 24.6 billion kilometers from Earth, the furthest manmade object in space, and it's 3.7 meter S and X-band Cassegrain antenna \"(picture below)\" is still able to communicate with ground stations.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "40854", "revid": "28438779", "url": "https://en.wikipedia.org/wiki?curid=40854", "title": "Cell relay", "text": "In computer networking, cell relay refers to a method of statistically multiplexing small fixed-length packets, called \"cells\", to transport data between computers or kinds of network equipment. It is a reliable, connection-oriented packet switched data communications protocol.\nTransmission Rates.\nCell relay transmission rates usually are between 56 kbit/s and several gigabits per second. ATM, a particularly popular form of cell relay, is most commonly used for home DSL connections, which often runs between 128\u00a0kbit/s and 1.544 Mbit/s (DS1), and for high-speed backbone connections (OC-3 and faster).\nCell relay protocols have neither flow control nor error correction capability, are information-content independent, and correspond only to layers one and two of the OSI Reference Model.\nCell relay can be used for delay- and jitter-sensitive traffic such as voice and video.\nHow Cell Relay Works.\nCell relay systems break variable-length user packets into groups of fixed-length cells, that add addressing and verification information. Frame length is fixed in networking hardware, based on time delay and user packet-length considerations. One user data message may be segmented over many cells.\nCell relay systems may also carry bitstream-based data such as PDH traffic, by breaking it into streams of cells, with a lightweight synchronization and clock recovery shim. Thus cell relay systems may potentially carry any combination of stream-based and packet-based data. This is a form of statistical time division multiplexing.\nCell relay is an implementation of fast packet-switching technology that is used in connection-oriented broadband integrated services digital networks (B-ISDN, and its better-known supporting technology ATM) and connectionless IEEE 802.6 switched multi-megabit data service (SMDS).\nAt any time there is information to be transmitted; the switch basically sends the data units. Connections don't have to be negotiated like circuit switching. Channels don't have to be allocated because channels do not exist in ATM, and on condition that there is an adequate amount of bandwidth to maintain it, there can be indefinite transmissions over the same facility.\nCell relay utilizes data cells of a persistent size. Frames are comparable to data packets; however they contrast from cells in that they may fluctuate in size based on circumstances. This type of technology is not secure for the reason that its procedures do not support error handling or data recovery. Per se, all delicate and significant transmissions may perhaps be transported faster via fixed-sized cells, which are simpler to transmit compared to variable-sized frames or packets.\nReliability.\nCell relay is extremely reliable for transporting vital data. Switching devices give the precise method to cells as each endpoint address embedded in a cell. An example of cell relay is ATM, a prevalent form utilized to transfer a cell with a fixed size of 53 bytes.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "40855", "revid": "36767729", "url": "https://en.wikipedia.org/wiki?curid=40855", "title": "Cellular mobile", "text": ""}
{"id": "40857", "revid": "157381", "url": "https://en.wikipedia.org/wiki?curid=40857", "title": "Centrex service", "text": ""}
{"id": "40858", "revid": "37569883", "url": "https://en.wikipedia.org/wiki?curid=40858", "title": "Caesium standard", "text": "Primary frequency standard\nThe caesium standard is a primary frequency standard in which the photon absorption by transitions between the two hyperfine ground states of caesium-133 atoms is used to control the output frequency. The first caesium clock was built by Louis Essen in 1955 at the National Physical Laboratory in the UK and promoted worldwide by Gernot M. R. Winkler of the United States Naval Observatory.\nCaesium atomic clocks are one of the most accurate time and frequency standards, and serve as the primary standard for the definition of the second in the International System of Units (SI), the modern metric system. By definition, radiation produced by the transition between the two hyperfine ground states of caesium-133 (in the absence of external influences such as the Earth's magnetic field) has a frequency, \u0394\"\u03bd\"Cs, of exactly . That value was chosen so that the caesium second equaled, to the limit of measuring ability in 1960 when it was adopted, the existing standard ephemeris second based on the Earth's orbit around the Sun. Because no other measurement involving time had been as precise, the effect of the change was less than the experimental uncertainty of all existing measurements.\nWhile the second is the only base unit to be explicitly defined in terms of the caesium standard, the majority of SI units have definitions that mention either the second, or other units defined using the second. Consequently, every base unit except the mole and every named derived unit except the coulomb, gray, sievert, radian, and steradian have values that are implicitly at least partially defined by the properties of the caesium-133 hyperfine transition radiation. And of these, all but the mole, the coulomb, and the dimensionless radian and steradian are implicitly defined by the general properties of electromagnetic radiation.\nTechnical details.\nThe official definition of the second was first given by the BIPM at the 13th General Conference on Weights and Measures in 1967 as: \"\"The second is the duration of periods of the radiation corresponding to the transition between the two hyperfine levels of the ground state of the caesium 133 atom.\" At its 1997 meeting the BIPM added to the previous definition the following specification: \"This definition refers to a caesium atom at rest at a temperature of 0\u00a0K.\"\nThe BIPM restated this definition in its 26th conference (2018), \"The second is defined by taking the fixed numerical value of the caesium frequency \u2206\u03bdCs, the unperturbed ground-state hyperfine transition frequency of the caesium 133 atom, to be 9 192 631 770 when expressed in the unit Hz, which is equal to s\u22121.\"\"\nThe meaning of the preceding definition is as follows. The caesium atom has a ground state electron state with configuration [Xe] 6s1 and, consequently, atomic term symbol 2S1/2. This means that there is one unpaired electron and the total electron spin of the atom is 1/2. Moreover, the nucleus of caesium-133 has a nuclear spin equal to 7/2. The simultaneous presence of electron spin and nuclear spin leads, by a mechanism called hyperfine interaction, to a (small) splitting of all energy levels into two sub-levels. One of the sub-levels corresponds to the electron and nuclear spin being parallel (i.e., pointing in the same direction), leading to a total spin \"F\" equal to \"F\" = 7/2 + 1/2 = 4; the other sub-level corresponds to anti-parallel electron and nuclear spin (i.e., pointing in opposite directions), leading to a total spin \"F\" = 7/2 \u2212 1/2 = 3. In the caesium atom it so happens that the sub-level lowest in energy is the one with \"F\" = 3, while the \"F\" = 4 sub-level lies energetically slightly above. When the atom is irradiated with electromagnetic radiation having an energy corresponding to the energetic difference between the two sub-levels the radiation is absorbed and the atom is excited, going from the \"F\" = 3 sub-level to the \"F\" = 4 one. After some time the atom will re-emit the radiation and return to its \"F\" = 3 ground state. From the definition of the second it follows that the radiation in question has a frequency of exactly , corresponding to a wavelength of about 3.26\u00a0cm and therefore belonging to the microwave range.\nNote that a common confusion involves the conversion from angular frequency (formula_1) to frequency (formula_2), or vice versa. Angular frequencies are conventionally given as s\u22121 in scientific literature, but here the units implicitly mean \"radians\" per second. In contrast, the unit Hz should be interpreted as \"cycles\" per second. The conversion formula is formula_3, which implies that 1 Hz corresponds to an angular frequency of approximately 6.28 radians per second (or 6.28 s\u22121 where radians is omitted for brevity by convention).\nParameters and significance in the second and other SI units.\nSuppose the caesium standard has the parameters:\nTime and frequency.\nThe first set of units defined using the caesium standard were those relating to time, with the second being defined in 1967 as \"the duration of 9 192 631 770 periods of the radiation corresponding to the transition between the two hyperfine levels of the ground state of the caesium 133 atom\" meaning that:\nThis also linked the definitions of the derived units relating to force and energy (see below) and of the ampere, whose definition at the time made reference to the newton, to the caesium standard. Before 1967 the SI units of time and frequency were defined using the tropical year and before 1960 by the length of the mean solar day\nLength.\nIn 1983, the meter was, indirectly, defined in terms of the caesium standard with the formal definition \"The metre is the length of the path travelled by light in vacuum during a time interval of 1/299 792 458 of a second. This implied:\nBetween 1960 and 1983, the metre had been defined by the wavelength of a different transition frequency associated with the krypton-86 atom. This had a much higher frequency and shorter wavelength than the caesium standard, falling inside the visible spectrum. The first definition, used between 1889 and 1960, was by the international prototype meter.\nMass, energy, and force.\nFollowing the 2019 revision of the SI, electromagnetic radiation, in general, was explicitly defined to have the exact parameters:\nThe caesium-133 hyperfine transition radiation was explicitly defined to have frequency:\nThough the above values for \"c\" and \u0394\"\u03bd\"Cs were already obviously implicit in the definitions of the metre and second. Together they imply:\nNotably, the wavelength has a fairly human-sized value of about 3.26 centimetres and the photon energy is surprisingly close to the average molecular kinetic energy per degree of freedom per kelvin. From these it follows that:\nPrior to the revision, between 1889 and 2019, the family of metric (and later SI) units relating to mass, force, and energy were somewhat notoriously defined by the mass of the International Prototype of the Kilogram (IPK), a specific object stored at the headquarters of the International Bureau of Weights and Measures in Paris, meaning that any change to the mass of that object would have resulted in a change to the size of the kilogram and of the many other units whose value at the time depended on that of the kilogram.\nTemperature.\nFrom 1954 to 2019, the SI temperature scales were defined using the triple point of water and absolute zero. The 2019 revision replaced these with an assigned value for the Boltzmann constant, \"k\", of J/K, implying:\nAmount of substance.\nThe mole is an extremely large number of \"elementary entities\" (i.e. atoms, molecules, ions, etc). From 1969 to 2019, this number was 0.012 \u00d7 the mass ratio between the IPK and a carbon 12 atom. The 2019 revision simplified this by assigning the Avogadro constant the exact value elementary entities per mole, thus, uniquely among the base units, the mole maintained its independence from the caesium standard:\nElectromagnetic units.\nPrior to the revision, the ampere was defined as the current needed to produce a force between 2 parallel wires 1 m apart of 0.2 \u03bcN per meter. The 2019 revision replaced this definition by giving the charge on the electron, \"e\", the exact value coulombs. Somewhat incongruously, the coulomb is still considered a derived unit and the ampere a base unit, rather than vice versa. In any case, this convention entailed the following exact relationships between the SI electromagnetic units, elementary charge, and the caesium-133 hyperfine transition radiation:\nOptical units.\nFrom 1967 to 1979 the SI optical units, lumen, lux, and candela are defined using the incandescent glow of platinum at its melting point. After 1979, the candela was defined as the luminous intensity of a monochromatic visible light source of frequency 540\u00a0THz (i.e that of the caesium standard) and radiant intensity watts per steradian. This linked the definition of the candela to the caesium standard and, until 2019, to the IPK. Unlike the units relating to mass, energy, temperature, amount of substance, and electromagnetism, the optical units were \"not\" massively redefined in 2019, though they were indirectly affected since their values depend on that of the watt, and hence of the kilogram. The frequency used to define the optical units has the parameters:\nThis implies:\nSummary.\nThe parameters of the caesium-133 hyperfine transition radiation expressed exactly in SI units are:\nIf the seven base units of the SI are expressed explicitly in terms of the SI defining constants, they are:\nUltimately, 6 of the 7 base units (all but the dimensionless mole) notably have values that depend on that of \u0394\"\u03bd\"Cs, which appears far more often than any of the other defining constants. However, the derived unit of one coulomb, which is an ampere-second, is a dimensionful unit defined purely in terms of the elementary charge and hence is independent of \u0394\"\u03bd\"Cs.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "40859", "revid": "84951", "url": "https://en.wikipedia.org/wiki?curid=40859", "title": "Chadless tape", "text": ""}
{"id": "40860", "revid": "50056326", "url": "https://en.wikipedia.org/wiki?curid=40860", "title": "Channel", "text": "Channel, channels, channeling, etc., may refer to:\n&lt;templatestyles src=\"Template:TOC_right/styles.css\" /&gt;\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "40861", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=40861", "title": "Channel noise level", "text": "Noise measurement in telecommunications\nIn telecommunications, the term channel noise level has the following meanings:\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "40862", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=40862", "title": "Channel reliability", "text": "In telecommunications, channel reliability (ChR) is the percentage of time a communication channel was available for use in a specified period of scheduled availability. \nChannel reliability is given by \nformula_1\nwhere \"T\" o is the channel total outage time, \"T\" s is the channel total scheduled time, and \"T\" a is the channel total available time.\nReferences.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "40863", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=40863", "title": "Channel service unit", "text": "Telecommunications equipment\nIn telecommunications, a channel service unit (CSU) is a line bridging device for use with T-carrier, which\nCommon varieties.\nCSUs can be categorized by the class of service they support (DS1, DS3, DDS, etc.) and by the capabilities within that class. For example, basic DS1 (T1) CSUs support loopback of each interface and will produce alarm indication signal to the provider's network interface device (NID) in the event of loss of signal from the customer-premises equipment (CPE). More advanced units will include internal monitors of the performance of the carrier in both directions and may have test pattern generation and monitor capabilities.\nCommon practice.\nCSUs are required by PSTN providers at digital interfaces that terminate in a DSU on the customer side. They are not used when the service terminates in a modem, such as the DSL family of service. The maintenance capabilities of the CSU provide important guidance as to whether the provider needs to dispatch a repairman to the customer location.\nReferences.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "40865", "revid": "757572", "url": "https://en.wikipedia.org/wiki?curid=40865", "title": "Character interval", "text": "Total number of unit intervals required to transmit any given character\nCharacter interval: In a communications system, the total number of unit intervals required to transmit any given character, including synchronizing, information, error checking, or control characters, but not including signals that are not associated with individual characters. \nAn example of a time interval that is excluded when determining character interval is any time added between the end of a stop signal and the beginning of the next start signal to accommodate changing transmission conditions, such as a change in data signaling rate or buffering requirements. This added time is defined as a part of the intercharacter interval.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "40866", "revid": "7903804", "url": "https://en.wikipedia.org/wiki?curid=40866", "title": "Characteristic impedance", "text": "Property of an electrical circuit\nThe characteristic impedance or surge impedance (usually written \"Z\"0) of a uniform transmission line is the ratio of the amplitudes of voltage and current of a wave travelling in one direction along the line in the absence of reflections in the other direction. Equivalently, it can be defined as the input impedance of a transmission line when its length is infinite. Characteristic impedance is determined by the geometry and materials of the transmission line and, for a uniform line, is not dependent on its length. The SI unit of characteristic impedance is the ohm.\nThe characteristic impedance of a lossless transmission line is purely real, with no reactive component (see below). Energy supplied by a source at one end of such a line is transmitted through the line without being dissipated in the line itself. A transmission line of finite length (lossless or lossy) that is terminated at one end with an impedance equal to the characteristic impedance appears to the source like an infinitely long transmission line and produces no reflections.\nTransmission line model.\nThe characteristic impedance \"Z\"(\"\u03c9\") of an infinite transmission line at a given angular frequency \u03c9 is the ratio of the voltage and current of a pure sinusoidal wave of the same frequency travelling along the line. This relation is also the case for finite transmission lines until the wave reaches the end of the line. Generally, a wave is reflected back along the line in the opposite direction. When the reflected wave reaches the source, it is reflected yet again, adding to the transmitted wave and changing the ratio of the voltage and current at the input, causing the voltage-current ratio to no longer equal the characteristic impedance. This new ratio including the reflected energy is called the input impedance of that particular transmission line and load.\nThe input impedance of an infinite line is equal to the characteristic impedance since the transmitted wave is never reflected back from the end. Equivalently: the characteristic impedance of a line is that impedance which, when terminating an arbitrary length of line at its output, produces an input impedance of equal value. This is so because there is no reflection on a line terminated in its own characteristic impedance.\nApplying the transmission line model based on the telegrapher's equations as derived below, the general expression for the characteristic impedance of a transmission line is:\nformula_1\nwhere\nThis expression extends to DC by letting \u03c9 tend to 0.\nA surge of energy on a finite transmission line will see an impedance of \"Z\"0 prior to any reflections returning; hence \"surge impedance\" is an alternative name for \"characteristic impedance\".\nAlthough an infinite line is assumed, since all quantities are per unit length, the \u201cper length\u201d parts of all the units cancel, and the characteristic impedance is independent of the length of the transmission line.\nThe voltage and current phasors on the line are related by the characteristic impedance as:\nformula_2\nwhere the subscripts (+) and (\u2212) mark the separate constants for the waves traveling forward (+) and backward (\u2212). The rightmost expression has a negative sign because the current in the backward wave has the opposite direction to current in the forward wave.\nDerivation.\nUsing the telegrapher's equation.\nThe differential equations describing the dependence of the voltage and current on time and space are linear, so that a linear combination of solutions is again a solution. This means that we can consider solutions with a time dependence formula_3. Doing so allows to factor out the time dependence, leaving an ordinary differential equation for the coefficients, which will be phasors, dependent on position (space) only. Moreover, the parameters can be generalized to be frequency-dependent.\nConsider a steady-state problem such that the voltage and current can be written as:\nformula_4\nTake the positive direction for V and I in the loop to be clockwise. Substitution in the telegraph equations and factoring out the time dependence formula_3 now gives:\nformula_6\nwith impedance Z and admittance Y. Derivation and substitution of these two first-order differential equations results in two uncoupled second-order differential equations:\nformula_7\nwith \"k\"2 = \"ZY\" = (\"R\" + \"j\u03c9L\")(\"G\" + \"j\u03c9LC\") and \"k\" = \"\u03b1\" + \"j\u03b2\" called the propagation constant. \nThe solution to these types of equations can be written as:\nformula_8\nwith A, \"A\"1, B and \"B\"1 the constants of integration. Substituting these constants in the first-order system gives:\nformula_9\nwhere\nformula_10\nIt can be seen that the constant \"Z\"0, defined in the above equations has the dimensions of impedance (ratio of voltage to current) and is a function of primary constants of the line and operating frequency. It is called the \"&lt;dfn &gt;characteristic impedance&lt;/dfn&gt;\" of the transmission line. \nThe general solution of the telegrapher's equations can now be written as:\nformula_11\nBoth the solution for the voltage and the current can be regarded as a superposition of two travelling waves in the \"x\"(+) and \"x\"(\u2212) directions.\nFor typical transmission lines, that are carefully built from wire with low loss resistance R and small insulation leakage conductance G; further, used for high frequencies, the inductive reactance \u03c9L and the capacitive admittance \u03c9C will both be large. In those cases, the phase constant and characteristic impedance are typically very close to being real numbers:\nformula_12\nManufacturers make commercial cables to approximate this condition very closely over a wide range of frequencies.\nAs a limiting case of infinite ladder networks.\nIntuition.\nConsider an infinite ladder network consisting of a series impedance Z and a shunt admittance Y. Let its input impedance be \"Z\"IT. If a new pair of impedance Z and admittance Y is added in front of the network, its input impedance \"Z\"IT remains unchanged since the network is infinite. Thus, it can be reduced to a finite network with one series impedance Z and two parallel impedances 1/\"Y\" and \"Z\"IT. Its input impedance is given by the expression\nformula_13\nwhich is also known as its iterative impedance. Its solution is:\nformula_14\nFor a transmission line, it can be seen as a limiting case of an infinite ladder network with infinitesimal impedance and admittance at a constant ratio. Taking the positive root, this equation simplifies to:\nformula_15\nDerivation.\nUsing this insight, many similar derivations exist in several books and are applicable to both lossless and lossy lines.\nHere, we follow an approach posted by Tim Healy. The line is modeled by a series of differential segments with differential series elements (\"R\"\u2009d\"x\", \"L\"\u2009d\"x\") and shunt elements (\"C\"\u2009d\"x\", \"G\"\u2009d\"x\") (as shown in the figure at the beginning of the article). The characteristic impedance is defined as the ratio of the input voltage to the input current of a semi-infinite length of line. We call this impedance \"Z\"0. That is, the impedance looking into the line on the left is \"Z\"0. But, of course, if we go down the line one differential length d\"x\", the impedance into the line is still \"Z\"0. Hence we can say that the impedance looking into the line on the far left is equal to \"Z\"0 in parallel with \"C\"\u2009d\"x\" and \"G\"\u2009d\"x\", all of which is in series with \"R\"\u2009d\"x\" and \"L\"\u2009d\"x\". Hence:\nformula_16\nThe added \"Z\"0 terms cancel, leaving\nformula_17\nThe first-power d\"x\" terms are the highest remaining order. Dividing out the common factor of d\"x\", and dividing through by the factor (\"G\" + \"j\u03c9C\"), we get\nformula_18\nIn comparison to the factors whose d\"x\" divided out, the last term, which still carries a remaining factor d\"x\", is infinitesimal relative to the other, now finite terms, so we can drop it. That leads to\nformula_19\nReversing the sign \u00b1 applied to the square root has the effect of reversing the direction of the flow of current.\nLossless line.\nThe analysis of lossless lines provides an accurate approximation for real transmission lines that simplifies the mathematics considered in modeling transmission lines. A lossless line is defined as a transmission line that has no line resistance and no dielectric loss. This would imply that the conductors act like perfect conductors and the dielectric acts like a perfect dielectric. For a lossless line, \"R\" and \"G\" are both zero, so the equation for characteristic impedance derived above reduces to:\nformula_20\nIn particular, \"Z\"0 does not depend any more upon the frequency. The above expression is wholly real, since the imaginary term j has canceled out, implying that \"Z\"0 is purely resistive. For a lossless line terminated in \"Z\"0, there is no loss of current across the line, and so the voltage remains the same along the line. The lossless line model is a useful approximation for many practical cases, such as low-loss transmission lines and transmission lines with high frequency. For both of these cases, R and G are much smaller than \"\u03c9L\" and \"\u03c9C\", respectively, and can thus be ignored.\nThe solutions to the long line transmission equations include incident and reflected portions of the voltage and current:\nformula_21\nWhen the line is terminated with its characteristic impedance, the reflected portions of these equations are reduced to 0 and the solutions to the voltage and current along the transmission line are wholly incident. Without a reflection of the wave, the load that is being supplied by the line effectively blends into the line making it appear to be an infinite line. In a lossless line this implies that the voltage and current remain the same everywhere along the transmission line. Their magnitudes remain constant along the length of the line and are only rotated by a phase angle.\nSurge impedance loading.\nIn electric power transmission, the characteristic impedance of a transmission line is expressed in terms of the surge impedance loading (SIL), or natural loading, being the power loading at which reactive power is neither produced nor absorbed:\nformula_22\nin which \"V\"LL is the root mean square (RMS) line-to-line voltage in volts.\nLoaded below its SIL, the voltage at the load will be greater than the system voltage. Above it, the load voltage is depressed. The Ferranti effect describes the voltage gain towards the remote end of a very lightly loaded (or open ended) transmission line. Underground cables normally have a very low characteristic impedance, resulting in an SIL that is typically in excess of the thermal limit of the cable.\nPractical examples.\nThe characteristic impedance of coaxial cables (coax) is commonly chosen to be for RF and microwave applications. Coax for video applications is usually for its lower loss .\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nSources.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;\nExternal links.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "40867", "revid": "44062", "url": "https://en.wikipedia.org/wiki?curid=40867", "title": "Chip", "text": "Chip may refer to:\n&lt;templatestyles src=\"Template:TOC_right/styles.css\" /&gt;\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "40868", "revid": "2789093", "url": "https://en.wikipedia.org/wiki?curid=40868", "title": "Chirping", "text": "Chirping may refer to: \nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "40869", "revid": "7294", "url": "https://en.wikipedia.org/wiki?curid=40869", "title": "Chroma keying", "text": ""}
{"id": "40870", "revid": "7852030", "url": "https://en.wikipedia.org/wiki?curid=40870", "title": "Circuit", "text": "Circuit may refer to:\n&lt;templatestyles src=\"Template:TOC_right/styles.css\" /&gt;\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "40871", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=40871", "title": "Circuit noise level", "text": "At any point in a transmission system, the ratio of the circuit noise at that point to an arbitrary level chosen as a reference. \nThe circuit noise level is usually expressed in dBrn0, signifying the reading of a circuit noise meter, or in dBa0, signifying circuit noise meter reading adjusted to represent an interfering effect under specified conditions. \n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "40872", "revid": "33839581", "url": "https://en.wikipedia.org/wiki?curid=40872", "title": "Circuit reliability", "text": "Circuit reliability (also time availability) (\"CiR\") is the percentage of time an electronic circuit was available for use in a specified period of scheduled availability. Circuit reliability is given by \nformula_1 \nwhere formula_2 is the circuit total outage time, formula_3 is the circuit total scheduled time, and formula_4 is the circuit total available time.\nIn addition, circuit reliability is the expected lifespan of operation of a functioning system under nominal conditions.\nReferences.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "40873", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=40873", "title": "Circuit restoration", "text": "In telecommunications, circuit restoration is the process by which a communications circuit is established between two users after disruption or loss of the original circuit. The loss may be widespread due to a natural disaster like an ice storm or hurricane, or local by being cut underground in construction or damaged in a thunderstorm or car accident.\nCircuit restoration is usually performed in accordance with planned procedures and priorities. Restoration may be effected automatically, such as by switching to a hot standby, or manually, such as by manual patching.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "40874", "revid": "10202399", "url": "https://en.wikipedia.org/wiki?curid=40874", "title": "Circuit switching", "text": "Telecommunications connection selection method\nCircuit switching is a method of implementing a telecommunications network in which two network nodes establish a dedicated communications channel (circuit) through the network before the nodes may communicate. The circuit guarantees the full bandwidth of the channel and remains connected for the duration of the communication session. The circuit functions as if the nodes were physically connected as with an electrical circuit.\nCircuit switching originated in analog telephone networks where the network created a dedicated circuit between two telephones for the duration of a telephone call. It contrasts with message switching and packet switching used in modern digital networks in which the trunklines between switching centres carry data between many different nodes in the form of data packets without dedicated circuits.\nDescription.\nThe defining example of a circuit-switched network is the early analogue telephone network. When a call is made from one telephone to another, switches within the telephone exchanges create a continuous wire circuit between the two telephones for as long as the call lasts.\nIn circuit switching, the bit delay is constant during a connection (as opposed to packet switching, where packet queues may cause varying and potentially indefinitely long packet transfer delays). No circuit can be degraded by competing users because it is protected from use by other callers until the circuit is released and a new connection is set up. Even if no actual communication is taking place, the channel remains reserved and protected from competing users.\nWhile circuit switching is commonly used for connecting voice circuits, the concept of a dedicated path persisting between two communicating parties or nodes can be extended to signal content other than voice. The advantage of using circuit switching is that it provides for continuous transfer without the overhead associated with packets, making maximal use of available bandwidth for that communication. One disadvantage is that it can be relatively inefficient because unused capacity guaranteed to a connection cannot be used by other connections on the same network. In addition, calls cannot be established or will be dropped if the circuit is broken.\nThe call.\nFor call setup and control (and other administrative purposes), it is possible to use a separate dedicated signalling channel from the end node to the network. ISDN is one such service that uses a separate signalling channel while plain old telephone service (POTS) does not.\nThe method of establishing the connection and monitoring its progress and termination through the network may also utilize a separate control channel as in the case of links between telephone exchanges which use CCS7 packet-switched signalling protocol to communicate the call setup and control information and use TDM to transport the actual circuit data.\nEarly telephone exchanges were a suitable example of circuit switching. The subscriber would ask the operator to connect to another subscriber, whether on the same exchange or via an inter-exchange link and another operator. The result was a physical electrical connection between the two subscribers' telephones for the duration of the call. The copper wire used for the connection could not be used to carry other calls at the same time, even if the subscribers were in fact not talking and the line was silent.\nAlternatives.\nIn circuit switching, a route and its associated bandwidth is reserved from source to destination, making circuit switching relatively inefficient since capacity is reserved whether or not the connection is in continuous use. Circuit switching contrasts with message switching and packet switching. Both of these methods can make better use of available network bandwidth between multiple communication sessions under typical conditions in data communication networks.\nMessage switching routes messages in their entirety, one hop at a time, that is, store and forward of the entire message. Packet switching divides the data to be transmitted into packets transmitted through the network independently. Instead of being dedicated to one communication session at a time, network links are shared by packets from multiple competing communication sessions, resulting in the loss of the quality of service guarantees that are provided by circuit switching.\nPacket switching can be based on connection-oriented communication or connectionless communication. That is, based on virtual circuits or datagrams.\nVirtual circuits use packet switching technology that emulates circuit switching, in the sense that the connection is established before any packets are transferred, and packets are delivered in order.\nConnection-less packet switching divides the data to be transmitted into packets, called datagrams, transmitted through the network independently. Each datagram is labelled with its destination and a sequence number for ordering related packets, precluding the need for a dedicated path to help the packet find its way to its destination. Each datagram is dispatched independently and each may be routed via a different path. At the destination, the original message is reordered based on the packet number to reproduce the original message. As a result, datagram packet switching networks do not require a circuit to be established and allow many pairs of nodes to communicate concurrently over the same channel.\nMultiplexing multiple telecommunications connections over the same physical conductor has been possible for a long time, but each channel on the multiplexed link was either dedicated to one call at a time or it was idle between calls.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "40875", "revid": "20483999", "url": "https://en.wikipedia.org/wiki?curid=40875", "title": "Circular polarization", "text": "Polarization state\nIn electrodynamics, circular polarization of an electromagnetic wave is a polarization state in which, at each point, the electromagnetic field of the wave has a constant magnitude and is rotating at a constant rate in a plane perpendicular to the direction of the wave.\nIn electrodynamics, the strength and direction of an electric field is defined by its electric field vector. In the case of a circularly polarized wave, the tip of the electric field vector, at a given point in space, relates to the phase of the light as it travels through time and space. At any instant of time, the electric field vector of the wave indicates a point on a helix oriented along the direction of propagation. A circularly polarized wave can rotate in one of two possible senses: \"right-handed circular polarization (RHCP)\" in which the electric field vector rotates in a right-hand sense with respect to the direction of propagation, and \"left-handed circular polarization (LHCP)\" in which the vector rotates in a left-hand sense.\n\"Circular polarization\" is a limiting case of \"elliptical polarization\". The other special case is the easier-to-understand \"linear polarization\". All three terms were coined by Augustin-Jean Fresnel, in a memoir read to the French Academy of Sciences on 9\u00a0December 1822. Fresnel had first described the case of circular polarization, without yet naming it, in 1821.\nThe phenomenon of polarization arises as a consequence of the fact that light behaves as a two-dimensional transverse wave.\nCircular polarization occurs when the two orthogonal electric field component vectors are of equal magnitude and are out of phase by exactly 90\u00b0, or one-quarter wavelength.\nCharacteristics.\nIn a circularly polarized electromagnetic wave, the individual electric field vectors, as well as their combined vector, have a constant magnitude, and with changing phase angle. Given that this is a plane wave, each vector represents the magnitude and direction of the electric field for an entire plane that is perpendicular to the optical axis. Specifically, given that this is a circularly polarized plane wave, these vectors indicate that the electric field, from plane to plane, has a constant strength while its direction steadily rotates. Refer to these two images in the plane wave article to better appreciate this dynamic. This light is considered to be right-hand, clockwise circularly polarized if viewed by the receiver. Since this is an electromagnetic wave, each electric field vector has a corresponding, but not illustrated, magnetic field vector that is at a right angle to the electric field vector and proportional in magnitude to it. As a result, the magnetic field vectors would trace out a second helix if displayed.\nCircular polarization is often encountered in the field of optics and, in this section, the electromagnetic wave will be simply referred to as light.\nThe nature of circular polarization and its relationship to other polarizations is often understood by thinking of the electric field as being divided into two components that are perpendicular to each other. The vertical component and its corresponding plane are illustrated in blue, while the horizontal component and its corresponding plane are illustrated in green. Notice that the rightward (relative to the direction of travel) horizontal component leads the vertical component by one quarter of a wavelength, a 90\u00b0 phase difference. It is this quadrature phase relationship that creates the helix and causes the points of maximum magnitude of the vertical component to correspond with the points of zero magnitude of the horizontal component, and vice versa. The result of this alignment are select vectors, corresponding to the helix, which exactly match the maxima of the vertical and horizontal components.\nTo appreciate how this quadrature phase shift corresponds to an electric field that rotates while maintaining a constant magnitude, imagine a dot traveling clockwise in a circle. Consider how the vertical and horizontal displacements of the dot, relative to the center of the circle, vary sinusoidally in time and are out of phase by one quarter of a cycle. The displacements are said to be out of phase by one quarter of a cycle because the horizontal maximum displacement (toward the left) is reached one quarter of a cycle before the vertical maximum displacement is reached. Now referring again to the illustration, imagine the center of the circle just described, traveling along the axis from the front to the back. The circling dot will trace out a helix with the displacement toward our viewing left, leading the vertical displacement. Just as the horizontal and vertical displacements of the rotating dot are out of phase by one quarter of a cycle in time, the magnitude of the horizontal and vertical components of the electric field are out of phase by one quarter of a wavelength.\nThe next pair of illustrations is that of left-handed, counterclockwise circularly polarized light when viewed by the receiver. Because it is left-handed, the rightward (relative to the direction of travel) horizontal component is now \"lagging\" the vertical component by one quarter of a wavelength, rather than leading it.\nReversal of handedness.\nWaveplate.\nTo convert circularly polarized light to the other handedness, one can use a half-waveplate. A half-waveplate shifts a given linear component of light one half of a wavelength relative to its orthogonal linear component.\nReflection.\nThe handedness of polarized light is reversed reflected off a surface at normal incidence. Upon such reflection, the rotation of the plane of polarization of the reflected light is identical to that of the incident field. However, with propagation now in the \"opposite\" direction, the same rotation direction that would be described as \"right-handed\" for the incident beam, is \"left-handed\" for propagation in the reverse direction, and vice versa. Aside from the reversal of handedness, the ellipticity of polarization is also preserved (except in cases of reflection by a birefringent surface).\nNote that this principle only holds strictly for light reflected at normal incidence. For instance, right circularly polarized light reflected from a dielectric surface at grazing incidence (an angle beyond the Brewster angle) will still emerge as right-handed, but elliptically polarized. Light reflected by a metal at non-normal incidence will generally have its ellipticity changed as well. Such situations may be solved by decomposing the incident circular (or other) polarization into components of linear polarization parallel and perpendicular to the plane of incidence, commonly denoted \"p\" and \"s\" respectively. The reflected components in the \"p\" and \"s\" linear polarizations are found by applying the Fresnel coefficients of reflection, which are generally different for those two linear polarizations. Only in the special case of normal incidence, where there is no distinction between \"p\" and \"s\", are the Fresnel coefficients for the two components identical, leading to the above property.\nConversion to linear polarization.\nCircularly polarized light can be converted into linearly polarized light by passing it through a quarter-waveplate. Passing linearly polarized light through a quarter-waveplate with its axes at 45\u00b0 to its polarization axis will convert it to circular polarization. In fact, this is the most common way of producing circular polarization in practice. Note that passing linearly polarized light through a quarter-waveplate at an angle \"other\" than 45\u00b0 will generally produce elliptical polarization.\nHandedness conventions.\n \nCircular polarization may be referred to as right-handed or left-handed, and clockwise or anti-clockwise, depending on the direction in which the electric field vector rotates. Unfortunately, two opposing historical conventions exist.\nFrom the point of view of the source.\nUsing this convention, polarization is defined from the point of view of the source. When using this convention, left- or right-handedness is determined by pointing one's left or right thumb away from the source, in the same direction that the wave is propagating, and matching the curling of one's fingers to the direction of the temporal rotation of the field at a given point in space. When determining if the wave is clockwise or anti-clockwise circularly polarized, one again takes the point of view of the source, and while looking away from the source and in the same direction of the wave's propagation, one observes the direction of the field's temporal rotation.\nUsing this convention, the electric field vector of a left-handed circularly polarized wave is as follows:\nformula_1\nAs a specific example, refer to the circularly polarized wave in the first animation. Using this convention, that wave is defined as right-handed because when one points one's right thumb in the same direction of the wave's propagation, the fingers of that hand curl in the same direction of the field's temporal rotation. It is considered clockwise circularly polarized because, from the point of view of the source, looking in the same direction of the wave's propagation, the field rotates in the clockwise direction. The second animation is that of left-handed or anti-clockwise light, using this same convention.\nThis convention is in conformity with the Institute of Electrical and Electronics Engineers (IEEE) standard and, as a result, it is generally used in the engineering community.\nQuantum physicists also use this convention of handedness because it is consistent with their convention of handedness for a particle's spin.\nRadio astronomers also use this convention in accordance with an International Astronomical Union (IAU) resolution made in 1973.\nFrom the point of view of the receiver.\nIn this alternative convention, polarization is defined from the point of view of the receiver. Using this convention, left- or right-handedness is determined by pointing one's left or right thumb toward the source, against the direction of propagation, and then matching the curling of one's fingers to the temporal rotation of the field.\nWhen using this convention, in contrast to the other convention, the defined handedness of the wave matches the handedness of the screw type nature of the field in space. Specifically, if one freezes a right-handed wave in time, when one curls the fingers of one's right hand around the helix, the thumb will point in the direction of progression for the helix, given the sense of rotation. Note that, in the context of the nature of all screws and helices, it does not matter in which direction you point your thumb when determining its handedness.\nWhen determining if the wave is clockwise or anti-clockwise circularly polarized, one again takes the point of view of the receiver and, while looking toward the source, against the direction of propagation, one observes the direction of the field's temporal rotation.\nJust as in the other convention, right-handedness corresponds to a clockwise rotation, and left-handedness corresponds to an anti-clockwise rotation.\nMany optics textbooks use this second convention. It is also used by SPIE as well as the International Union of Pure and Applied Chemistry (IUPAC).\nUses of the two conventions.\nAs stated earlier, there is significant confusion with regards to these two conventions. As a general rule, the engineering, quantum physics, and radio astronomy communities use the first convention, in which the wave is observed from the point of view of the source. In many physics textbooks dealing with optics, the second convention is used, in which the light is observed from the point of view of the receiver.\nTo avoid confusion, it is good practice to specify \"as defined from the point of view of the source\" or \"as defined from the point of view of the receiver\" when discussing polarization matters.\nThe archive of the https:// proposes two contradictory conventions of handedness.\nNote that the IEEE defines RHCP and LHCP the opposite as those used by physicists. The IEEE 1979 Antenna Standard will show RHCP on the South Pole of the Poincare Sphere. The IEEE defines RHCP using the right hand with thumb pointing in the direction of transmit, and the fingers showing the direction of rotation of the E field with time. The rationale for the opposite conventions used by Physicists and Engineers is that Astronomical Observations are always done with the incoming wave traveling toward the observer, where as for most engineers, they are assumed to be standing behind the transmitter watching the wave traveling away from them. This article is not using the IEEE 1979 Antenna Standard and is not using the +t convention typically used in IEEE work.\nFM radio.\nFM broadcast radio stations sometimes employ circular polarization to improve signal penetration into buildings and vehicles. It is one example of what the International Telecommunication Union refers to as \"mixed polarization\", i.e. radio emissions that include both horizontally- and vertically-polarized components. In the United States, Federal Communications Commission regulations state that horizontal polarization is the standard for FM broadcasting, but that \"circular or elliptical polarization may be employed if desired\".\nDichroism.\nCircular dichroism (CD) is the differential absorption of left- and right-handed circularly polarized light. Circular dichroism is the basis of a form of spectroscopy that can be used to determine the optical isomerism and secondary structure of molecules.\nIn general, this phenomenon will be exhibited in absorption bands of any optically active molecule. As a consequence, circular dichroism is exhibited by most biological molecules, because of the dextrorotary (e.g., some sugars) and levorotary (e.g., some amino acids) molecules they contain. Noteworthy as well is that a secondary structure will also impart a distinct CD to its respective molecules. Therefore, the alpha helix, beta sheet and random coil regions of proteins and the double helix of nucleic acids have CD spectral signatures representative of their structures.\nAlso, under the right conditions, even non-chiral molecules will exhibit magnetic circular dichroism \u2014 that is, circular dichroism induced by a magnetic field.\nLuminescence.\n\"Circularly polarized luminescence\" (CPL) can occur when either a luminophore or an ensemble of luminophores is chiral. The extent to which emissions are polarized is quantified in the same way it is for circular dichroism, in terms of the \"dissymmetry factor\", also sometimes referred to as the \"anisotropy factor\". This value is given by:\nformula_2\nwhere formula_3 corresponds to the quantum yield of left-handed circularly polarized light, and formula_4 to that of right-handed light. The maximum absolute value of \"g\"em, corresponding to purely left- or right-handed circular polarization, is therefore 2. Meanwhile, the smallest absolute value that \"g\"em can achieve, corresponding to linearly polarized or unpolarized light, is zero.\nMathematical description.\nThe classical sinusoidal plane wave solution of the electromagnetic wave equation for the electric and magnetic fields is:\nformula_5\nwhere k is the wavenumber;\nformula_6\nis the angular frequency of the wave; formula_7 is an orthogonal formula_8 matrix whose columns span the transverse x-y plane; and formula_9 is the speed of light.\nHere,\nformula_10\nis the amplitude of the field, and\nformula_11\nis the normalized Jones vector in the x-y plane.\nIf formula_12 is rotated by formula_13 radians with respect to formula_14 and the x amplitude equals the y amplitude, the wave is circularly polarized. The Jones vector is:\nformula_15\nwhere the plus sign indicates left circular polarization, and the minus sign indicates right circular polarization. In the case of circular polarization, the electric field vector of constant magnitude rotates in the \"x\"-\"y\" plane.\nIf basis vectors are defined such that:\nformula_16\nand:\nformula_17\nthen the polarization state can be written in the \"R-L basis\" as:\nformula_18\nwhere:\nformula_19\nand:\nformula_20\nAntennas.\nA number of different types of antenna elements can be used to produce circularly polarized (or nearly so) radiation; following Balanis, one can use \"dipole elements\":\n\"... two crossed dipoles provide the two orthogonal field components... If the two dipoles are identical, the field intensity of each along zenith ... would be of the same intensity. Also, if the two dipoles were fed with a 90\u00b0 degree time-phase difference (phase quadrature), the polarization along zenith would be circular... One way to obtain the 90\u00b0 time-phase difference between the two orthogonal field components, radiated respectively by the two dipoles, is by feeding one of the two dipoles with a transmission line which is 1/4 wavelength longer or shorter than that of the other,\" p.80;\nor \"helical elements\":\n\"To achieve circular polarization [in axial or end-fire mode] ... the circumference \"C\" of the helix must be ... with \"C\"/wavelength = 1 near optimum, and the spacing about \"S\" = wavelength/4,\" p.571;\nor \"patch elements\":\n\"... circular and elliptical polarizations can be obtained using various feed arrangements or slight modifications made to the elements... Circular polarization can be obtained if two orthogonal modes are excited with a 90\u00b0 time-phase difference between them. This can be accomplished by adjusting the physical dimensions of the patch... For a square patch element, the easiest way to excite ideally circular polarization is to feed the element at two adjacent edges... The quadrature phase difference is obtained by feeding the element with a 90\u00b0 power divider,\" p.859.\nIn quantum mechanics.\nIn the quantum mechanical view, light is composed of photons. Polarization is a manifestation of the spin angular momentum of light. More specifically, in quantum mechanics, the direction of spin of a photon is tied to the handedness of the circularly polarized light, and the spin of a beam of photons is similar to the spin of a beam of particles, such as electrons. In the physics convention (from the point of view of the source), a right-handed circular polarization corresponds to a positive spin (denoted formula_21), whereas a left-handed circular polarization corresponds to a negative spin (denoted formula_22).\nIn nature.\nOnly a few mechanisms in nature are known to systematically produce circularly polarized light. In 1911, Albert Abraham Michelson discovered that light reflected from the golden scarab beetle \"Chrysina resplendens\" is preferentially left-polarized. Since then, circular polarization has been measured in several other scarab beetles such as \"Chrysina gloriosa\", as well as some crustaceans such as the mantis shrimp. In these cases, the underlying mechanism is the molecular-level helicity of the chitinous cuticle.\nThe bioluminescence of the larvae of fireflies is also circularly polarized, as reported in 1980 for the species \"Photuris lucicrescens\" and \"Photuris versicolor\". For fireflies, it is more difficult to find a microscopic explanation for the polarization, because the left and right lanterns of the larvae were found to emit polarized light of opposite senses. The authors suggest that the light begins with a linear polarization due to inhomogeneities inside aligned photocytes, and it picks up circular polarization while passing through linearly birefringent tissue.\nCircular polarization has been detected in light reflected from leaves and photosynthetic microbes.\nWater-air interfaces provide another source of circular polarization. Sunlight that gets scattered back up towards the surface is linearly polarized. If this light is then totally internally reflected back down, its vertical component undergoes a phase shift. To an underwater observer looking up, the faint light outside Snell's window therefore is (partially) circularly polarized.\nWeaker sources of circular polarization in nature include multiple scattering by linear polarizers, as in the circular polarization of starlight, and selective absorption by circularly dichroic media.\nRadio emission from pulsars can be strongly circularly polarized.\nTwo species of mantis shrimp have been reported to be able to detect circular polarized light.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "40876", "revid": "7611264", "url": "https://en.wikipedia.org/wiki?curid=40876", "title": "Circulator", "text": "Electronic circuit in which a signal entering any port exits at the next port\nIn electrical engineering, a circulator is a passive, non-reciprocal three- or four-port device that only allows a microwave or radio-frequency (RF) signal to exit through the port directly after the one it entered. Optical circulators have similar behavior. Ports are where an external waveguide or transmission line, such as a microstrip line or a coaxial cable, connects to the device. For a three-port circulator, a signal applied to port 1 only comes out of port 2; a signal applied to port 2 only comes out of port 3; a signal applied to port 3 only comes out of port 1. An ideal three-port circulator thus has the following scattering matrix:\nformula_1\nTheory of operation.\nMicrowave circulators rely on the anisotropic and \"non-reciprocal\" properties of magnetized microwave ferrite material. Microwave electromagnetic waves propagating in magnetized ferrite interact with electron spins in the ferrite and are consequently influenced by the microwave magnetic permeability of the ferrite. This permeability is mathematically described by a linear vector operator, also known as a tensor. In the case of magnetized ferrite, the permeability tensor is the Polder tensor. The permeability is a function of the direction of microwave propagation relative to the direction of static magnetization of the ferrite material. Hence, microwave signals propagating in different directions in the ferrite experience different magnetic permeabilities.\nIn the CGS system, the Polder tensor is\nformula_2\nwhere (neglecting damping)\nformula_3\nformula_4\nformula_5\nwhere formula_6 is the Permeability of Free Space and formula_7 is the Absolute permittivity of the ferrite material. In a circulator, these propagation constants describe waves having Elliptical polarization that would propagate in the direction of the static magnetic bias field, which is through the thickness of the ferrite. The plus and minus subscripts of the propagation constants indicate opposite wave polarizations.\nTypes.\nMicrowave circulators fall into two main classes: differential phase shift circulators and junction circulators, both of which are based on cancellation of waves propagating over two different paths in or near magnetized ferrite material. Waveguide circulators may be of either type, while more compact devices based on stripline are usually of the junction type. Two or more junction circulators can be combined in a single component to give four or more ports. Typically permanent magnets produce a static magnetic bias in the microwave ferrite material. Ferrimagnetic garnet crystal is used in optical circulators.\nJunction circulators.\nStripline junction circulators.\nA stripline junction circulator contains a resonator, which is located at the central junction of the striplines. This resonator may have any shape that has three-fold Rotational symmetry, such as a disk, hexagon, or triangle. An RF/microwave signal entering a circulator port is connected via a stripline to the resonator, where energy is coupled into two counter-rotating circular modes formed by the elliptically polarized waves. These circular modes have different phase velocities which can cause them to combine constructively or destructively at a given port. This produces an anti-node at one port (port 2 if the signal is incident upon port 1) and a node or null at another port (port 3 if the microwave energy is coupled from port 1 to port 2 and not reflected back into port 2).\nIf losses are neglected for simplification, the counter-rotating modes must differ in phase by an integer multiple of formula_8 for signal propagation from port 1 to port 2 (or from port 2 to port 3, or from port 3 to port 1):\nformula_9\nand similarly, for the remaining port (port 3 if signal propagation is from port 1 to port 2) to be nulled,\nformula_10\nwhere formula_11 is the path length between adjacent ports and formula_12 and formula_13 are integers. Solving the two preceding equations simultaneously, for proper circulation the necessary conditions are\nformula_14\nand\nformula_15\nEach of the two counter-rotating modes has its own resonant frequency. The two resonant frequencies are known as the split frequencies. The circulator operating frequency is set between the two split frequencies.\nThese circulator types operate based on faraday rotation. Wave cancellation occurs when waves propagate with and against the circulator's direction of circulation. An incident wave arriving at any port is split equally into two waves. They propagate in each direction around the circulator with different phase velocities. When they arrive at the output port they have different phase relationships and thus combine accordingly. This combination of waves propagating at different phase velocities is how junction circulators fundamentally operate.\nThe geometry of a stripline junction circulator comprises two ferrite disks or triangles separated by a stripline center conductor and sandwiched between two parallel ground planes. A stripline circulator is essentially a stripline center conductor sandwich on ferrite, between ground planes. That is, there is one ferrite disk above the stripline circuit and one ferrite disk below the stripline circuit. Stripline circulators do not have to be constructed with disk- or triangle-shaped ferrites; the ferrites can have almost any shape that has three-way symmetry. This is also true of the resonator (the center junction portion of the center conductor)- it can be any shape that has three-way symmetry, although there are electrical considerations.\nThe ferrites are magnetized through their thicknesses, i.e., the static magnetic bias field is perpendicular to the plane of the device and the direction of signal propagation is transverse to the direction of the static magnetic field. Both ferrites are in the same static ad RF magnetic fields. The two ferrites can be thought of as one continuous ferrite with an embedded stripline center conductor. For practical manufacturing reasons, the center conductor is not generally embedded in ferrite, so two discrete ferrites are used. The static magnetic bias field is typically provided by permanent magnets that are located external to the circulator ground planes. Magnetic shielding incorporated into the circulator design prevents detuning or partial demagnetization of the circulator in the presence of external magnetic fields or ferrous materials, and protects nearby devices from the effects of the circulator's static magnetic field.\nWaveguide junction circulators.\nA waveguide junction circulator contains a magnetized ferrite resonator, which is located at the junction of three waveguides. In contrast with a stripline junction circulator, the ferrite itself is the resonator, rather than the metal central portion of a stripline center conductor. The ferrite resonator may have any shape that has three-fold Rotational symmetry, such as a cylinder or Triangular prism. The resonator is often just one ferrite, but it is sometimes composed of two or more ferrites, which may be coupled to each other, in various geometrical configurations. The geometry of the resonator is influenced by electrical and thermal performance considerations. Waveguide junction circulators function in much the same way as stripline junction circulators, and their basic theory of operation is the same.\nThe internal geometry of a waveguide junction circulator comprises a junction of three waveguides, the ferrite resonator, and impedance matching structures. Many of these circulators contain pedestals located in the central junction, on which the ferrite resonator is located. These pedestals effectively reduce the height of the waveguide, reducing its characteristic impedance in the resonator region to optimize electrical performance. The reduced-height waveguide sections leading from the resonator to the full-height waveguides serve as impedance transformers.\nThe ferrite resonator is magnetized through its height, i.e., the static magnetic bias field is perpendicular to the plane of the device and the direction of signal propagation is transverse to the direction of the static magnetic field. The static magnetic bias field is typically provided by permanent magnets that are external to the waveguide junction.\nE-Field Plots Showing Electromagnetic Wave Propagation in Waveguide Junction Circulators\nMicrostrip junction circulators.\nThe microstrip junction circulator is another widely used form of circulator that utilizes the microstrip transmission line topology. A microstrip circulator consists primarily of a circuit pattern on a ferrite substrate. The circuit is typically formed using thick-film or thin-film metallization processes, often including photolithography. The ferrite substrate is sometimes bonded to a ferrous metal carrier, which serves to improve the efficiency of the magnetic circuit, increase the mechanical strength of the circulator, and protect the ferrite from thermal expansion mismatches between it and the surface to which the circulator is mounted. A permanent magnet that is bonded to the circuit face of the ferrite substrate provides the static magnetic bias to the ferrite. Microstrip circulators function in the same way as stripline junction circulators, and their basic theory of operation is the same. In comparison with stripline circulators, electrical performance of microstrip circulators is somewhat reduced because of radiation and dispersion effects.\nThe performance disadvantages of microstrip circulators are offset by their relative ease of integration with other planar circuitry. The electrical connections of these circulators to adjacent circuitry are typically made using wire bonds or ribbon bonds. Another advantage of microstrip circulators is their smaller size and correspondingly lower mass than stripline circulators. Despite this advantage, microstrip circulators are often the largest components in microwave modules.\nSelf-biased junction circulators.\nSelf-biased junction circulators are unique in that they do not utilize permanent magnets that are separate from the microwave ferrite. The elimination of external magnets significantly reduces the size and weight of the circulator compared to electrically-equivalent microstrip junction circulators for similar applications.\nMonolithic ferrites that are used for self-biased circulators are M-type uniaxial (single magnetic axis) hexagonal ferrites that have been optimized to have low microwave losses. In contrast with the magnetically soft (low-coercivity) ferrites used in other circulators, the hexagonal ferrites used for self-biased circulators are magnetically hard (high-coercivity) materials. These ferrites are essentially ceramic permanent magnets. In addition to their high magnetic remanence, these ferrites have very large magnetic anisotropy fields, enabling circulator operation up to high microwave frequencies.\nBecause of their thin, planar shape, self-biased circulators can be conveniently integrated with other planar circuitry. Integration of self-biased circulators with semiconductor wafers has been demonstrated at KA-band and V-band frequencies.\nLumped-element circulators.\nLumped-element circulators are small-size devices that are typically used at frequencies in the HF through UHF bands. In a junction circulator, the size of the ferrite(s) is proportional to signal wavelength, but in a lumped-element circulator, the ferrite can be smaller because there is no such wavelength proportionality.\nIn a lumped-element circulator, conductors are wrapped around the ferrite, forming what is typically a woven mesh. The conductor strips are insulated from each other by thin dielectric layers. In some circulators, the mesh is in the form of traces on a printed wiring board with metallized vias to make connections between layers. The conductive strips can be thought of as non-reciprocally coupled inductors. Impedance matching circuitry and broad-banding circuitry in lumped-element circulators are often constructed using discrete ceramic capacitors and air-core inductors.\nThis class of circulator offers a considerable size reduction compared with the junction circulators. On the other hand, lumped-element circulators generally have lower RF power handling capacity than equivalent junction devices and are more complex from a mechanical perspective. The discrete lumped-element inductors and capacitors can be less stable when exposed to vibration or mechanical shocks than the simple distributed impedance transformers in a stripline junction circulator.\nSwitching circulators.\nSwitching circulators are similar to other junction circulators, and their microwave theory of operation is the same, except that their direction of circulation can be electronically controlled.\nJunction circulators use permanent magnets to provide the static magnetic bias for the ferrite(s). However, switching circulators typically rely on the remanent magnetization of the ferrite itself. The ferrites that are used in switching circulators have square magnetic hysteresis loops and often sub-Oersted coercivities. Such a ferrite material requires a relatively small magnetic field and low energy level to flip its magnetic polarity. This is distinctly advantageous for a switching circulator, but the absence of permanent magnets would be a disadvantage of a non-switching junction circulator that must retain its magnetic bias despite exposures to the potentially demagnetizing effects of stray magnetic fields, nearby ferrous materials, and temperature variations.\nThe magnetization polarity of the ferrite, and hence the direction of circulation of a switching circulator, is controlled using a magnetizing coil that loops through the ferrite. The coil is connected to electronic driver circuitry that sends current pulses of the correct polarity through the magnetizing coil to magnetize the ferrite in the polarity to provide the desired direction of circulation.\nDifferential phase shift circulators.\nDifferential phase shift circulators are mainly used in high power microwave applications. They are usually built from rectangular waveguide components. These circulators are 4-port devices having circulation in the sequence 1 - 2 - 3 - 4 - 1, with ports numbered as shown in the schematic. There are various feasible circulator architectures, the most common of which utilizes a magic tee hybrid coupler, a quadrature hybrid coupler, and two oppositely magnetized differential phase shifters.\nA differential phase shifter provides \"non-reciprocal\" transmission phase shift. That is, the forward phase shift is different from the phase shift in the reverse transmission direction. It is this difference in phase shifts that enables the non-reciprocal behavior of the circulator. A differential phase shifter consists of one or more ferrite slabs, usually positioned on the broad wall(s) of the waveguide. Permanent magnets located outside the waveguide provide static magnetic bias field to the ferrite(s). The ferrite-loaded waveguide is another example of a \"transverse-field\" device as described in . Different microwave propagation constants corresponding to different directions of signal propagation give rise to different phase velocities and hence, different transmission phase shifts.\nDepending on which circulator port an incident signal enters, phase shift relationships in the hybrid couplers and the differential phase shifts cause signals to combine at one other port and cancel at each of the remaining two ports. Differential phase shift circulators are often used as 3-port circulators by connecting one circulator port to a reflectionless termination, or they can be used as isolators by terminating two circulator ports.\nNon-ferrite circulators.\nThough ferrite circulators can provide good \"forward\" signal circulation while suppressing greatly the \"reverse\" circulation, their major shortcomings, especially at low frequencies, are the bulky sizes and the narrow bandwidths.\nEarly work on non-ferrite circulators includes active circulators using transistors that are non-reciprocal in nature. In contrast to ferrite circulators which are passive devices, active circulators require power. Major issues associated with transistor-based active circulators are the power limitation and the signal-to-noise degradation, which are critical when it is used as a duplexer for sustaining the strong transmit power and clean reception of the signal from the antenna.\nVaractors offer one solution. One study employed a structure similar to a time-varying transmission line with the effective nonreciprocity triggered by a one-direction propagating carrier pump. This is like an AC-powered active circulator. The research claimed to be able to achieve positive gain and low noise for receiving path and broadband nonreciprocity. Another study used resonance with nonreciprocity triggered by angular-momentum biasing, which more closely mimics the way that signals passively circulate in a ferrite circulator.\nIn 1964, Mohr presented and experimentally demonstrated a circulator based on transmission lines and switches. In April, 2016 a research team significantly extended this concept, presenting an integrated circuit circulator based on N-path filter concepts. It offers the potential for full-duplex communication (transmitting and receiving at the same time with a single shared antenna over a single frequency). The device uses capacitors and a clock and is much smaller than conventional devices.\nApplications.\nIsolator.\nWhen one port of a three-port circulator is terminated in a matched load, it can be used as an \"isolator\", since a signal can travel in only one direction between the remaining ports. An isolator is used to shield equipment on its input side from the effects of conditions on its output side; for example, to prevent a microwave source being detuned by a mismatched load.\nDuplexer.\nIn radar, circulators are used as a type of duplexer, to route signals from the transmitter to the antenna and from the antenna to the receiver, without allowing signals to pass directly from transmitter to receiver. The alternative type of duplexer is a \"transmit-receive switch\" (\"TR switch\") that alternates between connecting the antenna to the transmitter and to the receiver. The use of chirped pulses and a high dynamic range may lead to temporal overlap of the sent and received pulses, however, requiring a circulator for this function.\nReflection amplifier.\nA reflection amplifier is a type of microwave amplifier circuit utilizing negative differential resistance diodes such as tunnel diodes and Gunn diodes. Negative differential resistance diodes can amplify signals, and often perform better at microwave frequencies than two-port devices. However, since the diode is a one-port (two terminal) device, a nonreciprocal component is needed to separate the outgoing amplified signal from the incoming input signal. By using a 3-port circulator with the signal input connected to one port, the biased diode connected to a second, and the output load connected to the third, the output and input can be uncoupled.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "40877", "revid": "22045319", "url": "https://en.wikipedia.org/wiki?curid=40877", "title": "Cladding", "text": "Cladding is an outer layer of material covering another. It may refer to the following:\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "40878", "revid": "6289403", "url": "https://en.wikipedia.org/wiki?curid=40878", "title": "Cladding mode", "text": ""}
{"id": "40879", "revid": "43537120", "url": "https://en.wikipedia.org/wiki?curid=40879", "title": "Clearing", "text": "Clearing or The Clearing may refer to:\n&lt;templatestyles src=\"Template:TOC_right/styles.css\" /&gt;\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "40880", "revid": "13054498", "url": "https://en.wikipedia.org/wiki?curid=40880", "title": "Clear message", "text": ""}
{"id": "40881", "revid": "50049053", "url": "https://en.wikipedia.org/wiki?curid=40881", "title": "Thomas L. Cleave", "text": "Thomas Latimer (Peter) Cleave (1906\u20131983) was a surgeon captain who researched the negative health effects of consuming refined carbohydrate (notably sugar and white flour) which would not have been available during early human evolution. Known as 'Peter' to his friends and colleagues, Cleave was born in Exeter in 1906, and educated at Clifton College. Between 1922 and 1927, he attended medical schools at the Bristol Royal Infirmary, and St Mary's Hospital, London, where he was an academic prodigy winning prize after prize and qualifying at the early age of 21, having passed his primary FRCS examination at the age of 18 and ultimately achieving MRCS and LRCP.\nAt Bristol, one of his teachers was Rendle Short, who had proposed that appendicitis is caused by a lack of cellulose in the diet. Charles Darwin's writings provided the intellectual framework to Cleave's lifelong engagement with the relationship between diet and health, built upon the premise that the human body is ill-adapted to the diet of modern (western) man.\nCleave's interest focussed on preventative medicine where he observed the harmful effects of the overconsumption of refined carbohydrates such as sugar and refined flour which he called 'The Saccharine Disease'. He noticed that the saccharine manifestations did not occur in wild creatures or among primitive people living on traditional unrefined food.\nHe considered refined carbohydrates (white flour and sugar) to be the most transformed food, and therefore the most dangerous. After completing his medical training, Cleave entered the Royal Navy in 1927 as Surgeon Lieutenant.\nMilitary career.\nBetween 1938 and 1940, he served as Medical Specialist at RN Hospital, Hong Kong. It was during his war service, in 1941, whilst on the battleship King George V, that he acquired his naval nickname 'the bran man' when he had sacks of bran brought on board to combat the common occurrence of constipation amongst sailors. Cleave's intention was 'to give them bowel movements as efficient as the guns they fired.' The ship assisted in the sinking of the Bismarck. Cleave was on the bridge as the Bismarck was going down and a fellow officer exclaimed to him 'Well done Doc, you deserve a medal, our bowels were working like clockwork!\u2019\nFollowing war service, he worked at Royal Naval Hospitals in Chatham (1945\u20131948), Malta (1949\u20131951) and Plymouth (1952\u20131953). He retired from the Royal Navy in 1962 as Surgeon Captain, having finished his naval career as Director of Medical Research at the RN Medical School\nPost-military career.\nCleaves unique contribution to medical thought was his realisation that three mechanisms were at work when refined carbohydrates are eaten; fibre depletion, over-consumption and protein stripping, with over-consumption being the most serious.\nIn 1969, Dr. Cleave brought public attention to the low amount of dietary fiber in modern diets that had become rich in processed ingredients. His work was bolstered by the supporting work of Dr. Denis Burkitt.\nAwards and honors.\nIn 1976 Cleave was elected a Fellow of the Royal College of Physicians. In 1979 he was awarded both the Harben gold medal of the Royal Institute of Public Health and Hygiene and the Gilbert Blane medal for naval medicine by the Royal College of Physicians and Surgeons. Dr. Cleave was a 2009 inductee into the Orthomolecular Medicine Hall of Fame.\nSelected publications.\nCleave published during his lifetime:\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "40882", "revid": "1296936583", "url": "https://en.wikipedia.org/wiki?curid=40882", "title": "Clipping", "text": "Clipping may refer to:\n&lt;templatestyles src=\"Template:TOC_right/styles.css\" /&gt;\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "40883", "revid": "50818167", "url": "https://en.wikipedia.org/wiki?curid=40883", "title": "Closed captioning", "text": "Process of displaying interpretive texts to screens\nClosed captioning (CC) is the process of displaying text on a television, video screen, or other visual display to provide additional or interpretive information, where the viewer is given the choice of whether the text is displayed. Closed captions are typically used as a transcription of the audio portion of a program as it occurs (either verbatim or in edited form), sometimes including descriptions of non-speech elements. Other uses have included providing a textual alternative language translation of a presentation's primary audio language that is usually burned in (or \"open\") to the video and unselectable.\nHTML5 defines subtitles as a \"transcription or translation of the dialogue when sound is available but not understood\" by the viewer (for example, dialogue in a foreign language) and captions as a \"transcription or translation of the dialogue, sound effects, relevant musical cues, and other relevant audio information when sound is unavailable or not clearly audible\" (for example, when audio is muted or the viewer is deaf or hard of hearing).\nTerminology.\nThe term \"closed\" indicates that the captions are not visible until activated by the viewer, usually via the remote control or menu option. On the other hand, the terms \"open\", \"burned-in\", \"baked on\", \"hard-coded\", or simply \"hard\" indicate that the captions are visible to all viewers as they are embedded in the video.\nIn the United States and Canada, the terms \"subtitles\" and \"captions\" have different meanings. \"Subtitles\" assume the viewer can hear but cannot understand the language or accent, or the speech is not entirely clear, so they transcribe only dialogue and some on-screen text. \"Captions\" aim to describe to the deaf and hard of hearing all significant audio content\u2014spoken dialogue and non-speech information such as the identity of speakers and, occasionally, their manner of speaking\u2014along with any significant music or sound effects using words or symbols. Also, the term \"closed caption\" has come to be used to also refer to the North American EIA-608 encoding that is used with NTSC-compatible video.\nThe United Kingdom, Ireland, and a number of other countries do not distinguish between subtitles and captions and use \"subtitles\" as the general term. The equivalent of \"captioning\" is usually referred to as \"subtitles for the hard of hearing\". Their presence is referenced on screen by notation which says \"Subtitles\", or previously \"Subtitles 888\" or just \"888\" (the latter two are in reference to the conventional videotext channel for captions), which is why the term \"subtitle\" is also used to refer to the Ceefax-based videotext encoding that is used with PAL-compatible video. The term \"subtitle\" has been replaced with \"caption\" in a number of markets\u2014such as Australia and New Zealand\u2014that purchase large amounts of imported US material, with much of that video having had the US CC logo already superimposed over the start of it. In New Zealand, broadcasters superimpose an ear logo with a line through it that represents subtitles for the hard of hearing, even though they are currently referred to as captions. In the UK, modern digital television services have subtitles for the majority of programs, so it is no longer necessary to highlight which have subtitling/captioning and which do not.\nRemote control handsets for TVs, DVD players, and similar devices in most European markets often use \"SUB\" or \"SUBTITLE\" on the button used to control the display of subtitles and captions.\nHistory.\nOpen captioning.\nRegular open-captioned broadcasts began on PBS's \"The French Chef\" in 1972. WGBH began open captioning of the programs \"Zoom\", \"ABC World News Tonight\", and \"Once Upon a Classic\" shortly thereafter.\nTechnical development of closed captioning.\nClosed captioning was first demonstrated in the United States at the First National Conference on Television for the Hearing Impaired at the University of Tennessee in Knoxville, Tennessee, in December 1971. A second demonstration of closed captioning was held at Gallaudet College (now Gallaudet University) on February 15, 1972, where ABC and the National Bureau of Standards demonstrated closed captions embedded within a normal broadcast of \"The Mod Squad\".\nAt the same time in the UK the BBC was demonstrating its Ceefax text based broadcast service which they were already using as a foundation to the development of a closed caption production system. They were working with professor Alan Newell from the University of Southampton who had been developing prototypes in the late 1960s which became part of the core specification of the Teletext platform.\nThe closed captioning system was successfully encoded and broadcast in 1973 with the cooperation of PBS station WETA. As a result of these tests, the FCC in 1976 set aside Line 21 for the transmission of closed captions. PBS engineers then developed the caption editing consoles that would be used to caption prerecorded programs.\nThe BBC in the UK was the first broadcaster to include closed captions (called subtitles in the UK) in 1979 based on the Teletext framework for pre-recorded programming.\nReal-time captioning.\nReal-time captioning, a process for captioning live broadcasts, was developed by the National Captioning Institute in 1982. As developed in 1992, real-time captioning used stenotype operators who are able to type at speeds of up to 375 words per minute provide captions for live television programs, allowing the viewer to see the captions within two to three seconds of the words being spoken.\nImprovements in speech recognition technology mean that live captioning may be fully or partially automated. BBC Sport broadcasts use a \"respeaker\": a trained human who repeats the running commentary (with careful enunciation and some simplification and markup) for input to the automated text generation system. This is generally reliable, though errors are not unknown.\nIn the 1980s, DARPA sponsored a number of projects aimed at developing automatic speech recognition software. Much of this work was done by researchers at Carnegie Mellon University. In the 1990s, this program included a novel focus of using this technology for news transcription purposes. Later developments have yielded live, real-time AI-based captioning generating systems.\nFull-scale closed captioning.\nThe National Captioning Institute was created in 1979 in order to get the cooperation of the commercial television networks.\nThe first use of regularly scheduled closed captioning on American television occurred on March 16, 1980. PBS developed the line-21 decoder, a decoding unit that could be connected to a standard television set. This was sold commercially by Sears under the name Telecaption. The first programs seen with captioning were a \"Disney's Wonderful World\" presentation of the film \"Son of Flubber\" on NBC, an \"ABC Sunday Night Movie\" airing of \"Semi-Tough\", and \"Masterpiece Theatre\" on PBS.\nSince 2010 the BBC has provided captioning for all programming across all seven of its main broadcast channels BBC One, BBC Two, BBC Three, BBC Four, CBBC, CBeebies and BBC News.\nBBC iPlayer was launched in 2008 as the first captioned video-on-demand service from a major broadcaster with levels of captioning comparable to those provided on its broadcast channels.\nLegislative development in the U.S..\nUntil the passage of the Television Decoder Circuitry Act of 1990, television captioning was performed by a set-top box manufactured by Sanyo Electric and marketed by the National Captioning Institute (NCI). (At that time a set-top decoder cost about as much as a TV set itself, approximately $200.) Through discussions with the manufacturer it was established that the appropriate circuitry integrated into the television set would be less expensive than the stand-alone box, and Ronald May, then a Sanyo employee, provided the expert witness testimony on behalf of Sanyo and Gallaudet University in support of the passage of the bill. On January 23, 1991, the Television Decoder Circuitry Act of 1990 was passed by Congress. This Act gave the Federal Communications Commission (FCC) power to enact rules on the implementation of closed captioning. This Act required all analog television receivers with screens of at least 13\u00a0inches or greater, either sold or manufactured, to have the ability to display closed captioning by July 1, 1993.\nAlso, in 1990, the Americans with Disabilities Act (ADA) was passed to ensure equal opportunity for persons with disabilities. The ADA prohibits discrimination against persons with disabilities in public accommodations or commercial facilities. Title III of the ADA requires that public facilities\u2014such as hospitals, bars, shopping centers and museums (but not movie theaters)\u2014provide access to verbal information on televisions, films and slide shows.\nThe Federal Communications Commission requires all providers of programs to caption material which has audio in English or Spanish, with certain exceptions specified in Section 79.1(d) of the commission's rules. These exceptions apply to new networks; programs in languages other than English or Spanish; networks having to spend over 2% of income on captioning; networks having less than US$3,000,000 in revenue; and certain local programs; among other exceptions. Those who are not covered by the exceptions may apply for a hardship waiver.\nThe Telecommunications Act of 1996 expanded on the Decoder Circuitry Act to place the same requirements on digital television receivers by July 1, 2002. All TV programming distributors in the U.S. are required to provide closed captions for Spanish-language video programming as of January 1, 2010.\nA bill, H.R. 3101, the Twenty-First Century Communications and Video Accessibility Act of 2010, was passed by the United States House of Representatives in July 2010. A similar bill, S. 3304, with the same name, was passed by the United States Senate on August 5, 2010 and by the House of Representatives on September 28, 2010, and was signed by President Barack Obama on October 8, 2010. The Act requires, in part, any ATSC-decoding set-top box remote to have a button to turn the closed captioning in the output signal on or off. It also requires broadcasters to provide captioning for television programs redistributed on the Internet.\nOn February 20, 2014, the FCC unanimously approved the implementation of quality standards for closed captioning, addressing accuracy, timing, completeness, and placement. This is the first time the FCC has addressed quality issues in captions.\nIn 2015, a law was passed in Hawaii requiring two screenings a week of each movie with captions on the screen. In 2022 a law took effect in New York City requiring movie theaters to offer captions on the screen for up to four showtimes per movie each week, including weekends and Friday nights.\nSome state and local governments (including Boston, Massachusetts; Portland, Oregon; Rochester, New York; and the State of Washington) require closed captioning to be activated on TVs in public places at all times, even if no one has requested it.\nPhilippines.\nAs amended by RA 10905, all TV networks in the Philippines are required to provide closed captions. As of 2018, the three major TV networks in the country are currently testing the closed captioning system on their transmissions. ABS-CBN added closed captions in their daily \"3 O'Clock Habit\" in the afternoon. TV5 started implementing closed captions on their live noon and nightly news programs. GMA once broadcast news programs with closed captions but since stopped. Only select Korean drama and local or foreign movies, \" Drew\" (English title \"Drew's Travel Adventure\") and \"\" (English title \"Kitchen Idol\") are broadcast with proper closed captioning.\nSince 2016 all Filipino-language films, as well as some streaming services, like iWant, have included English subtitles in some showings. The law regarding this was proposed by Gerald Anthony Gullas Jr., a lawmaker from Cebu City, who had implemented the regulations on standardizing both official languages of the Philippines, as the people had not mastered English vocabulary.\nLegislative development in Australia.\nThe government of Australia provided seed funding in 1981 for the establishment of the Australian Caption Centre (ACC) and the purchase of equipment. Captioning by the ACC commenced in 1982 and a further grant from the Australian government enabled the ACC to achieve and maintain financial self-sufficiency. The ACC, now known as Media Access Australia, sold its commercial captioning division to Red Bee Media in December 2005. Red Bee Media continues to provide captioning services in Australia today.\nFunding development in New Zealand.\nIn 1981, TVNZ held a telethon to raise funds for Teletext-encoding equipment used for the creation and editing of text-based broadcast services for the deaf. The service came into use in 1984 with caption creation and importing paid for as part of the public broadcasting fee until the creation of the NZ On Air taxpayer fund, which is used to provide captioning for NZ On Air content and TVNZ news shows and for conversion of EIA-608 US captions to the preferred EBU STL format for only TVNZ 1, TV 2 and TV 3 with archived captions available to FOUR and select Sky programming. During the second half of 2012, TV3 and FOUR began providing non-Teletext DVB image-based captions on their HD service and used the same format on the satellite service, which has since caused major timing issues in relation to server load and the loss of captions from most SD DVB-S receivers, such as the ones Sky Television provides their customers. As of April 2, 2013, only the Teletext page 801 caption service will remain in use with the informational Teletext non-caption content being discontinued.\nApplication.\nClosed captions were created for deaf and hard of hearing individuals to assist in comprehension. They can also be used as a tool by those learning to read, or those learning to speak a non-native language, or in environments where the audio is difficult to hear or is intentionally muted. Captions can also be used by viewers who simply wish to read a transcript along with the program audio. \nIn the United States, the National Captioning Institute noted that English as a foreign or second language (ESL) learners were the largest group buying decoders in the late 1980s and early 1990s before built-in decoders became a standard feature of US television sets. This suggested that the largest audience of closed captioning was people whose native language was not English. In the United Kingdom, of 7.5 million people using TV subtitles (closed captioning), 6 million have no hearing impairment.\nClosed captions are also used in public environments, such as bars and restaurants, where patrons may not be able to hear over the background noise or where several televisions are displaying different programs. In addition, online videos may be treated through digital processing of their audio content by various robotic algorithms (robots). Several chains of errors are the result. When a video is truly and accurately transcribed, then the closed-captioning publication serves a useful purpose, and the content is available for search engines to index and make available to users on the Internet.\nSome television sets can be set to turn captioning on automatically when the volume is muted.\nTelevision and video.\nFor live programs, spoken words comprising the television program's soundtrack are transcribed by a human operator (a speech-to-text reporter) using stenotype- or stenomask-type machines, the phonetic output of which is instantly translated into text by a computer and displayed on the screen. This technique was developed in the 1970s as an initiative of the BBC's Ceefax teletext service. In collaboration with the BBC, a university student took on the research project of writing the first phonetics-to-text conversion program for this purpose. Sometimes the captions of live broadcasts, such as news bulletins, sports events, live entertainment shows and other live shows, fall behind by a few seconds. This delay is because the machine does not know what the person is going to say next, so after the person on the show says the sentence, the captions appear. Automatic computer speech recognition works well when trained to recognize a single voice, and so since 2003 the BBC has done live subtitling by having someone respeak what is being broadcast. Live captioning is also a form of real-time text. Meanwhile, sport events on ESPN are using court reporters, using a special (steno) keyboard and individually constructed \u2018dictionaries\u2019.\nIn some cases the transcript is available beforehand and captions are simply displayed during the program after being edited. For programs that have a mix of prepared and live content, such as news bulletins, a combination of techniques is used.\nFor prerecorded programs, commercials and home videos, audio is transcribed and captions are prepared, positioned and timed in advance.\nFor all types of NTSC programming, captions are encoded into Line 21 of the vertical blanking interval\u00a0\u2013 a part of the TV picture that sits just above the visible portion and is usually unseen. For ATSC (digital television) programming, three streams are encoded in the video: two are backward-compatible \"Line 21\" captions, and the third is a set of up to 63 additional caption streams encoded in EIA-708 format.\nCaptioning is modulated and stored differently in PAL and SECAM countries (625 lines, 50 fields per second), where teletext is used rather than in EIA-608, but the methods of preparation and the Line 21 field used are similar. For home Beta and VHS videotapes, a shift down of this Line 21 field must be done due to the greater number of VBI lines used in 625 line PAL countries, though only a small minority of European PAL VHS machines support this (or any) format for closed caption recording. Like all teletext fields, teletext captions can not be stored by a standard 625 line VHS recorder (due to the lack of field shifting support); they are available on all professional S-VHS recordings due to all fields being recorded. Recorded Teletext caption fields also suffer from a higher number of caption errors due to increased number of bits and a low signal-to-noise ratio, especially on low-bandwidth VHS. This is why Teletext captions were stored on floppy disk, separate from the analogue master tape. DVDs have their own system for subtitles and captions, which are digitally inserted in the data stream and decoded on playback into video.\nFor older televisions, a set-top box or other decoder is usually required. In the US, since the passage of the Television Decoder Circuitry Act, manufacturers of most television receivers sold have been required to include closed captioning display capability. High-definition TV sets, receivers, and tuner cards are also covered, though the technical specifications are different (high-definition display screens, as opposed to high-definition TVs, may lack captioning). Canada has no similar law but receives the same sets as the US in most cases.\nDuring transmission, single byte errors can be replaced by a white space which can appear at the beginning of the program. More byte errors during EIA-608 transmission can affect the screen momentarily, by defaulting to a real-time mode such as the \"roll up\" style, type random letters on screen, and then revert to normal. Uncorrectable byte errors within the teletext page header will cause whole captions to be dropped. EIA-608, due to using only two characters per video frame, sends these captions ahead of time storing them in a second buffer awaiting a command to display them; Teletext sends these in real-time.\nThe use of capitalization varies among caption providers. Most caption providers capitalize all words while others such as WGBH and non-American providers prefer to use mixed-case letters.\nThere are two main styles of Line 21 closed captioning:\nCaption formatting.\nTVNZ Access Services and Red Bee Media for BBC and Australia example:\nUK IMS for ITV and Sky example:\nUS WGBH Access Services example:\nUS National Captioning Institute example:\nUS CaptionMax example:\nUS in-house real-time roll-up example:\nNon-US in-house real-time roll-up example:\nUS VITAC example:\nSyntax.\nFor real-time captioning done outside of captioning facilities, the following syntax is used:\nStyles of syntax that are used by various captioning producers:\nTechnical aspects.\nThere were many shortcomings in the original Line 21 specification from a typographic standpoint, since, for example, it lacked many of the characters required for captioning in languages other than English. Since that time, the core Line 21 character set has been expanded to include quite a few more characters, handling most requirements for languages common in North and South America such as French, Spanish, and Portuguese, though those extended characters are not required in all decoders and are thus unreliable in everyday use. The problem has been almost eliminated with a market specific full set of Western European characters and a private adopted Norpak extension for South Korean and Japanese markets. The full EIA-708 standard for digital television has worldwide character set support, but there has been little use of it due to EBU Teletext dominating DVB countries, which has its own extended character sets.\nCaptions are often edited to make them easier to read and to reduce the amount of text displayed onscreen. This editing can be very minor, with only a few occasional unimportant missed lines, to severe, where virtually every line spoken by the actors is condensed. The measure used to guide this editing is words per minute, commonly varying from 180 to 300, depending on the type of program. Offensive words are also captioned, but if the program is censored for TV broadcast, the broadcaster might not have arranged for the captioning to be edited or censored also. The \"TV Guardian\", a television set-top box, is available to parents who wish to censor offensive language of programs\u2014the video signal is fed into the box and if it detects an offensive word in the captioning, the audio signal is bleeped or muted for that period of time.\nCaption channels.\nThe Line 21 data stream can consist of data from several data channels multiplexed together. Odd field 1 can have four data channels: two separate synchronized captions (CC1, CC2) with caption-related text, such as website URLs (T1, T2). Even field 2 can have five additional data channels: two separate synchronized captions (CC3, CC4) with caption related text (T3, T4), and Extended Data Services (XDS) for Now/Next EPG details. XDS data structure is defined in CEA-608.\nAs CC1 and CC2 share bandwidth, if there is a lot of data in CC1, there will be little room for CC2 data and is generally only used for the primary audio captions. Similarly, CC3 and CC4 share the second even field of line 21. Since some early caption decoders supported only single field decoding of CC1 and CC2, captions for SAP in a second language were often placed in CC2. This led to bandwidth problems, and the U.S. Federal Communications Commission (FCC) recommendation is that bilingual programming should have the second caption language in CC3. Many Spanish television networks such as Univision and Telemundo, for example, offer English subtitles for many of its Spanish programs in CC3. Conversely, the now-defunct Qubo Channel used CC3 to provide Spanish subtitles for numerous English programs. Canadian broadcasters use CC3 for French translated SAPs, which is also a similar practice in South Korea and Japan.\nCeefax and Teletext can have a larger number of captions for other languages due to the use of multiple VBI lines. However, only European countries used a second subtitle page for second language audio tracks where either the NICAM dual mono or Zweikanalton were used.\nDigital television interoperability issues.\nThe US ATSC digital television system originally specified two different kinds of \"closed captioning\" datastream standards: the original analog-compatible (available by Line 21) and the more modern digital-only CEA-708 formats are delivered within the video stream. The US FCC mandates that broadcasters deliver (and generate, if necessary) both datastream formats with the CEA-708 format merely a conversion of the Line 21 format. The Canadian CRTC has not mandated that broadcasters either broadcast both datastream formats or exclusively in one format. Most broadcasters and networks to avoid large conversion cost outlays just provide EIA-608 captions along with a transcoded CEA-708 version encapsulated within CEA-708 packets.\nIncompatibility issues with digital TV.\nMany viewers find that when they acquire a digital television or set-top box they are unable to view closed caption (CC) information, even though the broadcaster is sending it and the TV is able to display it.\nOriginally, CC information was included in the picture (\"line 21\") via a composite video input, but there is no equivalent capability in digital video interconnects (such as DVI and HDMI) between the display and a \"source\". A \"source\", in this case, can be a DVD player or a terrestrial or cable digital television receiver. When CC information is encoded in the MPEG-2 data stream, only the device that decodes the MPEG-2 data (a source) has access to the \"closed caption\" information; there is no standard for transmitting the CC information to a display monitor separately. Thus, if there is CC information, the source device needs to overlay the CC information on the picture prior to transmitting to the display over the interconnect's video output.\nThe responsibility of decoding the CC information and overlaying onto the visible video image has been taken away from the TV display and put into the \"source\" of DVI and HDMI digital video interconnects. Because the TV handles \"mute\" and, when using DVI and HDMI, a different device handles turning on and off CC, this means the \"captions come on automatically when the TV is muted\" feature no longer works. That source device\u2014such as a DVD player or set-top box\u2014must \"burn\" the image of the CC text into the picture data carried by the HDMI or DVI cable; there's no other way for the CC text to be carried over the HDMI or DVI cable.\nMany source devices do not have the ability to overlay CC information, for controlling the CC overlay can be complicated. For example, the Motorola DCT-5xxx and -6xxx cable set-top receivers have the ability to decode CC information located on the MPEG-2 stream and overlay it on the picture, but turning CC on and off requires turning off the unit and going into a special setup menu (it is not on the standard configuration menu and it cannot be controlled using the remote). Historically, DVD players, VCRs and set-top tuners did not need to do this overlaying, since they simply passed this information on to the TV, and they are not mandated to perform this overlaying.\nMany modern digital television receivers can be directly connected to cables, but often cannot receive scrambled channels that the user is paying for. Thus, the lack of a standard way of sending CC information between components, along with the lack of a mandate to add this information to a picture, results in CC being unavailable to many hard-of-hearing and deaf users.\nThe EBU Ceefax-based teletext systems are the source for closed captioning signals, thus when teletext is embedded into DVB-T or DVB-S the closed captioning signal is included. However, for DVB-T and DVB-S, it is not necessary for a teletext page signal to also be present (ITV1, for example, does not carry analogue teletext signals on Sky Digital, but does carry the embedded version, accessible from the \"Services\" menu of the receiver, or more recently by turning them off/on from a mini menu accessible from the \"help\" button).\nThe BBC's Subtitle (Captioning) Editorial Guidelines were born out of the capabilities of Teletext but are now used by multiple European broadcasters as the editorial and design best practice guide\nNew Zealand.\nIn New Zealand, captions use an EBU Ceefax-based teletext system on DVB broadcasts via satellite and cable television with the exception of MediaWorks New Zealand channels who completely switched to DVB RLE subtitles in 2012 on both Freeview satellite and UHF broadcasts, this decision was made based on the TVNZ practice of using this format on only DVB UHF broadcasts (aka Freeview HD). This made composite video connected TVs incapable of decoding the captions on their own. Also, these pre-rendered subtitles use classic caption style opaque backgrounds with an overly large font size and obscure the picture more than the more modern, partially transparent backgrounds.\nDigital television standard captioning improvements.\nThe CEA-708 specification provides for dramatically improved \"captioning\"\nAs of 2009, most closed captioning for digital television environments is done using tools designed for analog captioning (working to the CEA-608 NTSC specification rather than the CEA-708 ATSC specification). The captions are then run through transcoders made by companies like EEG Enterprises or Evertz, which convert the analog Line 21 caption format to the digital format. This means that none of the CEA-708 features are used unless they were also contained in CEA-608.\nUses in other media.\nDVDs and Blu-ray Discs.\nNTSC DVDs may carry closed captions in data packets of the MPEG-2 video streams inside of the Video-TS folder. Once played out of the analog outputs of a set top DVD player, the caption data is converted to the Line 21 format. They are output by the player to the composite video (or an available RF connector) for a connected TV's built-in decoder or a set-top decoder as usual. They can not be output on S-Video or component video outputs due to the lack of a colorburst signal on Line 21. (Actually, regardless of this, if the DVD player is in interlaced rather than progressive mode, closed captioning will be displayed on the TV over component video input if the TV captioning is turned on and set to CC1.) When viewed on a personal computer, caption data can be viewed by software that can read and decode the caption data packets in the MPEG-2 streams of the DVD-Video disc. Windows Media Player (before Windows 7) in Vista supported only closed caption channels 1 and 2 (not 3 or 4). Apple's DVD Player does not have the ability to read and decode Line 21 caption data which are recorded on a DVD made from an over-the-air broadcast. It can display some movie DVD captions.\nIn addition to Line 21 closed captions, video DVDs may also carry subtitles, which generally rendered from the EIA-608 captions as a bitmap overlay that can be turned on and off via a set top DVD player or DVD player software, just like the textual captions. This type of captioning is usually carried in a subtitle track labeled either \"English for the hearing impaired\" or, more recently, \"SDH\" (subtitled for the deaf and Hard of hearing). Many popular Hollywood DVD-Videos can carry both subtitles and closed captions (e.g. \"Stepmom\" DVD by Columbia Pictures). On some DVDs, the Line 21 captions may contain the same text as the subtitles; on others, only the Line 21 captions include the additional non-speech information (even sometimes song lyrics) needed for deaf and hard-of-hearing viewers. European Region 2 DVDs do not carry Line 21 captions, and instead list the subtitle languages available\u2014English is often listed twice, one as the representation of the dialogue alone, and a second subtitle set which carries additional information for the deaf and hard-of-hearing audience. (Many deaf/HOH subtitle files on DVDs are reworkings of original teletext subtitle files.)\nBlu-ray media typically cannot carry any VBI data such as Line 21 closed captioning due to the design of DVI-based High-Definition Multimedia Interface (HDMI) specifications that was only extended for synchronized digital audio replacing older analog standards, such as VGA, S-Video, component video, and SCART. However, a few early titles from 20th Century Fox Home Entertainment carried Line 21 closed captions that are output when using the analog outputs (typically composite video) of a few Blu-ray players. Both Blu-ray and DVD can use either PNG bitmap subtitles or 'advanced subtitles' to carry SDH type subtitling, the latter being an XML-based textual format which includes font, styling and positioning information as well as a unicode representation of the text. Advanced subtitling can also include additional media accessibility features such as \"descriptive audio\".\nMovies.\nThere are several competing technologies used to provide captioning for movies in theaters. Cinema captioning falls into the categories of open and closed. The definition of \"closed\" captioning in this context is different from television, as it refers to any technology that allows as few as one member of the audience to view the captions.\nOpen captioning in a film theater can be accomplished through burned-in captions, projected text or bitmaps, or (rarely) a display located above or below the movie screen. Typically, this display is a large LED sign. In a digital theater, open caption display capability is built into the digital projector. Closed caption capability is also available, with the ability for 3rd-party closed caption devices to plug into the digital cinema server.\nProbably the best known closed captioning option for film theaters is the Rear Window Captioning System from the National Center for Accessible Media. Upon entering the theater, viewers requiring captions are given a panel of flat translucent glass or plastic on a gooseneck stalk, which can be mounted in front of the viewer's seat. In the back of the theater is an LED display that shows the captions in mirror image. The panel reflects captions for the viewer but is nearly invisible to surrounding patrons. The panel can be positioned so that the viewer watches the movie through the panel, and captions appear either on or near the movie image. A company called Cinematic Captioning Systems has a similar reflective system called Bounce Back. A major problem for distributors has been that these systems are each proprietary, and require separate distributions to the theater to enable them to work. Proprietary systems also incur license fees.\nFor film projection systems, Digital Theater Systems, the company behind the DTS surround sound standard, has created a digital captioning device called the DTS-CSS (Cinema Subtitling System). It is a combination of a laser projector which places the captioning (words, sounds) anywhere on the screen and a thin playback device with a CD that holds many languages. If the Rear Window Captioning System is used, the DTS-CSS player is also required for sending caption text to the Rear Window sign located in the rear of the theater.\nSpecial effort has been made to build accessibility features into digital projection systems (see digital cinema). Through SMPTE, standards now exist that dictate how open and closed captions, as well as hearing-impaired and visually impaired narrative audio, are packaged with the rest of the digital movie. This eliminates the proprietary caption distributions required for film, and the associated royalties. SMPTE has also standardized the communication of closed caption content between the digital cinema server and 3rd-party closed caption systems (the CSP/RPL protocol). As a result, new, competitive closed caption systems for digital cinema are now emerging that will work with any standards-compliant digital cinema server. These newer closed caption devices include cupholder-mounted electronic displays and wireless glasses which display caption text in front of the wearer's eyes. Bridge devices are also available to enable the use of Rear Window systems. As of mid-2010, the remaining challenge to the wide introduction of accessibility in digital cinema is the industry-wide transition to SMPTE DCP, the standardized packaging method for very high quality, secure distribution of digital movies.\nSports venues.\nCaptioning systems have also been adopted by most major league and high-profile college stadiums and arenas, typically through dedicated portions of their main scoreboards or as part of balcony fascia LED boards. These screens display captions of the public address announcer and other spoken content, such as those contained within in-game segments, public service announcements, and lyrics of songs played in-stadium. In some facilities, these systems were added as a result of discrimination lawsuits. Following a lawsuit under the Americans with Disabilities Act, FedExField added caption screens in 2006. Some stadiums utilize on-site captioners while others outsource them to external providers who caption remotely.\nVideo games.\nThe infrequent appearance of closed captioning in video games became a problem in the 1990s as games began to commonly feature voice tracks, which in some cases contained information which the player needed in order to know how to progress in the game. Closed captioning of video games is becoming more common. One of the first video game companies to feature closed captioning was Bethesda Softworks in their 1990 release of \"Hockey League Simulator\" and \"The Terminator 2029\". Infocom also offered \"Zork Grand Inquisitor\" in 1997. Many games since then have at least offered subtitles for spoken dialog during cutscenes, and many include significant in-game dialog and sound effects in the captions as well; for example, with subtitles turned on in the \"Metal Gear Solid\" series of stealth games, not only are subtitles available during cut scenes, but any dialog spoken during real-time gameplay will be captioned as well, allowing players who can't hear the dialog to know what enemy guards are saying and when the main character has been detected. Also, in many of developer Valve's video games (such as \"Half-Life 2\" or \"Left 4 Dead\"), when closed captions are activated, dialog and nearly all sound effects either made by the player or from other sources (e.g. gunfire, explosions) will be captioned.\nVideo games don't offer Line 21 captioning, decoded and displayed by the television itself but rather a built-in subtitle display, more akin to that of a DVD. The game systems themselves have no role in the captioning either; each game must have its subtitle display programmed individually.\nReid Kimball, a game designer who is hearing impaired, is attempting to educate game developers about closed captioning for games. Reid started the Games[CC] group to closed caption games and serve as a research and development team to aid the industry. Kimball designed the Dynamic Closed Captioning system, writes articles and speaks at developer conferences. Games[CC]'s first closed captioning project called Doom3[CC] was nominated for an award as Best Doom3 Mod of the Year for IGDA's Choice Awards 2006 show.\nOnline video streaming.\nInternet video streaming service YouTube offers captioning services in videos. The author of the video can upload a SubViewer (*.SUB), SubRip (*.SRT) or *.SBV file. As a beta feature, the site also added the ability to automatically transcribe and generate captioning on videos, with varying degrees of success based upon the content of the video. However, on August 30, 2020, the company announced that communal captions will end on September 28. The automatic captioning is often inaccurate on videos with background music or exaggerated emotion in speaking. Variations in volume can also result in nonsensical machine-generated captions. Additional problems arise with strong accents, sarcasm, differing contexts, or homonyms.\nOn June 30, 2010, YouTube announced a new \"YouTube Ready\" designation for professional caption vendors in the United States. The initial list included twelve companies who passed a caption quality evaluation administered by the Described and Captioned Media Project, have a website and a YouTube channel where customers can learn more about their services and have agreed to post rates for the range of services that they offer for YouTube content.\nFlash Video also supports captions using the Distribution Exchange profile (DFXP) of W3C timed text format. The latest Flash authoring software adds free player skins and caption components that enable viewers to turn captions on/off during playback from a web page. Previous versions of Flash relied on the Captionate 3rd party component and skin to caption Flash Video. Custom Flash players designed in Flex can be tailored to support the timed-text exchange profile, Captionate .XML, or SAMI file (e.g. Hulu captioning). This is the preferred method for most US broadcast and cable networks that are mandated by the U.S. Federal Communications Commission to provide captioned on-demand content. The media encoding firms generally use software such as MacCaption to convert EIA-608 captions to this format. The Silverlight Media Framework also includes support for the timed-text exchange profile for both download and adaptive streaming media.\nWindows Media Video can support closed captions for both video on demand streaming or live streaming scenarios. Typically, Windows Media captions support the SAMI file format but can also carry embedded closed caption data.\nEBU-TT-D distribution format supports multiple players across multiple platforms.\nQuickTime video supports raw EIA-608 caption data via proprietary closed caption track, which are just EIA-608 byte pairs wrapped in a QuickTime packet container with different IDs for both Line 21 fields. These captions can be turned on and off and appear in the same style as TV closed captions, with all the standard formatting (pop-on, roll-up, paint-on), and can be positioned and split anywhere on the video screen. QuickTime closed caption tracks can be viewed in macOS or Windows versions of QuickTime Player, iTunes (via QuickTime), iPod Nano, iPod Classic, iPod Touch, iPhone, and iPad.\nTypical modern browsers, such as Edge and Chrome, and/or typical modern operating systems, such as iOS 12+, Android 10+, Windows 10+, may manage the CC subtitle style.\nTheatre.\nLive plays can be open captioned by a captioner who displays lines from the script and including non-speech elements on a large display screen near the stage. Software is also now available that automatically generates the captioning and streams the captioning to individuals sitting in the theater, with that captioning being viewed using heads-up glasses or on a smartphone or computer tablet.\nTelephones.\nA captioned telephone is a telephone that displays real-time captions of the current conversation. The captions are typically displayed on a screen embedded into the telephone base.\nVideo conferencing.\nSome online video conferencing services, such as Google Meet, offer the ability to display captions in real time of the current conversation.\nMedia monitoring services.\nIn the United States especially, most media monitoring services capture and index closed captioning text from news and public affairs programs, allowing them to search the text for client references. The use of closed captioning for television news monitoring was pioneered by Universal Press Clipping Bureau (Universal Information Services) in 1992, and later in 1993 by Tulsa-based NewsTrak of Oklahoma (later known as Broadcast News of Mid-America, acquired by video news release pioneer Medialink Worldwide Incorporated in 1997). US patent 7,009,657 describes a \"method and system for the automatic collection and conditioning of closed caption text originating from multiple geographic locations\" as used by news monitoring services.\nConversations.\nSoftware programs are available that automatically generate a closed-captioning of conversations. Examples of such conversations include discussions in conference rooms, classroom lectures, or religious services.\nNon-linear video editing systems and closed captioning.\nIn 2010, Vegas Pro, the professional non-linear editor, was updated to support importing, editing, and delivering CEA-608 closed captions. Vegas Pro 10, released on October 11, 2010, added several enhancements to the closed captioning support. TV-like CEA-608 closed captioning can now be displayed as an overlay when played back in the Preview and Trimmer windows, making it easy to check placement, edits, and timing of CC information. CEA708 style Closed Captioning is automatically created when the CEA-608 data is created. Line 21 closed captioning is now supported, as well as HD-SDI closed captioning capture and print from AJA and Blackmagic Design cards. Line 21 support provides a workflow for existing legacy media. Other improvements include increased support for multiple closed captioning file types, as well as the ability to export closed caption data for DVD Architect, YouTube, RealPlayer, QuickTime, and Windows Media Player.\nIn mid-2009, Apple released Final Cut Pro version 7 and began support for inserting closed caption data into SD and HD tape masters via FireWire and compatible video capture cards. Up until this time, it was not possible for video editors to insert caption data with both CEA-608 and CEA-708 to their tape masters. The typical workflow included first printing the SD or HD video to a tape and sending it to a professional closed caption service company that had a stand-alone closed caption hardware encoder.\nThis new closed captioning workflow known as e-Captioning involves making a proxy video from the non-linear system to import into a third-party non-linear closed captioning software. Once the closed captioning software project is completed, it must export a closed caption file compatible with the non-linear editing system. In the case of Final Cut Pro 7, three different file formats can be accepted: a .SCC file (Scenarist Closed Caption file) for Standard Definition video, a QuickTime 608 closed caption track (a special 608 coded track in the .mov file wrapper) for standard-definition video, and finally a QuickTime 708 closed caption track (a special 708 coded track in the .mov file wrapper) for high-definition video output.\nAlternatively, Matrox video systems devised another mechanism for inserting closed caption data by allowing the video editor to include CEA-608 and CEA-708 in a discrete audio channel on the video editing timeline. This allows real-time preview of the captions while editing and is compatible with Final Cut Pro 6 and 7.\nOther non-linear editing systems indirectly support closed captioning only in Standard Definition Line 21. Video files on the editing timeline must be composited with a Line 21 VBI graphic layer known in the industry as a \"blackmovie\" with closed caption data. Alternately, video editors working with the DV25 and DV50 FireWire workflows must encode their DV .avi or .mov file with VAUX data which includes CEA-608 closed caption data.\nLogo.\nThe current and most familiar logo for closed captioning consists of two Cs (for \"closed captioned\") inside a television screen. It was created at WGBH. The other logo, trademarked by the National Captioning Institute, is that of a simple geometric rendering of a television set merged with the tail of a speech balloon; two such versions exist \u2013 one with a tail on the left, the other with a tail on the right.\nReferences.\nCitations.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "40884", "revid": "17596857", "url": "https://en.wikipedia.org/wiki?curid=40884", "title": "Closed-circuit", "text": ""}
{"id": "40885", "revid": "46802587", "url": "https://en.wikipedia.org/wiki?curid=40885", "title": "Closed-loop transfer function", "text": "Function describing the effects of feedback on a control system\nIn control theory, a closed-loop transfer function is a mathematical function describing the net result of the effects of a feedback control loop on the input signal to the plant under control.\nOverview.\nThe closed-loop transfer function is measured at the output. The output signal can be calculated from the closed-loop transfer function and the input signal. Signals may be waveforms, images, or other types of data streams.\nAn example of a closed-loop block diagram, from which a transfer function may be computed, is shown below:\nThe summing node and the \"G\"(\"s\") and \"H\"(\"s\") blocks can all be combined into one block, which would have the following transfer function:\n formula_1\nformula_2 is called the feed forward transfer function, formula_3 is called the feedback transfer function, and their product formula_4 is called the open-loop transfer function.\nDerivation.\nWe define an intermediate signal Z (also known as error signal) shown as follows:\nUsing this figure we write:\n formula_5\n formula_6\nNow, plug the second equation into the first to eliminate Z(s):\nformula_7\nMove all the terms with Y(s) to the left hand side, and keep the term with X(s) on the right hand side:\nformula_8\nTherefore,\nformula_9\nformula_10"}
{"id": "40886", "revid": "15970", "url": "https://en.wikipedia.org/wiki?curid=40886", "title": "Closed waveguide", "text": ""}
{"id": "40888", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=40888", "title": "Code conversion", "text": "Concept in telecommunications\nIn telecommunications, the term code conversion has the following meanings: \n1. Conversion of signals, or groups of signals, in one code into corresponding signals, or groups of signals, in another code. \n2. A process for converting a code of some predetermined bit structure, such as 5, 7, or 14 bits per character interval, to another code with the same or a different number of bits per character interval. \nIn code conversion, alphabetical order is not significant.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "40889", "revid": "11987994", "url": "https://en.wikipedia.org/wiki?curid=40889", "title": "Code-division", "text": ""}
{"id": "40890", "revid": "11555324", "url": "https://en.wikipedia.org/wiki?curid=40890", "title": "Coded set", "text": "Secret communications\nIn telecommunications, a coded set is a set of elements onto which another set of elements has been mapped according to a code.\nExamples of coded sets include the list of names of airports that is mapped onto a set of corresponding three-letter representations of airport names, the list of classes of emission that is mapped onto a set of corresponding standard symbols, and the names of the months of the year mapped onto a set of two-digit decimal numbers.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "40891", "revid": "11487766", "url": "https://en.wikipedia.org/wiki?curid=40891", "title": "Code word (communication)", "text": "Element of a standardized code or protocol\nIn communication, a code word is an element of a standardized code or protocol. Each code word is assembled in accordance with the specific rules of the code and assigned a unique meaning. Code words are typically used for reasons of reliability, clarity, brevity, or secrecy."}
{"id": "40892", "revid": "44062", "url": "https://en.wikipedia.org/wiki?curid=40892", "title": "Coding", "text": "Coding may refer to:\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "40893", "revid": "252195", "url": "https://en.wikipedia.org/wiki?curid=40893", "title": "Coherence length", "text": "Distance over which a propagating wave maintains a certain degree of coherence\nIn physics, coherence length is the propagation distance over which a coherent wave (e.g. an electromagnetic wave) maintains a specified degree of coherence. Wave interference is strong when the paths taken by all of the interfering waves differ by less than the coherence length. A wave with a longer coherence length is closer to a perfect sinusoidal wave. Coherence length is important in holography and telecommunications engineering.\nThis article focuses on the coherence of classical electromagnetic fields. In quantum mechanics, there is a mathematically analogous concept of the quantum coherence length of a wave function.\nFormulas.\nIn radio-band systems, the coherence length is approximated by\nformula_1\nwhere formula_2 is the speed of light in vacuum, formula_3 is the refractive index of the medium, and formula_4 is the bandwidth of the source or formula_5 is the signal wavelength and formula_6 is the width of the range of wavelengths in the signal.\nIn optical communications and optical coherence tomography (OCT), assuming that the source has a Gaussian emission spectrum, the roundtrip coherence length formula_7 is given by \nformula_8\nwhere formula_5 is the central wavelength of the source, formula_10 is the group refractive index of the medium, and formula_11 is the (FWHM) spectral width of the source. If the source has a Gaussian spectrum with FWHM spectral width formula_12, then a path offset of formula_13 will reduce the fringe visibility to 50%. It is important to note that this is a roundtrip coherence length \u2014 this definition is applied in applications like OCT where the light traverses the measured displacement twice (as in a Michelson interferometer). In transmissive applications, such as with a Mach\u2013Zehnder interferometer, the light traverses the displacement only once, and the coherence length is effectively doubled.\nThe coherence length can also be measured using a Michelson interferometer and is the optical path length difference of a self-interfering laser beam which corresponds to formula_14 fringe visibility, where the fringe visibility is defined as\nformula_15\nwhere formula_16 is the fringe intensity.\nIn long-distance transmission systems, the coherence length may be reduced by propagation factors such as dispersion, scattering, and diffraction.\nLasers.\nMultimode helium\u2013neon lasers have a typical coherence length on the order of centimeters, while the coherence length of longitudinally single-mode lasers can exceed 1\u00a0km. Semiconductor lasers can reach some 100\u00a0m, but small, inexpensive semiconductor lasers have shorter lengths, with one source claiming up to 20\u00a0cm, although multi-mode diodes will have even shorter coherence lengths. Singlemode fiber lasers with linewidths of a few kHz can have coherence lengths exceeding 100\u00a0km. Similar coherence lengths can be reached with optical frequency combs due to the narrow linewidth of each tooth. Non-zero visibility is present only for short intervals of pulses repeated after cavity length distances up to this long coherence length.\nOther light sources.\nTolansky's \"An introduction to Interferometry\" has a chapter on sources which quotes a line width of around 0.052 angstroms for each of the Sodium D lines in an uncooled low-pressure sodium lamp, corresponding to a coherence length of around 67\u00a0mm for each line by itself. Cooling the low pressure sodium discharge to liquid nitrogen temperatures increases the individual D line coherence length by a factor of 6. A very narrow-band interference filter would be required to isolate an individual D line.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "40894", "revid": "23939382", "url": "https://en.wikipedia.org/wiki?curid=40894", "title": "Coherence time", "text": "Time of electromagnetic wave coherence\nFor an electromagnetic wave, the coherence time is the time over which a propagating wave (especially a laser or maser beam) may be considered coherent, meaning that its phase is, on average, predictable.\nIn long-distance transmission systems, the coherence time may be reduced by propagation factors such as dispersion, scattering, and diffraction.\nThe coherence time, usually designated \u03c4, is calculated by dividing the coherence length by the phase velocity of light in a medium; approximately given by\nformula_1\nwhere \u03bb is the central wavelength of the source, \u0394\"\u03bd\" and \u0394\"\u03bb\" is the spectral width of the source in units of frequency and wavelength respectively, and \"c\" is the speed of light in vacuum.\nA single mode fiber laser has a linewidth of a few kHz, corresponding to a coherence time of a few hundred microseconds. Hydrogen masers have linewidth around 1\u00a0Hz, corresponding to a coherence time of about one second. Their coherence length approximately corresponds to the distance from the Earth to the Moon.\nAs of 2022, research groups worldwide have demonstrated superconducting qubits with coherence times up to several 100 \u03bcs."}
{"id": "40895", "revid": "285145", "url": "https://en.wikipedia.org/wiki?curid=40895", "title": "Coherent differential phase-shift keying", "text": ""}
{"id": "40896", "revid": "31530", "url": "https://en.wikipedia.org/wiki?curid=40896", "title": "Collective routing", "text": "Collective routing is routing in which a switching center automatically delivers messages to a specified list of destinations.\nCollective routing avoids the need to list each single address in the message heading.\nMajor relay stations usually transmit messages bearing collective-routing indicators to tributary, minor, and other major relay stations.\nReferences.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "40897", "revid": "23939382", "url": "https://en.wikipedia.org/wiki?curid=40897", "title": "Collinear antenna array", "text": "Antenna system type\nIn telecommunications, a collinear antenna array (sometimes spelled colinear antenna array) is an array of dipole or quarter-wave antennas mounted in such a manner that the corresponding elements of each antenna are parallel and collinear; that is, they are located along a common axis. \nCollinear arrays are high gain omnidirectional antennas. Both dipoles and quarter-wavelength monopoles have an omnidirectional radiation pattern in free space when oriented vertically; they radiate equal radio power in all azimuthal directions perpendicular to the antenna, with the signal strength dropping to zero on the antenna axis. The purpose of stacking multiple antennas in a vertical collinear array is to increase the power radiated in horizontal directions and reduce the power radiated into the sky or down toward the earth, where it is wasted. They radiate vertically polarized radio waves. Theoretically, when stacking idealized lossless antennas in such a fashion, doubling their number will produce double the gain, with an increase of 3.01 dB. In practice, the gain realized will be below this due to imperfect radiation spread and losses.\nCollinear arrays are frequently constructed as a stack of dipoles, but can also be constructed as a stack of phased quarter-wave antennas. In this configuration, the individual radiators within the array are often constructed of coaxial feedlines with the center conductor of one element being connected electrically to the shield of the one above, and so on in alternating phase for as many elements are specified by gain or overall length requirements. The final or 'top' element in the stack is a quarter-wave radiator connected directly to the center conductor of the element below it. This style of collinear antenna is usually housed in a fiberglass radome, to provide both support and environmental protection to the relatively fragile coaxial elements.\nA third type of collinear array, rarely seen outside amateur radio VHF/UHF applications, uses half-wavelength monopole elements with phasing coils between each consecutive pair of elements to achieve the necessary phase shift. This style tends to be less efficient due to coil losses, but has the advantage that it can be constructed with the elements supporting themselves, doing away with the need for a protective radome.\nCollinear arrays are often used as the antennas for base stations for land mobile radio systems that communicate with mobile two-way radios in vehicles, such as police, fire, ambulance, and taxi dispatchers. They are also sometimes used for broadcasting..\nReferences.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "40898", "revid": "46628330", "url": "https://en.wikipedia.org/wiki?curid=40898", "title": "Collision", "text": "Instance of two or more bodies physically contacting each other within a short period of time\nIn physics, a collision is any event in which two or more bodies exert forces on each other in a relatively short time. Although the most common use of the word \"collision\" refers to incidents in which two or more objects collide with great force, the scientific use of the term implies nothing about the magnitude of the force.\nTypes of collisions.\nCollision is short-duration interaction between two bodies or more than two bodies simultaneously causing change in motion of bodies involved due to internal forces acted between them during this. Collisions involve forces (there is a change in velocity). The magnitude of the velocity difference just before impact is called the closing speed. All collisions conserve momentum. What distinguishes different types of collisions is whether they also conserve kinetic energy of the system before and after the collision. Collisions are of two types:\nThe degree to which a collision is elastic or inelastic is quantified by the coefficient of restitution, a value that generally ranges between zero and one. A perfectly elastic collision has a coefficient of restitution of one; a perfectly inelastic collision has a coefficient of restitution of zero. The line of impact is the line that is collinear to the common normal of the surfaces that are closest or in contact during impact. This is the line along which internal force of collision acts during impact, and Newton's coefficient of restitution is defined only along this line. \nCollisions in ideal gases approach perfectly elastic collisions, as do scattering interactions of sub-atomic particles which are deflected by the electromagnetic force. Some large-scale interactions like the slingshot type gravitational interactions between satellites and planets are almost perfectly elastic.\nExamples.\nBilliards.\nCollisions play an important role in cue sports. Because the collisions between billiard balls are nearly elastic, and the balls roll on a surface that produces low rolling friction, their behavior is often used to illustrate Newton's laws of motion. After a zero-friction collision of a moving ball with a stationary one of equal mass, the angle between the directions of the two balls is 90 degrees. This is an important fact that professional billiards players take into account, although it assumes the ball is moving without any impact of friction across the table rather than rolling with friction.\nConsider an elastic collision in two dimensions of any two masses \"m\"a and \"m\"b, with respective initial velocities va1 and vb1 where vb1 = 0, and final velocities va2 and vb2.\nConservation of momentum gives \"m\"ava1 = \"m\"ava2 + \"m\"bvb2.\nConservation of energy for an elastic collision gives (1/2)\"m\"a|va1|2 = (1/2)\"m\"a|va2|2 + (1/2)\"m\"b|vb2|2.\nNow consider the case \"m\"a = \"m\"b: we obtain va1 = va2 + vb2 and |va1|2 = |va2|2 + |vb2|2.\nTaking the dot product of each side of the former equation with itself, |va1|2 = va1\u2022va1 = |va2|2 + |vb2|2 + 2va2\u2022vb2. Comparing this with the latter equation gives va2\u2022vb2 = 0, so they are perpendicular unless va2 is the zero vector (which occurs if and only if the collision is head-on).\nPerfect inelastic collision.\nIn a perfect inelastic collision, i.e., a zero coefficient of restitution, the colliding particles coalesce. Using conservation of momentum:\nformula_1\nthe final velocity is given by\nformula_2\nThe reduction of total kinetic energy is equal to the total kinetic energy before the collision in a center of momentum frame with respect to the system of two particles, because in such a frame the kinetic energy after the collision is zero. In this frame most of the kinetic energy before the collision is that of the particle with the smaller mass. In another frame, in addition to the reduction of kinetic energy there may be a transfer of kinetic energy from one particle to the other; the fact that this depends on the frame shows how relative this is.\nWith time reversed we have the situation of two objects pushed away from each other, e.g. shooting a projectile, or a rocket applying thrust (compare the derivation of the Tsiolkovsky rocket equation).\nAnimal locomotion.\nCollisions of an animal's foot or paw with the underlying substrate are generally termed ground reaction forces. These collisions are inelastic, as kinetic energy is not conserved. An important research topic in prosthetics is quantifying the forces generated during the foot-ground collisions associated with both disabled and non-disabled gait. This quantification typically requires subjects to walk across a force platform (sometimes called a \"force plate\") as well as detailed kinematic and dynamic (sometimes termed kinetic) analysis.\nHypervelocity impacts.\nHypervelocity is very high velocity, approximately over 3,000 meters per second (11,000\u00a0km/h, 6,700\u00a0mph, 10,000\u00a0ft/s, or Mach 8.8). In particular, hypervelocity is velocity so high that the strength of materials upon impact is very small compared to inertial stresses. Thus, metals and fluids behave alike under hypervelocity impact. An impact under extreme hypervelocity results in vaporization of the impactor and target. For structural metals, hypervelocity is generally considered to be over 2,500\u00a0m/s (5,600\u00a0mph, 9,000\u00a0km/h, 8,200\u00a0ft/s, or Mach 7.3). Meteorite craters are also examples of hypervelocity impacts.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "40899", "revid": "575347", "url": "https://en.wikipedia.org/wiki?curid=40899", "title": "Combat-net radio", "text": "Type of radio used in military operations\nIn telecommunications, a combat-net radio (CNR) is a radio operating in a network that (a) provides a half-duplex circuit and (b) uses either a single radio frequency or a discrete set of radio frequencies when in a frequency hopping mode.\nCNRs are primarily used for push-to-talk-operated radio nets for command and control of combat, combat support, and combat service support operations among military ground, sea, and air forces.\nIn the United States, two military standards govern the use of combat net radios and the host applications that communicate over the network: MIL-STD-188-220 and MIL-STD-2045-47001. In addition to IETF RFCs governing UDP, TCP, and IPv4/IPv6, all seven layers of the OSI communications architecture are addressed. MIL-STD-2045-47001 covers layer 7 (application), while MIL-STD-188-220 covers layers 1 through 3 (physical, data link, and network).\nReferences.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "40900", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=40900", "title": "Combined distribution frame", "text": "In telecommunications, a combined distribution frame (CDF) is a distribution frame that combines the functions of main and intermediate distribution frames and contains both vertical and horizontal terminating blocks. \nThe vertical blocks are used to terminate the permanent outside lines entering the station. Horizontal blocks are used to terminate inside plant equipment. This arrangement permits the association of any outside line with any desired terminal equipment. These connections are made either with twisted pair wire, normally referred to as jumper wire, or with optical fiber cables, normally referred to as jumper cables. \nIn technical control facilities, the vertical side may be used to terminate equipment as well as outside lines. The horizontal side is then used for jackfields and battery terminations.\nReferences.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "40901", "revid": "29981771", "url": "https://en.wikipedia.org/wiki?curid=40901", "title": "Comma-free code", "text": "A comma-free code is block code in which no concatenation of two code words contains a valid code word that overlaps both.\nComma-free codes are also known as \"self-synchronizing block codes\" because no synchronization is required to find the beginning of a code word.\nIn the literature, the requirement that all code words have to have the same length is sometimes dropped, resulting in the same class as self-synchronizing codes, see Self-synchronizing code.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "40904", "revid": "3936334", "url": "https://en.wikipedia.org/wiki?curid=40904", "title": "Command and control warfare", "text": ""}
{"id": "40905", "revid": "28190", "url": "https://en.wikipedia.org/wiki?curid=40905", "title": "Command menu", "text": ""}
{"id": "40906", "revid": "1754504", "url": "https://en.wikipedia.org/wiki?curid=40906", "title": "Commercial refile", "text": ""}
{"id": "40908", "revid": "7903804", "url": "https://en.wikipedia.org/wiki?curid=40908", "title": "Common battery", "text": "In telecommunications, a common battery is a single electrical power source used to energize more than one circuit, electronic component, equipment, or system. \nA common battery is usually a string of electrolytic cells and is usually centrally located to the equipment that it serves. In many telecommunications applications, the common battery is at a nominal \u221248 VDC. A central office common battery in the battery room supplies power to operate all directly connected instruments. \"Common battery\" may include one or more power conversion devices to transform commercial power to direct current, with a rechargeable battery floating across the output. \nBefore 1891, telephones in the United States always needed a separate power source to supply the subscriber line at each end. Each telephone needed a battery to power the transmitter and a hand generator to help signal the central office to manually connect the line to other subscribers and disconnect the line at the end of each call, and the cost of maintaining all this equipment was a large part of why early telephone service was so expensive. During the 1890s, common batteries became prevalent in telephone company central offices across the United States. They helped drive down the cost of phone service by reducing the complexity of the telephone equipment to be deployed to each subscriber, since each telephone would now draw all the power needed from the central office. The telephone hook replaced the hand generator as the primary method for signaling the exchange for when the line was in use or no longer in use.\nThe presence of a common battery at the central office also explains why landline telephones can often still work when the regular electrical grid is down. \nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nFurther reading.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "40909", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=40909", "title": "Booting", "text": "Process of starting a computer\nIn computing, booting is the process of starting a computer as initiated via hardware such as a physical button on the computer or by a software command. After it is switched on, a computer's central processing unit (CPU) has no software in its main memory, so some process must load software into memory before it can be executed. This may be done by hardware or firmware in the CPU, or by a separate processor in the computer system. On some systems a power-on reset (POR) does not initiate booting and the operator must initiate booting after POR completes. IBM uses the term Initial Program Load (IPL) on some product lines.\nRestarting a computer is also called \"rebooting\", which can be \"hard\", e.g. after electrical power to the CPU is switched from off to on, or \"soft\", where the power is not cut. On some systems, a soft boot may optionally clear RAM to zero. Both hard and soft booting can be initiated by hardware, such as a button press, or by a software command. Booting is complete when the operative runtime system, typically the operating system and some applications, is attained.\nThe process of returning a computer from a state of sleep (suspension) does not involve booting; however, restoring it from a state of hibernation does. Minimally, some embedded systems do not require a noticeable boot sequence to begin functioning, and when turned on, may simply run operational programs that are stored in read-only memory (ROM). All computing systems are state machines, and a reboot may be the only method to return to a designated zero-state from an unintended, locked state.\nIn addition to loading an operating system or stand-alone utility, the boot process can also load a storage dump program for diagnosing problems in an operating system.\n\"Boot\" is short for \"bootstrap\" or \"bootstrap load\" and derives from the phrase \"to pull oneself up by one's bootstraps\". The usage calls attention to the requirement that, if most software is loaded onto a computer by other software already running on the computer, some mechanism must exist to load the initial software onto the computer. Early computers used a variety of ad-hoc methods to get a small program into memory to solve this problem. The invention of ROM of various types solved this paradox by allowing computers to be shipped with a start-up program, stored in the boot ROM of the computer, that could not be erased. Growth in the capacity of ROM has allowed ever more elaborate start up procedures to be implemented.\nHistory.\nThere are many different methods available to load a short initial program into a computer. These methods range from simple, physical input to removable media that can hold more complex programs.\nPre integrated-circuit-ROM examples.\nEarly computers.\nEarly computers in the 1940s and 1950s were one-of-a-kind engineering efforts that could take weeks to program, and program loading was one of many problems that had to be solved. An early computer, ENIAC, had no program stored in memory but was set up for each problem by a configuration of interconnecting cables. Bootstrapping did not apply to ENIAC, whose hardware configuration was ready for solving problems as soon as power was applied.\nThe EDSAC system, the second stored-program computer to be built, used stepping switches to transfer a fixed program into memory when its start button was pressed. The program stored on this device, which David Wheeler completed in late 1948, loaded further instructions from punched tape and then executed them.\nFirst commercial computers.\nThe first programmable computers for commercial sale, such as the UNIVAC I and the IBM 701 included features to make their operation simpler. They typically included instructions that performed a complete input or output operation. The same hardware logic could be used to load the contents of a punch card (the most typical ones) or other input media, such as a magnetic drum or magnetic tape, that contained a bootstrap program by pressing a single button. This booting concept was called a variety of names for IBM computers of the 1950s and early 1960s, but IBM used the term \"Initial Program Load\" with the IBM 7030 Stretch and later used it for their mainframe lines, starting with the System/360 in 1964.\nThe IBM 701 computer (1952\u20131956) had a \"Load\" button that initiated reading of the first 36-bit word into main memory from a punched card in a card reader, a magnetic tape in a tape drive, or a magnetic drum unit, depending on the position of the Load Selector switch. The left 18-bit half-word was then executed as an instruction, which usually read additional words into memory. The loaded boot program was then executed, which, in turn, loaded a larger program from that medium into memory without further help from the human operator. The IBM 704, IBM 7090, and IBM 7094 had similar mechanisms, but with different load buttons for different devices. The term \"boot\" has been used in this sense since at least 1958.\nOther IBM computers of that era had similar features. For example, the IBM 1401 system (1959) used a card reader to load a program from a punched card. The 80 characters stored in the punched card were read into memory locations 001 to 080, then the computer would branch to memory location 001 to read its first stored instruction. This instruction was always the same: move the information in these first 80 memory locations to an assembly area where the information in punched cards 2, 3, 4, and so on, could be combined to form the stored program. Once this information was moved to the assembly area, the machine would branch to an instruction in location 080 (read a card) and the next card would be read and its information processed.\nAnother example was the IBM 650 (1953), a decimal machine, which had a group of ten 10-position switches on its operator panel that were addressable as a memory word (address 8000) and could be executed as an instruction. Thus, setting the switches to 7004000400 and pressing the appropriate button would read the first card in the card reader into memory (op code 70), starting at address 400 and then jump to 400 to begin executing the program on that card. The IBM 7040 and 7044 have a similar mechanism, in which the Load button causes the instruction set up in the entry keys on the front panel is executed, and the channel that instruction sets up is given a command to transfer data to memory starting at address 00100; when that transfer finishes, the CPU jumps to address 00101.\nIBM's competitors also offered single-button program load.\nA noteworthy variation of this is found on the Burroughs B1700 where there is neither a boot ROM nor a hardwired IPL operation. Instead, after the system is reset it reads and executes microinstructions sequentially from a cassette tape drive mounted on the front panel; this sets up a boot loader in RAM which is then executed. However, since this makes few assumptions about the system it can equally well be used to load diagnostic (Maintenance Test Routine) tapes which display an intelligible code on the front panel even in cases of gross CPU failure.\nIBM System/360 and successors.\nIn the IBM System/360 and its successors, including the current z/Architecture machines, the boot process is known as \"Initial Program Load\" (IPL).\nIBM coined this term for the 7030 (Stretch), revived it for the design of the System/360, and continues to use it in those environments today. In the System/360 processors, an IPL is initiated by the computer operator by selecting the three hexadecimal digit device address (CUU; C=I/O Channel address, UU=Control unit and Device address) followed by pressing the \"LOAD\" button. On the high end System/360 models, most System/370 and some later systems, the functions of the switches and the LOAD button are simulated using selectable areas on the screen of a graphics console, often an IBM 2250-like device or an IBM 3270-like device. For example, on the System/370 Model 158, the keyboard sequence 0-7-X (zero, seven and X, in that order) results in an IPL from the device address that was keyed into the input area. The Amdahl 470V/6 and related CPUs supported four hexadecimal digits on those CPUs that had the optional second channel unit installed, for a total of 32 channels. Later, IBM would also support more than 16 channels.\nThe IPL function in the System/360 and its successors prior to IBM Z, and its compatibles, such as Amdahl's, reads 24 bytes from an operator-specified device into main storage starting at real address zero. The second and third groups of eight bytes are treated as Channel Command Words (CCWs) to continue loading the startup program (the first CCW is always simulated by the CPU and consists of a Read IPL command, 02h, with command chaining and suppress incorrect length indication being enforced). When the I/O channel commands are complete, the first group of eight bytes is then loaded into the processor's Program Status Word (PSW) and the startup program begins execution at the location designated by that PSW. The IPL device is usually a disk drive, hence the special significance of the 02h read-type command, but exactly the same procedure is also used to IPL from other input-type devices, such as tape drives, or even card readers, in a device-independent manner, allowing, for example, the installation of an operating system on a brand-new computer from an OS initial distribution magnetic tape. For disk controllers, the 02h command also causes the selected device to seek to cylinder 0000h, head 0000h, simulating a Seek cylinder and head command, 07h, and to search for record 01h, simulating a Search ID Equal command, 31h; seeks and searches are not simulated by tape and card controllers, as for these device classes a Read IPL command is simply a sequential read command.\nThe disk, tape or card deck must contain a special program to load the actual operating system or standalone utility into main storage, and for this specific purpose, \"IPL Text\" is placed on the disk by the stand-alone DASDI (Direct Access Storage Device Initialization) program or an equivalent program running under an operating system, e.g., ICKDSF, but IPL-able tapes and card decks are usually distributed with this \"IPL Text\" already present.\nIBM introduced some evolutionary changes in the IPL process, changing some details for System/370 Extended Architecture (S/370-XA) and later, and adding a new type of IPL for z/Architecture.\nMinicomputers.\nMinicomputers, starting with the Digital Equipment Corporation (DEC) PDP-5 and PDP-8 (1965) simplified design by using the CPU to assist input and output operations. This saved cost but made booting more complicated than pressing a single button. Minicomputers typically had some way to \"toggle in\" short programs by manipulating an array of switches on the front panel. Since the early minicomputers used magnetic-core memory, which did not lose its information when power was off, these bootstrap loaders would remain in place unless they were erased. Erasure sometimes happened accidentally when a program bug caused a loop that overwrote all of memory.\nOther minicomputers with such simple form of booting include Hewlett-Packard's HP 2100 series (mid-1960s), the original Data General Nova (1969), and DEC's PDP-4 (1962) and PDP-11 (1970).\nAs the I/O operations needed to cause a read operation on a minicomputer I/O device were typically different for different device controllers, different bootstrap programs were needed for different devices.\nDEC later added, in 1971, an optional diode matrix read-only memory for the PDP-11 that stored a bootstrap program of up to 32 words (64 bytes). It consisted of a printed circuit card, the M792, that plugged into the Unibus and held a 32 by 16 array of semiconductor diodes. With all 512 diodes in place, the memory contained all \"one\" bits; the card was programmed by cutting off each diode whose bit was to be \"zero\". DEC also sold versions of the card, the BM792-Yx series, pre-programmed for many standard input devices by simply omitting the unneeded diodes.\nFollowing the older approach, the earlier PDP-1 has a hardware loader, such that an operator need only push the \"load\" switch to instruct the paper tape reader to load a program directly into core memory. The PDP-7, PDP-9, and PDP-15 successors to the PDP-4 have an added Read-In button to read a program in from paper tape and jump to it. The Data General Supernova used front panel switches to cause the computer to automatically load instructions into memory from a device specified by the front panel's data switches, and then jump to loaded code.\nEarly minicomputer boot loader examples.\nIn a minicomputer with a paper tape reader, the first program to run in the boot process, the boot loader, would read into core memory either the second-stage boot loader (often called a \"Binary Loader\") that could read paper tape with checksum or the operating system from an outside storage medium. Pseudocode for the boot loader might be as simple as the following eight instructions:\nA related example is based on a loader for a Nicolet Instrument Corporation minicomputer of the 1970s, using the paper tape reader-punch unit on a Teletype Model 33 ASR teleprinter. The bytes of its second-stage loader are read from paper tape in reverse order.\nThe length of the second stage loader is such that the final byte overwrites location 7. After the instruction in location 6 executes, location 7 starts the second stage loader executing. The second stage loader then waits for the much longer tape containing the operating system to be placed in the tape reader. The difference between the boot loader and second stage loader is the addition of checking code to trap paper tape read errors, a frequent occurrence with relatively low-cost, \"part-time-duty\" hardware, such as the Teletype Model 33 ASR. (Friden Flexowriters were far more reliable, but also comparatively costly.)\nBooting the first microcomputers.\nThe earliest microcomputers, such as the Altair 8800 (released first in 1975) and an even earlier, similar machine (based on the Intel 8008 CPU) had no bootstrapping hardware as such. When powered-up, the CPU would see memory that would contain random data. The front panels of these machines carried toggle switches for entering addresses and data, one switch per bit of the computer memory word and address bus. Simple additions to the hardware permitted one memory location at a time to be loaded from those switches to store bootstrap code. Meanwhile, the CPU was kept from attempting to execute memory content. Once correctly loaded, the CPU was enabled to execute the bootstrapping code. This process, similar to that used for several earlier minicomputers, was tedious and had to be error-free.\nIntegrated circuit read-only memory era.\nThe introduction of integrated circuit read-only memory (ROM), with its many variants, including mask-programmed ROMs, programmable ROMs (PROM), erasable programmable ROMs (EPROM), and flash memory, reduced the physical size and cost of ROM. This allowed firmware boot programs to be included as part of the computer.\nMinicomputers.\nThe Data General Nova 1200 (1970) and Nova 800 (1971) had a program load switch that, in combination with options that provided two ROM chips, loaded a program into main memory from those ROM chips and jumped to it. Digital Equipment Corporation introduced the integrated-circuit-ROM-based BM873 (1974), M9301 (1977), M9312 (1978), REV11-A and REV11-C, MRV11-C, and MRV11-D ROM memories, all usable as bootstrap ROMs. The PDP-11/34 (1976), PDP-11/60 (1977), PDP-11/24 (1979), and most later models include boot ROM modules.\nAn Italian telephone switching computer, called \"Gruppi Speciali\", patented in 1975 by Alberto Ciaramella, a researcher at CSELT, included an (external) ROM. Gruppi Speciali was, starting from 1975, a fully single-button machine booting into the operating system from a ROM memory composed from semiconductors, not from ferrite cores. Although the ROM device was not natively embedded in the computer of Gruppi Speciali, due to the design of the machine, it also allowed the single-button ROM booting in machines not designed for that (therefore, this \"bootstrap device\" was architecture-independent), e.g. the PDP-11. Storing the state of the machine after the switch-off was also in place, which was another critical feature in the telephone switching contest.\nSome minicomputers and superminicomputers include a separate console processor that bootstraps the main processor. The PDP-11/44 had an Intel 8085 as a console processor; the VAX-11/780, the first member of Digital's VAX line of 32-bit superminicomputers, had an LSI-11-based console processor, and the VAX-11/730 had an 8085-based console processor. These console processors could boot the main processor from various storage devices.\nSome other superminicomputers, such as the VAX-11/750, implement console functions, including the first stage of booting, in CPU microcode.\nMicroprocessors and microcomputers.\nTypically, a microprocessor will, after a reset or power-on condition, perform a start-up process that usually takes the form of \"begin execution of the code that is found starting at a specific address\" or \"look for a multibyte code at a specific address and jump to the indicated location to begin execution\". A system built using that microprocessor will have the permanent ROM occupying these special locations so that the system always begins operating without operator assistance. For example, Intel x86 processors always start by running the instructions beginning at F000:FFF0, while for the MOS 6502 processor, initialization begins by reading a two-byte vector address at $FFFD (MS byte) and $FFFC (LS byte) and jumping to that location to run the bootstrap code.\nApple Computer's first computer, the Apple 1 introduced in 1976, featured PROM chips that eliminated the need for a front panel for the boot process (as was the case with the Altair 8800) in a commercial computer. According to Apple's ad announcing it, \"No More Switches, No More Lights ... the firmware in PROMS enables you to enter, display and debug programs (all in hex) from the keyboard.\"\nDue to the expense of read-only memory at the time, the Apple II booted its disk operating systems using a series of very small incremental steps, each passing control onward to the next phase of the gradually more complex boot process. (See Apple DOS: Boot loader). Because so little of the disk operating system relied on ROM, the hardware was also extremely flexible and supported a wide range of customized disk copy protection mechanisms. (See Software Cracking: History.)\nSome operating systems, most notably pre-1995 Macintosh systems from Apple, are so closely interwoven with their hardware that it is impossible to natively boot an operating system other than the standard one. This is the opposite extreme of the scenario using switches mentioned above; it is highly inflexible but relatively error-proof and foolproof as long as all hardware is working normally. A common solution in such situations is to design a boot loader that works as a program belonging to the standard OS that hijacks the system and loads the alternative OS. This technique was used by Apple for its A/UX Unix implementation and copied by various freeware operating systems and BeOS Personal Edition 5.\nSome machines, like the Atari ST microcomputer, were \"instant-on\", with the operating system executing from a ROM. Retrieval of the OS from secondary or tertiary store was thus eliminated as one of the characteristic operations for bootstrapping. To allow system customizations, accessories, and other support software to be loaded automatically, the Atari's floppy drive was read for additional components during the boot process. There was a timeout delay that provided time to manually insert a floppy as the system searched for the extra components. This could be avoided by inserting a blank disk. The Atari ST hardware was also designed so the cartridge slot could provide native program execution for gaming purposes as a holdover from Atari's legacy making electronic games; by inserting the Spectre GCR cartridge with the Macintosh system ROM in the game slot and turning the Atari on, it could \"natively boot\" the Macintosh operating system rather than Atari's own TOS.\nThe IBM Personal Computer included ROM-based firmware called the BIOS; one of the functions of that firmware was to perform a power-on self test when the machine was powered up, and then to read software from a boot device and execute it. Firmware compatible with the BIOS on the IBM Personal Computer is used in IBM PC compatible computers. The UEFI was developed by Intel, originally for Itanium-based machines, and later also used as an alternative to the BIOS in x86-based machines, including Apple Macs using Intel processors.\nUnix workstations originally had vendor-specific ROM-based firmware. Sun Microsystems later developed OpenBoot, later known as Open Firmware, which incorporated a Forth interpreter, with much of the firmware being written in Forth. It was standardized by the IEEE as IEEE standard 1275-1994; firmware that implements that standard was used in PowerPC-based Macs and some other PowerPC-based machines, as well as Sun's own SPARC-based computers. The Advanced RISC Computing specification defined another firmware standard, which was implemented on some MIPS-based and Alpha-based machines and the SGI Visual Workstation x86-based workstations.\nModern boot loaders.\nWhen a computer is turned off, its software\u200d\u2014\u200cincluding operating systems, application code, and data\u200d\u2014\u200cremains stored on non-volatile memory. When the computer is powered on, it typically does not have an operating system or its loader in random-access memory (RAM). The computer first executes a relatively small program stored in read-only memory (ROM, and later EEPROM, NOR flash) which support execute in place, to initialize CPU and motherboard, to initialize the memory (especially on x86 systems), to initialize and access the storage (usually a block-addressed device, e.g. hard disk drive, NAND flash, solid-state drive) from which the operating system programs and data can be loaded into RAM, and to initialize other I/O devices.\nThe small program that starts this sequence is known as a \"bootstrap loader\", \"bootstrap\" or \"boot loader\". Often, multiple-stage boot loaders are used, during which several programs of increasing complexity load one after the other in a process of chain loading.\nSome earlier computer systems, upon receiving a boot signal from a human operator or a peripheral device, may load a very small number of fixed instructions into memory at a specific location, initialize at least one CPU, and then point the CPU to the instructions and start their execution. These instructions typically start an input operation from some peripheral device (which may be switch-selectable by the operator). Other systems may send hardware commands directly to peripheral devices or I/O controllers that cause an extremely simple input operation (such as \"read sector zero of the system device into memory starting at location 1000\") to be carried out, effectively loading a small number of boot loader instructions into memory; a completion signal from the I/O device may then be used to start execution of the instructions by the CPU.\nSmaller computers often use less flexible but more automatic boot loader mechanisms to ensure that the computer starts quickly and with a predetermined software configuration. In many desktop computers, for example, the bootstrapping process begins with the CPU executing software contained in ROM (for example, the BIOS of an IBM PC) at a predefined address (some CPUs, including the Intel x86 series are designed to execute this software after reset without outside help). This software contains rudimentary functionality to search for devices eligible to participate in booting, and load a small program from a special section (most commonly the boot sector) of the most promising device, typically starting at a fixed entry point such as the start of the sector.\nBoot loaders may face peculiar constraints, especially in size; for instance, on the IBM PC and compatibles, the boot code must fit in the Master Boot Record (MBR) and the Partition Boot Record (PBR), which in turn are limited to a single sector; on the IBM System/360, the size is limited by the IPL medium, e.g., card size, track size.\nOn systems with those constraints, the first program loaded into RAM may not be sufficiently large to load the operating system and, instead, must load another, larger program. The first program loaded into RAM is called a first-stage boot loader, and the program it loads is called a second-stage boot loader. On many embedded CPUs, the CPU built-in boot ROM, sometimes called the zero-stage boot loader, can find and load first-stage boot loaders.\nFirst-stage boot loaders.\nExamples of first-stage (hardware initialization stage) boot loaders include BIOS, UEFI, coreboot, Libreboot and Das U-Boot. On the IBM PC, the boot loader in the Master Boot Record (MBR) and the Partition Boot Record (PBR) was coded to require at least 32\u00a0KB (later expanded to 64\u00a0KB) of system memory and only use instructions supported by the original 8088/8086 processors.\nSecond-stage boot loaders.\nSecond-stage (OS initialization stage) boot loaders, such as shim, GNU GRUB, rEFInd, BOOTMGR, Syslinux, and NTLDR, are not themselves operating systems, but are able to load an operating system properly and transfer execution to it; the operating system subsequently initializes itself and may load extra device drivers. The second-stage boot loader does not need drivers for its own operation, but may instead use generic storage access methods provided by system firmware such as the BIOS, UEFI or Open Firmware, though typically with restricted hardware functionality and lower performance.\nMany boot loaders (like GNU GRUB, rEFInd, Windows's BOOTMGR, Syslinux, and Windows NT/2000/XP's NTLDR) can be configured to give the user multiple booting choices. These choices can include different operating systems (for dual or multi-booting from different partitions or drives), different versions of the same operating system (in case a new version has unexpected problems), different operating system loading options (e.g., booting into a rescue or safe mode), and some standalone programs that can function without an operating system, such as memory testers (e.g., memtest86+), a basic shell (as in GNU GRUB), or even games (see List of PC Booter games). Some boot loaders can also load other boot loaders; for example, GRUB loads BOOTMGR instead of loading Windows directly. Usually a default choice is preselected with a time delay during which a user can press a key to change the choice; after this delay, the default choice is automatically run so normal booting can occur without interaction.\nThe boot process can be considered complete when the computer is ready to interact with the user, or the operating system is capable of running system programs or application programs.\nFirst and second stages boot loaders.\nSome boot loaders, such as Das U-Boot and iBoot, included both first and second stages boot functions.\nEmbedded and multi-stage boot loaders.\nMany embedded systems must boot immediately. For example, waiting a minute for a digital television or a GPS navigation device to start is generally unacceptable. Therefore, such devices have software systems in ROM or flash memory so the device can begin functioning immediately; little or no loading is necessary, because the loading can be precomputed and stored on the ROM when the device is made.\nLarge and complex systems may have boot procedures that proceed in multiple phases until finally the operating system and other programs are loaded and ready to execute. Because operating systems are designed as if they never start or stop, a boot loader might load the operating system, configure itself as a mere process within that system, and then irrevocably transfer control to the operating system. The boot loader then terminates normally as any other process would.\nNetwork booting.\nMost computers are also capable of booting over a computer network. In this scenario, the operating system is stored on the disk of a server, and certain parts of it are transferred to the client using a simple protocol such as the Trivial File Transfer Protocol (TFTP). After these parts have been transferred, the operating system takes over the control of the booting process.\nAs with the second-stage boot loader, network booting begins by using generic network access methods provided by the network interface's boot ROM, which typically contains a Preboot Execution Environment (PXE) image. No drivers are required, but the system functionality is limited until the operating system kernel and drivers are transferred and started. As a result, once the ROM-based booting has completed it is entirely possible to network boot into an operating system that itself does not have the ability to use the network interface.\nIBM-compatible personal computers (PC).\nBoot devices.\nThe boot device is the storage device from which the operating system is loaded. A modern PC's UEFI or BIOS firmware supports booting from various devices, typically a local solid-state drive or hard disk drive via the GPT or Master Boot Record (MBR) on such a drive or disk, an optical disc drive (using El Torito), a USB mass storage device (USB flash drive, memory card reader, USB hard disk drive, USB optical disc drive, USB solid-state drive, etc.), or a network interface card (using PXE). Older, less common BIOS-bootable devices include floppy disk drives, Zip drives, and LS-120 drives. IBM-compatible PCs are examples that use horizontal integrated hardware and UEFI/BIOS firmware.\nTypically, the system firmware (UEFI or BIOS) will allow the user to configure a \"boot order\". If the boot order is set to \"first, the DVD drive; second, the hard disk drive\", then the firmware will try to boot from the DVD drive, and if this fails (e.g. because there is no DVD in the drive), it will try to boot from the local hard disk drive.\nFor example, on a PC with Windows installed on the hard drive, the user could set the boot order to the one given above, and then insert a Linux Live CD in order to try out Linux without having to install an operating system onto the hard drive. This is an example of dual booting, in which the user chooses which operating system to start after the computer has performed its power-on self-test (POST). In this example of dual booting, the user chooses by inserting or removing the DVD from the computer, but it is more common to choose which operating system to boot by selecting from a boot manager menu on the selected device, by using the computer keyboard to select from a BIOS or UEFI boot menu, or both; the boot menu is typically entered by pressing or keys during the POST; the BIOS setup is typically entered by pressing or keys during the POST.\nSeveral devices are available that enable the user to \"quick-boot\" into what is usually a variant of Linux for various simple tasks such as Internet access; examples are Splashtop and Latitude ON.\nBoot sequence.\nUpon starting, an IBM-compatible personal computer's x86 CPU, executes in real mode, the instruction located at reset vector (the physical memory address FFFF0h on 16-bit x86 processors and FFFFFFF0h on 32-bit and 64-bit x86 processors), usually pointing to the firmware (UEFI or BIOS) entry point inside the ROM. This memory location typically contains a jump instruction that transfers execution to the location of the firmware (UEFI or BIOS) start-up program. This program runs a power-on self-test (POST) to check and initialize required devices such as main memory (DRAM), the PCI bus and the PCI devices (including running embedded option ROMs). One of the most involved steps is setting up DRAM over SPD, further complicated by the fact that at this point memory is very limited.\nAfter initializing required hardware, the firmware (UEFI or BIOS) goes through a pre-configured list of non-volatile storage devices (\"boot device sequence\") until it finds one that is bootable.\nBIOS.\nOnce the BIOS has found a bootable device it loads the boot sector to linear address 7C00h (usually segment:offset 0000h:7C00h, but some BIOSes erroneously use 07C0h:0000h) and transfers execution to the boot code. In the case of a hard disk, this is referred to as the Master Boot Record (MBR). The conventional MBR code checks the MBR's partition table for a partition set as \"bootable\" (the one with \"active\" flag set). If an active partition is found, the MBR code loads the boot sector code from that partition, known as Volume Boot Record (VBR), and executes it. The MBR boot code is often operating-system specific.\nA bootable MBR device is defined as one that can be read from, and where the last two bytes of the first sector contain the little-endian word AA55h, found as byte sequence 55h, AAh on disk (also known as the MBR boot signature), or where it is otherwise established that the code inside the sector is executable on x86 PCs.\nThe boot sector code is the first-stage boot loader. It is located on fixed disks and removable drives, and must fit into the first 446 bytes of the Master Boot Record in order to leave room for the default 64-byte partition table with four partition entries and the two-byte boot signature, which the BIOS requires for a proper boot loader \u2014 or even less, when additional features like more than four partition entries (up to 16 with 16 bytes each), a disk signature (6 bytes), a disk timestamp (6 bytes), an Advanced Active Partition (18 bytes) or special multi-boot loaders have to be supported as well in some environments. In floppy and superfloppy Volume Boot Records, up to 59 bytes are occupied for the Extended BIOS Parameter Block on FAT12 and FAT16 volumes since DOS 4.0, whereas the FAT32 EBPB introduced with DOS 7.1 requires even 87 bytes, leaving only 423 bytes for the boot loader when assuming a sector size of 512 bytes. Microsoft boot sectors therefore traditionally imposed certain restrictions on the boot process, for example, the boot file had to be located at a fixed position in the root directory of the file system and stored as consecutive sectors, conditions taken care of by the codice_1 command and slightly relaxed in later versions of DOS. The boot loader was then able to load the first three sectors of the file into memory, which happened to contain another embedded boot loader able to load the remainder of the file into memory. When Microsoft added LBA and FAT32 support, they even switched to a boot loader reaching over \"two\" physical sectors and using 386 instructions for size reasons. At the same time other vendors managed to squeeze much more functionality into a single boot sector without relaxing the original constraints on only minimal available memory (32\u00a0KB) and processor support (8088/8086). For example, DR-DOS boot sectors are able to locate the boot file in the FAT12, FAT16 and FAT32 file system, and load it into memory as a whole via CHS or LBA, even if the file is not stored in a fixed location and in consecutive sectors.\nThe VBR is often OS-specific; however, its main function is to load and execute the operating system boot loader file (such as codice_2 or codice_3), which is the second-stage boot loader, from an active partition. Then the boot loader loads the OS kernel from the storage device.\nIf there is no active partition, or the active partition's boot sector is invalid, the MBR may load a secondary boot loader which will select a partition (often via user input) and load its boot sector, which usually loads the corresponding operating system kernel. In some cases, the MBR may also attempt to load secondary boot loaders before trying to boot the active partition. If all else fails, it should issue an INT 18h BIOS interrupt call (followed by an INT 19h just in case INT 18h would return) in order to give back control to the BIOS, which would then attempt to boot off other devices, attempt a remote boot via network.\nUEFI.\nMany modern systems (Intel Macs and newer PCs) use UEFI.\nUnlike BIOS, UEFI (not Legacy boot via CSM) does not rely on boot sectors, UEFI system loads the boot loader (EFI application file in USB disk or in the EFI System Partition) directly, and the OS kernel is loaded by the boot loader.\nSoCs, embedded systems, microcontrollers, and FPGAs.\nMany modern CPUs, SoCs and microcontrollers (for example, TI OMAP) or sometimes even digital signal processors (DSPs) may have a boot ROM integrated directly into their silicon, so such a processor can perform a simple boot sequence on its own and load boot programs (firmware or software) from boot sources such as NAND flash or eMMC. It is difficult to hardwire all the required logic for handling such devices, so an integrated boot ROM is used instead in such scenarios. Also, a boot ROM may be able to load a boot loader or diagnostic program via serial interfaces like UART, SPI, USB and so on. This feature is often used for system recovery purposes, or it could also be used for initial non-volatile memory programming when there is no software available in the non-volatile memory yet. Many modern microcontrollers (e.g. flash memory controller on USB flash drives) have firmware ROM integrated directly into their silicon.\nSome embedded system designs may also include an intermediary boot sequence step. For example, Das U-Boot may be split into two stages: the platform would load a small SPL (Secondary Program Loader), which is a stripped-down version of U-Boot, and the SPL would do some initial hardware configuration (e.g. DRAM initialization using CPU cache as RAM) and load the larger, fully featured version of U-Boot. Such embedded systems may used highly customized and vertical integrated hardware and software, and their boot programs may be simpler. Some CPUs and SoCs may not use CPU cache as RAM on boot process, they use an integrated boot processor to do some hardware configuration, to reduce cost.\nIt is also possible to take control of a system by using a hardware debug interface such as JTAG. Such an interface may be used to write the boot loader program into bootable non-volatile memory (e.g. flash) by instructing the processor core to perform the necessary actions to program non-volatile memory. Alternatively, the debug interface may be used to upload some diagnostic or boot code into RAM, and then to start the processor core and instruct it to execute the uploaded code. This allows, for example, the recovery of embedded systems where no software remains on any supported boot device, and where the processor does not have any integrated boot ROM. JTAG is a standard and popular interface; many CPUs, microcontrollers and other devices are manufactured with JTAG interfaces (as of 2009[ [update]]).\nSome microcontrollers provide special hardware interfaces that cannot be used to take arbitrary control of a system or directly run code, but instead they allow the insertion of boot code into bootable non-volatile memory (like flash memory) via simple protocols. Then at the manufacturing phase, such interfaces are used to inject boot code (and possibly other code) into non-volatile memory. After system reset, the microcontroller begins to execute code programmed into its non-volatile memory, just like usual processors are using ROMs for booting. Most notably, this technique is used by Atmel AVR microcontrollers, and by others as well. In many cases such interfaces are implemented by hardwired logic. In other cases such interfaces could be created by software running in integrated on-chip boot ROM from GPIO pins.\nMost DSPs have a serial mode boot and a parallel mode boot, such as the host port interface (HPI boot).\nIn the case of DSPs, there is often a second microprocessor or microcontroller present in the system design, and this is responsible for overall system behavior, interrupt handling, dealing with external events, user interface, etc., while the DSP is dedicated to signal processing tasks only. In such systems, the DSP could be booted by another processor which is sometimes referred as the \"host processor\" (giving name to a Host Port). Such a processor is also sometimes referred as the \"master\", since it usually boots first from its own memories and then controls overall system behavior, including booting of the DSP, and then further controlling the DSP's behavior. The DSP often lacks its own boot memories and relies on the host processor to supply the required code instead. The most notable systems with such a design are cell phones, modems, audio and video players and so on, where a DSP and a CPU/microcontroller coexist.\nMany FPGA chips load their configuration from an external configuration ROM, typically a serial EEPROM, on power-up.\nSecurity.\nVarious measures have been implemented that enhance the security of the booting process. Some of them are made mandatory, others can be disabled or enabled by the end user. Traditionally, booting did not involve the use of cryptography. The security can be bypassed by unlocking the boot loader, which might or might not be approved by the manufacturer. Modern boot loaders make use of concurrency, meaning they can run multiple processor cores and threads at the same time, which add extra layers of complexity to secure booting.\nMatthew Garrett argued that booting security serves a legitimate goal but in doing so chooses defaults that are hostile to users.\nBootloop.\nWhen debugging a concurrent and distributed system of systems, a bootloop (also written boot loop or boot-loop) is a diagnostic condition of an erroneous state that occurs on computing devices; when those devices repeatedly fail to complete the booting process and restart before a boot sequence is finished, a restart might prevent a user from accessing the regular interface.\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;As the complexity of today's products increases, single projects, single departments or even single companies can no longer develop total products, causing concurrent and distributed development. Today and worldwide, industries are facing complex product development and its vast array of associated problems, relating to project organization, project control and product quality. Many processes will become distributed as well. The defect detection process, so important for measuring and eventually achieving product quality, is typically one of the first to experience problems caused by the distributed nature of the project. The distribution of defect detection activities over several parties introduces risks like the inadequate review of work products, occurrence of \"blind spots\" with respect to test coverage or over-testing of components. Lifecycle-wide coordination of defect detection is therefore needed to ensure effectiveness and efficiency of defect detection activities. \u2014J.J.M. Trienekens; R.J. Kusters. (2004)\nDetection of an erroneous state.\nThe system might exhibit its erroneous state in, for example, an explicit bootloop or a blue screen of death, before recovery is indicated. Detection of an erroneous state may require a distributed event store and stream-processing platform for real-time operation of a distributed system.\nRecovery from an erroneous state.\nAn erroneous state can trigger bootloops; this state can be caused by misconfiguration from previously known-good operations. Recovery attempts from that erroneous state then enter a reboot, in an attempt to return to a known-good state. In Windows OS operations, for example, the recovery procedure was to reboot three times, the reboots needed to return to a usable menu.\nRecovery policy.\nRecovery might be specified via Security Assertion Markup Language (SAML), which can also implement Single sign-on (SSO) for some applications; in the zero trust security model identification, authorization, and authentication are separable concerns in an SSO session. When recovery of a site is indicated (viz. a blue screen of death is displayed on an airport terminal screen) personal site visits might be required to remediate the situation.\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "40910", "revid": "31711350", "url": "https://en.wikipedia.org/wiki?curid=40910", "title": "Common carrier", "text": "Term in common law legal systems for transporters of goods/people\nA common carrier in common law countries (corresponding to a public carrier in some civil law systems, usually called simply a carrier) is a person or company that transports goods or people for any person or company and is responsible for any possible loss of the goods during transport. A common carrier offers its services to the general public under license or authority provided by a regulatory body, which has usually been granted \"ministerial authority\" by the legislation that created it. The regulatory body may create, interpret, and enforce its regulations upon the common carrier (subject to judicial review) with independence and finality as long as it acts within the bounds of the enabling legislation.\nA common carrier (also called a \"public carrier\" in British English) is distinguished from a contract carrier, which is a carrier that transports goods for only a certain number of clients and that can refuse to transport goods for anyone else, and from a private carrier. A common carrier holds itself out to provide service to the general public without discrimination (to meet the needs of the regulator's quasi-judicial role of impartiality toward the public's interest) for the \"public convenience and necessity.\" A common carrier must further demonstrate to the regulator that it is \"fit, willing, and able\" to provide those services for which it is granted authority. Common carriers typically transport persons or goods according to defined and published routes, time schedules, and rate tables upon the approval of regulators. Public airlines, railroads, bus lines, taxicab companies, phone companies, internet service providers, cruise ships, motor carriers (i.e., canal operating companies, trucking companies), and other freight companies generally operate as common carriers. Under US law, an ocean freight forwarder cannot act as a common carrier.\nThe term \"common carrier\" is a common law term and is seldom used in Continental Europe because it has no exact equivalent in civil-law systems. In Continental Europe, the functional equivalent of a common carrier is referred to as a \"public carrier\" or simply as a \"carrier\". However, \"public carrier\" in Continental Europe is different from \"public carrier\" in British English in which it is a synonym for \"contract carrier.\"\nGeneral.\nAlthough common carriers generally transport people or goods, in the United States the term may also refer to telecommunications service providers and public utilities. In certain U.S. states, amusement parks that operate roller coasters and comparable rides have been found to be common carriers; a famous example is Disneyland.\nRegulatory bodies may also grant carriers the authority to operate under contract with their customers instead of under common carrier authority, rates, schedules and rules. These regulated carriers, known as contract carriers, must demonstrate that they are \"fit, willing and able\" to provide service, according to standards enforced by the regulator. However, contract carriers are specifically not required to demonstrate that they will operate for the \"public convenience and necessity.\" A contract carrier may be authorized to provide service over either fixed routes and schedules, i.e., as regular route carrier or on an \"ad hoc\" basis as an irregular route carrier.\nIt should be mentioned that the carrier refers only to the person (legal or physical) that enters into a contract of carriage with the shipper. The carrier does not necessarily have to own or even be in the possession of a means of transport. Unless otherwise agreed upon in the contract, the carrier may use whatever means of transport approved in its operating authority, as long as it is the most favorable from the cargo interests' point of view. The carriers' duty is to get the goods to the agreed destination within the agreed time or within reasonable time.\nThe person that is physically transporting the goods on a means of transport is referred to as the \"actual carrier\". When a carrier subcontracts with another provider, such as an independent contractor or a third-party carrier, the common carrier is said to be providing \"substituted service\". The same person may hold both common carrier and contract carrier authority. In the case of a rail line in the US, the owner of the property is said to retain a \"residual common carrier obligation\", unless otherwise transferred (such as in the case of a commuter rail system, where the authority operating passenger trains may acquire the property but not this obligation from the former owner), and must operate the line if service is terminated. \nIn contrast, private carriers are not licensed to offer a service to the public. Private carriers generally provide transport on an irregular or \"ad hoc\" basis for their owners.\nCarriers were very common in rural areas prior to motorised transport. Regular services by horse-drawn vehicles would ply to local towns, taking goods to market or bringing back purchases for the village. If space permitted, passengers could also travel.\nCases have also established limitations to the common carrier designation. In a case concerning a hot air balloon, \"Grotheer v. Escape Adventures, Inc.\", the court affirmed a hot air balloon was not a common carrier, holding the key inquiry in determining whether or not a transporter can be classified as a common carrier is whether passengers expect the transportation to be safe because the operator is reasonably capable of controlling the risk of injury.\nTelecommunications.\nIn the United States, telecommunications carriers are regulated by the Federal Communications Commission under title II of the Communications Act of 1934.\nThe Telecommunications Act of 1996 made extensive revisions to the \"Title II\" provisions regarding common carriers and repealed the judicial 1982 AT&amp;T consent decree (often referred to as the Modification of Final Judgment) that effectuated the breakup of AT&amp;T's Bell System. Further, the Act gives telephone companies the option of providing video programming on a common carrier basis or as a conventional cable television operator. If it chooses the former, the telephone company will face less regulation but will also have to comply with FCC regulations requiring what the Act refers to as \"open video systems\". The Act generally bars, with certain exceptions including most rural areas, acquisitions by telephone companies of more than a 10 percent interest in cable operators (and vice versa) and joint ventures between telephone companies and cable systems serving the same areas.\nInternet Service Providers.\nUsing provisions of the Communications Act of 1934, the FCC classified Internet service providers as common carriers, effective June 12, 2015, for the purpose of enforcing net neutrality. Led by the Trump administration's appointed commissioner Ajit Pai, on December 14, 2017 the FCC reversed its rules on net neutrality, effectively revoking common carrier status as a requirement for Internet service providers. Following this, in 2018 the U.S. Senate narrowly passed a non-binding resolution aiming to reverse the FCC's decision and restore FCC's net neutrality rules. On 25 April 2024, the FCC voted 3\u20132 to reinstate net neutrality in the United States by reclassifying the Internet under Title II. However, legal challenges filed by ISPs resulted in an appeals court order that stays the net neutrality rules until the court makes a final ruling, with the court opining that the ISPs are likely to prevail over the FCC on the merits.\nPipelines.\nIn the United States, many oil, gas and CO2 pipelines are common carriers. The Federal Energy Regulatory Commission (FERC) regulates rates charged and other tariff terms imposed by interstate common carrier pipelines. Intrastate common carrier pipeline tariffs are often regulated by state agencies. The US and many states have delegated the power of eminent domain to common carrier gas pipelines.\nLegal implications.\nCommon carriers are subject to special laws and regulations that differ depending on the means of transport used, e.g. sea carriers are often governed by quite different rules from road carriers or railway carriers. In common law jurisdictions as well as under international law, a common carrier is absolutely liable for goods carried by it, with four exceptions:\nA sea carrier may also, according to the Hague-Visby Rules, escape liability on other grounds than the above-mentioned, e.g. a sea carrier is not liable for damages to the goods if the damage is the result of a fire on board the ship or the result of a navigational error committed by the ship's master or other crewmember.\nCarriers typically incorporate further exceptions into a contract of carriage, often specifically claiming not to be a common carrier.\nAn important legal requirement for common carrier as public provider is that it cannot \"discriminate,\" that is refuse the service unless there is some compelling reason. As of 2007, the status of Internet service providers as common carriers and their rights and responsibilities is widely debated (network neutrality).\nThe term common carrier does not exist in continental Europe but is distinctive to common law systems, particularly law systems in the US.\nIn \"Ludditt v Ginger Coote Airways\" the Privy Council (Lord Macmillan, Lord Wright, Lord Porter and Lord Simonds) held the liability of a public or common carrier of passengers is only to carry with due care. This is more limited than that of a common carrier of goods. The complete freedom of a carrier of passengers at common law to make such contracts as he thinks fit was not curtailed by the Railway and Canal Traffic Act 1854, and a specific contract that enlarges, diminishes or excludes his duty to take care (e.g., by a condition that the passenger travels \"at his own risk against all casualties\") cannot be pronounced to be unreasonable if the law authorises it. There was nothing in the provisions of the Canadian Transport Act 1938 section 25 that would invalidate a provision excluding liability. \"Grand Trunk Railway Co of Canada v Robinson\" [1915] A.C. 740 was followed and \"Peek v North Staffordshire Railway\" 11 E.R. 1109 was distinguished.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "40911", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=40911", "title": "Common control", "text": "In telecommunications, common control is a principle of switching telephone calls in an automatic telephone exchange that employs shared control equipment which is attached to the circuit of a call only for the duration of establishing or otherwise controlling the call. Thus, such control equipment need only be provided in as few units to satisfy overall exchange traffic, rather than being duplicated for every subscriber line.\nIn contrast, \"direct control\" systems have subsystems for call control that are an integral part of the switching network. Strowger exchanges are usually direct control systems, whereas crossbar, and electronic exchanges (including all stored program control systems) are common control systems. Common control is also known as indirect control or register control.\nHistory.\nEarly semi-mechanical installations with common control components existed, for example rotary systems in Sweden and France in 1915, and the first panel switches in Newark, New Jersey, also in 1915. The first large-scale, fully automatic, common control switching system deployed in commercial production service was the \"ATlantic\" central office in Omaha, Nebraska, a panel system cut over on December 10, 1921. Other panel offices for Kansas City and New York City (the \"PENnsylvania exchange\") were in planning at the same time and opened shortly after.\nIn 1922, common control was introduced in Strowger-type step-by-step systems, resulting in the first installations of Director systems in Havana, Cuba in 1924, and in London, England in 1927.\nBy the mid-1920s, common control ideas had extended to include marker systems for testing for idle trunks.\nDuring the 1960s, common control exchanges became stored program control exchanges, and by the 1970s they used common-channel signaling in which the channels that are used for signaling are not used for message traffic (out of band signaling).\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "40912", "revid": "35936988", "url": "https://en.wikipedia.org/wiki?curid=40912", "title": "Common Management Information Service", "text": "The Common Management Information Service (CMIS) is the service interface specified in http:// that is employed by OSI network elements for network management. It defines the service interface that is implemented by the Common Management Information Protocol (CMIP) as specified in http://. CMIS is part of the Open Systems Interconnection (OSI) body of international network standards.\nNote the term CMIP is sometimes used erroneously when CMIS is intended. CMIS/CMIP is most often used in telecommunications applications, in other areas SNMP has become more popular.\nServices.\nThe following services are made available by the Common Management Information Service Element (CMISE) to allow management of network elements:\nManagement association services.\nTo transfer management information between open systems using CMIS/CMIP, peer connections, \"i.e.,\" associations, must be established. This requires the establishment of an Application layer association, a Session layer connection, a Transport layer connection, and, depending on supporting communications technology, Network layer and Link layer connections.\nCMIS initially defined management association services but it was later decided these services could be provided by ACSE and these services were removed. Below is a list of these services which were subsequently removed from ISO 9595:"}
{"id": "40913", "revid": "3727527", "url": "https://en.wikipedia.org/wiki?curid=40913", "title": "Common-mode interference", "text": ""}
{"id": "40914", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=40914", "title": "Communications", "text": ""}
{"id": "40915", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=40915", "title": "Communications blackout", "text": "Halt to communication abilities or utilization\nIn telecommunications, communications blackouts are\nTechnical failures.\nUptime being a key goal of most communications networks, power supplies and backup generators are typically used to ensure high-reliability power.\nWireless networks may be subject to radio jamming; wired networks can be physically severed. Network design can also play a role in maintaining communications reliability; depending on the constraints in building a fiber-optic network, a self-healing ring topology may be used.\nSpacecraft reentry.\nThe communications blackouts that affect spacecraft re-entering the Earth's atmosphere, which are also known as radio blackouts, ionization blackouts, or reentry blackouts, are caused by an envelope of ionized air around the craft, created by the heat from the compression of the atmosphere by the craft. The ionized air interferes with radio signals. For the Mercury, Gemini, and Apollo spacecraft, such communications blackouts lasted for several minutes. Gemini 2, for example, endured such a blackout for four minutes, beginning at 9 minutes 5 seconds into the descent.\nFor Apollo missions, the communications blackout was approximately three minutes long. For Apollo 16, for example, pre-advisory data (PAD) for re-entry listed the expected times for re-entry communications blackout to be from 0 minutes 16 seconds after entry interface to 3 minutes 33 seconds after entry interface (a total of 3 minutes 17 seconds). For the Apollo 13 mission, the blackout was much longer than normal because the flight path of the spacecraft was unexpectedly at a much shallower angle than normal. According to the mission log maintained by Gene Kranz, the Apollo 13 re-entry blackout lasted around 6 minutes, beginning at 142:39 and ending at 142:45, and was 1 minute 27 seconds longer than had been predicted.\nCommunications blackouts for re-entry are not solely confined to entry into Earth's atmosphere. They apply to entry into any atmosphere where such ionization occurs around a craft. The Mars Pathfinder endured a 30-second communications blackout as it entered Mars' atmosphere, for example. The Huygens probe endured a communications blackout as it entered the atmosphere of Titan.\nUntil the creation of the Tracking and Data Relay Satellite System (TDRSS), the Space Shuttle endured a 30-minute blackout. The TDRSS allowed the Shuttle to communicate by relay with a Tracking and Data Relay Satellite during re-entry, through a \"hole\" in the ionized air envelope at the tail end of the craft, created by the Shuttle's shape.\nSpace weather.\nRadio blackouts on Earth caused by solar flares are measured by the National Oceanic and Atmospheric Administration on a scale from 1 (minor) to 5 (extreme).\nSolar position.\nCommunications can also be lost when the Sun is blocking or behind one station in the same line of sight; Sun outages periodically interrupt communications with geosynchronous satellites. It is also a common problem for interplanetary space missions.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "40916", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=40916", "title": "Communications center", "text": "In telecommunications, the term communications center has the following meanings: "}
{"id": "40918", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=40918", "title": "Communications deception", "text": "In telecommunications, the term communications deception has the following meanings:"}
{"id": "40919", "revid": "28422", "url": "https://en.wikipedia.org/wiki?curid=40919", "title": "Communications-electronics", "text": "In telecommunications, communications-electronics (C-E) is the specialized field concerned with the use of electronic devices and systems for the acquisition or acceptance, processing, storage, display, analysis, protection, disposition, and transfer of information.\nC-E includes the wide range of responsibilities and actions relating to:\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "40920", "revid": "5400961", "url": "https://en.wikipedia.org/wiki?curid=40920", "title": "Communications jamming", "text": ""}
{"id": "40921", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=40921", "title": "Communications protection", "text": "Application of communications security measures to telecommunications systems\nIn telecommunications, communications protection is the application of communications security (COMSEC) measures to telecommunications systems in order to: (a) deny unauthorized access to sensitive unclassified information of value, (b) prevent disruption of telecommunications services, or (c) ensure the authenticity of information handled by telecommunications systems.\nReferences.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "40922", "revid": "34074157", "url": "https://en.wikipedia.org/wiki?curid=40922", "title": "Communications security", "text": "Discipline of telecommunications\nCommunications security is the discipline of preventing unauthorized interceptors from accessing telecommunications in an intelligible form, while still delivering content to the intended recipients.\nIn the North Atlantic Treaty Organization culture, including United States Department of Defense culture, it is often referred to by the abbreviation COMSEC. The field includes cryptographic security, transmission security, emissions security and physical security of COMSEC equipment and associated keying material.\nCOMSEC is used to protect both classified and unclassified traffic on military communications networks, including voice, video, and data. It is used for both analog and digital applications, and both wired and wireless links.\nVoice over secure internet protocol VOSIP has become the de facto standard for securing voice communication, replacing the need for Secure Terminal Equipment (STE) in much of NATO, including the U.S.A. USCENTCOM moved entirely to VOSIP in 2008.\nRelated terms.\nTypes of COMSEC equipment:\nDoD Electronic Key Management System.\nThe Electronic Key Management System (EKMS) is a United States Department of Defense (DoD) key management, COMSEC material distribution, and logistics support system. The National Security Agency (NSA) established the EKMS program to supply electronic key to COMSEC devices in securely and timely manner, and to provide COMSEC managers with an automated system capable of ordering, generation, production, distribution, storage, security accounting, and access control.\nThe Army's platform in the four-tiered EKMS, AKMS, automates frequency management and COMSEC management operations. It eliminates paper keying material, hardcopy Signal operating instructions (SOI) and saves the time and resources required for courier distribution. It has 4 components:\nKey Management Infrastructure (KMI) Program.\nKMI is intended to replace the legacy Electronic Key Management System to provide a means for securely ordering, generating, producing, distributing, managing, and auditing cryptographic products (e.g., asymmetric keys, symmetric keys, manual cryptographic systems, and cryptographic applications). This system is currently being fielded by Major Commands and variants will be required for non-DoD Agencies with a COMSEC Mission."}
{"id": "40923", "revid": "13833", "url": "https://en.wikipedia.org/wiki?curid=40923", "title": "Communications subsystem", "text": ""}
{"id": "40924", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=40924", "title": "Communications survivability", "text": "Telecommunications engineering ability\nIn telecommunications, communications survivability is the ability of communications systems to continue to operate effectively under adverse conditions, though portions of the system may be damaged or destroyed. \nVarious methods may be used to maintain communications services, such as using alternate routing, different transmission media or methods, redundant equipment, and sites and equipment that are radiation hardened.\nReferences.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "40925", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=40925", "title": "Communications system", "text": "Assembled components serving a common purpose for communications\nA communications system is a collection of individual telecommunications networks systems, relay stations, tributary stations, and terminal equipment usually capable of interconnection and interoperation to form an integrated whole. Communication systems allow the transfer of information from one place to another or from one device to another through a specified channel or medium. The components of a communications system serve a common purpose, are technically compatible, use common procedures, respond to controls, and operate in union.\nIn the structure of a communication system, the transmitter first converts the data received from the source into a light signal and transmits it through the medium to the destination of the receiver. The receiver connected at the receiving end converts it to digital data, maintaining certain protocols e.g. FTP, ISP assigned protocols etc.\nTelecommunications is a method of communication (e.g., for sports broadcasting, mass media, journalism, etc.). Communication is the act of conveying intended meanings from one entity or group to another through the use of mutually understood signs and semiotic rules.\nTypes.\nBy media.\nAn optical communication system is any form of communications system that uses light as the transmission medium. Equipment consists of a transmitter, which encodes a \"message\" into an optical \"signal\", a \"communication channel\", which carries the signal to its destination, and a receiver, which reproduces the message from the received optical signal. Fiber-optic communication systems transmit information from one place to another by sending light through an optical fiber. The light forms a carrier signal that is modulated to carry information.\nA radio communication system is composed of several communications subsystems that give exterior communications capabilities. A radio communication system comprises a transmitting conductor in which electrical oscillations or currents are produced and which is arranged to cause such currents or oscillations to be propagated through the free space medium from one point to another remote therefrom and a receiving conductor at such distant point adapted to be excited by the oscillations or currents propagated from the transmitter.\nPower-line communication systems operate by impressing a modulated carrier signal on power wires. Different types of power-line communications use different frequency bands, depending on the signal transmission characteristics of the power wiring used. Since the power wiring system was originally intended for transmission of AC power, the power wire circuits have only a limited ability to carry higher frequencies. The propagation problem is a limiting factor for each type of power line communications.\nBy technology.\nA duplex communication system is a system composed of two connected parties or devices which can communicate with one another in both directions. The term \"duplex\" is used when describing communication between two parties or devices. Duplex systems are employed in nearly all communications networks, either to allow for a communication \"two-way street\" between two connected parties or to provide a \"reverse path\" for the monitoring and remote adjustment of equipment in the field. An antenna is basically a small length of a conductor that is used to radiate or receive electromagnetic waves. It acts as a conversion device. At the transmitting end it converts high frequency current into electromagnetic waves. At the receiving end it transforms electromagnetic waves into electrical signals that is fed into the input of the receiver. several types of antenna are used in communication.\nExamples of communications subsystems include the Defense Communications System (DCS).\nBy application area.\nThe term transmission system is used in the telecommunications industry to emphasize the intermediate media, protocols, and equipment in the circuit, rather than particular end-user applications.\nA tactical communications system is a communications system that \n(a) is used within, or in direct support of tactical forces\n(b) is designed to meet the requirements of changing tactical situations and varying environmental conditions, \n(c) provides securable communications, such as voice, data, and video, among mobile users to facilitate command and control within, and in support of, tactical forces, and \n(d) usually requires extremely short installation times, usually on the order of hours, in order to meet the requirements of frequent relocation.\nAn Emergency communication system is any system (typically computer based) that is organized for the primary purpose of supporting the two way communication of emergency messages between both individuals and groups of individuals. These systems are commonly designed to integrate the cross-communication of messages between are variety of communication technologies.\nAn Automatic call distributor (ACD) is a communication system that automatically queues, assigns and connects callers to handlers. This is used often in customer service (such as for product or service complaints), ordering by telephone (such as in a ticket office), or coordination services (such as in air traffic control).\nA Voice Communication Control System (VCCS) is essentially an ACD with characteristics that make it more adapted to use in critical situations (no waiting for dial tone, or lengthy recorded announcements, radio and telephone lines equally easily connected to, individual lines immediately accessible etc..)\nKey components.\nSources.\nSources can be classified as electric or non-electric; they are the origins of a message or input signal. Examples of sources include but are not limited to the following:\nInput transducers (sensors).\nSensors, like microphones and cameras, capture non-electric sources, like sound and light (respectively), and convert them into electrical signals. These types of sensors are called input transducers in modern analog and digital communication systems. Without input transducers there would not be an effective way to transport non-electric sources or signals over great distances, i.e. humans would have to rely solely on our eyes and ears to see and hear things despite the distances.\nOther examples of input transducers include:\nTransmitter.\nOnce the source signal has been converted into an electric signal, the transmitter will modify this signal for efficient transmission. In order to do this, the signal must pass through an electronic circuit containing the following components:\nAfter the signal has been amplified, it is ready for transmission. At the end of the circuit is an antenna, the point at which the signal is released as electromagnetic waves (or electromagnetic radiation).\nCommunication channel.\nA communication channel is simply referring to the medium by which a signal travels. There are two types of media by which electrical signals travel, i.e. guided and unguided. Guided media refers to any medium that can be directed from transmitter to receiver by means of connecting cables. In optical fiber communication, the medium is an optical (glass-like) fiber. Other guided media might include coaxial cables, telephone wire, twisted-pairs, etc... The other type of media, unguided media, refers to any communication channel that creates space between the transmitter and receiver. For radio or RF communication, the medium is air. Air is the only thing between the transmitter and receiver for RF communication while in other cases, like sonar, the medium is usually water because sound waves travel efficiently through certain liquid media. Both types of media are considered unguided because there are no connecting cables between the transmitter and receiver. Communication channels include almost everything from the vacuum of space to solid pieces of metal; however, some mediums are preferred more than others. That is because differing sources travel through subjective mediums with fluctuating efficiencies.\nReceiver.\nOnce the signal has passed through the communication channel, it must be effectively captured by a receiver. The goal of the receiver is to capture and reconstruct the signal before it passed through the transmitter (i.e. the A/D converter, modulator and encoder). This is done by passing the \"received\" signal through another circuit containing the following components:\nMost likely the signal will have lost some of its energy after having passed through the communication channel or medium. The signal can be boosted by passing it through a signal amplifier. When the analog signal converted into digital signal.\nOutput transducer.\nThe output transducer simply converts the electric signal (created by the input transducer) back into its original form. Examples of output transducers include but are not limited to the following:\nOther.\nSome common pairs of input and output transducers include:\nAgain, input transducers convert non-electric signals like voice into electric signals that can be transmitted over great distances very quickly. Output transducers convert the electric signal back into sound or picture, etc... There are many different types of transducers and the combinations are limitless.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nFurther reading.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt; "}
{"id": "40926", "revid": "314825287", "url": "https://en.wikipedia.org/wiki?curid=40926", "title": "Communications system engineering", "text": ""}
{"id": "40927", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=40927", "title": "Companding", "text": "Method of mitigating the detrimental effects of a channel with limited dynamic range\nIn telecommunications and signal processing, companding (occasionally called compansion) is a method of mitigating the detrimental effects of a channel with limited dynamic range. The name is a portmanteau of the words compressing and expanding, which are the functions of a compander at the transmitting and receiving ends, respectively. The use of companding allows signals with a large dynamic range to be transmitted over facilities that have a smaller dynamic range capability. Companding is employed in telephony and other audio applications such as professional wireless microphones and analog recording.\nHow it works.\nThe dynamic range of a signal is compressed before transmission and is expanded to the original value at the receiver. The electronic circuit that does this is called a compander and works by compressing or expanding the dynamic range of an analog electronic signal such as sound recorded by a microphone. One variety is a triplet of amplifiers: a logarithmic amplifier, followed by a variable-gain linear amplifier, and ending with an exponential amplifier. Such a triplet has the property that its output voltage is proportional to the input voltage raised to an adjustable power.\nCompanded quantization is the combination of three functional building blocks \u2013 namely, a (continuous-domain) signal dynamic range \"compressor\", a limited-range uniform quantizer, and a (continuous-domain) signal dynamic range \"expander\" that inverts the compressor function. This type of quantization is frequently used in telephony systems.\nIn practice, companders are designed to operate according to relatively simple dynamic range compressor functions that are suitable for implementation as simple analog electronic circuits. The two most popular compander functions used for telecommunications are the A-law and \u03bc-law functions.\nApplications.\nCompanding is used in digital telephony systems, compressing before input to an analog-to-digital converter, and then expanding after a digital-to-analog converter. This is equivalent to using a non-linear ADC as in a T-carrier telephone system that implements A-law or \u03bc-law companding. This method is also used in digital file formats for better signal-to-noise ratio (SNR) at lower bit depths. For example, a linearly encoded 16-bit PCM signal can be converted to an 8-bit WAV or AU file while maintaining a decent SNR by compressing before the transition to 8-bit and expanding after conversion back to 16-bit. This is effectively a form of lossy audio data compression.\nProfessional wireless microphones do this since the dynamic range of the microphone audio signal itself is larger than the dynamic range provided by radio transmission. Companding also reduces the noise and crosstalk levels at the receiver.\nCompanders are used in concert audio systems and in some noise reduction schemes.\nHistory.\nThe use of companding in an analog picture transmission system was patented by A. B. Clark of AT&amp;T in 1928 (filed in 1925):\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;In the transmission of pictures by electric currents, the method which consists in sending currents varied in a non-linear relation to the light values of the successive elements of the picture to be transmitted, and at the receiving end exposing corresponding elements of a sensitive surface to light varied in inverse non-linear relation to the received current.\u2014\u200a\nIn 1942, Clark and his team completed the SIGSALY secure voice transmission system that included the first use of companding in a PCM (digital) system.\nIn 1953, B. Smith showed that a nonlinear DAC could be complemented by the inverse nonlinearity in a successive-approximation ADC configuration, simplifying the design of digital companding systems.\nIn 1970, H. Kaneko developed the uniform description of segment (piecewise linear) companding laws that had by then been adopted in digital telephony.\nIn the 1980s and 1990s, many of the music equipment manufacturers (Roland, Yamaha, Korg) used companding when compressing the library waveform data in their digital synthesizers. However, exact algorithms are unknown, neither if any of the manufacturers ever used the Companding scheme which is described in this article. The only known thing is that manufacturers did use data compression in the mentioned time period and that some people refer to it as \"companding\" while in reality it might mean something else, for example data compression and expansion. This dates back to the late '80s when memory chips were often one of the most costly components in the instrument. Manufacturers usually quoted the amount of memory in its compressed form: i.e. 24 MB of physical waveform ROM in a Korg Trinity is actually 48 MB when uncompressed. Similarly, Roland SR-JV expansion boards were usually advertised as 8 MB boards with '16 MB-equivalent content'. Careless copying of this technical information, omitting the \"equivalence\" reference, can often cause confusion.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "40928", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=40928", "title": "Comparably efficient interconnection", "text": "Equal-access concept in telecommunications\nIn telecommunications, a comparably efficient interconnection (CEI) is an equal-access concept developed by the FCC stating that, \"...if a carrier offers an enhanced service, it should be required to offer network interconnection (or colocation) opportunities to others that are comparably efficient to the interconnection that its enhanced service enjoys. Accordingly, a carrier would be required to implement CEI only as it introduces new enhanced services.\"\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "40929", "revid": "50034649", "url": "https://en.wikipedia.org/wiki?curid=40929", "title": "Comparator", "text": "Device that compares two voltages or currents\nIn electronics, a comparator is a device that compares two voltages or currents and outputs a digital signal indicating which is larger. It has two analog input terminals formula_1 and formula_2 and one binary digital output formula_3. The output is ideally\n formula_4\nA comparator consists of a specialized high-gain differential amplifier. They are commonly used in devices that measure and digitize analog signals, such as analog-to-digital converters (ADCs), as well as relaxation oscillators.\nDifferential voltage.\nThe differential voltages must stay within the limits specified by the manufacturer. Early integrated comparators, like the LM111 family, and certain high-speed comparators like the LM119 family, require differential voltage ranges substantially lower than the power-supply voltages (\u00b115\u00a0V vs. 36\u00a0V). \"Rail-to-rail\" comparators allow any differential voltages within the power-supply range. When powered from a bipolar (dual rail) supply, \n formula_5\nor when powered from an unipolar TTL/CMOS power supply,\n formula_6.\nSpecific rail-to-rail comparators with p\u2013n\u2013p input transistors, like the LM139 family, allow the input potential to drop 0.3\u00a0volts \"below\" the negative supply rail, but do not allow it to rise above the positive rail. Specific ultra-fast comparators, like the LMH7322, allow the input signal to swing below the negative rail \"and\" above the positive rail, although by a narrow margin of only 0.2\u00a0V. Differential input voltage (the voltage between two inputs) of a modern rail-to-rail comparator is usually limited only by the full swing of power supply.\nOp-amp voltage comparator.\nAn operational amplifier (op-amp) has a well balanced difference input and a very high gain. This parallels the characteristics of comparators and can be substituted in applications with low-performance requirements.\nA comparator circuit compares two voltages and outputs either a 1 (the voltage at the plus side) or a 0 (the voltage at the negative side) to indicate which is larger. Comparators are often used, for example, to check whether an input has reached some predetermined value. In most cases a comparator is implemented using a dedicated comparator IC, but op-amps may be used as an alternative. Comparator diagrams and op-amp diagrams use the same symbols.\nA simple comparator circuit made using an op-amp without feedback simply heavily amplifies the voltage difference between Vin and VREF and outputs the result as Vout. If Vin is greater than VREF, then voltage at Vout will rise to its positive saturation level; that is, to the voltage at the positive side. If Vin is lower than VREF, then Vout will fall to its negative saturation level, equal to the voltage at the negative side.\nIn practice, this circuit can be improved by incorporating a hysteresis voltage range to reduce its sensitivity to noise.\nBecause of the difference in characteristics of an operational amplifier and comparator, using an operational amplifier as a comparator presents several disadvantages as compared to using a dedicated comparator.\nDesign.\nA comparator consists of a high gain differential amplifier whose output is compatible with the logic gates used in the digital circuit. The gain is high enough that a very small difference between the input voltages will saturate the output, the output voltage will be in either the low logic voltage band or the high logic voltage band of the gate input. Analogue op amps have been used as comparators, however a dedicated comparator chip will generally be faster than a general-purpose operational amplifier used as a comparator, and may also contain additional features such as an accurate, internal reference voltage, adjustable hysteresis, and a clock gated input.\nA dedicated voltage comparator chip such as LM339 is designed to interface with a digital logic interface (to a TTL or a CMOS). The output is a binary state often used to interface real world signals to digital circuitry (see analog-to-digital converter). If there is a fixed voltage source from, for example, a DC adjustable device in the signal path, a comparator is just the equivalent of a cascade of amplifiers. When the voltages are nearly equal, the output voltage will not fall into one of the logic levels, thus analog signals will enter the digital domain with unpredictable results. To make this range as small as possible, the amplifier cascade is high gain. The circuit consists of mainly bipolar transistors. For very high frequencies, the input impedance of the stages is low. This reduces the saturation of the slow, large p\u2013n junction bipolar transistors that would otherwise lead to long recovery times. Fast small Schottky diodes, like those found in binary logic designs, improve the performance significantly though the performance still lags that of circuits with amplifiers using analog signals. Slew rate has no meaning for these devices. For applications in flash ADCs the distributed signal across eight ports matches the voltage and current gain after each amplifier, and resistors then behave as level-shifters.\nOpen collector output.\nSome comparators (e.g. LM339) use open collector output to help interface to different logic families. When the inverting input is at a higher voltage than the non inverting input, the output of the comparator connects to the negative power supply. When the non inverting input is higher than the inverting input, the output is high impedance, so the output voltage in this state can be set by an external pull-up resistor to a different voltage supply.\nKey specifications.\nWhile it is easy to understand the basic task of a comparator, that is, comparing two voltages or currents, several parameters must be considered while selecting a suitable comparator:\nSpeed and power.\nWhile in general comparators are \"fast,\" their circuits are not immune to the classic speed-power tradeoff. High speed comparators use transistors with larger aspect ratios and hence also consume more power. Depending on the application, select either a comparator with high speed or one that saves power. For example, nano-powered comparators in space-saving chip-scale packages (UCSP), DFN or SC70 packages such as MAX9027,&lt;ref name=qv_pk/4268&gt;&lt;/ref&gt; LTC1540, LPV7215, MAX9060,&lt;ref name=qv_pk/5823&gt;&lt;/ref&gt; and MCP6541, are ideal for ultra-low-power, portable applications. Likewise if a comparator is needed to implement a relaxation oscillator circuit to create a high speed clock signal then comparators having few Nanoseconds of propagation delay may be suitable. ADCMP572 (CML output), LMH7220 (LVDS Output), MAX999 (CMOS output / TTL output), LT1719 (CMOS output / TTL output), MAX9010 (TTL output), and MAX9601 (PECL output), are examples of some good high speed comparators.\nHysteresis.\nA comparator normally changes its output state when the voltage between its inputs crosses through approximately zero volts. Small voltage fluctuations due to noise, always present on the inputs, can cause undesirable rapid changes between the two output states when the input voltage difference is near zero volts. To prevent this output oscillation, a small hysteresis of a few millivolts is integrated into many modern comparators. \nFor example, the LTC6702, MAX9021, and MAX9031, have internal hysteresis desensitizing them from input noise. In place of one switching point, hysteresis introduces two: one for rising voltages, and one for falling voltages. The difference between the higher-level trip value (VTRIP+) and the lower-level trip value (VTRIP-) equals the hysteresis voltage (VHYST).\nIf the comparator does not have internal hysteresis or if the input noise is greater than the internal hysteresis then an external hysteresis network can be built using positive feedback from the output to the non-inverting input of the comparator. The resulting Schmitt trigger circuit gives additional noise immunity and a cleaner output signal. Some comparators such as LMP7300, LTC1540, MAX931, MAX971,&lt;ref name=qv_pk/1279&gt;&lt;/ref&gt; and ADCMP341, also provide the hysteresis control through a separate hysteresis pin. These comparators make it possible to add a programmable hysteresis without feedback or complicated equations. Using a dedicated hysteresis pin is also convenient if the source impedance is high since the inputs are isolated from the hysteresis network. When hysteresis is added then a comparator cannot resolve signals within the hysteresis band.\nOutput type.\nBecause comparators have only two output states, their outputs are either near zero or near the supply voltage. Bipolar rail-to-rail comparators have a common-emitter output that produces a small voltage drop between the output and each rail. That drop is equal to the collector-to-emitter voltage of a saturated transistor. When output currents are light, output voltages of CMOS rail-to-rail comparators, which rely on a saturated MOSFET, range closer to the rail voltages than their bipolar counterparts.\nOn the basis of outputs, comparators can also be classified as open-drain or push\u2013pull. Comparators with an open-drain output stage use an external pull-up resistor to a positive supply that defines the logic high level. Open-drain comparators are more suitable for mixed-voltage system design. Since the output has high impedance for logic high level, open-drain comparators can also be used to connect multiple comparators to a single bus. Push\u2013pull output does not need a pull-up resistor and can also source current, unlike an open-drain output.\nInternal reference.\nThe most frequent application for comparators is the comparison between a voltage and a stable reference. TL431 is widely used for this purpose. Most comparator manufacturers also offer comparators in which a reference voltage is integrated on to the chip. Combining the reference and comparator in one chip not only saves space, but also draws less supply current than a comparator with an external reference. ICs with wide range of references are available such as MAX9062 (200 mV reference), LT6700 (400 mV reference), ADCMP350 (600\u00a0mV reference), MAX9025 (1.236\u00a0V reference), MAX9040 (2.048\u00a0V reference), TLV3012 (1.24\u00a0V reference), and TSM109 (2.5\u00a0V reference). \nContinuous versus clocked.\nA continuous comparator will output either a \"1\" or a \"0\" any time a high or low signal is applied to its input and will change quickly when the inputs are updated. However, many applications only require comparator outputs at certain instances, such as in A/D converters and memory. By only strobing a comparator at certain intervals, higher accuracy and lower power can be achieved with a clocked (or dynamic) comparator structure, also called a latched comparator. Often latched comparators employ strong positive feedback for a \"regeneration phase\" when a clock is high, and have a \"reset phase\" when the clock is low. \nThis is in contrast to a continuous comparator, which can only employ weak positive feedback since there is no reset period.\nApplications.\nNull detectors.\nA null detector identifies when a given value is zero. Comparators are ideal for null detection comparison measurements, since they are equivalent to a very high gain amplifier with well-balanced inputs and controlled output limits. The null detector circuit compares two input voltages: an unknown voltage and a reference voltage, usually referred to as vu and vr. The reference voltage is usually on the non-inverting input (+), while the unknown voltage is usually on the inverting input (\u2212). (A circuit diagram would display the inputs according to their sign with respect to the output when a particular input is greater than the other.) Unless the inputs are nearly equal (see below), the output is either positive or negative, for example \u00b112\u00a0V. In the case of a null detector the aim is to detect when the input voltages are nearly equal, which gives the value of the unknown voltage since the reference voltage is known.\nWhen using a comparator as a null detector, accuracy is limited; an output of zero is given whenever the magnitude of the voltage difference multiplied by the gain of the amplifier is within the voltage limits. For example, if the gain is 106, and the voltage limits are \u00b16\u00a0V, then an output of zero will be given if the voltage difference is less than 6\u00a0\u03bcV. One could refer to this as a fundamental uncertainty in the measurement.\nZero-crossing detectors.\nFor this type of detector, a comparator detects each time an AC pulse changes polarity. The output of the comparator changes state each time the pulse changes its polarity, that is the output is HI (high) for a positive pulse and LO (low) for a negative pulse squares the input signal.\nRelaxation oscillator.\nA comparator can be used to build a relaxation oscillator. It uses both positive and negative feedback. The positive feedback is a Schmitt trigger configuration. Alone, the trigger is a bistable multivibrator. However, the slow negative feedback added to the trigger by the RC circuit causes the circuit to oscillate automatically. That is, the addition of the RC circuit turns the hysteretic bistable multivibrator into an astable multivibrator.\nLevel shifter.\nThis circuit requires only a single comparator with an open-drain output as in the LM393, TLV3011, or MAX9028. The circuit provides great flexibility in choosing the voltages to be translated by using a suitable pull up voltage. It also allows the translation of bipolar \u00b15\u00a0V logic to unipolar 3\u00a0V logic by using a comparator like the MAX972.\nAnalog-to-digital converters.\nWhen a comparator performs the function of telling if an input voltage is above or below a given threshold, it is essentially performing a 1-bit quantization. This function is used in nearly all analog to digital converters (such as flash, pipeline, successive approximation, delta-sigma modulation, folding, interpolating, dual-slope and others) in combination with other devices to achieve a multi-bit quantization.\nWindow detectors.\nComparators can also be used as window detectors. In a window detector, a comparator is used to compare two voltages and determine whether a given input voltage is under voltage or over voltage.\nAbsolute-value detectors.\nComparators can be used to create absolute-value detectors. In an absolute-value detector, two comparators and a digital logic gate are used to compare the absolute values of two voltages.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt; "}
{"id": "40930", "revid": "39349169", "url": "https://en.wikipedia.org/wiki?curid=40930", "title": "Compatibility", "text": "Compatibility may refer to:\n&lt;templatestyles src=\"Template:TOC_right/styles.css\" /&gt;\nOther uses.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "40931", "revid": "63286", "url": "https://en.wikipedia.org/wiki?curid=40931", "title": "Compatible sideband transmission", "text": "Type of sideband modulation\nA Compatible sideband transmission, also known as amplitude modulation equivalent (AME) or Single sideband reduced-carrier (SSB-RC), is a type of single sideband RF modulation in which the carrier is deliberately reinserted at a lower level after its normal suppression to permit reception by conventional AM receivers. The general convention is to filter the lower-sideband, and communicate using only the upper-sideband and a partial carrier. \nThe benefits of compatible-sideband over conventional AM are increased spectral efficiency due to a reduction in bandwidth of 50% as well as a decrease in wasted power. \nBy using compatible sideband instead of AM, less RF power is required at the transmitter to transmit the same quality of signal the same distance. This results in compatible sideband being almost 100% power-efficient, where regular AM is comparably only 16% power-efficient (84% of RF power wasted). \nThis modulation was first used by the first West-German longwave transmitter between 1953 and 1962, which worked on 151\u00a0kHz and is currently mostly used in high frequency military communications.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "40932", "revid": "20836525", "url": "https://en.wikipedia.org/wiki?curid=40932", "title": "Compile", "text": ""}
{"id": "40933", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=40933", "title": "Complementary network service", "text": "In telecommunications, a complementary network service (CNS) is a means for an enhanced-service provider customer to connect to a network and to the enhanced service provider. \nComplementary network services usually consist of the customer local service, such as a business or residence, and several associated service features, such as a call-forwarding service.\nReferences.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "40934", "revid": "6908984", "url": "https://en.wikipedia.org/wiki?curid=40934", "title": "Component", "text": "Component may refer to:\n&lt;templatestyles src=\"Template:TOC_right/styles.css\" /&gt;\nIn engineering, science, and technology.\nChinese.\nChinese character component, a structural unit between Chinese character strokes and Chinese whole characters.\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "40935", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=40935", "title": "Composite cable", "text": ""}
{"id": "40936", "revid": "41294494", "url": "https://en.wikipedia.org/wiki?curid=40936", "title": "Compromise", "text": "Negotiation strategy\nTo compromise is to make a deal between different parties where each party gives up part of their demand. In arguments, compromise means finding agreement through communication, through a mutual acceptance of terms\u2014often involving variations from an original goal or desires. Defining and finding the best possible compromise is an important problem in fields like game theory and the voting system.\nResearch indicates that suboptimal compromises are often the result of negotiators failing to realize when they have interests that are completely compatible with those of the other party, leading them to settle for suboptimal agreements. Mutually better outcomes can often be found by careful investigation of both parties' interests, especially if done early in negotiations.\nThe compromise solution of a multicriteria decision making or multi-criteria decision analysis problem that is the closest to the ideal could be determined by the VIKOR method, which provides a maximum utility of the majority, and a minimum individual regret of the opponent.\nPolitics.\nIn international politics, compromises often discussed include infamous deals with dictators, such as Neville Chamberlain's appeasement of Adolf Hitler. Margalit calls these \"rotten compromises.\" In the United States and other democratic countries , many politicians of recent times permanently campaign to gain reelection. Thus, United States Ambassador to Germany Amy Gutmann and political scientist Dennis F. Thompson have observed that compromise is more difficult. The problem of political compromise in general is an important subject in political ethics.\nPoliticians being willing to compromise can reduce partisanship and hostility. Politics is sometimes called the \"art of compromise\". Polling by the American Survey Center indicates that Americans take a favorable view of political compromise.\nHuman relationships.\nIn human relationships, \"compromise\" can make no party happy because the parties involved feel that they either gave away too much or that they received too little. Compromise may be referred to as capitulation, a \"surrender\" of objectives, principles, or material. Extremism is often considered as an antonym to compromise, which, depending on context, may be associated with concepts of balance and tolerance.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "40937", "revid": "22883165", "url": "https://en.wikipedia.org/wiki?curid=40937", "title": "Computer conferencing", "text": "Computer conferencing may refer to:\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "40938", "revid": "51448", "url": "https://en.wikipedia.org/wiki?curid=40938", "title": "Computer network operating system", "text": ""}
{"id": "40939", "revid": "57939", "url": "https://en.wikipedia.org/wiki?curid=40939", "title": "COMSEC equipment", "text": ""}
{"id": "40940", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=40940", "title": "Concentrator", "text": "In telecommunications, the term concentrator has the following meanings:\nTheory.\nIn the evolution of modern telecommunications systems there was a requirement to connect large numbers of low-speed access devices with large telephone company 'central office' switches over common paths. During the first generations of digital networks, analog signals were digitized on line cards attached to the telephone exchange switches. In an effort to reduce local loop costs, it was decided to push this conversion closer to the customer premises by deploying small conversion devices in customer neighborhoods. These devices would combine multiple digital signals on a single link to a larger telephone switch, which would provide service to the customer. These devices were initially called remote concentrators or simply remotes.\nIn fibre-optic distribution systems which offer triple-play services (voice, television, internet) the digitization has arrived at the customer premises and signals are digitized at the source and combined using customer edge routers. This traffic enters the distribution network at an Optical Network Termination and is carried to the central office using Wavelength division multiplexing and Passive optical networking.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "40941", "revid": "1951353", "url": "https://en.wikipedia.org/wiki?curid=40941", "title": "Concentricity error", "text": "Measure characterizing an optical fiber\nThe concentricity error of an optical fiber is the distance between the center of the two concentric circles that specify the cladding diameter and the center of the two concentric circles that specify the core diameter.\nThe concentricity error is used in conjunction with tolerance fields to specify or characterize optical fiber core and cladding geometry.\nReferences.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "40942", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=40942", "title": "Conditioning equipment", "text": "In telecommunications, the term conditioning equipment has the following meanings: \nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "40943", "revid": "45555970", "url": "https://en.wikipedia.org/wiki?curid=40943", "title": "Conducted interference", "text": ""}
{"id": "40944", "revid": "11391880", "url": "https://en.wikipedia.org/wiki?curid=40944", "title": "Conduction band", "text": ""}
{"id": "40945", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=40945", "title": "Conductive coupling", "text": ""}
{"id": "40946", "revid": "47070250", "url": "https://en.wikipedia.org/wiki?curid=40946", "title": "Conference operation", "text": ""}
{"id": "40947", "revid": "64485", "url": "https://en.wikipedia.org/wiki?curid=40947", "title": "Configuration control", "text": ""}
{"id": "40948", "revid": "7903804", "url": "https://en.wikipedia.org/wiki?curid=40948", "title": "Configuration management", "text": "Process for maintaining consistency of a product attributes with its design\nConfiguration management (CM) is a management process for establishing and maintaining consistency of a product's performance, functional, and physical attributes with its requirements, design, and operational information throughout its life. The CM process is widely used by military engineering organizations to manage changes throughout the system lifecycle of complex systems, such as weapon systems, military vehicles, and information systems. Outside the military, the CM process is also used with IT service management as defined by ITIL, and with other domain models in the civil engineering and other industrial engineering segments such as roads, bridges, canals, dams, and buildings.\nIntroduction.\nCM applied over the life cycle of a system provides visibility and control of its performance, functional, and physical attributes. CM verifies that a system performs as intended, and is identified and documented in sufficient detail to support its projected life cycle. The CM process facilitates orderly management of system information and system changes for such beneficial purposes as to revise capability; improve performance, reliability, or maintainability; extend life; reduce cost; reduce risk and liability; or correct defects. The relatively minimal cost of implementing CM is returned manyfold in cost avoidance. The lack of CM, or its ineffectual implementation, can be very expensive and sometimes can have such catastrophic consequences such as failure of equipment or loss of life.\nCM emphasizes the functional relation between parts, subsystems, and systems for effectively controlling system change. It helps to verify that proposed changes are systematically considered to minimize adverse effects. Changes to the system are proposed, evaluated, and implemented using a standardized, systematic approach that ensures consistency, and proposed changes are evaluated in terms of their anticipated impact on the entire system. CM verifies that changes are carried out as prescribed and that documentation of items and systems reflects their true configuration. A complete CM program includes provisions for the storing, tracking, and updating of all system information on a component, subsystem, and system basis.\nA structured CM program ensures that documentation (e.g., requirements, design, test, and acceptance documentation) for items is accurate and consistent with the actual physical design of the item. In many cases, without CM, the documentation exists but is not consistent with the item itself. For this reason, engineers, contractors, and management are frequently forced to develop documentation reflecting the actual status of the item before they can proceed with a change. This reverse engineering process is wasteful in terms of human and other resources and can be minimized or eliminated using CM.\nHistory.\nConfiguration Management originated in the United States Department of Defense in the 1950s as a technical management discipline for hardware material items\u2014and it is now a standard practice in virtually every industry. The CM process became its own technical discipline sometime in the late 1960s when the DoD developed a series of military standards called the \"480 series\" (i.e., MIL-STD-480, MIL-STD-481 and MIL-STD-483) that were subsequently issued in the 1970s. In 1991, the \"480 series\" was consolidated into a single standard known as the MIL\u2013STD\u2013973 that was then replaced by MIL\u2013HDBK\u201361 pursuant to a general DoD goal that reduced the number of military standards in favor of industry technical standards supported by standards developing organizations (SDO). This marked the beginning of what has now evolved into the most widely distributed and accepted standard on CM, ANSI\u2013EIA\u2013649\u20131998. Now widely adopted by numerous organizations and agencies, the CM discipline's concepts include systems engineering (SE), Integrated Logistics Support (ILS), Capability Maturity Model Integration (CMMI), ISO 9000, Prince2 project management method, COBIT, ITIL, product lifecycle management, and Application Lifecycle Management. Many of these functions and models have redefined CM from its traditional holistic approach to technical management. Some treat CM as being similar to a librarian activity, and break out change control or change management as a separate or stand alone discipline.\nOverview.\nCM is the practice of handling changes systematically so that a system maintains its integrity over time. CM implements the policies, procedures, techniques, and tools that manage, evaluate proposed changes, track the status of changes, and maintain an inventory of system and support documents as the system changes. CM programs and plans provide technical and administrative direction to the development and implementation of the procedures, functions, services, tools, processes, and resources required to successfully develop and support a complex system. During system development, CM allows program management to track requirements throughout the life-cycle through acceptance and operations and maintenance. As changes inevitably occur in the requirements and design, they must be approved and documented, creating an accurate record of the system status. Ideally the CM process is applied throughout the system lifecycle. Most professionals mix up or get confused with Asset management (AM, see also ISO/IEC 19770), where it inventories the assets on hand. The key difference between CM and AM is that the former does not manage the financial accounting aspect but on service that the system supports or in other words, that the later (AM) is trying to realize value from an IT asset.\nThe CM process for both hardware- and software-configuration items comprises five distinct disciplines as established in the MIL\u2013HDBK\u201361A and in ANSI/EIA-649. Members of an organization interested in applying a standard change-management process will employ these disciplines as policies and procedures for establishing baselines, manage and control change, and monitor and assess the effectiveness and correctness of progress. The IEEE 12207 process IEEE 12207.2 also has these activities and adds \"Release management and delivery\".\nThe five disciplines are:\nSoftware.\nThe software configuration management (SCM) process is looked upon by practitioners as the best solution to handling changes in software projects. It identifies the functional and physical attributes of software at various points in time, and performs systematic control of changes to the identified attributes for the purpose of maintaining software integrity and traceability throughout the software development life cycle.\nThe SCM process further defines the need to trace changes, and the ability to verify that the final delivered software has all of the planned enhancements that are supposed to be included in the release. It identifies four procedures that must be defined for each software project to ensure that a sound SCM process is implemented. They are:\nThese terms and definitions change from standard to standard, but are essentially the same.\nConfiguration management database.\nITIL specifies the use of a configuration management system (CMS) or configuration management database (CMDB) as a means of achieving industry best practices for Configuration Management. CMDBs are used to track Configuration Items (CIs) and the dependencies between them, where CIs represent the things in an enterprise that are worth tracking and managing, such as but not limited to computers, software, software licenses, racks, network devices, storage, and even the components within such items. CMS helps manage a federated collection of CMDBs.\nThe benefits of a CMS/CMDB includes being able to perform functions like root cause analysis, impact analysis, change management, and current state assessment for future state strategy development. \n\"Configuration Management\" (CM) is an ITIL-specific ITSM process that tracks all of the individual CIs in an IT system which may be as simple as a single server, or as complex as the entire IT department. In large organizations a configuration manager may be appointed to oversee and manage the CM process. In ITIL version 3, this process has been renamed as \"Service Asset and Configuration Management\".\nInformation assurance.\nFor information assurance, CM can be defined as the management of security features and assurances through control of changes made to hardware, software, firmware, documentation, test, test fixtures, and test documentation throughout the life cycle of an information system. CM for information assurance, sometimes referred to as \"secure configuration management\" (SCM), relies upon performance, functional, and physical attributes of IT platforms and products and their environments to determine the appropriate security features and assurances that are used to measure a system configuration state. For example, configuration requirements may be different for a network firewall that functions as part of an organization's Internet boundary versus one that functions as an internal local network firewall.\nMaintenance systems.\nConfiguration management is used to maintain an understanding of the status of complex assets with a view to maintaining the highest level of serviceability for the lowest cost. Specifically, it aims to ensure that operations are not disrupted due to the asset (or parts of the asset) overrunning limits of planned lifespan or below quality levels.\nIn the military, this type of activity is often classed as \"mission readiness\", and seeks to define which assets are available and for which type of mission; a classic example is whether aircraft on board an aircraft carrier are equipped with bombs for ground support or missiles for defense.\nOperating system configuration management.\nConfiguration management can be used to maintain OS configuration files. Many of these systems utilize Infrastructure as Code to define and maintain configuration.\nThe Promise theory of configuration maintenance was developed by Mark Burgess, with a practical implementation on present day computer systems in the software CFEngine able to perform real time repair as well as preventive maintenance.\nPreventive maintenance.\nUnderstanding the \"as is\" state of an asset and its major components is an essential element in preventive maintenance as used in maintenance, repair, and overhaul and enterprise asset management systems.\nComplex assets such as aircraft, ships, industrial machinery etc. depend on many different components being serviceable. This serviceability is often defined in terms of the amount of usage the component has had since it was new, since fitted, since repaired, the amount of use it has had over its life and several other limiting factors. Understanding how near the end of their life each of these components is has been a major undertaking involving labor-intensive record keeping until recent developments in software.\nPredictive maintenance.\nMany types of component use electronic sensors to capture data which provides live condition monitoring. This data is analyzed on board or at a remote location by computer to evaluate its current serviceability and increasingly its likely future state using algorithms which predict potential future failures based on previous examples of failure through field experience and modeling. This is the basis for \"predictive maintenance\".\nAvailability of accurate and timely data is essential in order for CM to provide operational value and a lack of this can often be a limiting factor. Capturing and disseminating the operating data to the various support organizations is becoming an industry in itself.\nThe consumers of this data have grown more numerous and complex with the growth of programs offered by original equipment manufacturers (OEMs). These are designed to offer operators guaranteed availability and make the picture more complex with the operator managing the asset but the OEM taking on the liability to ensure its serviceability.\nStandards.\nA number of standards support or include configuration management, including:\nConstruction.\nMore recently configuration management has been applied to large construction projects which can often be very complex and have a huge number of details and changes that need to be documented. Construction agencies such as the Federal Highway Administration have used configuration management for their infrastructure projects. There are construction-based configuration management tools that aim to document change orders and RFIs in order to ensure a project stays on schedule and on budget. These programs can also store information to aid in the maintenance and modification of the infrastructure when it is completed. One such application, CCSNet, was tested in a case study funded by the Federal Transportation Administration (FTA) in which the efficacy of configuration management was measured through comparing the approximately 80% complete construction of the Los Angeles County Metropolitan Transit Agency (LACMTA) first and second segments of the Red Line, a $5.3\u00a0billion rail construction project. This study yielded results indicating a benefit to using configuration management on projects of this nature.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "40949", "revid": "1297505965", "url": "https://en.wikipedia.org/wiki?curid=40949", "title": "Congestion", "text": "Congestion may refer to:\nOther uses.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "40950", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=40950", "title": "Connectionless communication", "text": "Network protocol with routing per packet\nConnectionless communication, often referred to as CL-mode communication, is a data transmission method used in packet switching networks, using data packets that are frequently called datagrams, in which each data packet is individually addressed and routed based on information carried in each packet, rather than in the setup information of a prearranged, fixed data channel as in connection-oriented communication. Connectionless protocols are usually described as stateless protocols, the Internet Protocol (IP) and User Datagram Protocol (UDP) are examples.\nAttributes.\nUnder connectionless communication between two network endpoints, a message can be sent from one endpoint to another without prior arrangement. The device at one end of the communication transmits data addressed to the other, without first ensuring that the recipient is available and ready to receive the data. Some protocols allow for error correction by requesting retransmission.\nConnectionless protocols are stateless protocols because the endpoints have no protocol-defined way to remember where they are in a \"conversation\" of message exchanges. It has lower overhead than connection-oriented communication because, in connection-oriented communication, the communicating peers must first establish a logical or physical data channel or \"connection\" in a dialog preceding the exchange of user data. It allows for multicast and broadcast operations in which the same data are transmitted to several recipients in a single transmission.\nIn connectionless transmissions the service provider usually cannot guarantee that there will be no loss, error insertion, misdelivery, duplication, or out-of-sequence delivery of the packet. However, the effect of errors may be reduced by implementing error correction within an application protocol.\nIn connectionless mode, there is less opportunity for optimization when sending several data units between the same two peers. By establishing a connection at the beginning of such a data exchange the components (routers, bridges) along the network path would be able to pre-compute (and hence cache) routing-related information, avoiding re-computation for every packet. In connection-oriented communication, network components can also reserve capacity for the transfer of the subsequent data units of a video download, for example.\nArchitecture and implementations.\nDistinction between connectionless and connection-oriented transmission may take place at several layers of the OSI Reference Model:\nNotable connectionless protocols are: Internet Protocol (IP), User Datagram Protocol (UDP), Internet Control Message Protocol (ICMP), Internetwork Packet Exchange (IPX), Transparent Inter-process Communication, NetBIOS, and Fast and Secure Protocol (FASP).\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "40951", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=40951", "title": "Connections per circuit hour", "text": "In telecommunications, the term connections per circuit hour (CCH) has the following meanings: \nThe magnitude of the CCH is an instantaneous value subject to change as a function of time (i.e. from moment to moment), and is subject to study including load curve and busy hour as other measures of traffic are."}
{"id": "40952", "revid": "42522270", "url": "https://en.wikipedia.org/wiki?curid=40952", "title": "Connectivity exchange", "text": "Radio network process\nConnectivity exchange (CONEX): In an adaptive or manually operated high-frequency (HF) radio network, the automatic or manual exchange of information concerning routes to stations that are not directly reachable by the exchange originator. \nThe purpose of the exchange is to identify indirect paths and/or possible relay stations to those stations that are not directly reachable.\nReferences.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "40953", "revid": "1754504", "url": "https://en.wikipedia.org/wiki?curid=40953", "title": "Conservation of radiance", "text": ""}
{"id": "40954", "revid": "27335766", "url": "https://en.wikipedia.org/wiki?curid=40954", "title": "Contention", "text": "Contention or contentious may refer to:\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "40955", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=40955", "title": "Continuous operation", "text": "Telecommunications mode in which components are operational at all times\nIn telecommunications, continuous operation is an operation in which certain components, such as nodes, facilities, circuits, or equipment, are in an operational state at all times. Continuous operation usually requires that there be fully redundant configuration, or at least a sufficient \"X\" out of \"Y\" degree of redundancy for compatible equipment, where \"X\" is the number of spare components and \"Y\" is the number of operational components. \n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "40956", "revid": "19398868", "url": "https://en.wikipedia.org/wiki?curid=40956", "title": "Contrast", "text": "Contrast may refer to:\n&lt;templatestyles src=\"Template:TOC_right/styles.css\" /&gt;\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "40957", "revid": "48552905", "url": "https://en.wikipedia.org/wiki?curid=40957", "title": "Control communications", "text": "Technological branch for communication facilities dedicated to control purposes\nIn telecommunications, control communications is the branch of technology devoted to the design, development, and application of communications facilities used specifically for control purposes, such as for controlling (a) industrial processes, (b) movement of resources, (c) electric power generation, distribution, and utilization, (d) communications networks, and (e) transportation systems.\nReferences.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "40958", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=40958", "title": "Controlled area", "text": "Secure telecommunications area\nIn telecommunications, a controlled area is an area in which uncontrolled movement will not result in compromise of classified information, that is designed to provide administrative control and safety, or that serves as a buffer for controlling access to limited-access areas. It can also refer to an area to which security controls have been applied to protect an information-processing system's equipment and wirelines, equivalent to that required for the information transmitted through the system.\nReferences.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "40959", "revid": "87591", "url": "https://en.wikipedia.org/wiki?curid=40959", "title": "Controlled security operation", "text": ""}
{"id": "40960", "revid": "23958", "url": "https://en.wikipedia.org/wiki?curid=40960", "title": "Control of electromagnetic radiation", "text": ""}
{"id": "40961", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=40961", "title": "Control operation", "text": "In telecommunications, a control operation (control function) is an operation that affects the recording, processing, transmission, or interpretation of data. \nExamples of control operations include a font change, or a rewind; and transmitting an end-of-transmission (EOT) control character.\nReferences.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "40962", "revid": "47988270", "url": "https://en.wikipedia.org/wiki?curid=40962", "title": "Convolutional code", "text": "Type of error-correcting code using convolution\nIn telecommunication, a convolutional code is a type of error-correcting code that generates parity symbols via the sliding application of a boolean polynomial function to a data stream. The sliding application represents the 'convolution' of the encoder over the data, which gives rise to the term 'convolutional coding'. The sliding nature of the convolutional codes facilitates trellis decoding using a time-invariant trellis. Time invariant trellis decoding allows convolutional codes to be maximum-likelihood soft-decision decoded with reasonable complexity.\nThe ability to perform economical maximum likelihood soft decision decoding is one of the major benefits of convolutional codes. This is in contrast to classic block codes, which are generally represented by a time-variant trellis and therefore are typically hard-decision decoded. Convolutional codes are often characterized by the base code rate and the depth (or memory) of the encoder formula_1. The base code rate is typically given as formula_2, where n is the raw input data rate and k is the data rate of output channel encoded stream. n is less than k because channel coding inserts redundancy in the input bits. The memory is often called the \"constraint length\" K, where the output is a function of the current input as well as the previous formula_3 inputs. The depth may also be given as the number of memory elements v in the polynomial or the maximum possible number of states of the encoder (typically: formula_4).\nConvolutional codes are often described as continuous. However, it may also be said that convolutional codes have arbitrary block length, rather than being continuous, since most real-world convolutional encoding is performed on blocks of data. Convolutionally encoded block codes typically employ termination. The arbitrary block length of convolutional codes can also be contrasted to classic block codes, which generally have fixed block lengths that are determined by algebraic properties.\nThe code rate of a convolutional code is commonly modified via symbol puncturing. For example, a convolutional code with a 'mother' code rate formula_5 may be punctured to a higher rate of, for example, formula_6 simply by not transmitting a portion of code symbols. The performance of a punctured convolutional code generally scales well with the amount of parity transmitted. The ability to perform economical soft decision decoding on convolutional codes, as well as the block length and code rate flexibility of convolutional codes, makes them very popular for digital communications.\nHistory.\nConvolutional codes were introduced in 1955 by Peter Elias. It was thought that convolutional codes could be decoded with arbitrary quality at the expense of computation and delay. In 1967, Andrew Viterbi determined that convolutional codes could be maximum-likelihood decoded with reasonable complexity using time invariant trellis based decoders \u2014 the Viterbi algorithm. Other trellis-based decoder algorithms were later developed, including the BCJR decoding algorithm.\nRecursive systematic convolutional codes were invented by Claude Berrou around 1991. These codes proved especially useful for iterative processing including the processing of concatenated codes such as turbo codes.\nUsing the \"convolutional\" terminology, a classic convolutional code might be considered a Finite impulse response (FIR) filter, while a recursive convolutional code might be considered an Infinite impulse response (IIR) filter.\nWhere convolutional codes are used.\nConvolutional codes are used extensively to achieve reliable data transfer in numerous applications, such as digital video, radio, mobile communications (e.g., in GSM, GPRS, EDGE and 3G networks (until 3GPP Release 7)) and satellite communications. These codes are often implemented in concatenation with a hard-decision code, particularly Reed\u2013Solomon. Prior to turbo codes such constructions were the most efficient, coming closest to the Shannon limit.\nConvolutional encoding.\nTo convolutionally encode data, start with \"k\" memory registers, each holding one input bit. Unless otherwise specified, all memory registers start with a value of 0. The encoder has \"n\" modulo-2 adders (a modulo 2 adder can be implemented with a single Boolean XOR gate, where the logic is: 0+0\u00a0=\u00a00, 0+1\u00a0=\u00a01, 1+0\u00a0=\u00a01, 1+1\u00a0=\u00a00), and \"n\" generator polynomials \u2014 one for each adder (see figure below). An input bit \"m\"1 is fed into the leftmost register. Using the generator polynomials and the existing values in the remaining registers, the encoder outputs \"n\" symbols. These symbols may be transmitted or punctured depending on the desired code rate. Now bit shift all register values to the right (\"m\"1 moves to \"m\"0, \"m\"0 moves to \"m\"\u22121) and wait for the next input bit. If there are no remaining input bits, the encoder continues shifting until all registers have returned to the zero state (flush bit termination).\nThe figure below is a rate &lt;templatestyles src=\"Fraction/styles.css\" /&gt;1\u20443 (&lt;templatestyles src=\"Fraction/styles.css\" /&gt;\"m\"\u2044\"n\") encoder with constraint length (\"k\") of 3. Generator polynomials are \"G\"1 = (1,1,1), \"G\"2 = (0,1,1), and \"G\"3 = (1,0,1). Therefore, output bits are calculated (modulo 2) as follows:\n\"n\"1 = \"m\"1 + \"m\"0 + \"m\"\u22121\n\"n\"2 = \"m\"0 + \"m\"\u22121\n\"n\"3 = \"m\"1 + \"m\"\u22121.\nConvolutional codes can be systematic and non-systematic:\nNon-systematic convolutional codes are more popular due to better noise immunity. It relates to the free distance of the convolutional code.\nRecursive and non-recursive codes.\nThe encoder on the picture above is a \"non-recursive\" encoder. Here's an example of a recursive one and as such it admits a feedback structure:\nThe example encoder is \"systematic\" because the input data is also used in the output symbols (Output 2). Codes with output symbols that do not include the input data are called \"non-systematic.\"\nRecursive codes are typically systematic and, conversely, non-recursive codes are typically non-systematic. It isn't a strict requirement, but a common practice.\nThe example encoder in Img. 2. is an 8-state encoder because the 3 registers will create 8 possible encoder states (23). A corresponding decoder trellis will typically use 8 states as well.\nRecursive systematic convolutional (RSC) codes have become more popular due to their use in Turbo Codes. Recursive systematic codes are also referred to as pseudo-systematic codes.\nOther RSC codes and example applications include:\nUseful for LDPC code implementation and as inner constituent code for serial concatenated convolutional codes (SCCC's).\nUseful for SCCC's and multidimensional turbo codes.\nUseful as constituent code in low error rate turbo codes for applications such as satellite links. Also suitable as SCCC outer code.\nImpulse response, transfer function, and constraint length.\nA convolutional encoder is called so because it performs a \"convolution\" of the input stream with the encoder's \"impulse responses\":\nformula_7\nwhere x is an input sequence, yj is a sequence from output j, hj is an impulse response for output j and formula_8 denotes convolution.\nA convolutional encoder is a discrete linear time-invariant system. Every output of an encoder can be described by its own transfer function, which is closely related to the generator polynomial. An impulse response is connected with a transfer function through Z-transform.\nTransfer functions for the first (non-recursive) encoder are:\nTransfer functions for the second (recursive) encoder are:\nDefine m by\n formula_14\nwhere, for any rational function formula_15,\n formula_16.\nThen m is the maximum of the polynomial degrees of the\nformula_17, and the \"constraint length\" is defined as formula_18. For instance, in the first example the constraint length is 3, and in the second the constraint length is 4.\nTrellis diagram.\nA convolutional encoder is a finite state machine. An encoder with \"n\" binary cells will have 2\"n\" states.\nImagine that the encoder (shown on Img.1, above) has '1' in the left memory cell (\"m\"0), and '0' in the right one (\"m\"\u22121). (\"m\"1 is not really a memory cell because it represents a current value). We will designate such a state as \"10\". According to an input bit the encoder at the next turn can convert either to the \"01\" state or the \"11\" state. One can see that not all transitions are possible for (e.g., a decoder can't convert from \"10\" state to \"00\" or even stay in \"10\" state).\nAll possible transitions can be shown as below:\nAn actual encoded sequence can be represented as a path on this graph. One valid path is shown in red as an example.\nThis diagram gives us an idea about \"decoding\": if a received sequence doesn't fit this graph, then it was received with errors, and we must choose the nearest \"correct\" (fitting the graph) sequence. The real decoding algorithms exploit this idea.\nFree distance and error distribution.\nThe free distance (\"d\") is the minimal Hamming distance between different encoded sequences. The \"correcting capability\" (\"t\") of a convolutional code is the number of errors that can be corrected by the code. It can be calculated as\nformula_19\nSince a convolutional code doesn't use blocks, processing instead a continuous bitstream, the value of \"t\" applies to a quantity of errors located relatively near to each other. That is, multiple groups of \"t\" errors can usually be fixed when they are relatively far apart.\nFree distance can be interpreted as the minimal length of an erroneous \"burst\" at the output of a convolutional decoder. The fact that errors appear as \"bursts\" should be accounted for when designing a concatenated code with an inner convolutional code. The popular solution for this problem is to interleave data before convolutional encoding, so that the outer block (usually Reed\u2013Solomon) code can correct most of the errors.\nDecoding convolutional codes.\nSeveral algorithms exist for decoding convolutional codes. For relatively small values of \"k\", the Viterbi algorithm is universally used as it provides maximum likelihood performance and is highly parallelizable. Viterbi decoders are thus easy to implement in VLSI hardware and in software on CPUs with SIMD instruction sets.\nLonger constraint length codes are more practically decoded with any of several sequential decoding algorithms, of which the Fano algorithm is the best known. Unlike Viterbi decoding, sequential decoding is not maximum likelihood but its complexity increases only slightly with constraint length, allowing the use of strong, long-constraint-length codes. Such codes were used in the Pioneer program of the early 1970s to Jupiter and Saturn, but gave way to shorter, Viterbi-decoded codes, usually concatenated with large Reed\u2013Solomon error correction codes that steepen the overall bit-error-rate curve and produce extremely low residual undetected error rates.\nBoth Viterbi and sequential decoding algorithms return hard decisions: the bits that form the most likely codeword. An approximate confidence measure can be added to each bit by use of the Soft output Viterbi algorithm. Maximum a posteriori (MAP) soft decisions for each bit can be obtained by use of the BCJR algorithm.\nPopular convolutional codes.\nIn fact, predefined convolutional codes structures obtained during scientific researches are used in the industry. This relates to the possibility to select catastrophic convolutional codes (causes larger number of errors).\nAn especially popular Viterbi-decoded convolutional code, used at least since the Voyager program, has a constraint length K of 7 and a rate \"r\" of 1/2.\nMars Pathfinder, Mars Exploration Rover and the Cassini probe to Saturn use a K of 15 and a rate of 1/6; this code performs about 2\u00a0dB better than the simpler formula_20 code at a cost of 256\u00d7 in decoding complexity (compared to Voyager mission codes).\nThe convolutional code with a constraint length of 2 and a rate of 1/2 is used in GSM as an error correction technique.\nPunctured convolutional codes.\nConvolutional code with any code rate can be designed based on polynomial selection; however, in practice, a puncturing procedure is often used to achieve the required code rate. Puncturing is a technique used to make a \"m\"/\"n\" rate code from a \"basic\" low-rate (e.g., 1/\"n\") code. It is achieved by deleting of some bits in the encoder output. Bits are deleted according to a \"puncturing matrix\". The following puncturing matrices are the most frequently used:\nFor example, if we want to make a code with rate 2/3 using the appropriate matrix from the above table, we should take a basic encoder output and transmit every first bit from the first branch and every bit from the second one. The specific order of transmission is defined by the respective communication standard.\nPunctured convolutional codes are widely used in the satellite communications, for example, in Intelsat systems and Digital Video Broadcasting.\nPunctured convolutional codes are also called \"perforated\".\nTurbo codes: replacing convolutional codes.\nSimple Viterbi-decoded convolutional codes are now giving way to turbo codes, a new class of iterated short convolutional codes that closely approach the theoretical limits imposed by Shannon's theorem with much less decoding complexity than the Viterbi algorithm on the long convolutional codes that would be required for the same performance. Concatenation with an outer algebraic code (e.g., Reed\u2013Solomon) addresses the issue of error floors inherent to turbo code designs.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nFurther reading.\nPublications.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "40965", "revid": "248739", "url": "https://en.wikipedia.org/wiki?curid=40965", "title": "Copy", "text": "Copy may refer to:\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "40966", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=40966", "title": "Cord circuit", "text": "Switchboard circuit\nIn telecommunications, a cord circuit is a switchboard circuit in which a plug-terminated cord is used to establish connections manually between user lines or between trunks and user lines. A number of cord circuits are furnished as part of the switchboard position equipment. The cords may be referred to as front cord and rear cord or trunk cord and station cord. In modern cordless switchboards, the cord-circuit function is switch operated and may be programmable.\nIn early and middle 20th century telephone exchanges this task was done by a supervisory relay set known variously as junctor circuit or district junctor. Later designs made it a function of the trunk circuit or absorbed it into software."}
{"id": "40967", "revid": "1303068289", "url": "https://en.wikipedia.org/wiki?curid=40967", "title": "Core", "text": "Core or cores may refer to:\n&lt;templatestyles src=\"Template:TOC_right/styles.css\" /&gt;\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "40968", "revid": "26394783", "url": "https://en.wikipedia.org/wiki?curid=40968", "title": "Corner reflector", "text": "Retroreflector with three orthogonal, intersecting flat surfaces\nA corner reflector is a retroreflector consisting of three mutually perpendicular, intersecting flat reflective surfaces. It reverses the direction of an incoming wave with being translated by reflections on the three orthogonal sides. The three intersecting surfaces often are triangles (forming a tetrahedron) or may have square shapes. Radar corner reflectors made of metal are used to reflect radio waves from radar sets. Optical corner reflectors, called corner cubes or cube corners, made of three-sided glass prisms, are used in surveying and laser ranging.\nPrinciple.\nThe incoming ray is reflected three times, once by each surface, which results in a reversal of direction. To see this, the three corresponding normal vectors of the corner's perpendicular sides can be considered to form a basis (a rectangular coordinate system) (\"x\", \"y\", \"z\") in which to represent the direction of an arbitrary incoming ray, [\"a\",\u2009\"b\",\u2009\"c\"]. When the ray reflects from the first side, say \"x\", the ray's \"x\" component, \"a\", is reversed to \u2212\"a\" without changing the \"y\" and \"z\" components, resulting in a direction of [\u2212\"a\",\u2009\"b\",\u2009\"c\"]. Similarly, when reflected from side \"y\" and finally from side \"z\", the \"b\" and \"c\" components are reversed. Therefore, the ray direction goes from [\"a\",\u2009\"b\",\u2009\"c\"] to [\u2212\"a\",\u2009\"b\",\u2009\"c\"] to [\u2212\"a\",\u2009\u2212\"b\",\u2009\"c\"] to [\u2212\"a\",\u2009\u2212\"b\",\u2009\u2212\"c\"], and it leaves the corner reflector with all three components of direction exactly reversed. The distance travelled, relative to a plane normal to the direction of the rays, is also equal for any ray entering the reflector, regardless of the location where it first reflects.\nIn radar.\nRadar corner reflectors are designed to reflect the microwave radio waves emitted by radar sets back toward the radar antenna. This causes them to show a strong \"return\" on radar screens. A simple corner reflector consists of three conducting sheet metal or screen surfaces at 90\u00b0 angles to each other, attached to one another at the edges, forming a \"corner\". These reflect radio waves coming from in front of them back parallel to the incoming beam. To create a corner reflector that will reflect radar waves coming from any direction, 8 corner reflectors are placed back-to-back in an octahedron (diamond) shape. The reflecting surfaces must be larger than several wavelengths of the radio waves to function.\nIn maritime navigation they are placed on bridge abutments, buoys, ships and, especially, lifeboats, to ensure that these show up strongly on ship radar screens. Corner reflectors are placed on the vessel's masts at a height of at least above sea level (giving them an approximate minimum horizon distance of ). Marine radar uses X-band microwaves with wavelengths of , so small reflectors less than across are used. In aircraft navigation, corner reflectors are installed on rural runways, to make them show up on aircraft radar.\nAn object that has multiple reflections from smooth surfaces produces a radar return of greater magnitude than might be expected from the physical size of the object. This effect was put to use on the ADM-20 Quail, a small decoy missile which had the same radar cross section as a B-52.\nThe corner reflector is not the only efficient radar reflector design; other retroreflector designs have also seen use. Luneburg lens, for example, are used on the ADM-141 TALD.\nIn optics.\nIn optics, corner reflectors typically consist of three mirrors or reflective prism faces which return an incident light beam in the opposite direction. In surveying, retroreflector prisms are commonly used as targets for long-range electronic distance measurement using a total station.\nFive arrays of optical corner reflectors have been placed on the Moon for use by Lunar Laser Ranging experiments observing a laser's time-of-flight to measure the Moon's orbit more precisely than was possible before. The three largest were placed by NASA as part of the Apollo program, and the Soviet Union built two smaller ones into the Lunokhod rovers.\nAutomobile and bicycle tail lights are molded with arrays of small corner reflectors, with different sections oriented for viewing from different angles. Reflective paint for visibility at night usually contains retroreflective spherical beads.\nThin plastic with microscopic corner reflector structures can be used as tape, on signs, or sewn or molded onto clothing.\nOther examples.\nCorner reflectors can also occur accidentally. Tower blocks with balconies are often accidental acoustic corner reflectors and return a distinctive echo to an observer making a sharp sound noise, such as a hand clap, nearby.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "40969", "revid": "1279182523", "url": "https://en.wikipedia.org/wiki?curid=40969", "title": "Cosmic noise", "text": "Physical phenomenon from outside of the Earth's atmosphere\nCosmic noise, also known as galactic radio noise, is radio-frequency electromagnetic radiation from sources outside of the Earth's atmosphere. Its characteristics are comparable to those of thermal noise. Cosmic noise occurs at frequencies above about 15\u00a0MHz when highly directional antennas are pointed toward the Sun or other regions of the sky, such as the center of the Milky Way Galaxy. Celestial objects like quasars, which are super dense objects far from Earth, emit electromagnetic waves in their full spectrum, including radio waves. The fall of a meteorite can also be heard through a radio receiver; the falling object burns from friction with the Earth's atmosphere, ionizing surrounding gases and producing radio waves. Cosmic microwave background radiation (CMBR) from outer space is also a form of cosmic noise. CMBR is thought to be a relic of the Big Bang, and pervades the space almost homogeneously over the entire celestial sphere. The bandwidth of the CMBR is wide, though the peak is in the microwave range.\nHistory.\nKarl Jansky, an American physicist and radio engineer, first discovered radio waves from the Milky Way in August, 1931. At Bell Telephone Laboratories in 1932, Jansky built an antenna designed to receive radio waves at a frequency of 20.5\u00a0MHz, which is a wavelength of approximately 14.6\u00a0meters.\nAfter recording signals with this antenna for several months, Jansky categorized them into three types: nearby thunderstorms, distant thunderstorms, and a faint steady hiss of an unknown origin. He discovered the location of maximum intensity rose and fell once a day, which led him to believe he was detecting radiation from the Sun.\nA few months went by following this signal thought to be from the Sun, and Jansky found that the brightest point moved away from the Sun and concluded the cycle repeated every 23 hours and 56 minutes. After this discovery, Jansky concluded the radiation was coming from the Milky Way and was strongest in the direction of the center of the galaxy.\nJansky's work helped to distinguish between the radio sky and the optical sky. The optical sky is what is seen by the human eye, whereas the radio sky consists of daytime meteors, solar bursts, quasars, and gravitational waves.\nLater in 1963, American physicist and radio astronomer Arno Allan Penzias (born April 26, 1933) discovered cosmic microwave background radiation. Penzias's discovery of cosmic microwave background radiation helped establish the Big Bang theory of cosmology. Penzias and his partner, Robert Woodrow Wilson worked together on ultra-sensitive cryogenic microwave receivers, originally intended for radio astronomy observations. In 1964, upon creating their most sensitive antenna/receiver system, the Holmdel Horn Antenna, the two discovered a radio noise they could not explain. After further investigation, Penzias contacted Robert Dicke, who suggested it could be the background radiation predicted by cosmological theories, a radio remnant of the Big Bang. Penzias and Wilson won the Nobel Prize in Physics in 1978.\nNASA's work.\nThe Absolute Radiometer for Cosmology, Astrophysics, and Diffuse Emission (ARCADE) is a device designed to observe the transition out of the \"cosmic dark ages\" as the first stars ignite in nuclear fusion and the universe begins to resemble its current form.\nARCADE consists of 7 precision radiometers carried to an altitude of over 35\u00a0km (21\u00a0miles) by a scientific research balloon. The device measures the tiny heating of the early universe by the first generation of stars and galaxies to form after the Big Bang.\nSources of cosmic noise.\nCosmic noise refers to the background radio frequency radiation from galactic sources, which have constant intensity during geomagnetically quiet periods.\nSun flares.\nCosmic noise can be traced from solar flares, which are sudden explosive releases of stored magnetic energy in the atmosphere of the Sun, causing sudden brightening of the photosphere. Solar flares can last from a few minutes to several hours.\nDuring solar flare events, particles and electromagnetic emissions can affect Earth's atmosphere by fluctuating the level of ionization in the Earth's ionosphere. Increased ionization results in absorption of the cosmic radio noise as it passes through the ionosphere.\nSolar wind.\nSolar wind is a flux of particles, protons and electrons together with nuclei of heavier elements in smaller numbers, that are accelerated by the high temperatures of the solar corona to velocities large enough to allow them to escape from the Sun's gravitational field.\nSolar wind causes sudden bursts of cosmic noise absorption in the Earth's ionosphere. These bursts can only be detected only if the magnitude of the geomagnetic field perturbation caused by the solar wind shock is large enough.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "40970", "revid": "18731412", "url": "https://en.wikipedia.org/wiki?curid=40970", "title": "Costas loop", "text": "Phase-locked loop-based demodulator circuit\nA Costas loop is a phase-locked loop (PLL) based circuit which is used for carrier frequency recovery from suppressed-carrier modulation signals (e.g. double-sideband suppressed carrier signals) and phase modulation signals (e.g. BPSK, QPSK). It was invented by John P. Costas at General Electric in the 1950s. Its invention was described as having had \"a profound effect on modern digital communications\".\nThe primary application of Costas loops is in wireless receivers. Its advantage over other PLL-based detectors is that at small deviations the Costas loop error voltage is formula_1 as compared to formula_2. This translates to double the sensitivity and also makes the Costas loop uniquely suited for tracking Doppler-shifted carriers, especially in OFDM and GPS receivers.\nClassical implementation.\nIn the classical implementation of a Costas loop, a local voltage-controlled oscillator (VCO) provides quadrature outputs, one to each of two phase detectors, \"e.g.\", product detectors. The same phase of the input signal is also applied to both phase detectors, and the output of each phase detector is passed through a low-pass filter. The outputs of these low-pass filters are inputs to another phase detector, the output of which passes through a noise-reduction filter before being used to control the voltage-controlled oscillator. The coupled in-phase coherent detector and quadrature-phase coherent detector provide negative feedback to ensure the oscillator is synchronous with the carrier wave. \nThe overall loop response is controlled by the two individual low-pass filters that precede the third phase detector, while the third low-pass filter serves a trivial role in terms of gain and phase margin. The above figure of a Costas loop is drawn under the \"locked\" state, where the VCO frequency and the incoming carrier frequency have become the same due to the Costas loop process. The figure does not represent the \"unlocked\" state.\nMathematical models.\nIn the time domain.\nIn the simplest case formula_3. Therefore, formula_3 does not affect the input of the noise-reduction filter.\nThe carrier and voltage-controlled oscillator (VCO) signals are periodic oscillations formula_5 with high-frequencies formula_6.\nThe block formula_7 is an analog multiplier.\nA linear filter can be described mathematically by a system of linear differential equations:\nformula_8\nwhere formula_9 is a constant matrix, formula_10 is a state vector of the filter, formula_11 and formula_12 are constant vectors.\nThe model of a VCO is usually assumed to be linear:\nformula_13\nwhere formula_14 is the free-running frequency of the VCO and formula_15 is the VCO gain factor. Similarly, it is possible to consider various nonlinear models of VCO.\nSuppose that the frequency of the master generator is constant\nformula_16\nEquation of VCO and equation of filter yield\nformula_17\nThe system is non-autonomous and rather tricky for investigation.\nIn the phase-frequency domain.\nIn the simplest case, when\n formula_18\nThe standard engineering assumption is that the filter removes the upper sideband frequency from the input but leaves the lower sideband without change. Thus it is assumed that the VCO input is formula_19 This makes a Costas loop equivalent to a phase-locked loop with phase detector characteristic formula_20 corresponding to the particular waveforms formula_21 and formula_22 of the input and VCO signals. It can be proved that filter outputs in the time and phase-frequency domains are almost equal.\nThus it is possible to study the simpler autonomous system of differential equations\nformula_23.\nThe Krylov\u2013Bogoliubov averaging method allows one to prove that solutions of non-autonomous and autonomous equations are close under some assumptions.\nThus, the Costas loop block diagram in the time domain can be asymptotically changed to the block diagram on the level of phase-frequency relations.\nThe transition to the analysis of an autonomous dynamical model of the Costas loop (in place of the non-autonomous one) allows one to overcome the difficulties related to modeling the Costas loop in the time domain, where one has to simultaneously observe a very fast time scale of the input signals and slow time scale of signal's phase. This idea makes it possible to calculate core performance characteristics - hold-in, pull-in, and lock-in ranges.\nFrequency acquisition.\nThe classical Costas loop will work towards making the phase difference between the carrier and the VCO become a small, ideally zero, value. The small phase difference implies that frequency lock has been achieved.\nQPSK Costas loop.\nThe classical Costas loop can be adapted to QPSK modulation for higher data rates.\nThe input QPSK signal is as follows\nformula_24\nInputs of low-pass filters LPF1 and LPF2 are\nformula_25\nAfter synchronization,\nthe outputs of LPF1 formula_26 and LPF2 formula_27 are used to get demodulated data (formula_28 and formula_29). To adjust the frequency of the VCO to the reference frequency, signals formula_26 and formula_27 are limited and cross-multiplied:\nformula_32\nThen the signal formula_33 is filtered by the loop filter and forms the tuning signal for the VCO formula_34, similar to BPSK Costas loop. Thus, QPSK Costas can be described by a system of ordinary differential equations:\nformula_35\nHere formula_36 are parameters of LPF1 and LPF2 and formula_37 are parameters of the loop filter.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "40972", "revid": "1261730182", "url": "https://en.wikipedia.org/wiki?curid=40972", "title": "Coupling", "text": "Mechanical connection between two objects\nA coupling is a device used to connect two shafts together at their ends for the purpose of transmitting power. The primary purpose of couplings is to join two pieces of rotating equipment while permitting some degree of misalignment or end movement or both. In a more general context, a coupling can also be a mechanical device that serves to connect the ends of adjacent parts or objects. Couplings do not normally allow disconnection of shafts during operation, however there are torque-limiting couplings which can slip or disconnect when some torque limit is exceeded. Selection, installation and maintenance of couplings can lead to reduced maintenance time and maintenance cost.\nUses.\nShaft couplings are used in machinery for several purposes. A primary function is to transfer power from one end to another end (ex: motor transfer power to pump through coupling).\nOther common uses:\nTypes.\nBeam.\nA \"beam\" coupling, also known as \"helical\" coupling, is a flexible coupling for transmitting torque between two shafts while allowing for angular misalignment, parallel offset and even axial motion, of one shaft relative to the other. This design utilizes a single piece of material and becomes flexible by removal of material along a spiral path resulting in a curved flexible beam of helical shape. Since it is made from a single piece of material, the beam style coupling does not exhibit the backlash found in some multi-piece couplings. Another advantage of being an all machined coupling is the possibility to incorporate features into the final product while still keeping the single piece's integrity.\nChanges to the lead of the helical beam provide changes to misalignment capabilities as well as other performance characteristics such as torque capacity and torsional stiffness. It is even possible to have multiple starts within the same helix.\nThe material used to manufacture the beam coupling also affects its performance and suitability for specific applications such as food, medical and aerospace. Materials are typically aluminum alloy and stainless steel, but they can also be made in acetal, maraging steel and titanium. The most common applications are attaching rotary encoders to shafts and motion control for robotics.\nBeam couplings can be known by various names depending upon industry. These names include flexible coupling, flexible beam coupling, flexible shaft coupling, flexure, helical coupling, and shaft coupling.\nThe primary benefit to using a flexible beam coupling to join two rotating shafts is to reducing vibration and reaction loads which in turn will reduce overall wear and tear on machinery and prolong equipment life.\nBush pin flange.\nBush pin flange coupling is used for slightly imperfect alignment of the two shafts.\nThis is modified form of the protected type flange coupling. This type of coupling has pins and it works with coupling bolts. The rubber or leather bushes are used over the pins. The coupling has two halves dissimilar in construction. The pins are rigidly fastened by nuts to one of the flange and kept loose on the other flange. This coupling is used to connect shafts which have a small parallel misalignment, angular misalignment or axial misalignment. In this coupling the rubber bushing absorbs shocks and vibration during its operations. This type of coupling is mostly used to couple electric motors and machines.\nConstant velocity.\nThere are various types of constant-velocity (CV) couplings: Rzeppa joint, Double cardan joint, and Thompson coupling.\nClamp or split-muff.\nIn this coupling, the muff or sleeve is made into two halves parts of the cast iron and they are joined by means of mild steel studs or bolts. The advantages of this coupling is that assembling or disassembling of the coupling is possible without changing the position of the shaft. This coupling is used for heavy power transmission at moderate speed.\nDiaphragm.\nDiaphragm couplings transmit torque from the outside diameter of a flexible plate to the inside diameter, across the spool or spacer piece, and then from inside to outside diameter. The deforming of a plate or series of plates from I.D. to O.D accomplishes the misalignment.\nDisc.\nDisc couplings transmit torque from a driving to a driven bolt tangentially on a common bolt circle. Torque is transmitted between the bolts through a series of thin, stainless steel discs assembled in a pack. Misalignment is accomplished by deforming of the material between the bolts.\nElastic.\nAn elastic coupling transmits torque or other load by means of an elastic component. One example is the coupling used to join a windsurfing rig (sail, mast, and components) to the sailboard. In windsurfing terminology it is usually called a \"universal joint\", but modern designs are usually based on a strong flexible material, and better technically described as an elastic coupling. They can be tendon or hourglass-shaped, and are constructed of a strong and durable elastic material. In this application, the coupling does not transmit torque, but instead transmits sail-power to the board, creating thrust (some portion of sail-power is also transmitted through the rider's body).\nFlexible.\nFlexible couplings are usually used to transmit torque from one shaft to another when the two shafts are slightly misaligned. They can accommodate varying degrees of misalignment up to 1.5\u00b0 and some parallel misalignment. They can also be used for vibration damping or noise reduction. In rotating shaft applications a flexible coupling can protect the driving and driven shaft components (such as bearings) from the harmful effects of conditions such as misaligned shafts, vibration, shock loads, and thermal expansion of the shafts or other components.\nAt first, flexible couplings separate into two essential groups, metallic and elastomeric.\nMetallic types utilize freely fitted parts that roll or slide against one another or, on the other hand, non-moving parts that bend to take up misalignment.\nElastomeric types, then again, gain flexibility from resilient, non-moving, elastic or plastic elements transmitting torque between metallic hubs.\nGear.\nA \"gear coupling\" is a mechanical device for transmitting torque between two shafts that are not collinear. It consists of a flexible joint fixed to each shaft. The two joints are connected by a third shaft, called the spindle.\nEach joint consists of a 1:1 gear ratio internal\u2013external gear pair. The tooth flanks and outer diameter of the external gear are crowned to allow for angular displacement between the two gears. Mechanically, the gears are equivalent to rotating splines with modified profiles. They are called gears because of the relatively large size of the teeth.\nGear couplings and universal joints are used in similar applications. Gear couplings have higher torque densities than universal joints designed to fit a given space while universal joints induce lower vibrations. The limit on torque density in universal joints is due to the limited cross sections of the cross and yoke. The gear teeth in a gear coupling have high backlash to allow for angular misalignment. The excess backlash can contribute to vibration.\nGear couplings are generally limited to angular misalignments, i.e., the angle of the spindle relative to the axes of the connected shafts, of 4\u00b0\u20135\u00b0. Universal joints are capable of higher misalignments.\nSingle joint gear couplings are also used to connect two nominally coaxial shafts. In this application the device is called a gear-type flexible, or flexible coupling. The single joint allows for minor misalignments such as installation errors and changes in shaft alignment due to operating conditions. These types of gear couplings are generally limited to angular misalignments of &lt;templatestyles src=\"Fraction/styles.css\" /&gt;1\u20444\u00b0\u2013&lt;templatestyles src=\"Fraction/styles.css\" /&gt;1\u20442\u00b0.\nGrid.\nA \"grid coupling\" is composed of two shaft hubs, a metallic grid spring, and a split cover kit. Torque is transmitted between the two coupling shaft hubs through the metallic grid spring element.\nLike metallic gear and disc couplings, grid couplings have a high torque density. A benefit of grid couplings, over either gear or disc couplings, is the ability their grid coupling spring elements have to absorb and spread peak load impact energy over time. This reduces the magnitude of peak loads and offers some vibration dampening capability. A negative of the grid coupling design is that it generally is very limited in its ability to accommodate the misalignment.\nHighly flexible.\nHighly flexible couplings are installed when resonance or torsional vibration might be an issue, since they are designed to eliminate torsional vibration problems and to balance out shock impacts.\nThey are used in installations where the systems require a high level of torsional flexibility and misalignment capacity. This type of coupling provides an effective damping of torsional vibrations, and high displacement capacity, which protects the drive. The design of the highly flexible elastic couplings makes assembly easier. These couplings also compensate shaft displacements (radial, axial and angular) and the torque is transmitted in shear. Depending on the size and stiffness of the coupling, the flexible part may be single- or multi-row.\nHirth joints.\nHirth joints use tapered teeth on two shaft ends meshed together to transmit torque.\nJaw.\nJaw coupling is also known as spider or Lovejoy coupling.\nMagnetic.\nA magnetic coupling uses magnetic forces to transmit the power from one shaft to another without any contact. This allows for full medium separation. It can provide the ability to hermetically separate two areas whilst continuing to transmit mechanical power from one to the other making these couplings ideal for applications where prevention of cross-contamination is essential.\nOldham.\nAn \"Oldham coupling\" has three discs, one coupled to the input, one coupled to the output, and a middle disc that is joined to the first two by tongue and groove. The tongue and groove on one side is perpendicular to the tongue and groove on the other. The middle disc rotates around its center at the same speed as the input and output shafts. Its center traces a circular orbit, twice per rotation, around the midpoint between input and output shafts. Often springs are used to reduce backlash of the mechanism. An advantage to this type of coupling, as compared to two universal joints, is its compact size. The coupler is named for John Oldham who invented it in Ireland, in 1821, to solve a problem in a paddle steamer design.\nRag joint.\nRag joints are commonly used on automotive steering linkages and drive trains. When used on a drive train they are sometimes known as giubos.\nRigid.\nRigid couplings are used when precise shaft alignment is required; any shaft misalignment will affect the coupling's performance as well as its life span, because rigid couplings do not have the ability to compensate for misalignment. Due to this, their application is limited, and they're typically used in applications involving vertical drivers.\nClamped or compression rigid couplings come in two parts and fit together around the shafts to form a sleeve. They offer more flexibility than sleeved models, and can be used on shafts that are fixed in place. They generally are large enough so that screws can pass all the way through the coupling and into the second half to ensure a secure hold. Flanged rigid couplings are designed for heavy loads or industrial equipment. They consist of short sleeves surrounded by a perpendicular flange. One coupling is placed on each shaft so the two flanges line up face to face. A series of screws or bolts can then be installed in the flanges to hold them together. Because of their size and durability, flanged units can be used to bring shafts into alignment before they are joined.\nSleeve, box, or muff.\nA sleeve coupling consists of a pipe whose bore is finished to the required tolerance based on the shaft size. Based on the usage of the coupling a keyway is made in the bore in order to transmit the torque by means of the key. Two threaded holes are provided in order to lock the coupling in position.\nSleeve couplings are also known as box couplings. In this case shaft ends are coupled together and abutted against each other which are enveloped by \"muff\" or \"sleeve\".\nA \"gib head sunk keys\" hold the two shafts and sleeve together (this is the simplest type of the coupling) It is made from the cast iron and very simple to design and manufacture. It consists of a hollow pipe whose inner diameter is same as diameter of the shafts.\nThe hollow pipe is fitted over a two or more ends of the shafts with the help of the taper sunk key. A key and sleeve are useful to transmit power from one shaft to another shaft.\nTapered shaft lock.\nA tapered lock is a form of keyless shaft locking device that does not require any material to be removed from the shaft. The basic idea is similar to a clamp coupling but the moment of rotation is closer to the center of the shaft. An alternative coupling device to the traditional parallel key, the tapered lock removes the possibility of play due to worn keyways. It is more robust than using a key because maintenance only requires one tool and the self-centering balanced rotation means it lasts longer than a keyed joint would, but the downside is that it costs more.\nTwin spring.\nA flexible coupling made from two counter-wound springs with a ball bearing in the center, which allows torque transfer from input to output shaft. Requires no lubrication to consistently run as it has no internal components.\nMaintenance and failure.\nCoupling maintenance requires a regularly scheduled inspection of each coupling. It consists of:\nEven with proper maintenance, however, couplings can fail. Underlying reasons for failure, other than maintenance, include:\nExternal signs that indicate potential coupling failure include:\nBalance.\nCouplings are normally balanced at the factory prior to being shipped, but they occasionally go out of balance in operation. Balancing can be difficult and expensive, and is normally done only when operating tolerances are such that the effort and the expense are justified. The amount of coupling unbalance that can be tolerated by any system is dictated by the characteristics of the specific connected machines and can be determined by detailed analysis or experience.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "40973", "revid": "16944068", "url": "https://en.wikipedia.org/wiki?curid=40973", "title": "Cover", "text": "Cover or covers may refer to:\n&lt;templatestyles src=\"Template:TOC_right/styles.css\" /&gt;\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "40974", "revid": "39795743", "url": "https://en.wikipedia.org/wiki?curid=40974", "title": "Critical angle", "text": "Critical angle may refer to:\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "40975", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=40975", "title": "Critical frequency", "text": "In telecommunications, the term critical frequency has the following meanings:\nCritical Frequency changes with time of day, atmospheric conditions and angle of fire of the radio waves by antenna.\nThe existence of the critical frequency is the result of electron limitation, \"i.e.,\" the inadequacy of the existing number of free electrons to support reflection at higher frequencies.\nIn signal processing the \"critical frequency\" it is also another name for the Nyquist frequency.\nCritical frequency is the highest magnitude of frequency above which the waves penetrate the ionosphere and below which the waves are reflected back from the ionosphere.\nIt is denoted by \"\"f\"c\".\nIts value is not fixed and it depends upon the electron density of the ionosphere.\nEquations.\nCritical frequency as a function of electron density.\nCritical frequency can be computed with the electron density given by:\nformula_1\nwhere \"N\"max is maximum electron density per m3 and \"f\"c is in Hz.\nCritical frequency as a function of maximum usable frequency.\nCritical frequency can be computed by:\nformula_2\nwhere MUF is maximum usable frequency and formula_3 is the angle of incidence\nRelationship with plasma frequency.\nThe dependence of critical frequency with respect with electron density can be related through plasma oscillation concept particularly the 'Cold' Electrons mechanism.\nformula_4\nUsing the electron charge formula_5, electron mass formula_6 and permittivity of free space formula_7gives,\nformula_8\nand solving for the frequency,\nformula_9\nRelationship with index of refraction.\nThe index of refraction has the formula formula_10which shows dependence in wavelength. The result that the force due to the polarization field in an ionized gas of low concentration is canceled by the effect of collisions between ions and electrons is re\u2010established in a simple manner that clearly displays the physical basis for the effect. Because of this cancellation the Sellmeyer formula, determines the relation between the electron number density, N, and the index of refraction, n, in the ionosphere when collisions are neglected.\nformula_11.\nUsing the default values for electron charge formula_12, permittivity of free space and electron mass formula_13, and changing angular velocity formula_14with respect to frequency formula_15this yields to\nformula_16\nand solving for the refraction index n,\nformula_17\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "40976", "revid": "1214359858", "url": "https://en.wikipedia.org/wiki?curid=40976", "title": "Crosstalk (disambiguation)", "text": "Crosstalk refers to any signal or circuit unintentionally affecting another signal or circuit.\nCrosstalk may also refer to:\nArts and entertainment.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "40977", "revid": "30607451", "url": "https://en.wikipedia.org/wiki?curid=40977", "title": "International Cryptology Conference", "text": ""}
{"id": "40978", "revid": "46977059", "url": "https://en.wikipedia.org/wiki?curid=40978", "title": "Cryptochannel", "text": "System of crypto-communicationsIn telecommunications, a cryptochannel is a complete system of crypto-communications between two or more holders or parties. It includes: (a) the cryptographic aids prescribed; (b) the holders thereof; (c) the indicators or other means of identification; (d) the area or areas in which effective; (e) the special purpose, if any, for which provided; and (f) pertinent notes as to distribution, usage, \"etc.\" A cryptochannel is analogous to a radio circuit.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "40979", "revid": "30547298", "url": "https://en.wikipedia.org/wiki?curid=40979", "title": "Crystal oscillator", "text": "Electronic oscillator circuit\nA crystal oscillator is an electronic oscillator circuit that uses a piezoelectric crystal as a frequency-selective element. The oscillator frequency is often used to keep track of time, as in quartz wristwatches, to provide a stable clock signal for digital integrated circuits, and to stabilize frequencies for radio transmitters and receivers. The most common type of piezoelectric resonator used is a quartz crystal, so oscillator circuits incorporating them became known as crystal oscillators. However, other piezoelectric materials including polycrystalline ceramics are used in similar circuits.\nA crystal oscillator relies on the slight change in shape of a quartz crystal under an electric field, a property known as inverse piezoelectricity. A voltage applied to the electrodes on the crystal causes it to change shape; when the voltage is removed, the crystal generates a small voltage as it elastically returns to its original shape. The quartz oscillates at a stable resonant frequency (relative to other low-priced oscillators) with frequency accuracy measured in parts per million (ppm). It behaves like an RLC circuit, but with a much higher Q factor (lower energy loss on each cycle of oscillation and higher frequency selectivity) than can be reliably achieved with discrete capacitors (C) and inductors (L), which suffer from parasitic resistance (R). Once a quartz crystal is adjusted to a particular frequency (which is affected by the mass of electrodes attached to the crystal, the orientation of the crystal, temperature and other factors), it maintains that frequency with high stability.\nQuartz crystals are manufactured for frequencies from a few tens of kilohertz to hundreds of megahertz. As of 2003, around two billion crystals were manufactured annually. Most are used for consumer devices such as wristwatches, clocks, radios, computers, and cellphones. However, in applications where small size and weight is needed crystals can be replaced by thin-film bulk acoustic resonators, specifically if ultra-high frequency (more than roughly 1.5\u00a0GHz) resonance is needed. Quartz crystals are also found inside test and measurement equipment, such as counters, signal generators, and oscilloscopes.\nTerminology.\nA crystal oscillator is an electric oscillator type circuit that uses a piezoelectric resonator, a crystal, as its frequency-determining element. \"Crystal\" is the common term used in electronics for the frequency-determining component, a wafer of quartz crystal or ceramic with electrodes connected to it. A more accurate term for \"crystal\" is \"piezoelectric resonator\". Crystals are also used in other types of electronic circuits, such as crystal filters.\nPiezoelectric resonators are sold as separate components for use in crystal oscillator circuits. They are also often incorporated in a single package with the crystal oscillator circuit.\nHistory.\nPiezoelectricity was discovered by Jacques and Pierre Curie in 1880. Paul Langevin first investigated quartz resonators for use in sonar during World War I. The first crystal-controlled oscillator, using a crystal of Rochelle salt, was built in 1917 and patented in 1918 by Alexander M. Nicolson at Western Electric, although his priority was disputed by Walter Guyton Cady. Cady built the first quartz crystal oscillator in 1921.\nOther early innovators in quartz crystal oscillators include G. W. Pierce and Louis Essen.\nQuartz crystal oscillators were developed for high-stability frequency references during the 1920s and 1930s. Prior to crystals, radio stations controlled their frequency with tuned circuits, which could easily drift off frequency by 3\u20134\u00a0kHz. Since broadcast stations were assigned frequencies only 10\u00a0kHz (Americas) or 9\u00a0kHz (elsewhere) apart, interference between adjacent stations due to frequency drift was a common problem. In 1925, Westinghouse installed a crystal oscillator in its flagship station KDKA, and by 1926, quartz crystals were used to control the frequency of many broadcasting stations and were popular with amateur radio operators. In 1928, Warren Marrison of Bell Laboratories developed the first quartz-crystal clock. With accuracies of up to 1 second in 30 years (30\u00a0ms/y, or 0.95\u00a0ns/s), quartz clocks replaced precision pendulum clocks as the world's most accurate timekeepers until atomic clocks were developed in the 1950s. Using the early work at Bell Laboratories, American Telephone and Telegraph Company (AT&amp;T) eventually established their Frequency Control Products division, later spun off and known today as Vectron International.\nA number of firms started producing quartz crystals for electronic use during this time. Using what are now considered primitive methods, about 100,000 crystal units were produced in the United States during 1939. Through World War II crystals were made from natural quartz crystal, virtually all from Brazil. Shortages of crystals during the war caused by the demand for accurate frequency control of military and naval radios and radars spurred postwar research into culturing synthetic quartz, and by 1950 a hydrothermal process for growing quartz crystals on a commercial scale was developed at Bell Laboratories. By the 1970s virtually all crystals used in electronics were synthetic.\nIn 1968, Juergen Staudte invented a photolithographic process for manufacturing quartz crystal oscillators while working at North American Aviation (now Rockwell) that allowed them to be made small enough for portable products like watches.\nAlthough crystal oscillators still most commonly use quartz crystals, devices using other materials are becoming more common, such as ceramic resonators.\nPrinciple.\nA crystal is a solid in which the constituent atoms, molecules, or ions are packed in a regularly ordered, repeating pattern extending in all three spatial dimensions.\nAlmost any object made of an elastic material could be used like a crystal, with appropriate transducers, since all objects have natural resonant frequencies of vibration. For example, steel is very elastic and has a high speed of sound. It was often used in mechanical filters before quartz. The resonant frequency depends on size, shape, elasticity, and the speed of sound in the material. High-frequency crystals are typically cut in the shape of a simple rectangle or circular disk. Low-frequency crystals, such as those used in digital watches, are typically cut in the shape of a tuning fork. For applications not needing very precise timing, a low-cost ceramic resonator is often used in place of a quartz crystal.\nWhen a crystal of quartz is properly cut and mounted, it can be made to distort in an electric field by applying a voltage to an electrode near or on the crystal. This property is known as inverse piezoelectricity. When the field is removed, the quartz generates an electric field as it returns to its previous shape, and this can generate a voltage. The result is that a quartz crystal behaves like an RLC circuit, composed of an inductor, capacitor and resistor, with a precise resonant frequency.\nQuartz has the further advantage that its elastic constants and its size change in such a way that the frequency dependence on temperature can be very low. The specific characteristics depend on the mode of vibration and the angle at which the quartz is cut (relative to its crystallographic axes). Therefore, the resonant frequency of the plate, which depends on its size, does not change much. This means that a quartz clock, filter or oscillator remains accurate. For critical applications the quartz oscillator is mounted in a temperature-controlled container, called a crystal oven, and can also be mounted on shock absorbers to prevent perturbation by external mechanical vibrations.\nModeling.\nElectrical model.\nA quartz crystal can be modeled as an electrical network with low-impedance (series) and high-impedance (parallel) resonance points spaced closely together. Mathematically, using the Laplace transform, the impedance of this network can be written as:\n formula_1\nor\n formula_2\nwhere formula_3 is the complex frequency (formula_4), formula_5 is the series resonant angular frequency, and formula_6 is the parallel resonant angular frequency.\nAdding capacitance across a crystal causes the (parallel) resonant frequency to decrease. Adding inductance across a crystal causes the (parallel) resonant frequency to increase. These effects can be used to adjust the frequency at which a crystal oscillates. Crystal manufacturers normally cut and trim their crystals to have a specified resonant frequency with a known \"load\" capacitance added to the crystal. For example, a crystal intended for a 6\u00a0pF load has its specified parallel resonant frequency when a 6.0\u00a0pF capacitor is placed across it. Without the load capacitance, the resonant frequency is higher.\nResonance modes.\nA quartz crystal provides both series and parallel resonance. The series resonance is a few kilohertz lower than the parallel one. Crystals below 30\u00a0MHz are generally operated between series and parallel resonance, which means that the crystal appears as an inductive reactance in operation, this inductance forming a parallel resonant circuit with externally connected parallel capacitance. \nAny small additional capacitance in parallel with the crystal pulls the frequency lower. Moreover, the effective inductive reactance of the crystal can be reduced by adding a capacitor in series with the crystal. This latter technique can provide a useful method of trimming the oscillatory frequency within a narrow range; in this case inserting a capacitor in series with the crystal raises the frequency of oscillation. For a crystal to operate at its specified frequency, the electronic circuit has to be exactly that specified by the crystal manufacturer. Note that these points imply a subtlety concerning crystal oscillators in this frequency range: the crystal does not usually oscillate at precisely either of its resonant frequencies.\nCrystals above 30\u00a0MHz (up to &gt;200\u00a0MHz) are generally operated at series resonance where the impedance appears at its minimum and equal to the series resistance. For these crystals the series resistance is specified (&lt;100\u00a0\u03a9) instead of the parallel capacitance. To reach higher frequencies, a crystal can be made to vibrate at one of its overtone modes, which occur near multiples of the fundamental resonant frequency. Only odd numbered overtones are used. Such a crystal is referred to as a 3rd, 5th, or even 7th overtone crystal. To accomplish this, the oscillator circuit usually includes additional LC circuits to select the desired overtone.\nTemperature effects.\nA crystal's frequency characteristic depends on the shape or \"cut\" of the crystal. A tuning-fork crystal is usually cut such that its frequency dependence on temperature is quadratic with the maximum around 25\u00a0\u00b0C. This means that a tuning-fork crystal oscillator resonates close to its target frequency at room temperature, but slows when the temperature either increases or decreases from room temperature. A common parabolic coefficient for a 32\u00a0kHz tuning-fork crystal is \u22120.04 ppm/\u00b0C2:\n formula_7\nIn a real application, this means that a clock built using a regular 32\u00a0kHz tuning-fork crystal keeps good time at room temperature, but loses 2 minutes per year at 10\u00a0\u00b0C above or below room temperature and loses 8 minutes per year at 20\u00a0\u00b0C above or below room temperature due to the quartz crystal.\nCrystal oscillator circuits.\nThe crystal oscillator circuit sustains oscillation by taking a voltage signal from the quartz resonator, amplifying it, and feeding it back to the resonator. The rate of expansion and contraction of the quartz is the resonant frequency, and is determined by the cut and size of the crystal. When the energy of the generated output frequencies matches the losses in the circuit, an oscillation can be sustained.\nAn oscillator crystal has two electrically conductive plates, with a slice or tuning fork of quartz crystal sandwiched between them. During startup, the controlling circuit places the crystal into an unstable equilibrium, and due to the positive feedback in the system, any tiny fraction of noise is amplified, ramping up the oscillation. The crystal resonator can also be seen as a highly frequency-selective filter in this system: it only passes a very narrow subband of frequencies around the resonant one, attenuating everything else. Eventually, only the resonant frequency is active. As the oscillator amplifies the signals coming out of the crystal, the signals in the crystal's frequency band becomes stronger, eventually dominating the output of the oscillator. The narrow resonance band of the quartz crystal filters out the unwanted frequencies.\nThe output frequency of a quartz oscillator can be either that of the fundamental resonance or of a multiple of that resonance, called a harmonic frequency. Harmonics are an exact integer multiple of the fundamental frequency. But, like many other mechanical resonators, crystals exhibit several modes of oscillation, usually at approximately odd integer multiples of the fundamental frequency. These are termed \"overtone modes\", and oscillator circuits can be designed to excite them. The overtone modes are at frequencies which are approximate, but not exact odd integer multiples of that of the fundamental mode, and overtone frequencies are therefore not exact harmonics of the fundamental.\nHigh frequency crystals are often designed to operate at third, fifth, or seventh overtones. Manufacturers have difficulty producing crystals thin enough to produce fundamental frequencies over 30\u00a0MHz. To produce higher frequencies, manufacturers make overtone crystals tuned to put the 3rd, 5th, or 7th overtone at the desired frequency, because they are thicker and therefore easier to manufacture than a fundamental crystal that would produce the same frequency\u2014although exciting the desired overtone frequency requires a slightly more complicated oscillator circuit.\nA fundamental crystal oscillator circuit is simpler and more efficient and has more pullability than a third overtone circuit.\nDepending on the manufacturer, the highest available fundamental frequency may be 25\u00a0MHz to 66\u00a0MHz.\nA major reason for the wide use of crystal oscillators is their high Q factor. A typical \"Q\" value for a quartz oscillator ranges from 104 to 106, compared to perhaps 102 for an LC oscillator. The maximum \"Q\" for a high stability quartz oscillator can be estimated as \"Q\" = 1.6 \u00d7 107/\"f\", where \"f\" is the resonant frequency in megahertz.\nOne of the most important traits of quartz crystal oscillators is that they can exhibit very low phase noise.\nIn many oscillators, any spectral energy at the resonant frequency is amplified by the oscillator, resulting in a collection of tones at different phases.\nIn a crystal oscillator, the crystal mostly vibrates in one axis, therefore only one phase is dominant.\nThis property of low phase noise makes them particularly useful in telecommunications where stable signals are needed, and in scientific equipment where very precise time references are needed.\nEnvironmental changes of temperature, humidity, pressure, and vibration can change the resonant frequency of a quartz crystal, but there are several designs that reduce these environmental effects. These include the TCXO, MCXO, and OCXO which are defined below. These designs, particularly the OCXO, often produce devices with excellent short-term stability. The limitations in short-term stability are due mainly to noise from electronic components in the oscillator circuits. Long-term stability is limited by aging of the crystal.\nDue to aging and environmental factors (such as temperature and vibration), it is difficult to keep even the best quartz oscillators within one part in 1010 of their nominal frequency without constant adjustment. For this reason, atomic oscillators are used for applications requiring better long-term stability and accuracy.\nSpurious frequencies.\nFor crystals operated at series resonance or pulled away from the main mode by the inclusion of a series inductor or capacitor, significant (and temperature-dependent) spurious responses may be experienced. Though most spurious modes are typically some tens of kilohertz above the wanted series resonance, their temperature coefficient is different from the main mode, and the spurious response may move through the main mode at certain temperatures. Even if the series resistances at the spurious resonances appear higher than the one at the wanted frequency, a rapid change in the main mode series resistance can occur at specific temperatures when the two frequencies are coincidental.\nA consequence of these activity dips is that the oscillator may lock at a spurious frequency at specific temperatures. This is generally minimized by ensuring that the maintaining circuit has insufficient gain to activate unwanted modes.\nSpurious frequencies are also generated by subjecting the crystal to vibration. This modulates the resonant frequency to a small degree by the frequency of the vibrations. SC-cut (Stress Compensated) crystals are designed to minimize the frequency effect of mounting stress and they are therefore less sensitive to vibration. Acceleration effects including gravity are also reduced with SC-cut crystals, as is frequency change with time due to long term mounting stress variation.\nThere are disadvantages with SC-cut shear mode crystals, such as the need for the maintaining oscillator to discriminate against other closely related unwanted modes and increased frequency change due to temperature when subject to a full ambient range. SC-cut crystals are most advantageous where temperature control at their temperature of zero temperature coefficient (turnover) is possible, under these circumstances an overall stability performance from premium units can approach the stability of rubidium frequency standards.\nCommonly used crystal frequencies.\nCrystals can be manufactured for oscillation over a wide range of frequencies, from a few kilohertz up to several hundred megahertz. Many applications call for a crystal oscillator frequency conveniently related to some other desired frequency, so hundreds of standard crystal frequencies are made in large quantities and stocked by electronics distributors. For example 3.579545\u00a0MHz crystals, which were made in large quantities for NTSC color television receivers, are now popular for many non-television applications (although most modern television receivers now use other frequency crystals for the color decoder). Using frequency dividers, frequency multipliers and phase-locked loop circuits, it is practical to derive a wide range of frequencies from one reference frequency.\nCrystal structures and materials.\nQuartz.\nThe most common material for oscillator crystals is quartz. At the beginning of the technology, natural quartz crystals were used but now synthetic crystalline quartz grown by hydrothermal synthesis is predominant due to higher purity, lower cost and more convenient handling. One of the few remaining uses of natural crystals is for pressure transducers in deep wells. During World War II and for some time afterwards, natural quartz was considered a strategic material by the USA. Large crystals were imported from Brazil. Raw \"lascas\", the source material quartz for hydrothermal synthesis, are imported to USA or mined locally by Coleman Quartz. The average value of as-grown synthetic quartz in 1994 was 60 USD/kg.\nTypes.\nTwo types of quartz crystals exist: left-handed and right-handed. The two differ in their optical rotation but they are identical in other physical properties. Both left and right-handed crystals can be used for oscillators, if the cut angle is correct. In manufacture, right-handed quartz is generally used. The SiO4 tetrahedrons form parallel helices; the direction of twist of the helix determines the left- or right-hand orientation. The helixes are aligned along the c-axis and merged, sharing atoms. The mass of the helixes forms a mesh of small and large channels parallel to the c-axis. The large ones are large enough to allow some mobility of smaller ions and molecules through the crystal.\nQuartz exists in several phases. At 573\u00a0\u00b0C at 1 atmosphere (and at higher temperatures and higher pressures) the \u03b1-quartz undergoes quartz inversion, transforms reversibly to \u03b2-quartz. The reverse process however is not entirely homogeneous and crystal twinning occurs. Care must be taken during manufacturing and processing to avoid phase transformation. Other phases, e.g. the higher-temperature phases tridymite and cristobalite, are not significant for oscillators. All quartz oscillator crystals are the \u03b1-quartz type.\nQuality.\nInfrared spectrophotometry is used as one of the methods for measuring the quality of the grown crystals. The wavenumbers 3585, 3500, and 3410\u00a0cm\u22121 are commonly used. The measured value is based on the absorption bands of the OH radical and the infrared Q value is calculated. The electronic grade crystals, grade C, have Q of 1.8 million or above; the premium grade B crystals have Q of 2.2 million, and special premium grade A crystals have Q of 3.0 million. The Q value is calculated only for the z region; crystals containing other regions can be adversely affected. Another quality indicator is the etch channel density; when the crystal is etched, tubular channels are created along linear defects. For processing involving etching, e.g. the wristwatch tuning fork crystals, low etch channel density is desirable. The etch channel density for swept quartz is about 10\u2013100 and significantly more for unswept quartz. Presence of etch channels and etch pits degrades the resonator's Q and introduces nonlinearities.\nProduction.\nQuartz crystals can be grown for specific purposes.\nCrystals for AT-cut are the most common in mass production of oscillator materials; the shape and dimensions are optimized for high yield of the required wafers. High-purity quartz crystals are grown with especially low content of aluminium, alkali metal and other impurities and minimal defects; the low amount of alkali metals provides increased resistance to ionizing radiation. Crystals for wrist watches, for cutting the tuning fork 32768\u00a0Hz crystals, are grown with very low etch channel density.\nCrystals for SAW devices are grown as flat, with large X-size seed with low etch channel density.\nSpecial high-Q crystals, for use in highly stable oscillators, are grown at constant slow speed and have constant low infrared absorption along the entire Z axis. Crystals can be grown as Y-bar, with a seed crystal in bar shape and elongated along the Y axis, or as Z-plate, grown from a plate seed with Y-axis direction length and X-axis width. The region around the seed crystal contains a large number of crystal defects and should not be used for the wafers.\nCrystals grow anisotropically; the growth along the Z axis is up to 3 times faster than along the X axis. The growth direction and rate also influences the rate of uptake of impurities. Y-bar crystals, or Z-plate crystals with long Y axis, have four growth regions usually called +X, \u2212X, Z, and S. The distribution of impurities during growth is uneven; different growth areas contain different levels of contaminants. The Z regions are the purest, the small occasionally present S regions are less pure, the +X region is yet less pure, and the -X region has the highest level of impurities. The impurities have a negative impact on radiation hardness, susceptibility to twinning, filter loss, and long and short term stability of the crystals. Different-cut seeds in different orientations may provide other kinds of growth regions. The growth speed of the \u2212X direction is slowest due to the effect of adsorption of water molecules on the crystal surface; aluminium impurities suppress growth in two other directions. The content of aluminium is lowest in Z region, higher in +X, yet higher in \u2212X, and highest in S; the size of S regions also grows with increased amount of aluminium present. The content of hydrogen is lowest in Z region, higher in +X region, yet higher in S region, and highest in \u2212X. Aluminium inclusions transform into color centers with gamma-ray irradiation, causing a darkening of the crystal proportional to the dose and level of impurities; the presence of regions with different darkness reveals the different growth regions.\nThe dominant type of defect of concern in quartz crystals is the substitution of an Al(III) for a Si(IV) atom in the crystal lattice. The aluminium ion has an associated interstitial charge compensator present nearby, which can be a H+ ion (attached to the nearby oxygen and forming a hydroxyl group, called Al\u2212OH defect), Li+ ion, Na+ ion, K+ ion (less common), or an electron hole trapped in a nearby oxygen atom orbital. The composition of the growth solution, whether it is based on lithium or sodium alkali compounds, determines the charge compensating ions for the aluminium defects. The ion impurities are of concern as they are not firmly bound and can migrate through the crystal, altering the local lattice elasticity and the resonant frequency of the crystal. Other common impurities of concern are e.g. iron(III) (interstitial), fluorine, boron(III), phosphorus(V) (substitution), titanium(IV) (substitution, universally present in magmatic quartz, less common in hydrothermal quartz), and germanium(IV) (substitution). Sodium and iron ions can cause inclusions of acnite and elemeusite crystals. Inclusions of water may be present in fast-grown crystals; interstitial water molecules are abundant near the crystal seed. Another defect of importance is the hydrogen containing growth defect, when instead of a Si\u2212O\u2212Si structure, a pair of Si\u2212OH HO\u2212Si groups is formed; essentially a hydrolyzed bond. Fast-grown crystals contain more hydrogen defects than slow-grown ones. These growth defects source as supply of hydrogen ions for radiation-induced processes and forming Al-OH defects. Germanium impurities tend to trap electrons created during irradiation; the alkali metal cations then migrate towards the negatively charged center and form a stabilizing complex. Matrix defects can also be present; oxygen vacancies, silicon vacancies (usually compensated by 4 hydrogens or 3 hydrogens and a hole), peroxy groups, etc. Some of the defects produce localized levels in the forbidden band, serving as charge traps; Al(III) and B(III) typically serve as hole traps while electron vacancies, titanium, germanium, and phosphorus atoms serve as electron traps. The trapped charge carriers can be released by heating; their recombination is the cause of thermoluminescence.\nThe mobility of interstitial ions depends strongly on temperature. Hydrogen ions are mobile down to 10 K, but alkali metal ions become mobile only at temperatures around and above 200 K.\nThe hydroxyl defects can be measured by near-infrared spectroscopy. The trapped holes can be measured by electron spin resonance. The Al\u2212Na+ defects show as an acoustic loss peak due to their stress-induced motion; the Al\u2212Li+ defects do not form a potential well so are not detectable this way. Some of the radiation-induced defects during their thermal annealing produce thermoluminescence; defects related to aluminium, titanium, and germanium can be distinguished.\nSwept crystals are crystals that have undergone a solid-state electrodiffusion purification process. Sweeping involves heating the crystal above 500\u00a0\u00b0C in a hydrogen-free atmosphere, with a voltage gradient of at least 1 kV/cm, for several hours (usually over 12). The migration of impurities and the gradual replacement of alkali metal ions with hydrogen (when swept in air) or electron holes (when swept in vacuum) causes a weak electric current through the crystal; decay of this current to a constant value signals the end of the process. The crystal is then left to cool, while the electric field is maintained. The impurities are concentrated at the cathode region of the crystal, which is cut off afterwards and discarded. Swept crystals have increased resistance to radiation, as the dose effects are dependent on the level of alkali metal impurities; they are suitable for use in devices exposed to ionizing radiation, e.g. for nuclear and space technology. Sweeping under vacuum at higher temperatures and higher field strengths yields yet more radiation-hard crystals. The level and character of impurities can be measured by infrared spectroscopy. Quartz can be swept in both \u03b1 and \u03b2 phase; sweeping in \u03b2 phase is faster, but the phase transition may induce twinning. Twinning can be mitigated by subjecting the crystal to compression stress in the X direction, or an AC or DC electric field along the X axis while the crystal cools through the phase transformation temperature region.\nSweeping can also be used to introduce one kind of an impurity into the crystal. Lithium, sodium, and hydrogen swept crystals are used for, e.g., studying quartz behavior.\nVery small crystals for high fundamental-mode frequencies can be manufactured by photolithography.\nCrystals can be adjusted to exact frequencies by laser trimming. A technique used in the world of amateur radio for slight decrease of the crystal frequency may be achieved by exposing crystals with silver electrodes to vapors of iodine, which causes a slight mass increase on the surface by forming a thin layer of silver iodide; such crystals however had problematic long-term stability. Another method commonly used is electrochemical increase or decrease of silver electrode thickness by submerging a resonator in lapis lazuli dissolved in water, citric acid in water, or water with salt, and using the resonator as one electrode, and a small silver electrode as the other.\nBy choosing the direction of current one can either increase or decrease the mass of the electrodes.\nDetails were published in \"Radio\" magazine (3/1978) by UB5LEV.\nRaising frequency by scratching off parts of the electrodes is not advised as this may damage the crystal and lower its Q factor. Capacitor trimmers can be also used for frequency adjustment of the oscillator circuit.\nOther materials.\nSome other piezoelectric materials than quartz can be employed. These include single crystals of lithium tantalate, lithium niobate, lithium borate, berlinite, gallium arsenide, lithium tetraborate, aluminium phosphate, bismuth germanium oxide, polycrystalline zirconium titanate ceramics, high-alumina ceramics, silicon-zinc oxide composite, or dipotassium tartrate. Some materials may be more suitable for specific applications. An oscillator crystal can be also manufactured by depositing the resonator material on the silicon chip surface. Crystals of gallium phosphate, langasite, langanite and langatate are about 10 times more pullable than the corresponding quartz crystals, and are used in some VCXO oscillators.\nStability.\nThe frequency stability is determined by the crystal's quality factor (Q factor). It is inversely dependent on the frequency, and on the constant that is dependent on the particular cut. Other factors influencing Q are the overtone used, the temperature, the level of driving of the crystal, the quality of the surface finish, the mechanical stresses imposed on the crystal by bonding and mounting, the geometry of the crystal and the attached electrodes, the material purity and defects in the crystal, type and pressure of the gas in the enclosure, interfering modes, and presence and absorbed dose of ionizing and neutron radiation.\nThe stability of AT cut crystals decreases with increasing frequency. For more accurate higher frequencies it is better to use a crystal with lower fundamental frequency, operating at an overtone.\nA badly designed oscillator circuit may suddenly begin oscillating on an overtone. In 1972, a train in Fremont, California crashed due to a faulty oscillator. An inappropriate value of the tank capacitor caused the crystal in a control board to be overdriven, jumping to an overtone, and causing the train to speed up instead of slowing down.\nTemperature.\nTemperature influences the operating frequency; various forms of compensation are used, from analog compensation (TCXO) and microcontroller compensation (MCXO) to stabilization of the temperature with a crystal oven (OCXO). The crystals possess temperature hysteresis; the frequency at a given temperature achieved by increasing the temperature is not equal to the frequency on the same temperature achieved by decreasing the temperature. The temperature sensitivity depends primarily on the cut; the temperature compensated cuts are chosen as to minimize frequency/temperature dependence. Special cuts can be made with linear temperature characteristics; the LC cut is used in quartz thermometers. Other influencing factors are the overtone used, the mounting and electrodes, impurities in the crystal, mechanical strain, crystal geometry, rate of temperature change, thermal history (due to hysteresis), ionizing radiation, and drive level.\nCrystals tend to suffer anomalies in their frequency/temperature and resistance/temperature characteristics, known as activity dips. These are small downward frequency or upward resistance excursions localized at certain temperatures, with their temperature position dependent on the value of the load capacitors.\nMechanical stress.\nMechanical stresses also influence the frequency. The stresses can be induced by mounting, bonding, and application of the electrodes, by differential thermal expansion of the mounting, electrodes, and the crystal itself, by differential thermal stresses when there is a temperature gradient present, by expansion or shrinkage of the bonding materials during curing, by the air pressure that is transferred to the ambient pressure within the crystal enclosure, by the stresses of the crystal lattice itself (nonuniform growth, impurities, dislocations), by the surface imperfections and damage caused during manufacture, and by the action of gravity on the mass of the crystal; the frequency can therefore be influenced by position of the crystal. Other dynamic stress inducing factors are shocks, vibrations, and acoustic noise. Some cuts are less sensitive to stresses; the SC (stress-compensated) cut is an example. Atmospheric pressure changes can also introduce deformations to the housing, influencing the frequency by changing stray capacitances.\nAtmospheric humidity influences the thermal transfer properties of air, and can change electrical properties of plastics by diffusion of water molecules into their structure, altering the dielectric constants and electrical conductivity.\nOther factors influencing the frequency are the power supply voltage, load impedance, magnetic fields, electric fields (in case of cuts that are sensitive to them, e.g., SC cuts), the presence and absorbed dose of \u03b3-particles and ionizing radiation, and the age of the crystal.\nAging.\nCrystals undergo slow gradual change of frequency with time, known as aging. There are many mechanisms involved. The mounting and contacts may undergo relief of the built-in stresses. Molecules of contamination either from the residual atmosphere, outgassed from the crystal, electrodes or packaging materials, or introduced during sealing the housing can be adsorbed on the crystal surface, changing its mass; this effect is exploited in quartz crystal microbalances. The composition of the crystal can be gradually altered by outgassing, diffusion of atoms of impurities or migrating from the electrodes, or the lattice can be damaged by radiation. Slow chemical reactions may occur on or in the crystal, or on the inner surfaces of the enclosure. Electrode material, e.g. chromium or aluminium, can react with the crystal, creating layers of metal oxide and silicon; these interface layers can undergo changes in time. The pressure in the enclosure can change due to varying atmospheric pressure, temperature, leaks, or outgassing of the materials inside. Factors outside of the crystal itself are e.g. aging of the oscillator circuitry (and e.g. change of capacitances), and drift of parameters of the crystal oven. External atmosphere composition can also influence the aging; hydrogen can diffuse through nickel housing. Helium can cause similar issues when it diffuses through glass enclosures of rubidium standards.\nGold is a favored electrode material for low-aging resonators; its adhesion to quartz is strong enough to maintain contact even at strong mechanical shocks, but weak enough to not support significant strain gradients (unlike chromium, aluminium, and nickel). Gold also does not commonly form oxides; it adsorbs organic contaminants from the air, but these are easy to remove. However, gold alone can undergo delamination; a layer of chromium is therefore sometimes used for improved binding strength. Silver and aluminium are often used as electrodes; however both form oxide layers with time that increases the crystal mass and lowers frequency. Silver can be passivated by exposure to iodine vapors, forming a layer of silver iodide. Aluminium oxidizes readily but slowly, until about 5\u00a0nm thickness is reached; increased temperature during artificial aging does not significantly increase the oxide forming speed; a thick oxide layer can be formed during manufacture by anodizing. Exposition of silver-plated crystal to iodine vapors can also be used in amateur conditions for lowering the crystal frequency slightly; the frequency can also be increased by scratching off parts of the electrodes, but that carries risk of damage to the crystal and loss of Q.\nA DC voltage bias between the electrodes can accelerate the initial aging, probably by induced diffusion of impurities through the crystal. Placing a capacitor in series with the crystal and a several-megaohm resistor in parallel can minimize such voltages.\nAging decreases logarithmically with time, the largest changes occurring shortly after manufacture. Artificially aging a crystal by prolonged storage at 85 to 125\u00a0\u00b0C can increase its long-term stability.\nMechanical damage.\nCrystals are sensitive to shock. The mechanical stress causes a short-term change in the oscillator frequency due to the stress-sensitivity of the crystal, and can introduce a permanent change of frequency due to shock-induced changes of mounting and internal stresses (if the elastic limits of the mechanical parts are exceeded), desorption of contamination from the crystal surfaces, or change in parameters of the oscillator circuit. High magnitudes of shocks may tear the crystals off their mountings (especially in the case of large low-frequency crystals suspended on thin wires), or cause cracking of the crystal. Crystals free of surface imperfections are highly shock-resistant; chemical polishing can produce crystals able to survive tens of thousands of g.\nCrystals have no inherent failure mechanisms; some have operated in devices for decades. Failures may be, however, introduced by faults in bonding, leaky enclosures, corrosion, frequency shift by aging, breaking the crystal by too high mechanical shock, or radiation-induced damage when non-swept quartz is used. Crystals can be also damaged by overdriving.\nFrequency fluctuations.\nCrystals suffer from minor short-term frequency fluctuations as well. The main causes of such noise are e.g. thermal noise (which limits the noise floor), phonon scattering (influenced by lattice defects), adsorption/desorption of molecules on the surface of the crystal, noise of the oscillator circuits, mechanical shocks and vibrations, acceleration and orientation changes, temperature fluctuations, and relief of mechanical stresses. The short-term stability is measured by four main parameters: Allan variance (the most common one specified in oscillator data sheets), phase noise, spectral density of phase deviations, and spectral density of fractional frequency deviations. The effects of acceleration and vibration tend to dominate the other noise sources; surface acoustic wave devices tend to be more sensitive than bulk acoustic wave (BAW) ones, and the stress-compensated cuts are even less sensitive. The relative orientation of the acceleration vector to the crystal dramatically influences the crystal's vibration sensitivity. Mechanical vibration isolation mountings can be used for high-stability crystals.\nPhase noise plays a significant role in frequency synthesis systems using frequency multiplication; a multiplication of a frequency by N increases the phase noise power by N2. A frequency multiplication by 10 times multiplies the magnitude of the phase error by 10 times. This can be disastrous for systems employing PLL or FSK technologies.\nMagnetic fields have little effect on the crystal itself, as quartz is diamagnetic; eddy currents or AC voltages can however be induced into the circuits, and magnetic parts of the mounting and housing may be influenced.\nAfter the power-up, the crystals take several seconds to minutes to \"warm up\" and stabilize their frequency. The oven-controlled OCXOs require usually 3\u201310 minutes for heating up to reach thermal equilibrium; the oven-less oscillators stabilize in several seconds as the few milliwatts dissipated in the crystal cause a small but noticeable level of internal heating.\nDrive level.\nThe crystals have to be driven at the appropriate drive level. Low-frequency crystals, especially flexural-mode ones, may fracture at too high drive levels. The drive level is specified as the amount of power dissipated in the crystal. The appropriate drive levels are about 5 \u03bcW for flexural modes up to 100\u00a0kHz, 1 \u03bcW for fundamental modes at 1\u20134\u00a0MHz, 0.5 \u03bcW for fundamental modes 4\u201320\u00a0MHz and 0.5 \u03bcW for overtone modes at 20\u2013200\u00a0MHz. Too low drive level may cause problems with starting the oscillator. Low drive levels are better for higher stability and lower power consumption of the oscillator. Higher drive levels, in turn, reduce the impact of noise by increasing the signal-to-noise ratio.\nCrystal cuts.\nThe resonator plate can be cut from the source crystal in many different ways. The orientation of the cut influences the crystal's aging characteristics, frequency stability, thermal characteristics, and other parameters. These cuts operate at bulk acoustic wave (BAW); for higher frequencies, surface acoustic wave (SAW) devices are employed.\nhttps://\nThe letter \u2018T\u2019 in the cut name marks a temperature-compensated cut \u2013 a cut oriented in a way that the temperature coefficients of the lattice are minimal; the FC and SC\u00a0cuts are also temperature-compensated.\nThe high frequency cuts are mounted by their edges, usually on springs; the stiffness of the spring has to be optimal, as if it is too stiff, mechanical shocks could be transferred to the crystal and cause it to break, and too little stiffness may allow the crystal to collide with the inside of the package when subjected to a mechanical shock, and break. Strip resonators, usually AT\u00a0cuts, are smaller and therefore less sensitive to mechanical shocks. At the same frequency and overtone, the strip has less pullability, higher resistance, and higher temperature coefficient.\nThe low frequency cuts are mounted at the nodes where they are virtually motionless; thin wires are attached at such points on each side between the crystal and the leads. The large mass of the crystal suspended on the thin wires makes the assembly sensitive to mechanical shocks and vibrations.\nThe crystals are usually mounted in hermetically sealed glass or metal cases, filled with a dry and inert atmosphere, usually vacuum, nitrogen, or helium. Plastic housings can be used as well, but those are not hermetic and another secondary sealing has to be built around the crystal.\nSeveral resonator configurations are possible, in addition to the classical way of directly attaching leads to the crystal. E.g. the BVA resonator (Bo\u00eetier \u00e0 Vieillissement Am\u00e9lior\u00e9, Enclosure with Improved Aging), developed in 1976; the parts that influence the vibrations are machined from a single crystal (which reduces the mounting stress), and the electrodes are deposited not on the resonator itself but on the inner sides of two condenser discs made of adjacent slices of the quartz from the same bar, forming a three-layer sandwich with no stress between the electrodes and the vibrating element. The gap between the electrodes and the resonator act as two small series capacitors, making the crystal less sensitive to circuit influences. The architecture eliminates the effects of the surface contacts between the electrodes, the constraints in the mounting connections, and the issues related to ion migration from the electrodes into the lattice of the vibrating element. The resulting configuration is rugged, resistant to shock and vibration, resistant to acceleration and ionizing radiation, and has improved aging characteristics. AT\u00a0cut is usually used, though SC\u00a0cut variants exist as well. BVA resonators are often used in spacecraft applications.\nIn the 1930s to 1950s, it was fairly common for people to adjust the frequency of the crystals by manual grinding. The crystals were ground using a fine abrasive slurry, or even a toothpaste, to increase their frequency. A slight decrease by 1\u20132\u00a0kHz when the crystal was overground was possible by marking the crystal face with a pencil lead, at the cost of a lowered Q.\nThe frequency of the crystal is slightly adjustable (\"pullable\") by modifying the attached capacitances. A varactor, a diode with capacitance depending on applied voltage, is often used in voltage-controlled crystal oscillators, VCXO. The crystal cuts are usually AT or rarely SC, and operate in fundamental mode; the amount of available frequency deviation is inversely proportional to the square of the overtone number, so a third overtone has only one-ninth of the pullability of the fundamental mode. SC\u00a0cuts, while more stable, are significantly less pullable.\nCircuit notations and abbreviations.\nOn electrical schematic diagrams, \"crystals\" are designated with the class letter \"Y\" (Y1, Y2, etc.). Oscillators, whether they are crystal oscillators or others, are designated with the class letter \"G\" (G1, G2, etc.). Crystals may also be designated on a schematic with \"X\" or \"XTAL\" (a phonetic abbreviation, comparable to using \"Xmas\" for \"Christmas\"), or a crystal oscillator with XO.\nCrystal oscillator types and their abbreviations:\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "40980", "revid": "5066583", "url": "https://en.wikipedia.org/wiki?curid=40980", "title": "Curve-fitting compaction", "text": "Curve-fitting compaction is data compaction accomplished by replacing data to be stored or transmitted with an analytical expression.\nExamples of curve-fitting compaction consisting of discretization and then interpolation are:\nReferences.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "40981", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=40981", "title": "Customer office terminal", "text": "In telecommunications, the term customer office terminal has the following meanings:\n\"Note:\" An example of a customer office terminal is a stand-alone multiplexer located on the customer premises. \n\"Note:\" This function may be integrated into the ET.\nReferences.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "40982", "revid": "45807063", "url": "https://en.wikipedia.org/wiki?curid=40982", "title": "Customer-premises equipment", "text": "Terminal and associated equipment located at a subscriber's premises\nIn telecommunications, a customer-premises equipment or customer-provided equipment (CPE) is any terminal and associated equipment located at a subscriber's premises and connected with a carrier's telecommunication circuit at the demarcation point (\"demarc\"). The demarc is a point established in a building or complex to separate customer equipment from the equipment located in either the distribution infrastructure or central office of the communications service provider.\nCPE generally refers to devices such as telephones, routers, network switches, residential gateways (RG), set-top boxes, fixed mobile convergence products, home networking adapters and Internet access gateways that enable consumers to access providers' communication services and distribute them in a residence or enterprise with a local area network (LAN).\nA CPE can be an active equipment, as the ones mentioned above, or passive equipment such as analog telephone adapters (ATA) or xDSL-splitters. This includes key telephone systems and most private branch exchanges. Excluded from the CPE category are overvoltage protection equipment and pay telephones. Other types of materials that are necessary for the delivery of the telecommunication service, but are not defined as equipment, such as manuals and cable packages, and cable adapters are instead referred to as CPE-peripherals.\nCPE can refer to devices purchased by the subscriber, or to those provided by the operator or service provider.\nHistory.\nThe two phrases, \"customer-\"premises\" equipment\" and \"customer-\"provided\" equipment\", reflect the history of this equipment.\nUnder the Bell System monopoly in the United States (post Communications Act of 1934), the Bell System owned the telephones, and one could not attach privately owned or supplied devices to the network, or to the station apparatus. Telephones were located on customers' premises, hence, customer-\"premises\" equipment. In the U.S. Federal Communications Commission (FCC) proceeding the Second Computer Inquiry, the FCC ruled that telecommunications carriers could no longer bundle CPE with telecommunications service, uncoupling the market power of the telecommunications service monopoly from the CPE market, and creating a competitive CPE market.\nWith the gradual breakup of the Bell monopoly, starting with Hush-A-Phone v. United States [1956], which allowed some non-Bell owned equipment to be connected to the network (a process called interconnection), equipment on customers' premises became increasingly owned by customers. Indeed, subscribers were eventually permitted to purchase telephones \u2013 hence, customer-\"provided\" equipment.\nIn the pay-TV industry many operators and service providers offer subscribers a set-top box with which to receive video services, in return for a monthly fee. As offerings have evolved to include multiple services [voice and data] operators have increasingly given consumers the opportunity to rent or buy additional devices like access modems, internet gateways and video extenders that enable them to access multiple services, and distribute them to a range of consumer electronics devices in the home.\nTechnology evolution.\nHybrid devices.\nThe growth of multiple system operators, offering triple or quad-play services, required the development of hybrid CPE to make it easy for subscribers to access voice, video and data services. The development of this technology was led by Pay TV operators looking for a way to deliver video services via both traditional broadcast and broadband IP networks. Spain's Telefonica was the first operator to launch a hybrid broadcast and broadband TV service in 2003 with its Movistar TV DTT/IPTV offering, while Polish satellite operator 'n' was the first to offer its subscribers a Three-way hybrid (or Tri-brid) broadcast and broadband TV service, which launched in 2009\nSet-back box.\nThe term set-back box is used in the digital TV industry to describe a piece of consumer hardware that enables them to access both linear broadcast and internet-based video content, plus a range of interactive services like Electronic Programme Guides (EPG), Pay Per View (PPV) and video on demand (VOD) as well as internet browsing, and view them on a large screen television set. Unlike standard set-top boxes, which sit on top of or below the TV, a set-back box has a smaller form factor to enable it to be mounted to the rear of the display panel flat panel TV, hiding it from view.\nResidential gateway.\nA residential gateway is a networking device used to connect devices in the home to the Internet or other wide area network (WAN).\nIt is an umbrella term, used to cover multi-function networking appliances used in homes, which may combine a DSL modem or cable modem, a network switch, a consumer-grade router, and a wireless access point. In the past, such functions were provided by separate devices, but in recent years technological convergence has enabled multiple functions to be merged into a single device.\nOne of the first home gateway devices to be launched was selected by Telecom Italia to enable the operator to offer triple play services in 2002: Along with a SIP VoIP handset for making voice calls, it enabled subscribers to access voice, video and data services over a 10MB symmetrical ADSL fiber connection.\nVirtual gateway.\nThe virtual gateway concept enables consumers to access video and data services and distribute them around their homes using software rather than hardware. The first virtual gateway was introduced in 2010 by Advanced Digital Broadcast at the IBC exhibition in Amsterdam. The ADB Virtual Gateway uses software that resides within the middleware and is based on open standards, including DLNA home networking and the DTCP-IP standard, to ensure that all content, including paid-for encrypted content like Pay TV services, can only be accessed by secure CE devices.\nBroadband.\nA subscriber unit, or SU is a broadband radio that is installed at a business or residential location to connect to an access point to send/receive high speed data wired or wirelessly. Devices commonly referred to as a subscriber unit include cable modems, access gateways, home networking adapters and mobile phones. Example brands and vendors include SpeedTouch, DrayTek, Ubee Interactive, 2Wire and Efficient Networks.\nWAN.\nCPE may also refer to any devices that terminate a WAN circuit, such as an ISDN, E-carrier/T-carrier, DSL, or metro Ethernet. This includes any customer-owned hardware at the customer's site: routers, firewalls, network switches, PBXs, VoIP gateways, sometimes CSU/DSU and modems.\nApplication areas:"}
{"id": "40983", "revid": "1951353", "url": "https://en.wikipedia.org/wiki?curid=40983", "title": "Customer service unit", "text": "Telecommunications device\nIn telecommunications, a customer service unit (CSU) is a device that provides an accessing arrangement at a user location to either switched or point-to-point, data-conditioned circuits at a specifically established data signaling rate.\nA CSU provides local loop equalization, transient protection, isolation, and central office loop-back testing capability.\nReferences.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "40984", "revid": "569432", "url": "https://en.wikipedia.org/wiki?curid=40984", "title": "Custom local area signaling service", "text": ""}
{"id": "40985", "revid": "8978739", "url": "https://en.wikipedia.org/wiki?curid=40985", "title": "Cutback technique", "text": "In telecommunications, a cutback technique is a destructive technique for determining certain optical fiber transmission characteristics, such as attenuation and bandwidth.\nProcedure.\nThe measurement technique consists of:\nThe cut should be made to retain 1\u00a0meter or more of the fiber, in order to establish equilibrium mode distribution conditions for the second measurement. In a multimode fiber, the lack of an equilibrium mode distribution could introduce errors in the measurement due to output coupling effects. In a single-mode fiber, measuring a shorter cutback fiber could result in significant transmission of cladding modes (light carried in the cladding rather than the core of the optical fiber), distorting the measurement. The errors introduced will result in conservative results (\"i.e.\", higher transmission losses and lower bandwidths) than would be realized under equilibrium conditions.\nBenefits.\nThe benefit of this technique is that it allows measurement of the fiber characteristics without introducing errors due to variation in the launch conditions. For example, the coupling efficiency of the light source is kept consistent between the initial and the cutback measurements.\nSeveral characteristics may be determined using the same test fiber.\nAttenuation measurement.\nSince the attenuation is defined as proportional to the logarithm of the ratio between formula_1 and formula_2, where formula_3 is the power at point formula_4 and formula_5 respectively. Using the cutback technique, the power transmitted through a fiber of known length is measured and compared with the same measurement for the same fiber cut to a length of formula_6 approximately.\nRelated techniques.\nA variation of the cutback technique is the substitution method, in which measurements are made on a full-length of fiber, and then on a short length of fiber having the same characteristics (core size, numerical aperture), with the results from the short length being subtracted to give the results for the full length.\nReferences.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "40986", "revid": "7903804", "url": "https://en.wikipedia.org/wiki?curid=40986", "title": "Cutoff frequency", "text": "Frequency response boundary\nIn physics and electrical engineering, a cutoff frequency, corner frequency, or break frequency is a boundary in a system's frequency response at which energy flowing through the system begins to be reduced (attenuated or reflected) rather than passing through.\nTypically in electronic systems such as filters and communication channels, cutoff frequency applies to an edge in a lowpass, highpass, bandpass, or band-stop characteristic \u2013 a frequency characterizing a boundary between a passband and a stopband. It is sometimes taken to be the point in the filter response where a transition band and passband meet, for example, as defined by a half-power point (a frequency for which the output of the circuit is approximately \u22123.01\u00a0dB of the nominal passband value). Alternatively, a stopband corner frequency may be specified as a point where a transition band and a stopband meet: a frequency for which the attenuation is larger than the required stopband attenuation, which for example may be 30\u00a0dB or 100\u00a0dB.\nIn the case of a waveguide or an antenna, the cutoff frequencies correspond to the lower and upper cutoff wavelengths.\nElectronics.\nIn electronics, cutoff frequency or corner frequency is the frequency either above or below which the power output of a circuit, such as a line, amplifier, or electronic filter has fallen to a given proportion of the power in the passband. Most frequently this proportion is one half the passband power, also referred to as the 3\u00a0dB point since a fall of 3\u00a0dB corresponds approximately to half power. As a voltage ratio this is a fall to formula_1 of the passband voltage. Other ratios besides the 3\u00a0dB point may also be relevant, for example see below. Far from the cutoff frequency in the transition band, the rate of increase of attenuation (roll-off) with logarithm of frequency is asymptotic to a constant. For a first-order network, the roll-off is \u221220\u00a0dB per decade (approximately \u22126\u00a0dB per octave.)\nSingle-pole transfer function example.\nThe transfer function for the simplest low-pass filter,\nformula_2\nhas a single pole at \"s\" = \u22121/\"\u03b1\". The magnitude of this function in the \"j\u03c9\" plane is\nformula_3\nAt cutoff\nformula_4\nHence, the cutoff frequency is given by\nformula_5\nWhere s is the s-plane variable, \u03c9 is angular frequency and \"j\" is the imaginary unit.\nChebyshev filters.\nSometimes other ratios are more convenient than the 3\u00a0dB point. For instance, in the case of the Chebyshev filter it is usual to define the cutoff frequency as the point after the last peak in the frequency response at which the level has fallen to the design value of the passband ripple. The amount of ripple in this class of filter can be set by the designer to any desired value, hence the ratio used could be any value.\nRadio communications.\nIn radio communication, skywave communication is a technique in which radio waves are transmitted at an angle into the sky and reflected back to Earth by layers of charged particles in the ionosphere. In this context, the term \"cutoff frequency\" refers to the maximum usable frequency, the frequency above which a radio wave fails to reflect off the ionosphere at the incidence angle required for transmission between two specified points by reflection from the layer.\nWaveguides.\nThe cutoff frequency of an electromagnetic waveguide is the lowest frequency for which a mode will propagate in it. In fiber optics, it is more common to consider the cutoff wavelength, the maximum wavelength that will propagate in an optical fiber or waveguide. The cutoff frequency is found with the characteristic equation of the Helmholtz equation for electromagnetic waves, which is derived from the electromagnetic wave equation by setting the longitudinal wave number equal to zero and solving for the frequency. Thus, any exciting frequency lower than the cutoff frequency will attenuate, rather than propagate. The following derivation assumes lossless walls. The value of c, the speed of light, should be taken to be the group velocity of light in whatever material fills the waveguide.\nFor a rectangular waveguide, the cutoff frequency is\nformula_6\nwhere formula_7 are the mode numbers for the rectangle's sides of length formula_8 and formula_9 respectively. For TE modes, formula_10 (but formula_11 is not allowed), while for TM modes formula_12.\nThe cutoff frequency of the TM01 mode (next higher from dominant mode TE11) in a waveguide of circular cross-section (the transverse-magnetic mode with no angular dependence and lowest radial dependence) is given by \nformula_13\nwhere formula_14 is the radius of the waveguide, and formula_15 is the first root of formula_16, the Bessel function of the first kind of order 1.\nThe dominant mode TE11 cutoff frequency is given by\nformula_17\nHowever, the dominant mode cutoff frequency can be reduced by the introduction of baffle inside the circular cross-section waveguide. For a single-mode optical fiber, the cutoff wavelength is the wavelength at which the normalized frequency is approximately equal to 2.405.\nMathematical analysis.\nThe starting point is the wave equation (which is derived from the Maxwell equations),\nformula_18\nwhich becomes a Helmholtz equation by considering only functions of the form \nformula_19\nSubstituting and evaluating the time derivative gives\nformula_20\nThe function formula_21 here refers to whichever field (the electric field or the magnetic field) has no vector component in the longitudinal direction - the \"transverse\" field. It is a property of all the eigenmodes of the electromagnetic waveguide that at least one of the two fields is transverse. The \"z\" axis is defined to be along the axis of the waveguide.\nThe \"longitudinal\" derivative in the Laplacian can further be reduced by considering only functions of the form \nformula_22\nwhere formula_23 is the longitudinal wavenumber, resulting in\nformula_24\nwhere subscript T indicates a 2-dimensional transverse Laplacian. The final step depends on the geometry of the waveguide. The easiest geometry to solve is the rectangular waveguide. In that case, the remainder of the Laplacian can be evaluated to its characteristic equation by considering solutions of the form \nformula_25\nThus for the rectangular guide the Laplacian is evaluated, and we arrive at\nformula_26\nThe transverse wavenumbers can be specified from the standing wave boundary conditions for a rectangular geometry cross-section with dimensions a and b:\nformula_27\nformula_28\nwhere n and m are the two integers representing a specific eigenmode. Performing the final substitution, we obtain\nformula_29\nwhich is the dispersion relation in the rectangular waveguide. The cutoff frequency formula_30 is the critical frequency between propagation and attenuation, which corresponds to the frequency at which the longitudinal wavenumber formula_31 is zero. It is given by\nformula_32\nThe wave equations are also valid below the cutoff frequency, where the longitudinal wave number is imaginary. In this case, the field decays exponentially along the waveguide axis and the wave is thus evanescent.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt; "}
{"id": "40987", "revid": "252195", "url": "https://en.wikipedia.org/wiki?curid=40987", "title": "Cutoff wavelength", "text": ""}
{"id": "40989", "revid": "122189", "url": "https://en.wikipedia.org/wiki?curid=40989", "title": "Data access arrangement", "text": "Concept in circuitry networks\nThe term data access arrangement (DAA) has the following meanings: \nData access arrangements are an integral part of all modems built for the public telephone network. In view of mixed voice and data access, DAAs are more generally referred to as direct access arrangements.\nRequirement for DAAs.\nWhile DAA now describes an integral component of a device that connects to the telephone network, during the 60s and 70s it described a separate device mandated by the Bell System, connected between the telephone line and non-Bell equipment, typically a modem.\nFollowing the Carterfone decision, which required Bell to allow customers to attach any non-harmful equipment to their network, Bell mandated that subscribers use PCAs/DAAs - purchased exclusively from Western Electric - to ensure the network was protected. These devices were not required for Bell-provided equipment, only equipment made by independent manufacturers.\nAt the time, some subscribers believed that the DAA was a scheme by AT&amp;T to penalize and discourage use of non-Bell modems and recover lost profits from hardware sales, and the FCC began investigations into the legality of the practice. Subscribers also became frustrated when Bell failed to deliver DAAs in a timely fashion after the ruling, leading to the use of unauthorized third-party DAAs.\nThere were two main varieties of DAA described by AT&amp;T: manual and automatic. A manual DAA required a call to be initiated (or answered) as normal, at which point it could then be connected to the third-party device, while an automatic DAA allowed an attached device to be connected without human intervention, important for receiving modem use.\nIn 1975, the FCC implemented Part 68 of the FCC Rules, which granted permission for direct connection of any equipment to the telephone network given compliance with specific electrical requirements. This technically eliminated the need for DAAs, although the first modem that didn't require a separate DAA was not marketed until 1977 when a court ruled that Part 68 was legal.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "40990", "revid": "36384157", "url": "https://en.wikipedia.org/wiki?curid=40990", "title": "Data bank", "text": ""}
{"id": "40991", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=40991", "title": "Data compaction", "text": "In telecommunications, data compaction is the reduction of the number of data elements, bandwidth, cost, and time for the generation, transmission, and storage of data without loss of information by eliminating unnecessary redundancy, removing irrelevancy, or using special coding. \nExamples of data compaction methods are the use of fixed-tolerance bands, variable-tolerance bands, slope-keypoints, sample changes, curve patterns, curve fitting, variable-precision coding, frequency analysis, and probability analysis. \nSimply squeezing noncompacted data into a smaller space, for example by increasing packing density by transferring images from newsprint to microfilm or by transferring data on punched cards onto magnetic tape, is not data compaction.\nEveryday examples.\nThe use of acronyms in texting is an everyday example. The number of bits required to transmit and store \"WYSIWYG\" (What You See Is What You Get) is reduced from its expanded equivalent (7 characters vs 28). The representation of Mersenne primes is another example. The largest known as of \u00a02013[ [update]] is over 17 million digits long but it is represented as \"M\"57885161 in a much more compacted form.\nReferences.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "40992", "revid": "42342156", "url": "https://en.wikipedia.org/wiki?curid=40992", "title": "Data element", "text": "Semantic representation of data\nIn metadata, the term data element is an atomic unit of data that has precise meaning or precise semantics. \nData elements usage can be discovered by inspection of software applications or application data files through a process of manual or automated Application Discovery and Understanding. Once data elements are discovered they can be registered in a metadata registry.\nIn the areas of databases and data systems more generally a data element is a concept forming part of a data model. As an element of data representation, a collection of data elements forms a data structure.\nProperties.\nA data element has:\nName.\nA data element name is a name given to a data element in, for example, a data dictionary or metadata registry. In a formal data dictionary, there is often a requirement that no two data elements may have the same name, to allow the data element name to become an identifier, though some data dictionaries may provide ways to qualify the name in some way, for example by the application system or other context in which it occurs.\nIn a database driven data dictionary, the fully qualified data element name may become the primary key, or an alternate key, of a Data Elements table of the data dictionary.\nThe data element name typically conforms to ISO/IEC 11179 metadata registry naming conventions and has at least three parts:\nMany standards require the use of Upper camel case to differentiate the components of a data element name. This is the standard used by ebXML, GJXDM and NIEM.\nExample of ISO/IEC 11179 name in XML.\nUsers frequently encounter ISO/IEC 11179 when they are exposed to XML Data Element names that have a multi-part Camel Case format:\nObject [Qualifier] Property RepresentationTerm\nThe specification also includes normative documentation in appendices.\nFor example, the XML element for a person's given (first) name would be expressed as:\n&lt;PersonGivenName&gt;John&lt;/PersonGivenName&gt;\nWhere Person is the Object=Person, Property=Given and Representation term=\"Name\". In this case the optional qualifier is not used, in spite of being implicit in the data element name. This requires knowledge based on data element name, rather than use of structured data.\nDefinition.\nIn metadata, a data element definition is a human readable phrase or sentence associated with a data element within a data dictionary that describes the meaning or semantics of a data element.\nData element definitions are critical for external users of any data system. Good definitions can dramatically ease the process of mapping one set of data into another set of data. This is a core feature of distributed computing and intelligent agent development.\nThere are several guidelines that should be followed when creating high-quality data element definitions.\nProperties of clear definitions.\nA good definition is:\nDefinitions should not refer to terms or concepts that might be misinterpreted by others or that have different meanings based on the context of a situation. Definitions should not contain acronyms that are not clearly defined or linked to other precise definitions.\nIf one is creating a large number of data elements, all the definitions should be consistent with related concepts.\nCritical Data Element \u2013 Not all data elements are of equal importance or value to an organization. A key metadata property of an element is categorizing the data as a Critical Data Element (CDE). This categorization provides focus for data governance and data quality. An organization often has various sub-categories of CDEs, based on use of the data. e.g.: \nStandards such as the ISO/IEC 11179 Metadata Registry specification give guidelines for creating precise data element definitions. Specifically chapter four of the ISO/IEC 11179 metadata registry standard.\nCommon words such as play or run database documents over 57 different distinct meanings for the word \"play\" but only a single definition for the term dramatic play. Fewer definitions in a chosen word's dictionary entry is preferable. This minimizes misinterpretation related to a reader's context and background. The process of finding a good meaning of a word is called Word-sense disambiguation\nExamples of definitions that could be improved.\nHere is the definition of \"person\" data element as defined in the www.w3c.org Friend of a Friend specification http://:\nPerson: A person.\nAlthough most people do have an intuitive understanding of what a person is, the definition has much room for improvement. The first problem is that the definition is circular. Note that this definition really does not help most readers and needs to be clarified.\nHere is the definition of the \"Person\" Data Element in the Global Justice XML Data Model 3.0 https://:\nperson: Describes inherent and frequently associated characteristics of a person.\nNote that once again the definition is still circular. Person should not reference itself. The definition should use terms other than person to describe what a person is.\nHere is a more precise but shorter definition of a person:\nPerson: An individual human being.\nNote that it uses the word \"individual\" to state that this is an instance of a class of things called human being. Technically you might use \"homo sapiens\" in your definition, but more people are familiar with the term \"human being\" than \"homo sapiens,\" so commonly used terms, if they are still precise, are always preferred.\nSometimes your system may have cultural norms and assumptions in the definitions. For example, if your \"Person\" data element tracked characters in a science fiction series that included aliens you may need a more general term other than \"human being\".\nPerson: An individual of a sentient species.\nIn telecommunications.\nIn telecommunications, the term data element has the following components:\nIn practice.\nIn practice, data elements (fields, columns, attributes, etc.) are sometimes \"overloaded\", meaning a given data element will have multiple potential meanings. While a known bad practice, overloading is nevertheless a very real factor or barrier to understanding what a system is doing.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "40993", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=40993", "title": "Data forwarder", "text": "Type of telecommunications of data routing equipment\nIn telecommunications, a data forwarder or data forwarding device is a device that receives data from one data link and retransmits data representing the same information, using proper format and link protocols, to another data link. In the packet networks, this act of forwarding is referred to as packet forwarding and is performed by network switches or network routers.\nFor example, in the tactical communications a data forwarder may forward data between: \nMore than one source notes that routers and bridges can function as data forwarding devices. Another source suggests, in fact, that gateways are a \"special type of data-forwarding device\".\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nMost of the content arises from: &lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "40994", "revid": "28481209", "url": "https://en.wikipedia.org/wiki?curid=40994", "title": "Datagram", "text": "Basic data transfer unit associated with a packet-switched network\nA datagram is a basic transfer unit associated with a packet-switched network. Datagrams are typically structured in header and payload sections. Datagrams provide a connectionless communication service across a packet-switched network. The delivery, arrival time, and order of arrival of datagrams need not be guaranteed by the network.\nHistory.\nIn the early 1970s, the term \"datagram\" was created by combining the words \"data\" and \"telegram\" by the CCITT rapporteur on packet switching, Halvor Bothner-By. While the word was new, the concept had already a long history.\nIn 1964, Paul Baran described, in a RAND Corporation report, a hypothetical military network having to resist a nuclear attack. Small standardized \"message blocks\", bearing source and destination addresses, were stored and forwarded in computer nodes of a highly redundant meshed computer network. Baran wrote: \"The network user who has called up a \"virtual connection\" to an end station and has transmitted messages ... might also view the system as a black box providing an apparent circuit connection\". The concept of what we now call a virtual circuit appears in the design, although no network was built.\nIn 1967, Donald Davies published a seminal article in which he introduced the \"packet\" and \"packet switching\". His proposed core network is similar to the one proposed by Paul Baran though developed independently. He assumes that \"all users of the network will provide themselves with some kind of error control\". His target is a \"common-carrier communication network\". To support remote access to computer services by user terminals, which at that time were transmitted character by character, he included, at the network periphery, interface computers that convert character flows into packet flows and vice versa. Davies wrote: \"we were really rather against the virtual circuit, because we believed that a communication network should only concern itself with packets, and that any protocols involved in assembling these packets should be done end-to-end, between the customers themselves.\"\nIn 1970, Lawrence Roberts and Barry D. Wessler published an article about ARPANET, the first multi-node packet-switching network. An accompanying paper described its switching nodes (the IMPs) and its packet formats. The network core performed datagram switching as in Baran's and Davies' model, but the service offered to hosts by the network was connection oriented. A reliable message transfer service was thus offered to user computers, thus greatly simplifying the network design. This made the ARPANET what would come to be called a virtual circuit network.\nRoberts presented the idea of packet switching to the communication professionals and faced anger and hostility. Before ARPANET was operating, they argued that the router buffers would quickly run out. After the ARPANET was operating, they argued packet switching would never be economic without the government subsidy. Baran faced the same rejection and thus failed to convince the military to construct a packet-switching network.\nIn 1973, Louis Pouzin presented his design for CYCLADES, the first large-scale network implementing the pure Davies datagram model. The CYCLADES team has thus been the first to tackle the highly complex problem of providing user applications a reliable virtual circuit service while using the end-to-end principle in a network service known to possibly produce non-negligible datagram losses and reordering. Although Pouzin's concern \"in a first stage is not to make breakthrough [sic] in packet switching technology, but to build a reliable communications tool for Cyclades\", two members of his team, Hubert Zimmerman and G\u00e9rard Le Lann, made significant contributions to the design of Internet's TCP that Vint Cerf, its main designer, acknowledged.\nIn 1981, the Defense Advanced Research Projects Agency (DARPA) issued the first specification the Internet Protocol (IP). It introduced a major evolution of the datagram concept: \"fragmentation.\" With fragmentation, some parts of the global network may use large packet size (typically local area networks to minimize processing overhead), while some others may impose smaller packet sizes (typically wide area networks to minimize response time). Network nodes may fragment a datagram into several smaller packets.\nIn 1999, the Internet Engineering Task Force (IETF) sanctioned the use of the already largely deployed network address translation (NAT) whereby each public address can be shared by several private devices. With it, the forthcoming Internet Address exhaustion was delayed, leaving enough time to introduce IPv6, the new generation of Internet Protocol supporting longer addresses. The initial principle of full end to end network transparency to datagrams was for this relaxed: NAT nodes had to manage per-connection states, making them in part connection oriented.\nIn 2015, the IETF upgraded its \"informational\" 1998 RFC 2309 that datagram switching nodes perform active queue management, to make it a stronger and more detailed \"best current practice\" recommendation through the publication of RFC 7567. While the initial datagram queueing model was simple to implement and needed no more tuning than queue lengths, support of more sophisticated and parametrized mechanisms were found necessary \"to improve and preserve Internet performance\" (RED, ECN etc.). Further research on the subject was also called for, with a list of identified items.\nDefinition.\nThe term \"datagram\" is defined as follows:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;\"A self-contained, independent entity of data carrying sufficient information to be routed from the source to the destination computer without reliance on earlier exchanges between this source and destination computer and the transporting network.\"\u2014\u200a\nA datagram needs to be self-contained without reliance on earlier exchanges because there is no connection of fixed duration between the two communicating points as there is, for example, in most voice telephone conversations.\nDatagram service is often compared to a mail delivery service; the user only provides the destination address but receives no guarantee of delivery, and no confirmation upon successful receipt. Datagram service is therefore considered unreliable. Datagram service routes datagrams without first creating a predetermined path. Datagram service is therefore considered connectionless. There is also no consideration given to the order in which it and other datagrams are sent or received. In fact, many datagrams in the same group can travel along different paths before reaching the same destination in a different order.\nStructure.\nEach datagram has two components, a header and a data payload. The header contains all the information sufficient for routing from the originating equipment to the destination without relying on prior exchanges between the equipment and the network. Headers may include source and destination addresses as well as type and length fields. The payload is the data to be transported. This process of nesting data payloads in a tagged header is called encapsulation.\nExamples.\nInternet Protocol.\nThe Internet Protocol (IP) defines standards for several types of datagrams. The internet layer is a datagram service provided by an IP. For example, UDP is run by a datagram service on the internet layer. IP is an entirely connectionless, best effort, unreliable, message delivery service. TCP is a higher-level protocol running on top of IP that provides a reliable connection-oriented service.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "40995", "revid": "7611264", "url": "https://en.wikipedia.org/wiki?curid=40995", "title": "Data integrity", "text": "Maintenance of data over its entire life-cycle\nData integrity is the maintenance of, and the assurance of, data accuracy and consistency over its entire life-cycle. It is a critical aspect to the design, implementation, and usage of any system that stores, processes, or retrieves data. The term is broad in scope and may have widely different meanings depending on the specific context even under the same general umbrella of computing. It is at times used as a proxy term for data quality, while data validation is a prerequisite for data integrity.\nDefinition.\nData integrity is the opposite of data corruption. The overall intent of any data integrity technique is the same: ensure data is recorded exactly as intended (such as a database correctly rejecting mutually exclusive possibilities). Moreover, upon later retrieval, ensure the data is the same as when it was originally recorded. In short, data integrity aims to prevent unintentional changes to information. Data integrity is not to be confused with data security, the discipline of protecting data from unauthorized parties.\nAny unintended changes to data as the result of a storage, retrieval or processing operation, including malicious intent, unexpected hardware failure, and human error, is failure of data integrity. If the changes are the result of unauthorized access, it may also be a failure of data security. Depending on the data involved this could manifest itself as benign as a single pixel in an image appearing a different color than was originally recorded, to the loss of vacation pictures or a business-critical database, to even catastrophic loss of human life in a life-critical system.\nIntegrity types.\nPhysical integrity.\nPhysical integrity deals with challenges which are associated with correctly storing and fetching the data itself. Challenges with physical integrity may include electromechanical faults, design flaws, material fatigue, corrosion, power outages, natural disasters, and other special environmental hazards such as ionizing radiation, extreme temperatures, pressures and g-forces. Ensuring physical integrity includes methods such as redundant hardware, an uninterruptible power supply, certain types of RAID arrays, radiation hardened chips, error-correcting memory, use of a clustered file system, using file systems that employ block level checksums such as ZFS, storage arrays that compute parity calculations such as exclusive or or use a cryptographic hash function and even having a watchdog timer on critical subsystems.\nPhysical integrity often makes extensive use of error detecting algorithms known as error-correcting codes. Human-induced data integrity errors are often detected through the use of simpler checks and algorithms, such as the Damm algorithm or Luhn algorithm. These are used to maintain data integrity after manual transcription from one computer system to another by a human intermediary (e.g. credit card or bank routing numbers). Computer-induced transcription errors can be detected through hash functions.\nIn production systems, these techniques are used together to ensure various degrees of data integrity. For example, a computer file system may be configured on a fault-tolerant RAID array, but might not provide block-level checksums to detect and prevent silent data corruption. As another example, a database management system might be compliant with the ACID properties, but the RAID controller or hard disk drive's internal write cache might not be.\nLogical integrity.\nThis type of integrity is concerned with the correctness or rationality of a piece of data, given a particular context. This includes topics such as referential integrity and entity integrity in a relational database or correctly ignoring impossible sensor data in robotic systems. These concerns involve ensuring that the data \"makes sense\" given its environment. Challenges include software bugs, design flaws, and human errors. Common methods of ensuring logical integrity include things such as check constraints, foreign key constraints, program assertions, and other run-time sanity checks.\nPhysical and logical integrity often share many challenges such as human errors and design flaws, and both must appropriately deal with concurrent requests to record and retrieve data, the latter of which is entirely a subject on its own.\nIf a data sector only has a logical error, it can be reused by overwriting it with new data. In case of a physical error, the affected data sector is permanently unusable.\nDatabases.\nData integrity contains guidelines for data retention, specifying or guaranteeing the length of time data can be retained in a particular database (typically a relational database). To achieve data integrity, these rules are consistently and routinely applied to all data entering the system, and any relaxation of enforcement could cause errors in the data. Implementing checks on the data as close as possible to the source of input (such as human data entry), causes less erroneous data to enter the system. Strict enforcement of data integrity rules results in lower error rates, and time saved troubleshooting and tracing erroneous data and the errors it causes to algorithms.\nData integrity also includes rules defining the relations a piece of data can have to other pieces of data, such as a \"Customer\" record being allowed to link to purchased \"Products\", but not to unrelated data such as \"Corporate Assets\". Data integrity often includes checks and correction for invalid data, based on a fixed schema or a predefined set of rules. An example being textual data entered where a date-time value is required. Rules for data derivation are also applicable, specifying how a data value is derived based on algorithm, contributors and conditions. It also specifies the conditions on how the data value could be re-derived.\nTypes of integrity constraints.\nData integrity is normally enforced in a database system by a series of integrity constraints or rules. Three types of integrity constraints are an inherent part of the relational data model: entity integrity, referential integrity and domain integrity.\nIf a database supports these features, it is the responsibility of the database to ensure data integrity as well as the consistency model for the data storage and retrieval. If a database does not support these features, it is the responsibility of the applications to ensure data integrity while the database supports the consistency model for the data storage and retrieval.\nHaving a single, well-controlled, and well-defined data-integrity system increases:\nModern databases support these features (see Comparison of relational database management systems), and it has become the de facto responsibility of the database to ensure data integrity. Companies, and indeed many database systems, offer products and services to migrate legacy systems to modern databases.\nExamples.\nAn example of a data-integrity mechanism is the parent-and-child relationship of related records. If a parent record owns one or more related child records all of the referential integrity processes are handled by the database itself, which automatically ensures the accuracy and integrity of the data so that no child record can exist without a parent (also called being orphaned) and that no parent loses their child records. It also ensures that no parent record can be deleted while the parent record owns any child records. All of this is handled at the database level and does not require coding integrity checks into each application.\nFile systems.\nVarious research results show that neither widespread filesystems (including UFS, Ext, XFS, JFS and NTFS) nor hardware RAID solutions provide sufficient protection against data integrity problems.\nSome filesystems (including Btrfs and ZFS) provide internal data and metadata checksumming that is used for detecting silent data corruption and improving data integrity. If a corruption is detected that way and internal RAID mechanisms provided by those filesystems are also used, such filesystems can additionally reconstruct corrupted data in a transparent way. This approach allows improved data integrity protection covering the entire data paths, which is usually known as end-to-end data protection.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "40996", "revid": "3632083", "url": "https://en.wikipedia.org/wiki?curid=40996", "title": "Data link", "text": "Means of connecting one location to another in order to transmit information between them\nA data link is a means of connecting one location to another for the purpose of transmitting and receiving digital information (data communication). It can also refer to a set of electronics assemblies, consisting of a transmitter and a receiver (two pieces of data terminal equipment) and the interconnecting data telecommunication circuit. These are governed by a link protocol enabling digital data to be transferred from a data source to a data sink.\nTypes.\nThere are at least three types of basic data-link configurations that can be conceived of and used:\nAviation.\nIn civil aviation, a data-link system (known as Controller Pilot Data Link Communications) is used to send information between aircraft and air traffic controllers for example when an aircraft is too far from the ATC to make voice radio communication and radar observations possible. Such systems are used for aircraft crossing the Atlantic, Pacific and Indian oceans. One such system, used by Nav Canada and NATS over the North Atlantic, uses a five-digit data link sequence number confirmed between air traffic control and the pilots of the aircraft before the aircraft proceeds to cross the ocean. This system uses the aircraft's flight management computer to send location, speed and altitude information about the aircraft to the ATC. ATC can then send messages to the aircraft regarding any necessary change of course. \nIn unmanned aircraft, land vehicles, boats, and spacecraft, a two-way (full-duplex or half-duplex) data-link is used to send control signals, and to receive telemetry."}
{"id": "40997", "revid": "45807063", "url": "https://en.wikipedia.org/wiki?curid=40997", "title": "Data service unit", "text": "A data service unit (DSU), sometimes called a digital service unit, is a piece of telecommunications circuit terminating equipment that transforms digital data between telephone company lines and local equipment. The device converts bipolar digital signals coming ultimately from a digital circuit and directly from a Channel service unit (CSU), into a format (e.g. RS- 530) compatible with the piece of data terminal equipment (DTE) (e.g. a router) to which the data is sent. The DSU also performs a similar process in reverse for data heading from the DTE toward the circuit. The telecommunications service a DSU supports can be a point-to-point or multipoint operation in a digital data network.\nForm and purpose.\nA DSU is a two or more port device; one port is called the WAN (wide area network) port and the other is called a DTE port. The purpose of the DSU is to transfer serial data synchronously between the WAN port and the DTE ports. If more than one DTE port is used, the DSU assigns the DTE data according to time slots (channels) on the WAN side.\nOn the WAN side, the DSU, via a CSU, interfaces with a digital carrier such as DS1 or DS3 or a low speed Digital Data Service. On the DTE side, the DSU provides control lines, timing lines and appropriate physical and electrical interface. To maintain the synchronous relationship between the ports, the DSU manages timing by slaving ports to the bit rate of another or to its internal clock. Typically, the DTE port provides timing to the data terminal equipment while the WAN port dictates the rate.\nDSUs usually include some maintenance capabilities. At minimum, they can loop data back at either the WAN or DTE ports, or at both. When only one port is looped back, the data received at that port is simultaneously sent back toward the port and passed in normal fashion to the other port. Most DSUs also allow various data patterns to be generated and monitored to measure error rate of the communication link. A DSU may be a separate piece of equipment, or may be combined in a CSU/DSU.\nReferences.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "40998", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=40998", "title": "Data signaling rate", "text": "Rate in telecommunications\nIn telecommunications, data signaling rate (DSR), also known as gross bit rate, is the aggregate rate at which data passes a point in the transmission path of a data transmission system.\nMaximum rate.\nThe \"maximum user signaling rate\", synonymous to gross bit rate or data signaling rate, is the maximum rate, in bits per second, at which binary information can be transferred in a given direction between users over the communications system facilities dedicated to a particular information transfer transaction, under conditions of continuous transmission and no overhead information.\nFor a single channel, the signaling rate is given by formula_2, where \"SCSR\" is the single-channel signaling rate in bits per second, \"T\" is the minimum time interval in seconds for which each level must be maintained, and n is the number of significant conditions of modulation of the channel.\nIn the case where an individual end-to-end telecommunications service is provided by parallel channels, the parallel-channel signaling rate is given by formula_3, where \"PCSR\" is the total signaling rate for \"m\" channels, \"m\" is the number of parallel channels, \"Ti\" is the minimum interval between significant instants for the \"I\"-th channel, and \"ni\" is the number of significant conditions of modulation for the \"I\"-th channel.\nIn the case where an end-to-end telecommunications service is provided by tandem channels, the end-to-end signaling rate is the lowest signaling rate among the component channels."}
{"id": "40999", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=40999", "title": "Data transmission circuit", "text": ""}
{"id": "41000", "revid": "1310059784", "url": "https://en.wikipedia.org/wiki?curid=41000", "title": "Date-time group", "text": "Set of characters used to express date and time\nIn communications messages, a date-time group (DTG) is a set of characters, usually in a prescribed format, used to express the year, the month, the day of the month, the hour of the day, the minute of the hour, and the time zone, if different from Coordinated Universal Time (UTC). The order in which these elements are presented may vary. The DTG is usually placed in the header of the message. One example is \" (UTC)\"; while another example is \"\".\nThe DTG may indicate either the date and time a message was dispatched by a transmitting station or the date and time it was handed into a transmission facility by a user or originator for dispatch.\nThe DTG may be used as a message identifier if it is unique for each message.\nUS military date-time group.\nA form of DTG is used in the US military's Defense Message System (a form of Automated Message Handling System). In US military messages and communications (e.g., on maps showing troop movements) the format is \"DD HHMM (SS)\" Z\" MON YY\". Although occasionally seen with spaces, it can also be written as a single string of characters. Three different formats can be found:\nZ references the military identifier of time zone:\nExamples.\nExample 1: 051100Z represents the 5th day of the current month 11:00 (UTC).\nExample 2: 091630Tjul11 represents 9 July 2011 4:30 pm (MST).\nExample 3: represents the current time of refresh: (UTC).\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nExternal links.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41001", "revid": "44024615", "url": "https://en.wikipedia.org/wiki?curid=41001", "title": "DB (car)", "text": "Deutsch-Bonnet (DB), is a brand of sports cars created in 1937 by Charles Deutsch and Ren\u00e9 Bonnet and disappeared in 1962.\nThe D.B Coup\u00e9s, in racing or customer versions, were intensively involved in rallying and on international circuits: 24 Hours of Le Mans, 12 Hours of Sebring, 12 Hours of Reims, TdF Automobile (notably with Prince Rainier of Monaco at the wheel, who made the brand famous).\nHistory.\nDB (until 1947 known as Deutsch-Bonnet) was a French automobile maker between 1938 and 1961, based in Champigny-sur-Marne near Paris. The firm was founded by Charles Deutsch and Ren\u00e9 Bonnet, an offshoot of the Deutsch family's existing coachbuilding shop which had been taken over by Bonnet in 1932. Immediately before the war the partners concentrated on making light-weight racing cars, but a few years after the war, starting with the presentation of a Panhard based cabriolet at the 1950 Paris Motor Show, the company also began to produce small road-going sports cars. By 1952 the company no longer had its own stand at the Paris Motor Show, but one of their cars appeared as a star attraction on the large Panhard stand, reflecting the level of cooperation between the two businesses.\nThe company was defunct by 1961, as Deutsch and Bonnet's differing design philosophies hamstrung further cooperation. The number of DB's built is not certain; estimates of up to 2,000 cars are mentioned but more conservative numbers are closer to one thousand.\nLight-weight engineering.\nThe business produced light sports cars, originally in steel or aluminium but subsequently with fibreglass bodies mainly powered by Panhard flat-twin engines, most commonly of 610, 744, or 848\u00a0cc. Deutsch was a \"theoretical engineer who had a natural instinct for aerodynamics,\" while Bonnet was a more \"pragmatic mechanical engineer\".\nThe fibreglass bodies covered a tubular central beam chassis made from steel, with front wheel drive and four wheel independent suspension directly lifted from the Panhard donors. Until 1952 all DBs had been intended for competition purposes only.\nRacing origins.\nBonnet had been promised a works drive in an Amilcar P\u00e9gase in the 1936 French Grand Prix for sports cars, but when this failed to materialise they set about building their own racer. The 1938 alloy-bodied DB1 roadster was a special, built using the remains of a Citro\u00ebn Traction Avant 11CV. The construction took seventeen months. A series of numbered successors followed. The close-roofed 1.5-litre DB2's career was hindered by the war and was sold later, without Deutsch ever using it. The DB3 was a monocoque project developed during the war, but was never built, as the improved pontoon-bodied DB4 took preference. With a central beam chassis with a forked cradle for the 1.5 litre Traction 7A-based engine (originally intended for the DB2) it was finished in July 1945, with most of the work having been carried out in secret during the occupation. The very similar 2-litre DB5 was finished soon thereafter. Their two specials both placed in the first postwar race in France, in Paris in 1945, being the only post-war cars entered. An open-wheeled DB7 appeared in 1947 (preceded by the heavy and large DB6 which saw very little action), after which the \"Automobiles Deutsch &amp; Bonnet\" was officially formed.\nNeither single-seater DB was at all successful, but they did show Deutsch - who had hitherto preferred dependable standard units - that a tuned engine would become necessary. DB thus moved into the performance parts market, developing and offering a four-speed conversion for Citro\u00ebns and an overhead camshaft head - developed with the aid of engine specialists Maurice Sainturat and Dante Giacosa. The DB8 appeared in 1948, and won two \"concours d'\u00e9legances\" before partaking in any competitions. Their early cars were all built using Citro\u00ebn parts, but supply was troublesome and DB soon moved on to using Panhard technology. This relationship came about as Deutsch was an officer of independent racer's club AGACI. When this organization decided to begin a \"Mouvement Racer 500\", modelled on the British Formula 3, Deutsch offered club members the design of a racing car using a Panhard 500 engine. One member asked to have DB build such a car, and after it made a star appearance at the 1949 Paris Salon Panhard was happy to support the construction of about fifteen more. The formula expired in 1951, with the DB Panhard 500 never competitive abroad.\nDB was very active in competition, especially in Le Mans 24 Hours and other long distance racing. Nearly all DBs, even the road cars, were designed with competition foremost in mind. In 1952, a DB Speedster was entered in the 12 Hours of Sebring and won its class handsomely, beginning its career in the United States market. Steve Lansing and Ward Morehouse were the drivers.\nAt the 1954 Le Mans DB entered five cars and were also involved with Panhards \"Monopole\" racers. Ren\u00e9 Bonnet himself, together with racing legend \u00c9lie Bayol, finished tenth overall and best of the DBs. The other Panhard-engined also finished (in 16th), while three Renault-engined central-seater DB designs all failed to complete the race. The Renault-engined designs had been created as a concession to pressure from DB's customers, but they did very badly in the race, in part because of a shortage of preparation time for what was an unknown entity to Deutsch and Bonnet. In either case, DB proceeded to focus on Panhard designs exclusively.\nRoad cars.\nThe 1949 DB8 was bodied by Antem of Belgium and shown at the 1949 Paris Salon. While a handsome (winning two concours d'\u00e9legances) and modern design, Citro\u00ebn refused to allow the provision of parts for series production. After DB began to depend on Panhard for engines, Antem was again commissioned to make a cabriolet with the intent to build a small series of street cars. long, the car weighed and used the Dyna's 750\u00a0cc flat-two and much of the suspension and drivetrain. As with most DBs, it had a central frame with two outliers. An 850\u00a0cc version was also offered, a model which could reach 140 rather than the 125\u2013130\u00a0km/h of the smaller one. Naturally, Panhard developed a racing \"barquette\" version (called the Tank) of the Antem cabriolet. These competed at Le Mans 1951 as well as several other races. About twenty Antem cabriolets were built in 1951, but DB chose to let it die in favor of a coup\u00e9 version of the same (\"Coach\" in French). A few DB-Antem Coach were built, mostly for competition. These had bodywork designed by Deutsch, and again mainly relied on Dyna underpinnings and a central steel-tube frame.\nThe steel-bodied, Frua-designed 1952 \"Mille Miles\" (celebrating class victories at the Mille Miglia) was a mini-GT with a 65\u00a0hp Panhard two-cylinder. It was somewhat expensive, and at the 1953 Paris Salon a Chausson-designed DB Coach in fibreglass, although it did not enter production until 1954. The HBR 4/5 model (1954\u20131959) was the partners' most successful project to date, with several hundred of the little cars produced between 1954 and 1959. This was followed by the Le Mans convertible and hardtop, which was shown in 1959 and built by DB until 1962, and continued until 1964 by Ren\u00e9 Bonnet. About 660 of the Mille Miles/Coach/HBR were built, and 232 DB Le Mans (not including the Bonnet-built cars). Later versions could be equipped with engines of 1 and 1.3\u00a0litres, and superchargers were also available. No two cars may have been alike, as they were built according to customer specifications from a wide range of options.\nMore racing success.\nDeutsch's very efficient and influential aerodynamic designs allowed DB race cars to reach impressive top speeds despite the small Panhard flat-twin engine. DB's received class victories at Le Mans (three times), Sebring (twice), and Mille Miglia (four times). DB even managed an outright win in the handicapped 1954 Tourist Trophy sports car race, with Laureau and Armagnac driving. DB always showed strongly in the \"Index of Performance\", a category especially suitable for DB's small-engined, aerodynamic little racers. The Index of Performance is perhaps best known at the Le Mans 24 hours competition, but the category also existed at many French automobile races of the era, such as the Tour de France. DBs were also successful in American SCCA racing, where they racked up an impressive number of victories in the H-sports category.\nDisagreement and the end of the partnership.\nDeutsch and Bonnet disagreed whether they should build cars of front-wheel drive or mid-engined design. There was also disagreement on which engines to use. Charles Deutsch, wanting to stick to Panhard engines, left DB in 1961 to found his own firm (CD). Bonnet founded \"Automobiles Ren\u00e9 Bonnet\", producing mid-engined cars equipped with Renault power units: this business was later to become part of Matra Automobiles. Deutsch ended up an engineering consultant.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41002", "revid": "27015025", "url": "https://en.wikipedia.org/wiki?curid=41002", "title": "Weighting filter", "text": "Filter used to emphasize or suppress aspects in measurements\nA weighting filter is used to emphasize or suppress some aspects of a phenomenon compared to others, for measurement or other purposes.\nAudio applications.\nIn each field of audio measurement, special units are used to indicate a weighted measurement as opposed to a basic physical measurement of energy level. For sound, the unit is the phon (1\u00a0kHz equivalent level).\nSound.\nSound has three basic components, the wavelength, frequency, and speed. In sound measurement, we measure the loudness of the sound in decibels (dB). Decibels are logarithmic with 0\u00a0dB as the reference. There are also a range of frequencies that sounds can have. Frequency is the number of times a sine wave repeats itself in a second. Normal auditory systems can usually hear between 20 and 20,000\u00a0Hz. When we measure sound, the measurement instrument takes the incoming auditory signal and analyzes it for these different features. Weighting filters in these instruments then filter out certain frequencies and decibel levels depending on the filter. A weighted filters are most similar to natural human hearing. This allows the sound level meter to determine what decibel level the incoming sound would likely be for a normal hearing human's auditory system.\nLoudness measurements.\nIn the measurement of loudness, for example, an A-weighting filter is commonly used to emphasize frequencies around 3\u20136\u00a0kHz where the human ear is most sensitive, while attenuating very high and very low frequencies to which the ear is insensitive. The aim is to ensure that measured loudness corresponds well with subjectively perceived loudness.\nA-weighting is only really valid for relatively quiet sounds and for pure tones as it is based on the 40-phon Fletcher\u2013Munson equal-loudness contour. The B and C curves were intended for louder sounds (though they are less used) while the D curve is used in assessing loud aircraft noise (IEC 537). B curves filter out more medium loudness levels when compared to an A curves. This curve is rarely ever used in the assessment or monitoring of noise levels anymore. C curves differ from both A and B in the fact that they filter less of the lower and higher frequencies. The filter is a much flatter shape and is used in sound measurement in especially loud and noisy environments. A weighted curves follow a 40 phon curve while C weighted follows a 100 phon curve. The three curves differ not in their measurement of exposure levels, but in the frequencies measured. A weighted curves allow more frequencies equal to or less than 500\u00a0Hz through, which is most representative of the human ear.\nLoudness measurements with weighting filters.\nThere are a variety of reasons for measuring sound. This includes following regulations to protect worker's hearing, following noise ordinances, in telecommunications, and many more. At the basis of sound measurement is the idea of breaking down an incoming signal based on its different properties. Every incoming sinusoidal wave of sound has a frequency and amplitude. Using this information, a sound level can be deduced from the root-sums-of-squares of the amplitudes of all the incoming auditory information. Whether using a sound level meter or a noise dosimeter, the processing is somewhat similar. With a calibrated sound level meter, the incoming sounds are going to be picked up by the microphone and then measured by the internal electronic circuits. The sound measurement that the device outputs can be filtered through an A, B, or C weighting curve. The curve used will have slight effects on the resulting decibel level.\nTelecommunications.\nIn the field of telecommunications, weighting filters are widely used in the measurement of electrical noise on telephone circuits, and in the assessment of noise as perceived through the acoustic response of different types of instrument (handset). Other noise-weighting curves have existed, e.g. DIN standards. The term \"psophometric weighting\", though referring in principle to any weighting curve intended for noise measurement, is often used to refer to a particular weighting curve, used in telephony for narrow-bandwidth voiceband speech circuits.\nEnvironmental noise measurement.\nA-weighted decibels are abbreviated dB(A) or dBA. When acoustic (calibrated microphone) measurements are being referred to, then the units used will be dB SPL (sound pressure level) referenced to 20 micropascals = 0\u00a0dB SPL.\nThe A-weighting curve has been widely adopted for environmental noise measurement, and is standard in many sound level meters (see ITU-R 468 weighting for a further explanation).\nA-weighting is also in common use for assessing potential hearing damage caused by loud noise, though this seems to be based on the widespread availability of sound level meters incorporating A-Weighting rather than on any good experimental evidence to suggest that such use is valid. The distance of the measuring microphone from a sound source is often \"forgotten\", when SPL measurements are quoted, making the data useless. In the case of environmental or aircraft noise, distance need not be quoted as it is the level at the point of measurement that is needed, but when measuring refrigerators and similar appliances the distance should be stated; where not stated it is usually one metre (1 m). An extra complication here is the effect of a reverberant room, and so noise measurement on appliances should state \"at 1 m in an open field\" or \"at 1 m in anechoic chamber\". Measurements made outdoors will approximate well to anechoic conditions.\nA-weighted SPL measurements of noise level are increasingly to be found on sales literature for domestic appliances such as refrigerators and freezers, and computer fans. Although the threshold of hearing is typically around 0\u00a0dB SPL, this is in fact very quiet indeed, and appliances are more likely to have noise levels of 30 to 40\u00a0dB SPL.\nAudio reproduction and broadcasting equipment.\nHuman sensitivity to noise in the region of 6\u00a0kHz became particularly apparent in the late 1960s with the introduction of compact cassette recorders and Dolby-B noise reduction. A-weighted noise measurements were found to give misleading results because they did not give sufficient prominence to the 6\u00a0kHz region where the noise reduction was having greatest effect, and sometimes one piece of equipment would measure worse than another and yet sound better, because of differing spectral content.\nITU-R 468 noise weighting was therefore developed to more accurately reflect the subjective loudness of all types of noise, as opposed to tones. This curve, which came out of work done by the BBC Research Department, and was standardised by the CCIR and later adopted by many other standards bodies (IEC, BSI/) and, as of 2006[ [update]], is maintained by the ITU. Noise measurements using this weighting typically also use a quasi-peak detector law rather than slow averaging. This also helps to quantify the audibility of bursty noise, ticks and pops that might go undetected with a slow rms measurement.\nITU-R 468 noise weighting with quasi-peak detection is widely used in Europe, especially in telecommunications, and in broadcasting particularly after it was adopted by the Dolby corporation who realised its superior validity for their purposes. It is commonly used by broadcasters in Britain, Europe, and former countries of the British Empire such as Australia and South Africa.\nThough the noise level of 16-bit audio systems (such as CD players) is commonly quoted (on the basis of calculations that take no account of subjective effect) as \u221296\u00a0dB relative to FS (full scale), the best 468-weighted results are in the region of \u221268\u00a0dB relative to Alignment Level (commonly defined as 18\u00a0dB below FS) i.e. \u221286\u00a0dB relative to FS.\nOther applications of weighting.\nIn the measurement of gamma rays or other ionising radiation, a radiation monitor or dosimeter will commonly use a filter to attenuate those energy levels or wavelengths that cause the least damage to the human body, while letting through those that do the most damage, so that any source of radiation may be measured in terms of its true danger rather than just its 'strength'. The sievert is a unit of weighted radiation dose for ionising radiation, which supersedes the older unit the REM (roentgen equivalent man).\nWeighting is also applied to the measurement of sunlight when assessing the risk of skin damage through sunburn, since different wavelengths have different biological effects. Common examples are the SPF of sunscreen, and the UV index.\nAnother use of weighting is in television, where the red, green and blue components of the signal are weighted according to their perceived brightness. This ensures compatibility with black and white receivers, and also benefits noise performance and allows separation into meaningful luminance and chrominance signals for transmission.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41003", "revid": "12331483", "url": "https://en.wikipedia.org/wiki?curid=41003", "title": "DBm", "text": "Power level referenced to one milliwatt\ndBm or dBmW (decibel-milliwatts) is a unit of power level expressed using a logarithmic decibel (dB) scale respective to one milliwatt (mW). It is commonly used by radio, microwave and fiber-optical communication technicians &amp; engineers to measure the power of system transmissions on a log scale, which can express both very large and very small values in a short form. dBW is a similar unit measured relative to one watt (1000\u00a0mW) rather than a milliwatt.\nThe decibel (dB) is a dimensionless unit, used for quantifying the ratio between two values, such as signal-to-noise ratio. The dBm is also dimensionless, but since it compares to a fixed reference value, the dBm quantity is an absolute one.\nThe dBm is not a part of the International System of Units (SI) and therefore is discouraged from use in documents or systems that adhere to SI units. (The corresponding SI unit is the watt.) However, the unit decibel (dB) for relative quantities, without any suffix, is a non-SI unit that is accepted for use alongside SI units. The level of a power \"P\" of ten decibels relative to one milliwatt may be written \"L\"\"P\"/(1\u00a0mW) = 10\u00a0dB to comply with the SI.\nIn audio and telephony, dBm is typically referenced relative to the 600-ohm impedance commonly used in telephone voice networks, while in radio-frequency work dBm is typically referenced relative to a 50-ohm impedance.\nUnit conversions.\nA power level of 0\u00a0dBm corresponds to a power of 1 milliwatt. An increase in level of 10\u00a0dB is equivalent to a ten-fold increase in power. Therefore, a 20\u00a0dB increase in level is equivalent to a 100-fold increase in power. A 3\u00a0dB increase in level is approximately equivalent to doubling the power, which means that a level of 3\u00a0dBm corresponds roughly to a power of 2\u00a0mW. Similarly, for each 3\u00a0dB decrease in level, the power is reduced by about one half, making \u22123\u00a0dBm correspond to a power of about 0.5\u00a0mW.\nTo express an arbitrary power P in mW as x in dBm, the following expression may be used:\nformula_1\nConversely, to express an arbitrary power level x in dBm, as P in mW:\nformula_2\nTable of examples.\nBelow is a table summarizing useful cases:\nStandards.\nThe signal intensity (power per unit area) can be converted to received signal power by multiplying by the square of the wavelength and dividing by 4\u03c0 (see Free-space path loss).\nIn United States Department of Defense practice, unweighted measurement is normally understood, applicable to a certain bandwidth, which must be stated or implied.\nIn European practice, psophometric weighting may be, as indicated by context, equivalent to dBm0p, which is preferred.\nIn audio, 0\u00a0dBm often corresponds to approximately 0.775 volts, since 0.775\u00a0V dissipates 1\u00a0mW in a 600\u00a0\u03a9 load. The corresponding voltage level is 0 dBu, without the 600\u00a0\u03a9 restriction. Conversely, for RF situations with a 50\u00a0\u03a9 load, 0\u00a0dBm corresponds to approximately 0.224 volts, since 0.224\u00a0V dissipates 1\u00a0mW in a 50\u00a0\u03a9 load.\nIn general the relationship between the level of a power P in dBm and the RMS voltage V in volts across a load of resistance R (typically used to terminate a transmission line with impedance Z) is:\nformula_3\nExpression in dBm is typically used for optical and electrical power measurements, not for other types of power (such as thermal). A listing by power levels in watts is available that includes a variety of examples not necessarily related to electrical or optical power.\nThe dBm was first proposed as an industry standard in 1940.\nReferences.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41004", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41004", "title": "DBrn", "text": "The symbol dBrn or dB(rn) is an abbreviation for decibels above reference noise. \nWeighted noise power in dB is referred to 1.0 picowatt. Thus, 0 dBrn = -90 dBm. Use of 144 line, 144-receiver, or C-message weighting, or flat weighting, can be indicated in parentheses.\nWith C-message weighting, a one-milliwatt, 1000 Hz tone will read +90 dBrn, but the same power as white noise, randomly distributed over a 3 kHz band will read approximately +88.5 dBrn, because of the frequency weighting. \nWith 144 weightings, a one milliwatt, 1000 Hz white noise tone will also read +90 dBrn, but the same 3 kHz power will only read +82 dBrn, because of the different frequency weighting.\nNote: In telecommunications, \"dBrn adjusted\" also called dBa denotes \"decibels adjusted\", i.e. weighted absolute noise power. This is totally unrelated to and not a synonym for dB(A).\nReferences.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41005", "revid": "44721228", "url": "https://en.wikipedia.org/wiki?curid=41005", "title": "Data circuit-terminating equipment", "text": "Communications system component\nA data circuit-terminating equipment (DCE) is a device that sits between the data terminal equipment (DTE) and a data transmission circuit. It is also called data communication(s) equipment and data carrier equipment. Usually, the DTE device is the terminal (or computer), and the DCE is a modem.\nIn a \"data station\", the DCE performs functions such as signal conversion, coding, and line clocking and may be a part of the DTE or intermediate equipment. Interfacing equipment may be required to couple the DTE into a transmission circuit or channel and from a transmission circuit or channel into the DTE.\nUsage.\nAlthough the terms are most commonly used with RS-232, several data communication standards define different types of interfaces between a DCE and a DTE. The DCE is a device that communicates with a DTE device in these standards. Standards that use this nomenclature include:\nA general rule is that DCE devices provide the clock signal (internal clocking) and the DTE device synchronizes on the provided clock (external clocking). D-sub connectors follow another rule for pin assignment. DTE devices usually transmit on pin connector number 2 and receive on pin connector number 3. DCE devices are just the opposite: pin connector number 2 receives and pin connector number 3 transmits the signals.\nWhen two devices, that are both DTE or both DCE, must be connected together without a modem or a similar media translator between them, a crossover cable must be used, e.g. a null modem for RS-232 or an Ethernet crossover cable.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41006", "revid": "11292982", "url": "https://en.wikipedia.org/wiki?curid=41006", "title": "Decollimation", "text": ""}
{"id": "41007", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=41007", "title": "Decrypt", "text": ""}
{"id": "41008", "revid": "3091149", "url": "https://en.wikipedia.org/wiki?curid=41008", "title": "Degradation", "text": "Degradation may refer to:\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "41009", "revid": "892079", "url": "https://en.wikipedia.org/wiki?curid=41009", "title": "Degree of isochronous distortion", "text": "Measure of distortion in a data transmission\nThe degree of isochronous distortion, in data transmission, is the ratio of the absolute value of the maximum measured difference between the actual and the theoretical intervals separating any two significant instants of modulation (or demodulation), to the unit interval. These instants are not necessarily consecutive. This value is usually expressed as a percentage.\nThe result of the measurement should be qualified by an indication if the period, usually limited, of the observation. For a prolonged modulation (or demodulation), it will be appropriate to consider the probability that an assigned value of the degree of distortion will be exceeded.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41010", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41010", "title": "Degree of start-stop distortion", "text": "In telecommunications, the term degree of start-stop distortion has the following meanings: \nThe degree of distortion of a start-stop modulation (or demodulation) is usually expressed as a percentage. Distinction can be made between the degree of late (positive) distortion and the degree of early (negative) distortion.\nReferences.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41011", "revid": "629503", "url": "https://en.wikipedia.org/wiki?curid=41011", "title": "Dejitterizer", "text": ""}
{"id": "41012", "revid": "3525933", "url": "https://en.wikipedia.org/wiki?curid=41012", "title": "Delay", "text": "Delay or DeLay may refer to:\n&lt;templatestyles src=\"Template:TOC_right/styles.css\" /&gt;\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "41013", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41013", "title": "Delay encoding", "text": ""}
{"id": "41014", "revid": "1157097511", "url": "https://en.wikipedia.org/wiki?curid=41014", "title": "Delay line", "text": "Delay line may refer to:\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "41015", "revid": "18731412", "url": "https://en.wikipedia.org/wiki?curid=41015", "title": "Delta modulation", "text": "Signal conversion technique\nDelta modulation (DM, \u0394M, or \u0394-modulation) is an analog-to-digital and digital-to-analog signal conversion technique used for transmission of voice information where quality is not of primary importance. DM is the simplest form of differential pulse-code modulation (DPCM) where the difference between successive samples is encoded into n-bit data streams. In delta modulation, the transmitted data are reduced to a 1-bit data stream representing either up (\u2197) or down (\u2198). Its main features are:\nTo achieve high signal-to-noise ratio, delta modulation must use oversampling techniques, that is, the analog signal is sampled at a rate several times higher than the Nyquist rate.\nDerived forms of delta modulation are continuously variable slope delta modulation, delta-sigma modulation, and differential modulation. Differential pulse-code modulation is the superset of DM.\nPrinciple.\nRather than quantizing the value of the input analog waveform, delta modulation quantizes the difference between the input signal and the integral of all previous quantization steps. This quantized signal effectively represents the derivative of the input signal, so the original signal is recovered by integration, as shown in the block diagram in Fig.\u00a02:\nIn its simplest form, the quantizer can be realized with a comparator referenced to 0 (a two-level quantizer), whose output is \"1\" or \"-1\" depending on whether the quantizer's input is positive or negative. The demodulator contains an integrator (just like the one in the feedback loop) whose output rises or falls with each 1 or -1 received. An optional low-pass filter will remove high frequency zigzags (see the blue output signal of Fig.\u00a01), so only frequencies in the band of interest remain, to recover a smooth cleaned version of the original signal.\nBecause each sample is only 1\u00a0bit, the transmission bit rate equals the sampling rate.\nTransfer characteristics.\nThe two sources of noise in delta modulation are \"slope overload\", when step size is too small to track the original waveform, and \"granularity\", when step size is too large. But a 1971 study shows that slope overload is less objectionable compared to granularity than one might expect based solely on SNR measures.\nSlope overload.\nIn delta modulation, there is no limit to the number of pulses of the same sign that may occur, so it is capable of tracking signals of any amplitude without clipping provided that the signal doesn't change too rapidly. However, if an input signal formula_1 has a derivative formula_2 larger thanformula_3,where formula_4 is the sampling frequency and formula_5 is the quantization step size, then the signal changes too fast, causing slope overload. For example, if the input signal is a cosine wave with frequency formula_6 and amplitude formula_7,\nformula_8,\nthen its derivative,\nformula_9,\ncan be as large as\nformula_10.\nThus, slope overload won't occur for a sinusoidal input if\nformula_11.\nConsequently, a sinusoidal signal can be transmitted without slope overload if its amplitude is not bigger than\nformula_12.\nA real input signal may be more complex than a single sinusoid, but this example illustrates how a transmitted signal may be attenuated depending on the sampling frequency, step size, and the input signal's frequency.\nWhile slope overload (also referred to as slope clipping) can be avoided by increasing the quantum step size or sampling rate, very high sampling rates, typically 20 times the highest frequency of interest, are required to achieve the same quality as pulse-code modulation (PCM).\nInability to transmit DC.\nBecause the modulated signal contains only the derivative of the input, any DC and low-frequency content of the signal is lost (which may be ok for voice and other applications which do not have low frequencies), transmission errors are accumulated, and high-frequency noise is amplified. An improvement to DM called delta-sigma modulation avoids these downsides by rearranging the integrator's position so that the modulated signal represents the amplitude of the input signal instead of just its derivative.\nHistory.\nThe seminal paper combining feedback with oversampling to achieve delta modulation was by F.\u00a0de\u00a0Jager of Philips Research Laboratories in 1952. Initial patents include:\nAsynchronous delta modulation.\nThe 1947 Deloraine, 1950 Cutler, and 1952 Jager designs were synchronous (or time-quantized). Delta modulation is also possible without a fixed sampling rate. A February 1966 paper by H. Inose \"Asynchronous delta-modulation system\" uses Schmitt triggers to detect when the input signal exceeds the local demodulator by a predetermined difference, with the benefit of reducing the number of output pulses. A November 1973 paper \"Signal Coding Using Asynchronous Delta Modulation\" (presented in 1974) investigates an algorithm that varies the sampling rate to transmit fewer samples during periods of small signal variation.\nAdaptive delta modulation.\nAdaptive delta modulation (ADM) was first published by Dr. John E. Abate (Bell Labs Fellow) in his doctoral thesis at NJ Institute Of Technology in 1968. ADM was later selected as the standard for all NASA communications between mission control and space-craft.\nIn the mid-1980s, Massachusetts audio company DBX marketed a commercially unsuccessful digital recording system based on adaptive delta modulation. See DBX 700.\nAdaptive delta modulation or Continuously variable slope delta modulation (CVSD) is a modification of DM in which the step size is not fixed. Rather, when several consecutive bits have the same direction value, the encoder and decoder assume that slope overload is occurring, and the step size becomes progressively larger.\nOtherwise, the step size becomes gradually smaller over time. ADM reduces slope error, at the expense of increasing quantization error. This error can be reduced by using a low-pass filter. ADM provides robust performance in the presence of bit errors meaning error detection and correction are not typically used in an ADM radio design, it is this very useful technique that allows for adaptive-delta-modulation.\nApplications.\nVideo game sound effects.\nThe Nintendo Entertainment System's audio processing unit (the Ricoh 2A03 chip) includes a Delta Modulation Channel (DMC) to demodulate percussion and sound effects. The DMC reads delta-encoded audio data via direct memory access into a shift register, which gets shifted out serially into an up/down counter acting as the demodulator's integrator. Because the shift register is clocked by a configurable timer, the audio's frequency can be shifted by adjusting the playback speed. The counter's value is outputted though a 7-bit digital-to-analog converter (DAC). Note: writing PCM samples directly to the counter bypasses the DM demodulation to instead provide low-bit PCM output.\nSatellite Business Systems 24 kbps delta modulation.\nDelta modulation was used by Satellite Business Systems (SBS) for its voice ports to provide long distance phone service to large domestic corporations with a significant inter-corporation communications need (such as IBM). Each traffic channel had a 32\u00a0kbit/s bitrate. This system was in service throughout the 1980s. The voice ports used digitally implemented 24\u00a0kbit/s delta modulation with Voice Activity Compression (VAC) and echo suppressors to control the half second echo path through the satellite. They performed formal listening tests to verify the 24\u00a0kbit/s delta modulator achieved full voice quality with no discernible degradation as compared to a high quality phone line or the standard 64\u00a0kbit/s \u03bc-law companded PCM. This provided an eight to three improvement in satellite channel capacity. IBM developed the Satellite Communications Controller and the voice port functions.\nThe original proposal in 1974, used a state-of-the-art 24\u00a0kbit/s delta modulator with a single integrator and a \"Shindler Compander\" modified for gain error recovery. This proved to have less than full phone line speech quality. In 1977, one engineer with two assistants in the IBM Research Triangle Park, NC laboratory was assigned to improve the quality.\nThe final implementation replaced the integrator with a predictor implemented with a two pole complex pair low-pass filter designed to approximate the long term average speech spectrum. The theory was that ideally the integrator should be a predictor designed to match the signal spectrum. A nearly perfect Shindler Compander replaced the modified version. It was found the modified compander resulted in a less than perfect step size at most signal levels and the fast gain error recovery increased the noise as determined by actual listening tests as compared to simple signal to noise measurements. The final compander achieved a very mild gain error recovery due to the natural truncation rounding error caused by twelve bit arithmetic.\nThe complete function of delta modulation, VAC and Echo Control for six ports was implemented in a single digital integrated circuit chip with twelve bit arithmetic. A single digital-to-analog converter (DAC) was shared by all six ports providing voltage compare functions for the modulators and feeding sample and hold circuits for the demodulator outputs. A single card held the chip, DAC and all the analog circuits for the phone line interface including transformers.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nSources.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41016", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41016", "title": "Demand assignment", "text": "In telecommunications, a demand assignment is a method which several users share access to a communication channel on a real-time basis, \"i.e.\", a user needing to communicate with another user on the same network requests the required circuit, uses it, and when the call is finished, the circuit is released, making the circuit available to other users. \nDemand assignment is similar to conventional telephone switching, in which common trunks are provided for many users, on a demand basis, through a limited-size trunk group.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "41017", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41017", "title": "Demand factor", "text": "Refers to the fractional amount of some quantity\nIn telecommunications, electronics and the electrical power industry, the term demand factor is used to refer to the fractional amount of some quantity being used relative to the maximum amount that could be used by the same system. The demand factor is always less than or equal to one. As the amount of demand is a time dependent quantity so is the demand factor.\n formula_1\nThe demand factor is often implicitly averaged over time when the time period of demand is understood by the context.\nElectrical engineering.\nIn electrical engineering the demand factor is taken as a time independent quantity where the numerator is taken as the maximum demand in the specified time period instead of the averaged or instantaneous demand.\n formula_2\nThis is the peak in the load profile divided by the full load of the device.\nExample:\nIf a residence has equipment which could draw 6,000 W when all equipment was drawing a full load, drew a maximum of 3,000 W in a specified time, then the demand factor = 3,000 W / 6,000 W = 0.5\nThis quantity is relevant when trying to establish the amount of load for which a system should be rated. In the above example, it would be unlikely that the system would be rated to 6,000 W, even though there may be a slight possibility that this amount of power can be drawn. This is closely related to the load factor which is the average load divided by the peak load in a specified time period.\n formula_3"}
{"id": "41018", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41018", "title": "Demand load", "text": "In telecommunications, the term demand load can have the following meanings: \nReferences.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41019", "revid": "36510957", "url": "https://en.wikipedia.org/wiki?curid=41019", "title": "Desensitation", "text": ""}
{"id": "41020", "revid": "3488865", "url": "https://en.wikipedia.org/wiki?curid=41020", "title": "Design objective", "text": "In communications systems, a design objective (DO) is a desired performance characteristic for communications circuits and equipment that is based on engineering analyses, but (a) is not considered feasible to mandate in a standard, or (b) has not been tested. \nDOs are used because applicable systems standards are not in existence. \nExamples of reasons for designating a performance characteristic as a DO rather than as a standard are (a) it may be bordering on an advancement in the state of the art, (b) the requirement may not have been fully confirmed by measurement or experience with operating circuits, and (c) it may not have been demonstrated that the requirement can be met considering other constraints, such as cost and size. \nA DO is sometimes established in a standard for developmental consideration. A DO may also specify a performance characteristic used in the preparation of specifications for development or procurement of new equipment or systems.\nDesign is the process of formulation of a plan for satisfaction of human needs\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41021", "revid": "7304691", "url": "https://en.wikipedia.org/wiki?curid=41021", "title": "Detector (disambiguation)", "text": "A detector is a device capable of registering a specific substance or physical phenomenon.\nDetector may also refer to:\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "41022", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41022", "title": "Deterministic routing", "text": "In telecommunications, deterministic routing is the advance determination of the routes between given pairs of nodes. Examples:\nNotes and references.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41023", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=41023", "title": "D4 framing standard", "text": ""}
{"id": "41024", "revid": "49715992", "url": "https://en.wikipedia.org/wiki?curid=41024", "title": "Pulse dialing", "text": "Telephone address signaling protocol\nPulse dialing is a signaling technology in telecommunications in which a direct current local loop circuit is interrupted according to a defined coding system for each signal transmitted, usually a digit. This lends the method the often used name loop disconnect dialing. In the most common variant of pulse dialing, decadic dialing, each of the ten Arabic numerals are encoded in a sequence of up to ten pulses. The most common version decodes the digits 1 through 9, as one to nine pulses, respectively, and the digit 0 as ten pulses. Historically, the most common device to produce such pulse trains is the rotary dial of the telephone, lending the technology another name, rotary dialing.\nThe pulse repetition rate was historically determined based on the response time needed for electromechanical switching systems to operate reliably. Most telephone systems used the nominal rate of ten pulses per second, but operator dialing within and between central offices often used pulse rates up to twenty per second.\nEarly automatic exchanges.\nAutomatic telephone exchange systems were developed in the late 19th and early 20th century. For identification, telephone subscribers were assigned a telephone number unique to each circuit. Various methods evolved to signal the desired destination telephone number for a telephone call directly dialed by the subscriber. An automatic switch-hook was designed by Hilborne Roosevelt.\nThe first commercial automatic telephone exchange, designed by Almon Brown Strowger, opened in La Porte, Indiana on 3 November 1892, and used two telegraph-type keys on the telephone, which had to be operated the correct number of times to control the vertical and horizontal relay magnets in the exchange. But the use of separate keys with separate conductors to the exchange was not practical. The most common signaling system became a system of using direct-current pulse trains generated in the telephone sets of subscribers by interrupting the single-pair wire loop of the telephone circuit.\nRotary dial.\nThe first rotary dials worked by creating current interruptions on two separate direct current circuits (three-wire system) to actuate stepping motors in a vertical and horizontal (rotational) switch arrangement. Operating the dial error-free required smooth rotary motion of the finger wheel by the user, but was found as too unreliable. Later dials after c. 1907 produced just a single pulse train on a 2-wire circuit as the user rotated the dial to the finger stop starting at a different position for each digit transmitted. This mechanism was refined to include a recoil spring and a centrifugal governor to control the recoil speed. The user selected a digit to be dialed by inserting a finger into the corresponding hole and rotated the dial to the finger stop. When released from this position, the dial pulsing contacts were opened and closed repeatedly, thus interrupting the loop current in a pattern on the return to the home position. The exchange switch decoded the pattern for each digit thus transmitted by stepping relays or by accumulation in digit registers.\nPulse rate and coding.\nThe mechanical nature of the stepping switches or relays employed at the switching system generally limited the speed of operation, the pulsing rate, to ten pulses per second.\nThe specifications of the Bell System in the US required service personnel to adjust dials in customer stations to a precision of 9.5 to 10.5 pulses per second (PPS), but the tolerance of the switching equipment was generally between 8 and 11 PPS. The British (GPO, later Post Office Telecommunications) standard for Strowger switch exchanges has been ten impulses per second (allowable range 7 to 12) and a 66% break ratio (allowable range 63% to 72%).\nIn most switching systems one pulse is used for the digit 1, two pulses for 2, and so on, with ten pulses for the digit 0; this makes the code unary, excepting the digit 0. Exceptions to this are Sweden, with one pulse for 0, two pulses for 1, and so on, and New Zealand, with ten pulses for 0, nine pulses for 1, etc. Oslo, the capital city of Norway, used the New Zealand system, but the rest of the country did not. Systems that used this encoding of the ten digits in a sequence of up to ten pulses are known as \"decadic dialing\" systems.\nSome switching systems used digit registers that doubled the allowable pulse rate up to twenty pulses per second, and the inter-digital pause could be reduced as the switch selection did not have to be completed during the pause. These included access lines to the panel switch in the 1920s, crossbar systems, the later version (7A2) of the rotary system, and the earlier 1970s stored program control exchanges.\nIn some telephones, the pulses may be heard in the receiver as clicking sounds. However, in general, such effects were undesirable and telephone designers suppressed them with off-normal switches on the dial to exclude the receiver from the circuit, or greatly attenuated them by electrical means with a varistor connected across the receiver.\nSuccessors.\nIt was recognized as early as the 1910s that push buttons were faster than rotary dials, when the first panel switches used keys for dialing at the operator stations. In the 1940s, Bell Laboratories conducted field trials of pushbutton telephones for customer dialing to determine accuracy and efficiency. However, the technology of using mechanical reed relays was too unreliable until transistors transformed the industry. In 1963, the Bell System introduced to the public dual-tone multi-frequency (DTMF) technology under the name Touch-Tone, which was a trademark in the U.S. until 1984. The Touch-Tone system used push-button telephones. In the decades after 1963, rotary dials were gradually phased out on new telephone models in favor of keypads and the primary dialing method to the central office became touchtone dialing. Most central office systems still support rotary telephones today. Some keypad telephones have a switch or configuration method for the selection of tone or pulse dialing.\nMobile telephones and most voice-over-IP systems use out-of-band signaling and do not send any digits until the entire number has been keyed by the user. Many VoIP systems are based on the Session Initiation Protocol (SIP), which uses a form of Uniform Resource Identifiers (URI) for addressing, instead of digits alone.\nSwitch-hook dialing.\nAs pulse dialing is achieved by interruption of the local loop, it was possible to dial digits by rapidly tapping, i.e. depressing, the switch hook the corresponding number of times at approximately ten taps per second. However, many telephone makers implemented a slow switch hook release to prevent rapid switching.\nIn the United Kingdom, it was once possible to make calls from coin-box phones by tapping the switch hook without depositing coins. Unlawfully obtaining a free telephone call was deemed a criminal offence, abstracting electricity from the General Post Office, which operated the telephone system, and several cases were prosecuted.\nIn popular culture, tapping was shown in the film \"Red Dragon\" when prisoner Hannibal Lecter dialed out on a telephone without dialing mechanism. The technique was also used by the character Phantom Phreak in the film \"Hackers\".\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41025", "revid": "24207291", "url": "https://en.wikipedia.org/wiki?curid=41025", "title": "Dial-up", "text": ""}
{"id": "41026", "revid": "2051880", "url": "https://en.wikipedia.org/wiki?curid=41026", "title": "Dielectric", "text": "Electrically insulating substance able to be polarised by an applied electric field\nIn electromagnetism, a dielectric (or dielectric medium) is an electrical insulator that can be polarised by an applied electric field. When a dielectric material is placed in an electric field, electric charges do not flow through the material as they do in an electrical conductor, because they have no loosely bound, or free, electrons that may drift through the material, but instead they shift, only slightly, from their average equilibrium positions, causing dielectric polarisation. Because of dielectric polarisation, positive charges are displaced in the direction of the field and negative charges shift in the direction opposite to the field. This creates an internal electric field that reduces the overall field within the dielectric itself. If a dielectric is composed of weakly bonded molecules, those molecules not only become polarised, but also reorient so that their symmetry axes align to the field.\nThe study of dielectric properties concerns storage and dissipation of electric and magnetic energy in materials. Dielectrics are important for explaining various phenomena in electronics, optics, solid-state physics and cell biophysics.\nTerminology.\nAlthough the term \"insulator\" implies low electrical conduction, \"dielectric\" typically means materials with a high polarisability. The latter is expressed by a number called the relative permittivity. \"Insulator\" is generally used to indicate electrical obstruction while \"dielectric\" is used to indicate the energy storing capacity of the material (by means of polarisation). A common example of a dielectric is the electrically insulating material between the metallic plates of a capacitor. The polarisation of the dielectric by the applied electric field increases the capacitor's surface charge for the given electric field strength.\nThe term \"dielectric\" was coined by William Whewell (from \"dia\" + \"electric\") in response to a request from Michael Faraday.\nA \"perfect dielectric\" is a material with zero electrical conductivity (cf. perfect conductor infinite electrical conductivity), thus exhibiting only a displacement current; therefore it stores and returns electrical energy as if it were an ideal capacitor.\nElectric susceptibility.\nThe electric susceptibility formula_1 of a dielectric material is a measure of how easily it polarises in response to an electric field. This, in turn, determines the electric permittivity of the material and thus influences many other phenomena in that medium, from the capacitance of capacitors to the speed of light.\nIt is defined as the constant of proportionality (which may be a tensor) relating an electric field formula_2 to the induced dielectric polarisation density formula_3 such that\nformula_4\nwhere formula_5 is the electric permittivity of free space.\nThe susceptibility of a medium is related to its relative permittivity formula_6 by\nformula_7\nSo in the case of a classical vacuum,\nformula_8\nThe electric displacement formula_9 is related to the polarisation density formula_3 by\nformula_11\nDispersion and causality.\nIn general, a material cannot polarise instantaneously in response to an applied field. The more general formulation as a function of time is\nformula_12\nThat is, the polarisation is a convolution of the electric field at previous times with time-dependent susceptibility given by formula_13. The upper limit of this integral can be extended to infinity as well if one defines formula_14 for formula_15. An instantaneous response corresponds to Dirac delta function susceptibility formula_16 .\nIt is more convenient in a linear system to take the Fourier transform and write this relationship as a function of frequency. Due to the convolution theorem, the integral becomes a simple product,\nformula_17\nThe susceptibility (or equivalently the permittivity) is frequency dependent. The change of susceptibility with respect to frequency characterises the dispersion properties of the material.\nMoreover, the fact that the polarisation can only depend on the electric field at previous times (i.e., formula_14 for formula_15), a consequence of causality, imposes Kramers\u2013Kronig constraints on the real and imaginary parts of the susceptibility formula_20.\nDielectric polarisation.\nBasic atomic model.\nIn the classical approach to the dielectric, the material is made up of atoms. Each atom consists of a cloud of negative charge (electrons) bound to and surrounding a positive point charge at its center. In the presence of an electric field, the charge cloud is distorted, as shown in the top right of the figure.\nThis can be reduced to a simple dipole using the superposition principle. A dipole is characterised by its dipole moment, a vector quantity shown in the figure as the blue arrow labeled \"M\". It is the relationship between the electric field and the dipole moment that gives rise to the behaviour of the dielectric. (Note that the dipole moment points in the same direction as the electric field in the figure. This is not always the case, and is a major simplification, but is true for many materials.)\nWhen the electric field is removed, the atom returns to its original state. The time required to do so is called relaxation time; an exponential decay.\nThis is the essence of the model in physics. The behaviour of the dielectric now depends on the situation. The more complicated the situation, the richer the model must be to accurately describe the behaviour. Important questions are:\nThe relationship between the electric field E and the dipole moment M gives rise to the behaviour of the dielectric, which, for a given material, can be characterised by the function F defined by the equation:\nformula_21\nWhen both the type of electric field and the type of material have been defined, one then chooses the simplest function \"F\" that correctly predicts the phenomena of interest. Examples of phenomena that can be so modelled include:\nDipolar polarisation.\nDipolar polarisation is a polarisation that is either inherent to polar molecules (orientation polarisation), or can be induced in any molecule in which the asymmetric distortion of the nuclei is possible (distortion polarisation). Orientation polarisation results from a permanent dipole, e.g., that arises from the 104.45\u00b0 angle between the asymmetric bonds between oxygen and hydrogen atoms in the water molecule, which retains polarisation in the absence of an external electric field. The assembly of these dipoles forms a macroscopic polarisation.\nWhen an external electric field is applied, the distance between charges within each permanent dipole, which is related to chemical bonding, remains constant in orientation polarisation; however, the direction of polarisation itself rotates. This rotation occurs on a timescale that depends on the torque and surrounding local viscosity of the molecules. Because the rotation is not instantaneous, dipolar polarisations lose the response to electric fields at the highest frequencies. A molecule rotates about 1 radian per picosecond in a fluid, thus this loss occurs at about 1011 Hz (in the microwave region). The delay of the response to the change of the electric field causes friction and heat.\nWhen an external electric field is applied at infrared frequencies or less, the molecules are bent and stretched by the field and the molecular dipole moment changes. The molecular vibration frequency is roughly the inverse of the time it takes for the molecules to bend, and this distortion polarisation disappears above the infrared.\nIonic polarisation.\nIonic polarisation is polarisation caused by relative displacements between positive and negative ions in ionic crystals (for example, NaCl).\nIf a crystal or molecule consists of atoms of more than one kind, the distribution of charges around an atom in the crystal or molecule leans to positive or negative. As a result, when lattice vibrations or molecular vibrations induce relative displacements of the atoms, the centers of positive and negative charges are also displaced. The locations of these centers are affected by the symmetry of the displacements. When the centers do not correspond, polarisation arises in molecules or crystals. This polarisation is called ionic polarisation.\nIonic polarisation causes the ferroelectric effect as well as dipolar polarisation. The ferroelectric transition, which is caused by the lining up of the orientations of permanent dipoles along a particular direction, is called an order-disorder phase transition. The transition caused by ionic polarisations in crystals is called a displacive phase transition.\nIn biological cells.\nIonic polarisation enables the production of energy-rich compounds in cells (the proton pump in mitochondria) and, at the plasma membrane, the establishment of the resting potential, energetically unfavourable transport of ions, and cell-to-cell communication (the Na+/K+-ATPase).\nAll cells in animal body tissues are electrically polarised \u2013 in other words, they maintain a voltage difference across the cell's plasma membrane, known as the membrane potential. This electrical polarisation results from a complex interplay between ion transporters and ion channels.\nIn neurons, the types of ion channels in the membrane usually vary across different parts of the cell, giving the dendrites, axon, and cell body different electrical properties. As a result, some parts of the membrane of a neuron may be excitable (capable of generating action potentials), whereas others are not.\nDielectric dispersion.\nIn physics, dielectric dispersion is the dependence of the permittivity of a dielectric material on the frequency of an applied electric field. Because there is a lag between changes in polarisation and changes in the electric field, the permittivity of the dielectric is a complex function of the frequency of the electric field. Dielectric dispersion is very important for the applications of dielectric materials and the analysis of polarisation systems.\nThis is one instance of a general phenomenon known as material dispersion: a frequency-dependent response of a medium for wave propagation.\nWhen the frequency becomes higher:\nIn the frequency region above ultraviolet, permittivity approaches the constant \"\u03b5\"0 in every substance, where \"\u03b5\"0 is the permittivity of the free space. Because permittivity indicates the strength of the relation between an electric field and polarisation, if a polarisation process loses its response, permittivity decreases.\nDielectric relaxation.\nDielectric relaxation is the momentary delay (or lag) in the dielectric constant of a material. This is usually caused by the delay in molecular polarisation with respect to a changing electric field in a dielectric medium (e.g., inside capacitors or between two large conducting surfaces). Dielectric relaxation in changing electric fields could be considered analogous to hysteresis in changing magnetic fields (e.g., in inductor or transformer cores). Relaxation in general is a delay or lag in the response of a linear system, and therefore dielectric relaxation is measured relative to the expected linear steady state (equilibrium) dielectric values. The time lag between electrical field and polarisation implies an irreversible degradation of Gibbs free energy.\nIn physics, dielectric relaxation refers to the relaxation response of a dielectric medium to an external, oscillating electric field. This relaxation is often described in terms of permittivity as a function of frequency, which can, for ideal systems, be described by the Debye equation. On the other hand, the distortion related to ionic and electronic polarisation shows behaviour of the resonance or oscillator type. The character of the distortion process depends on the structure, composition, and surroundings of the sample.\nDebye relaxation.\nDebye relaxation is the dielectric relaxation response of an ideal, noninteracting population of dipoles to an alternating external electric field. It is usually expressed in the complex permittivity \"\u03b5\" of a medium as a function of the field's angular frequency \"\u03c9\":\nformula_22\nwhere \"\u03b5\u221e\" is the permittivity at the high frequency limit, \u0394\"\u03b5\" \n \"\u03b5s\" \u2212 \"\u03b5\u221e\" where \"\u03b5s\" is the static, low frequency permittivity, and \"\u03c4\" is the characteristic relaxation time of the medium. Separating into the real part formula_23 and the imaginary part formula_24 of the complex dielectric permittivity yields:\nformula_25\nNote that the above equation for formula_26 is sometimes written with formula_27 in the denominator due to an ongoing sign convention ambiguity whereby many sources represent the time dependence of the complex electric field with formula_28 whereas others use formula_29. In the former convention, the functions formula_23 and formula_24 representing real and imaginary parts are given by formula_32 whereas in the latter convention formula_33. The above equation uses the latter convention.\nThe dielectric loss is also represented by the loss tangent:\nformula_34\nThis relaxation model was introduced by and named after the physicist Peter Debye (1913). It is characteristic for dynamic polarisation with only one relaxation time.\nParaelectricity.\nParaelectricity is the nominal behaviour of dielectrics when the dielectric permittivity tensor is proportional to the unit matrix, i.e., an applied electric field causes polarisation and/or alignment of dipoles only parallel to the applied electric field. Contrary to the analogy with a paramagnetic material, no permanent electric dipole needs to exist in a paraelectric material. Removal of the fields results in the dipolar polarisation returning to zero. The mechanisms that causes paraelectric behaviour are distortion of individual ions (displacement of the electron cloud from the nucleus) and polarisation of molecules or combinations of ions or defects.\nParaelectricity can occur in crystal phases where electric dipoles are unaligned and thus have the potential to align in an external electric field and weaken it.\nMost dielectric materials are paraelectrics. A specific example of a paraelectric material of high dielectric constant is strontium titanate.\nThe LiNbO3 crystal is ferroelectric below 1430 K, and above this temperature it transforms into a disordered paraelectric phase. Similarly, other perovskites also exhibit paraelectricity at high temperatures.\nParaelectricity has been explored as a possible refrigeration mechanism; polarising a paraelectric by applying an electric field under adiabatic process conditions raises the temperature, while removing the field lowers the temperature. A heat pump that operates by polarising the paraelectric, allowing it to return to ambient temperature (by dissipating the extra heat), bringing it into contact with the object to be cooled, and finally depolarising it, would result in refrigeration.\nTunability.\n\"Tunable dielectrics\" are insulators whose ability to store electrical charge changes when a voltage is applied.\nGenerally, strontium titanate (SrTiO3) is used for devices operating at low temperatures, while barium strontium titanate (Ba1\u2212xSrxTiO3) substitutes for room temperature devices. Other potential materials include microwave dielectrics and carbon nanotube (CNT) composites.\nIn 2013, multi-sheet layers of strontium titanate interleaved with single layers of strontium oxide produced a dielectric capable of operating at up to 125\u00a0GHz. The material was created via molecular beam epitaxy. The two have mismatched crystal spacing that produces strain within the strontium titanate layer that makes it less stable and tunable.\nSystems such as Ba1\u2212xSrxTiO3 have a paraelectric\u2013ferroelectric transition just below ambient temperature, providing high tunability. Films suffer significant losses arising from defects.\nApplications.\nCapacitors.\nCommercially manufactured capacitors typically use a solid dielectric material with high permittivity as the intervening medium between the stored positive and negative charges. This material is often referred to in technical contexts as the \"capacitor dielectric\".\nThe most obvious advantage to using such a dielectric material is that it prevents the conducting plates, on which the charges are stored, from coming into direct electrical contact. More significantly, however, a high permittivity allows a greater stored charge at a given voltage. This can be seen by treating the case of a linear dielectric with permittivity \"\u03b5\" and thickness \"d\" between two conducting plates with uniform charge density \"\u03c3\u03b5\". In this case the charge density is given by\nformula_35\nand the capacitance per unit area by\nformula_36\nFrom this, it can easily be seen that a larger \"\u03b5\" leads to greater charge stored and thus greater capacitance.\nDielectric materials used for capacitors are also chosen such that they are resistant to ionisation. This allows the capacitor to operate at higher voltages before the insulating dielectric ionises and begins to allow undesirable current.\nDielectric resonator.\nA \"dielectric resonator oscillator\" (DRO) is an electronic component that exhibits resonance of the polarisation response for a narrow range of frequencies, generally in the microwave band. It consists of a \"puck\" of ceramic that has a large dielectric constant and a low dissipation factor. Such resonators are often used to provide a frequency reference in an oscillator circuit. An unshielded dielectric resonator can be used as a dielectric resonator antenna (DRA).\nBST thin films.\nFrom 2002 to 2004, the United States Army Research Laboratory (ARL) conducted research on thin film technology. Barium strontium titanate (BST), a ferroelectric thin film, was studied for the fabrication of radio frequency and microwave components, such as voltage-controlled oscillators, tunable filters and phase shifters.\nThe research was part of an effort to provide the Army with highly-tunable, microwave-compatible materials for broadband electric-field tunable devices, which operate consistently in extreme temperatures. This work improved tunability of bulk barium strontium titanate, which is a thin film enabler for electronics components.\nIn a 2004 research paper, U.S. ARL researchers explored how small concentrations of acceptor dopants can dramatically modify the properties of ferroelectric materials such as BST.\nResearchers \"doped\" BST thin films with magnesium, analyzing the \"structure, microstructure, surface morphology and film/substrate compositional quality\" of the result. The Mg doped BST films showed \"improved dielectric properties, low leakage current, and good tunability\", meriting potential for use in microwave tunable devices.\nSome practical dielectrics.\nDielectric materials can be solids, liquids, or gases. (A high vacuum can also be a useful, nearly lossless dielectric even though its relative dielectric constant is only unity.)\nSolid dielectrics are perhaps the most commonly used dielectrics in electrical engineering, and many solids are very good insulators. Some examples include porcelain, glass, and most plastics. Air, nitrogen and sulfur hexafluoride are the three most commonly used gaseous dielectrics.\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41027", "revid": "10289486", "url": "https://en.wikipedia.org/wiki?curid=41027", "title": "Dielectric strength", "text": "Degree of insulation\nIn physics, the term dielectric strength has the following meanings:\nThe theoretical dielectric strength of a material is an intrinsic property of the bulk material, and is independent of the configuration of the material or the electrodes with which the field is applied. This \"intrinsic dielectric strength\" corresponds to what would be measured using pure materials under ideal laboratory conditions. At breakdown, the electric field frees bound electrons. If the applied electric field is sufficiently high, free electrons from background radiation may be accelerated to velocities that can liberate additional electrons by collisions with neutral atoms or molecules, in a process known as avalanche breakdown. Breakdown occurs quite abruptly (typically in nanoseconds), resulting in the formation of an electrically conductive path and a disruptive discharge through the material. In a solid material, a breakdown event severely degrades, or even destroys, its insulating capability.\nElectrical breakdown.\nElectric current is a flow of electrically charged particles in a material caused by an electric field. The mobile charged particles responsible for electric current are called charge carriers. In different substances different particles serve as charge carriers: in metals and other solids some of the outer electrons of each atom (conduction electrons) are able to move about the material; in electrolytes and plasma it is ions, electrically charged atoms or molecules, and electrons. A substance that has a high concentration of charge carriers available for conduction will conduct a large current with the given electric field created by a given voltage applied across it, and thus has a low electrical resistivity; this is called an electrical conductor. A material that has few charge carriers will conduct very little current with a given electric field and has a high resistivity; this is called an electrical insulator.\nHowever, when a large enough electric field is applied to any insulating substance, at a certain field strength the concentration of charge carriers in the material suddenly increases by many orders of magnitude, so its resistance drops and it becomes a conductor. This is called \"electrical breakdown\". The physical mechanism causing breakdown differs in different substances. In a solid, it usually occurs when the electric field becomes strong enough to pull outer valence electrons away from their atoms, so they become mobile. The field strength at which break down occurs is an intrinsic property of the material called its \"dielectric strength\".\nIn practical electric circuits electrical breakdown is often an unwanted occurrence, a failure of insulating material causing a short circuit, resulting in a catastrophic failure of the equipment. The sudden drop in resistance causes a high current to flow through the material, and the sudden extreme Joule heating may cause the material or other parts of the circuit to melt or vaporize explosively. However, breakdown itself is reversible. If the current supplied by the external circuit is sufficiently limited, no damage is done to the material, and reducing the applied voltage causes a transition back to the material's insulating state.\nBreak down field strength.\nThe field strength at which break down occurs depends on the respective geometries of the dielectric (insulator) and the electrodes with which the electric field is applied, as well as the rate of increase of the applied electric field. Because dielectric materials usually contain minute defects, the practical dielectric strength will be a significantly less than the intrinsic dielectric strength of an ideal, defect-free, material. Dielectric films tend to exhibit greater dielectric strength than thicker samples of the same material. For instance, the dielectric strength of silicon dioxide films of thickness around 1 \u03bcm is about 0.5GV/m. However very thin layers (below, say, 100 nm) become partially conductive because of electron tunneling. Multiple layers of thin dielectric films are used where maximum practical dielectric strength is required, such as high voltage capacitors and pulse transformers. Since the dielectric strength of gases varies depending on the shape and configuration of the electrodes, it is usually measured as a fraction of the dielectric strength of nitrogen gas.\nDielectric strength (in MV/m, or 106\u22c5volt/meter) of various common materials:\nUnits.\nIn SI, the unit of dielectric strength is volts per meter (V/m). It is also common to see related units such as volt per centimeter (V/cm), megavolts per meter (MV/m), and so on.\nIn United States customary units, dielectric strength is often specified in volt per mil (a mil is 1/1000 inch). The conversion is:\nformula_1\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41028", "revid": "1378709", "url": "https://en.wikipedia.org/wiki?curid=41028", "title": "Dielectric waveguide", "text": ""}
{"id": "41029", "revid": "547762", "url": "https://en.wikipedia.org/wiki?curid=41029", "title": "Differential encoding", "text": ""}
{"id": "41030", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41030", "title": "Differential Manchester encoding", "text": "Self-synchronizing line code\nDifferential Manchester encoding (DM) is a line code in digital frequency modulation in which data and clock signals are combined to form a single two-level self-synchronizing data stream. Each data bit is encoded by the presence or absence of a signal level transition in the middle of the bit period, followed by the mandatory level transition at the beginning. The code is insensitive to an inversion of polarity. In various specific applications, this method is also called by various other names, including biphase mark code (BMC), F2F (frequency/double frequency), Aiken biphase, and conditioned diphase.\nDefinition.\nDifferential Manchester encoding is a differential encoding technology, using the presence or absence of transitions to indicate logical value. An improvement to Manchester coding which is a special case of binary phase-shift keying, it is not necessary to know the initial polarity of the transmitted message signal, because the information is not represented by the absolute voltage levels but by their transitions.\nThere are two clock ticks per bit period (marked with full and dotted lines in the figure). At every second clock tick, marked with a dotted line, there is a potential level transition conditional on the data. At the other ticks, marked with full lines, the line state changes unconditionally to ease clock recovery. \nOne version of the code makes a transition for 0 and no transition for 1; the other makes a transition for 1 and no transition for 0. \nDifferential Manchester encoding has the following advantages:\nThese positive features are achieved at the expense of doubling the clock frequency of the encoded data stream.\nDifferential Manchester encoding is specified in the IEEE 802.5 standard for Token Ring local area networks, and is used for many other applications, including magnetic and optical storage. As Biphase Mark Code (BMC), it is used in AES3, S/PDIF, SMPTE time code, USB PD, xDSL and DALI. Many magnetic stripe cards also use BMC encoding, often called F2F (frequency/double frequency) or Aiken Biphase, according to the ISO/IEC 7811 standard. Differential Manchester encoding is also the original modulation method used for single-density floppy disks, followed by double-density modified frequency modulation (MFM).\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41031", "revid": "37011616", "url": "https://en.wikipedia.org/wiki?curid=41031", "title": "Diffraction grating", "text": "Optical component which splits light into several beams\nIn optics, a diffraction grating is a grating with a periodic structure of appropriate scale so as to diffract light, or another type of electromagnetic radiation, into several beams traveling in different directions (i.e., different diffraction angles) known as diffracted orders. The emerging coloration is a form of structural coloration. The directions or diffraction angles of these beams depend on the wave (light) incident angle to the diffraction grating, the spacing or periodic distance between adjacent diffracting elements (e.g., parallel slits for a transmission grating) on the grating, and the wavelength of the incident light. Because the grating acts as a dispersive element, diffraction gratings are commonly used in monochromators and spectrometers, but other applications are also possible such as optical encoders for high-precision motion control and wavefront measurement.\nFor typical applications, a reflective grating has ridges or \"rulings\" on its surface while a transmissive grating has transmissive or hollow slits on its surface. Such a grating modulates the amplitude of an incident wave to create a diffraction pattern. Some gratings modulate the phases of incident waves rather than the amplitude, and these types of gratings can be produced frequently by using holography.\nJames Gregory (1638\u20131675) observed the diffraction patterns caused by a bird feather, which was effectively the first diffraction grating (in a natural form) to be discovered, about a year after Isaac Newton's prism experiments. The first human-made diffraction grating was made around 1785 by Philadelphia inventor David Rittenhouse, who strung hairs between two finely threaded screws. This was similar to notable German physicist Joseph von Fraunhofer's wire diffraction grating in 1821. The principles of diffraction were discovered by Thomas Young and Augustin-Jean Fresnel. Using these principles, Fraunhofer was the first to use a diffraction grating to obtain line spectra and the first to measure the wavelengths of spectral lines with a diffraction grating.\nIn the 1860s, state-of-the-art diffraction gratings with small groove period, formula_1, were manufactured by Friedrich Adolph Nobert (1806\u20131881) in Greifswald; then the two Americans Lewis Morris Rutherfurd (1816\u20131892) and William B. Rogers (1804\u20131882) took over the lead. By the end of the 19th century, the concave gratings of Henry Augustus Rowland (1848\u20131901) were the best available.\nA diffraction grating can create \"rainbow\" colors when it is illuminated by a wide-spectrum (e.g., continuous) light source. Rainbow-like colors from closely spaced narrow tracks on optical data storage disks such as CDs or DVDs are an example of light diffraction caused by diffraction gratings. A usual diffraction grating has parallel lines (It is true for 1-dimensional gratings, but 2 or 3-dimensional gratings are also possible and they have their applications such as wavefront measurement), while a CD has a spiral of finely spaced data tracks. Diffraction colors also appear when one looks at a bright point source through a translucent fine-pitch umbrella fabric covering. Decorative patterned plastic films based on reflective grating patches are inexpensive and commonplace. A similar color separation seen from thin layers of oil (or gasoline, etc.) on water, known as iridescence, is not caused by diffraction from a grating but rather by thin film interference from the closely stacked transmissive layers.\nTheory of operation.\nFor a diffraction grating, the relationship between the grating spacing (i.e., the distance between adjacent grating grooves or slits), the angle of the wave (light) incidence to the grating, and the diffracted wave from the grating is known as the grating equation. Like many other optical formulas, the grating equation can be derived by using the Huygens\u2013Fresnel principle, stating that each point on a wavefront of a propagating wave can be considered to act as a point wave source, and a wavefront at any subsequent point can be found by adding together the contributions from each of these individual point wave sources on the previous wavefront.\nGratings may be of the 'reflective' or 'transmissive' type, analogous to a mirror or lens, respectively. A grating has a 'zero-order mode' (where the integer order of diffraction \"m\" is set to zero), in which a ray of light behaves according to the laws of reflection (like a mirror) and refraction (like a lens), respectively.\nAn idealized diffraction grating is made up of a set of slits of spacing formula_1, that must be wider than the wavelength of interest to cause diffraction. Assuming a plane wave of monochromatic light of wavelength formula_3 at normal incidence on a grating (i.e., wavefronts of the incident wave are parallel to the grating main plane), each slit in the grating acts as a quasi point wave source from which light propagates in all directions (although this is typically limited to the forward hemisphere from the point source). Of course, every point on every slit to which the incident wave reaches plays as a point wave source for the diffraction wave and all these contributions to the diffraction wave determine the detailed diffraction wave light property distribution, but diffraction angles (at the grating) at which the diffraction wave intensity is highest are determined only by these quasi point sources corresponding the slits in the grating. After the incident light (wave) interacts with the grating, the resulting diffracted light from the grating is composed of the sum of interfering wave components emanating from each slit in the grating; At any given point in space through which the diffracted light may pass, typically called observation point, the path length from each slit in the grating to the given point varies, so the phase of the wave emanating from each of the slits at that point also varies. As a result, the sum of the diffracted waves from the grating slits at the given observation point creates a peak, valley, or some degree between them in light intensity through additive and destructive interference. When the difference between the light paths from adjacent slits to the observation point is equal to an odd integer-multiple of the half of the wavelength, \"l\"formula_4 with an odd integer formula_5, the waves are out of phase at that point, and thus cancel each other to create the (locally) minimum light intensity. Similarly, when the path difference is a multiple of formula_3, the waves are in phase and the (locally) maximum intensity occurs. For light at the normal incidence to the grating, the intensity maxima occur at diffraction angles formula_7, which satisfy the relationship formula_8, where formula_7 is the angle between the diffracted ray and the grating's normal vector, formula_1 is the distance from the center of one slit to the center of the adjacent slit, and formula_11 is an integer representing the propagation-mode of interest called the diffraction order.\nWhen a plane light wave is normally incident on a grating of uniform period formula_1, the diffracted light has maxima at diffraction angles formula_7 given by a special case of the grating equation as\nformula_14\nIt can be shown that if the plane wave is incident at angle formula_15 relative to the grating normal, in the plane orthogonal to the grating periodicity, the grating equation becomes\nformula_16\nwhich describes in-plane diffraction as a special case of the more general scenario of conical, or off-plane, diffraction described by the generalized grating equation:\nformula_17\nwhere formula_18 is the angle between the direction of the plane wave and the direction of the grating grooves, which is orthogonal to both the directions of grating periodicity and grating normal. \nVarious sign conventions for formula_15, formula_7 and formula_11 are used; any choice is fine as long as the choice is kept through diffraction-related calculations. When solved for diffracted angle at which the diffracted wave intensity are maximized, the equation becomes\nformula_22\nThe diffracted light that corresponds to direct transmission for a transmissive diffraction grating or specular reflection for a reflective grating is called the zero order, and is denoted formula_23. The other diffracted light intensity maxima occur at angles formula_7 represented by non-zero integer diffraction orders formula_11. Note that formula_11 can be positive or negative, corresponding to diffracted orders on both sides of the zero-order diffracted beam.\nEven if the grating equation is derived from a specific grating such as the grating in the right diagram (this grating is called a blazed grating), the equation can apply to any regular structure of the same spacing, because the phase relationship between light scattered from adjacent diffracting elements of the grating remains the same. The detailed diffracted light property distribution (e.g., intensity) depends on the detailed structure of the grating elements as well as on the number of elements in the grating, but it always gives maxima in the directions given by the grating equation.\nDepending on how a grating modulates incident light on it to cause the diffracted light, there are the following grating types:\nAn optical axis diffraction grating, in which the optical axis is spatially and periodically modulated, is also considered either a reflection or transmission phase diffraction grating.\nThe grating equation applies to all these gratings due to the same phase relationship between the diffracted waves from adjacent diffracting elements of the gratings, even if the detailed distribution of the diffracted wave property depends on the detailed structure of each grating.\nQuantum electrodynamics.\nQuantum electrodynamics (QED) offers another derivation of the properties of a diffraction grating in terms of photons as particles (at some level). QED can be described intuitively with the path integral formulation of quantum mechanics. As such it can model photons as potentially following all paths from a source to a final point, each path with a certain probability amplitude. These probability amplitudes can be represented as a complex number or equivalent vector\u2014or, as Richard Feynman simply calls them in his book on QED, \"arrows\".\nFor the probability that a certain event will happen, one sums the probability amplitudes for all of the possible ways in which the event can occur, and then takes the square of the length of the result. The probability amplitude for a photon from a monochromatic source to arrive at a certain final point at a given time, in this case, can be modeled as an arrow that spins rapidly until it is evaluated when the photon reaches its final point. For example, for the probability that a photon will reflect off a mirror and be observed at a given point a given amount of time later, one sets the photon's probability amplitude spinning as it leaves the source, follows it to the mirror, and then to its final point, even for paths that do not involve bouncing off the mirror at equal angles. One can then evaluate the probability amplitude at the photon's final point; next, one can integrate over all of these arrows (see vector sum), and square the length of the result to obtain the probability that this photon will reflect off the mirror in the pertinent fashion. The times these paths take are what determines the angle of the probability amplitude arrow, as they can be said to \"spin\" at a constant rate (which is related to the frequency of the photon).\nThe times of the paths near the classical reflection site of the mirror are nearly the same, so the probability amplitudes point in nearly the same direction\u2014thus, they have a sizable sum. Examining the paths towards the edges of the mirror reveals that the times of nearby paths are quite different from each other, and thus we wind up summing vectors that cancel out quickly. So, there is a higher probability that light will follow a near-classical reflection path than a path further out. However, a diffraction grating can be made out of this mirror, by scraping away areas near the edge of the mirror that usually cancel nearby amplitudes out\u2014but now, since the photons don't reflect from the scraped-off portions, the probability amplitudes that would all point, for instance, at forty-five degrees, can have a sizable sum. Thus, this lets light of the right frequency sum to a larger probability amplitude, and as such possess a larger probability of reaching the appropriate final point.\nThis particular description involves many simplifications: a point source, a \"surface\" that light can reflect off (thus neglecting the interactions with electrons) and so forth. The biggest simplification is perhaps in the fact that the \"spinning\" of the probability amplitude arrows is actually more accurately explained as a \"spinning\" of the source, as the probability amplitudes of photons do not \"spin\" while they are in transit. We obtain the same variation in probability amplitudes by letting the time at which the photon left the source be indeterminate\u2014and the time of the path now tells us when the photon would have left the source, and thus what the angle of its \"arrow\" would be. However, this model and approximation is a reasonable one to illustrate a diffraction grating conceptually. Light of a different frequency may also reflect off the same diffraction grating, but with a different final point.\nGratings as dispersive elements.\nThe wavelength dependence in the grating equation shows that the grating separates an incident polychromatic beam into its constituent wavelength components at different angles, i.e., it is angular dispersive. Each wavelength of input beam spectrum is sent into a different direction, producing a rainbow of colors under white light illumination. This is visually similar to the operation of a prism, although the mechanism is very different. A prism refracts waves of different wavelengths at different angles due to their different refractive indices, while a grating diffracts different wavelengths at different angles due to interference at each wavelength.\nThe diffracted beams corresponding to consecutive orders may overlap, depending on the spectral content of the incident beam and the grating density. The higher the spectral order, the greater the overlap into the next order.\nThe grating equation shows that the angles of the diffracted orders only depend on the grooves' period, and not on their shape. By controlling the cross-sectional profile of the grooves, it is possible to concentrate most of the diffracted optical energy in a particular order for a given wavelength. A triangular profile is commonly used. This technique is called \"blazing\". The incident angle and wavelength for which the diffraction is most efficient (the ratio of the diffracted optical energy to the incident energy is the highest) are often called \"blazing angle\" and \"blazing wavelength\". The efficiency of a grating may also depend on the polarization of the incident light. Gratings are usually designated by their \"groove density\", the number of grooves per unit length, usually expressed in grooves per millimeter (g/mm), also equal to the inverse of the groove period. The groove period must be on the order of the wavelength of interest; the spectral range covered by a grating is dependent on groove spacing and is the same for ruled and holographic gratings with the same grating constant (meaning \"groove density\" or the groove period). The maximum wavelength that a grating can diffract is equal to twice the grating period, in which case the incident and diffracted light are at ninety degrees (90\u00b0) to the grating normal. To obtain frequency dispersion over a wider frequency one must use a prism. The optical regime, in which the use of gratings is most common, corresponds to wavelengths between 100 nm and 10 \u00b5m. In that case, the groove density can vary from a few tens of grooves per millimeter, as in echelle gratings, to a few thousands of grooves per millimeter.\nWhen groove spacing is less than half the wavelength of light, the only present order is the \"m\" = 0 order. Gratings with such small periodicity (with respect to the incident light wavelength) are called subwavelength gratings and exhibit special optical properties. Made on an isotropic material the subwavelength gratings give rise to form birefringence, in which the material behaves as if it were birefringent.\nFabrication.\nSR (Surface Relief) gratings.\nSR gratings are named due to its surface structure of depressions (low relief) and elevations (high relief). Originally, high-resolution gratings were ruled by high-quality \"ruling engines\" whose construction was a large undertaking. Henry Joseph Grayson designed a machine to make diffraction gratings, succeeding with one of 120,000 lines to the inch (approx. 4,724 lines per mm) in 1899. Later, photolithographic techniques created gratings via holographic interference patterns. A holographic grating has sinusoidal grooves as the result of an optical sinusoidal interference pattern on the grating material during its fabrication, and may not be as efficient as ruled gratings, but are often preferred in monochromators because they produce less stray light. A copying technique can make high quality replicas from master gratings of either type, thereby lowering fabrication costs.\nSemiconductor technology today is also used to etch holographically patterned gratings into robust materials such as fused silica. In this way, low stray-light holography is combined with the high efficiency of deep, etched transmission gratings, and can be incorporated into high-volume, low-cost semiconductor manufacturing technology.\nVPH (Volume Phase Holography) gratings.\nAnother method for manufacturing diffraction gratings uses a photosensitive gel sandwiched between two substrates. A holographic interference pattern exposes the gel, which is later developed. These gratings, called \"volume phase holography diffraction gratings\" (or VPH diffraction gratings) have no physical grooves, but instead a periodic modulation of the refractive index within the gel. This removes much of the surface scattering effects typically seen in other types of gratings. These gratings also tend to have higher efficiencies, and allow for the inclusion of complicated patterns into a single grating. A VPH diffraction grating is typically a transmission grating, through which incident light passes and is diffracted, but a VPH reflection grating can also be made by tilting the direction of a refractive index modulation with respect to the grating surface. In older versions of such gratings, environmental susceptibility was a trade-off, as the gel had to be contained at low temperature and humidity. Typically, the photosensitive substances are sealed between two substrates that make them resistant to humidity, and thermal and mechanical stresses. VPH diffraction gratings are not destroyed by accidental touches and are more scratch resistant than typical relief gratings.\nBlazed gratings.\nA blazed grating is manufactured with grooves that have a sawtooth-shaped cross section, unlike the symmetrical grooves of other gratings. This allows the grating to achieve maximum diffraction efficiency, but in only one diffraction order which is dependent on the angle of the sawtooth grooves, known as the blaze angle. Common uses include specific wavelength selection for tunable lasers, among others.\nOther gratings.\nA new technology for grating insertion into integrated photonic lightwave circuits is digital planar holography (DPH). DPH gratings are generated in computer and fabricated on one or several interfaces of an optical waveguide planar by using standard micro-lithography or nano-imprinting methods, compatible with mass-production. Light propagates inside the DPH gratings, confined by the refractive index gradient, which provides longer interaction path and greater flexibility in light steering.\nExamples.\nDiffraction gratings are often used in monochromators, spectrometers, lasers, wavelength division multiplexing devices, optical pulse compressing devices, interferometers, and many other optical instruments.\nOrdinary pressed CD and DVD media are every-day examples of diffraction gratings and can be used to demonstrate the effect by reflecting sunlight off them onto a white wall. This is a side effect of their manufacture, as one surface of a CD has many small pits in the plastic, arranged in a spiral; that surface has a thin layer of metal applied to make the pits more visible. The structure of a DVD is optically similar, although it may have more than one pitted surface, and all pitted surfaces are inside the disc.\nDue to the sensitivity to the refractive index of the media, diffraction grating can be used as sensor of fluid properties.\nIn a standard pressed vinyl record when viewed from a low angle perpendicular to the grooves, a similar but less defined effect to that in a CD/DVD is seen. This is due to viewing angle (less than the critical angle of reflection of the black vinyl) and the path of the light being reflected due to this being changed by the grooves, leaving a rainbow relief pattern behind.\nDiffraction gratings are also used to distribute evenly the frontlight of e-readers such as the Nook Simple Touch with GlowLight.\nGratings from electronic components.\nSome everyday electronic components contain fine and regular patterns, and as a result readily serve as diffraction gratings. For example, CCD sensors from discarded mobile phones and cameras can be removed from the device. With a laser pointer, diffraction can reveal the spatial structure of the CCD sensors. This can be done for LCD or LED displays of smart phones as well. Because such displays are usually protected just by transparent casing, experiments can be done without damaging the phones. If accurate measurements are not intended, a spotlight can reveal the diffraction patterns.\nNatural gratings.\nStriated muscle is the most commonly found natural diffraction grating and, this has helped physiologists in determining the structure of such muscle. Aside from this, the chemical structure of crystals can be thought of as diffraction gratings for types of electromagnetic radiation other than visible light, this is the basis for techniques such as X-ray crystallography.\nMost commonly confused with diffraction gratings are the iridescent colors of peacock feathers, mother-of-pearl, and butterfly wings. Iridescence in birds, fish and insects is often caused by thin-film interference rather than a diffraction grating. Diffraction produces the entire spectrum of colors as the viewing angle changes, whereas thin-film interference usually produces a much narrower range. The surfaces of flowers can also create a diffraction, but the cell structures in plants are usually too irregular to produce the fine slit geometry necessary for a diffraction grating. The iridescence signal of flowers is thus only appreciable very locally and hence not visible to man and flower visiting insects. However, natural gratings do occur in some invertebrate animals, like the peacock spiders, the antennae of seed shrimp, and have even been discovered in Burgess Shale fossils.\nDiffraction grating effects are sometimes seen in meteorology. Diffraction coronas are colorful rings surrounding a source of light, such as the Sun. These are usually observed much closer to the light source than halos, and are caused by very fine particles, like water droplets, ice crystals, or smoke particles in a hazy sky. When the particles are all nearly the same size they diffract the incoming light at very specific angles. The exact angle depends on the size of the particles. Diffraction coronas are commonly observed around light sources, like candle flames or street lights, in the fog. Cloud iridescence is caused by diffraction, occurring along coronal rings when the particles in the clouds are all uniform in size.\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "41032", "revid": "8390765", "url": "https://en.wikipedia.org/wiki?curid=41032", "title": "Digital access and cross-connect system", "text": ""}
{"id": "41033", "revid": "50168182", "url": "https://en.wikipedia.org/wiki?curid=41033", "title": "Digital filter", "text": "Device for suppressing part of a discretely-sampled signal\nIn signal processing, a digital filter is a system that performs mathematical operations on a sampled, discrete-time signal to reduce or enhance certain aspects of that signal. This is in contrast to the other major type of electronic filter, the analog filter, which is typically an electronic circuit operating on continuous-time analog signals.\nA digital filter system usually consists of an analog-to-digital converter (ADC) to sample the input signal, followed by a microprocessor and some peripheral components such as memory to store data and filter coefficients etc. Program Instructions (software) running on the microprocessor implement the digital filter by performing the necessary mathematical operations on the numbers received from the ADC. In some high performance applications, an FPGA or ASIC is used instead of a general purpose microprocessor, or a specialized digital signal processor (DSP) with specific paralleled architecture for expediting operations such as filtering.\nDigital filters may be more expensive than an equivalent analog filter due to their increased complexity, but they make practical many designs that are impractical or impossible as analog filters. Digital filters can often be made very high order, and are often finite impulse response filters, which allows for linear phase response. When used in the context of real-time analog systems, digital filters sometimes have problematic latency (the difference in time between the input and the response) due to the associated analog-to-digital and digital-to-analog conversions and anti-aliasing filters, or due to other delays in their implementation.\nDigital filters are commonplace and an essential element of everyday electronics such as radios, cellphones, and AV receivers.\nCharacterization.\nA digital filter is characterized by its transfer function, or equivalently, its difference equation. Mathematical analysis of the transfer function can describe how it will respond to any input. As such, designing a filter consists of developing specifications appropriate to the problem (for example, a second-order low-pass filter with a specific cut-off frequency), and then producing a transfer function that meets the specifications.\nThe transfer function for a linear, time-invariant, digital filter can be expressed as a transfer function in the \"Z\"-domain; if it is causal, then it has the form:\nformula_1\nwhere the order of the filter is the greater of \"N\" or \"M\".\nSee \"Z\"-transform's LCCD equation for further discussion of this transfer function.\nThis is the form for a recursive filter, which typically leads to an infinite impulse response (IIR) behaviour, but if the denominator is made equal to unity, i.e. no feedback, then this becomes a finite impulse response (FIR) filter.\nAnalysis techniques.\nA variety of mathematical techniques may be employed to analyze the behavior of a given digital filter. Many of these analysis techniques may also be employed in designs, and often form the basis of a filter specification.\nTypically, one characterizes filters by calculating how they will respond to a simple input such as an impulse. One can then extend this information to compute the filter's response to more complex signals.\nImpulse response.\nThe impulse response, often denoted formula_2 or formula_3, is a measurement of how a filter will respond to the Kronecker delta function. For example, given a difference equation, one would set formula_4 and formula_5 for formula_6 and evaluate. The impulse response is a characterization of the filter's behavior. Digital filters are typically considered in two categories: infinite impulse response (IIR) and finite impulse response (FIR).\nIn the case of linear time-invariant FIR filters, the impulse response is exactly equal to the sequence of filter coefficients, and thus:\nformula_7\nIIR filters on the other hand are recursive, with the output depending on both current and previous inputs as well as previous outputs. The general form of an IIR filter is thus:\nformula_8\nPlotting the impulse response reveals how a filter responds to a sudden, momentary disturbance.\nAn IIR filter is always recursive. While it is possible for a recursive filter to have a finite impulse response, a non-recursive filter always has a finite impulse response. An example is the moving average (MA) filter, which can be implemented both recursively and non recursively.\nDifference equation.\nIn discrete-time systems, the digital filter is often implemented by converting the transfer function to a linear constant-coefficient difference equation (LCCD) via the Z-transform. The discrete frequency-domain transfer function is written as the ratio of two polynomials. For example:\nformula_9\nThis is expanded:\nformula_10\nand to make the corresponding filter causal, the numerator and denominator are divided by the highest order of formula_11:\nformula_12\nThe coefficients of the denominator, formula_13, are the 'feed-backward' coefficients and the coefficients of the numerator are the 'feed-forward' coefficients, formula_14. The resultant linear difference equation is:\nformula_15\nor, for the example above:\nformula_16\nrearranging terms:\nformula_17\nthen by taking the inverse \"z\"-transform:\nformula_18\nand finally, by solving for formula_19:\nformula_20\nThis equation shows how to compute the next output sample, formula_19, in terms of the past outputs, formula_22, the present input, formula_23, and the past inputs, formula_24. Applying the filter to an input in this form is equivalent to a Direct Form I or II (see below) realization, depending on the exact order of evaluation.\nIn plain terms, for example, as used by a computer programmer implementing the above equation in code, it can be described as follows:\nformula_25 = the output, or filtered value\nformula_26 = the input, or incoming raw value\nformula_27 = the sample number, iteration number, or time period number\nand therefore:\nformula_19 = the current filtered (output) value\nformula_29 = the last filtered (output) value\nformula_30 = the 2nd-to-last filtered (output) value\nformula_23 = the current raw input value\nformula_32 = the last raw input value\nformula_33 = the 2nd-to-last raw input value\nFilter design.\nAlthough filters are easily understood and calculated, the practical challenges of their design and implementation are significant and are the subject of much advanced research.\nThere are two categories of digital filter: the recursive filter and the nonrecursive filter. These are often referred to as IIR filters and FIR filters, respectively.\nFilter realization.\nAfter a filter is designed, it must be \"realized\" by developing a signal flow diagram that describes the filter in terms of operations on sample sequences.\nA given transfer function may be realized in many ways. Consider how a simple expression such as formula_34 could be evaluated \u2013 one could also compute the equivalent formula_35. In the same way, all realizations may be seen as \"factorizations\" of the same transfer function, but different realizations will have different numerical properties. Specifically, some realizations are more efficient in terms of the number of operations or storage elements required for their implementation, and others provide advantages such as improved numerical stability and reduced round-off error. Some structures are better for fixed-point arithmetic and others may be better for floating-point arithmetic.\nDirect form I.\nA straightforward approach for IIR filter realization is direct form I, where the difference equation is evaluated directly. This form is practical for small filters, but may be inefficient and impractical (numerically unstable) for complex designs. In general, this form requires 2N delay elements (for both input and output signals) for a filter of order N.\nDirect form II.\nThe alternate direct form II only needs \"N\" delay units, where \"N\" is the order of the filter \u2013 potentially half as much as direct form I. This structure is obtained by reversing the order of the numerator and denominator sections of Direct Form I, since they are in fact two linear systems, and the commutativity property applies. Then, one will notice that there are two columns of delays (formula_36) that tap off the center net, and these can be combined since they are redundant, yielding the implementation as shown below.\nThe disadvantage is that direct form II increases the possibility of arithmetic overflow for filters of high \"Q\" or resonance. It has been shown that as \"Q\" increases, the round-off noise of both direct form topologies increases without bounds. This is because, conceptually, the signal is first passed through an all-pole filter (which normally boosts gain at the resonant frequencies) before the result of that is saturated, then passed through an all-zero filter (which often attenuates much of what the all-pole half amplifies).\nCascaded second-order sections.\nA common strategy is to realize a higher-order (greater than 2) digital filter as a cascaded series of second-order \"biquadratric\" (or \"biquad\") sections (see digital biquad filter). The advantage of this strategy is that the coefficient range is limited. Cascading direct form II sections results in \"N\" delay elements for filters of order \"N\". Cascading direct form I sections results in \"N\" + 2 delay elements, since the delay elements of the input of any section (except the first section) are redundant with the delay elements of the output of the preceding section.\nOther forms.\nOther forms include:\nComparison of analog and digital filters.\nDigital filters are not subject to the component tolerances, temperature variations, and non-linearities that greatly complicate the design of analog filters. Analog filters consist of imperfect electronic components, whose values may also change with temperature and drift with time. As the order of an analog filter increases, and thus its component count, the effect of variable component errors is greatly magnified. In digital filters, the coefficient values are stored in computer memory, making them far more stable and predictable.\nBecause the coefficients of digital filters are definite, they can be used to achieve much more complex and selective designs \u2013 specifically with digital filters, one can achieve a lower passband ripple, faster transition, and higher stopband attenuation than is practical with analog filters. Even if the design could be achieved using analog filters, the engineering cost of designing an equivalent digital filter would likely be much lower. Furthermore, one can readily modify the coefficients of a digital filter to make an adaptive filter or a user-controllable parametric filter. While these techniques are possible in an analog filter, they are again considerably more difficult.\nDigital filters can be used in the design of finite impulse response filters which can achieve extremely steep rolloff slopes with no phase shift. Analog filters that perform the same function are often significantly more complicated, as they would require many delay elements.\nDigital filters rely less on analog circuitry, potentially allowing for a better signal-to-noise ratio. A digital filter will introduce noise to a signal during analog low pass filtering, analog to digital conversion, digital to analog conversion and may introduce digital noise due to quantization. With analog filters, every component is a source of thermal noise (such as Johnson noise), so as the filter complexity grows, so does the noise.\nHowever, digital filters do introduce a higher fundamental latency to the system. In an analog filter, latency is often negligible; strictly speaking it is the time for an electrical signal to propagate through the filter circuit. In digital systems, latency is introduced not only by delay elements in the digital signal path, but also by analog-to-digital and digital-to-analog converters that are required for a system to process analog signals. Digital filters must also deal with quantization and rounding errors.\nAdditionally, in very simple cases or in cases where frequencies and filter slopes are fixed, it is more cost effective to use an analog filter. Introducing a digital filter requires considerable overhead circuitry, as previously discussed, including two low pass analog filters. Analog filters also require substantially less power than digital filters and are therefore the only solution when power requirements are tight.\nWhen making an electrical circuit on a PCB it is generally easier to use a digital solution, because the processing units are highly optimized over the years. Making the same circuit with analog components would take up a lot more space when using discrete components. Two alternatives are FPAAs and ASICs, but they are expensive for low quantities.\nTypes of digital filters.\nThere are various ways to characterize filters; for example:\nA filter can be represented by a block diagram, which can then be used to derive a sample processing algorithm to implement the filter with hardware instructions. A filter may also be described as a difference equation, a collection of zeros and poles or an impulse response or step response.\nSome digital filters are based on the fast Fourier transform, a mathematical algorithm that quickly extracts the frequency spectrum of a signal, allowing the spectrum to be manipulated (such as to create very high order band-pass filters) before converting the modified spectrum back into a time-series signal with an inverse FFT operation. These filters give O(n log n) computational costs whereas conventional digital filters tend to be O(n2).\nAnother form of a digital filter is that of a state-space model. A well used state-space filter is the Kalman filter published by Rudolf K\u00e1lm\u00e1n in 1960.\nTraditional linear filters are usually based on attenuation. Alternatively nonlinear filters can be designed, including energy transfer filters, which allow the user to move energy in a designed way so that unwanted noise or effects can be moved to new frequency bands either lower or higher in frequency, spread over a range of frequencies, split, or focused. Energy transfer filters complement traditional filter designs and introduce many more degrees of freedom in filter design. Digital energy transfer filters are relatively easy to design and to implement and exploit nonlinear dynamics.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41035", "revid": "84591", "url": "https://en.wikipedia.org/wiki?curid=41035", "title": "Digital milliwatt", "text": "Test signal in telecommunications\nIn digital telephony, the digital milliwatt is a standard test signal that serves as a reference for analog signal levels in the telecommunications network. When decoding the digital milliwatt, a PCM decoder produces a sinusoidal signal with a frequency of with one milliwatt in power (, a reference for dBm0).\nThe digital milliwatt signal is encoded by eight 8-bit words corresponding to one pulse-code modulated cycle of the signal, sampled 8000 times per second. It is typically stored in read-only memory (ROM) in the telecommunication equipment.\nThe digital milliwatt signal is often generated in instruments in place of separate test equipment. It has the advantage of being tied in frequency and amplitude to the relatively stable digital clock signal and power (voltage) supply, respectively, that are used by the digital channel bank.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41036", "revid": "28481209", "url": "https://en.wikipedia.org/wiki?curid=41036", "title": "Digital multiplex hierarchy", "text": "In telecommunications, a digital multiplex hierarchy is a hierarchy consisting of an ordered repetition of tandem digital multiplexers that produce signals of successively higher data rates at each level of the hierarchy.\nDigital multiplexing hierarchies may be implemented in many different configurations depending on; (a) the number of channels desired, (b) the signaling system to be used, and (c) the bit rate allowed by the communications media.\nSome currently available digital multiplexers have been designated as Dl-, DS-, or M-series, all of which operate at T-carrier rates.\nIn the design of digital multiplex hierarchies, care must be exercised to ensure interoperability of the multiplexers used in the hierarchy.\nDigroup.\nDigroup is an abbreviation for digital group. In telephony, a basic group in the digital multiplex hierarchy.\nIn the North American and Japanese T-carrier digital hierarchies, each digroup supports 12 PCM voice channels or their equivalent in other services. The DS1 line rate (2 digroups plus overhead bits) is 1.544\u00a0Mbit/s, supporting 24 voice channels or their equivalent in other services.\nIn the E-carrier European hierarchy, each digroup supports 15 PCM channels or their equivalent in other services. The DS1 line rate (2 digroups plus overhead bits) is 2.048\u00a0Mbit/s, supporting 30 voice channels or their equivalent in other services.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41037", "revid": "45807063", "url": "https://en.wikipedia.org/wiki?curid=41037", "title": "Digital Signal 0", "text": "Digital Signal 0 (DS0) is a basic digital signaling rate of 64 kilobits per second (kbit/s), corresponding to the capacity of one analog voice-frequency-equivalent communication channel. The DS0 rate, and its equivalents E0 in the E-carrier system and T0 in the T-carrier system, form the basis for the digital multiplex transmission hierarchy in telecommunications systems used in North America, Europe, Japan, and the rest of the world, for both the early plesiochronous systems such as T-carrier and for modern synchronous systems such as SDH/SONET.\nThe DS0 rate was introduced to carry a single digitized voice call. For a typical phone call, the audio sound is digitized at an 8 kHz sample rate, or 8000 samples per second, using 8-bit pulse-code modulation for each of the samples. This results in a data rate of 64 kbit/s.\nBecause of its fundamental role in carrying a single phone call, the DS0 rate forms the basis for the digital multiplex transmission hierarchy in telecommunications systems used in North America. To limit the number of wires required between two involved in exchanging voice calls, a system was built in which multiple DS0s are multiplexed together on higher capacity circuits. In this system, twenty-four (24) DS0s are multiplexed into a DS1 signal. Twenty-eight (28) DS1s are multiplexed into a DS3. When carried over copper wire, this is the well-known T-carrier system, with T1 and T3 corresponding to DS1 and DS3, respectively. \nBesides its use for voice communications, the DS0 rate may support twenty 2.4\u00a0kbit/s channels, ten 4.8\u00a0kbit/s channels, five 9.67\u00a0kbit/s channels, one 56\u00a0kbit/s channel, or one 64\u00a0kbit/s clear channel.\nE0 (standardized as ITU G.703) is the European equivalent of the North American DS0 for carrying a single voice call. However, there are some subtle differences in implementation. Voice signals are encoded for carriage over E0 according to ITU G.711. Note that when a T-carrier system is used as in North America, robbed bit signaling can mean that a DS0 channel carried over that system is not an error-free bit-stream. The out-of-band signaling used in the European E-carrier system avoids this.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nSee also.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41038", "revid": "7903804", "url": "https://en.wikipedia.org/wiki?curid=41038", "title": "Digital subscriber line", "text": "Family of technologies that are used to transmit digital data over telephone lines\nDigital subscriber line (DSL; originally digital subscriber loop) is a family of technologies that are used to transmit digital data over telephone lines. In telecommunications marketing, the term DSL is widely understood to mean asymmetric digital subscriber line (ADSL), the most commonly installed DSL technology, for Internet access.\nIn ADSL, the data throughput in the upstream direction (the direction to the service provider) is lower, hence the designation of \"asymmetric\" service. In symmetric digital subscriber line (SDSL) services, the downstream and upstream data rates are equal.\nDSL service can be delivered simultaneously with wired telephone service on the same telephone line since DSL uses higher frequency bands for data transmission. On the customer premises, a DSL filter is installed on each telephone to prevent undesirable interaction between DSL and telephone service.\nThe bit rate of consumer ADSL services typically ranges from up to , while the later VDSL+ technology delivers between 16 Mbit/s and 250 Mbit/s in the direction to the customer (downstream), with up to 40 Mbit/s upstream. The exact performance is depending on technology, line conditions, and service-level implementation. Researchers at Bell Labs have reached SDSL speeds over 1 Gbit/s using traditional copper telephone lines, though such speeds have not been made available for the end customers yet.\nHistory.\nInitially, it was believed that ordinary phone lines could only be used at modest speeds, usually less than 9600 bits per second. In the 1950s, ordinary twisted-pair telephone cable often carried 4\u00a0MHz television signals between studios, suggesting that such lines would allow transmitting many megabits per second. One such circuit in the United Kingdom ran some between the BBC studios in Newcastle-upon-Tyne and the Pontop Pike transmitting station. However, these cables had other impairments besides Gaussian noise, preventing such rates from becoming practical in the field. The 1980s saw the development of techniques for broadband communications that allowed the limit to be greatly extended. A patent was filed in 1979 for the use of existing telephone wires for both telephones and data terminals that were connected to a remote computer via a digital data carrier system.\nThe motivation for digital subscriber line technology was the Integrated Services Digital Network (ISDN) specification proposed in 1984 by the CCITT (now ITU-T) as part of Recommendation I.120, later reused as ISDN digital subscriber line (IDSL). Employees at Bellcore (now Telcordia Technologies) developed asymmetric digital subscriber line (ADSL) by placing wide-band digital signals at frequencies above the existing baseband analog voice signal carried on conventional twisted pair cabling between telephone exchanges and customers. A patent was filed by AT&amp;T Bell Labs on the basic DSL concept in 1988.\nJoseph W. Lechleider's contribution to DSL was his insight that an asymmetric arrangement offered more than double the bandwidth capacity of symmetric DSL. This allowed Internet service providers to offer efficient service to consumers, who benefited greatly from the ability to download large amounts of data but rarely needed to upload comparable amounts. ADSL supports two modes of transport: fast channel and interleaved channel. Fast channel is preferred for streaming multimedia, where an occasional \"dropped bit\" is acceptable, but lags are less so. Interleaved channel works better for file transfers, where the delivered data must be error-free but latency (time delay) incurred by the retransmission of error-containing packets is acceptable.\nConsumer-oriented ADSL was designed to operate on existing lines already conditioned for Basic Rate Interface ISDN services. Engineers developed high-speed DSL facilities such as high bit rate digital subscriber line (HDSL) and symmetric digital subscriber line (SDSL) to provision traditional Digital Signal 1 (DS1) services over standard copper pair facilities.\nOlder ADSL standards delivered 8 Mbit/s to the customer over about of unshielded twisted-pair copper wire. Newer variants improved these rates. Distances greater than significantly reduce the bandwidth usable on the wires, thus reducing the data rate. But ADSL loop extenders increase these distances by repeating the signal, allowing the local exchange carrier (LEC) to deliver DSL speeds to any distance.\nUntil the late 1990s, the cost of digital signal processors for DSL was prohibitive. All types of DSL employ highly complex digital signal processing algorithms to overcome the inherent limitations of the existing twisted pair wires. Due to the advancements of very-large-scale integration (VLSI) technology, the cost of the equipment associated with a DSL deployment lowered significantly. The two main pieces of equipment are a digital subscriber line access multiplexer (DSLAM) at one end and a DSL modem at the other end.\nIt is possible to set up a DSL connection over an existing cable. Such deployment, even including equipment, is much cheaper than installing a new, high-bandwidth fiber-optic cable over the same route and distance. This is true both for ADSL and SDSL variations. DSL's continued use reflects advancements in electronics that have improved performance and reduced costs, despite the high expense of laying new physical infrastructure.\nThese advantages made ADSL a better proposition for customers requiring Internet access than metered dial up, while also allowing voice calls to be received at the same time as a data connection. Telephone companies were also under pressure to move to ADSL owing to competition from cable companies, which use DOCSIS cable modem technology to achieve similar speeds. Demand for high bandwidth applications, such as video and file sharing, also contributed to the popularity of ADSL technology. Some of the first field trials for DSL were carried out in 1996.\nEarly DSL service required a dedicated dry loop, but when the U.S. Federal Communications Commission (FCC) required incumbent local exchange carriers (ILECs) to lease their lines to competing DSL service providers, shared-line DSL became available. Also known as DSL over unbundled network element, this unbundling of services allows a single subscriber to receive two separate services from two separate providers on one cable pair. The DSL service provider's equipment is co-located in the same telephone exchange as that of the ILEC supplying the customer's pre-existing voice service. The subscriber's circuit is rewired to interface with hardware supplied by the ILEC which combines a DSL frequency and plain old telephone service (POTS) signals on a single copper pair.\nSince 1999, certain ISPs have been offering microfilters. These devices are installed indoors and serve the same purpose as DSL splitters, which are deployed outdoors: they divide the frequencies needed for ADSL and POTS phone calls. These filters originated out of a desire to make self-installation of DSL service possible and eliminate early outdoor DSL splitters which were installed at or near the demarcation point between the customer and the ISP.\nBy 2012, some carriers in the United States reported that DSL remote terminals with fiber backhaul were replacing older ADSL systems.\nOperation.\nTelephones are connected to the telephone exchange via a local loop, which is a physical pair of wires. The local loop was originally intended mostly for the transmission of speech, encompassing an audio frequency range of 300 to 3400 hertz (commercial bandwidth). However, as long-distance trunks were gradually converted from analog to digital operation, the idea of being able to pass data through the local loop (by using frequencies above the voiceband) took hold, ultimately leading to DSL.\nThe local loop connecting the telephone exchange to most subscribers has the capability of carrying frequencies well beyond the 3400\u00a0Hz upper limit of POTS. Depending on the length and quality of the loop, the upper limit can be tens of megahertz. DSL takes advantage of this unused bandwidth of the local loop by creating 4312.5\u00a0Hz wide channels starting between 10 and 100\u00a0kHz, depending on how the system is configured. Allocation of channels continues to higher frequencies (up to 1.1\u00a0MHz for ADSL) until new channels are deemed unusable. Each channel is evaluated for usability in much the same way an analog modem would on a POTS connection. More usable channels equate to more available bandwidth, which is why distance and line quality are a factor (the higher frequencies used by DSL travel only short distances).\nThe pool of usable channels is then split into two different frequency bands for upstream and downstream traffic, based on a preconfigured ratio. This segregation reduces interference. Once the channel groups have been established, the individual channels are bonded into a pair of virtual circuits, one in each direction. Like analog modems, DSL transceivers constantly monitor the quality of each channel and will add or remove them from service depending on whether they are usable. Once upstream and downstream circuits are established, a subscriber can connect to a service such as an Internet service provider or other network services, like a corporate MPLS network.\nThe underlying technology of transport across DSL facilities uses modulation of high-frequency carrier waves, an analog signal transmission. A DSL circuit terminates at each end in a modem which modulates patterns of bits into certain high-frequency impulses for transmission to the opposing modem. Signals received from the far-end modem are demodulated to yield a corresponding bit pattern that the modem passes on, in digital form, to its interfaced equipment, such as a computer, router, switch, etc.\nUnlike traditional dial-up modems, which modulate bits into signals in the 300\u20133400\u00a0Hz audio baseband, DSL modems modulate frequencies from 4000\u00a0Hz to as high as 4\u00a0MHz. This frequency band separation enables DSL service and plain old telephone service (POTS) to coexist on the same cables, known as voice-grade cables. On the subscriber's end of the circuit, inline DSL filters are installed on each telephone to pass voice frequencies but filter the high-frequency signals that would otherwise be heard as hiss. Also, nonlinear elements in the phone could otherwise generate audible intermodulation and may impair the operation of the data modem in the absence of these low-pass filters. DSL and RADSL modulations do not use the voice-frequency band so high-pass filters are incorporated in the circuitry of DSL modems filter out voice frequencies.\nBecause DSL operates above the 3.4\u00a0kHz voice limit, it cannot pass through a loading coil, which is an inductive coil that is designed to counteract loss caused by shunt capacitance (capacitance between the two wires of the twisted pair). Loading coils are commonly set at regular intervals in POTS lines. Voice service cannot be maintained past a certain distance without such coils. Therefore, some areas that are within range for DSL service are disqualified from eligibility because of loading coil placement. Because of this, phone companies endeavor to remove loading coils on copper loops that can operate without them. Longer lines that require them can be replaced with fiber to the neighborhood or node (FTTN).\nMost residential and small-office DSL implementations reserve low frequencies for POTS, so that (with suitable filters and/or splitters) the existing voice service continues to operate independently of the DSL service. Thus POTS-based communications, including fax machines and dial-up modems, can share the wires with DSL. Only one DSL modem can use the subscriber line at a time. The standard way to let multiple computers share a DSL connection uses a router that establishes a connection between the DSL modem and a local Ethernet, powerline, or Wi-Fi network on the customer's premises.\nThe theoretical foundations of DSL, like much of communication technology, can be traced back to Claude Shannon's seminal 1948 paper, \"A Mathematical Theory of Communication\". Generally, higher bit rate transmissions require a wider frequency band, though the ratio of bit rate to symbol rate and thus to bandwidth are not linear due to significant innovations in digital signal processing and digital modulation methods.\nNaked DSL.\nNaked DSL is a way of providing only DSL services over a local loop. It is useful when the customer does not need the traditional telephony voice service because voice service is received either on top of the DSL services (usually VoIP) or through another network (E.g., mobile telephony). It is also commonly called an unbundled network element (UNE) in the United States; in Australia it is known as an unconditioned local loop (ULL); in Belgium it is known as \"raw copper\" and in the UK it is known as Single Order GEA (SoGEA).\nIt started making a comeback in the United States in 2004 when Qwest started offering it, closely followed by Speakeasy. As a result of AT&amp;T's merger with SBC, and Verizon's merger with MCI, those telephone companies have an obligation to offer naked DSL to consumers.\nTypical setup.\nOn the customer side, a DSL modem is hooked up to a phone line. The telephone company connects the other end of the line to a DSLAM (digital subscriber line access multiplexer), which concentrates a large number of individual DSL connections into a single box. The DSLAM cannot be located too far from the customer because of attenuation between the DSLAM and the user's DSL modem. It is common for a few residential blocks to be connected to one DSLAM.\nThe above figure is a schematic of a simple DSL connection (in blue). The right side shows a DSLAM residing in the telephone company's telephone exchange. The left side shows the customer premises equipment with an optional router. The router manages a local area network which connects PCs and other local devices. The customer may opt for a modem that contains both a router and wireless access. This option (within the dashed bubble) often simplifies the connection.\nExchange equipment.\nAt the exchange, a DSLAM terminates the DSL circuits and aggregates them, where they are handed off to other networking transports such as a PON network or Ethernet. The DSLAM terminates all connections and recovers the original digital information. In the case of ADSL, the voice component is also separated at this step, either by a filter or splitter integrated in the DSLAM or by specialized filtering equipment installed before it. Load coils in phone lines, used for extending their range in rural areas, must be removed to allow DSL to operate as they only allow frequencies of up to 4000\u00a0Hz to pass through phone cables.\nCustomer equipment.\nThe customer end of the connection consists of a DSL modem. This converts data between the digital signals used by computers and the analog voltage signal of a suitable frequency range which is then applied to the phone line.\nIn some DSL variations (for example, HDSL), the modem connects directly to the computer via a serial interface, using protocols such as Ethernet or V.35. In other cases (particularly ADSL), it is common for the customer equipment to be integrated with higher-level functionality, such as routing, firewalling, or other application-specific hardware and software. In this case, the equipment is referred to as a gateway.\nMost DSL technologies require the installation of appropriate DSL filters at the customer's premises to separate the DSL signal from the low-frequency voice signal. The separation can take place either at the demarcation point, or with filters installed at the telephone outlets inside the customer premises. It is possible for a DSL gateway to integrate the filter, and allow telephones to connect through the gateway.\nModern DSL gateways often integrate routing and other functionality. The system boots, synchronizes the DSL connection and finally establishes the internet IP services and connection between the local network and the service provider, using protocols such as DHCP or PPPoE.\nProtocols and configurations.\nMany DSL technologies implement an Asynchronous Transfer Mode (ATM) layer over the low-level bitstream layer to enable the adaptation of a number of different technologies over the same link.\nDSL implementations may create bridged or routed networks. In a bridged configuration, the group of subscriber computers effectively connect into a single subnetwork. The earliest implementations used DHCP to provide the IP address to the subscriber equipment, with authentication via MAC address or an assigned hostname. Later implementations often use Point-to-Point Protocol (PPP) to authenticate with a user ID and password.\nTransmission modulation methods.\nTransmission methods vary by market, region, carrier, and equipment.\nDSL technologies.\nDSL technologies (sometimes collectively summarized as xDSL) include:\nThe line-length limitations from telephone exchange to subscriber impose severe limits on data transmission rates. Technologies such as VDSL provide very high-speed but short-range links. VDSL is used as a method of delivering triple play services (typically implemented in fiber to the curb network architectures).\nTerabit DSL, is a technology that proposes the use of the space between the dielectrics (insulators) on copper twisted pair lines in telephone cables, as waveguides for 300\u00a0GHz signals that can offer speeds of up to 1 terabit per second at distances of up to 100 meters, 100 gigabits per second for 300 meters, and 10 gigabits per second for 500 meters. The first experiment for this was carried out with copper lines that were parallel to each other, and not twisted, inside a metal pipe meant to simulate the metal armoring in large telephone cables.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41039", "revid": "1754504", "url": "https://en.wikipedia.org/wiki?curid=41039", "title": "Digital switch", "text": ""}
{"id": "41040", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41040", "title": "Digital transmission group", "text": "In telecommunications, a digital transmission group is a group of digitized voice or data channels or both with bit streams that are combined into a single digital bit stream for transmission over communications media. \nDigital transmission groups usually are categorized by their maximum capacity, not by a specific number of channels. However, the maximum digital transmission group capacity must be equal to or greater than the sum of the individual multiplexer input channel capacities.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41042", "revid": "8066546", "url": "https://en.wikipedia.org/wiki?curid=41042", "title": "Digitizer", "text": ""}
{"id": "41043", "revid": "88809", "url": "https://en.wikipedia.org/wiki?curid=41043", "title": "Digroup", "text": "In the mathematical area of algebra, a digroup is a generalization of a group that has two one-sided product operations, formula_1 and formula_2, instead of the single operation in a group. Digroups were introduced independently by Liu (2004), Felipe (2006), and Kinyon (2007), inspired by a question about Leibniz algebras.\nTo explain digroups, consider a group. In a group there is one operation, such as addition in the set of integers; there is a single \"unit\" element, like 0 in the integers, and there are inverses, like formula_3 in the integers, for which both the following equations hold: formula_4 and formula_5. A digroup replaces the one operation by two operations that interact in a complicated way, as stated below. A digroup may also have more than one \"unit\", and an element formula_6 may have different inverses for each \"unit\". This makes a digroup vastly more complicated than a group. Despite that complexity, there are reasons to consider digroups, for which see the references.\nDefinition.\nA digroup is a set \"D\" with two binary operations, formula_1 and formula_2, that satisfy the following laws (e.g., Ongay 2010):\nformula_1 and formula_2 are associative,\nformula_11\nformula_12\nformula_13\nformula_16\nThe set of bar units is called the halo of \"D\".\nformula_19\nGeneralized digroup.\nIn a generalized digroup or g-digroup, a generalization due to Salazar-D\u00edaz, Vel\u00e1squez, and Wills-Toro (2016), each element has a left inverse and a right inverse instead of one two-sided inverse.\nOne reason for this generalization is that it permits analogs of the isomorphism theorems of group theory that cannot be formulated within digroups.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41044", "revid": "13467261", "url": "https://en.wikipedia.org/wiki?curid=41044", "title": "Direct access", "text": "Direct access may refer to:\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "41045", "revid": "161735", "url": "https://en.wikipedia.org/wiki?curid=41045", "title": "Direct connect", "text": "Direct connect may refer to:\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "41046", "revid": "3938795", "url": "https://en.wikipedia.org/wiki?curid=41046", "title": "Direct distance dialing", "text": "Telecommunication service\nDirect distance dialing (DDD) is a telecommunications service in North America by which a caller may call any other subscriber outside the local calling area without operator assistance, DDD was introduced in the United States in 1951, on a trial basis, with service from Englewood, New Jersey. Direct long-distance dialing by subscribers requires extra digits, called an area code, to be dialed as prefixes to the directory telephone number of the destination. International Direct Distance Dialing (IDDD) extends the system beyond the geographic boundaries of the North American Numbering Plan (NANP).\nHistory.\nThe first direct-dialed long-distance telephone calls were possible in the New Jersey communities of Englewood and Teaneck. Customers of the ENglewood 3, ENglewood 4 and TEaneck 7 exchanges, who could already dial telephone numbers in the New York City area, could place calls to eleven major cities across the United States by dialing the three-digit area code and the seven-digit directory number. Local telephone numbers still consisted of the first two letters of the central office name and five digits. On November 10, 1951, Englewood mayor M. Leslie Denning made the first customer-dialed long-distance call, to Mayor Frank Osborne of Alameda, California.\nThe destinations, and their area codes, equipped with a long-distance toll-switch at that time were:\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nOther areas could not yet be included in DDD as they did not have the necessary toll switching equipment, or because they still did not use a seven-digit local numbering plan. Montreal, Quebec, and Toronto, Ontario, in Canada, for example, had a mix of six- and seven-digit telephone numbers from 1951 to 1957, and did not have DDD until 1958. Whitehorse, Yukon, had seven-digit numbers starting in 1965, but the necessary switching equipment was not in place until 1972.\nSan Francisco required the special area code 318 due to temporary routing requirements. San Francisco and Oakland each had their own separate toll-switches, so calls had to be routed accordingly depending on the final destination. As the telephone equipment used at the time could only handle three-digit translation, the temporary use of area code 318 was required to distinguish between the two areas. Area code 318 was temporarily used to specify San Francisco and areas north of the Golden Gate, while calls with destinations in Oakland and the East Bay continued to use area code 415. When the electromechanical card-translator box became available in the 1952\u201353 period, six-digit translation became possible and the use of area code 318 was no longer required. Area code 318 was reclaimed for future use (now used as an area code for northern Louisiana), and the entire San Francisco Bay Area returned to using area code 415.\nHardware.\nThe No. 4 Crossbar switching system had been introduced in the early 1940s to switch four-wire circuits and replace the incoming operator. With semiautomatic operation analogous to the early days of the panel switch, the operator in the originating city used a multifrequency keypad to dial an access code to connect to the correct city and to send the seven-digit number to incoming equipment at the terminating city. This design was further refined to serve DDD.\nThe card sorter of the 4A/CTS (Number 4A Crossbar / Card Translator System) allowed six-digit translation of the central office code number dialed by the customer. This determined the proper trunk circuits to use, where separate circuit groups were used for different cities in the same area code, as in the case of Oakland and San Francisco. The new device used metal cards similar in principle to computer punched cards, and they were rapidly scanned as they fell past a light beam. CTS machines were called 4A (Advanced) if the translator was included in the original installation, and 4M (Modified) if it was added later. A 1970s version of 4XB, the 4A/ETS, used a computer to translate. For international dialing, Traffic Service Position System (TSPS) provided the extra computer power.\nThe reach of DDD was limited due to the inefficiency and expense of switching equipment, and the limited ability to process records of completed calls. An early obstacle was that the majority of switching systems did not provide Automatic Number Identification (ANI). Common control switches, such as the 1XB switch, were fairly quickly retrofitted to provide ANI, and most 5XB switches were initially installed with ANI services. Panel switch were eventually retrofitted, as were some step-by-step systems that were not scheduled for immediate replacement. Even if a switch had ANI, it could not identify callers on party lines. This was only partly overcome by tip-party identification for two-party lines. As the cost of subscriber line carrier declined, party lines were gradually phased out.\nAs this and other improved technologies became available, as well as Automatic Message Accounting (AMA) computers to process the long-distance records into customer bills, the reach of DDD was slow in the 1950s, but quickened in the early 1960s. Electronic switching systems allowed electronic processing of the dialed digits, referring to electronic memories to determine call routing, and this has reached the state of the art, with digital telephone exchanges which are basically specialized computers that route voice traffic from one \"peripheral\" to another as digital data. Call routing can now be done based on the area code, central office code and even the first two digits of the line number, although routing based on digits past the central office code is usually limited to cases of competitive local exchange carriers, number pooling and number portability.\nIDDD.\nIn the 1960s, with the domestic conversion still underway, plans were laid to extend Direct Distance Dialing beyond North America (including a number of the Caribbean Islands). Some subscribers could already directly dial transatlantic telephone calls to certain destinations as early as in 1957 over the recently completed Atlantic cable to England. A new systematic extension of Direct Distance Dialing was developed and was introduced as International Direct Distance Dialing (IDDD) in March 1970.\nWith so much new equipment already working that could not handle more than the requisite ten-digit telephone numbers of DDD, the new system was based on designs by which most toll offices did not have to store and forward the whole international telephone number. Gateway offices were set up in New York, London and Paris, connected to the ordinary automatic toll network. The New York gateway was at 32 Avenue of the Americas. The new LT1 5XB switch on the tenth floor of 435 West 50th Street received new originating registers and outgoing senders able to handle fifteen-digit telephone numbers, with appropriate modifications to completing markers and other equipment. Other 5XB switches in the next few years were installed with IDDD as original equipment, and in the 1970s ESS offices also provided the service.\nThe key to the new system was two-stage multi-frequency pulsing. The outgoing sender sent its Class 4 toll center an off-hook signal as usual, received a wink as usual as a \"proceed to send\" signal, and outpulsed only a special three-digit (later six-digit) access code. The toll center picked a trunk through the long-distance network to the gateway office, which sent a second wink to the originating office, which then sent the whole dialed number. Thus the toll switching system needed no modification except at the gateway. The international trunks used Signaling System No. 5, a \"North Atlantic\" version of the North American multi-frequency signaling system, with minor modifications including slightly higher digit rate. European MF systems of the time used compelled signaling, which would slow down too much on a long transoceanic connection.\nIn the 1970s, toll centers were modified by adding the Traffic Service Position System (TSPS). With these new computers in place, digit storage in the toll system was no longer a problem. End offices were less extensively modified, and sent all their digits in a single stream. TSPS handled the gateway codes and other complexities of toll connections to the gateway office.\nEquivalent service in the United Kingdom.\nIn the United Kingdom and other parts of the Commonwealth of Nations, an equivalent service to direct distance dialing is subscriber trunk dialing (STD), and ISD for international subscriber trunk dialing. Queen Elizabeth II inaugurated STD on 5 December 1958, when she dialed a call from Bristol to Edinburgh and spoke to the Lord Provost.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41047", "revid": "1479481", "url": "https://en.wikipedia.org/wiki?curid=41047", "title": "Directional coupler", "text": ""}
{"id": "41048", "revid": "613716", "url": "https://en.wikipedia.org/wiki?curid=41048", "title": "Directive gain", "text": ""}
{"id": "41049", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41049", "title": "Direct-sequence spread spectrum", "text": "Modulation technique to reduce signal interference\nIn telecommunications, direct-sequence spread spectrum (DSSS) is a spread-spectrum modulation technique primarily used to reduce overall signal interference. The direct-sequence modulation makes the transmitted signal wider in bandwidth than the information bandwidth. \nAfter the despreading or removal of the direct-sequence modulation in the receiver, the information bandwidth is restored, while the unintentional and intentional interference is substantially reduced.\nSwiss inventor, Gustav Guanella proposed a \"means for and method of secret signals\". With DSSS, the message symbols are modulated by a sequence of complex values known as \"spreading sequence\". Each element of the spreading sequence, a so-called \"chip\", has a shorter duration than the original message symbols. The modulation of the message symbols scrambles and spreads the signal in the spectrum, and thereby results in a bandwidth of the spreading sequence. The smaller the chip duration, the larger the bandwidth of the resulting DSSS signal; more bandwidth multiplexed to the message signal results in better resistance against narrowband interference.\nSome practical and effective uses of DSSS include the code-division multiple access (CDMA) method, the IEEE 802.11b specification used in Wi-Fi networks, and the Global Positioning System.\nTransmission method.\nDirect-sequence spread-spectrum transmissions multiply the symbol sequence being transmitted with a spreading sequence that has a higher rate than the original message rate. Usually, sequences are chosen such that the resulting spectrum is spectrally white. Knowledge of the same sequence is used to reconstruct the original data at the receiving end. This is commonly implemented by the element-wise multiplication with the spreading sequence, followed by summation over a message symbol period. This process, \"despreading\", is mathematically a correlation of the transmitted spreading sequence with the spreading sequence. In an AWGN channel, the despreaded signal's signal-to-noise ratio is increased by the spreading factor, which is the ratio of the spreading-sequence rate to the data rate.\nWhile a transmitted DSSS signal occupies a wider bandwidth than the direct modulation of the original signal would require, its spectrum can be restricted by conventional pulse-shape filtering.\nIf an undesired transmitter transmits on the same channel but with a different spreading sequence, the despreading process reduces the power of that signal. This effect is the basis for the code-division multiple access (CDMA) method of multi-user medium access, which allows multiple transmitters to share the same channel within the limits of the cross-correlation properties of their spreading sequences.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41050", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41050", "title": "Disengagement originator", "text": "In telecommunications, a disengagement originator is the user or execution unit that initiates a disengagement attempt. The disengagement originator may be the originating user, the destination user, or the communications system. The communications system may deliberately originate the disengagement because of preemption, or inadvertently due to system malfunction.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41051", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41051", "title": "Dispersion-limited operation", "text": "A dispersion-limited operation is an operation of a communications link in which signal waveform degradation attributable to the dispersive effects of the communications medium is the dominant mechanism that limits link performance. The dispersion is the filter-like effect which a link has on the signal, due to the different propagation speeds of the eigenmodes of the link. Practically, this means that the waveform at the input will be different from the waveform at the output of the link.\nNote that the amount of allowable degradation is dependent on the quality of the receiver. In fiber optic communications, \"dispersion-limited operation\" is often confused with \"distortion-limited operation\"."}
{"id": "41052", "revid": "1712722", "url": "https://en.wikipedia.org/wiki?curid=41052", "title": "Distortion", "text": "Alteration of the original shape of a signal\nIn signal processing, distortion is the alteration of the original shape (or other characteristic) of a signal. In communications and electronics it means the alteration of the waveform of an information-bearing signal, such as an audio signal representing sound or a video signal representing images, in an electronic device or communication channel.\nDistortion is usually unwanted, and so engineers strive to eliminate or minimize it. In some situations, however, distortion may be desirable. For example, in noise reduction systems like the Dolby system, an audio signal is deliberately distorted in ways that emphasize aspects of the signal that are subject to electrical noise, then it is symmetrically \"undistorted\" after passing through a noisy communication channel, reducing the noise in the received signal. Distortion is also used as a musical effect, particularly with electric guitars.\nThe addition of noise or other outside signals (hum, interference) is not considered distortion, though the effects of quantization distortion are sometimes included in noise. Quality measures that reflect both noise and distortion include the signal-to-noise and distortion (SINAD) ratio and total harmonic distortion plus noise (THD+N).\nElectronic signals.\nIn telecommunications and signal processing, a noise-free system can be characterised by a transfer function, such that the output formula_1 can be written as a function of the input formula_2 as\n formula_3\nWhen the transfer function comprises only a perfect gain constant \"A\" and perfect delay \"T\"\n formula_4\nthe output is undistorted. Distortion occurs when the transfer function \"F\" is more complicated than this. If \"F\" is a linear function, for instance a filter whose gain and/or delay varies with frequency, the signal suffers linear distortion. Linear distortion does not introduce new frequency components to a signal but does alter the balance of existing ones.\nThis diagram shows the behaviour of a signal (made up of a square wave followed by a sine wave) as it is passed through various distorting functions.\nThe transfer function of an ideal amplifier, with perfect gain and delay, is only an approximation. The true behavior of the system is usually different. Nonlinearities in the transfer function of an active device (such as vacuum tubes, transistors, and operational amplifiers) are a common source of non-linear distortion; in passive components (such as a coaxial cable or optical fiber), linear distortion can be caused by inhomogeneities, reflections, and so on in the propagation path.\nAmplitude distortion.\nAmplitude distortion is distortion occurring in a system, subsystem, or device when the output amplitude is not a linear function of the input amplitude under specified conditions.\nHarmonic distortion.\nHarmonic distortion adds overtones that are whole number multiples of a sound wave's frequencies. Nonlinearities that give rise to amplitude distortion in audio systems are most often measured in terms of the harmonics (overtones) added to a pure sinewave fed to the system. Harmonic distortion may be expressed in terms of the relative strength of individual components, in decibels, or the root mean square of all harmonic components: Total harmonic distortion (THD), as a percentage. The level at which harmonic distortion becomes audible depends on the exact nature of the distortion. Different types of distortion (like crossover distortion) are more audible than others (like soft clipping) even if the THD measurements are identical. Harmonic distortion in radio frequency applications is rarely expressed as THD.\nFrequency response distortion.\nNon-flat frequency response is a form of distortion that occurs when different frequencies are amplified by different amounts in a filter. For example, the non-uniform frequency response curve of AC-coupled cascade amplifier is an example of frequency distortion. In the audio case, this is mainly caused by room acoustics, poor loudspeakers and microphones, long loudspeaker cables in combination with frequency dependent loudspeaker impedance, etc.\nPhase distortion.\nThis form of distortion mostly occurs due to electrical reactance. Here, all the components of the input signal are not amplified with the same phase shift, hence making some parts of the output signal out of phase with the rest of the output.\nGroup delay distortion.\nCan be found only in dispersive media. In a waveguide, phase velocity varies with frequency. In a filter, group delay tends to peak near the cut-off frequency, resulting in pulse distortion. When analog long distance trunks were commonplace, for example in 12 channel carrier, group delay distortion had to be corrected in repeaters.\nCorrection of distortion.\nAs the system output is given by y(t) = F(x(t)), then if the inverse function F\u22121 can be found, and used intentionally to distort either the input or the output of the system, then the distortion is corrected.\nAn example of a similar correction is where LP/vinyl recordings or FM audio transmissions are deliberately pre-emphasised by a linear filter, the reproducing system applies an inverse filter to make the overall system undistorted.\nCorrection is not possible if the inverse does not exist\u2014for instance if the transfer function has flat spots (the inverse would map multiple input points to a single output point). This produces an uncorrectable loss of information. Such a situation can occur when an amplifier is overdriven\u2014causing clipping or slew rate distortion when, for a moment, the amplifier characteristics alone and not the input signal determine the output.\nCancellation of even-order harmonic distortion.\nMany symmetrical electronic circuits reduce the magnitude of even harmonics generated by the non-linearities of the amplifier's components, by combining two signals from opposite halves of the circuit where distortion components that are roughly the same magnitude but out of phase. Examples include push-pull amplifiers and long-tailed pairs.\nTeletypewriter or modem signaling.\nIn binary signaling such as FSK, distortion is the shifting of the significant instants of the signal pulses from their proper positions relative to the beginning of the start pulse. The magnitude of the distortion is expressed in percent of an ideal unit pulse length. This is sometimes called \"bias distortion\".\nTelegraphic distortion is a similar and older problem, distorting the ratio between \"mark\" and \"space\" intervals.\nAudio distortion.\nWith respect to audio, distortion refers to any kind of deformation of an output waveform compared to its input, usually clipping, harmonic distortion, or intermodulation distortion (mixing phenomena) caused by non-linear behavior of electronic components and power supply limitations. Terms for specific types of nonlinear audio distortion include: crossover distortion and slew-induced distortion (SID).\nOther forms of audio distortion are non-flat frequency response, compression, modulation, aliasing, quantization noise, wow and flutter from analog media such as vinyl records and magnetic tape. The human ear cannot hear phase distortion, except that it may affect the stereo imaging.\nIn most fields, distortion is characterized as unwanted change to a signal. Distortion in music is often intentionally used as an effect when applied to an electric guitar signal in styles of rock music such as heavy metal and punk rock.\nDistortion in art.\nIn the visual arts a distortion is any change made by an artist to the size, shape or visual character of a form in order to express an idea, convey a feeling, or enhance visual impact. Such distortions or \"abstractions\" primarily refer to purposeful deviations from photorealistic perspective or from realistic proportionality. Examples include \"The Weeping Woman\" by Picasso and \"The Adoration of the Shepherds\" by El Greco, whose human subject matters are irregularly and (as is often with physical distortions) asymmetrically proportioned in a way that is not possible in standard perspective.\nOptics.\nIn optics, image/optical distortion is a divergence from rectilinear projection caused by a change in magnification with increasing distance from the optical axis of an optical system.\nMap projections.\nIn cartography, a distortion is the misrepresentation of the area or shape of a feature. The Mercator projection, for example, distorts by exaggerating the size of regions at high latitude.\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41053", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41053", "title": "Distortion-limited operation", "text": "Signal-related condition in telecommunications engineering\nIn telecommunications, distortion-limited operation is the condition prevailing when distortion of a received signal, rather than its attenuated amplitude (or power), limits performance under stated operational conditions and limits. \n\"Note:\" Distortion-limited operation is reached when the system distorts the shape of the waveform beyond specified limits. For linear systems, distortion-limited operation is equivalent to bandwidth-limited operation.\nReferences.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41054", "revid": "7903804", "url": "https://en.wikipedia.org/wiki?curid=41054", "title": "Distributed database", "text": "Database whose data is stored in different physical locations\nA distributed database is a database in which data is stored across different physical locations. It may be stored in multiple computers located in the same physical location (e.g. a data centre); or maybe dispersed over a network of interconnected computers. Unlike parallel systems, in which the processors are tightly coupled and constitute a single database system, a distributed database system consists of loosely coupled sites that share no physical components.\nSystem administrators can distribute collections of data (e.g. in a database) across multiple physical locations. A distributed database can reside on organised network servers or decentralised independent computers on the Internet, on corporate intranets or extranets, or on other organisation networks. Because distributed databases store data across multiple computers, distributed databases may improve performance at end-user worksites by allowing transactions to be processed on many machines, instead of being limited to one.\nTwo processes ensure that the distributed databases remain up-to-date and current: replication and duplication.\nBoth replication and duplication can keep the data current in all distributive locations.\nBesides distributed database replication and fragmentation, there are many other distributed database design technologies. For example, local autonomy, synchronous, and asynchronous distributed database technologies. The implementation of these technologies can and do depend on the needs of the business and the sensitivity/confidentiality of the data stored in the database and the price the business is willing to spend on ensuring data security, consistency and integrity.\nWhen discussing access to distributed databases, Microsoft favors the term distributed query, which it defines in protocol-specific manner as \"[a]ny SELECT, INSERT, UPDATE, or DELETE statement that references tables and rowsets from one or more external OLE DB data sources\".\nOracle provides a more language-centric view in which distributed queries and distributed transactions form part of distributed SQL.\nArchitecture.\nThere are 3 main architecture types for distributed databases:\nIn the shared-memory and shared-disk architectures, the data is not partitioned, but it has to be in a shared-nothing architecture.\nShared-disk architecture is more common for cloud databases than for on-premise.\nHistorically, shared-nothing was the first architecture to be implemented on the cloud, before the advent of shared cloud storage made shared-disk possible.\nIn practice, different layers of the database can have different architectures. It is now common to have a compute layer with a shared nothing architecture, and a storage layer with a shared disk architecture. This is for instance the case of Snowflake and AWS Aurora.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41055", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41055", "title": "Distributed-queue dual-bus", "text": "In telecommunications, a distributed-queue dual-bus network (DQDB) is a distributed multi-access network that (a) supports integrated communications using a dual bus and distributed queuing, (b) provides access to local or metropolitan area networks, and (c) supports connectionless data transfer, connection-oriented data transfer, and isochronous communications, such as voice communications.\nIEEE 802.6 is an example of a network providing DQDB access methods.\nConcept of operation.\nThe DQDB medium access control (MAC) algorithm is generally credited to Robert Newman who developed this algorithm in his PhD thesis in the 1980s at the University of Western Australia. To appreciate the innovative value of the DQDB MAC algorithm, it must be seen against the background of LAN protocols at that time, which were based on broadcast (such as IEEE 802.3 Ethernet) or a ring (like IEEE 802.5 Token Ring and FDDI). The DQDB may be thought of as two token rings, one carrying data in each direction around the ring. This improves reliability which is important in Metropolitan Area Networks (MAN), where repairs may take longer than in a LAN and Wi-Fi because the damage may be inaccessible.\nThe DQDB standard IEEE 802.6 was developed while ATM (Broadband ISDN) was still in early development, but there was strong interaction between the two standards. ATM cells and DQDB frames were harmonized. They both settled on essentially a 48-byte data frame with a 5-byte header. In the DQDB algorithm, a distributed queue was implemented by communicating queue state information via the header. Each node in a DQDB network maintains a pair of state variables which represent its position in the distributed queue and the size of the queue. The headers on the reverse bus communicated requests to be inserted in the distributed queue so that upstream nodes would know that they should allow DQDB cells to pass unused on the forward bus. The algorithm was remarkable for its extreme simplicity.\nCurrently DQDB systems are being installed by many carriers in entire cities, with lengths that reach up to with speeds of a DS3 line (44.736\u00a0Mbit/s). Other implementations use optical fiber for a length of up to 100\u00a0km and speeds around 150\u00a0Mbit/s.\nReferences.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41056", "revid": "32654661", "url": "https://en.wikipedia.org/wiki?curid=41056", "title": "Distributed switching", "text": "Distributed switching is an architecture in which multiple processor-controlled switching units are distributed. There is often a hierarchy of switching elements, with a centralized host switch and with remote switches located close to concentrations of users.\nUse in telephony networks.\nDistributed switching is often used in telephone networks, though it is often called \"host-remote switching\".\nIn rural areas, population centers tend to be too small for economical deployment of a full-featured dedicated telephone exchange, and distances between these centers make transmission costs relatively high. Normal telephone traffic patterns show that most calling is done between people in a community of interest, in this case a geographical one: the population center. Use of distributed switching allows for the majority of calls that are local to that population center to be switched there without needing to be transported to and from the host switch.\nThe host switch provides connectivity between the remote switches and to the larger network, and the host may also directly handle some rare and complex call types (conference calling, for example) that the remote itself is not equipped to handle. Host switches also perform OAM&amp;P (Operation, Administration, Maintenance, and Provisioning) functions, including billing, for the entire cluster of the host and its remote switches.\nA key capability of a remote switch is the ability to act in emergency standalone (ESA) mode, wherein local calls can still be placed even in the event that the connection between that remote and the host has been lost. In this mode, only local calling is available anyway, so the billing capability of the host switch is not required. ESA is increasingly available on digital loop carrier platforms as well as on purpose-built remote switches in order to improve the scope of their utility.\nUse within telecommunications equipment platforms.\nMany data-centric telecommunications platforms such as routers and Ethernet switches utilize distributed switching on separate cards within an equipment chassis. Even when this is used, it is common to have a centralized switching fabric to interconnect the distributed switches. This architecture has become less common as backplane bus speeds and centralized switch fabric capacities have increased."}
{"id": "41057", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41057", "title": "Disturbance voltage", "text": "In telecommunications, a disturbance voltage is an unwanted voltage induced in a system by natural or man-made sources.\nIn telecommunications systems, the disturbance voltage creates currents that limit or interfere with the interchange of information. An example of a disturbance voltage is a voltage that produces (a) false signals in a telephone, (b) Noise (radio) in a radio receiver, or (c) distortion in a received signal."}
{"id": "41058", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41058", "title": "Diurnal phase shift", "text": "In telecommunications, diurnal phase shift is the phase shift of electromagnetic signals associated with daily changes in the ionosphere. The major changes usually occur during the period of time when sunrise or sunset is present at critical points along the path. Significant phase shifts may occur on paths wherein a reflection area of the path is subject to a large tidal range. In cable systems, significant phase shifts can be occasioned by diurnal temperature variance.\nReferences.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "41060", "revid": "285145", "url": "https://en.wikipedia.org/wiki?curid=41060", "title": "Diversity transmission", "text": ""}
{"id": "41061", "revid": "27823944", "url": "https://en.wikipedia.org/wiki?curid=41061", "title": "Department of Defense master clock", "text": "Atomic master clock for the U.S. Department of Defense\nThe Department of Defense master clock is the atomic master clock to which time and frequency measurements for the United States Department of Defense are referenced.\nLocated in Washington D.C., the U.S. Naval Observatory master clock is designated as the \"DOD Master Clock\". It is one of the two standard time and frequency references for the U.S. Government in accordance with Federal Standard 1002-A. The other standard time and frequency reference for the U.S. Government is the National Institute of Standards and Technology (NIST) master clock.\nThe U.S. Naval Observatory also maintains an alternate clock designated \"USNO Alternate Master Clock\" at Schriever Space Force Base, Colorado.\nReferences.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41062", "revid": "757572", "url": "https://en.wikipedia.org/wiki?curid=41062", "title": "Double-ended synchronization", "text": "Synchronization control scheme\nFor two connected exchanges in a communications network, a double-ended synchronization (also called double-ended control) is a synchronization control scheme in which the phase error signals used to control the clock at one telephone exchange are derived by comparison with the phase of the incoming digital signal and the phase of the internal clocks at both exchanges."}
{"id": "41063", "revid": "41798688", "url": "https://en.wikipedia.org/wiki?curid=41063", "title": "Double-sideband reduced-carrier transmission", "text": "Type of amplitude modulation\nDouble-sideband reduced carrier transmission (DSB-RC): transmission in which (a) the frequencies produced by amplitude modulation are symmetrically spaced above and below the carrier and (b) the carrier level is reduced for transmission at a fixed level below that which is provided to the modulator. \n\"Note:\" In DSB-RC transmission, the carrier is usually transmitted at a level suitable for use as a reference by the receiver, except for the case in which it is reduced to the minimum practical level, i.e. the carrier is suppressed."}
{"id": "41064", "revid": "18731412", "url": "https://en.wikipedia.org/wiki?curid=41064", "title": "Double-sideband suppressed-carrier transmission", "text": "Transmission method used in amateur radio\nDouble-sideband suppressed-carrier transmission (DSB-SC) is transmission in which frequencies produced by amplitude modulation (AM) are symmetrically spaced above and below the carrier frequency and the carrier level is reduced to the lowest practical level, ideally being completely suppressed.\nIn DSB-SC, unlike simple AM, the wave carrier is not transmitted; thus, much of the power is distributed between the side bands, which implies an increase of arial coverage in DSB-SC, for the same power consumption.\nDSB-SC transmission is a special case of double-sideband reduced carrier transmission. It is used for radio data systems. This mode is frequently used in amateur radio voice communications, especially on high-frequency bands.\nSpectrum.\nDSB-SC is basically an amplitude modulation wave without the carrier, therefore reducing power waste, giving it a 50% efficiency. This is an increase compared to normal AM transmission (DSB) that has a maximum efficiency of 33.333%, since 2/3 of the power is in the carrier which conveys no useful information and both sidebands containing identical copies of the same information. Single sideband suppressed-carrier (SSB-SC) is 100% efficient.\nSpectrum plot of a DSB-SC signal:\nGeneration.\nDSB-SC is generated by a mixer. The signal produced is the product of the message signal and a carrier signal. The mathematical representation of this process is shown below, where the product-to-sum trigonometric identity is used.\nformula_1\nDemodulation.\nIn DSBSC, coherent demodulation is achieved by multiplying the DSB-SC signal with the carrier signal of the same phase as in the modulation process, analogous to the modulation process. This resultant signal is passed through a low pass filter to produce a scaled version of the original message signal:\nformula_2\nformula_3\nThis equation shows that by multiplying the modulated signal by the carrier signal, the result is a scaled version of the original message signal plus a second term. Since formula_4, this second term is much higher in frequency than the original message. Once this signal passes through a low pass filter, the higher frequency component is removed, leaving just the original message.\nDistortion and attenuation.\nFor demodulation, the demodulation oscillator's frequency and phase must be exactly the same as the modulation oscillator's, otherwise, distortion and/or attenuation will occur.\nTo see this effect, take the following conditions:\nThe resultant signal can then be given by\nformula_8\nformula_9\nformula_10\nThe formula_11 terms results in distortion and attenuation of the original message signal. In particular, if the frequencies are correct, but the phase is wrong, contribution from formula_12 is a constant attenuation factor, also formula_13 represents a cyclic inversion of the recovered signal, which is a serious form of distortion.\nWaveforms.\nBelow is a message signal that one may wish to modulate onto a carrier, consisting of a couple of sinusoidal components with frequencies respectively 800 Hz and 1200 Hz.\nThe equation for this message signal is formula_14.\nThe carrier, in this case, is a plain 5\u00a0kHz (formula_15) sinusoid\u2014pictured below.\nThe modulation is performed by multiplication in the time domain, which yields a 5\u00a0kHz carrier signal, whose amplitude varies in the same manner as the message signal.\nformula_16\nThe name \"suppressed carrier\" comes about because the carrier signal component is suppressed\u2014it does not appear in the output signal. This is apparent when the spectrum of the output signal is viewed. In the picture shown below we see four peaks, the two peaks below 5000 Hz are the lower sideband (LSB) and the two peaks above 5000 Hz are the upper sideband (USB), but there is no peak at the 5000 Hz mark, which is the frequency of the suppressed carrier.\nReferences.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41065", "revid": "102175", "url": "https://en.wikipedia.org/wiki?curid=41065", "title": "Doubly clad fiber", "text": ""}
{"id": "41066", "revid": "666342", "url": "https://en.wikipedia.org/wiki?curid=41066", "title": "D region", "text": ""}
{"id": "41067", "revid": "50797039", "url": "https://en.wikipedia.org/wiki?curid=41067", "title": "Drift", "text": "Drift or Drifts may refer to:\n&lt;templatestyles src=\"Template:TOC_right/styles.css\" /&gt;\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "41068", "revid": "49703205", "url": "https://en.wikipedia.org/wiki?curid=41068", "title": "Drop (liquid)", "text": "Small unit of liquid\nA drop or droplet is a small column of liquid, bounded completely or almost completely by free surfaces. A drop may form when liquid accumulates at the end of a tube or other surface boundary, producing a hanging drop called a pendant drop. Drops may also be formed by the condensation of a vapor or by atomization of a larger mass of solid. Water vapor will condense into droplets depending on the temperature. The temperature at which droplets form is called the dew point.\nSurface tension.\nLiquid forms drops because it exhibits surface tension.\nA simple way to form a drop is to allow liquid to flow slowly from the lower end of a vertical tube of small diameter. The surface tension of the liquid causes the liquid to hang from the tube, forming a pendant. When the drop exceeds a certain size it is no longer stable and detaches itself. The falling liquid is also a drop held together by surface tension.\nViscosity and pitch drop experiments.\nSome substances that appear to be solid, can be shown to instead be extremely viscous liquids, because they form drops and display droplet behavior. In the famous pitch drop experiments, pitch \u2013 a substance somewhat like solid bitumen \u2013 is shown to be a liquid in this way. Pitch in a funnel slowly forms droplets, each droplet taking about 10 years to form and break off.\nPendant drop test.\nIn the pendant drop test, a drop of liquid is suspended from the end of a tube or by any surface by surface tension. The force due to surface tension is proportional to the length of the boundary between the liquid and the tube, with the proportionality constant usually denoted formula_1. Since the length of this boundary is the circumference of the tube, the force due to surface tension is given by\n formula_2\nwhere \"d\" is the tube diameter.\nThe mass \"m\" of the drop hanging from the end of the tube can be found by equating the force due to gravity (formula_3) with the component of the surface tension in the vertical direction (formula_4) giving the formula\n formula_5\nwhere \u03b1 is the angle of contact with the tube's front surface, and \"g\" is the acceleration due to gravity.\nThe limit of this formula, as \u03b1 goes to 90\u00b0, gives the maximum weight of a pendant drop for a liquid with a given surface tension, formula_1.\n formula_7\nThis relationship is the basis of a convenient method of measuring surface tension, commonly used in the petroleum industry. More sophisticated methods are available to take account of the developing shape of the pendant as the drop grows. These methods are used if the surface tension is unknown.\nDrop adhesion to a solid.\nThe drop adhesion to a solid can be divided into two categories: lateral adhesion and normal adhesion. Lateral adhesion resembles friction (though tribologically lateral adhesion is a more accurate term) and refers to the force required to slide a drop on the surface, namely the force to detach the drop from its position on the surface only to translate it to another position on the surface. Normal adhesion is the adhesion required to detach a drop from the surface in the normal direction, namely the force to cause the drop to fly off from the surface. The measurement of both adhesion forms can be done with the Centrifugal Adhesion Balance (CAB). The CAB uses a combination of centrifugal and gravitational forces to obtain any ratio of lateral and normal forces. For example, it can apply a normal force at zero lateral force for the drop to fly off away from the surface in the normal direction or it can induce a lateral force at zero normal force (simulating zero gravity).\nDroplet.\nThe term droplet is a diminutive form of 'drop' \u2013 and as a guide is typically used for liquid particles of less than 500\u00a0\u03bcm diameter. In spray application, droplets are usually described by their perceived size (i.e., diameter) whereas the dose (or number of infective particles in the case of biopesticides) is a function of their volume. This increases by a cubic function relative to diameter; thus, a 50\u00a0\u03bcm droplet represents a dose in 65\u00a0pl and a 500\u00a0\u03bcm drop represents a dose in 65 nanolitres.\nSpeed.\nA droplet with a diameter of 3\u00a0mm has a terminal velocity of approximately 8\u00a0m/s.\nDrops smaller than 1 mm in diameter will attain 95% of their terminal velocity within 2 m. But above this size the distance to get to terminal velocity increases sharply. An example is a drop with a diameter of 2 mm that may achieve this at 5.6 m.\nOptics.\nDue to the different refractive index of water and air, refraction and reflection occur on the surfaces of raindrops, leading to rainbow formation.\nSound.\nThe major source of sound when a droplet hits a liquid surface is the resonance of excited bubbles trapped underwater. These oscillating bubbles are responsible for most liquid sounds, such as running water or splashes, as they actually consist of many drop-liquid collisions.\n\"Dripping tap\" noise prevention.\nReducing the surface tension of a body of liquid makes possible to reduce or prevent noise due to droplets falling into it. This would involve adding soap, detergent or a similar substance to water. The reduced surface tension reduces the noise from dripping.\nShape.\nThe classic shape associated with a drop (with a pointy end in its upper side) comes from the observation of a droplet clinging to a surface. The shape of a drop falling through a gas is actually more or less spherical for drops less than 2\u00a0mm in diameter. Larger drops tend to be flatter on the bottom part due to the pressure of the gas they move through. As a result, as drops get larger, a concave depression forms which leads to the eventual breakup of the drop.\nCapillary length.\nThe capillary length is a length scaling factor that relates gravity, density, and surface tension, and is directly responsible for the shape a droplet for a specific fluid will take. The capillary length stems from the Laplace pressure, using the radius of the droplet.\nUsing the capillary length we can define microdrops and macrodrops. Microdrops are droplets with radius smaller than the capillary length, where the shape of the droplet is governed by surface tension and they form a more or less spherical cap shape. If a droplet has a radius larger than the capillary length, they are known as macrodrops and the gravitational forces will dominate. Macrodrops will be 'flattened' by gravity and the height of the droplet will be reduced.\nSize.\nRaindrop sizes typically range from 0.5\u00a0mm to 4\u00a0mm, with size distributions quickly decreasing past diameters larger than 2\u20132.5\u00a0mm.\nScientists traditionally thought that the variation in the size of raindrops was due to collisions on the way down to the ground. In 2009, French researchers succeeded in showing that the distribution of sizes is due to the drops' interaction with air, which deforms larger drops and causes them to fragment into smaller drops, effectively limiting the largest raindrops to about 6\u00a0mm diameter. However, drops up to 10\u00a0mm (equivalent in volume to a sphere of radius 4.5\u00a0mm) are theoretically stable and could be levitated in a wind tunnel.\nThe largest recorded raindrop was 8.8\u00a0mm in diameter, located at the base of a cumulus congestus cloud in the vicinity of Kwajalein Atoll in July 1999. A raindrop of identical size was detected over northern Brazil in September 1995.\nStandardized droplet sizes in medicine.\nIn medicine, this property is used to create droppers and IV infusion sets which have a standardized diameter, in such a way that 1 millilitre is equivalent to 20 drops. When smaller amounts are necessary (such as paediatrics), microdroppers or paediatric infusion sets are used, in which 1 millilitre = 60 microdrops."}
{"id": "41069", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41069", "title": "Drop and insert", "text": "In a multichannel transmission system, drop and insert is a process that diverts (\"drop\") a portion of the multiplexed aggregate signal at an intermediate point, and introduces (\"insert\") a different signal for subsequent transmission in the same position.\nDrop and insert is practiced, for example, in time-division multiplexing (TDM) when a time slot or frequency band is replaced from another source. The diverted signal may be demodulated or reinserted into another transmission system in the same or another time slot or frequency band. The time slot or frequency band vacated by the diverted signal need not necessarily be reoccupied by another signal. Likewise, a previously unoccupied time slot or frequency band may be occupied by a signal inserted at the drop-and-insert point.\nSignals not of interest at the drop-and-insert point are not diverted.\nReferences.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41070", "revid": "29918470", "url": "https://en.wikipedia.org/wiki?curid=41070", "title": "Dropout", "text": "Dropout or drop out may refer to:\n&lt;templatestyles src=\"Template:TOC_right/styles.css\" /&gt;\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "41071", "revid": "47198456", "url": "https://en.wikipedia.org/wiki?curid=41071", "title": "DTE", "text": "DTE may refer to:\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "41072", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41072", "title": "Dual access", "text": "In telecommunications, the term dual access has the following meanings: \nAlso, network hardware company D-Link has named technology which allows two simultaneous connections over one cable, for example 1) Internet and 2) provider's local FTP or game servers or IPTV data flow.\nReferences.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "41073", "revid": "43007828", "url": "https://en.wikipedia.org/wiki?curid=41073", "title": "Dual in-line package", "text": "Type of electronic component package\nIn microelectronics, a dual in-line package (DIP or DIL) is an electronic component package with a rectangular housing and two parallel rows of electrical connecting pins. The package may be through-hole mounted to a printed circuit board (PCB) or inserted in a socket. The dual-inline format was invented by Don Forbes, Rex Rice and Bryant Rogers at Fairchild R&amp;D in 1964, when the restricted number of leads available on circular transistor-style packages became a limitation in the use of integrated circuits. Increasingly complex circuits required more signal and power supply leads (as observed in Rent's rule); eventually microprocessors and similar complex devices required more leads than could be put on a DIP package, leading to development of higher-density chip carriers. Furthermore, square and rectangular packages made it easier to route printed-circuit traces beneath the packages.\nA DIP is usually referred to as a DIP\"n\", where \"n\" is the total number of pins, and sometimes appended with the row-to-row package width \"N\" for narrow (0.3\") or \"W\" for wide (0.6\"). For example, a microcircuit package with two rows of seven vertical leads would be a DIP14 or DIP14N. The photograph at the upper right shows three DIP14 ICs. Common packages have as few as four and as many as 64 leads. Many analog and digital integrated circuit types are available in DIP packages, as are arrays of transistors, switches, light emitting diodes, and resistors. DIP plugs for ribbon cables can be used with standard IC sockets.\nDIP packages are usually made from an opaque molded epoxy plastic pressed around a tin-, silver-, or gold-plated lead frame that supports the device die and provides connection pins. Some types of IC are made in ceramic DIP packages, where high temperature or high reliability is required, or where the device has an optical window to the interior of the package. Most DIP packages are secured to a PCB by inserting the pins through holes in the board and soldering them in place. Where replacement of the parts is necessary, such as in test fixtures or where programmable devices must be removed for changes, a DIP socket is used. Some sockets include a zero insertion force (ZIF) mechanism.\nVariations of the DIP package include those with only a single row of pins, e.g. a resistor array, possibly including a heat sink tab in place of the second row of pins, and types with four rows of pins, two rows, staggered, on each side of the package. DIP packages have been mostly displaced by surface-mount package types, which avoid the expense of drilling holes in a PCB and which allow higher density of interconnections.\nApplications.\nTypes of devices.\nDIPs are commonly used for integrated circuits (ICs). Other devices in DIP packages include resistor networks, DIP switches, LED segmented and bar graph displays, and electromechanical relays.\nDIP connector plugs for ribbon cables are common in computers and other electronic equipment.\nDallas Semiconductor manufactured integrated DIP real-time clock (RTC) modules which contained an IC chip and a non-replaceable 10-year lithium battery.\nDIP header blocks onto which discrete components could be soldered were used where groups of components needed to be easily removed, for configuration changes, optional features or calibration.\nUses.\nThe original dual-in-line package was invented by Bryant \"Buck\" Rogers in 1964 while working for Fairchild Semiconductor. The first devices had 14 pins and looked much like they do today. The rectangular shape allowed integrated circuits to be packaged more densely than previous round packages. The package was well-suited to automated assembly equipment; a PCB could be populated with scores or hundreds of ICs, then all the components on the circuit board could be soldered at one time on a wave soldering machine and passed on to automated testing machines, with very little human labor required. DIP packages were still large with respect to the integrated circuits within them. By the end of the 20th century, surface-mount packages allowed further reduction in the size and weight of systems. DIP chips are still popular for circuit prototyping on a breadboard because of how easily they can be inserted and used there.\nDIPs were the mainstream of the microelectronics industry in the 1970s and 1980s. Their use has declined in the first decade of the 21st century due to the emerging new surface-mount technology (SMT) packages such as plastic leaded chip carrier (PLCC) and small-outline integrated circuit (SOIC), though DIPs continued in extensive use through the 1990s, and still continue to be used today. Because some modern chips are available only in surface-mount package types, a number of companies sell various prototyping adapters to allow those surface-mount devices (SMD) to be used like DIP devices with through-hole breadboards and soldered prototyping boards (such as stripboard and perfboard). (SMT can pose quite a problem, at least an inconvenience, for prototyping in general; most of the characteristics of SMT that are advantages for mass production are difficulties for prototyping.)\nFor programmable devices like EPROMs and GALs, DIPs remained popular for many years due to their easy handling with external programming circuitry (i.e., the DIP devices could be simply plugged into a socket on the programming device.) However, with in-system programming (ISP) technology now state-of-the-art, this advantage of DIPs is rapidly losing importance as well.\nThrough the 1990s, devices with fewer than 20 leads were typically manufactured in a DIP format in addition to the newer formats. Since about 2000, newer devices are often unavailable in the DIP format.\nMounting.\nDIPs can be mounted either by through-hole soldering or in sockets. Sockets allow easy replacement of a device and eliminates the risk of damage from overheating during soldering. Generally sockets were used for high-value or large ICs, which cost much more than the socket. Where devices would be frequently inserted and removed, such as in test equipment or EPROM programmers, a zero insertion force socket would be used.\nDIPs are also used with breadboards, a temporary mounting arrangement for education, design development or device testing. Some hobbyists, for one-off construction or permanent prototyping, use point-to-point wiring with DIPs, and their appearance when physically inverted as part of this method inspires the informal term \"dead bug style\" for the method.\nConstruction.\nThe body (housing) of a DIP containing an IC chip is usually made from molded plastic or ceramic. The hermetic nature of a ceramic housing is preferred for extremely high reliability devices. However, the vast majority of DIPs are manufactured via a thermoset molding process in which an epoxy mold compound is heated and transferred under pressure to encapsulate the device. Typical cure cycles for the resins are less than 2 minutes and a single cycle may produce hundreds of devices.\nThe leads emerge from the longer sides of the package along the seam, parallel to the top and bottom planes of the package, and are bent downward approximately 90 degrees (or slightly less, leaving them angled slightly outward from the centerline of the package body). (The SOIC, the SMT package that most resembles a typical DIP, appears essentially the same, notwithstanding size scale, except that after being bent down the leads are bent upward again by an equal angle to become parallel with the bottom plane of the package.) In ceramic (CERDIP) packages, an epoxy or grout is used to hermetically seal the two halves together, providing an air and moisture tight seal to protect the IC die inside. Plastic DIP (PDIP) packages are usually sealed by fusing or cementing the plastic halves around the leads, but a high degree of hermeticity is not achieved because the plastic itself is usually somewhat porous to moisture and the process cannot ensure a good microscopic seal between the leads and the plastic at all points around the perimeter. However, contaminants are usually still kept out well enough that the device can operate reliably for decades with reasonable care in a controlled environment.\nInside the package, the lower half has the leads embedded, and at the center of the package is a rectangular space, chamber, or void into which the IC die is cemented. The leads of the package extend diagonally inside the package from their positions of emergence along the periphery to points along a rectangular perimeter surrounding the die, tapering as they go to become fine contacts at the die. Ultra-fine bond wires (barely visible to the naked human eye) are welded between these die periphery contacts and bond pads on the die itself, connecting one lead to each bond pad, and making the final connection between the microcircuits and the external DIP leads. The bond wires are not usually taut but loop upward slightly to allow slack for thermal expansion and contraction of the materials; if a single bond wire breaks or detaches, the entire IC may become useless. The top of the package covers all of this delicate assemblage without crushing the bond wires, protecting it from contamination by foreign materials.\nUsually, a company logo, alphanumeric codes and sometimes words are printed on top of the package to identify its manufacturer and type, when it was made (usually as a year and a week number), sometimes where it was made, and other proprietary information (perhaps revision numbers, manufacturing plant codes, or stepping ID codes.)\nThe necessity of laying out all of the leads in a basically radial pattern in a single plane from the die perimeter to two rows on the periphery of the package is the main reason that DIP packages with higher lead counts must have wider spacing between the lead rows, and it effectively limits the number of leads which a practical DIP package may have. Even for a very small die with many bond pads (e.g. a chip with 15 inverters, requiring 32 leads), a wider DIP would still be required to accommodate the radiating leads internally. This is one of the reasons that four-sided and multiple rowed packages, such as PGAs, were introduced (around the early 1980s).\nA large DIP package (such as the DIP64 used for the Motorola 68000 CPU) has long leads inside the package between pins and the die, making such a package unsuitable for high speed devices.\nSome other types of DIP devices are built very differently. Most of these have molded plastic housings and straight leads or leads that extend directly out of the bottom of the package. For some, LED displays particularly, the housing is usually a hollow plastic box with the bottom/back open, filled (around the contained electronic components) with a hard translucent epoxy material from which the leads emerge. Others, such as DIP switches, are composed of two (or more) plastic housing parts snapped, welded, or glued together around a set of contacts and tiny mechanical parts, with the leads emerging through molded-in holes or notches in the plastic.\nVariants.\nSeveral DIP variants for ICs exist, mostly distinguished by packaging material:\nEPROMs were sold in ceramic DIPs manufactured with a circular window of clear quartz over the chip die to allow the part to be erased by ultraviolet light. Often, the same chips were also sold in less expensive windowless PDIP or CERDIP packages as one-time programmable (OTP) versions. Windowed and windowless packages were also used for microcontrollers, and other devices, containing EPROM memory. Windowed CERDIP-packaged EPROMs were used for the BIOS ROM of many early IBM PC clones with an adhesive label covering the window to prevent inadvertent erasure through exposure to ambient light.\nMolded plastic DIPs are much lower in cost than ceramic packages; one 1979 study showed that a plastic 14 pin DIP cost around US$0.063 and a ceramic package cost US$0.82.\nSingle in-line.\nA single in-line package (SIP or SIL package) has one row of connecting pins. It is not as popular as the DIP, but has been used for packaging RAM chips and multiple resistors with a common pin. As compared to DIPs with a typical maximum pin count of 64, SIPs have a typical maximum pin count of 24 with lower package costs.\nOne variant of the single in-line package uses part of the lead frame for a heat sink tab. This multi-leaded power package is useful for such applications as audio power amplifiers, for example.\nQuad in-line.\nThe QIP, sometimes called a \"QIL package\", has the same dimensions as a DIL package, but the leads on each side are bent into an alternating zigzag configuration so as to fit four lines of solder pads (instead of two with a DIL). The QIL design increased the spacing between solder pads without increasing package size, for two reasons: \nLead count and spacing.\nCommonly found DIP packages that conform to JEDEC standards use an inter-lead spacing (lead pitch) of (JEDEC MS-001BA). Row spacing varies depending on lead counts, with 0.3 in. (7.62\u00a0mm) (JEDEC MS-001) or 0.6\u00a0inch (15.24\u00a0mm) (JEDEC MS-011) the most common. Less common standardized row spacings include 0.4\u00a0inch (10.16\u00a0mm) (JEDEC MS-010) and 0.9\u00a0inch (22.86\u00a0mm), as well as a row spacing of 0.3\u00a0inch, 0.6\u00a0inch or 0.75\u00a0inch with a 0.07\u00a0inch (1.778\u00a0mm) lead pitch.\nThe former Soviet Union and Eastern bloc countries used similar packages, but with a metric pin-to-pin spacing of 2.5\u00a0mm rather than .\nThe number of leads is always even. For 0.3\u00a0inch wide rows, typical lead counts are 8, 14, 16, and 20; less common are 4, 6, 18, 24, and 28 lead counts. To have an even number of leads some DIPs have unused not connected (NC) leads to the internal chip, or are duplicated, e.g. two ground pins. For 0.6\u00a0inch rows, typical lead counts are 24, 28, 32, and 40; less common are 36, 42, 48, 52, and 64 lead counts. For 0.9 inch rows, typical lead counts are 50 and 64 leads. Some microprocessors, such as the Motorola 68000 and Zilog Z180, used lead counts of 64; this is typically the maximum number of leads for a DIP package.\nOrientation and lead numbering.\nAs shown in the diagram, leads are numbered consecutively from Pin 1. When the identifying notch in the package is at the top, Pin 1 is the top left corner of the device. Sometimes Pin 1 is identified with an indent or paint dot mark.\nFor example, for a 14-lead DIP, with the notch at the top, the left leads are numbered from 1 to 7 (top to bottom) and the right row of leads are numbered 8 to 14 (bottom to top).\nLeads are skipped on some DIP devices (e.g. segmented LED displays, relays, or devices that replace leads with a heat sink fin). The remaining leads are numbered as if all positions had leads.\nIn addition to providing for human visual identification of the orientation of the package, the notch allows automated chip-insertion machinery to confirm correct orientation of the chip by mechanical sensing.\nDescendants.\nThe SOIC (Small Outline IC), a surface-mount package which is currently very popular, particularly in consumer electronics and personal computers, is essentially a shrunk version of the standard IC PDIP, the fundamental difference which makes it an SMT device being a second bend in the leads to flatten them parallel to the bottom plane of the plastic housing. The SOJ (Small Outline J-lead) and other SMT packages with \"SOP\" (for \"Small Outline Package\") in their names can be considered further relatives of the DIP, their original ancestor. SOIC packages tend to have half the pitch of DIP, and SOP are half that, a fourth of DIP. (0.1\"/2.54\u00a0mm, 0.05\"/1.27\u00a0mm, and 0.025\"/0.635\u00a0mm, respectively)\nPin grid array (PGA) packages may be considered to have evolved from the DIP. PGAs with the same pin centers as most DIPs were popular for microprocessors from the early to mid-1980s through the 1990s. Owners of personal computers containing Intel 80286 through P5 Pentium processors may be most familiar with these PGA packages, which were often inserted into ZIF sockets on motherboards. The similarity is such that a PGA socket may be physically compatible with some DIP devices, though the converse is rarely true.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41074", "revid": "11292982", "url": "https://en.wikipedia.org/wiki?curid=41074", "title": "Dual-tone multifrequency signaling", "text": ""}
{"id": "41075", "revid": "47130321", "url": "https://en.wikipedia.org/wiki?curid=41075", "title": "Duct", "text": "The word duct is derived from the Latin word for \"led/leading\". It may refer to:\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "41076", "revid": "668752", "url": "https://en.wikipedia.org/wiki?curid=41076", "title": "Duobinary signal", "text": ""}
{"id": "41077", "revid": "1883085", "url": "https://en.wikipedia.org/wiki?curid=41077", "title": "Duplexer", "text": "Device that allows bi-directional communication over a single path\nA duplexer is an electronic device that allows bi-directional (duplex) communication over a single path. In radar and radio communications systems, it isolates the receiver from the transmitter while permitting them to share a common antenna. Most radio repeater systems include a duplexer. Duplexers can be based on frequency (often a waveguide filter), polarization (such as an orthomode transducer), or timing (as is typical in radar).\nTypes.\nTransmit-receive switch.\nIn radar, a transmit/receive (TR) switch alternately connects the transmitter and receiver to a shared antenna. In the simplest arrangement, the switch consists of a gas-discharge tube across the input terminals of the receiver. When the transmitter is active, the resulting high voltage causes the tube to conduct, shorting together the receiver terminals to protect it, while its complementary, the anti-transmit/receive (ATR) switch, is a similar discharge tube which decouples the transmitter from the antenna while not operating, to prevent it from wasting received energy.\nHybrid.\nA hybrid, such as a magic T, may be used as a duplexer by terminating the fourth port in a matched load.\nThis arrangement suffers from the disadvantage that half of the transmitter power is lost in the matched load, while thermal noise in the load is delivered to the receiver.\nFrequency domain.\nIn radio communications (as opposed to radar), the transmitted and received signals can occupy different frequency bands, and so may be separated by frequency-selective filters. These are effectively a higher-performance version of a diplexer, typically with a narrow split between the two frequencies in question (typically around 2%-5% for a commercial two-way radio system).\nWith a duplexer the high- and low-frequency signals are traveling in opposite directions at the shared port of the duplexer.\nModern duplexers often use nearby frequency bands, so the frequency separation between the two ports is also much less. For example, the transition between the uplink and downlink bands in the GSM frequency bands may be about one percent (915\u00a0MHz to 925\u00a0MHz). Significant attenuation (isolation) is needed to prevent the transmitter's output from overloading the receiver's input, so such duplexers employ multi-pole filters. Duplexers are commonly made for use on the 30-50 MHz (\"low band\"), 136-174 MHz (\"high band\"), 380-520 MHz (\"UHF\"), plus the 790\u2013862 MHz (\"800\"), 896-960 MHz (\"900\") and 1215-1300 MHz (\"1200\") bands.\nThere are two predominant types of duplexer in use: notch duplexers, which block just those frequencies used for communication in the opposite direction, and bandpass duplexers, which pass only the wanted frequency band.\nOn shared-antenna sites, the bandpass duplexer variety is greatly preferred because this virtually eliminates interference between transmitters and receivers by removing out-of-band transmit emissions and considerably improving the selectivity of receivers. Most professionally engineered sites ban the use of notch duplexers and insist on bandpass duplexers for this reason.\n\"Note 1:\" A duplexer must be designed for operation in the frequency band used by the receiver and transmitter, and must be capable of handling the output power of the transmitter.\n\"Note 2:\" A duplexer must provide adequate rejection of transmitter noise occurring at the receive frequency, and must be designed to operate at, or less than, the frequency separation between the transmitter and receiver.\n\"Note 3:\" A duplexer must provide sufficient isolation to prevent receiver desensitization.\nSource: from Federal Standard 1037C\nHistory.\nThe first duplexers were invented for use on the electric telegraph and were known as \"duplex\" rather than \"duplexer\". They were an early form of the hybrid coil. The telegraph companies were keen to have such a device since the ability to have simultaneous traffic in both directions had the potential to save the cost of thousands of miles of telegraph wire. The first of these devices was designed in 1853 by Julius Wilhelm Gintl of the Austrian State Telegraph. Gintl's design was not very successful. Further attempts were made by Carl Frischen of Hanover with an artificial line to balance the real line as well as by Siemens &amp; Halske, who bought and modified Frischen's design. The first truly successful duplex was designed by Joseph Barker Stearns of Boston in 1872. This was further developed into the quadruplex telegraph by Thomas Edison. The device is estimated to have saved Western Union $500,000 per year in construction of new telegraph lines.\nThe first duplexers for radar, sometimes referred to as Transmit/Receive Switches, were invented by Robert Morris Page and Leo C. Young of the United States Naval Research Laboratory in July 1936. The receiver input tube grid current reflected a high impedance to the transmitter, which prevented the transmitter power from burning out the receiver.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41078", "revid": "1296771605", "url": "https://en.wikipedia.org/wiki?curid=41078", "title": "Duty cycle", "text": "Activity fraction of a periodic system\nA duty cycle or power cycle is the fraction of one period in which a signal or system is active. Duty cycle is commonly expressed as a percentage or a ratio. A period is the time it takes for a signal to complete an on-and-off cycle. As a formula, a duty cycle (%) may be expressed as:\nformula_1\nEqually, a duty cycle (ratio) may be expressed as:\nformula_2\nwhere formula_3 is the duty cycle, formula_4 is the pulse width (pulse active time), and formula_5 is the total period of the signal. Thus, a 60% duty cycle means the signal is on 60% of the time and off 40% of the time. The \"on time\" for a 60% duty cycle could be a fraction of a second, a day, or even a week, depending on the length of the period.\nDuty cycles can be used to describe the percent time of an active signal in an electrical device such as the power switch in a switching power supply or the firing of action potentials by a living system such as a neuron.\nSome publications use formula_6 as the symbol for duty cycle.\nAs a ratio, duty cycle is unitless and may be given as decimal fraction and percentage alike.\nAn alternative term in use is duty factor.\nApplications.\nElectrical and electronics.\nIn electronics, duty cycle is the percentage of the ratio of pulse duration, or pulse width (PW) to the total period (T) of the waveform. It is generally used to represent time duration of a pulse when it is high (1). In digital electronics, signals are used in rectangular waveform which are represented by logic 1 and logic 0. Logic 1 stands for presence of an electric pulse and 0 for absence of an electric pulse. For example, a signal (10101010) has 50% duty cycle, because the pulse remains high for 1/2 of the period and low for 1/2 of the period. Similarly, for pulse (10001000) the duty cycle will be 25% because the pulse remains high only for 1/4 of the period and remains low for 3/4 of the period.\nElectric motors typically use a duty cycle of less than 100%. For example, if a motor runs for one out of 100 seconds (or 1/100 of the time), its duty cycle is 1/100, or 1%.\nPulse-width modulation (PWM) is used in a variety of electronic situations, such as power delivery and voltage regulation.\nIn electronic music, music synthesizers vary the duty cycle of their audio-frequency oscillators to obtain a subtle effect on the tone colors. This technique is known as pulse-width modulation.\nIn the printer / copier industry, the duty cycle specification refers to the rated throughput (that is, printed pages) of a device per month.\nIn a welding power supply, the maximum duty cycle is defined as the percentage of time in a 10-minute period that it can be operated continuously before overheating.\nBiological systems.\nThe concept of duty cycles is also used to describe the activity of neurons and muscle fibers. In neural circuits for example, a duty cycle specifically refers to the proportion of a cycle period in which a neuron remains active.\nGeneration.\nOne way to generate fairly accurate square wave signals with 1/\"n\" duty factor, where \"n\" is an integer, is to vary the duty cycle until the \"n\"th-harmonic is significantly suppressed. For audio-band signals, this can even be done \"by ear\"; for example, a \u221240\u00a0dB reduction in the 3rd harmonic corresponds to setting the duty factor to 1/3 with a precision of 1% and \u221260\u00a0dB reduction corresponds to a precision of 0.1%.\nMark\u2013space ratio.\nMark\u2013space ratio, or mark-to-space ratio, is another term for the same concept, to describe the temporal relationship between two alternating periods of a waveform. However, whereas the duty cycle relates the duration of one period to the duration of the entire cycle, the mark\u2013space ratio relates the durations of the two individual periods:\nformula_7\nwhere formula_8 and formula_9 are the durations of the two alternating periods.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41079", "revid": "7903804", "url": "https://en.wikipedia.org/wiki?curid=41079", "title": "Dynamic range", "text": "Ratio between the largest and smallest values that a certain quantity can assume\nDynamic range (abbreviated DR, DNR, or DYR) is the ratio between the largest and smallest measurable values of a specific quantity. It is often used in the context of signals, like sound and light. It is measured either as a ratio or as a base-10 (decibel) or base-2 (doublings, bits or stops) logarithmic value of the ratio between the largest and smallest signal values.\nElectronically reproduced audio and video is often processed to fit the original material with a wide dynamic range into a narrower recorded dynamic range for easier storage and reproduction. This process is called dynamic range compression.\nHuman perception.\nThe human senses of sight and hearing have a relatively high dynamic range. However, a human cannot perform these feats of perception at both extremes of the scale at the same time. The human eye takes time to adjust to different light levels, and its dynamic range in a given scene is actually quite limited due to optical glare. The instantaneous dynamic range of human audio perception is similarly subject to masking so that, for example, a whisper cannot be heard in loud surroundings.\nA human is capable of hearing (and usefully discerning) anything from a quiet murmur in a soundproofed room to the loudest heavy metal concert. Such a difference can exceed 100\u00a0dB which represents a factor of 100,000 in amplitude and a factor of 10,000,000,000 in power. The dynamic range of human hearing is roughly 140\u00a0dB, varying with frequency, from the threshold of hearing (around \u22129\u00a0dB SPL at 3\u00a0kHz) to the threshold of pain (from 120 to 140\u00a0dB SPL). This wide dynamic range cannot be perceived all at once, however; the tensor tympani, stapedius muscle, and outer hair cells all act as mechanical dynamic range compressors to adjust the sensitivity of the ear to different ambient levels.\nA human can see objects in starlight or in bright sunlight, even though on a moonless night objects receive one billionth (10\u22129) of the illumination they would on a bright sunny day; a dynamic range of 90\u00a0dB. Change of sensitivity is achieved in part through adjustments of the iris and slow chemical changes, which take some time.\nIn practice, it is difficult for humans to achieve the full dynamic experience using electronic equipment. For example, a good quality liquid-crystal display (LCD) has a dynamic range limited to around 1000:1, and some of the latest CMOS image sensors now have measured dynamic ranges of about 23,000:1. Paper reflectance can produce a dynamic range of about 100:1. A professional video camera such as the Sony Digital Betacam achieves a dynamic range of greater than 90\u00a0dB in audio recording.\nAudio.\nAudio engineers use \"dynamic range\" to describe the ratio of the amplitude of the loudest possible undistorted signal to the noise floor, say of a microphone or loudspeaker. Dynamic range is therefore the signal-to-noise ratio (SNR) for the case where the signal is the loudest possible for the system. For example, if the ceiling of a device is 5\u00a0V (rms) and the noise floor is 10\u00a0\u03bcV (rms) then the dynamic range is 500000:1, or 114\u00a0dB:\nformula_1\nIn digital audio theory the dynamic range is limited by quantization error. The maximum achievable dynamic range for a digital audio system with \"Q\"-bit uniform quantization is calculated as the ratio of the largest sine-wave rms to rms noise is:\nformula_2\nHowever, the usable dynamic range may be greater, as a properly dithered recording device can record signals well below the noise floor.\nThe 16-bit compact disc has a theoretical undithered dynamic range of about 96\u00a0dB; however, the \"perceived\" dynamic range of 16-bit audio can be 120\u00a0dB or more with noise-shaped dither, taking advantage of the frequency response of the human ear.\nDigital audio with undithered 20-bit quantization is theoretically capable of 120\u00a0dB dynamic range, while 24-bit digital audio affords 144\u00a0dB dynamic range. Most Digital audio workstations process audio with 32-bit floating-point representation which affords even higher dynamic range and so loss of dynamic range is no longer a concern in terms of digital audio processing. Dynamic range limitations typically result from improper gain staging, recording technique including ambient noise and intentional application of dynamic range compression.\nDynamic range in analog audio is the difference between low-level thermal noise in the electronic circuitry and high-level signal saturation resulting in increased distortion and, if pushed higher, clipping. Multiple noise processes determine the noise floor of a system. Noise can be picked up from microphone self-noise, preamp noise, wiring and interconnection noise, media noise, etc.\nEarly 78 rpm phonograph discs had a dynamic range of up to 40\u00a0dB, soon reduced to 30\u00a0dB and worse due to wear from repeated play. Vinyl microgroove phonograph records typically yield 55-65\u00a0dB, though the first play of the higher-fidelity outer rings can achieve a dynamic range of 70\u00a0dB.\nGerman magnetic tape in 1941 was reported to have had a dynamic range of 60\u00a0dB, though modern-day restoration experts of such tapes note 45-50\u00a0dB as the observed dynamic range. Ampex tape recorders in the 1950s achieved 60\u00a0dB in practical usage, In the 1960s, improvements in tape formulation processes resulted in 7\u00a0dB greater range, and Ray Dolby developed the Dolby A-Type noise reduction system that increased low- and mid-frequency dynamic range on magnetic tape by 10\u00a0dB, and high-frequency by 15\u00a0dB, using companding (compression and expansion) of four frequency bands. The peak of professional analog magnetic recording tape technology reached 90\u00a0dB dynamic range in the midband frequencies at 3% distortion, or about 80\u00a0dB in practical broadband applications. The Dolby SR noise reduction system gave a 20\u00a0dB further increased range resulting in 110\u00a0dB in the midband frequencies at 3% distortion.\nCompact Cassette tape performance ranges from 50 to 56\u00a0dB depending on tape formulation, with type IV tape tapes giving the greatest dynamic range, and systems such as XDR, dbx and Dolby noise reduction system increasing it further. Specialized bias and record head improvements by Nakamichi and Tandberg combined with Dolby C noise reduction yielded 72\u00a0dB dynamic range for the cassette.\nA dynamic microphone is able to withstand high sound intensity and can have a dynamic range of up to 140\u00a0dB. Condenser microphones are also rugged but their dynamic range may be limited by the overloading of their associated electronic circuitry. Practical considerations of acceptable distortion levels in microphones combined with typical practices in a recording studio result in a useful dynamic range of 125\u00a0dB.\nIn 1981, researchers at Ampex determined that a dynamic range of 118\u00a0dB on a dithered digital audio stream was necessary for subjective noise-free playback of music in quiet listening environments.\nSince the early 1990s, it has been recommended by several authorities, including the Audio Engineering Society, that measurements of dynamic range be made with an audio signal present, which is then filtered out in the noise floor measurement used in determining dynamic range. This avoids questionable measurements based on the use of blank media, or muting circuits.\nThe term \"dynamic range\" may be confusing in audio production because it has two conflicting definitions, particularly in the understanding of the loudness war phenomenon. \"Dynamic range\" may refer to micro-dynamics, related to crest factor, whereas the European Broadcasting Union, in EBU3342 Loudness Range, defines \"dynamic range\" as the difference between the quietest and loudest volume, a matter of macro-dynamics.\nElectronics.\nIn electronics dynamic range is used in the following contexts: \nIn audio and electronics applications, the ratio involved is often large enough that it is converted to a logarithm and specified in decibels.\nMetrology.\nIn metrology, such as when performed in support of science, engineering or manufacturing objectives, dynamic range refers to the range of values that can be measured by a sensor or metrology instrument. Often this dynamic range of measurement is limited at one end of the range by saturation of a sensing signal sensor or by physical limits that exist on the motion or other response capability of a mechanical indicator. The other end of the dynamic range of measurement is often limited by one or more sources of random noise or uncertainty in signal levels that may be described as defining the sensitivity of the sensor or metrology device. When digital sensors or sensor signal converters are a component of the sensor or metrology device, the dynamic range of measurement will be also related to the number of binary digits (bits) used in a digital numeric representation in which the measured value is linearly related to the digital number. For example, a 12-bit digital sensor or converter can provide a dynamic range in which the ratio of the maximum measured value to the minimum measured value is up to 212 = 4096.\nMetrology systems and devices may use several basic methods to increase their basic dynamic range. These methods include averaging and other forms of filtering, correction of receivers characteristics, repetition of measurements, nonlinear transformations to avoid saturation, etc. In more advance forms of metrology, such as multiwavelength digital holography, interferometry measurements made at different scales (different wavelengths) can be combined to retain the same low-end resolution while extending the upper end of the dynamic range of measurement by orders of magnitude.\nMusic.\nIn music, dynamic range describes the difference between the quietest and loudest volume of an instrument, part or piece of music. In modern recording, this range is often limited through dynamic range compression, which allows for louder volume, but can make the recording sound less exciting or live.\nThe dynamic range of music as normally perceived in a concert hall does not exceed 80\u00a0dB, and human speech is normally perceived over a range of about 40\u00a0dB.\nPhotography.\nPhotographers use \"dynamic range\" to describe the luminance range of a scene being photographed, or the limits of luminance range that a given digital camera or film can capture, or the opacity range of developed film images, or the reflectance range of images on photographic papers.\nThe dynamic range of digital photography is comparable to the capabilities of photographic film and both are comparable to the capabilities of the human eye.\nThere are photographic techniques that support even higher dynamic range. \nConsumer-grade image file formats sometimes restrict dynamic range. The most severe dynamic-range limitation in photography may not involve encoding, but rather reproduction to, say, a paper print or computer screen. In that case, not only local tone mapping but also \"dynamic range adjustment\" can be effective in revealing detail throughout light and dark areas: The principle is the same as that of dodging and burning (using different lengths of exposures in different areas when making a photographic print) in the chemical darkroom. The principle is also similar to gain riding or automatic level control in audio work, which serves to keep a signal audible in a noisy listening environment and to avoid peak levels that overload the reproducing equipment, or which are unnaturally or uncomfortably loud.\nIf a camera sensor is incapable of recording the full dynamic range of a scene, high-dynamic-range (HDR) techniques may be used in postprocessing, which generally involve combining multiple exposures using software.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41080", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=41080", "title": "Earth terminal complex", "text": ""}
