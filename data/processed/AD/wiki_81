{"id": "41081", "revid": "50450951", "url": "https://en.wikipedia.org/wiki?curid=41081", "title": "Echo (mythology)", "text": "In Greek mythology, Echo (; , \"\u0112kh\u014d\", \"echo\", from \u1f26\u03c7\u03bf\u03c2 (\"\u0113chos\"), \"sound\") was an Oread who resided on Mount Cithaeron. Zeus loved consorting with beautiful nymphs and often visited them on Earth. Eventually, Zeus's wife, Hera, became suspicious, and came from Mount Olympus in an attempt to catch Zeus with the nymphs. Echo, by trying to protect Zeus (as he had ordered her to do), endured Hera's wrath, and Hera made her only able to speak the last words spoken to her. When Echo met Narcissus and fell in love with him, she was unable to tell him how she felt and was forced to watch him as he fell in love with himself.\nClassical depiction.\n\"Metamorphoses\".\nIn \"Metamorphoses\" (8 AD), the poet Ovid tells of Juno (Hera in Greek mythology) and the jealousy she felt over her husband Jupiter's (Zeus in Greek mythology) many affairs. Though vigilant, whenever she was about to catch him, Echo distracted her with lengthy conversations. When at last Juno realized the truth, she cursed Echo. From that moment on, the once loquacious nymph could only repeat the most recently spoken words of another person.\nSometime after being cursed, Echo spied a young man, Narcissus, while he was out hunting deer with his companions. She immediately fell in love with him and, infatuated, followed quietly. The more she looked at the young man, the more she longed for him. Though she wished with all her heart to call out to Narcissus, Juno's curse prevented her.\nDuring the hunt, Narcissus became separated from his companions and called out, \"is anyone there\" and heard the nymph repeat his words. Startled, Narcissus answered the voice, \u2018come here,\u2019 only to be told the same. When Narcissus saw that nobody had emerged from the glade, he concluded that the owner of the voice must be running away from him and called out again. Finally, he shouted, \"This way, we must come together.\" Taking this to be a reciprocation of her love, Echo concurred ecstatically, \"We must come together!\"\nIn her delight, Echo rushed to Narcissus ready to throw her arms around her beloved. Narcissus, however, was appalled and, spurning her, exclaimed, \u2018Hands off! May I die before you enjoy my body.\u2019 All Echo could whisper in reply was, \u2018enjoy my body\u2019 and having done so she fled, scorned, humiliated, and shamed.\nDespite the harshness of his rejection, Echo's love for Narcissus only grew. When Narcissus died, wasting away before his own reflection, consumed by a love that could not be, Echo mourned over his body. When Narcissus, looking one last time into the pool uttered, \"Oh marvellous boy, I loved you in vain, farewell\", Echo too chorused, \"Farewell.\"\nEventually, Echo, too, began to waste away. Her beauty faded, her skin shrivelled, and her bones turned to stone. Today, all that remains of Echo is the sound of her voice.\n\"Daphnis and Chloe\".\nThe tale of \"Daphnis and Chloe\" is a 2nd-century romance by Greek author Longus. At one point in the novel, Daphnis and Chloe are staring out at the boats gliding across the sea. Chloe, having never heard an echo before, is confused on hearing the fisherman's song repeated in a nearby valley. Daphnis promises to tell her the story of Echo in exchange for ten more kisses.\nDaphnis\u2019 rendition differs radically from Ovid's account. According to Daphnis, Echo was raised among the Nymph\u00e6 because her mother was a nymph. Her father, however, was merely a man and hence Echo was not herself a nymph but mortal. Echo spent her days dancing with the Nymphae and singing with the Muses who taught her all manner of musical instruments. Pan then grew angry with her, envious of her musical virtuosity and covetous of her virginity, which she would yield neither to men nor gods. Pan drove the men of the fields mad, and, like wild animals, they tore Echo apart and scattered the still singing fragments of her body across the earth.\nShowing favour to the Nymphae, Gaia hid the shreds of Echo within herself providing shelter for her music and at the Muses\u2019 command, Echo's body will still sing, imitating with perfect likeness the sound of any earthly thing. Daphnis recounts that Pan himself often hears his very own pipes and, giving chase across the mountains, looks in vain for the secret student he can never find.Echo in Classical Art and Iconography\nEcho appears in a small but significant body of Greco-Roman visual art, typically in scenes depicting her unrequited love for Narcissus. Among the most famous depictions is the Pompeian fresco found in the House of Marcus Lucretius Fronto, where Echo is shown standing partly behind Narcissus as he gazes into the pool. The placement slightly removed from the center, and rendered in softer detail, appears to be a visual metaphor for the loss of agency and bodily presence described in Ovid\u2019s Metamorphoses. Vase paintings from the 5th and 4th centuries BCE similarly depict Echo as an Oread integrated into the natural landscape, often emerging in rock formations or tree lines to emphasize her connection to mountains and her eventual dissolution into voice alone. These depictions stress Echo\u2019s liminal role in myth: physically present but visually fading from the narrative space.\nOther.\nBoth the \"Homeric Hymn\" and \"Orphic Hymn\" to Pan reiterate Longus' tale of Pan chasing Echo's secret voice across the mountains.\nCodex 190 of Photius' \"Bibliotheca\" states that Pan's unrequited love for Echo was placed there by Aphrodite, angry at his verdict in a beauty contest.\nNonnus' \"Dionysiaca\" contains a number of references to Echo. In Nonnus' account, though Pan frequently chased Echo, he never won her affection. Book VI also makes reference to Echo in the context of the Great Deluge. Nonnus states that the waters rose so far that even high on the hills Echo was forced to swim. Having escaped the advances of Pan, she feared now the lust of Poseidon.\nWhereas Nonnus is adamant that Pan never wins Echo, in Apuleius' \"The Golden Ass\" Pan is described with Echo in his arms, teaching the nymph to repeat all manner of songs. Similarly in the \"Suda\", Echo is described as bearing Pan a child, \"Iynx\". Other fragments mention a second daughter, \"Iambe\".\nMedieval depiction.\nHarrison describes how medieval European writers transformed Echo from a nymph into a mortal noblewoman, adapting her story to explore themes of courtly love, desire, and pride. These retellings often used Echo to comment on human emotion and social conduct within aristocratic culture.\n\"The Lay of Narcissus\".\n\"The Lay of Narcissus\", one of many titles by which the work is known, is a Norman-French verse narrative written towards the end of the 12th century. In the four manuscripts that remain, an unknown author borrows from the Echo and Narcissus of Ovid to create a story better suited to the needs of his time.\nThis medieval account alters the characters of both Echo and Narcissus. In Ovid's account Echo is a beautiful nymph residing with the Muses, and Narcissus is a haughty prince. In \"The Lay of Narcissus\", Echo is replaced by the princess Dan\u00e9. Conversely, Narcissus loses the royal status he bore in Ovid's account: in this rendition he is no more than a commoner, a vassal of Dan\u00e9's father, the King.\nIn the \"Lay\", Dan\u00e9 is pierced by the arrows of Amor and falls madly in love with Narcissus. Though aware that she should first consult her father, she nonetheless shares her feelings with Narcissus. Despite her emphasising her royal lineage, Narcissus spurns her just as he spurns and flees from all women.\nHumiliated, Dan\u00e9 calls out to Amor, and, in response, the god curses Narcissus. In a classic example of poetic justice, Narcissus is forced to suffer the same pain he inflicted on others, namely the pain of unrequited love. The vehicle of this justice is a pool of water in which Narcissus falls in love with his own reflection, which he at first mistakes for a woman. Deranged by lust, Dan\u00e9 searches for Narcissus, naked but for a cloak, and finds him at the point of death. Devastated, Dan\u00e9 repents ever calling to Amor. Dan\u00e9 expresses her love for the last time, pulls close to her beloved and dies in his arms. The poet warns men and women alike not to disdain suitors lest they suffer a similar fate.\nWhile Ovid's story is still recognisable, many of the details have changed considerably. Almost all references to pagan deities are gone, save Amor who is little more than a personification of love. Narcissus is demoted to the status of a commoner while Echo is elevated to the status of princess. Allusions to Narcissus\u2019 homosexuality are expunged. While Ovid talks of Narcissus' disdain for both male and female suitors, the \"Lay\" only mentions his hatred of women. Similarly, in the \"Lay\", Narcissus mistakes his reflection for that of a woman, whereas no mention is made of this in Ovid's account. Finally, the tale is overtly moralized with messages about courtly love. Such exhortations were entirely absent from the \"Metamorphoses\" rendition.\n\"The Romance of the Rose\".\n\"The Romance of the Rose\" is a medieval French poem, the first section of which was written by Guillaume de Lorris in around 1230. The poem was completed by Jean de Meun in around 1275. Part of a much larger narrative, the tale of Echo and Narcissus is relayed when the central figure stumbles across the pool wherein Narcissus first glimpsed his own reflection.\nIn this rendition, Echo is not a nymph, or a princess, but a noble lady. She fell madly in love with Narcissus, so much so that she declared that she would die should he fail to love her in turn. Narcissus refuses, not because he despises all women, but merely because he is haughty and excessively proud of his own beauty.\nGuillaume relays that on hearing Narcissus\u2019 rejection, Echo's grief and anger were so great that she died at once. However, in a similar vein to the \"Lay of Narcissus\", just before she dies, Echo calls out to Deus. She asks that Narcissus might one day be tormented by unrequited love as she had been, and, in so doing, understand how the spurned suffer.\nAs in the classical myth, Narcissus comes across a pool following a hunt. Though Echo prayed to Deus, and the tale notes that he answered her prayer, it is Amor who waits for Narcissus by the water. Amor causes Narcissus to fall for his own reflection, leading quickly to his death. The tale makes clear that this is not merely justice for Echo, but also punishment for Narcissus\u2019 slight against love itself.\nThe tale concludes with an exhortation to all men warning them that, should they scorn their lovers, God will repay the offence.\nGuillaume's rendition builds on the themes of courtly love emphasised in the \"Lay\" and moves further away from Ovid's initial account. The curse of Hera is absent entirely, and the tale is overtly moralised. Unlike in the \"Lay\", however, this moral message is aimed solely at women; this despite the fact that the offending behaviour is perpetrated by Narcissus not Echo.\nLegacy.\nModern terminology.\nEcho\u2019s name is the source of several medical and psychiatric terms. Athanasiadis notes that \u201cecholalia\u201d and \u201cechopraxia,\u201d conditions involving the involuntary repetition of words or actions, derive from her role in myth as a nymph who could only repeat others\u2019 speech.\nSymbolism and influence.\nGraziani identifies Ovid\u2019s \"Metamorphoses\" as the most influential literary version of the Echo myth. She notes that Echo symbolizes revelation through repetition, functioning as a figure whose voice exposes or reflects hidden truths. Graziani also highlights Echo\u2019s influence on early modern European literature, especially in the development of the \u201cecho dialogue,\u201d a poetic form that relies on repeated syllables to create ironic or layered meanings.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41082", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41082", "title": "Effective data transfer rate", "text": "Information transmission rate\nIn telecommunications, effective data transfer rate is the average number of units of data, such as bits, characters, blocks, or frames, transferred per unit time from a source and accepted as valid by a sink.\n\"Note:\" The effective data transfer rate is usually expressed in bits, characters, blocks, or frames per second. The effective data transfer rate may be averaged over a period of seconds, minutes, or hours.\nReferences.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "41083", "revid": "6727347", "url": "https://en.wikipedia.org/wiki?curid=41083", "title": "Effective Earth radius", "text": ""}
{"id": "41084", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41084", "title": "Effective height", "text": "In telecommunications, the effective height of an antenna is the height of the antenna's center of radiation above the ground.\nIn low-frequency applications involving loaded or nonloaded vertical antennas, the effective height is the moment of the current distribution in the vertical section, divided by the input current. For an antenna with a symmetrical current distribution, the center of radiation is the center of the distribution. For an antenna with asymmetrical current distribution, the center of radiation is the center of current moments when viewed from points near the direction of maximum radiation.\nIn antenna theory, it is often used interchangeably with antenna effective length, where it is defined as the ratio of the open circuit voltage across the antenna's terminals to the incident electric field of a radio signal.\nReferences.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41085", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41085", "title": "Effective input noise temperature", "text": "In telecommunications, effective input noise temperature is the source noise temperature in a two-port network or amplifier that will result in the same output noise power, when connected to a noise-free network or amplifier, as that of the actual network or amplifier connected to a noise-free source. If \"F\" is the noise factor numeric and 290K the standard noise temperature, then the effective noise temperature is given by \"T\" n = 290(\"F\" \u2212 1).\nReferences.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41086", "revid": "1264797562", "url": "https://en.wikipedia.org/wiki?curid=41086", "title": "Effective mode volume", "text": " \nFor an optical fiber, the effective mode volume is the square of the product of the diameter of the near-field pattern and the sine of the radiation angle of the far-field pattern. The diameter of the near-field radiation pattern is defined here as the full width at half maximum and the radiation angle at half maximum radiant intensity. Effective mode volume is proportional to the breadth of the relative distribution of power amongst the modes in a multimode fiber. It is not truly a spatial volume but rather an \"optical volume\" equal to the product of area and solid angle. The power divided by the effective mode volume is proportional to the radiance of the light emitted by the fiber."}
{"id": "41087", "revid": "17820167", "url": "https://en.wikipedia.org/wiki?curid=41087", "title": "Effective power", "text": "Effective power may refer to:\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "41088", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41088", "title": "Effective transmission rate", "text": "Rate at which information is processed by a transmission facility\nIn telecommunications, effective transmission rate (\"average rate of transmission, effective speed of transmission\") is the rate at which information is processed by a transmission facility. \nReferences.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41089", "revid": "294180", "url": "https://en.wikipedia.org/wiki?curid=41089", "title": "Efficiency factor", "text": "Efficiency factor is a ratio of some measure of performance to an expected value.\nData communication.\nIn data communications, the factor is the ratio of the time to transmit a text automatically at a specified modulation rate to the time actually required to receive the same text at a specified maximum error rate. All of the communication facilities are assumed to be in the normal condition of adjustment and operation. The practical conditions of measurement should be specified, especially the duration of the measurement.\nTelegraph communications may have different temporal efficiency factors for the two directions of transmission.\nIndustrial engineering.\nIn industrial engineering, the efficiency factor is the relationship between the allowance time and the time taken, in the form of percentage.\nEfficiency factors are used in performance rating and remuneration calculation exercises. The efficiency factor is an extremely simple to use and readily comprehensible index, the prerequisite being exact time management for maintaining the allowed times."}
{"id": "41090", "revid": "8066546", "url": "https://en.wikipedia.org/wiki?curid=41090", "title": "EIA interface", "text": ""}
{"id": "41091", "revid": "30462305", "url": "https://en.wikipedia.org/wiki?curid=41091", "title": "Electrical length", "text": "Parameter characterizing an AC conductor\nIn electrical engineering, electrical length is a dimensionless parameter equal to the physical length of an electrical conductor such as a cable or wire, divided by the wavelength of alternating current at a given frequency traveling through the conductor. In other words, it is the length of the conductor measured in wavelengths. It can alternately be expressed as an angle, in radians or degrees, equal to the phase shift the alternating current experiences traveling through the conductor.\nElectrical length is defined for a conductor operating at a specific frequency or narrow band of frequencies. It varies according to the construction of the cable, so different cables of the same length operating at the same frequency can have different electrical lengths. A conductor is called \"electrically long\" if it has an electrical length much greater than one (i.e. it is much longer than the wavelength of the alternating current passing through it), and \"electrically short\" if it is much shorter than a wavelength. Electrical lengthening and electrical shortening mean adding reactance (capacitance or inductance) to an antenna or conductor to increase or decrease its electrical length, usually for the purpose of making it resonant at a different resonant frequency.\nThis concept is used throughout electronics, and particularly in radio frequency circuit design, transmission line and antenna theory and design. Electrical length determines when wave effects (phase shift along conductors) become important in a circuit. Ordinary lumped element electric circuits only work well for alternating currents at frequencies for which the circuit is electrically small (electrical length much less than one). For frequencies high enough that the wavelength approaches the size of the circuit (the electrical length approaches one) the lumped element model on which circuit theory is based becomes inaccurate, and transmission line techniques must be used.\nDefinition.\nElectrical length is defined for conductors carrying alternating current (AC) at a single frequency or narrow band of frequencies. An alternating electric current of a single frequency formula_1 is an oscillating sine wave which repeats with a period of formula_2. This current flows through a given conductor such as a wire or cable at a particular phase velocity formula_3. It takes time for later portions of the wave to reach a given point on the conductor so the spatial distribution of current and voltage along the conductor at any time is a moving sine wave. After a time equal to the period formula_4 a complete cycle of the wave has passed a given point and the wave repeats; during this time a point of constant phase on the wave has traveled a distance of \nformula_5 \nso formula_6 (Greek lambda) is the wavelength of the wave along the conductor, the distance between successive crests of the wave.\nThe \"electrical length\" formula_7 of a conductor with a physical length of formula_8 at a given frequency formula_1 is the number of wavelengths or fractions of a wavelength of the wave along the conductor; in other words the conductor's length measured in wavelengths\nformula_10\nThe phase velocity formula_3 at which electrical signals travel along a transmission line or other cable depends on the construction of the line. Therefore, the wavelength formula_6 corresponding to a given frequency varies in different types of lines, thus at a given frequency different conductors of the same physical length can have different electrical lengths.\nPhase shift definition.\nIn radio frequency applications, when a delay is introduced due to a conductor, it is often the phase shift formula_13, the difference in phase of the sinusoidal wave between the two ends of the conductor, that is of importance. The length of a sinusoidal wave is commonly expressed as an angle, in units of degrees (with 360\u00b0 in a wavelength) or radians (with 2\u03c0 radians in a wavelength). So alternately the electrical length can be expressed as an angle which is the phase shift of the wave between the ends of the conductor\nformula_14\nformula_15\nSignificance.\nThe electrical length of a conductor determines when wave effects (phase shift along the conductor) are important. If the electrical length formula_7 is much less than one, that is the physical length of a conductor is much shorter than the wavelength, say less than one tenth of the wavelength (formula_17) it is called \"electrically short\". In this case the voltage and current are approximately constant along the conductor, so it acts as a simple connector which transfers alternating current with negligible phase shift. In circuit theory the connecting wires between components are usually assumed to be electrically short, so the lumped element circuit model is only valid for alternating current when the circuit is \"electrically small\", much smaller than a wavelength. When the electrical length approaches or is greater than one, a conductor will have significant reactance, inductance or capacitance, depending on its length. So simple circuit theory is inadequate and transmission line techniques (the distributed-element model) must be used.\nVelocity factor.\nIn a vacuum an electromagnetic wave (radio wave) travels at the speed of light formula_18 2.9979\u00d7108 meters per second, and very close to this speed in air, so the \"free space wavelength\" of the wave is formula_19. (in this article free space variables are distinguished by a subscript 0) Thus a physical length formula_8 of a radio wave in space or air has an electrical length of \nformula_21 wavelengths. \nIn the SI system of units, empty space has a permittivity of formula_22 8.854\u00d710\u221212 F/m (farads per metre) and a magnetic permeability of formula_23 1.257\u00d710\u22126 H/m (henries per meter). These universal constants determine the speed of light\nformula_24\nIn most transmission lines, the series resistance of the wires and shunt conductance of the insulation is low enough that the line can be approximated as lossless (see diagram). This means the inductance and capacitance per unit length of the line determine the phase velocity.\nIn an electrical cable, for a cycle of the alternating current to move a given distance along the line, it takes time to charge the capacitance between the conductors, and the rate of change of the current is slowed by the series inductance of the wires. This determines the phase velocity formula_3 at which the wave moves along the line. In cables and transmission lines an electrical signal travels at a rate determined by the effective shunt capacitance formula_26 and series inductance formula_27 per unit length of the transmission line\nformula_28\nSome transmission lines consist only of bare metal conductors, if they are far away from other high permittivity materials their signals propagate at very close to the speed of light, formula_29. In most transmission lines the material construction of the line slows the velocity of the signal so it travels at a reduced phase velocity This property of the line is specified by a dimensionless number between 0 and 1 called the \"velocity factor\" formula_30:\nformula_31 \ncharacteristic of the type of line, equal to the ratio of signal velocity in the line to the speed of light.\nMost transmission lines contain a dielectric material (insulator) filling some or all of the space in between the conductors. The permittivity formula_32 or \"dielectric constant\" of that material increases the distributed capacitance formula_26 in the cable, which reduces the velocity factor below unity. If there is a material with high magnetic permeability (formula_34) in the line such as steel or ferrite which increases the distributed inductance formula_27, it can also reduce formula_30, but this is almost never the case. If all the space around the transmission line conductors containing the near fields was filled with a material of permittivity formula_32 and permeability formula_34, the phase velocity on the line would be\nformula_39\u00a0\u00a0 \nThe effective permittivity formula_32 and permeability formula_34 per unit length of the line are frequently given as dimensionless constants; relative permittivity: formula_42 and relative permeability: formula_43 equal to the ratio of these parameters compared to the universal constants formula_44 and formula_45\nformula_46\nso the phase velocity is\nformula_47\nSo the velocity factor of the line is\nformula_48\nIn many lines, for example twin lead, only a fraction of the space surrounding the line containing the fields is occupied by a solid dielectric. With only part of the electromagnetic field effected by the dielectric, there is less reduction of the wave velocity. In this case an \"effective permittivity\" formula_49 can be calculated which if it filled all the space around the line would give the same phase velocity. This is computed as a weighted average of the relative permittivity of free space, unity, and that of the dielectric:\nformula_50\nwhere the \"fill factor\" \"F\" expresses the effective proportion of space around the line occupied by dielectric.\nIn most transmission lines there are no materials with high magnetic permeability, so formula_51 and formula_52 and so \nformula_53 (no magnetic materials)\u00a0\u00a0\u00a0\u00a0 \nSince the electromagnetic waves travel slower in the line than in free space, the wavelength of the wave in the transmission line formula_6 is shorter than the free space wavelength by the factor VF: formula_55. Therefore, more wavelengths fit in a transmission line of a given length formula_8 than in the same length of wave in free space, so the electrical length of a transmission line is longer than the electrical length of a wave of the same frequency in free space\nformula_57\nTransmission lines.\nOrdinary electrical cable suffices to carry alternating current when the cable is \"electrically short\"; the electrical length of the cable is small compared to one, that is when the physical length of the cable is small compared to a wavelength, say formula_17.\nAs frequency gets high enough that the length of the cable becomes a significant fraction of a wavelength, formula_59, ordinary wires and cables become poor conductors of AC. Impedance discontinuities at the source, load, connectors and switches begin to reflect the electromagnetic current waves back toward the source, creating bottlenecks so not all the power reaches the load. Ordinary wires act as antennas, radiating the power into space as radio waves, and in radio receivers can also pick up radio frequency interference (RFI).\nTo mitigate these problems, at these frequencies transmission line is used instead. A transmission line is a specialized cable designed for carrying electric current of radio frequency. The distinguishing feature of a transmission line is that it is constructed to have a constant characteristic impedance along its length and through connectors and switches, to prevent reflections. This also means AC current travels at a constant phase velocity along its length, while in ordinary cable phase velocity may vary. The velocity factor formula_30 depends on the details of construction, and is different for each type of transmission line. However the approximate velocity factor for the major types of transmission lines is given in the table.\nElectrical length is widely used with a graphical aid called the Smith chart to solve transmission line calculations. A Smith chart has a scale around the circumference of the circular chart graduated in wavelengths and degrees, which represents the electrical length of the transmission line from the point of measurement to the source or load.\nThe equation for the voltage as a function of time along a transmission line with a matched load, so there is no reflected power, is\nformula_61\nwhere\nformula_62 is the peak voltage along the line\nformula_63 is the angular frequency of the alternating current in radians per second \nformula_64 is the wavenumber, equal to the number of radians of the wave in one meter\nformula_65 is the distance along the line\nformula_66 is time\nIn a matched transmission line, the current is in phase with the voltage, and their ratio is the characteristic impedance formula_67 of the line\nformula_68\nAntennas.\nAn important class of radio antenna is the \"thin element antenna\" in which the radiating elements are conductive wires or rods. These include monopole antennas and dipole antennas, as well as antennas based on them such as the whip antenna, T antenna, mast radiator, Yagi, log periodic, and turnstile antennas. These are resonant antennas, in which the radio frequency electric currents travel back and forth in the antenna conductors, reflecting from the ends.\nIf the antenna rods are not too thick (have a large enough length to diameter ratio), the current along them is close to a sine wave, so the concept of electrical length also applies to these. The current is in the form of two oppositely directed sinusoidal traveling waves which reflect from the ends, which interfere to form standing waves. The electrical length of an antenna, like a transmission line, is its length in wavelengths of the current on the antenna at the operating frequency. An antenna's resonant frequency, radiation pattern, and driving point impedance depend not on its physical length but on its electrical length. A thin antenna element is resonant at frequencies at which the standing current wave has a node (zero) at the ends (and in monopoles an antinode (maximum) at the ground plane). A dipole antenna is resonant at frequencies at which its electrical length is a half wavelength (formula_69) or a multiple of it. A monopole antenna is resonant at frequencies at which its electrical length is a quarter wavelength (formula_70) or a multiple of it.\nResonant frequency is important because at frequencies at which the antenna is resonant the input impedance it presents to its feedline is purely resistive. If the resistance of the antenna is matched to the characteristic resistance of the feedline, it absorbs all the power supplied to it, while at other frequencies it has reactance and reflects some power back down the line toward the transmitter, causing standing waves (high SWR) on the feedline. Since only a portion of the power is radiated this causes inefficiency, and can possibly overheat the line or transmitter. Therefore, transmitting antennas are usually designed to be resonant at the transmitting frequency; and if they cannot be made the right length they are \"electrically lengthened\" or \"shortened\" to be resonant (see below).\nEnd effects.\nA thin-element antenna can be thought of as a transmission line with the conductors separated, so the near-field electric and magnetic fields extend further into space than in a transmission line, in which the fields are mainly confined to the vicinity of the conductors. Near the ends of the antenna elements the electric field is not perpendicular to the conductor axis as in a transmission line but spreads out in a fan shape (fringing field). As a result, the end sections of the antenna have increased capacitance, storing more charge, so the current waveform departs from a sine wave there, decreasing faster toward the ends. When approximated as a sine wave, the current does not quite go to zero at the ends; the nodes of the current standing wave, instead of being at the ends of the element, occur somewhat beyond the ends. Thus the electrical length of the antenna is longer than its physical length.\nThe electrical length of an antenna element also depends on the length-to-diameter ratio of the conductor. As the ratio of the diameter to wavelength increases, the capacitance increases, so the node occurs farther beyond the end, and the electrical length of the element increases. When the elements get too thick, the current waveform becomes significantly different from a sine wave, so the entire concept of electrical length is no longer applicable, and the behavior of the antenna must be calculated by electromagnetic simulation computer programs like NEC.\nAs with a transmission line, an antenna's electrical length is increased by anything that adds shunt capacitance or series inductance to it, such as the presence of high permittivity dielectric material around it. In microstrip antennas which are fabricated as metal strips on printed circuit boards, the dielectric constant of the substrate board increases the electrical length of the antenna. Proximity to the Earth or a ground plane, a dielectric coating on the conductor, nearby grounded towers, metal structural members, guy lines and the capacitance of insulators supporting the antenna also increase the electrical length.\nThese factors, called \"end effects\", cause the electrical length of an antenna element to be somewhat longer than the length of the same wave in free space. In other words, the physical length of the antenna at resonance will be somewhat shorter than the resonant length in free space (one-half wavelength for a dipole, one-quarter wavelength for a monopole). As a rough generalization, for a typical dipole antenna, the physical resonant length is about 5% shorter than the free space resonant length.\nElectrical lengthening and shortening.\nIn many circumstances for practical reasons it is inconvenient or impossible to use an antenna of resonant length. An antenna of nonresonant length at the operating frequency can be made resonant by adding a reactance, a capacitance or inductance, either in the antenna itself or in a matching network between the antenna and its feedline. A nonresonant antenna appears at its feedpoint electrically equivalent to a resistance in series with a reactance. Adding an equal but opposite type of reactance in series with the feedline will cancel the antenna's reactance; the combination of the antenna and reactance will act as a series resonant circuit, so at its operating frequency its input impedance will be purely resistive, allowing it to be fed power efficiently at a low SWR without reflections.\nIn a common application, an antenna which is \"electrically short\", shorter than its fundamental resonant length, a monopole antenna with an electrical length shorter than a quarter-wavelength (formula_71), or a dipole antenna shorter than a half-wavelength (formula_72) will have capacitive reactance. Adding an inductor (coil of wire), called a loading coil, at the feedpoint in series with the antenna, with inductive reactance equal to the antenna's capacitive reactance at the operating frequency, will cancel the capacitance of the antenna, so the combination of the antenna and coil will be resonant at the operating frequency. Since adding inductance is equivalent to increasing the electrical length, this technique is called electrically lengthening the antenna. This is the usual technique for matching an electrically short transmitting antenna to its feedline, so it can be fed power efficiently. However, an electrically short antenna that has been loaded in this way still has the same radiation pattern; it does not radiate as much power, and therefore has lower gain than a full-sized antenna.\nConversely, an antenna longer than resonant length at its operating frequency, such as a monopole longer than a quarter wavelength but shorter than a half wavelength, will have inductive reactance. This can be cancelled by adding a capacitor of equal but opposite reactance at the feed point to make the antenna resonant. This is called electrically shortening the antenna.\nScaling properties of antennas.\nTwo antennas that are similar (scaled copies of each other), fed with different frequencies, will have the same radiation resistance and radiation pattern and fed with equal power will radiate the same power density in any direction if they have the same electrical length at the operating frequency; that is, if their lengths are in the same proportion as the wavelengths.\nformula_73\nThis means the length of antenna required for a given antenna gain scales with the wavelength (inversely with the frequency), or equivalently the aperture scales with the square of the wavelength.\nElectrically short antennas.\nAn electrically short conductor, much shorter than one wavelength, makes an inefficient radiator of electromagnetic waves. As the length of an antenna is made shorter than its fundamental resonant length (a half-wavelength for a dipole antenna and a quarter-wavelength for a monopole), the radiation resistance the antenna presents to the feedline decreases with the square of the electrical length, that is the ratio of physical length to wavelength, formula_74. As a result, other resistances in the antenna, the ohmic resistance of metal antenna elements, the ground system if present, and the loading coil, dissipate an increasing fraction of transmitter power as heat. A monopole antenna with an electrical length below .05formula_6 or 18\u00b0 has a radiation resistance of less than one ohm, making it very hard to drive.\nA second disadvantage is that since the capacitive reactance of the antenna and inductive reactance of the required loading coil do not decrease, the Q factor of the antenna increases; it acts electrically like a high Q tuned circuit. As a result, the bandwidth of the antenna decreases with the square of electrical length, reducing the data rate that can be transmitted. At VLF frequencies even the huge toploaded wire antennas that must be used have bandwidths of only ~10 hertz, limiting the data rate that can be transmitted.\nRegimes of electromagnetics.\nThe field of electromagnetics is the study of electric fields, magnetic fields, electric charge, electric currents and electromagnetic waves. Classic electromagnetism is based on the solution of Maxwell's equations. These equations are mathematically difficult to solve in all generality, so approximate methods have been developed that apply to situations in which the electrical length of the apparatus is very short (formula_76) or very long (formula_77). Electromagnetics is divided into three regimes or fields of study depending on the electrical length of the apparatus, that is the physical length formula_8 of the apparatus compared to the wavelength formula_79 of the waves: Completely different apparatus is used to conduct and process electromagnetic waves in these different wavelength ranges\nHistorically, electric circuit theory and optics developed as separate branches of physics until at the end of the 19th century James Clerk Maxwell's electromagnetic theory and Heinrich Hertz's discovery that light was electromagnetic waves unified these fields as branches of electromagnetism.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41092", "revid": "28146993", "url": "https://en.wikipedia.org/wiki?curid=41092", "title": "Electric field", "text": "Physical field surrounding an electric charge\nAn electric field (sometimes called E-field) is a physical field that surrounds electrically charged particles such as electrons. In classical electromagnetism, the electric field of a single charge (or group of charges) describes their capacity to exert attractive or repulsive forces on another charged object. Charged particles exert attractive forces on each other when the sign of their charges are opposite, one being positive while the other is negative, and repel each other when the signs of the charges are the same. Because these forces are exerted mutually, two charges must be present for the forces to take place. These forces are described by Coulomb's law, which says that the greater the magnitude of the charges, the greater the force, and the greater the distance between them, the weaker the force. Informally, the greater the charge of an object, the stronger its electric field. Similarly, an electric field is stronger nearer charged objects and weaker further away. Electric fields originate from electric charges and time-varying electric currents. Electric fields and magnetic fields are both manifestations of the electromagnetic field. Electromagnetism is one of the four fundamental interactions of nature.\nElectric fields are important in many areas of physics, and are exploited in electrical technology. For example, in atomic physics and chemistry, the interaction in the electric field between the atomic nucleus and electrons is the force that holds these particles together in atoms. Similarly, the interaction in the electric field between atoms is the force responsible for chemical bonding that result in molecules.\nThe electric field is defined as a vector field that associates to each point in space the force per unit of charge exerted on an infinitesimal positive test charge at rest at that point. The SI unit for the electric field is the volt per meter (V/m), which is equal to the newton per coulomb (N/C).\nDescription.\nThe electric field is defined at each point in space as the force that would be experienced by an infinitesimally small stationary positive test charge at that point divided by the charge. The electric field is defined in terms of force, and force is a vector (i.e. having both magnitude and direction), so it follows that an electric field may be described by a vector field. The electric field acts between two charges similarly to the way that the gravitational field acts between two masses, as they both obey an inverse-square law with distance. This is the basis for Coulomb's law, which states that, for stationary charges, the electric field varies with the source charge and varies inversely with the square of the distance from the source. This means that if the source charge were doubled, the electric field would double, and if you move twice as far away from the source, the field at that point would be only one-quarter its original strength.\nThe electric field can be visualized with a set of lines whose direction at each point is the same as those of the field, a concept introduced by Michael Faraday, whose term 'lines of force' is still sometimes used. This illustration has the useful property that, when drawn so that each line represents the same amount of flux, the strength of the field is proportional to the density of the lines. Field lines due to stationary charges have several important properties, including that they always originate from positive charges and terminate at negative charges, they enter all good conductors at right angles, and they never cross or close in on themselves. The field lines are a representative concept; the field actually permeates all the intervening space between the lines. More or fewer lines may be drawn depending on the precision to which it is desired to represent the field. The study of electric fields created by stationary charges is called electrostatics.\nFaraday's law describes the relationship between a time-varying magnetic field and the electric field. One way of stating Faraday's law is that the curl of the electric field is equal to the negative time derivative of the magnetic field. In the absence of time-varying magnetic field, the electric field is therefore called conservative (i.e. curl-free). This implies there are two kinds of electric fields: electrostatic fields and fields arising from time-varying magnetic fields. While the curl-free nature of the static electric field allows for a simpler treatment using electrostatics, time-varying magnetic fields are generally treated as a component of a unified electromagnetic field. The study of magnetic and electric fields that change over time is called electrodynamics.\nMathematical formulation.\nElectric fields are caused by electric charges, described by Gauss's law, and time varying magnetic fields, described by Faraday's law of induction. Together, these laws are enough to define the behavior of the electric field. However, since the magnetic field is described as a function of electric field, the equations of both fields are coupled and together form Maxwell's equations that describe both fields as a function of charges and currents.\nElectrostatics.\nIn the special case of a steady state (stationary charges and currents), the Maxwell-Faraday inductive effect disappears. The resulting two equations (Gauss's law formula_1 and Faraday's law with no induction term formula_2), taken together, are equivalent to Coulomb's law, which states that a particle with electric charge formula_3 at position formula_4 exerts a force on a particle with charge formula_5 at position formula_6 of:\nformula_7\nwhere\nNote that formula_17 must be replaced with formula_18, permittivity, when charges are in non-empty media.\nWhen the charges formula_5 and formula_3 have the same sign this force is positive, directed away from the other charge, indicating the particles repel each other. When the charges have unlike signs the force is negative, indicating the particles attract.\nTo make it easy to calculate the Coulomb force on any charge at position formula_6 this expression can be divided by formula_5 leaving an expression that only depends on the other charge (the \"source\" charge)\nformula_23\nwhere formula_24 is the component of the electric field at formula_9 due to formula_10.\nThis is the \"electric field\" at point formula_6 due to the point charge formula_3; it is a vector-valued function equal to the Coulomb force per unit charge that a positive point charge would experience at the position formula_6.\nSince this formula gives the electric field magnitude and direction at any point formula_6 in space (except at the location of the charge itself, formula_4, where it becomes infinite) it defines a vector field.\nFrom the above formula it can be seen that the electric field due to a point charge is everywhere directed away from the charge if it is positive, and toward the charge if it is negative, and its magnitude decreases with the inverse square of the distance from the charge.\nThe Coulomb force on a charge of magnitude formula_32 at any point in space is equal to the product of the charge and the electric field at that point\nformula_33\nThe SI unit of the electric field is the newton per coulomb (N/C), or volt per meter (V/m); in terms of the SI base units it is kg\u22c5m\u22c5s\u22123\u22c5A\u22121.\nSuperposition principle.\nDue to the linearity of Maxwell's equations, electric fields satisfy the superposition principle, which states that the total electric field, at a point, due to a collection of charges is equal to the vector sum of the electric fields at that point due to the individual charges. This principle is useful in calculating the field created by multiple point charges. If charges formula_34 are stationary in space at points formula_35, in the absence of currents, the superposition principle says that the resulting field is the sum of fields generated by each particle as described by Coulomb's law:\nformula_36\nwhere \nContinuous charge distributions.\nThe superposition principle allows for the calculation of the electric field due to a distribution of charge density formula_43. By considering the charge formula_44 in each small volume of space formula_45 at point formula_46 as a point charge, the resulting electric field, formula_47, at point formula_39 can be calculated as \nformula_49\nwhere \nThe total field is found by summing the contributions from all the increments of volume by integrating the charge density over the volume formula_56:\nformula_57\nSimilar equations follow for a surface charge with surface charge density formula_58 on surface formula_59\nformula_60\nand for line charges with linear charge density formula_61 on line formula_62\nformula_63\nElectric potential.\nIf a system is static, such that magnetic fields are not time-varying, then by Faraday's law, the electric field is curl-free. In this case, one can define an electric potential, that is, a function formula_64 such that This is analogous to the gravitational potential. The difference between the electric potential at two points in space is called the potential difference (or voltage) between the two points.\nIn general, however, the electric field cannot be described independently of the magnetic field. Given the magnetic vector potential, A, defined so that &amp;NoBreak;}&amp;NoBreak;, one can still define an electric potential formula_65 such that:\nformula_66\nwhere formula_67 is the gradient of the electric potential and formula_68 is the partial derivative of A with respect to time.\nFaraday's law of induction can be recovered by taking the curl of that equation \nformula_69\nwhich justifies, a posteriori, the previous form for E.\nContinuous vs. discrete charge representation.\nThe equations of electromagnetism are best described in a continuous description. However, charges are sometimes best described as discrete points; for example, some models may describe electrons as point sources where charge density is infinite on an infinitesimal section of space.\nA charge formula_32 located at formula_71 can be described mathematically as a charge density &amp;NoBreak;&amp;NoBreak;, where the Dirac delta function (in three dimensions) is used. Conversely, a charge distribution can be approximated by many small point charges.\nElectrostatic fields.\nElectrostatic fields are electric fields that do not change with time. Such fields are present when systems of charged matter are stationary, or when electric currents are unchanging. In that case, Coulomb's law fully describes the field.\nParallels between electrostatic and gravitational fields.\nCoulomb's law, which describes the interaction of electric charges:\nformula_72\nSince the physical interpretation of this indicates that the electric field at a point is governed by the particle's state at a point of time in the future, it is considered as an unphysical solution and hence neglected. However, there have been theories exploring the advanced time solutions of Maxwell's equations, such as Feynman Wheeler absorber theory.\nThe above equation, although consistent with that of uniformly moving point charges as well as its non-relativistic limit, are not corrected for quantum-mechanical effects.\nCommon formulas.\nElectric field infinitely close to a conducting surface in electrostatic equilibrium having charge density formula_73 at that point is formula_74 since charges are only formed on the surface and the surface at the infinitesimal scale resembles an infinite 2D plane. In the absence of external fields, spherical conductors exhibit a uniform charge distribution on the surface and hence have the same electric field as that of uniform spherical surface distribution.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41093", "revid": "27015025", "url": "https://en.wikipedia.org/wiki?curid=41093", "title": "Electromagnetic compatibility", "text": "Electrical engineering concept\nElectromagnetic compatibility (EMC) is the ability of electrical equipment and systems to function acceptably in their electromagnetic environment, by limiting the unintentional generation, propagation and reception of electromagnetic energy which may cause unwanted effects such as electromagnetic interference (EMI) or even physical damage to operational equipment. The goal of EMC is the correct operation of different equipment in a common electromagnetic environment. It is also the name given to the associated branch of electrical engineering.\nEMC pursues three main classes of issue. \"Emission\" is the generation of electromagnetic energy, whether deliberate or accidental, by some source and its release into the environment. EMC studies the unwanted emissions and the countermeasures which may be taken in order to reduce unwanted emissions. The second class, \"susceptibility\", is the tendency of electrical equipment, referred to as the victim, to malfunction or break down in the presence of unwanted emissions, which are known as Radio frequency interference (RFI). \"Immunity\" is the opposite of susceptibility, being the ability of equipment to function correctly in the presence of RFI, with the discipline of \"hardening\" equipment being known equally as susceptibility or immunity. A third class studied is \"coupling\", which is the mechanism by which emitted interference reaches the victim.\nInterference mitigation and hence electromagnetic compatibility may be achieved by addressing any or all of these issues, i.e., quieting the sources of interference, inhibiting coupling paths and/or hardening the potential victims. In practice, many of the engineering techniques used, such as grounding and shielding, apply to all three issues.\nHistory.\nOrigins.\nThe earliest EMC issue was lightning strike (lightning electromagnetic pulse, or LEMP) on ships and buildings. Lightning rods or lightning conductors began to appear in the mid-18th century. With the advent of widespread electricity generation and power supply lines from the late 19th century on, problems also arose with equipment short-circuit failure affecting the power supply, and with local fire and shock hazard when the power line was struck by lightning. Power stations were provided with output circuit breakers. Buildings and appliances would soon be provided with input fuses, and later in the 20th century miniature circuit breakers (MCB) would come into use.\nEarly twentieth century.\nIt may be said that radio interference and its correction arose with the first spark-gap experiment of Marconi in the late 1800s. As radio communications developed in the first half of the 20th century, interference between broadcast radio signals began to occur and an international regulatory framework was set up to ensure interference-free communications.\nSwitching devices became commonplace through the middle of the 20th century, typically in petrol powered cars and motorcycles but also in domestic appliances such as thermostats and refrigerators. This caused transient interference with domestic radio and (after World War II) TV reception, and in due course laws were passed requiring the suppression of such interference sources.\nESD problems first arose with accidental electric spark discharges in hazardous environments such as coal mines and when refuelling aircraft or motor cars. Safe working practices had to be developed.\nPostwar period.\nAfter World War II the military became increasingly concerned with the effects of nuclear electromagnetic pulse (NEMP), lightning strike, and even high-powered radar beams, on vehicle and mobile equipment of all kinds, and especially aircraft electrical systems.\nWhen high RF emission levels from other sources became a potential problem (such as with the advent of microwave ovens), certain frequency bands were designated for Industrial, Scientific and Medical (ISM) use, allowing emission levels limited only by thermal safety standards. Later, the International Telecommunication Union adopted a Recommendation providing limits of radiation from ISM devices in order to protect radiocommunications. A variety of issues such as sideband and harmonic emissions, broadband sources, and the ever-increasing popularity of electrical switching devices and their victims, resulted in a steady development of standards and laws.\nFrom the late 1970s, the popularity of modern digital circuitry rapidly grew. As the technology developed, with ever-faster switching speeds (increasing emissions) and lower circuit voltages (increasing susceptibility), EMC increasingly became a source of concern. Many more nations became aware of EMC as a growing problem and issued directives to the manufacturers of digital electronic equipment, which set out the essential manufacturer requirements before their equipment could be marketed or sold. Organizations in individual nations, across Europe and worldwide, were set up to maintain these directives and associated standards. In 1979, the American FCC published a regulation that required the electromagnetic emissions of all \"digital devices\" to be below certain limits. This regulatory environment led to a sharp growth in the EMC industry supplying specialist devices and equipment, analysis and design software, and testing and certification services. Low-voltage digital circuits, especially CMOS transistors, became more susceptible to ESD damage as they were miniaturised and, despite the development of on-chip hardening techniques, a new ESD regulatory regime had to be developed.\nModern era.\nFrom the 1980s on the explosive growth in mobile communications and broadcast media channels put huge pressure on the available airspace. Regulatory authorities began squeezing band allocations closer and closer together, relying on increasingly sophisticated EMC control methods, especially in the digital communications domain, to keep cross-channel interference to acceptable levels. Digital systems are inherently less susceptible than analogue systems, and also offer far easier ways (such as software) to implement highly sophisticated protection and error-correction measures.\nIn 1985, the USA released the ISM bands for low-power mobile digital communications, leading to the development of Wi-Fi and remotely-operated car door keys. This approach relies on the intermittent nature of ISM interference and use of sophisticated error-correction methods to ensure lossless reception during the quiet gaps between any bursts of interference.\nConcepts.\n\"Electromagnetic interference\" (EMI) is defined as the \"degradation in the performance of equipment or transmission channel or a system caused by an electromagnetic disturbance\" (IEV 161-01-06) while \"electromagnetic disturbance\" is defined as \"\"an electromagnetic phenomenon that can degrade the performance of a device, equipment or system, or adversely affect living or inert matter\" (IEV 161-01-05). The terms \"electromagnetic disturbance\" and \"electromagnetic interference\" designate respectively the cause and the effect,\nElectromagnetic compatibility (EMC) is an equipment \"characteristic\" or \"property\" and is defined as \" \"the ability of equipment or a system to function satisfactorily in its electromagnetic environment without introducing intolerable electromagnetic disturbances to anything in that environment\" \" (IEV 161-01-07).\nEMC ensures the correct operation, in the same electromagnetic environment, of different equipment items which use or respond to electromagnetic phenomena, and the avoidance of any interference. Another way of saying this is that EMC is the control of EMI so that unwanted effects are prevented.\nBesides understanding the phenomena in themselves, EMC also addresses the countermeasures, such as control regimes, design and measurement, which should be taken in order to prevent emissions from causing any adverse effect.\nTechnical characteristics of interference.\nTypes of interference.\nEMC is often understood as the control of electromagnetic interference (EMI). Electromagnetic interference divides into several categories according to the source and signal characteristics.\nThe origin of interference, often called \"noise\" in this context, can be man-made (artificial) or natural.\nContinuous, or continuous wave (CW), interference comprises a given range of frequencies. This type is naturally divided into sub-categories according to frequency range, and as a whole is sometimes referred to as \"DC to daylight\". One common classification is into narrowband and broadband, according to the spread of the frequency range.\nAn electromagnetic pulse (EMP), sometimes called a transient disturbance, is a short-duration pulse of energy. This energy is usually broadband by nature, although it often excites a relatively narrow-band \"damped sine wave\" response in the victim. Pulse signals divide broadly into isolated and repetitive events.\nCoupling mechanisms.\nWhen a source emits interference, it follows a route to the victim known as the coupling path. There are four basic coupling mechanisms: conductive, capacitive, magnetic or inductive, and radiative. Any coupling path can be broken down into one or more of these coupling mechanisms working together.\nConductive coupling occurs when the coupling path between the source and victim is formed by direct electrical contact with a conducting body.\nCapacitive coupling occurs when a varying electrical field exists between two adjacent conductors, inducing a change in voltage on the receiving conductor.\nInductive coupling or magnetic coupling occurs when a varying magnetic field exists between two parallel conductors, inducing a change in voltage along the receiving conductor.\nRadiative coupling or electromagnetic coupling occurs when source and victim are separated by a large distance. Source and victim act as radio antennas: the source emits or radiates an electromagnetic wave which propagates across the space in between and is picked up or received by the victim.\nControl.\nThe damaging effects of electromagnetic interference pose unacceptable risks in many areas of technology, and it is necessary to control such interference and reduce the risks to acceptable levels.\nThe control of electromagnetic interference (EMI) and assurance of EMC comprises a series of related disciplines:\nThe risk posed by the threat is usually statistical in nature, so much of the work in threat characterisation and standards setting is based on reducing the probability of disruptive EMI to an acceptable level, rather than its assured elimination.\nFor a complex or novel piece of equipment, this may require the production of a dedicated \"EMC control plan\" summarizing the application of the above and specifying additional documents required.\nCharacterisation of the problem requires understanding of:\nDesign.\nBreaking a coupling path is equally effective at either the start or the end of the path, therefore many aspects of good EMC design practice apply equally to potential sources and to potential victims. A design which easily couples energy to the outside world will equally easily couple energy in and will be susceptible. A single improvement will often reduce both emissions and susceptibility.\nGrounding and shielding aim to reduce emissions or divert EMI away from the victim by providing an alternative, low-impedance path. Techniques include:\nOther general measures include:\nAdditional measures to reduce emissions include:\nAdditional measures to reduce susceptibility include:\nTesting.\nTesting is required to confirm that a particular device meets the required standards. It is divided broadly into emissions testing and susceptibility testing. Open-area test sites, or OATS, are the reference sites in most standards. They are especially useful for emissions testing of large equipment systems. However, RF testing of a physical prototype is most often carried out indoors, in a specialized EMC test chamber. Types of the chamber include anechoic, reverberation and the gigahertz transverse electromagnetic cell (GTEM cell). Sometimes computational electromagnetics simulations are used to test virtual models. Like all compliance testing, it is important that the test equipment, including the test chamber or site and any software used, be properly calibrated and maintained. Typically, a given run of tests for a particular piece of equipment will require an \"EMC test plan\" and a follow-up \"test report\". The full test program may require the production of several such documents.\nEmissions are typically measured for radiated field strength and where appropriate for conducted emissions along cables and wiring. Inductive (magnetic) and capacitive (electric) field strengths are near-field effects and are only important if the device under test (DUT) is designed for a location close to other electrical equipment. For conducted emissions, typical transducers include the LISN (line impedance stabilization network) or AMN (artificial mains network) and the RF current clamp. For radiated emission measurement, antennas are used as transducers. Typical antennas specified include dipole, biconical, log-periodic, double ridged guide and conical log-spiral designs. Radiated emissions must be measured in all directions around the DUT. Specialized EMI test receivers or EMI analyzers are used for EMC compliance testing. These incorporate bandwidths and detectors as specified by international EMC standards. An EMI receiver may be based on a spectrum analyser to measure the emission levels of the DUT across a wide band of frequencies (frequency domain), or on a tunable narrower-band device which is swept through the desired frequency range. EMI receivers along with specified transducers can often be used for both conducted and radiated emissions. Pre-selector filters may also be used to reduce the effect of strong out-of-band signals on the front-end of the receiver. Some pulse emissions are more usefully characterized using an oscilloscope to capture the pulse waveform in the time domain.\nRadiated field susceptibility testing typically involves a high-powered source of RF or EM energy and a radiating antenna to direct the energy at the potential victim or device under test (DUT). Conducted voltage and current susceptibility testing typically involves a high-powered signal generator, and a current clamp or other type of transformer to inject the test signal. Transient or EMP signals are used to test the immunity of the DUT against powerline disturbances including surges, lightning strikes and switching noise. In motor vehicles, similar tests are performed on battery and signal lines. The transient pulse may be generated digitally and passed through a broadband pulse amplifier, or applied directly to the transducer from a specialized pulse generator. Electrostatic discharge testing is typically performed with a piezo spark generator called an \"ESD pistol\". Higher energy pulses, such as lightning or nuclear EMP simulations, can require a large current clamp or a large antenna which completely surrounds the DUT. Some antennas are so large that they are located outdoors, and care must be taken not to cause an EMP hazard to the surrounding environment.\nLegislation.\nSeveral organizations, both national and international, work to promote international co-operation on standardization (harmonization), including publishing various EMC standards. Where possible, a standard developed by one organization may be adopted with little or no change by others. This helps for example to harmonize national standards across Europe.\nInternational standards organizations include:\nAmong the main national organizations are:\nCompliance with national or international standards is usually laid down by laws passed by individual nations. Different nations can require compliance with different standards.\nIn European law, EU directive 2014/30/EU (previously 2004/108/EC) on EMC defines the rules for the placing on the market/putting into service of electric/electronic equipment within the European Union. The Directive applies to a vast range of equipment including electrical and electronic appliances, systems and installations. Manufacturers of electric and electronic devices are advised to run EMC tests in order to comply with compulsory CE-labeling. More are given in the list of EMC directives. Compliance with the applicable harmonised standards whose reference is listed in the OJEU under the EMC Directive gives presumption of conformity with the corresponding essential requirements of the EMC Directive.\nIn 2019, the USA adopted a program for the protection of critical infrastructure against an electromagnetic pulse, whether caused by a geomagnetic storm or a high-altitude nuclear weapon.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41094", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41094", "title": "Electromagnetic environment", "text": "In telecommunications, the term electromagnetic environment (EME) has the following meanings:\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "41096", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41096", "title": "Electromagnetic interference control", "text": "Control of radiated and conducted energy\nIn electrical systems, such as telecommunications, power electronics, industrial electronics, power engineering; electromagnetic interference control is the control of radiated and conducted energy such that emissions that are unnecessary for system, subsystem, or equipment operation are reduced, minimized, or eliminated. \n\"Note:\" Electromagnetic radiated and conducted emissions are controlled regardless of their origin within the system, subsystem, or equipment. Successful electromagnetic interference control with effective susceptibility control leads to electromagnetic compatibility.\nReferences.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41097", "revid": "7903804", "url": "https://en.wikipedia.org/wiki?curid=41097", "title": "Nuclear electromagnetic pulse", "text": "Effect of a nuclear explosion on electronic equipment\nA nuclear electromagnetic pulse (nuclear EMP or NEMP) is a burst of electromagnetic radiation created by a nuclear explosion. The resulting rapidly varying electric and magnetic fields may couple with electrical and electronic systems to produce damaging current and voltage surges. The specific characteristics of a particular nuclear EMP event vary according to a number of factors, the most important of which is the altitude of the detonation.\nThe term \"electromagnetic pulse\" generally excludes optical (infrared, visible, ultraviolet) and ionizing (such as X-ray and gamma radiation) ranges. In military terminology, a nuclear warhead detonated tens to hundreds of miles above the Earth's surface is known as a high-altitude electromagnetic pulse (HEMP) device. Effects of a HEMP device depend on factors including the altitude of the detonation, energy yield, gamma ray output, interactions with the Earth's magnetic field and electromagnetic shielding of targets.\nHistory.\nThe fact that an electromagnetic pulse is produced by a nuclear explosion was known in the earliest days of nuclear weapons testing. The magnitude of the EMP and the significance of its effects were not immediately realized.\nDuring the first United States nuclear test on 16 July 1945, electronic equipment was shielded because Enrico Fermi expected the electromagnetic pulse. The official technical history for that first nuclear test states, \"All signal lines were completely shielded, in many cases doubly shielded. In spite of this many records were lost because of spurious pickup at the time of the explosion that paralyzed the recording equipment.\"53 During British nuclear testing in 1952\u201353, instrumentation failures were attributed to \"radioflash\", which was their term for EMP.\nThe first openly reported observation of the unique aspects of high-altitude nuclear EMP occurred during the helium balloon-lofted Yucca nuclear test of the Hardtack I series on 28 April 1958. In that test, the electric field measurements from the 1.7 kiloton weapon exceeded the range to which the test instruments were adjusted and was estimated to be about five times the limits to which the oscilloscopes were set.\nThe Yucca EMP was initially positive-going, whereas low-altitude bursts were negative-going pulses. Also, the polarization of the Yucca EMP signal was horizontal, whereas low-altitude nuclear EMP was vertically polarized. In spite of these many differences, the unique EMP results were dismissed as a possible wave propagation anomaly.\nThe high-altitude nuclear tests of 1962, as discussed below, confirmed the unique results of the Yucca high-altitude test and increased the awareness of high-altitude nuclear EMP beyond the original group of defense scientists. The larger scientific community became aware of the significance of the EMP problem after a three-article series on nuclear EMP was published in 1981 by William J. Broad in \"Science\".\nStarfish Prime.\nIn July 1962, the US carried out the Starfish Prime test, exploding a bomb above the mid-Pacific Ocean. This demonstrated that the effects of a high-altitude nuclear explosion were much larger than had been previously calculated. Starfish Prime made those effects known to the public by causing electrical damage in Hawaii, about away from the detonation point, disabling approximately 300 streetlights, triggering numerous burglar alarms and damaging a microwave link.\nStarfish Prime was the first success in the series of United States high-altitude nuclear tests in 1962 known as Operation Fishbowl. Subsequent tests gathered more data on the high-altitude EMP phenomenon.\nThe Bluegill Triple Prime and Kingfish high-altitude nuclear tests of October and November 1962 in Operation Fishbowl provided data that was clear enough to enable physicists to accurately identify the physical mechanisms behind the electromagnetic pulses.\nThe EMP damage of the Starfish Prime test was quickly repaired due, in part, to the fact that the EMP over Hawaii was relatively weak compared to what could be produced with a more intense pulse, and in part due to the relative ruggedness (compared to today) of Hawaii's electrical and electronic infrastructure in 1962.\nThe relatively small magnitude of the Starfish Prime EMP in Hawaii (about 5.6 kilovolts/metre) and the relatively small amount of damage (for example, only 1% to 3% of streetlights extinguished) led some scientists to believe, in the early days of EMP research, that the problem might not be significant. Later calculations showed that if the Starfish Prime warhead had been detonated over the northern continental United States, the magnitude of the EMP would have been much larger (22 to 30 kV/m) because of the greater strength of the Earth's magnetic field over the United States, as well as its different orientation at high latitudes. These calculations, combined with the accelerating reliance on EMP-sensitive microelectronics, heightened awareness that EMP could be a significant problem.\nSoviet Test 184.\nIn 1962, the Soviet Union performed three EMP-producing nuclear tests in space over Kazakhstan, the last in the \"Soviet Project K nuclear tests\". Although these weapons were much smaller (300 kiloton) than the Starfish Prime test, they were over a populated, large landmass and at a location where the Earth's magnetic field was greater. The damage caused by the resulting EMP was reportedly much greater than in Starfish Prime. The geomagnetic storm\u2013like E3 pulse from Test 184 induced a current surge in a long underground power line that caused a fire in the power plant in the city of Karaganda.\nAfter the dissolution of the Soviet Union, the level of this damage was communicated informally to US scientists. For a few years US and Russian scientists collaborated on the HEMP phenomenon. Funding was secured to enable Russian scientists to report on some of the Soviet EMP results in international scientific journals. As a result, formal documentation of some of the EMP damage in Kazakhstan exists, although it is still sparse in the open-scientific literature.\nFor one of the K Project tests, Soviet scientists instrumented a section of telephone line in the area that they expected to be affected by the pulse. The monitored telephone line was divided into sub-lines of in length, separated by repeaters. Each sub-line was protected by fuses and by gas-filled overvoltage protectors. The EMP from the 22 October (K-3) nuclear test (also known as Test 184) blew all of the fuses and destroyed all of the overvoltage protectors in all of the sub-lines.\nPublished reports, including a 1998 IEEE article, have stated that there were significant problems with ceramic insulators on overhead electrical power lines during the tests. A 2010 technical report written for Oak Ridge National Laboratory stated that \"Power line insulators were damaged, resulting in a short circuit on the line and some lines detaching from the poles and falling to the ground\".\nCharacteristics.\nNuclear EMP is a complex multi-pulse, usually described in terms of three components, as defined by the International Electrotechnical Commission (IEC).\nThe three components of nuclear EMP, as defined by the IEC, are called \"E1\", \"E2\", and \"E3\".\nThe three categories of high-altitude EMP are divided according to the time duration and occurrence of each pulse. E1 is the fastest or \"early time\" high-altitude EMP. Traditionally, the term \"EMP\" often refers specifically to this E1 component of high-altitude electromagnetic pulse.\nThe E2 and E3 pulses are often further subdivided into additional divisions according to causation. E2 is a much lower intensity \"intermediate time\" EMP, which is further divided into E2A (scattered gamma EMP) and E2B (neutron gamma EMP).\nE3 is a very long-duration \"late time\" pulse, which is extremely slow in rise and fall times compared to the other components of EMP. E3 is further divided into E3A (blast wave) and E3B (heave). E3 is also called magnetohydrodynamic EMP.\nE1.\nThe E1 pulse is a very fast component of nuclear EMP. E1 is a brief but intense electromagnetic field that induces high voltages in electrical conductors. E1 causes most of its damage by causing electrical breakdown voltages to be exceeded. E1 can destroy computers and communications equipment and it changes too quickly (nanoseconds) for ordinary surge protectors to provide effective protection from it. Fast-acting surge protectors (such as those using TVS diodes) will block the E1 pulse.\nE1 is produced when gamma radiation from the nuclear detonation ionizes (strips electrons from) atoms in the upper atmosphere. This is known as the Compton effect and the resulting current is called the \"Compton current\". The electrons travel in a generally downward direction at relativistic speeds (more than 90 percent of the speed of light). In the absence of a magnetic field, this would produce a large, radial pulse of electric current propagating outward from the burst location confined to the source region (the region over which the gamma photons are attenuated). The Earth's magnetic field exerts a force on the electron flow at a right angle to both the field and the particles' original vector, which deflects the electrons and leads to synchrotron radiation. Because the outward traveling gamma pulse is propagating at the speed of light, the synchrotron radiation of the Compton electrons adds coherently, leading to a radiated electromagnetic signal. This interaction produces a large, brief, pulse.\nSeveral physicists worked on the problem of identifying the mechanism of the HEMP E1 pulse. The mechanism was finally identified by Conrad Longmire of Los Alamos National Laboratory in 1963.\nLongmire gives numerical values for a typical case of E1 pulse produced by a second-generation nuclear weapon such as those of Operation Fishbowl. The typical gamma rays given off by the weapon have an energy of about 2MeV (mega electron-volts). The gamma rays transfer about half of their energy to the ejected free electrons, giving an energy of about 1MeV.\nIn a vacuum and absent a magnetic field, the electrons would travel with a current density of tens of amperes per square metre. Because of the downward tilt of the Earth's magnetic field at high latitudes, the area of peak field strength is a U-shaped region to the equatorial side of the detonation. As shown in the diagram, for nuclear detonations in the Northern Hemisphere, this U-shaped region is south of the detonation point. Near the equator, where the Earth's magnetic field is more nearly horizontal, the E1 field strength is more nearly symmetrical around the burst location.\nAt geomagnetic field strengths typical of the mid-latitudes, these initial electrons spiral around the magnetic field lines with a typical radius of about . These initial electrons are stopped by collisions with air molecules at an average distance of about . This means that most of the electrons are stopped by collisions with air molecules before completing a full spiral around the field lines.\nThis interaction of the negatively charged electrons with the magnetic field radiates a pulse of electromagnetic energy. The pulse typically rises to its peak value in some five nanoseconds. Its magnitude typically decays by half within 200 nanoseconds. (By the IEC definition, this E1 pulse ends 1000 nanoseconds after it begins.) This process occurs simultaneously on about 1025 electrons.\u00a0The simultaneous action of the electrons causes the resulting pulse from each electron to radiate coherently, adding to produce a single large-amplitude, short-duration, radiated pulse.\nSecondary collisions cause subsequent electrons to lose energy before they reach ground level. The electrons generated by these subsequent collisions have so little energy that they do not contribute significantly to the E1 pulse.\nThese 2 MeV gamma rays typically produce an E1 pulse near ground level at moderately high latitudes that peaks at about 50,000 volts per metre. The ionization process in the mid-stratosphere causes this region to become an electrical conductor, a process that blocks the production of further electromagnetic signals and causes the field strength to saturate at about 50,000 volts per metre. The strength of the E1 pulse depends upon the number and intensity of the gamma rays and upon the rapidity of the gamma-ray burst. Strength is also somewhat dependent upon altitude.\nThere are reports of \"super-EMP\" nuclear weapons that are able to exceed the 50,000 volts per metre limit by unspecified mechanisms. The reality and possible construction details of these weapons are classified and are, therefore, unconfirmed in the open scientific literature\nE2.\nThe E2 component is generated by scattered gamma rays and inelastic gammas produced by neutrons. This E2 component is an \"intermediate time\" pulse that, by IEC definition, lasts from about one microsecond to one second after the explosion. E2 has many similarities to lightning, although lightning-induced E2 may be considerably larger than a nuclear E2. Because of the similarities and the widespread use of lightning protection technology, E2 is generally considered to be the easiest to protect against.\nAccording to the United States EMP Commission, the main problem with E2 is that it immediately follows E1, which may have damaged the devices that would normally protect against E2.\nThe EMP Commission Executive Report of 2004 states, \"In general, it would not be an issue for critical infrastructure systems since they have existing protective measures for defense against occasional lightning strikes. The most significant risk is synergistic because the E2 component follows a small fraction of a second after the first component's insult, which has the ability to impair or destroy many protective and control features. The energy associated with the second component thus may be allowed to pass into and damage systems.\"6\nE3.\nThe E3 component is different from E1 and E2. E3 is a much slower pulse, lasting tens to hundreds of seconds. It is caused by the nuclear detonation's temporary distortion of the Earth's magnetic field. The E3 component has similarities to a geomagnetic storm. Like a geomagnetic storm, E3 can produce geomagnetically induced currents in long electrical conductors, damaging components such as power line transformers.\nBecause of the similarity between solar-induced geomagnetic storms and nuclear E3, it has become common to refer to solar-induced geomagnetic storms as \"Solar EMP\". \"Solar EMP\" does not include E1 or E2 components.\nGeneration.\nFactors that control weapon effectiveness include altitude, yield, construction details, target distance, intervening geographical features, and local strength of the Earth's magnetic field.\nWeapon altitude.\nAccording to an internet primer published by the Federation of American Scientists:\n A high-altitude nuclear detonation produces an immediate flux of gamma rays from the nuclear reactions within the device. These photons in turn produce high energy free electrons by Compton scattering at altitudes between (roughly) 20 and 40 km. These electrons are then trapped in the Earth's magnetic field, giving rise to an oscillating electric current. This current is asymmetric in general and gives rise to a rapidly rising radiated electromagnetic field called an electromagnetic pulse (EMP). Because the electrons are trapped essentially simultaneously, a very large electromagnetic source radiates coherently.\n The pulse can easily span continent-sized areas, and this radiation can affect systems on land, sea, and air. ... A large device detonated at over Kansas would affect all of the continental U.S. The signal from such an event extends to the visual horizon as seen from the burst point.\nThus, for equipment to be affected, the weapon needs to be above the visual horizon.\nThe altitude indicated above is greater than that of the International Space Station and many low Earth orbit satellites. Large weapons could have a dramatic impact on satellite operations and communications such as occurred during Operation Fishbowl. The damaging effects on orbiting satellites are usually due to factors other than EMP. In the Starfish Prime nuclear test, most damage was to the satellites' solar panels while passing through radiation belts created by the explosion.\nFor detonations within the atmosphere, the situation is more complex. Within the range of gamma ray deposition, simple laws no longer hold as the air is ionized and there are other EMP effects, such as a radial electric field due to the separation of Compton electrons from air molecules, together with other complex phenomena. For a surface burst, absorption of gamma rays by air would limit the range of gamma-ray deposition to approximately , while for a burst in the lower-density air at high altitudes, the range of deposition would be far greater.\nWeapon yield.\nTypical nuclear weapon yields used during Cold War planning for EMP attacks were in the range of .39 This is roughly 50 to 500 times the size of the Hiroshima and Nagasaki bombs. Physicists have testified at United States Congressional hearings that weapons with yields of or less can produce a large EMP.48\nThe EMP at a fixed distance from an explosion increases at most as the square root of the yield (see the illustration to the right). This means that although a weapon has only of the energy release of the Starfish Prime test, the EMP will be at least as powerful. Since the E1 component of nuclear EMP depends on the prompt gamma-ray output, which was only 0.1% of yield in Starfish Prime but can be of yield in low-yield pure nuclear fission weapons, a bomb can easily be \"5\" * '=' as powerful as the Starfish Prime at producing EMP.\nThe total prompt gamma-ray energy in a fission explosion is of the yield, but in a detonation the triggering explosive around the bomb core absorbs about of the prompt gamma rays, so the output is only about of the yield. In the thermonuclear Starfish Prime the fission yield was less than 100% and the thicker outer casing absorbed about 95% of the prompt gamma rays from the pusher around the fusion stage. Thermonuclear weapons are also less efficient at producing EMP because the first stage can pre-ionize the air which becomes conductive and hence rapidly shorts out the Compton currents generated by the fusion stage. Hence, small pure fission weapons with thin cases are far more efficient at causing EMP than most megaton bombs.\nThis analysis, however, only applies to the fast E1 and E2 components of nuclear EMP. The geomagnetic storm-like E3 component of nuclear EMP is more closely proportional to the total energy yield of the weapon.\nTarget distance.\nIn nuclear EMP all of the components of the electromagnetic pulse are generated outside of the weapon.\nFor high-altitude nuclear explosions, much of the EMP is generated far from the detonation (where the gamma radiation from the explosion hits the upper atmosphere). This electric field from the EMP is remarkably uniform over the large area affected.\nAccording to the standard reference text on nuclear weapons effects published by the U.S. Department of Defense, \"The peak electric field (and its amplitude) at the Earth's surface from a high-altitude burst will depend upon the explosion yield, the height of the burst, the location of the observer, and the orientation with respect to the geomagnetic field. As a general rule, however, the field strength may be expected to be tens of kilovolts per metre over most of the area receiving the EMP radiation.\"\nThe text also states that, \"...over most of the area affected by the EMP the electric field strength on the ground would exceed 0.5\"E\"max. For yields of less than a few hundred kilotons, this would not necessarily be true because the field strength at the Earth's tangent could be substantially less than 0.5\"E\"max.\"\nIn other words, the electric field strength in the entire area that is affected by the EMP will be fairly uniform for weapons with a large gamma-ray output. For smaller weapons, the electric field may fall at a faster rate as distance increases.\nSuper-EMP.\nAlso known as an \"Enhanced-EMP\", a super-electromagnetic pulse is a relatively new type of warfare in which a nuclear weapon is designed to create a far greater electromagnetic pulse in comparison to standard nuclear weapons of mass destruction. These weapons capitalize on the E1 pulse component of a detonation involving gamma rays, creating an EMP yield of potentially up to 200,000 volts per meter. For decades, numerous countries have experimented with the creation of such weapons, most notably China and Russia.\nChina.\nAccording to a statement made in writing by the Chinese military, the country has super-EMPs and has discussed their use in attacking Taiwan. Such an attack would debilitate information systems in the nation, allowing China to move in and attack it directly using soldiers. The Taiwanese military has subsequently confirmed Chinese possession of super-EMPs and their possible destruction to power grids.\nIn addition to Taiwan, the possible implications of attacking the United States with these weapons was examined by China. While the United States also possesses nuclear weapons, the country has not experimented with super-EMPs and is hypothetically highly vulnerable to any future attacks by nations. This is due to the country's reliance on computers to control much of the government and economy. Abroad, U.S. aircraft carriers stationed within a reasonable range of an exploding bomb could potentially be subject to complete destruction of missiles on board, as well as telecommunication systems that would allow them to communicate with nearby vessels and controllers on land.\nRussia.\nSince the Cold War, Russia has experimented with the design and effects of EMP bombs.\nThe Soviet Union designed a system to deliver nuclear weapons from the low Earth orbit. and proposals have been made by Russia to develop satellites supplied with EMP capabilities. This would call for detonations up to above the Earth's surface, with the potential to disrupt the electronic systems of U.S. satellites suspended in orbit around the planet, many of which are vital for deterrence and alerting the country of possible incoming missiles.\nEffects.\nAn energetic EMP can temporarily upset or permanently damage electronic equipment by generating high voltage and high current surges; semiconductor components are particularly at risk. The effects of damage can range from imperceptible to the eye, to devices blowing apart. Cables, even if short, can act as antennas to transmit pulse energy to the equipment.\nVacuum tube vs. solid-state electronics.\nOlder, vacuum tube (valve)-based equipment is generally much less vulnerable to nuclear EMP than solid-state equipment, which is much more susceptible to damage by large, brief voltage and current surges. Soviet Cold War-era military aircraft often had avionics based on vacuum tubes because solid-state capabilities were limited and vacuum-tube gear was believed to be more likely to survive.\nOther components in vacuum tube circuitry can be damaged by EMP. Vacuum tube equipment was damaged in the 1962 testing. The solid-state PRC-77 VHF manpackable two-way radio survived extensive EMP testing. The earlier PRC-25, nearly identical except for a vacuum tube final amplification stage, was tested in EMP simulators, but was not certified to remain fully functional.\nElectronics in operation vs. inactive.\nEquipment that is running at the time of an EMP is more vulnerable. Even a low-energy pulse has access to the power source, and all parts of the system are illuminated by the pulse. For example, a high-current arcing path may be created across the power supply, burning out some device along that path. Such effects are hard to predict and require testing to assess potential vulnerabilities.\nOn aircraft.\nMany nuclear detonations have taken place using aerial bombs. The B-29 aircraft that delivered the nuclear weapons at Hiroshima and Nagasaki did not lose power from electrical damage, because electrons (ejected from the air by gamma rays) are stopped quickly in normal air for bursts below roughly , so they are not significantly deflected by the Earth's magnetic field.517\nIf the aircraft carrying the Hiroshima and Nagasaki bombs had been within the intense nuclear radiation zone when the bombs exploded over those cities, then they would have suffered effects from the charge separation (radial) EMP. But this only occurs within the severe blast radius for detonations below about altitude.\nDuring Operation Fishbowl, EMP disruptions were suffered aboard a KC-135 photographic aircraft flying from the detonations at burst altitudes. The vital electronics were less sophisticated than today's and the aircraft was able to land safely.\nModern aircraft are heavily reliant on solid-state electronics which are very susceptible to EMP blasts. Therefore, airline authorities are creating high intensity radiated fields (HIRF) requirements for new airplanes to help prevent the chance of crashes caused by EMPs or electromagnetic interference (EMI). To do this all parts of the airplane must be conductive. This is the main shield from EMP blasts as long as there are no holes for the waves to penetrate into the interior of the airplane. Also, insulating some of the main computers inside the plane adds an extra layer of protection from EMP blasts.\nOn cars.\nAn EMP would probably not affect most cars, despite modern cars' heavy use of electronics, because cars' electronic circuits and cabling are likely too short to be affected. In addition, cars' metallic frames provide some protection. However, even a small percentage of cars breaking down due to an electronic malfunction would cause traffic jams.\nOn small electronics.\nAn EMP has a smaller effect on shorter lengths of an electrical conductor. Other factors affect the vulnerability of electronics as well, so no hard cutoff length determines whether some piece of equipment will survive. However, small electronic devices, such as wristwatches and cell phones, would most likely withstand an EMP.\nOn humans and animals.\nThough electric potential difference can accumulate in electrical conductors after an EMP, it will generally not flow out into human or animal bodies, and thus contact is safe.\nEMPs of sufficient magnitude and length have the potential to affect the human body. Possible side effects include cellular mutations, nervous system damages, internal burns, brain damage, and temporary problems with thinking and memory. However, this would be in extreme cases like being near the center of the blast and being exposed to a large amount of radiation and EMP waves.\nA study found that exposure to 200\u2013400 pulses of EMP caused the leaking of vessels in the brain, leakage that has been linked to small problems with thinking and memory recollection. These effects could last up to 12 hours after the exposure. Due to the long exposure time needed to see any of these effects it is unlikely that anyone would see these effects even if exposed for a small period of time. Also, the human body will see little effect as signals are passed chemically and not electrically making it hard to be affected by EMP waves.\nIndirect effects on agriculture.\nIn addition to these direct effects, it has also been estimated that the disruption caused by the NEMP would have large negative effects on agriculture, due to the disruption of supply chains for agricultural inputs like fertilizers and pesticides. This could reduce yields in highly industrialized agricultural regions like Central Europe by up to 75%.\nPost\u2013Cold War attack scenarios.\nThe United States EMP Commission was created by the United States Congress in 2001. The commission is formally known as the Commission to Assess the Threat to the United States from Electromagnetic Pulse (EMP) Attack.\nThe Commission brought together notable scientists and technologists to compile several reports. In 2008, the Commission released the \"Critical National Infrastructures Report\". This report describes the likely consequences of a nuclear EMP on civilian infrastructure. Although this report covered the United States, most of the information is applicable to other industrialized countries. The 2008 report was a follow-up to a more generalized report issued by the commission in 2004.\nIn written testimony delivered to the United States Senate in 2005, an EMP Commission staff member reported:\nThe United States EMP Commission determined that long-known protections are almost completely absent in the civilian infrastructure of the United States and that large parts of US military services were less-protected against EMP than during the Cold War. In public statements, the Commission recommended making electronic equipment and electrical components resistant to EMP \u2013 and maintaining spare parts inventories that would enable prompt repairs. The United States EMP Commission did not look at other nations.\nIn 2011, the Defense Science Board published a report about the ongoing efforts to defend critical military and civilian systems against EMP and other nuclear weapons effects.\nThe United States military services developed, and in some cases published, hypothetical EMP attack scenarios.\nIn 2016, the Los Alamos Laboratory started phase 0 of a multi-year study (through to phase 3) to investigate EMPs which prepared the strategy to be followed for the rest of the study.\nIn 2017, the US Department of Energy published the \"DOE Electromagnetic Pulse Resilience Action Plan\", Edwin Boston published a dissertation on the topic and the EMP Commission published \"Assessing the threat from electromagnetic pulse (EMP)\". The EMP commission was closed in summer 2017. They found that earlier reports had underestimated the effects of an EMP attack on the national infrastructure, highlighted issues with communications from the DoD due to the classified nature of the material, and recommended that the DHS instead of going to the DOE for guidance and direction should directly cooperate with the more knowledgeable parts of the DOE. Several reports are in process of being released to the general public.\nProtecting infrastructure.\nThe problem of protecting civilian infrastructure from electromagnetic pulse has been intensively studied throughout the European Union, and in particular by the United Kingdom.\nAs of 2017, several electric utilities in the United States had been involved in a three-year research program on the impact of HEMP to the United States power grid led by an industry non-profit organization, Electric Power Research Institute (EPRI).\nIn 2018, the US Department of Homeland Security released the Strategy for Protecting and Preparing the Homeland against Threats from Electromagnetic Pulse (EMP) and Geomagnetic Disturbance (GMD), which was the department's first articulation of a holistic, long-term, partnership-based approach to protecting critical infrastructure and preparing to respond and recover from potentially catastrophic electromagnetic incidents. Progress on that front is described in the EMP Program Status Report.\nNuScale, the small modular nuclear reactor company from Oregon, US, has made their reactor resistant to EMP.\nIn fiction and popular culture.\nBy 1981, a number of articles on nuclear electromagnetic pulse in the popular press spread knowledge of the nuclear EMP phenomenon into the popular culture. EMP has been subsequently used in a wide variety of fiction and other aspects of popular culture.\nThe popular media often depict EMP effects incorrectly, causing misunderstandings among the public and even professionals, and official efforts have been made in the United States to set the record straight. The United States Space Command commissioned science educator Bill Nye to narrate and produce a video called \"Hollywood vs. EMP\", so that inaccurate Hollywood fiction would not confuse those who must deal with real EMP events. The video is not available to the general public.\nReferences.\nCitations.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nSources.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;\nFurther reading.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "41098", "revid": "27015025", "url": "https://en.wikipedia.org/wiki?curid=41098", "title": "Electromagnetic radiation and health", "text": "Aspect of public health\nElectromagnetic radiation can be classified into two types: ionizing radiation and non-ionizing radiation, based on the capability of a single photon with more than 10\u00a0eV energy to ionize atoms or break chemical bonds. Extreme ultraviolet and higher frequencies, such as X-rays or gamma rays are ionizing, and these pose their own special hazards: see \"radiation poisoning\". The field strength of electromagnetic radiation is measured in volts per meter (V/m).\nThe most common health hazard of radiation is sunburn, which causes between approximately 100,000 and 1 million new skin cancers annually in the United States.\nIn 2011, the World Health Organization (WHO) and the International Agency for Research on Cancer (IARC) have classified radiofrequency electromagnetic fields as possibly carcinogenic to humans (Group 2B).\nHazards.\nDielectric heating from electromagnetic radiation can create a biological hazard. For example, touching or standing around an antenna while a high-power transmitter is in operation can cause burns. The mechanism is the same as that used in a microwave oven.\nThe heating effect varies with the power and the frequency of the electromagnetic energy, as well as the inverse square of distance to the source. The eyes and testes are particularly susceptible to radio frequency heating due to the paucity of blood flow in these areas that could otherwise dissipate the heat buildup.\nWorkplace exposure.\nRadio frequency (RF) energy at power density levels of 1\u201310\u00a0mW/cm2 or higher can cause measurable heating of tissues. Typical RF energy levels encountered by the general public are well below the level needed to cause significant heating, but certain workplace environments near high power RF sources may exceed safe exposure limits. A measure of the heating effect is the specific absorption rate or SAR, which has units of watts per kilogram (W/kg). The IEEE and many national governments have established safety limits for exposure to various frequencies of electromagnetic energy based on SAR, mainly based on ICNIRP Guidelines, which guard against thermal damage.\nIndustrial installations for induction hardening and melting or on welding equipment may produce considerably higher field strengths and require further examination. If the exposure cannot be determined upon manufacturers' information, comparisons with similar systems or analytical calculations, measurements have to be accomplished. The results of the evaluation help to assess possible hazards to the safety and health of workers and to define protective measures. Since electromagnetic fields may influence passive or active implants of workers, it is essential to consider the exposure at their workplaces separately in the risk assessment.\nLow-level exposure.\nThe World Health Organization (WHO) began a research effort in 1996 to study the health effects from the ever-increasing exposure of people to a diverse range of EMR sources. In 2011, the WHO/International Agency for Research on Cancer (IARC) classified radio frequency electromagnetic fields as possibly carcinogenic to humans (Group 2B), based on an increased risk for glioma and acoustic neuroma associated with wireless phone use. The group responsible for the classification did not quantify the risk. Furthermore, group 2B only indicates a credible association between disease and exposure but does not rule out confounding effects with reasonable confidence. A causal relationship has yet to be established.\nEpidemiological studies look for statistical correlations between EM exposure in the field and specific health effects. As of 2019, much of the current work is focused on the study of EM fields in relation to cancer. There are publications which support the existence of complex biological and neurological effects of weaker \"non-thermal\" electromagnetic fields (see Bioelectromagnetics), including weak ELF electromagnetic fields and modulated RF and microwave fields.\nEffects by frequency.\nWhile the most acute exposures to harmful levels of electromagnetic radiation are immediately realized as burns, the health effects due to chronic or occupational exposure may not manifest for months or years.\nExtremely low frequency.\nExtremely low frequency EM waves can span from 0 Hz to 3 kHz, though definitions vary across disciplines. The maximum recommended exposure for the general public is 5 kV/m.\nELF waves around 50 Hz to 60 Hz are emitted by power generators, transmission lines and distribution lines, power cables, and electric appliances. Typical household exposure to ELF waves ranges in intensity from 5 V/m for a light bulb to 180 V/m for a stereo, measured at and using 240V power. (120V power systems would be unable to reach this intensity unless an appliance has an internal voltage transformer.) \nOverhead power lines range from 1kV for local distribution to 1,150 kV for ultra high voltage lines. These can produce electric fields up to 10kV/m on the ground directly underneath, but 50 m to 100 m away these levels return to approximately ambient. Metal equipment must be maintained at a safe distance from energized high-voltage lines.\nExposure to ELF waves can induce an electric current. Because the human body is conductive, electric currents and resulting voltages differences typically accumulate on the skin but do not reach interior tissues. People can start to perceive high-voltage charges as tingling when hair or clothing in contact with the skin stands up or vibrates. In scientific tests, only about 10% of people could detect a field intensity in the range of 2-5 kV/m. Such voltage differences can also create electric sparks, similar to a discharge of static electricity when nearly touching a grounded object. When receiving such a shock at 5 kV/m, it was reported as painful by only 7% of test participants and by 50% of participants at 10 kV/m.\nAssociations between exposure to extremely low frequency magnetic fields (ELF MF) and various health outcomes have been investigated through a variety of epidemiological studies. A pooled analysis found consistent evidence of an effect of ELF-MF on childhood leukaemia. An assessment of the burden of disease potentially resulting from ELF MF exposure in Europe found that 1.5\u20132% of childhood leukaemia cases might be attributable to ELF MF, but uncertainties around causal mechanisms and models of dose-response were found to be considerable. \nThe International Agency for Research on Cancer (IARC) finds \"inadequate evidence\" for human carcinogenicity.\nShortwave.\nShortwave (1.6 to 30\u00a0MHz) diathermy (where EM waves are used to produce heat) can be used as a therapeutic technique for its analgesic effect and deep muscle relaxation, but has largely been replaced by ultrasound. Temperatures in muscles can increase by 4\u20136\u00a0\u00b0C, and subcutaneous fat by 15\u00a0\u00b0C. The FCC has restricted the frequencies allowed for medical treatment, and most machines in the US use 27.12\u00a0MHz. Shortwave diathermy can be applied in either continuous or pulsed mode. The latter came to prominence because the continuous mode produced too much heating too rapidly, making patients uncomfortable. The technique only heats tissues that are good electrical conductors, such as blood vessels and muscle. Adipose tissue (fat) receives little heating by induction fields because an electrical current is not actually going through the tissues.\nStudies have been performed on the use of shortwave radiation for cancer therapy and promoting wound healing, with some success. However, at a sufficiently high energy level, shortwave energy can be harmful to human health, potentially causing damage to biological tissues, for example by overheating or inducing electrical currents. The FCC limits for maximum permissible workplace exposure to shortwave radio frequency energy in the range of 3\u201330\u00a0MHz has a plane-wave equivalent power density of where f is the frequency in MHz, and 100\u00a0mW/cm2 from 0.3 to 3.0\u00a0MHz. For uncontrolled exposure to the general public, the limit is between 1.34 and 30\u00a0MHz.\nRadio and microwave frequencies.\nThe classification of mobile phone signals as \"possibly carcinogenic to humans\" by the World Health Organization (WHO)\u2009\u2014\u200a\"a positive association has been observed between exposure to the agent and cancer for which a causal interpretation is considered by the Working Group to be credible, but chance, bias or confounding could not be ruled out with reasonable confidence\"\u2014\u2009has been interpreted to mean that there is very little scientific evidence as to phone signal carcinogenesis.\nIn 2011, the International Agency for Research on Cancer (IARC) classified mobile phone radiation as group 2B, i.e. \"possibly carcinogenic,\" rather than group 2A (\"probably carcinogenic\") or group 1 (\"is carcinogenic\"). That means that there \"could be some risk\" of carcinogenicity, so additional research into the long-term, heavy use of mobile phones needs to be conducted. The WHO concluded in 2014 that \"A large number of studies have been performed over the last two decades to assess whether mobile phones pose a potential health risk. To date, no adverse health effects have been established as being caused by mobile phone use.\"\nSince 1962, the microwave auditory effect or tinnitus has been shown from radio frequency exposure at levels below significant heating. Studies during the 1960s in Europe and Russia claimed to show effects on humans, especially the nervous system, from low energy RF radiation; the studies were disputed at the time.\nIn 2019, reporters from the Chicago Tribune tested the level of radiation from smartphones and found that certain models emitted more than reported by the manufacturers and in some cases more than the U.S. Federal Communications Commission exposure limit. It is unclear if this resulted in any harm to consumers. Some problems apparently involved the phone's ability to detect proximity to a human body and lower the radio power. In response, the FCC began testing some phones itself rather than relying solely on manufacturer certifications.\nMicrowave and other radio frequencies cause heating, and this can cause burns or eye damage if delivered in high intensity, or hyperthermia as with any powerful heat source. Microwave ovens use this form of radiation, and have shielding to prevent it from leaking out and unintentionally heating nearby objects or people.\nMillimeter waves.\nIn 2009, the US TSA introduced full-body scanners as a primary screening modality in airport security, first as backscatter X-ray scanners, which use ionizing radiation and which the European Union banned in 2011 due to health and safety concerns. These were followed by non-ionizing millimeter wave scanners. Likewise WiGig for personal area networks have opened the 60\u00a0GHz and above microwave band to SAR exposure regulations. Previously, microwave applications in these bands were for point-to-point satellite communication with minimal human exposure.\nInfrared.\nInfrared wavelengths longer than 750\u00a0nm can produce changes in the lens of the eye. Glassblower's cataract is an example of a heat injury that damages the anterior lens capsule among unprotected glass and iron workers. Cataract-like changes can occur in workers who observe glowing masses of glass or iron without protective eyewear for prolonged periods over many years.\nExposing skin to infrared radiation near visible light (IR-A) leads to increased production of free radicals. Short-term exposure can be beneficial (activating protective responses), while prolonged exposure can lead to photoaging.\nAnother important factor is the distance between the worker and the source of radiation. In the case of arc welding, infrared radiation decreases rapidly as a function of distance, so that farther than three feet away from where welding takes place, it does not pose an ocular hazard anymore but, ultraviolet radiation still does. This is why welders wear tinted glasses and surrounding workers only have to wear clear ones that filter UV.\nVisible light.\nPhotic retinopathy is damage to the macular area of the eye's retina that results from prolonged exposure to sunlight, particularly with dilated pupils. This can happen, for example, while observing a solar eclipse without suitable eye protection. The Sun's radiation creates a photochemical reaction that can result in visual dazzling and a scotoma. The initial lesions and edema will disappear after several weeks, but may leave behind a permanent reduction in visual acuity.\nModerate and high-power lasers are potentially hazardous because they can burn the retina of the eye, or even the skin. To control the risk of injury, various specifications \u2013 for example ANSI Z136 in the US, EN 60825-1/A2 in Europe, and IEC 60825 internationally \u2013 define \"classes\" of lasers depending on their power and wavelength. Regulations prescribe required safety measures, such as labeling lasers with specific warnings, and wearing laser safety goggles during operation (see laser safety).\nAs with its infrared and ultraviolet radiation dangers, welding creates an intense brightness in the visible light spectrum, which may cause temporary flash blindness. Some sources state that there is no minimum safe distance for exposure to these radiation emissions without adequate eye protection.\nUltraviolet.\nSunlight includes sufficient ultraviolet power to cause sunburn within hours of exposure, and the burn severity increases with the duration of exposure. This effect is a response of the skin called erythema, which is caused by a sufficient strong dose of UV-B. The Sun's UV output is divided into UV-A and UV-B: solar UV-A flux is 100 times that of UV-B, but the erythema response is 1,000 times higher for UV-B. This exposure can increase at higher altitudes and when reflected by snow, ice, or sand. The UV-B flux is 2\u20134 times greater during the middle 4\u20136 hours of the day, and is not significantly absorbed by cloud cover or up to a meter of water.\nUltraviolet light, specifically UV-B, has been shown to cause cataracts and there is some evidence that sunglasses worn at an early age can slow its development in later life. Most UV light from the sun is filtered out by the atmosphere and consequently airline pilots often have high rates of cataracts because of the increased levels of UV radiation in the upper atmosphere. It is hypothesized that depletion of the ozone layer and a consequent increase in levels of UV light on the ground may increase future rates of cataracts. Note that the lens filters UV light, so if it is removed via surgery, one may be able to see UV light.\nProlonged exposure to ultraviolet radiation from the sun can lead to melanoma and other skin malignancies. Clear evidence establishes ultraviolet radiation, especially the non-ionizing medium wave UVB, as the cause of most non-melanoma skin cancers, which are the most common forms of cancer in the world. UV rays can also cause wrinkles, liver spots, moles, and freckles. In addition to sunlight, other sources include tanning beds, and bright desk lights. Damage is cumulative over one's lifetime, so that permanent effects may not be evident for some time after exposure.\nUltraviolet radiation of wavelengths shorter than 300\u00a0nm (actinic rays) can damage the corneal epithelium. This is most commonly the result of exposure to the sun at high altitude, and in areas where shorter wavelengths are readily reflected from bright surfaces, such as snow, water, and sand. UV generated by a welding arc can similarly cause damage to the cornea, known as \"arc eye\" or welding flash burn, a form of photokeratitis.\nFluorescent light bulbs and tubes internally produce ultraviolet light. Normally this is converted to visible light by the phosphor film inside a protective coating. When the film is cracked by mishandling or faulty manufacturing then UV may escape at levels that could cause sunburn or even skin cancer.\nRegulation.\nIn the United States, non-ionizing radiation is regulated in the Radiation Control for Health and Safety Act of 1968 and the Occupational Safety and Health Act of 1970. In Canada, various federal acts govern non-ionizing radiation by originating source, such as the Radiation Emitting Devices Act, the Canada Consumer Product Safety Act, and the Radiocommunication Act. For situations not under federal jurisdiction, Canadian provinces individually set regulations around use of non-ionizing radiation.\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nFurther reading.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "41099", "revid": "47070250", "url": "https://en.wikipedia.org/wiki?curid=41099", "title": "Electromagnetic survivability", "text": ""}
{"id": "41100", "revid": "47070250", "url": "https://en.wikipedia.org/wiki?curid=41100", "title": "Electronic deception", "text": ""}
{"id": "41101", "revid": "43283345", "url": "https://en.wikipedia.org/wiki?curid=41101", "title": "Electronic switching system", "text": "Telephone switch that interconnects telephone circuits\nIn telecommunications, an electronic switching system (ESS) is a telephone switch that uses solid-state electronics, such as digital electronics and computerized common control, to interconnect telephone circuits for the purpose of establishing telephone calls.\nThe generations of telephone switches before the advent of electronic switching in the 1950s used purely electro-mechanical relay systems and analog voice paths. These early machines typically utilized the step-by-step technique. The first generation of electronic switching systems in the 1960s were not entirely digital in nature, but used reed relay-operated metallic paths or crossbar switches operated by stored program control (SPC) systems.\nFirst announced in 1955, the first customer trial installation of an all-electronic central office commenced in Morris, Illinois in November 1960 by Bell Laboratories. The first large-scale electronic switching system was the Number One Electronic Switching System (1ESS) of the Bell System, cut over in Succasunna, New Jersey, in May 1965.\nJust three years later, in September 1968, Britain's Post Office opened the world's first all-digital pulse-code modulation (PCM) exchange named \"Empress\" (three decades after British scientist Alec Reeves had invented the PCM encoding system without the digital components to take full advantage). Other nations vying to reach the forefront of technical innovation would adopt metal\u2013oxide\u2013semiconductor (MOS) and PCM technologies to make their own transitions from analog to digital telephony throughout the 1970s. Later electronic switching systems implemented the digital representation of the electrical audio signals on subscriber loops by digitizing the analog signals and processing the resulting data for transmission between central offices. Time-division multiplexing (TDM) technology permitted the simultaneous transmission of multiple telephone calls on a single wire connection between central offices or other electronic switches, resulting in dramatic capacity improvements of the telephone network.\nWith the advances of digital electronics starting in the 1960s telephone switches employed semiconductor device components in increasing measure.\nIn the late 20th century most telephone exchanges without TDM processing were eliminated and the term \"electronic switching system\" became largely a historical distinction for the older SPC systems.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "41102", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41102", "title": "Electronic warfare support measures", "text": "Electronic intelligence gathering method\nIn military telecommunications, electronic support (ES) or electronic support measures (ESM) gather intelligence through passive \"listening\" to electromagnetic radiations of military interest. They are an aspect of electronic warfare involving actions taken under direct control of an operational commander to detect, intercept, identify, locate, record, and/or analyze sources of radiated electromagnetic energy for the purposes of immediate threat recognition (such as warning that fire control radar has locked on a combat vehicle, ship, or aircraft) or longer-term operational planning. Thus, electronic support provides a source of information required for decisions involving electronic protection (EP), electronic attack (EA), avoidance, targeting, and other tactical employment of forces. Electronic support data can be used to produce signals intelligence (SIGINT), communications intelligence (COMINT) and electronics intelligence (ELINT).\nElectronic support measures can provide (1) initial detection or knowledge of foreign systems, (2) a library of technical and operational data on foreign systems, and (3) tactical combat information utilizing that library. ESM collection platforms can remain electronically silent and detect and analyze RADAR transmissions beyond the RADAR detection range because of the greater power of the transmitted electromagnetic pulse with respect to a reflected echo of that pulse. United States airborne ESM receivers are designated in the AN/ALR series.\nDesirable characteristics for electromagnetic surveillance and collection equipment include (1) wide-spectrum or bandwidth capability because foreign frequencies are initially unknown, (2) wide dynamic range because the signal strength is initially unknown, (3) narrow bandpass to discriminate the signal of interest from other electromagnetic radiation on nearby frequencies, and (4) good angle-of arrival measurement for bearings to locate the transmitter. The frequency spectrum of interest ranges from 30\u00a0MHz to 50\u00a0GHz. Multiple receivers are typically required for surveillance of the entire spectrum, but tactical receivers may be functional within a specific signal strength threshold of a smaller frequency range.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41103", "revid": "20483999", "url": "https://en.wikipedia.org/wiki?curid=41103", "title": "Electro\u2013optic effect", "text": "Changes in optical properties from applied electric fields\nAn electro\u2013optic effect is a change in the optical properties of a material in response to an electric field that varies slowly compared with the frequency of light. The term encompasses a number of distinct phenomena, which can be subdivided into\nIn December 2015, two further electro-optic effects of type (b) were theoretically predicted to exist but have not, as yet, been experimentally observed.\nChanges in absorption can have a strong effect on refractive index for wavelengths near the absorption edge, due to the Kramers\u2013Kronig relation.\nUsing a less strict definition of the electro-optic effect allowing also electric fields oscillating at optical frequencies, one could also include nonlinear absorption (absorption depends on the light intensity) to category a) and the optical Kerr effect (refractive index depends on the light intensity) to category b). Combined with the photoeffect and photoconductivity, the electro-optic effect gives rise to the photorefractive effect.\nThe term \"electro-optic\" is often erroneously used as a synonym for \"optoelectronic\".\nApplications.\nElectro-optic modulators.\nElectro-optic modulators are usually built with electro-optic crystals exhibiting the Pockels effect. The transmitted beam is phase modulated with the electric signal applied to the crystal. Amplitude modulators can be built by putting the electro-optic crystal between two linear polarizers or in one path of a Mach\u2013Zehnder interferometer.\nAdditionally, Amplitude modulators can be constructed by deflecting the beam into and out of a small aperture such as a fiber. This design can be low loss (&lt;3\u00a0dB) and polarization independent depending on the crystal configuration.\nElectro-optic deflectors.\nElectro-optic deflectors utilize prisms of electro-optic crystals. The index of refraction is changed by the Pockels effect, thus changing the direction of propagation of the beam inside the prism. Electro-optic deflectors have only a small number of resolvable spots, but possess a fast response time. There are few commercial models available at this time. This is because of competing acousto-optic deflectors, the small number of resolvable spots and the relatively high price of electro-optic crystals.\nElectro-optic field sensors.\nThe electro-optic Pockels effect in nonlinear crystals (e.g. KDP, BSO, K*DP) can be used for electric field sensing via polarisation state modulation techniques. In this scenario, an unknown electric field results in polarisation rotation of a laser beam propagating through the electro-optic crystal; through inclusion of polarisers to modulate the light intensity incident on a photodiode, a time-resolved electric field measurement can be reconstructed from the obtained voltage trace. As the signals obtained from the crystalline probes are optical, they are inherently resistant to electrical noise pickup, hence can be used for low-noise field measurement even in areas with high levels of electromagnetic noise in the vicinity of the probe. Furthermore, as the polarisation rotation due to the Pockels effect scales linearly with electric field, \"absolute\" field measurements are obtained, with no need for numerical integration to reconstruct electric fields, as is the case with conventional probes sensitive to the time-derivative of the electric field. \nElectro-optic measurements of strong electromagnetic pulses from intense laser-matter interactions have been demonstrated in both the nanosecond and picosecond (sub-petawatt) laser pulse driver regimes.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41104", "revid": "20483999", "url": "https://en.wikipedia.org/wiki?curid=41104", "title": "Electro-optic modulator", "text": "Type of optical device\nAn electro\u2013optic modulator (EOM) is an optical device in which a signal-controlled element exhibiting an electro\u2013optic effect is used to modulate a beam of light. The modulation may be imposed on the phase, frequency, amplitude, or polarization of the beam. Modulation bandwidths extending into the gigahertz range are possible with the use of laser-controlled modulators.\nThe electro\u2013optic effect describes two phenomena, the change of absorption and the change in the refractive index of a material, resulting from the application of a DC or an electric field with much lower frequency than the optical carrier. This is caused by forces that distort the position, orientation, or shape of the molecules constituting the material. Generally, a nonlinear optical material, such as ferroelectrics like lithium niobate (LiNbO3) or barium titanate (BaTiO3), polymers, or organic electro\u2013optic materials, with an incident static or low frequency optical field will see a modulation of its refractive index.\nThe simplest kind of EOM consists of a crystal, such as lithium niobate, whose refractive index is a function of the strength of the local electric field. That means that if lithium niobate is exposed to an electric field, light will travel more slowly through it. But the phase of the light leaving the crystal is directly proportional to the length of time it takes that light to pass through it. Therefore, the phase of the laser light exiting an EOM can be controlled by changing the electric field in the crystal.\nNote that the electric field can be created by placing a parallel plate capacitor across the crystal. Since the field inside a parallel plate capacitor depends linearly on the potential, the index of refraction depends linearly on the field (for crystals where Pockels effect dominates), and the phase depends linearly on the index of refraction, the phase modulation must depend linearly on the potential applied to the EOM.\nThe voltage required for inducing a phase change of formula_1 is called the half-wave voltage (formula_2). For a Pockels cell, it is usually hundreds or even thousands of volts, so that a high-voltage amplifier is required. Suitable electronic circuits can switch such large voltages within a few nanoseconds, allowing the use of EOMs as fast optical switches.\nLiquid-crystal devices are electro\u2013optical phase modulators if no polarizers are used.\nPhase modulation.\nPhase modulation (PM) is a modulation pattern that encodes information as variations in the instantaneous phase of a carrier wave.\nThe phase of a carrier signal is modulated to follow the changing voltage level (amplitude) of modulation signal. The peak amplitude and frequency of the carrier signal remain constant, but as the amplitude of the information signal changes, the phase of the carrier changes correspondingly. The analysis and the final result (modulated signal) are similar to those of frequency modulation.\nA very common application of EOMs is for creating sidebands in a monochromatic laser beam. To see how this works, first imagine that the strength of a laser beam with frequency formula_3 entering the EOM is given by\nformula_4\nNow suppose we apply a sinusoidally varying potential voltage to the EOM with frequency formula_5 and small amplitude formula_6. This adds a time dependent phase to the above expression,\nformula_7\nSince formula_6 is small, we can use the Taylor expansion for the exponential\nformula_9\nto which we apply a simple identity for sine,\nformula_10\nThis expression we interpret to mean that we have the original carrier signal plus two small sidebands, one at formula_11 and another at formula_12. Notice however that we only used the first term in the Taylor expansion \u2013 in truth there are an infinite number of sidebands. There is a useful identity involving Bessel functions called the Jacobi\u2013Anger expansion which can be used to derive\nformula_13\nwhich gives the amplitudes of all the sidebands. Notice that if one modulates the amplitude instead of the phase, one gets only the first set of sidebands,\nformula_14\nAmplitude modulation.\nA phase modulating EOM can also be used as an amplitude modulator by using a Mach\u2013Zehnder interferometer. This alternative technique is often used in integrated optics where the requirements of phase stability is more easily achieved. The beam splitter divides the laser light into two paths, one of which has a phase modulator as described above. The beams are then recombined. Changing the electric field on the phase modulating path will then determine whether the two beams interfere constructively or destructively at the output, and thereby control the amplitude or intensity of the exiting light. This device is called a Mach\u2013Zehnder modulator (MZM) and it is widely used as intensity modulator (IM) in fiber-optic communications.\nPolarization modulation.\nDepending on the type and orientation of the nonlinear crystal, and on the direction of the applied electric field, the phase delay can depend on the polarization direction. A Pockels cell can thus be seen as a voltage-controlled waveplate, and it can be used for modulating the polarization state. For a linear input polarization (often oriented at 45\u00b0 to the crystal axis), the output polarization will in general be elliptical, rather than simply a linear polarization state with a rotated direction.\nPolarization modulation in electro\u2013optic crystals can also be used as a technique for time-resolved measurement of unknown electric fields.\nCompared to conventional techniques using conductive field probes and cabling for signal transport to read-out systems, electro\u2013optical measurement is inherently noise resistant as signals are carried by fiber-optics, preventing distortion of the signal by electrical noise sources. The polarization change measured by such techniques is linearly dependent on the electric field applied to the crystal, hence providing absolute measurements of the field, without the need for numerical integration of voltage traces, as is the case for conductive probes sensitive to the time-derivative of the electric field.\nEOM technologies.\nEOMs can be based on many operating principles and platforms. One can divide the EOMs in two categories \u2013 phase and amplitude modulation. In the following some prominent approaches in Silicon photonics are presented. Operating principles for phase modulation are the plasma dispersion effect, pockels effect, interband transitions, and carrier accumulation/depletion+Franz-Keldysh effect. For the amplitude modulation some operating principles are the Franz\u2013Keldysh effect, quantum-confined Stark effect, and electrical gating.\nThe plasma dispersion effect can be based on carrier injection, depletion, or accumulation. The most established Pockels type modulators are based on the lithium niobate on silicon platform. In recent years, other platforms were introduced, such as BTO on silicon, silicon polymer hybrid, silicon organic hybrids, plasmonics and thin-film lithium niobate. Interband transition rely on 2D materials and the carrier accumulation/depletion+Franz\u2013Keldysh is based on a III-V platform.\nThe Franz\u2013Keldysh effect is used in electro-absorption modulators which are semiconductor devices. It describes a change in the absorption spectrum due to a shift in the band gap edge when an electric field is present. They are often built on a silicon\u2013germanium platform. Modulators running on the quantum confined stark effect can rely on a III-V platform or on Ge-Si-Ge quantum wells. Electrical gating is built on a 2D material platform."}
{"id": "41105", "revid": "357569", "url": "https://en.wikipedia.org/wiki?curid=41105", "title": "Electro-optics", "text": "Branch of material physics involving photoelectronic devices\nElectro\u2013optics is a branch of electrical engineering, electronic engineering, materials science, and material physics involving components, electronic devices such as lasers, laser diodes, LEDs, waveguides, etc. which operate by the propagation and interaction of light with various tailored materials. It is closely related to photonics, the branch of optics that involves the application of the generation of photons. It is not only concerned with the \"electro\u2013optic effect\", since it deals with the interaction between the electromagnetic (optical) and the electrical (electronic) states of materials.\nElectro-optical devices.\nThe electro-optic effect is a change in the optical properties of an optically active material in response to changes in an electric field. This interaction usually results in a change in the birefringence, and not simply the refractive index of the medium. In a Kerr cell, the change in birefringence is proportional to the square of the electric field, and the material is usually a liquid. In a Pockels cell, the change in birefringence varies linearly with the electric field, and the material is usually a crystal. Non-crystalline, solid electro-optical materials have generated interest because of their low cost of production. These organic, polymer-based materials are also known as organic EO material, plastic EO material, or polymer EO material. They consist of nonlinear optical chromophores in a polymer lattice. The nonlinear optical chromophores can produce Pockels effect."}
{"id": "41106", "revid": "252195", "url": "https://en.wikipedia.org/wiki?curid=41106", "title": "Elliptical polarization", "text": "Polarization of electromagnetic radiation\nIn electrodynamics, elliptical polarization is the polarization of electromagnetic radiation such that the tip of the electric field vector describes an ellipse in any fixed plane intersecting, and normal to, the direction of propagation. An elliptically polarized wave may be resolved into two linearly polarized waves in phase quadrature, with their polarization planes at right angles to each other. Since the electric field can rotate clockwise or counterclockwise as it propagates, elliptically polarized waves exhibit chirality.\n\"Circular polarization\" and \"linear polarization\" can be considered to be special cases of \"elliptical polarization\". This terminology was introduced by Augustin-Jean Fresnel in 1822, before the electromagnetic nature of light waves was known.\nMathematical description.\nThe classical sinusoidal plane wave solution of the electromagnetic wave equation for the electric and magnetic fields is (Gaussian units)\nformula_1\nformula_2\nwhere formula_3 is the wavenumber, formula_4 is the angular frequency of the wave propagating in the +z direction, and formula_5 is the speed of light.\nHere formula_6 is the amplitude of the field and\nformula_7\nis the normalized Jones vector. This is the most complete representation of polarized electromagnetic radiation and corresponds in general to elliptical polarization.\nPolarization ellipse.\nAt a fixed point in space (or for fixed z), the electric vector formula_8 traces out an ellipse in the x-y plane. The semi-major and semi-minor axes of the ellipse have lengths A and B, respectively, that are given by\nformula_9 \nand \nformula_10,\nwhere formula_11 with the phases formula_12 and formula_13.\nThe orientation of the ellipse is given by the angle formula_14 the semi-major axis makes with the x-axis. This angle can be calculated from\nformula_15.\nIf formula_16, the wave is linearly polarized. The ellipse collapses to a straight line formula_17) oriented at an angle formula_18. This is the case of superposition of two simple harmonic motions (in phase), one in the x direction with an amplitude formula_19, and the other in the y direction with an amplitude formula_20. When formula_21 increases from zero, i.e., assumes positive values, the line evolves into an ellipse that is being traced out in the counterclockwise direction (looking in the direction of the propagating wave); this then corresponds to \"left-handed elliptical polarization\"; the semi-major axis is now oriented at an angle formula_22. Similarly, if formula_21 becomes negative from zero, the line evolves into an ellipse that is being traced out in the clockwise direction; this corresponds to \"right-handed elliptical polarization\".\nIf formula_24 and formula_25, formula_26, i.e., the wave is circularly polarized. When formula_27, the wave is left-circularly polarized, and when formula_28, the wave is right-circularly polarized.\nParameterization.\n Any fixed polarization can be described in terms of the shape and orientation of the polarization ellipse, which is defined by two parameters: axial ratio AR and tilt angle formula_29. The axial ratio is the ratio of the lengths of the major and minor axes of the ellipse, and is always greater than or equal to one.\nAlternatively, polarization can be represented as a point on the surface of the Poincar\u00e9 sphere, with formula_30 as the longitude and formula_31 as the latitude, where formula_32. The sign used in the argument of the formula_33 depends on the handedness of the polarization. Positive indicates left hand polarization, while negative indicates right hand polarization, as defined by IEEE.\nFor the special case of circular polarization, the axial ratio equals 1 (or 0 dB) and the tilt angle is undefined. For the special case of linear polarization, the axial ratio is infinite.\nIn nature.\nThe reflected light from some beetles (e.g. \"Cetonia aurata\") is elliptical polarized.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41107", "revid": "8385469", "url": "https://en.wikipedia.org/wiki?curid=41107", "title": "Emphasis (telecommunications)", "text": "Process for reducing noise\nIn signal processing, pre-emphasis is a technique to protect against anticipated noise and loss. The idea is to boost (and hence distort) the frequency range that is most susceptible to noise and loss beforehand, so that after a noisy and lossy process (transmission over cable, tape recording...) more information can be recovered from that frequency range. Removal of the distortion caused by pre-emphasis is called de-emphasis, making the output accurately reproduce the original input.\nEmphasis is commonly used in many places ranging from FM broadcasting (preemphasis improvement) and vinyl (e.g. LP) records to PCI Express. For example, high-frequency signal components may be emphasized to produce a more equal modulation index for a transmitted frequency spectrum, and therefore a better signal-to-noise ratio for the entire frequency range.\nIn audio signals.\nIn processing electronic audio signals, pre-emphasis refers to a system process designed to increase (within a frequency band) the magnitude of some (usually higher) frequencies with respect to the magnitude of other (usually lower) frequencies in order to improve the overall signal-to-noise ratio by minimizing the adverse effects of such phenomena as attenuation distortion or saturation of recording media in subsequent parts of the system. The mirror operation is called de-emphasis, and the system as a whole is called emphasis.\nPre-emphasis is achieved with a pre-emphasis network which is essentially a calibrated filter. The frequency response is decided by special time constants. The cutoff frequency can be calculated from that value.\nPre-emphasis is commonly used in telecommunications, digital audio recording, record cutting, in FM broadcasting transmissions, and in displaying the spectrograms of speech signals. One example of this is the RIAA equalization curve on 33\u00a0rpm and 45\u00a0rpm vinyl records. Another is the Dolby noise-reduction system as used with magnetic tape.\nPre-emphasis is employed in frequency modulation or phase modulation transmitters to equalize the modulating signal drive power in terms of deviation ratio. The receiver demodulation process includes a reciprocal network, called a de-emphasis network, to restore the original signal power distribution.\nDe-emphasis.\nIn telecommunications, de-emphasis is the complement of pre-emphasis, in the antinoise system called emphasis. De-emphasis is a system process designed to decrease, (within a band of frequencies), the magnitude of some (usually higher) frequencies with respect to the magnitude of other (usually lower) frequencies in order to improve the overall signal-to-noise ratio by minimizing the adverse effects of such phenomena as attenuation distortion or saturation of recording media in subsequent parts of the system.\nSpecial time constants dictate the frequency response curve, from which one can calculate the cutoff frequency.\nRed Book audio.\nAlthough rarely used, there exists the capability for standardized emphasis in Red Book CD mastering. As CD players were originally implemented with affordable 14-bit converters, a specification for pre-emphasis was included to compensate for quantization noise. After economies of scale eventually allowed full 16 bits, quantization noise became less of a concern, but emphasis remained an option. The pre-emphasis is described as a first-order filter with a gain of 10\u00a0dB (at 20\u00a0dB/decade) and time constants 50\u00a0\u03bcs and 15\u00a0\u03bcs.\nIn digital transmission.\nIn serial data transmission, emphasis is used to improve signal quality at the output of a communication channel. In transmitting signals at high data rates, the transmission medium may introduce distortions, so emphasis is used to distort the transmitted signal to correct for this distortion. When done properly this produces a received signal that more closely resembles the original or desired signal, allowing the use of higher data rates or producing fewer bit errors. Most real world channels have loss that increases with frequency 6 (effectively a low pass filter), so emphasis needs to invert this effect (functioning as a high pass filter).8 This makes emphasis a form of equalization, implemented at the transmit side of the channel.\nEmphasis can be implemented either by boosting high frequencies (pre-emphasis, increasing the amplitude of transition bits) or attenuating low frequencies (de-emphasis, reducing the amplitude of non-transition bits). Both have the same net effect of producing a flatter system frequency response; de-emphasis is typically more convenient to do in real circuits since it only requires attenuation rather than amplification.9 Well-known serial data standards such as PCI Express, SATA and SAS require transmitted signals to use de-emphasis.\nEffects of channel insertion loss.\nAs a lossy channel becomes longer, high-frequency attenuation worsens and the signal will be increasingly distorted.\nIn the demonstration below, a 5 Gbps PRBS-9 test pattern is sent through PCB traces of various lengths on standard FR-4 material.\nAt some point, depending on the specifics of the channel, the transmitter, and the receiver, the signal will become too distorted for the receiver to correctly interpret it and the link will experience a high error rate or completely fail. Emphasis is one way to undo this distortion and enable communication to be successful over such a channel.\nAnalog R-C circuit.\nDe-emphasis can be implemented by means of an analog high-pass filter circuit in parallel with an attenuator. This weakens the entire signal by a fixed amount, then allows extra energy to bypass the attenuator when the signal changes. The end result is a sharp spike at each transition followed by an exponential decay to the steady-state amplitude. \nIn the demonstration below, a 5 Gbps PRBS-9 test pattern is sent through a 300mm FR-4 channel with increasing levels of de-emphasis. Note that as the emphasis is increased, the signal amplitude is reduced.\nUnlike the FIR architecture discussed in the next section, with analog emphasis the shape of the overshoot is *independent* of the signal bit rate. Thus, at lower data rates the entire bit's amplitude is not increased, only the edge. In the example below, a deliberately excessive level of emphasis is used to make the overshoot more visible.\n3-tap FIR.\nOne common implementation of emphasis in real SERDES is a 3-tap feed-forward equalizer (FFE): rather than driving the output pin with the desired output voltage directly, the actual output voltage is a weighted sum of the desired bit value (main cursor), the previous bit (post cursor), and the next bit to be transmitted (pre cursor).10,24 The main cursor coefficient controls the nominal amplitude of the bit and is always positive (as a negative coefficient would invert the bit value). The pre cursor coefficient removes ISI at the receiver caused by bits which have not yet arrived (e.g. fields coupling across meanders in a delay-matched trace) and is typically zero or a very small negative value, as this is often not a major contribution to total ISI. The post cursor coefficient removes ISI at the receiver caused by the immediately preceding bit and is typically a larger negative value,16 with lossier channels requiring a larger tap value. Higher numbers of taps are possible but increase circuit complexity and tend to result in diminishing returns 14 so are not commonly used.\nThe effects of emphasis on a signal can be clearly seen in the eye pattern. In the following demonstration, we consider a 10.3125 Gbps PRBS-31 test pattern with NRZ modulation, typical for testing 10-Gigabit Ethernet. The channel has an insertion loss of roughly 2 dB at the fundamental, 3 dB at the 2nd harmonic, and 4 dB at the 3rd. The goal is to achieve a well-equalized channel response in which the eye is maximally open without excessive overshoot. Excessive equalization can worsen jitter, increase overshoot, and result in a less open eye than a properly equalized signal.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "41108", "revid": "5183450", "url": "https://en.wikipedia.org/wiki?curid=41108", "title": "Encode", "text": "Encode or encoding may refer to:\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "41109", "revid": "892079", "url": "https://en.wikipedia.org/wiki?curid=41109", "title": "End distortion", "text": "Shifting of the end of pulses\nIn start-stop teletypewriter operation, end distortion refers to the shifting of the end of all marking pulses, except the stop pulse, from their proper positions in relation to the beginning of the next start pulse. \nShifting of the end of the stop pulse is a deviation in character time and rate rather than an end distortion. \nSpacing end distortion is the termination of marking pulses before the proper time. Marking end distortion is the continuation of marking pulses past the proper time. \nThe magnitude of the distortion is expressed as a percentage of an ideal pulse length.\nReferences.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41110", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41110", "title": "End-of-Transmission character", "text": "Transmission control character\nIn telecommunications, an End-of-Transmission character (EOT) is a transmission control character. Its intended use is to indicate the conclusion of a transmission that may have included one or more texts and any associated message headings.\nAn EOT is often used to initiate other functions, such as releasing circuits, disconnecting terminals, or placing receive terminals in a standby condition. Its most common use today is to cause a Unix terminal driver to signal end of file and thus exit programs that are awaiting input.\nIn ASCII and Unicode, the character is encoded at . It can be referred to as , ^D in caret notation. Unicode provides the character for when EOT needs to be displayed graphically. In addition, can also be used as a graphic representation of EOT; it is defined in Unicode as \"symbol for End of Transmission\".\nMeaning in Unix.\nThe EOT character in Unix is different from the Control-Z in DOS. The DOS Control-Z byte is actually sent and/or placed in files to indicate where the text ends. In contrast, the Control-D causes the Unix terminal driver to signal the EOF condition, which is not a character, while the byte has no special meaning if actually read or written from a file or terminal.\nIn Unix, the end-of-file character (by default EOT) causes the terminal driver to make available all characters in its input buffer immediately; normally the driver would collect characters until it sees an end-of-line character. If the input buffer is empty (because no characters have been typed since the last end-of-line or end-of-file), a program reading from the terminal reads a count of zero bytes. In Unix, such a condition is understood as having reached the end of the file.\nThis can be demonstrated with the cat program on Unix-like operating systems such as Linux: Run the cat command with no arguments, so it accepts its input from the keyboard and prints output to the screen. Type a few characters without pressing , then type . The characters typed to that point are sent to cat, which then writes them to the screen. If is typed without typing any characters first, the input stream is terminated and the program ends. An actual EOT is obtained by typing then .\nIf the terminal driver is in \"raw\" mode, it no longer interprets control characters, and the EOT character is sent unchanged to the program, which is free to interpret it any way it likes. A program may then decide to handle the EOT byte as an indication that it should end the text; this would then be similar to how is handled by DOS programs.\nUsage in mainframe computer system communications protocols.\nThe EOT character is used in legacy communications protocols by mainframe computer manufacturers such as IBM, Burroughs Corporation, and the BUNCH. Terminal transmission control protocols such as IBM 3270 Poll/Select, or Burroughs TD830 Contention Mode protocol use the EOT character to terminate a communications sequence between two cooperating stations (such as a host multiplexer or Input/Output terminal).\nA single Poll (ask the station for data) or Select (send data to the station) operation will include two round-trip send-reply operations between the polling station and the station being polled, the final operation being transmission of a single EOT character to the initiating station.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41111", "revid": "12652641", "url": "https://en.wikipedia.org/wiki?curid=41111", "title": "Endurability", "text": "Property\nIn telecommunications, endurability is the property of a system, subsystem, equipment, or process that enables it to continue to function within specified performance limits for an extended period of time, usually months, despite a severe natural or man-made disturbance, such as a nuclear attack, or a loss of external logistic or utility support. \nEndurability is not compromised by temporary failures when the local capability exists to restore and maintain the system, subsystem, equipment, or process to an acceptable performance level.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "41112", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41112", "title": "Enhanced service", "text": "Service used in interstate communications\nEnhanced service is service offered over commercial carrier transmission facilities used in interstate communications, that employs computer processing applications that act on the format, content, code, protocol, or similar aspects of the subscriber's transmitted information; provides the subscriber with additional, different, or restructured information; or involves subscriber interaction with stored information.\nReferences.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41113", "revid": "45223555", "url": "https://en.wikipedia.org/wiki?curid=41113", "title": "Epoch", "text": "Reference point from which time is measured\nIn chronology and periodization, an epoch or reference epoch is an instant in time chosen as the origin of a particular calendar era. The \"epoch\" serves as a reference point from which time is measured.\nThe moment of epoch is usually decided by congruity, or by following conventions understood from the epoch in question. The epoch moment or date is usually defined from a specific, clear event of change, an \"epoch event\". In a more gradual change, a deciding moment is chosen when the \"epoch criterion\" was reached.\nCalendar eras.\nRegnal eras.\nThe official Japanese system numbers years from the accession of the current emperor, regarding the calendar year during which the accession occurred as the first year. A similar system existed in China before 1912, being based on the accession year of the emperor (1911 was thus the third year of the Xuantong period). With the establishment of the Republic of China in 1912, the republican era was introduced. It is still very common in Taiwan to date events via the republican era. The People's Republic of China adopted the common era calendar in 1949 (the 38th year of the Chinese Republic).\nOther applications.\nAn epoch in computing is the time at which the representation is zero. For example, Unix time is represented as the number of seconds since 00:00:00 UTC on 1 January 1970, not counting leap seconds. \nAn epoch in astronomy is a reference time used for consistency in calculation of positions and orbits. A common astronomical epoch is J2000, which is noon on January 1, 2000, Terrestrial Time.\nAn epoch in geochronology is a time period, typically in the order of tens of millions of years. The current epoch is the Holocene.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41114", "revid": "252195", "url": "https://en.wikipedia.org/wiki?curid=41114", "title": "Equilibrium length", "text": ""}
{"id": "41115", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41115", "title": "Equivalent noise resistance", "text": "In telecommunications, an equivalent noise resistance is a quantitative representation in resistance units of the spectral density of a noise-voltage generator, given by\nformula_1\nwhere formula_2 is the spectral density, formula_3 is the Boltzmann constant, formula_4 is the standard noise temperature (290 K), so formula_5.\n\"Note:\" The equivalent noise resistance in terms of the mean-square noise-generator voltage, \"e\"2, within a frequency increment, \u0394\u00a0\"f\", is given by\n formula_6\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41116", "revid": "35498457", "url": "https://en.wikipedia.org/wiki?curid=41116", "title": "Equivalent pulse code modulation noise", "text": "In telecommunications, equivalent pulse code modulation (PCM) noise is the amount of noise power on a frequency-division multiplexing (FDM) or wire communication channel necessary to approximate the same judgment of speech quality created by quantization noise in a PCM channel. \n\"Note 1: The speech quality judgment is based on comparative tests.\"\n\"Note 2: Generally, 33.5 dBrnC \u00b12.5 dB is considered the approximate equivalent PCM noise of a 7-bit PCM system.\"\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41118", "revid": "47587808", "url": "https://en.wikipedia.org/wiki?curid=41118", "title": "Error", "text": "Incorrect or inaccurate action\nAn error (from the Latin , meaning 'to wander') is an inaccurate or incorrect action, thought, or judgement.\nIn statistics, \"error\" refers to the difference between the value which has been computed and the correct value. An error could result in failure or in a deviation from the intended performance or behavior.\nHuman behavior.\nOne reference differentiates between \"error\" and \"mistake\" as follows:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;An 'error' is a deviation from accuracy or correctness. A 'mistake' is an error caused by a fault: the fault being misjudgment, carelessness, or forgetfulness. Now, say that I run a stop sign because I was in a hurry, and wasn't concentrating, and the police stop me, that is a mistake. If, however, I try to park in an area with conflicting signs, and I get a ticket because I was incorrect on my interpretation of what the signs meant, that would be an error. The first time it would be an error. The second time it would be a mistake since I should have known better.\nIn human behavior the norms or expectations for behavior or its consequences can be derived from the intention of the actor or from the expectations of other individuals or from a social grouping or from social norms. (See deviance.) Gaffes and faux pas can be labels for certain instances of this kind of error. More serious departures from social norms carry labels such as misbehavior and labels from the legal system, such as misdemeanor and crime. Departures from norms connected to religion can have other labels, such as sin.\nLanguage.\nAn individual language user's deviations from standard language norms in grammar, pronunciation and orthography are sometimes referred to as errors. However, in light of the role of language usage in everyday social class distinctions, many feel that linguistics should restrain itself from such prescriptivist judgments to avoid reinforcing dominant class value claims about what linguistic forms should and should not be used. One may distinguish various kinds of linguistic errors \u2013 some, such as aphasia or speech disorders, where the user is unable to say what they intend to, are generally considered errors, while cases where natural, intended speech is non-standard (as in vernacular dialects), are considered legitimate speech in scholarly linguistics, but might be considered errors in prescriptivist contexts. See also Error analysis (linguistics).\nGaffe.\nA gaffe is usually made in a social environment and may come from saying something that may be true but inappropriate. It may also be an erroneous attempt to reveal a truth. Gaffes can be malapropisms, grammatical errors or other verbal and gestural weaknesses or revelations through body language. Actually revealing factual or social truth through words or body language, however, can commonly result in embarrassment or, when the gaffe has negative connotations, friction between people involved.\nPhilosophers and psychologists interested in the nature of the gaffe include Sigmund Freud (Freudian slip) and Gilles Deleuze. Deleuze, in his \"The Logic of Sense\", places the gaffe in a developmental process that can culminate in stuttering.\nSportswriters and journalists commonly use \"gaffe\" to refer to any kind of mistake, e.g. a dropped ball (baseball error) by a player in a baseball game.\nScience and engineering.\nIn statistics, an \"error\" (or \"residual\") is not a \"mistake\" but rather a difference between a computed, estimated, or measured value and the accepted true, specified, or theoretically correct value.\nIn science and engineering in general, an error is defined as a difference between the desired and actual performance or behavior of a system or object. This definition is the basis of operation for many types of control systems, in which error is defined as the difference between a set point and the process value. An example of this would be the thermostat in a home heating system \u2013 the operation of the heating equipment is controlled by the difference (the error) between the thermostat setting and the sensed air temperature. Another approach is related to considering a scientific hypothesis as true or false, giving birth to two types of errors: Type 1 and Type 2. The first one is when a true hypothesis is considered false, while the second is the reverse (a false one is considered true).\nEngineers seek to design devices, machines and systems and in such a way as to mitigate or preferably avoid the effects of error, whether unintentional or not. Such errors in a system can be latent design errors that may go unnoticed for years, until the right set of circumstances arises that cause them to become active. Other errors in engineered systems can arise due to human error, which includes cognitive bias. Human factors engineering is often applied to designs in an attempt to minimize this type of error by making systems more forgiving or error-tolerant.\nA notable result of Engineering and Scientific errors that occurred in history is the Chernobyl disaster of 1986, which caused a nuclear meltdown in the City of Chernobyl in present-day Ukraine, and is used as a case study in many Engineering/Science research \nNumerical analysis.\nNumerical analysis provides a variety of techniques to represent (store) and compute approximations to mathematical numerical values. Errors arise from a trade-off between efficiency (space and computation time) and precision, which is limited anyway, since (using common floating-point arithmetic) only a finite amount of values can be represented exactly. The discrepancy between the exact mathematical value and the stored/computed value is called the approximation error.\nCybernetics.\nIn applying corrections to the trajectory or course being steered, cybernetics can be seen as the most general approach to error and its correction for the achievement of any goal. The term was suggested by Norbert Wiener to describe a new science of control and information in the animal and the machine. Wiener's early work was on noise.\nThe cybernetician Gordon Pask held that the error that drives a servomechanism can be seen as a difference between a pair of analogous concepts in a servomechanism: the current state and the goal state. Later he suggested error can also be seen as an innovation or a contradiction depending on the context and perspective of interacting (observer) participants. The founder of management cybernetics, \nStafford Beer, applied these ideas most notably in his viable system model.\nBiology.\nIn biology, an \"error\" is said to occur when perfect fidelity is lost in the copying of information. For example, in an asexually reproducing species, an error (or mutation) has occurred for each DNA nucleotide that differs between the child and the parent. Many of these mutations can be harmful, but unlike other types of errors, some are neutral or even beneficial. Mutations are an important force driving evolution. Mutations that make organisms more adapted to their environment increase in the population through natural selection as organisms with favorable mutations have more offspring.\nPhilately.\nIn philately, an \"error\" refers to a postage stamp or piece of postal stationery that exhibits a printing or production mistake that differentiates it from a normal specimen or from the intended result. Examples are stamps printed in the wrong color or missing one or more colors, printed with a vignette inverted in relation to its frame, produced without any perforations on one or more sides when the normal stamps are perforated, or printed on the wrong type of paper. Legitimate errors must always be produced and sold unintentionally. Such errors may or may not be scarce or rare. A \"design error\" may refer to a mistake in the design of the stamp, such as a mislabeled subject, even if there are no printing or production mistakes.\nLaw.\nIn appellate review, error typically refers to mistakes made by a trial court or some other court of first instance in applying the law in a particular legal case. This may involve such mistakes as improper admission of evidence, inappropriate instructions to the jury, or applying the wrong standard of proof.\nStock market.\nA stock market error is a stock market transaction that was done due to an error, due to human failure or computer errors.\nGovernmental policy.\nWithin United States government intelligence agencies, such as Central Intelligence Agency agencies, \"error\" refers to \"intelligence error\", as previous assumptions that used to exist at a senior intelligence level within senior intelligence agencies, but has since been disproven, and is sometimes eventually listed as unclassified, and therefore more available to the public and citizenry of the United States. The Freedom of information act provides American citizenry with a means to read intelligence reports that were mired in error. Per United States Central Intelligence Agency's website (as of August, 2008) intelligence error is described as:\n\"Intelligence errors are factual inaccuracies in analysis resulting from poor or missing data; intelligence failure is systemic organizational surprise resulting from incorrect, missing, discarded, or inadequate hypotheses.\"\nNumismatics.\nIn numismatics, an \"error\" refers to a coin or medal that has a minting mistake, similar to errors found in philately. Because the U.S. Bureau of the Mint keeps a careful eye on all potential errors, errors on U.S. coins are very few and usually very scarce. Examples of numismatic errors: extra metal attached to a coin, a clipped coin caused by the coin stamp machine stamping a second coin too early, double stamping of a coin. A coin that has been overdated, e.g. 1942/41, is also considered an error.\nLinguistics.\nIn applied linguistics, an error is an unintended deviation from the immanent rules of a language variety made by a second language learner. Such errors result from the learner's lack of knowledge of the correct rules of the target language variety. A significant distinction is generally made between \"errors\" (systematic deviations) and \"mistakes\" (speech performance errors) which are not treated the same from a linguistic viewpoint. The study of learners' errors has been the main area of investigation by linguists in the history of second-language acquisition research.\nMedicine.\nA medical error is a preventable adverse effect of care (\"iatrogenesis\"), whether or not it is evident or harmful to the patient. This might include an inaccurate or incomplete diagnosis or treatment of a disease, injury, syndrome, behavior, infection, or other ailment.\nThe word \"error\" in medicine is used as a label for nearly all of the clinical incidents that harm patients. Medical errors are often described as human errors in healthcare. Whether the label is a medical error or human error, one definition used in medicine says that it occurs when a healthcare provider chooses an inappropriate method of care, improperly executes an appropriate method of care, or reads the wrong CT scan. It has been said that the definition should be the subject of more debate. For instance, studies of hand hygiene compliance of physicians in an ICU show that compliance varied from 19% to 85%. The deaths that result from infections caught as a result of treatment providers improperly executing an appropriate method of care by not complying with known safety standards for hand hygiene are difficult to regard as innocent accidents or mistakes.\nThere are many types of medical error, from minor to major, and causality is often poorly determined.\nThere are many taxonomies for classifying medical errors.\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41119", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41119", "title": "Burst error", "text": "Contiguous sequence of errors occurring in a communications channel\nIn telecommunications, a burst error or error burst is a contiguous sequence of symbols, received over a communication channel, such that the first and last symbols are in error and there exists no contiguous subsequence of \"m\" correctly received symbols within the error burst. The integer parameter \"m\" is referred to as the \"guard band\" of the error burst. The last symbol in a burst and the first symbol in the following burst are accordingly separated by \"m\" correct symbols or more. The parameter \"m\" should be specified when describing an error burst.\nChannel model.\nThe Gilbert\u2013Elliott model is a simple channel model introduced by Edgar Gilbert and E. O. Elliott that is widely used for describing burst error patterns in transmission channels and enables simulations of the digital error performance of communications links. It is based on a Markov chain with two states \"G\" (for good or gap) and \"B\" (for bad or burst). In state \"G\" the probability of transmitting a bit correctly is \"k\" and in state \"B\" it is \"h\". Usually, it is assumed that \"k\"\u00a0=\u00a01. Gilbert provided equations for deriving the other three parameters (\"G\" and \"B\" state transition probabilities and \"h\") from a given success/failure sequence. In his example, the sequence was too short to correctly find \"h\" (a negative probability was found) and so Gilbert assumed that \"h\"\u00a0=\u00a00.5.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41120", "revid": "46700810", "url": "https://en.wikipedia.org/wiki?curid=41120", "title": "Error-correcting code", "text": ""}
{"id": "41121", "revid": "159886", "url": "https://en.wikipedia.org/wiki?curid=41121", "title": "Error-detecting system", "text": ""}
{"id": "41122", "revid": "8066546", "url": "https://en.wikipedia.org/wiki?curid=41122", "title": "Error ratio", "text": ""}
{"id": "41123", "revid": "39349169", "url": "https://en.wikipedia.org/wiki?curid=41123", "title": "Escape character", "text": "Character used to start an escape sequence\nIn computing and telecommunications, an escape character is a character (more specifically a metacharacter) that, based on a contextual convention, specifies an alternative interpretation of the sequence of characters that follow it. The escape character plus the characters that follow it to form a syntactic unit is called an escape sequence. A convention can define any particular character code as a sequence prefix. Some conventions use a normal, printable character such as backslash () or ampersand (). Others use a non-printable (a.k.a. control) character such as ASCII \"escape\".\nIn telecommunications, an escape character is used to indicate that the following characters are encoded differently. This is used to alter control characters that would otherwise be noticed and acted on by the underlying telecommunications hardware, such as illegal characters. In this context, the use of an escape character is sometimes referred to as quoting.\nDefinition.\nAn escape character may not have its own meaning, so all escape sequences are of two or more characters.\nEscape characters are part of the syntax for many programming languages, data formats, and communication protocols. For a given alphabet an escape character's purpose is to start character sequences (so named escape sequences), which have to be interpreted differently from the same characters occurring without the prefixed escape character.\nThe functions of escape sequences include:\nControl character.\nIn contrast to an escape character, a control character (i.e. carriage return) has meaning on its own; without a special prefix or following characters. An escape character has no meaning on its own. It only has meaning in the context of a sequence.\nGenerally, an escape character is not a particular case of (device) control characters, nor vice versa. If we define control characters as non-graphic, or as having a special meaning for an output device (e.g. printer or text terminal) then any escape character for this device is a control one. But escape characters used in programming (such as the backslash, ) are graphic, hence are not control characters. Conversely most (but not all) of the ASCII \"control characters\" have some control function in isolation, therefore they are not escape characters.\nIn many programming languages, an escape character also forms some escape sequences which are referred to as control characters. For example, line break has an escape sequence of .\nExamples.\nJavaScript.\nJavaScript uses the (backslash) as an escape character for:\nThe and escapes are not allowed in JSON strings.\nExample code:\nconsole.log(\"Using \\\\n \\nWill shift the characters after \\\\n one row down\")\nconsole.log(\"Using \\\\t \\twill shift the characters after \\\\t one tab length to the right\")\nconsole.log(\"Using \\\\r \\rWill imitate a carriage return, which means shifting to the start of the row\") // can be used to clear the screen on some terminals. Windows uses \\r\\n instead of \\n alone\nASCII escape character.\nThe ASCII \"escape\" character (octal: , hexadecimal: , or, in decimal, , also represented by the sequences or ) is used in many output devices to start a series of characters called a control sequence or escape sequence. Typically, the escape character was sent first in such a sequence to alert the device that the following characters were to be interpreted as a control sequence rather than as plain characters, then one or more characters would follow to specify some detailed action, after which the device would go back to interpreting characters normally. For example, the sequence of , followed by the printable characters , would cause a Digital Equipment Corporation (DEC) VT102 terminal to move its cursor to the 10th cell of the 2nd line of the screen. This was later developed into ANSI escape codes covered by the ANSI X3.64 standard. The escape character also starts each command sequence in the Hewlett-Packard Printer Command Language.\nAn early reference to the term \"escape character\" is found in Bob Bemer's IBM technical publications, who is credited with inventing this mechanism during his work on the ASCII character set.\nThe Escape key is usually found on standard PC keyboards. However, it is commonly absent from keyboards for PDAs and other devices not designed primarily for ASCII communications. The DEC VT220 series was one of the few popular keyboards that did not have a dedicated Esc key, instead of using one of the keys above the main keypad. In user interfaces of the 1970s\u20131980s it was not uncommon to use this key as an escape character, but in modern desktop computers, such use is dropped. Sometimes the key was identified with AltMode (for alternative mode). Even with no dedicated key, the escape character code could be generated by typing while simultaneously holding down .\nProgramming and data formats.\nMany modern programming languages specify the double-quote character () as a delimiter for a string literal. The backslash () escape character typically provides two ways to include double-quotes inside a string literal, either by modifying the meaning of the double-quote character embedded in the string ( becomes ), or by modifying the meaning of a sequence of characters including the hexadecimal value of a double-quote character ( becomes ).\nC, C++, Java, and Ruby all allow exactly the same two backslash escape styles. The PostScript language and Microsoft Rich Text Format also use backslash escapes. The quoted-printable encoding uses the equals sign as an escape character.\nURL and URI use %-escapes to quote characters with a special meaning, as for non-ASCII characters. The ampersand () character may be considered as an escape character in SGML and derived formats such as HTML and XML.\nSome programming languages also provide other ways to represent special characters in literals, without requiring an escape character (see e.g. delimiter collision).\nCommunication protocols.\nThe Point-to-Point Protocol (PPP) uses the octet (, or ASCII: codice_2) as an escape character. The octet immediately following should be XORed by before being passed to a higher level protocol. This is applied to both itself and the control character (which is used in PPP to mark the beginning and end of a frame) when those octets need to be transmitted by a higher level protocol encapsulated by PPP, as well as other octets negotiated when the link is established. That is, when a higher level protocol wishes to transmit , it is transmitted as the sequence , and is transmitted as .\nBourne shell.\nIn Bourne shell (sh), the asterisk () and question mark () characters are wildcard characters expanded via globbing. Without a preceding escape character, an will expand to the names of all files in the working directory that do not start with a period if and only if there are such files, otherwise remains unexpanded. So to refer to a file literally called \"*\", the shell must be told not to interpret it in this way, by preceding it with a backslash (). This modifies the interpretation of the asterisk ().\nCompare:\nSimilarly, characters like the ampersand, pipe and semicolon (used for command chaining), angle brackets (used for redirection), and parentheses have special syntactic meaning to the Bourne shell. These must also be escaped\u2014referred to as \"quoting\" in the manual page\u2014in order to be used literally as arguments to another program:\n$ echo (\uff40-\u00b4)&gt; # not escaped or quoted\nbash: syntax error near unexpected token `\uff40-\u00b4'\n$ echo \\(\uff40-\u00b4\\)\\&gt; # escaped with backslashes\n$ echo '(\uff40-\u00b4)&gt;' # protected by single quotes; same effect as above\n$ echo ;) # syntax error\n$ echo ';)' \\;\\) # both OK\nWindows Command Prompt.\nThe Windows command-line interpreter uses a caret character () to escape reserved characters that have special meanings (in particular: , codice_3, , , , , ). The DOS command-line interpreter, though it has similar syntax, does not support this.\nFor example, on the Windows Command Prompt, this will result in a syntax error.\nC:\\&gt;echo &lt;hello world&gt;\nThe syntax of the command is incorrect.\nwhereas this will output the string: \nC:\\&gt;echo ^&lt;hello world^&gt;\n&lt;hello world&gt;\nWindows PowerShell.\nIn Windows, the backslash is used as a path separator; therefore, it generally cannot be used as an escape character. PowerShell uses backtick ( ` ) instead.\nFor example, the following command:\nPS C:\\&gt; echo \"`tFirst line`nNew line\"\n First line\nNew line\nExternal links.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41124", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41124", "title": "Essential service (telecommunications)", "text": "In telecommunications, an essential service (critical service) is a network-provided service feature in which a priority dial tone is furnished. Essential service is typically provided to fewer than 10% of network users, and recommended for use in conjunction with NS/EP telecommunications services."}
{"id": "41125", "revid": "11521989", "url": "https://en.wikipedia.org/wiki?curid=41125", "title": "Exchange", "text": "Exchange or exchanged may refer to:\n&lt;templatestyles src=\"Template:TOC_right/styles.css\" /&gt;\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "41126", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41126", "title": "Exempted addressee", "text": "In telecommunications, an exempted addressee is an organization, activity, or person included in the collective address group of a message and deemed by the message originator as having no need for the information in the message.\nExempted addressees may be explicitly excluded from the collective address group for the particular message to which the exemption applies.\nReferences.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41127", "revid": "6593610", "url": "https://en.wikipedia.org/wiki?curid=41127", "title": "Extended-definition television", "text": ""}
{"id": "41128", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41128", "title": "Extended superframe", "text": ""}
{"id": "41130", "revid": "1234701", "url": "https://en.wikipedia.org/wiki?curid=41130", "title": "Extinction ratio", "text": "Concept of this parameter\nIn telecommunications, extinction ratio (\"r\"e) is the ratio of two optical power levels of a digital signal generated by an optical source, e.g., a laser diode. The extinction ratio may be expressed as a fraction, in dB, or as a percentage. It may be given by\nformula_1\nwhere \"P\"1 is the optical power level generated when the light source is on, and \"P\"0 is the power level generated when the light source is off.\nThe polarization extinction ratio (PER) is the ratio of optical powers of perpendicular polarizations, usually called TE (transverse electric) and TM (transverse magnetic). In telecommunications, the PER is used to characterize the degree of polarization in a polarization-maintaining device polarization-maintaining optical fiber. For coherent transmitter and receiver, the PER is a key parameter, since X\u00a0polarization and Y\u00a0polarization are coded with different signals.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41131", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41131", "title": "Eye pattern", "text": "Oscilloscope display of a digital data signal\nIn telecommunications, an eye pattern, also known as an eye diagram, is an oscilloscope display in which a digital signal from a receiver is repetitively sampled and applied to the vertical input (\"y-axis\"), while the data rate is used to trigger the horizontal sweep (\"x-axis\"). It is so called because, for several types of coding, the pattern looks like a series of eyes between a pair of rails. It is a tool for the evaluation of the combined effects of channel noise, dispersion and intersymbol interference on the performance of a baseband pulse-transmission system. The technique was first used with the WWII SIGSALY secure speech transmission system.\nFrom a mathematical perspective, an eye pattern is a visualization of the probability density function (PDF) of the signal, modulo the unit interval (UI). In other words, it shows the probability of the signal being at each possible voltage across the duration of the UI. Typically a color ramp is applied to the PDF in order to make small brightness differences easier to visualize.\nSeveral system performance measurements can be derived by analyzing the display. If the signals are too long, too short, poorly synchronized with the system clock, too high, too low, too noisy, or too slow to change, or have too much undershoot or overshoot, this can be observed from the eye diagram. An open eye pattern corresponds to minimal signal distortion. Distortion of the signal waveform due to intersymbol interference and noise appears as closure of the eye pattern.\nCalculation.\nSource data.\nThe first step of computing an eye pattern is normally to obtain the waveform being analyzed in a quantized form. This may be done by measuring an actual electrical system with an oscilloscope of sufficient bandwidth, or by creating synthetic data with a circuit simulator in order to evaluate the signal integrity of a proposed design. A combination of the two approaches may be used as well: simulating the effects of an arbitrary circuit or transmission line on a measured signal, perhaps to determine whether a signal will still be intelligible after passing through a long cable. Interpolation may also be applied at this time in order to increase the number of samples per unit interval (UI) and produce a smooth, gap-free plot which is more visually appealing and easier to understand.\nSlicing.\nNext, the position of each sample within the UI must be determined. There are several methods for doing this depending on the characteristics of the signal and the capabilities of the oscilloscope and software in use. This step is critically important for accurate visualization of jitter in the eye.\nTriggering.\nA very simple method of slicing is to set the oscilloscope display to be slightly more than one UI wide, trigger on both rising and falling edges in the signal, and enable display persistence so that all measured waveforms \"stack\" into a single plot. This has the advantage of being possible on almost any oscilloscope (even fully analog ones) and can provide decent visualization of noise and overall signal shape, but completely destroys the jitter content of the signal since the instrument's trigger re-synchronizes the plot to each UI. The only jitter visible with this method is that of the oscilloscope itself, as well as extremely high frequency jitter (frequencies with period less than the UI).\nFixed rate.\nA simple way to have the eye pattern display jitter in the signal is to estimate the symbol rate of the signal (perhaps by counting the average number of zero crossings in a known window of time) and acquiring many UIs in a single oscilloscope capture. The first zero crossing in the capture is located and declared to be the start of the first UI, and the remainder of the waveform is divided into chunks one UI long.\nThis approach can work adequately for stable signals in which the symbol rate remains exactly the same over time, however inaccuracies in the system mean that some drift is inevitable so it is rarely used in practice. In some protocols, such as SATA, the symbol rate is intentionally varied by use of spread-spectrum clocking, so assuming a fixed rate will lead to the eye grossly exaggerating the actual jitter present on the signal. (While spread spectrum modulation on a clock is technically jitter in the strict sense, receivers for these systems are designed to track the modulation. The only jitter of interest to a signal integrity engineer is jitter much faster than the modulation rate, which the receiver cannot track effectively.)\nReference clock.\nWith some protocols, such as HDMI, a reference clock is supplied along with the signal, either at the symbol rate or at a lower (but synchronized) frequency from which a symbol clock can be reconstructed. Since the actual receiver in the system uses the reference clock to sample the data, using this clock to determine UI boundaries allows the eye pattern to faithfully display the signal as the receiver sees it: only jitter between the signal and the reference clock is displayed.\nClock recovery.\nMost high speed serial signals, such as PCIe, DisplayPort, and most variants of Ethernet, use a line code which is intended to allow easy clock recovery by means of a PLL. Since this is how the actual receiver works, the most accurate way to slice data for the eye pattern is to implement a PLL with the same characteristics in software. Correct PLL configuration allows for the eye to conceal the effects of spread spectrum clocking and other long-term variation in the symbol rate which do not contribute to errors at the receiver, while still displaying higher frequency jitter.\nIntegration.\nThe samples are then accumulated into a two-dimensional histogram, with the X axis representing time within the UI and the Y axis representing voltage. This is then normalized by dividing the value in each histogram bin by the value in the largest bin. Tone mapping, logarithmic scaling, or other mathematical transformations may be applied in order to emphasize different portions of the distribution, and a color gradient is applied to the final eye for display.\nLarge amounts of data may be needed to provide an accurate representation of the signal; tens to hundreds of millions of UIs are frequently used for a single eye pattern. In the example below, the eye using twelve thousand UIs only shows the basic shape of the eye, while the eye using eight million UIs shows far more nuance on the rising and falling edges.\nModulation.\nEach form of baseband modulation produces an eye pattern with a unique appearance.\nNRZ.\nThe eye pattern of a NRZ signal should consist of two clearly distinct levels with smooth transitions between them.\nMLT-3.\nThe eye pattern of a MLT-3 signal should consist of three clearly distinct levels (nominally -1, 0, +1 from bottom to top). The 0 level should be located at zero volts and the overall shape should be symmetric about the horizontal axis. The +1 and -1 states should have equal amplitude. There should be smooth transitions from the 0 state to the +1 and -1 states, however there should be no direct transitions from the -1 to +1 state (which would indicate the signal is PAM-3 rather than MLT-3).\nPAM.\nThe eye pattern of a PAM signal should consist of N clearly distinct levels (depending on the PAM order, for example PAM-4 should have four levels and PAM-3 should have three). The overall shape should be symmetric about the horizontal axis and the spacing of all levels should be uniform.\nChannel effects.\nMany properties of a channel can be seen in the eye pattern.\nEmphasis.\nEmphasis applied to a signal produces an additional level for each value of the signal which is higher (for pre-emphasis) or lower (for de-emphasis) than the nominal value.\nThe eye pattern for a signal with emphasis may be mistaken for that of a PAM signal at first glance, however closer inspection reveals some key differences. Most notably, an emphasized signal has a limited set of legal transitions:\nAn emphasized signal will never transition from a weak state to the corresponding strong state, a weak state to another weak state, or remain in the same strong state for more than one UI. A PAM signal also normally has equally spaced levels while emphasized levels are normally closer to the nominal signal level.\nHigh-frequency loss.\nLoss of printed circuit board traces and cables increases with frequency due to dielectric loss, which causes the channel to behave as a low-pass filter. The effect of this is an increase in signal rise/fall time. If the data rate is high enough or the channel is lossy enough, the signal may not even reach its full value during a fast 0\u20131\u20130 or 1\u20130\u20131 transition, and only stabilize after a run of several identical bits. This results in vertical closure of the eye.\nThe image below shows a 1.25\u00a0Gbit/s NRZ signal after passing through a lossy channel \u2013 an RG-188 coaxial cable approximately in length. This channel has loss increasing in a fairly linear fashion from 0.1 dB at DC to 9 dB at 6\u00a0GHz.\nThe top and bottom \"rails\" of the eye show the final voltage the signal reaches after several consecutive bits with the same value. Since the channel has minimal loss at DC, the maximum signal amplitude is largely unaffected. Looking at the rising edge of the signal (a 0\u20131 pattern) we can see that the signal starts to level off around \u2212300 ps, but continues to rise slowly over the duration of the UI. At around +300 ps, the signal either begins falling again (a 0\u20131\u20130 pattern) or continues rising slowly (a 0\u20131\u20131 pattern).\nAs high frequency losses increase the overall shape of the eye gradually degrades into a sinusoid (once higher frequency harmonics of the data has been eliminated, all that remains is the fundamental) and decreases in amplitude.\nImpedance mismatches.\nStubs, impedance mismatches, and other defects in a transmission line can cause reflections visible as defects in the edges of the signal. Reflections with a delay greater than one UI often render the eye completely unreadable due to inter-symbol interference (ISI), however those with a shorter delay can be easily seen in the shape of the eye.\nIn the image below, a roughly open circuited stub is present in the line, causing an initial low-impedance effect (reduced amplitude) followed by a positive reflection from the end of the stub with a delay of about 320 ps or 0.4 UIs. This can be clearly seen as a \"step\" in the rising edge in which the signal rises to a fraction of the full value, levels off for the round trip delay of the stub, then rises to its full value when the reflection arrives.\nIn the image below, an additional three inches of cable is added to the end of the same stub. The same \"step\" is present but is now four times as long, producing reflections at about 1280 ps or 1.6 UI. This produces extreme ISI (since the reflection of each UI arrives during the subsequent UI) which completely closes the eye.\nMeasurements.\nThere are many measurements that can be obtained from an eye diagram:\nAmplitude measurements\nTime measurements\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41132", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41132", "title": "Telecommunications facility", "text": "In telecommunications, a facility is defined by Federal Standard 1037C as:\nGlobal telecom facility.\nTelecommunications facility, is where a service can be offered. A location where incumbent local exchange carriers (ILECs) have their hardware to process telecom services. A phone call made to Jamaica can not be processed by a telecom facility in Canada or the US. If a phone number series belongs to say operator or carriers in Jamaica then only that operator knows if that phone number is valid and further if that phone number is on net to place a valid phone call. So all carriers in the world get the signal from the Telecom Facilities of the Digicel in Jamaica.\nIn Canada.\nUnder Canadian federal and Qu\u00e9b\u00e9cois provincial law, a telecommunications facility, for the purposes of determining whether GST applies, is defined by \u00a7123(1) of the GST Act to be \"any facility, apparatus, or other thing (including any wire, cable, radio, optical, or other electromagnetic system, or any similar technical system or any part thereof) that is used or is capable of being used for telecommunications\". This is a very broad definition that includes a wide range of things from satellites and earth stations, to telephones and fax machines.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41133", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41133", "title": "Facsimile converter", "text": "One of two devices in telecommunications\nIn telecommunications, the term facsimile converter has the following meanings: \nReferences.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41134", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41134", "title": "Fade margin", "text": "In telecommunications, the term fade margin (fading margin) has the following meanings:\nReferences.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;\nTrevor Manning, \"Microwave Radio Transmission Design Guide\", 2nd edition; Artech House: London, UK, 2009."}
{"id": "41135", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41135", "title": "Fading distribution", "text": "Probability distribution\nFading distribution is the probability distribution of the value of signal fading, relative to a specified reference level.\nIn the case of phase interference fading, the time distribution of the instantaneous field strength usually approximates a Rayleigh distribution when several signal components of equal amplitude are present. \nThe field strength is usually measured in volts per meter. \nThe fading distribution may also be measured in terms of power level, where the unit of measure is usually watts per square meter and the expression is in decibels."}
{"id": "41136", "revid": "635492", "url": "https://en.wikipedia.org/wiki?curid=41136", "title": "Fail-safe", "text": "Design feature or practice\nIn engineering, a fail-safe is a design feature or practice that, in the event of a failure of the design feature, inherently responds in a way that will cause minimal or no harm to other equipment, to the environment or to people. Unlike inherent safety to a particular hazard, a system being \"fail-safe\" does not mean that failure is naturally inconsequential, but rather that the system's design prevents or mitigates unsafe consequences of the system's failure. If and when a \"fail-safe\" system fails, it remains at least as safe as it was before the failure. Since many types of failure are possible, failure mode and effects analysis is used to examine failure situations and recommend safety design and procedures.\nSome systems can never be made fail-safe, as continuous availability is needed. Redundancy, fault tolerance, or contingency plans are used for these situations (e.g. multiple independently controlled and fuel-fed engines).\nExamples.\nMechanical or physical.\nExamples include:\nElectrical or electronic.\nExamples include:\nProcedural safety.\nAs well as physical devices and systems fail-safe procedures can be created so that if a procedure is not carried out or carried out incorrectly no dangerous action results. \nFor example:\nOther terminology.\nFail-safe (foolproof) devices are also known as \"poka-yoke\" devices. \"Poka-yoke\", a Japanese term, was coined by Shigeo Shingo, a quality expert. \"Safe to fail\" refers to civil engineering designs such as the Room for the River project in Netherlands and the Thames Estuary 2100 Plan which incorporate flexible adaptation strategies or climate change adaptation which provide for, and limit, damage, should severe events such as 500-year floods occur.\nFail safe and fail secure.\n\"Fail-safe\" and \"fail-secure\" are distinct concepts. \"Fail-safe\" means that a device will not endanger lives or property when it fails. \"Fail-secure,\" also called \"fail-closed,\" means that access or data will not fall into the wrong hands in a security failure. Sometimes the approaches suggest opposite solutions. For example, if a building catches fire, fail-safe systems would unlock doors to ensure quick escape and allow firefighters inside, while fail-secure would lock doors to prevent unauthorized access to the building.\nThe opposite of \"fail-closed\" is called \"fail-open\".\nFail active operational.\nFail active operational can be installed on systems that have a high degree of redundancy so that a single failure of any part of the system can be tolerated (fail active operational) and a second failure can be detected \u2013 at which point the system will turn itself off (uncouple, fail passive). One way of accomplishing this is to have three identical systems installed, and a control logic which detects discrepancies. An example for this are many aircraft systems, among them inertial navigation systems and pitot tubes.\nFailsafe point.\nDuring the Cold War, \"failsafe point\" was the term used for the point of no return for American Strategic Air Command nuclear bombers, just outside Soviet airspace. In the event of receiving an attack order, the bombers were required to linger at the failsafe point and wait for a second confirming order; until one was received, they would not arm their bombs or proceed further. The design was to prevent any single failure of the American command system causing nuclear war. This sense of the term entered the American popular lexicon with the publishing of the 1962 novel \"Fail-Safe\".\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41137", "revid": "13051", "url": "https://en.wikipedia.org/wiki?curid=41137", "title": "Fail-safe operation", "text": ""}
{"id": "41138", "revid": "28790082", "url": "https://en.wikipedia.org/wiki?curid=41138", "title": "Fall time", "text": ""}
{"id": "41140", "revid": "252195", "url": "https://en.wikipedia.org/wiki?curid=41140", "title": "Far-field diffraction pattern", "text": ""}
{"id": "41141", "revid": "82835", "url": "https://en.wikipedia.org/wiki?curid=41141", "title": "Far-field region", "text": ""}
{"id": "41142", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41142", "title": "Fast packet switching", "text": "In telecommunications, fast packet switching is a variant of packet switching that increases the throughput by eliminating overhead associated with flow control and error correction functions, which are either offloaded to upper layer networking protocols or removed altogether. ATM and Frame Relay are two major implementations of fast packet switching."}
{"id": "41143", "revid": "10808929", "url": "https://en.wikipedia.org/wiki?curid=41143", "title": "Fault", "text": "Fault commonly refers to:\nFault(s) may also refer to:\n&lt;templatestyles src=\"Template:TOC_right/styles.css\" /&gt;\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "41144", "revid": "20483999", "url": "https://en.wikipedia.org/wiki?curid=41144", "title": "Fault management", "text": "In network management, fault management is the set of functions that detect, isolate, and correct malfunctions in a telecommunications network, compensate for environmental changes, and include maintaining and examining error logs, accepting and acting on error detection notifications, tracing and identifying faults, carrying out sequences of diagnostics tests, correcting faults, reporting error conditions, and localizing and tracing faults by examining and manipulating database information.\nWhen a fault or event occurs, a network component will often send a notification to the network operator using a protocol such as SNMP. An alarm is a persistent indication of a fault that clears only when the triggering condition has been resolved. A current list of problems occurring on the network component is often kept in the form of an active alarm list such as is defined in RFC 3877, the Alarm MIB. A list of cleared faults is also maintained by most network management systems.\nFault management systems may use complex filtering systems to assign alarms to severity levels. These can range in severity from debug to emergency, as in the syslog protocol. Alternatively, they could use the ITU X.733 Alarm Reporting Function's perceived severity field. This takes on values of cleared, indeterminate, critical, major, minor or warning. Note that the latest version of the syslog protocol draft under development within the IETF includes a mapping between these two different sets of severities. It is considered good practice to send a notification not only when a problem has occurred, but also when it has been resolved. The latter notification would have a severity of clear.\nA fault management console allows a network administrator or system operator to monitor events from multiple systems and perform actions based on this information. Ideally, a fault management system should be able to correctly identify events and automatically take action, either launching a program or script to take corrective action, or activating notification software that allows a human to take proper intervention (i.e. send e-mail or SMS text to a mobile phone). Some notification systems also have escalation rules that will notify a chain of individuals based on availability and severity of alarm.\nTypes.\nThere are two primary ways to perform fault management - these are active and passive. Passive fault management is done by collecting alarms from devices (normally via SNMP traps) when something happens in the devices. In this mode, the fault management system only knows if a device it is monitoring is intelligent enough to generate an error and report it to the management tool. However, if the device being monitored fails completely or locks up, it won't throw an alarm and the problem will not be detected. Active fault management addresses this issue by actively monitoring devices via tools such as ping to determine if the device is active and responding. If the device stops responding, active monitoring will throw an alarm showing the device as unavailable and allows for the proactive correction of the problem.\nFault management includes any tools or procedure for testing, diagnosing or repairing the network when a failure occurs.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41145", "revid": "42840590", "url": "https://en.wikipedia.org/wiki?curid=41145", "title": "FCC (disambiguation)", "text": "The FCC, or Federal Communications Commission, is an independent agency of the United States government.\nFCC may also refer to:\n&lt;templatestyles src=\"Template:TOC_right/styles.css\" /&gt;\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "41146", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41146", "title": "FCC registration program", "text": "Program for terminal equipment\nIn telecommunications, FCC registration program is the Federal Communications Commission (FCC) program and associated directives intended to assure that all connected terminal equipment and protective circuitry will not harm the public switched telephone network or certain private line services. \n\"Note 1:\" The FCC registration program requires the registering of terminal equipment and protective circuitry in accordance with Subpart C of part 68, Title 47 of the \"Code of Federal Regulations.\" This includes the assignment of identification numbers to the equipment and the testing of the equipment. \n\"Note 2:\" The FCC registration program contains no requirement that accepted terminal equipment be compatible with, or function with, the network. \nReferences.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41147", "revid": "7098284", "url": "https://en.wikipedia.org/wiki?curid=41147", "title": "Feed", "text": "Feed or The Feed may refer to:\n&lt;templatestyles src=\"Template:TOC_right/styles.css\" /&gt;\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "41148", "revid": "32983869", "url": "https://en.wikipedia.org/wiki?curid=41148", "title": "Optical amplifier", "text": "Device that amplifies an optical signal\nAn optical amplifier is a device that amplifies an optical signal directly, without the need to first convert it to an electrical signal. An optical amplifier may be thought of as a laser without an optical cavity, or one in which feedback from the cavity is suppressed. Optical amplifiers are important in optical communication and laser physics. They are used as optical repeaters in the long distance fiber-optic cables which carry much of the world's telecommunication links.\nThere are several different physical mechanisms that can be used to amplify a light signal, which correspond to the major types of optical amplifiers. In doped fiber amplifiers and bulk lasers, stimulated emission in the amplifier's gain medium causes amplification of incoming light. In semiconductor optical amplifiers (SOAs), electron\u2013hole recombination occurs. In Raman amplifiers, Raman scattering of incoming light with phonons in the lattice of the gain medium produces photons coherent with the incoming photons. Parametric amplifiers use parametric amplification.\nHistory.\nThe principle of optical amplification was invented by Gordon Gould on November 13, 1957. He filed US Patent US80453959A on April 6, 1959, titled \"Light Amplifiers Employing Collisions to Produce Population Inversions\" (subsequently amended as a continuation in part and finally issued as https:// on May 4, 1988). The patent covered \u201cthe amplification of light by the stimulated emission of photons from ions, atoms or molecules in gaseous, liquid or solid state.\u201d In total, Gould obtained 48 patents related to the optical amplifier that covered 80% of the lasers on the market at the time of issuance.\nGould co-founded an optical telecommunications equipment firm, Optelecom Inc., that helped start Ciena Corp with his former head of Light Optics Research, David Huber and Kevin Kimberlin. Huber and Steve Alexander of Ciena invented the dual-stage optical amplifier (https://) that was a key to the first dense wave division multiplexing (DWDM) system, that they released in June 1996. This marked the start of optical networking. Its significance was recognized at the time by optical authority, Shoichi Sudo and technology analyst, George Gilder in 1997, when Sudo wrote that optical amplifiers \u201cwill usher in a worldwide revolution called the Information Age\u201d and Gilder compared the optical amplifier to the integrated circuit in importance, predicting that it would make possible the Age of Information. Optical amplification WDM systems are the common basis of all local, metro, national, intercontinental and subsea telecommunications networks and the technology of choice for the fiber optic backbones of the Internet (e.g. fiber-optic cables form a basis of modern-day computer networking).\nLaser amplifiers.\nAlmost any laser active gain medium can be pumped to produce gain for light at the wavelength of a laser made with the same material as its gain medium. Such amplifiers are commonly used to produce high power laser systems. Special types such as regenerative amplifiers and chirped-pulse amplifiers are used to amplify ultrashort pulses.\nSolid-state amplifiers.\nSolid-state amplifiers are optical amplifiers that use a wide range of doped solid-state materials ( Yb:YAG, Ti:Sa) and different geometries (disk, slab, rod) to amplify optical signals. The variety of materials allows the amplification of different wavelengths, while the shape of the medium can distinguish between those more suitable for energy or average power scaling. Beside their use in fundamental research from gravitational wave detection to high energy physics at the National Ignition Facility they can also be found in many of today's ultra short pulsed lasers.\nDoped-fiber amplifiers.\nDoped-fiber amplifiers (DFAs) are optical amplifiers that use a doped optical fiber as a gain medium to amplify an optical signal. They are related to fiber lasers. The signal to be amplified and a pump laser are multiplexed into the doped fiber, and the signal is amplified through interaction with the doping ions.\nAmplification is achieved by stimulated emission of photons from dopant ions in the doped fiber. The pump laser excites ions into a higher energy from where they can decay via stimulated emission of a photon at the signal wavelength back to a lower energy level. The excited ions can also decay spontaneously (spontaneous emission) or even through nonradiative processes involving interactions with phonons of the glass matrix. These last two decay mechanisms compete with stimulated emission reducing the efficiency of light amplification.\nThe \"amplification window\" of an optical amplifier is the range of optical wavelengths for which the amplifier yields a usable gain. The amplification window is determined by the spectroscopic properties of the dopant ions, the glass structure of the optical fiber, and the wavelength and power of the pump laser.\nAlthough the electronic transitions of an isolated ion are very well defined, broadening of the energy levels occurs when the ions are incorporated into the glass of the optical fiber and thus the amplification window is also broadened. This broadening is both homogeneous (all ions exhibit the same broadened spectrum) and inhomogeneous (different ions in different glass locations exhibit different spectra). Homogeneous broadening arises from the interactions with phonons of the glass, while inhomogeneous broadening is caused by differences in the glass sites where different ions are hosted. Different sites expose ions to different local electric fields, which shifts the energy levels via the Stark effect. In addition, the Stark effect also removes the degeneracy of energy states having the same total angular momentum (specified by the quantum number J). Thus, for example, the trivalent erbium ion (Er3+) has a ground state with J = 15/2, and in the presence of an electric field splits into J + 1/2 = 8 sublevels with slightly different energies. The first excited state has J = 13/2 and therefore a Stark manifold with 7 sublevels. Transitions from the J = 13/2 excited state to the J= 15/2 ground state are responsible for the gain at 1500\u00a0nm wavelength. The gain spectrum of the EDFA has several peaks that are smeared by the above broadening mechanisms. The net result is a very broad spectrum (30\u00a0nm in silica, typically). The broad gain-bandwidth of fiber amplifiers make them particularly useful in wavelength-division multiplexed communications systems as a single amplifier can be utilized to amplify all signals being carried on a fiber and whose wavelengths fall within the gain window.\nAn \"erbium-doped waveguide amplifier\" (EDWA) is an optical amplifier that uses a waveguide to boost an optical signal.\nBasic principle of EDFA.\nA relatively high-powered beam of light is mixed with the input signal using a wavelength selective coupler (WSC). The input signal and the excitation light must be at significantly different wavelengths. The mixed light is guided into a section of fiber with erbium ions included in the core. This high-powered light beam excites the erbium ions to their higher-energy state. When the photons belonging to the signal at a different wavelength from the pump light meet the excited erbium ions, the erbium ions undergo stimulated emission and return to their lower-energy state.\nA significant point is that the erbium gives up its energy in the form of additional photons which are exactly in the same phase and direction as the signal being amplified. So the signal is amplified along its direction of travel only. This is not unusual \u2013 when an atom \"lases\" it always gives up its energy in the same direction and phase as the incoming light. Thus all of the additional signal power is guided in the same fiber mode as the incoming signal. An optical isolator is usually placed at the output to prevent reflections returning from the attached fiber. Such reflections disrupt amplifier operation and in the extreme case can cause the amplifier to become a laser.\nThe erbium doped amplifier is a high gain amplifier.\nNoise.\nThe principal source of noise in DFAs is amplified spontaneous emission (ASE), which has a spectrum approximately the same as the gain spectrum of the amplifier. Noise figure in an ideal DFA is 3\u00a0dB, while practical amplifiers can have noise figure as large as 6\u20138\u00a0dB.\nAs well as decaying via stimulated emission, electrons in the upper energy level can also decay by spontaneous emission, which occurs at random, depending upon the glass structure and inversion level. Photons are emitted spontaneously in all directions, but a proportion of those will be emitted in a direction that falls within the numerical aperture of the fiber and are thus captured and guided by the fiber. Those photons captured may then interact with other dopant ions, and are thus amplified by stimulated emission. The initial spontaneous emission is therefore amplified in the same manner as the signals, hence the term a\"mplified spontaneous emission\". ASE is emitted by the amplifier in both the forward and reverse directions, but only the forward ASE is a direct concern to system performance since that noise will co-propagate with the signal to the receiver where it degrades system performance. Counter-propagating ASE can, however, lead to degradation of the amplifier's performance since the ASE can deplete the inversion level and thereby reduce the gain of the amplifier and increase the noise produced relative to the desired signal gain.\nNoise figure can be analyzed in both the optical domain and in the electrical domain. In the optical domain, measurement of the ASE, the optical signal gain, and signal wavelength using an optical spectrum analyzer permits calculation of the noise figure. For the electrical measurement method, the detected photocurrent noise is evaluated with a low-noise electrical spectrum analyzer, which along with measurement of the amplifier gain permits a noise figure measurement. Generally, the optical technique provides a more simple method, though it is not inclusive of excess noise effects captured by the electrical method such multi-path interference (MPI) noise generation. In both methods, attention to effects such as the spontaneous emission accompanying the input signal are critical to accurate measurement of noise figure.\nGain saturation.\nGain is achieved in a DFA due to population inversion of the dopant ions. The inversion level of a DFA is set, primarily, by the power of the pump wavelength and the power at the amplified wavelengths. As the signal power increases, or the pump power decreases, the inversion level will reduce and thereby the gain of the amplifier will be reduced. This effect is known as gain saturation \u2013 as the signal level increases, the amplifier saturates and cannot produce any more output power, and therefore the gain reduces. Saturation is also commonly known as gain compression.\nTo achieve optimum noise performance DFAs are operated under a significant amount of gain compression (10\u00a0dB typically), since that reduces the rate of spontaneous emission, thereby reducing ASE. Another advantage of operating the DFA in the gain saturation region is that small fluctuations in the input signal power are reduced in the output amplified signal: smaller input signal powers experience larger (less saturated) gain, while larger input powers see less gain.\nThe leading edge of the pulse is amplified, until the saturation energy of the gain medium is reached. In some condition, the width (FWHM) of the pulse is reduced.\nInhomogeneous broadening effects.\nDue to the inhomogeneous portion of the linewidth broadening of the dopant ions, the gain spectrum has an inhomogeneous component and gain saturation occurs, to a small extent, in an inhomogeneous manner. This effect is known as \"spectral hole burning\" because a high power signal at one wavelength can 'burn' a hole in the gain for wavelengths close to that signal by saturation of the inhomogeneously broadened ions. Spectral holes vary in width depending on the characteristics of the optical fiber in question and the power of the burning signal, but are typically less than 1\u00a0nm at the short wavelength end of the C-band, and a few nm at the long wavelength end of the C-band. The depth of the holes are very small, though, making it difficult to observe in practice.\nPolarization effects.\nAlthough the DFA is essentially a polarization independent amplifier, a small proportion of the dopant ions interact preferentially with certain polarizations and a small dependence on the polarization of the input signal may occur (typically &lt; 0.5\u00a0dB). This is called polarization dependent gain (PDG).\nThe absorption and emission cross sections of the ions can be modeled as ellipsoids with the major axes aligned at random in all directions in different glass sites. The random distribution of the orientation of the ellipsoids in a glass produces a macroscopically isotropic medium, but a strong pump laser induces an anisotropic distribution by selectively exciting those ions that are more aligned with the optical field vector of the pump. Also, those excited ions aligned with the signal field produce more stimulated emission. The change in gain is thus dependent on the alignment of the polarizations of the pump and signal lasers \u2013 i.e. whether the two lasers are interacting with the same sub-set of dopant ions or not. \nIn an ideal doped fiber without birefringence, the PDG would be inconveniently large. Fortunately, in optical fibers small amounts of birefringence are always present and, furthermore, the fast and slow axes vary randomly along the fiber length. A typical DFA has several tens of meters, long enough to already show this randomness of the birefringence axes. These two combined effects (which in transmission fibers give rise to polarization mode dispersion) produce a misalignment of the relative polarizations of the signal and pump lasers along the fiber, thus tending to average out the PDG. The result is that PDG is very difficult to observe in a single amplifier (but is noticeable in links with several cascaded amplifiers).\nErbium-doped optical fiber amplifiers.\nThe erbium-doped fiber amplifier (EDFA) is the most deployed fiber amplifier as its amplification window coincides with the third transmission window of silica-based optical fiber. The core of a silica fiber is doped with trivalent erbium ions (Er3+) and can be efficiently pumped with a laser at or near wavelengths of 980\u00a0nm and 1480\u00a0nm, and gain is exhibited in the 1550\u00a0nm region. The EDFA amplification region varies from application to application and can be anywhere from a few nm up to ~80\u00a0nm. Typical use of EDFA in telecommunications calls for \"Conventional\", or C-band amplifiers (from ~1525\u00a0nm to ~1565\u00a0nm) or \"Long\", or L-band amplifiers (from ~1565\u00a0nm to ~1610\u00a0nm). Both of these bands can be amplified by EDFAs, but it is normal to use two different amplifiers, each optimized for one of the bands.\nThe principal difference between C- and L-band amplifiers is that a longer length of doped fiber is used in L-band amplifiers. The longer length of fiber allows a lower inversion level to be used, thereby giving emission at longer wavelengths (due to the band-structure of Erbium in silica) while still providing a useful amount of gain.\nEDFAs have two commonly used pumping bands \u2013 980\u00a0nm and 1480\u00a0nm. The 980\u00a0nm band has a higher absorption cross-section and is generally used where low-noise performance is required. The absorption band is relatively narrow and so wavelength stabilised laser sources are typically needed. The 1480\u00a0nm band has a lower, but broader, absorption cross-section and is generally used for higher power amplifiers. A combination of 980\u00a0nm and 1480\u00a0nm pumping is generally utilised in amplifiers.\nGain and lasing in erbium-doped fibers were first demonstrated in 1986\u201387 by two groups; one including David N. Payne, R. Mears, I.M Jauncey and L. Reekie, from the University of Southampton and one from AT&amp;T Bell Laboratories, consisting of E. Desurvire, P. Becker, and J. Simpson. The dual-stage optical amplifier which enabled dense wave division multiplexing (DWDM) was invented by Stephen B. Alexander at Ciena Corporation.\nDoped fiber amplifiers for other wavelength ranges.\nThulium doped fiber amplifiers have been used in the S-band (1450\u20131490\u00a0nm) and Praseodymium doped amplifiers in the 1300\u00a0nm region. However, those regions have not seen any significant commercial use so far and so those amplifiers have not been the subject of as much development as the EDFA. However, Ytterbium doped fiber lasers and amplifiers, operating near 1 micrometre wavelength, have many applications in industrial processing of materials, as these devices can be made with extremely high output power (tens of kilowatts).\nSemiconductor optical amplifier.\nSemiconductor optical amplifiers (SOAs) are amplifiers which use a semiconductor to provide the gain medium. These amplifiers have a similar structure to Fabry\u2013P\u00e9rot laser diodes but with anti-reflection design elements at the end faces. Recent designs include anti-reflective coatings and tilted wave guide and window regions which can reduce end face reflection to less than 0.001%. Since this creates a loss of power from the cavity which is greater than the gain, it prevents the amplifier from acting as a laser. Another type of SOA consists of two regions. One part has a structure of a Fabry-P\u00e9rot laser diode and the other has a tapered geometry in order to reduce the power density on the output facet.\nSemiconductor optical amplifiers are typically made from group III-V compound semiconductors such as GaAs/AlGaAs, InP/InGaAs, InP/InGaAsP and InP/InAlGaAs, though any direct band gap semiconductors such as II-VI could conceivably be used. Such amplifiers are often used in telecommunication systems in the form of fiber-pigtailed components, operating at signal wavelengths between 850\u00a0nm and 1600\u00a0nm and generating gains of up to 30\u00a0dB.\nThe semiconductor optical amplifier is of small size and electrically pumped. It can be potentially less expensive than the EDFA and can be integrated with semiconductor lasers, modulators, etc. However, the performance is still not comparable with the EDFA. The SOA has higher noise, lower gain, moderate polarization dependence and high nonlinearity with fast transient time. The main advantage of SOA is that all four types of nonlinear operations (cross gain modulation, cross phase modulation, wavelength conversion and four wave mixing) can be conducted. Furthermore, SOA can be run with a low power laser.\nThis originates from the short nanosecond or less upper state lifetime, so that the gain reacts rapidly to changes of pump or signal power and the changes of gain also cause phase changes which can distort the signals.\nThis nonlinearity presents the most severe problem for optical communication applications. However it provides the possibility for gain in different wavelength regions from the EDFA. \"Linear optical amplifiers\" using gain-clamping techniques have been developed.\nHigh optical nonlinearity makes semiconductor amplifiers attractive for all optical signal processing like all-optical switching and wavelength conversion. There has been much research on semiconductor optical amplifiers as elements for optical signal processing, wavelength conversion, clock recovery, signal demultiplexing, and pattern recognition.\nVertical-cavity SOA.\nA recent addition to the SOA family is the vertical-cavity SOA (VCSOA). These devices are similar in structure to, and share many features with, vertical-cavity surface-emitting lasers (VCSELs). The major difference when comparing VCSOAs and VCSELs is the reduced mirror reflectivity used in the amplifier cavity. With VCSOAs, reduced feedback is necessary to prevent the device from reaching lasing threshold. Due to the extremely short cavity length, and correspondingly thin gain medium, these devices exhibit very low single-pass gain (typically on the order of a few percent) and also a very large free spectral range (FSR). The small single-pass gain requires relatively high mirror reflectivity to boost the total signal gain. In addition to boosting the total signal gain, the use of the resonant cavity structure results in a very narrow gain bandwidth; coupled with the large FSR of the optical cavity, this effectively limits operation of the VCSOA to single-channel amplification. Thus, VCSOAs can be seen as amplifying filters.\nGiven their vertical-cavity geometry, VCSOAs are resonant cavity optical amplifiers that operate with the input/output signal entering/exiting normal to the wafer surface. In addition to their small size, the surface normal operation of VCSOAs leads to a number of advantages, including low power consumption, low noise figure, polarization insensitive gain, and the ability to fabricate high fill factor two-dimensional arrays on a single semiconductor chip. These devices are still in the early stages of research, though promising preamplifier results have been demonstrated. Further extensions to VCSOA technology are the demonstration of wavelength tunable devices. These MEMS-tunable vertical-cavity SOAs utilize a microelectromechanical systems (MEMS) based tuning mechanism for wide and continuous tuning of the peak gain wavelength of the amplifier. SOAs have a more rapid gain response, which is in the order of 1 to 100 ps.\nTapered amplifiers.\nFor high output power and broader wavelength range, tapered amplifiers are used. These amplifiers consist of a lateral single-mode section and a section with a tapered structure, where the laser light is amplified. The tapered structure leads to a reduction of the power density at the output facet.\nTypical parameters:\nRaman amplifier.\nIn a Raman amplifier, the signal is intensified by Raman amplification. Unlike the EDFA and SOA the amplification effect is achieved by a nonlinear interaction between the signal and a pump laser within an optical fiber. There are two types of Raman amplifier: distributed and lumped. A distributed Raman amplifier is one in which the transmission fiber is utilised as the gain medium by multiplexing a pump wavelength with signal wavelength, while a lumped Raman amplifier utilises a dedicated, shorter length of fiber to provide amplification. In the case of a lumped Raman amplifier, a highly nonlinear fiber with a small core is utilised to increase the interaction between signal and pump wavelengths, and thereby reduce the length of fiber required.\nThe pump light may be coupled into the transmission fiber in the same direction as the signal (co-directional pumping), in the opposite direction (contra-directional pumping) or both. Contra-directional pumping is more common as the transfer of noise from the pump to the signal is reduced.\nThe pump power required for Raman amplification is higher than that required by the EDFA, with in excess of 500\u00a0mW being required to achieve useful levels of gain in a distributed amplifier. Lumped amplifiers, where the pump light can be safely contained to avoid safety implications of high optical powers, may use over 1 W of optical power.\nThe principal advantage of Raman amplification is its ability to provide distributed amplification within the transmission fiber, thereby increasing the length of spans between amplifier and regeneration sites. The amplification bandwidth of Raman amplifiers is defined by the pump wavelengths utilised and so amplification can be provided over wider, and different, regions than may be possible with other amplifier types which rely on dopants and device design to define the amplification 'window'.\nRaman amplifiers have some fundamental advantages. First, Raman gain exists in every fiber, which provides a cost-effective means of upgrading from the terminal ends. Second, the gain is nonresonant, which means that gain is available over the entire transparency region of the fiber ranging from approximately 0.3 to 2\u00a0\u03bcm. A third advantage of Raman amplifiers is that the gain spectrum can be tailored by adjusting the pump wavelengths. For instance, multiple pump lines can be used to increase the optical bandwidth, and the pump distribution determines the gain flatness. Another advantage of Raman amplification is that it is a relatively broad-band amplifier with a bandwidth &gt; 5 THz, and the gain is reasonably flat over a wide wavelength range.\nHowever, a number of challenges for Raman amplifiers prevented their earlier adoption. First, compared to the EDFAs, Raman amplifiers have relatively poor pumping efficiency at lower signal powers. Although a disadvantage, this lack of pump efficiency also makes gain clamping easier in Raman amplifiers. Second, Raman amplifiers require a longer gain fiber. However, this disadvantage can be mitigated by combining gain and the dispersion compensation in a single fiber. A third disadvantage of Raman amplifiers is a fast response time, which gives rise to new sources of noise, as further discussed below. Finally, there are concerns of nonlinear penalty in the amplifier for the WDM signal channels.\n\"Note: The text of an earlier version of this article was taken from the public domain Federal Standard 1037C.\"\nOptical parametric amplifier.\nAn optical parametric amplifier allows the amplification of a weak signal-impulse in a nonlinear medium such as a noncentrosymmetric nonlinear medium (e.g. Beta barium borate (BBO)) or even a standard fused silica optical fiber via the Kerr effect. In contrast to the previously mentioned amplifiers, which are mostly used in telecommunication environments, this type finds its main application in expanding the frequency tunability of ultrafast solid-state lasers (e.g. Ti:sapphire). By using a noncollinear interaction geometry optical parametric amplifiers are capable of extremely broad amplification bandwidths.\n21st century.\nIn the 21st century high power fiber lasers were adopted as an industrial material processing tool, and were expanding into other markets including the medical and scientific markets. One key enhancement enabling penetration into the scientific market was improvement in high finesse fiber amplifiers, which became able to deliver single frequency linewidths (&lt;5\u00a0kHz) together with excellent beam quality and stable linearly polarized output. Systems meeting these specifications steadily progressed from a few watts of output power initially, to tens of watts and later hundreds of watts. This power increase was achieved with developments in fiber technology, such as the adoption of stimulated brillouin scattering (SBS) suppression/mitigation techniques within the fiber, and improvements in overall amplifier design, including large mode area (LMA) fibers with a low-aperture core, micro-structured rod-type fiber helical core, or chirally coupled core fibers, and tapered double-clad fibers (T-DCF). As of 2015[ [update]] high finesse, high power and pulsed fiber amplifiers delivered power levels exceeding those available from commercial solid-state single-frequency sources, and stable optimized performance, opening up new scientific applications.\nImplementations.\nThere are several simulation tools that can be used to design optical amplifiers. Popular commercial tools have been developed by Optiwave Systems and VPI Systems.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41149", "revid": "10202399", "url": "https://en.wikipedia.org/wiki?curid=41149", "title": "Fiber Distributed Data Interface", "text": "Standard for data transmission in a local area network\nFiber Distributed Data Interface (FDDI) is a standard for data transmission in a local area network.\nIt uses optical fiber as its standard underlying physical medium.\nIt was also later specified to use copper cable, in which case it may be called CDDI (Copper Distributed Data Interface), standardized as TP-PMD (Twisted-Pair Physical Medium-Dependent), also referred to as TP-DDI (Twisted-Pair Distributed Data Interface).\nFDDI was effectively made obsolete in local networks by Fast Ethernet which offered the same 100\u00a0Mbit/s speeds, but at a much lower cost and, from 1998 on, by Gigabit Ethernet due to its speed, even lower cost, and ubiquity.\nDescription.\nFDDI provides a 100\u00a0Mbit/s optical standard for data transmission in local area network that can extend in length up to . Although FDDI logical topology is a ring-based token network, it did not use the IEEE 802.5 Token Ring protocol as its basis; instead, its protocol was derived from the IEEE 802.4 token bus \"timed token\" protocol. In addition to covering large geographical areas, FDDI local area networks can support thousands of users. FDDI offers both a Dual-Attached Station (DAS), counter-rotating token ring topology and a Single-Attached Station (SAS), token bus passing ring topology.\nFDDI, as a product of American National Standards Institute X3T9.5 (now X3T12), conforms to the Open Systems Interconnection (OSI) model of functional layering using other protocols. The standards process started in the mid 1980s.\nFDDI-II, a version of FDDI described in 1989, added circuit-switched service capability to the network so that it could also handle voice and video signals. Work started to connect FDDI networks to synchronous optical networking (SONET) technology.\nAn FDDI network contains two rings, one as a secondary backup in case the primary ring fails. The primary ring offers up to 100\u00a0Mbit/s capacity. When a network has no requirement for the secondary ring to do backup, it can also carry data, extending capacity to 200\u00a0Mbit/s. The single ring can extend the maximum distance; a dual ring can extend . FDDI had a larger maximum frame size (4,352 bytes) than the standard Ethernet family, which only supports a maximum frame size of 1,500 bytes, allowing better effective data rates in some cases.\nTopology.\nDesigners normally constructed FDDI rings in a network topology such as a \"dual ring of trees\". A small number of devices, typically infrastructure devices such as routers and concentrators rather than host computers, were \"dual-attached\" to both rings. Host computers then connect as single-attached devices to the routers or concentrators. The dual ring in its most degenerate form simply collapses into a single device. Typically, a computer-room contained the whole dual ring, although some implementations deployed FDDI as a metropolitan area network.\nFDDI requires this network topology because the dual ring actually passes through each connected device and requires each such device to remain continuously operational.\nThe standard actually allows for optical bypasses, but network engineers consider these unreliable and error-prone. Devices such as workstations and minicomputers that might not come under the control of the network managers are not suitable for connection to the dual ring.\nAs an alternative to using a dual-attached connection, a workstation can obtain the same degree of resilience through a dual-homed connection made simultaneously to two separate devices in the same FDDI ring. One of the connections becomes active while the other one is automatically blocked. If the first connection fails, the backup link takes over with no perceptible delay.\nFrame format.\nThe frame check sequence uses the same cyclic redundancy check as Token Ring and Ethernet.\nThe Internet Engineering Task Force defined a standard for transmission of the Internet Protocol (which would be the protocol data unit in this case) over FDDI. It was first proposed in June 1989 and revised in 1990. Some aspects of the protocol were compatible with the IEEE 802.2 standard for logical link control. For example, the 48-bit MAC addresses that became popular with the Ethernet family. Thus other protocols such as the Address Resolution Protocol (ARP) could be common as well.\nDeployment.\nFDDI was considered an attractive campus backbone network technology in the early to mid 1990s since existing Ethernet networks only offered 10\u00a0Mbit/s data rates and Token Ring networks only offered 4\u00a0Mbit/s or 16\u00a0Mbit/s rates. Thus it was a relatively high-speed choice of that era, with speeds such as 100\u00a0Mbit/s.\nBy 1994, vendors included Cisco Systems, National Semiconductor, Network Peripherals, SysKonnect (acquired by Marvell Technology Group), and 3Com.\nFDDI installations have largely been replaced by Ethernet deployments.\nStandards.\nFDDI standards included:\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41150", "revid": "8541182", "url": "https://en.wikipedia.org/wiki?curid=41150", "title": "Field strength", "text": "Value of a vector-valued field\nIn physics, field strength refers to a value in a vector-valued field (e.g., in volts per meter, V/m, for an electric field E). \nFor example, an electromagnetic field has both electric field strength and magnetic field strength.\nField strength is a common term referring to a \"vector\" quantity. However, the word 'strength' may lead to confusion as it might be referring only to the magnitude of that vector. For both gravitational field strength and for electric field strength, The Institute of Physics glossary states \"this glossary avoids that term because it might be confused with the magnitude of the [gravitational or electric] field\".\nAs an application, in radio frequency telecommunications, the signal strength excites a receiving antenna and thereby induces a voltage at a specific frequency and polarization in order to provide an input signal to a radio receiver. Field strength meters are used for such applications as cellular, broadcasting, wi-fi and a wide variety of other radio-related applications.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41151", "revid": "42407475", "url": "https://en.wikipedia.org/wiki?curid=41151", "title": "File server", "text": "Computer that provides file systems in a computer network\nIn computing, a file server (or fileserver) is a computer attached to a network that provides a location for shared disk access, i.e. storage of computer files (such as text, image, sound, video) that can be accessed by workstations within a computer network. The term server highlights the role of the machine in the traditional client\u2013server scheme, where the clients are the workstations using the storage. A file server does not normally perform computational tasks or run programs on behalf of its client workstations (in other words, it is different from e.g. an application server, which is another type of server).\nFile servers are commonly found in schools and offices, where users use a local area network to connect their client computers.\nTypes of file servers.\nA file server may be dedicated or non-dedicated. A dedicated server is designed specifically for use as a file server, with workstations attached for reading and writing files and databases.\nFile servers may also be categorized by the method of access: Internet file servers are frequently accessed by File Transfer Protocol (FTP) or by Hypertext Transfer Protocol (HTTP) but are different from web servers that often provide dynamic web content in addition to static files. Servers on a LAN are usually accessed by SMB/CIFS protocol (Windows and Unix-like) or NFS protocol (Unix-like systems).\nDatabase servers, that provide access to a shared database via a database device driver, are not regarded as file servers even when the database is stored in files, as they are not designed to provide those files to users and tend to have differing technical requirements.\nDesign of file servers.\nIn modern businesses, the design of file servers is complicated by competing demands for storage space, access speed, recoverability, ease of administration, security, and budget. This is further complicated by a constantly changing environment, where new hardware and technology rapidly obsolesces old equipment, and yet must seamlessly come online in a fashion compatible with the older machinery. To manage throughput, peak loads, and response time, vendors may utilize queuing theory to model how the combination of hardware and software will respond over various levels of demand. Servers may also employ dynamic load balancing scheme to distribute requests across various pieces of hardware.\nThe primary piece of hardware equipment for servers over the last couple of decades has proven to be the hard disk drive. Although other forms of storage are viable (such as magnetic tape and solid-state drives) disk drives have continued to offer the best fit for cost, performance, and capacity.\nStorage.\nSince the crucial function of a file server is storage, technology has been developed to operate multiple disk drives together as a team, forming a disk array. A disk array typically has cache (temporary memory storage that is faster than the magnetic disks), as well as advanced functions like RAID and storage virtualization. Typically disk arrays increase level of availability by using redundant components other than RAID, such as power supplies. Disk arrays may be consolidated or virtualized in a SAN.\nNetwork-attached storage.\nNetwork-attached storage (NAS) is file-level computer data storage connected to a computer network providing data access to a heterogeneous group of clients. NAS devices specifically are distinguished from file servers generally in a NAS being a computer appliance \u2013 a specialized computer built from the ground up for serving files \u2013 rather than a general purpose computer being used for serving files (possibly with other functions). In discussions of NASs, the term \"file server\" generally stands for a contrasting term, referring to general purpose computers only.\n NAS devices are gaining popularity, offering a convenient method for sharing files between multiple computers. Potential benefits of network-attached storage, compared to non-dedicated file servers, include faster data access, easier administration, and simple configuration.\nNAS systems are networked appliances containing one or more hard drives, often arranged into logical, redundant storage containers or RAID arrays. Network Attached Storage removes the responsibility of file serving from other servers on the network. They typically provide access to files using network file sharing protocols such as NFS, SMB/CIFS (Server Message Block/Common Internet File System), or AFP.\nSecurity.\nFile servers generally offer some form of system security to limit access to files to specific users or groups. In large organizations, this is a task usually delegated to directory services, such as openLDAP, Novell's eDirectory or Microsoft's Active Directory.\nThese servers work within the hierarchical computing environment which treat users, computers, applications and files as distinct but related entities on the network and grant access based on user or group credentials. In many cases, the directory service spans many file servers, potentially hundreds for large organizations. In the past, and in smaller organizations, authentication could take place directly at the server itself.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41152", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41152", "title": "Filled cable", "text": "In telecommunications, a filled cable is a cable that has a non-hygroscopic material, usually a gel called icky-pick, inside the jacket or sheath. The nonhygroscopic material fills the spaces between the interior parts of the cable, preventing moisture from entering minor leaks in the sheath and migrating inside the cable. \nA metallic cable filled with a dielectric material, such as a coaxial cable or a metal waveguide, is not considered to be a \"filled cable\".\nReferences.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;\nFurther reading.\nSee Telcordia http://, \"Generic Requirements for Metallic Telecommunications Cables,\" for filled, polyolefin-insulated conductor (PIC) cable requirements."}
{"id": "41153", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=41153", "title": "Filtered symmetric differential phase-shift keying", "text": ""}
{"id": "41154", "revid": "25859633", "url": "https://en.wikipedia.org/wiki?curid=41154", "title": "TCP/IP", "text": ""}
{"id": "41155", "revid": "46282599", "url": "https://en.wikipedia.org/wiki?curid=41155", "title": "Firmware", "text": "Low-level computer software\nIn computing, firmware is software that provides low-level control of computing device hardware. \nFor a relatively simple device, firmware may perform all control, monitoring and data manipulation functionality.\nFor a more complex device, firmware may provide relatively low-level control as well as hardware abstraction services to higher-level software such as an operating system.\nFirmware is found in a wide range of computing devices including personal computers, smartphones, home appliances, vehicles, computer peripherals and in many of the integrated circuits inside each of these larger systems.\nFirmware is stored in non-volatile memory \u2013 either read-only memory (ROM) or programmable memory such as EPROM, EEPROM, or flash. Changing a device's firmware stored in ROM requires physically replacing the memory chip \u2013 although some chips are not designed to be removed after manufacture. Programmable firmware memory can be reprogrammed via a procedure sometimes called \"flashing\".\nCommon reasons for changing firmware include fixing bugs and adding features.\nHistory and etymology.\nAscher Opler used the term \"firmware\" in a 1967 \"Datamation\" article, as an intermediary term between \"hardware\" and \"software\". Opler projected that fourth-generation computer systems would have a writable control store (a small specialized high-speed memory) into which microcode firmware would be loaded. Many software functions would be moved to microcode, and instruction sets could be customized, with different firmware loaded for different instruction sets.\nAs computers began to increase in complexity, it became clear that various programs needed to first be initiated and run to provide a consistent environment necessary for running more complex programs at the user's discretion. This required programming the computer to run those programs automatically. Furthermore, as companies, universities, and marketers wanted to sell computers to laypeople with little technical knowledge, greater automation became necessary to allow a lay-user to easily run programs for practical purposes. This gave rise to a kind of software that a user would not consciously run, and it led to software that a lay user would not even know about.\nAs originally used, firmware contrasted with hardware (the CPU itself) and software (normal instructions executing on a CPU). It was not composed of CPU machine instructions, but of lower-level microcode involved in the implementation of machine instructions. It existed on the boundary between hardware and software; thus the name \"firmware\". Over time, popular usage extended the word \"firmware\" to denote any computer program that is tightly linked to hardware, including BIOS on PCs, boot firmware on smartphones, computer peripherals, or the control systems on simple consumer electronic devices, such as microwave ovens and remote controls.\nApplications.\nComputers.\nIn some respects, the various firmware components are as important as the operating system in a working computer. However, unlike most modern operating systems, firmware rarely has a well-evolved automatic mechanism of updating itself to fix any functionality issues detected after shipping the unit.\nA computer's firmware may be manually updated by a user via a small utility program. In contrast, firmware in mass storage devices (hard-disk drives, optical disc drives, flash memory storage e.g. solid state drive) is less frequently updated, even when flash memory (rather than ROM, EEPROM) storage is used for the firmware.\nMost computer peripherals are themselves special-purpose computers. Devices such as printers, scanners, webcams, and USB flash drives have internally-stored firmware; some devices may also permit field upgrading of their firmware. For modern simpler devices, such as USB keyboards, USB mouses and USB sound cards, the trend is to store the firmware in on-chip memory in the device's microcontroller, as opposed to storing it in a separate EEPROM chip.\nExamples of computer firmware include:\nHome and personal-use products.\nConsumer appliances like digital cameras and portable music players support firmware upgrades. Some companies use firmware updates to add new playable file formats (codecs). Other features that may change with firmware updates include the user interface or even the battery life.\nAutomobiles.\nSince 1996, most automobiles have employed an on-board computer and various sensors to detect mechanical problems. As of 2010[ [update]], modern vehicles also employ computer-controlled anti-lock braking systems (ABS) and computer-operated transmission control units (TCUs). The driver can also get in-dash information while driving in this manner, such as real-time fuel economy and tire pressure readings. Local dealers can update most vehicle firmware.\nOther examples.\nOther firmware applications include:\nFlashing.\n\"Flashing\" is a process that involves the overwriting of existing firmware or data, contained in EEPROM or flash memory module present in an electronic device, with new data. This can be done to upgrade a device or to change the provider of a service associated with the function of the device, such as changing from one mobile phone service provider to another or installing a new operating system. If firmware is upgradable, it is often done via a program from the provider, and will often allow the old firmware to be saved before upgrading so it can be reverted to if the process fails, or if the newer version performs worse. Free software replacements for vendor flashing tools have been developed, such as Flashrom.\nFirmware hacking.\nSometimes, third parties develop an unofficial new or modified (\"aftermarket\") version of firmware to provide new features or to unlock hidden functionality; this is referred to as custom firmware. An example is Rockbox as a firmware replacement for portable media players. There are many homebrew projects for various devices, which often unlock general-purpose computing functionality in previously limited devices (e.g., running Doom on iPods).\nFirmware hacks usually take advantage of the firmware update facility on many devices to install or run themselves. Some, however, must resort to exploits to run, because the manufacturer has attempted to lock the hardware to stop it from running unlicensed code.\nMost firmware hacks are free software.\nHDD firmware hacks.\nThe Moscow-based Kaspersky Lab discovered that a group of developers it refers to as the Equation Group has developed hard disk drive firmware modifications for various drive models, containing a trojan horse that allows data to be stored on the drive in locations that will not be erased even if the drive is formatted or wiped. Although the Kaspersky Lab report did not explicitly claim that this group is part of the United States National Security Agency (NSA), evidence obtained from the code of various Equation Group software suggests that they are part of the NSA.\nResearchers from the Kaspersky Lab categorized the undertakings by Equation Group as the most advanced hacking operation ever uncovered, also documenting around 500 infections caused by the Equation Group in at least 42 countries.\nSecurity risks.\nMark Shuttleworth, the founder of the company Canonical, which created the Ubuntu Linux distribution, has described proprietary firmware as a security risk, saying that \"firmware on your device is the NSA's best friend\" and calling firmware \"a trojan horse of monumental proportions\". He has asserted that low-quality, closed source firmware is a major threat to system security: \"Your biggest mistake is to assume that the NSA is the only institution abusing this position of trust\u00a0\u2013 in fact, it's reasonable to assume that all firmware is a cesspool of insecurity, courtesy of incompetence of the highest degree from manufacturers, and competence of the highest degree from a very wide range of such agencies\". As a potential solution to this problem, he has called for declarative firmware, which would describe \"hardware linkage and dependencies\" and \"should not include executable code\". Firmware should be open-source so that the code can be checked and verified.\nCustom firmware hacks have also focused on injecting malware into devices such as smartphones or USB devices. One such smartphone injection was demonstrated on the Symbian OS at MalCon, a hacker convention. A USB device firmware hack called BadUSB was presented at the Black Hat USA 2014 conference, demonstrating how a USB flash drive microcontroller can be reprogrammed to spoof various other device types to take control of a computer, exfiltrate data, or spy on the user. Other security researchers have worked further on how to exploit the principles behind BadUSB, releasing at the same time the source code of hacking tools that can be used to modify the behavior of different USB devices.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41157", "revid": "47070250", "url": "https://en.wikipedia.org/wiki?curid=41157", "title": "Flag sequence", "text": ""}
{"id": "41158", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41158", "title": "Flat weighting", "text": "Form of weighting in noise measurement\nIn a noise-measuring set, flat weighting is a noise weighting based on an amplitude-frequency characteristic that is flat over a frequency range that must be stated. \nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nSources.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41159", "revid": "27015025", "url": "https://en.wikipedia.org/wiki?curid=41159", "title": "Flood search routing", "text": "In a telephone network, flood search routing is non-deterministic routing in which a dialed number received at a switch is transmitted to all switches, \"i.e.,\" flooded, in the area code directly connected to that switch; if the dialed number is not an affiliated subscriber at that switch, the number is then retransmitted to all directly connected switches, and then routed through the switch that has the dialed number corresponding to the particular user end instrument affiliated with it. All digits of the numbering plan are used to identify a particular subscriber. Flood search routing allows subscribers to have telephone numbers independent of switch codes. Flood search routing provides the highest probability that a telephone call will go through even though a number of switches and links fail.\nFlood search routing is used in military telecommunication systems, such as the mobile subscriber equipment (MSE) system.\nReferences.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41160", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41160", "title": "Flutter (electronics and communication)", "text": "In electronics and communication, flutter is the rapid variation of signal parameters, such as amplitude, phase, and frequency. Examples of electronic flutter are:\nSee also.\n\"Electronic Flutter\"\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41161", "revid": "18054835", "url": "https://en.wikipedia.org/wiki?curid=41161", "title": "Flywheel effect", "text": "Continuation of oscillations in an oscillator circuit\nThe flywheel effect is the continuation of oscillations in an oscillator circuit after the control stimulus has been removed. This is usually caused by interacting inductive and capacitive elements in the oscillator. Circuits undergoing such oscillations are said to be flywheeling.\nThe flywheel effect may be desirable, such as in phase-locked loops used in synchronous systems, or undesirable, such as in voltage-controlled oscillators.\nFlywheel effect is used in Class C modulation where efficiency of modulation can be achieved as high as 90%.\nReferences.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41162", "revid": "668752", "url": "https://en.wikipedia.org/wiki?curid=41162", "title": "FM improvement factor", "text": ""}
{"id": "41163", "revid": "668752", "url": "https://en.wikipedia.org/wiki?curid=41163", "title": "FM improvement threshold", "text": ""}
{"id": "41164", "revid": "20585603", "url": "https://en.wikipedia.org/wiki?curid=41164", "title": "Foreign exchange service", "text": "Foreign exchange service may refer to:\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "41165", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41165", "title": "Foreign instrumentation signals intelligence", "text": "Foreign instrumentation signals intelligence, FISINT (\"F\"oreign \"I\"nstrumentation \"S\"ignature \"INT\"elligence) is intelligence from the interception of foreign electromagnetic emissions associated with the testing and operational deployment of foreign aerospace, surface, and subsurface systems. Since it deals with signals that have communicational content, it is a subset of Communications Intelligence (COMINT), which, in turn, is a subset of SIGINT. Unlike general COMINT signals, the content of FISINT signals is not in regular human language, but rather in machine to machine (instrumentation) language or in a combination of regular human language and instrumentation language. FISINT is also considered as a subset of MASINT (measurement and signature intelligence).\nTypical examples of such communication include:\nTelecommunications.\nIn telecommunications, the term FISINT has the following meanings: \nForeign instrumentation signals include but are not limited to signals from telemetry, beaconry, electronic interrogators, tracking/fusing/arming/firing command systems, and video data links.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "41166", "revid": "38627444", "url": "https://en.wikipedia.org/wiki?curid=41166", "title": "Forward echo", "text": "Propagation of a signal reflection in the same direction as the original\nIn telecommunications, a forward echo is the propagation of a signal reflection in the same direction as the original signal and consisting of energy reflected back by one discontinuity and then forward again by another discontinuity. Forward echoes can be supported by reflections caused by splices or other discontinuities in the transmission medium (e.g. optical fiber, twisted pair, or coaxial tube). In metallic lines, they may be supported by impedance mismatches between the source or load and the characteristic impedance of the transmission medium. They may cause attenuation distortion.\nReferences.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41167", "revid": "24247980", "url": "https://en.wikipedia.org/wiki?curid=41167", "title": "Forward error correction", "text": ""}
{"id": "41168", "revid": "27823944", "url": "https://en.wikipedia.org/wiki?curid=41168", "title": "Forward scatter", "text": "Small angle deflection of waves\nForward scattering is the deflection of waves by small angles so that they continue to move in close to the same direction as before the scattering. It can occur with all types of waves, for instance light, ultraviolet radiation, X-rays as well as matter waves such as electrons, neutrons and even water waves. It can be due to diffraction, refraction, and low angle reflection. It almost always occurs when the wavelength of the radiation used is small relative to the features which lead to the scattering. Forward scatter is essentially the reverse of backscatter.\nMany different examples exist, and there are very large fields where forward scattering dominates, in particular for electron diffraction and electron microscopy, X-ray diffraction and neutron diffraction. In these the relevant waves are transmitted through the samples. One case where there is forward scattering in a reflection geometry is reflection high-energy electron diffraction.\nGeneral description.\nWhenever waves encounter obstacles of any type there are changes in the direction of the waves (wave vector) by diffraction, and sometimes its energy by inelastic scattering. These processes occur for all types of waves, although how they behave varies with both their type and that of the obstacle. As illustrated in the figure, if the change in the wave vector q is fairly small the scattered wave moves in close to the same direction as the input\u2014it has been scattered. In most cases the change in the wave vector scales inversely with the size of obstacles, so forward scattering is more common when the obstacles are large compared to the wavelength of the radiation.\nIn many cases the waves of interest have relatively small wavelengths, for instance high-energy electrons or X-rays. However, the process is very general and can also be seen when water flows through a narrow channel as shown in the figure at the Blue Lagoon.\nComets.\nForward scattering can make a back-lit comet appear significantly brighter because the dust and ice crystals are reflecting and enhancing the apparent brightness of the comet by scattering that light towards the observer. Comets studied forward-scattering in visible-thermal photometry include C/1927 X1 (Skjellerup\u2013Maristany), C/1975 V1 (West), and C/1980 Y1 (Bradfield). Comets studied forward-scattering in SOHO non-thermal C3 coronograph photometry include 96P/Machholz and C/2004 F4 (Bradfield). The brightness of the great comets C/2006 P1 (McNaught) and Comet Skjellerup\u2013Maristany near perihelion were enhanced by forward scattering.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41169", "revid": "27823944", "url": "https://en.wikipedia.org/wiki?curid=41169", "title": "Frequency of optimum transmission", "text": "Frequency of optimum transmission (FOT), in the transmission of radio waves via ionospheric reflection, is the highest effective (i.e. working) frequency that is predicted to be usable for a specified path and time for 90% of the days of the month. The FOT is normally just below the value of the maximum usable frequency (MUF). In the prediction of usable frequencies, the FOT is commonly taken as 15% below the monthly median value of the MUF for the specified time and path. \nThe FOT is usually the most effective frequency for ionospheric reflection of radio waves between two specified points on Earth. \nSynonyms for this term include:"}
{"id": "41170", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41170", "title": "Four-wire circuit", "text": "In telecommunications, a four-wire circuit is a two-way circuit using two paths so arranged that the respective signals are transmitted in one direction only by one path and in the other direction by the other path. The four-wire circuit gets its name from the fact that is uses four conductors to create two complete electrical circuits, one for each direction. The two separate circuits (channels) allow full-duplex operation with low crosstalk.\nIn telephony a four-wire circuit was historically used to transport and switch baseband audio signals in the phone company telephone exchange before the advent of digital modulation and the electronic switching system eliminated baseband audio from the telco plant except for the local loop. The local loop is a two-wire circuit for one reason only: to save copper. Using half the number of copper wire conductors per circuit means that the infrastructure cost for wiring each circuit is halved. Although a lower quality circuit, the local loop allows full duplex operation by using a telephone hybrid to keep near and far voice levels equivalent.\nAs the public switched telephone network expanded in size and scope, using many individual wires inside the telco plant became so impractical and labor-intensive that in-office and inter-office signal wiring progressed to high bandwidth coaxial cable (still a popular interconnection method in the 21st century, used with the Lucent 5ESS Class-5 telephone switch to present day), microwave radio relay and ultimately fiber-optic communication for high-speed trunk circuits. At the end of the 20th century, four-wire circuits saw renewed growth for corporate local loop service for use in dedicated line service for computer modems to interconnect company computer networks and to connect networks to an Internet service provider for Internet connectivity before commodity DSL and cable modem connectivity was widely available.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41171", "revid": "28856560", "url": "https://en.wikipedia.org/wiki?curid=41171", "title": "Four-wire terminating set", "text": "A four-wire terminating set (4WTS) is a balanced transformer used to perform a conversion between four-wire and two-wire operation in telecommunication systems.\nFor example, a 4-wire circuit may, by means of a 4-wire terminating set, be connected to a 2-wire telephone set. Also, a pair of 4-wire terminating sets may be used to introduce an intermediate 4-wire circuit into a 2-wire circuit, in which loop repeaters may be situated to amplify signals in each direction without positive feedback and oscillation. \nThe 4WTS differs from a simple hybrid coil in being equipped to adjust its impedance to maximize return loss.\nFour-wire terminating sets were largely supplanted by resistance hybrids in the late 20th century.\nReferences.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41172", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41172", "title": "Frame (networking)", "text": "Data transmission unit\nA frame is a digital data transmission unit in computer networking and telecommunications. In packet switched systems, a frame is a simple container for a single network packet. In other telecommunications systems, a frame is a repeating structure supporting time-division multiplexing.\nA frame typically includes frame synchronization features consisting of a sequence of bits or symbols that indicate to the receiver the beginning and end of the payload data within the stream of symbols or bits it receives. If a receiver is connected to the system during frame transmission, it ignores the data until it detects a new frame synchronization sequence.\nPacket switching.\nIn the OSI model of computer networking, a frame is the protocol data unit at the data link layer. Frames are the result of the final layer of encapsulation before the data is transmitted over the physical layer. A frame is \"the unit of transmission in a link layer protocol, and consists of a link layer header followed by a packet.\" Each frame is separated from the next by an interframe gap. A frame is a series of bits generally composed of frame synchronization bits, the packet payload, and a frame check sequence. Examples are Ethernet frames, Wi-Fi frames, 4G frames, Point-to-Point Protocol (PPP) frames, Fibre Channel frames, and V.42 modem frames.\nOften, frames of several different sizes are nested inside each other. For example, when using Point-to-Point Protocol (PPP) over asynchronous serial communication, the eight bits of each individual byte are framed by start and stop bits, the payload data bytes in a network packet are framed by the header and footer, and several packets can be framed with frame boundary octets.\nTime-division multiplex.\nIn telecommunications, specifically in time-division multiplex (TDM) and time-division multiple access (TDMA) variants, a frame is a cyclically repeated data block that consists of a fixed number of time slots, one for each logical TDM channel or TDMA transmitter. In this context, a frame is typically an entity at the physical layer. TDM application examples are SONET/SDH and the ISDN circuit-switched B-channel, while TDMA examples are Circuit Switched Data used in early cellular voice services. The frame is also an entity for time-division duplex, where the mobile terminal may transmit during some time slots and receive during others.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41173", "revid": "1812441", "url": "https://en.wikipedia.org/wiki?curid=41173", "title": "Frame rate", "text": "Number of frames rendered in one second\nFrame rate, most commonly expressed in frame/s, &lt;templatestyles src=\"Template:Visible anchor/styles.css\" /&gt;frames per second or FPS, is typically the frequency (rate) at which consecutive images (frames) are captured or displayed. This definition applies to film and video cameras, computer animation, and motion capture systems. In these contexts, frame rate may be used interchangeably with &lt;templatestyles src=\"Template:Visible anchor/styles.css\" /&gt;frame frequency and refresh rate, which are expressed in hertz (Hz). Additionally, in the context of computer graphics performance, FPS is the rate at which a system, particularly a GPU, is able to generate frames, and refresh rate is the frequency at which a display shows completed frames. In electronic camera specifications frame rate refers to the maximum possible rate frames could be captured, but in practice, other settings (such as exposure time) may reduce the actual frequency to a lower number than the frame rate. \nHuman vision.\nThe temporal sensitivity and resolution of human vision varies depending on the type and characteristics of visual stimulus, and it differs between individuals. The human visual system can process 10 to 12 images per second and perceive them individually, while higher rates are perceived as motion. Modulated light (such as a computer display) is perceived as stable by the majority of participants in studies when the rate is higher than 50\u00a0FPS. This perception of modulated light as steady is known as the flicker fusion threshold. However, when the modulated light is non-uniform and contains an image, the flicker fusion threshold can be much higher, in the hundreds of hertz. With regard to image recognition, people have been found to recognize a specific image in an unbroken series of different images, each of which lasts as little as 13 milliseconds. Persistence of vision sometimes accounts for very short single-millisecond visual stimulus having a perceived duration of between 100\u00a0ms and 400\u00a0ms. Multiple stimuli that are very short are sometimes perceived as a single stimulus, such as a 10\u00a0ms green flash of light immediately followed by a 10\u00a0ms red flash of light perceived as a single yellow flash of light.\nFilm and video.\nSilent film.\nEarly silent films had stated frame rates anywhere from 16 to 24 frames per second (FPS), but since the cameras were hand-cranked, the rate often changed during the scene to fit the mood. Projectionists could also change the frame rate in the theater by adjusting a rheostat controlling the voltage powering the film-carrying mechanism in the projector. Film companies often intended for theaters to show their silent films at a higher frame rate than that at which they were filmed. These frame rates were enough for the sense of motion, but it was perceived as jerky motion. To minimize the perceived flicker, projectors employed dual- and triple-blade shutters, so each frame was displayed two or three times, increasing the flicker rate to 48 or 72\u00a0FPS and reducing eye strain. Thomas Edison said that 46 frames per second was the minimum needed for the eye to perceive motion: \"Anything less will strain the eye.\" In the mid to late 1920s, the frame rate for silent film increased to 20\u201326\u00a0FPS.\nSound film.\nWhen sound film was introduced in 1926, variations in film speed were no longer tolerated, as the human ear is more sensitive than the eye to changes in frequency. Many theaters had shown silent films at 22 to 26\u00a0FPS, which is why the industry chose 24\u00a0FPS for sound film as a compromise. From 1927 to 1930, as various studios updated equipment, the rate of 24\u00a0FPS became standard for 35\u00a0mm sound film. At 24\u00a0FPS, the film travels through the projector at a rate of per second. This allowed simple two-blade shutters to give a projected series of images at 48 per second, satisfying Edison's recommendation. Many modern 35\u00a0mm film projectors use three-blade shutters to give 72 images per second\u2014each frame is flashed on screen three times.\nAnimation.\nIn drawn animation, moving characters are often animated \"on twos\", meaning one drawing is displayed for every two frames of film. Since film typically runs at 24 frames per second, this results in a display of only 12 drawings per second. Even though the image update rate is low, the fluidity is satisfactory for most subjects. However, when a character is required to perform a quick movement, it is usually necessary to revert to animating \"on ones\", as \"twos\" are too slow to convey the motion adequately. A blend of the two techniques keeps the eye fooled and controls production cost.\nAnimation for most \"Saturday morning cartoons\" firstly introduced in the mid-1960s was produced as cheaply as possible and was most often shot on \"threes\" or even \"fours\", i.e. three or four frames per drawing. This translates to only 8 or 6 drawings per second respectively. Anime is also usually drawn on threes or twos.\nModern video standards.\nDue to the mains frequency of electric grids, analog television broadcast was developed with frame rates of 50\u00a0FPS (most of the world) or 60\u00a0FPS (Canada, US, Mexico, Philippines, Japan, South Korea). The frequency of the electricity grid was extremely stable and therefore it was logical to use for synchronization.\nThe introduction of color television technology made it necessary to lower that 60\u00a0FPS frequency by 0.1% to avoid \"dot crawl\", a display artifact appearing on legacy black-and-white displays, showing up on highly color-saturated surfaces. It was found that by lowering the frame rate by 0.1%, the undesirable effect was minimized.\nAs of 2025[ [update]], video transmission standards in North America, Japan, and South Korea are still based on 60/1.001 \u2248 59.94 images per second. Two sizes of images are typically used: (1080i interlaced or 1080p progressive) and (720p). Confusingly, \"interlaced\" formats are customarily stated at half their image rate, 29.97/25 FPS, and \"double\" their image height, but these statements are purely custom; in each format, 60 images per second are produced. A resolution of 1080i produces 59.94 or 50 images, each squashed to half-height in the photographic process and stretched back to fill the screen on playback in a television set. The 720p format produces 59.94/50 or 29.97/25 images, not squeezed, so that no expansion or squeezing of the image is necessary. This confusion was industry-wide in the early days of digital video software, with much software being written incorrectly, the developers believing that only 29.97 images were expected each second. While it was true that each picture element was polled and sent only 29.97 times per second, the pixel location immediately below that one was polled 1/60 of a second later, part of a completely separate image for the next 1/60-second frame.\nAt its native 24\u00a0FPS rate, film could not be displayed on 60\u00a0FPS video without the necessary pulldown process, often leading to \"judder\": to convert 24 frames per second into 60 frames per second, every odd frame is repeated, playing twice, while every even frame is tripled. This creates uneven motion, appearing stroboscopic. Other conversions have similar uneven frame doubling. Newer video standards support 120, 240, or 300 frames per second, so frames can be evenly sampled for standard frame rates such as 24, 48 and 60\u00a0FPS film or 25, 30, 50 or 60\u00a0FPS video. Of course these higher frame rates may also be displayed at their native rates.\nElectronic camera specifications.\nIn electronic camera specifications, frame rate refers to the maximum possible rate frames that can be captured (e.g. if the exposure time were set to near-zero), but in practice, other settings (such as exposure time) may reduce the actual frequency to a lower number than the specification frame rate.\nComputer games.\nIn computer video games, frame rate plays an important part in the experience as, unlike film, games are rendered in real-time. 60 frames per second has for a long time been considered the minimum frame rate for smoothly animated game play. Video games designed for PAL markets, before the sixth generation of video game consoles, had lower frame rates by design due to the 50\u00a0Hz output. This noticeably made fast-paced games, such as racing or fighting games, run slower; less frequently developers accounted for the frame rate difference and altered the game code to achieve (nearly) identical pacing across both regions, with varying degrees of success. Computer monitors marketed to competitive PC gamers can hit 360, 500\u00a0FPS, or more. High frame rates make action scenes look less blurry, such as sprinting through the wilderness in an open world game, spinning rapidly to face an opponent in a first-person shooter, or keeping track of details during an intense fight in a multiplayer online battle arena. Input latency is also reduced. Some people may have difficulty perceiving the differences between high frame rates, though.\nFrame time is related to frame rate, but it measures the time between frames. A game could maintain an average of 60 frames per second but appear choppy because of a poor frame time. Game reviews sometimes average the worst 1% of frame rates, reported as the 99th percentile, to measure how choppy the game appears. A small difference between the average frame rate and 99th percentile would generally indicate a smooth experience. To mitigate the choppiness of poorly optimized games, players can set frame rate caps closer to their 99% percentile.\nWhen a game's frame rate is different than the display's refresh rate, screen tearing can occur. Vsync mitigates this, but it caps the frame rate to the display's refresh rate, increases input lag, and introduces judder. Variable refresh rate displays automatically set their refresh rate equal to the game's frame rate, as long as it is within the display's supported range.\nFrame rate up-conversion.\nFrame rate up-conversion (FRC) is the process of increasing the temporal resolution of a video sequence by synthesizing one or more intermediate frames between two consecutive frames. A low frame rate causes aliasing, yields abrupt motion artifacts, and degrades the video quality. Consequently, the temporal resolution is an important factor affecting video quality. Algorithms for FRC are widely used in applications, including visual quality enhancement, video compression and slow-motion video generation.\nMethods.\nMost FRC methods can be categorized into optical flow or kernel-based and pixel hallucination-based methods.\nFlow-based FRC.\nFlow-based methods linearly combine predicted optical flows between two input frames to approximate flows from the target intermediate frame to the input frames. They also propose flow reversal (projection) for more accurate image warping. Moreover, there are algorithms that give different weights of overlapped flow vectors depending on the object depth of the scene via a flow projection layer.\nPixel hallucination-based FRC.\nPixel hallucination-based methods use deformable convolution to the center frame generator by replacing optical flows with offset vectors. There are algorithms that also interpolate middle frames with the help of deformable convolution in the feature domain. However, since these methods directly hallucinate pixels, unlike the flow-based FRC methods, the predicted frames tend to be blurry when fast-moving objects are present.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41174", "revid": "11487766", "url": "https://en.wikipedia.org/wiki?curid=41174", "title": "Constitution of Vermont", "text": "American state constitution\nThe Constitution of the State of Vermont is the fundamental body of law of the U.S. state of Vermont, describing and framing its government. It was adopted in 1793 following Vermont's admission to the Union in 1791 and is largely based upon the 1777 Constitution of the Vermont Republic which was drafted at Windsor in the Old Constitution House and amended in 1786. At 8,295 words, it is the shortest U.S. state constitution. Largely unchanged since 1777, Vermont's Constitution is the only active constitutional document to have been drafted and ratified outside of the United States.\nHistory.\n1777.\nFrom 1777 to 1791, Vermont was an independent country, often referred to in the present day as the Vermont Republic. During that time it was usually called the State of Vermont but sometimes called the Commonwealth of Vermont or the Republic of Vermont. Its first constitution, drafted in 1777, was among the most far-reaching in guaranteeing personal freedoms and individual rights. In particular, it banned adult slavery, saying male slaves become free at the age of 21 and females at 18. The 1777 constitution's Declaration of Rights of the Inhabitants of the State of Vermont anticipated the United States Bill of Rights by a dozen years. The first chapter, a \"Declaration of Rights of the Inhabitants of the State of Vermont\", is followed by a \"Plan or Frame of Government\" outlining the structure of governance. It provided that the governor would be elected by the freemen of the state, who could vote regardless of whether they owned property, that each town would be represented in the legislative assembly, that there would be a court of law in each county, and that the legislative assembly and the governor's council would jointly hold legislative power. Pennsylvania's Constitution inspired Vermont (and Georgia) to similarly establish a unicameral legislature. \n1786.\nIn 1786, the Constitution was extensively revised to establish a far greater separation of powers than what had prevailed under the 1777 Constitution. In particular, it forbade anyone to simultaneously hold more than one of certain offices, including those of judges, legislators, members of the governor's council, the governor, and the surveyor-general. It also provided that the legislature could no longer function as a court of appeals nor otherwise intervene in cases before the courts, as it had often done.\nThe 1786 Constitution continued in effect when, in 1791, Vermont made the transition from independence to the status of one of the states of the Union. In particular, the governor, the members of the governor's council, and other officers of the state, including judges in all courts, simply continued their terms of office that were already underway.\n1793.\nThe 1793 Constitution was adopted two years after Vermont's admission to the Union and continues in effect, with various later amendments, to this day. It eliminated all mention of grievances against King George III and against the State of New York. In 1790, New York's legislature finally renounced its claims that Vermont was a part of New York, the cessation of those claims being effective if and when Congress decided to admit Vermont to the Union.\nVermont held constitutional conventions in 1777, 1786, 1793, 1814, 1822, 1828, 1836, 1843, 1850, 1857, and 1870.\nCouncil of Censors.\n\"In order that the freedom of this Commonwealth may be preserved inviolate\" the 1777 constitution established a Council of Censors. This body consisted of thirteen elected members, chosen every seven years, but not from the Council or General Assembly. They were to check that \"the legislative and executive branches of government have performed their duty as guardians of the people\". They also had the power to call a convention, if needed, to amend the constitution. This council had been based on a similar element of the Pennsylvania Constitution of 1776.\nIn 1786, the constitution was amended with language proposed by the 1785 Council of Censors, their first meeting, and adopted by the 1786 Constitutional Convention. The section on the Council of Censors remained generally unchanged, with only an added clarification of scope.\nIn 1793, the constitution was amended with language proposed by the 1792 Council of Censors and adopted by the 1793 Constitutional Convention. The Council now had the \"power to send for persons, papers, and records\".\nIn 1870, the constitution was amended with language proposed by the 1869 Council of Censors, their last meeting, and adopted by the 1870 Constitutional Convention. The Council of Censors was abolished and replaced by a new procedure to amend the constitution.\nAmending the constitution.\nThe Vermont Constitution, Chapter 2, Section 72 establishes the procedure for amending the constitution. The Vermont General Assembly, the state's bi-cameral legislature, has the sole power to propose amendments to the Constitution of Vermont. The process must be initiated by a Senate that has been elected in an \"off-year\", that is, an election that does not coincide with the election of the U.S. president. An amendment must originate in the Senate and be approved by a two-thirds vote. It must then receive a majority vote in the House. Then, after a newly elected legislature is seated, the amendment must receive a majority vote in each chamber, first in the Senate, then in the House. The proposed amendment must then be presented to the voters as a referendum and receive a majority of the votes cast.\n1990s revision to gender-neutral language.\nIn 1991 and again in 1993, the Vermont General Assembly approved a constitutional amendment authorizing the justices of the Vermont Supreme Court to revise the Constitution in \"gender-inclusive\" language, replacing gender-specific terms. (Examples: \"men\" and \"women\" were replaced by \"persons\", and the \"Freeman's Oath,\" required of all newly registered voters in the state, was renamed the \"Voters' Oath\"). The revision was ratified by the voters in the general election of November 8, 1994. Vermont is one of six states whose constitutions are written in gender-neutral language.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41175", "revid": "29534843", "url": "https://en.wikipedia.org/wiki?curid=41175", "title": "Frame slip", "text": "In the reception of framed data, a frame slip is the loss of synchronization between a received frame and the receiver clock signal, causing a frame misalignment event, and resulting in the loss of the data contained in the received frame. \nA frame slip should not be confused with a dropped frame where synchronization is not lost, as in the case of buffer overflow, for example."}
{"id": "41176", "revid": "45807063", "url": "https://en.wikipedia.org/wiki?curid=41176", "title": "Frame synchronization", "text": "In telecommunications, frame synchronization or framing is the process by which, while receiving a stream of fixed-length frames, the receiver identifies the frame boundaries, permitting the data bits within the frame to be extracted for decoding or retransmission.\nWhen packets of varying length are sent, it is necessary to have an instantly recognizable packet-end delimiter (e.g., Ethernet's end of stream symbol). Loss of carrier signal can be interpreted as a packet-end delimiter in some cases. When a continuous stream of fixed-length frames are sent, a synchronized receiver can in principle identify frame boundaries forever. In practice, receivers can usually maintain synchronization despite transmission errors; bit slips are much rarer than bit errors. Thus, it is acceptable to use a much smaller frame boundary marker, at the expense of a lengthier process to establish synchronization in the first place.\nFrame synchronization is achieved when the incoming frame alignment signals are identified (that is, distinguished from data bits), permitting the data bits within the frame to be extracted for decoding or retransmission.\nFraming.\nIf the transmission is temporarily interrupted, or a bit slip event occurs, the receiver must re-synchronize.\nThe transmitter and the receiver must agree ahead of time on which frame synchronization scheme they will use.\nCommon frame synchronization schemes are:\nFrame synchronizer.\nIn telemetry applications, a \"frame synchronizer\" is used to locate frame boundaries within a serial pulse-code modulated (PCM) binary stream.\nThe frame synchronizer immediately follows the bit synchronizer in most telemetry applications. Without frame synchronization, decommutation is impossible.\nThe frame synchronizer searches the incoming bit-stream for occurrences of the frame synchronization pattern. If the pattern persists for long enough that an accidental match is implausible, the synchronizer declares the data synchronized and available for decoding. If a large number of mis-matches occur, the synchronizer declares a loss of synchronization.\nThe search can be sequential (only consider one starting point at a time), or multiple candidate starting points may be considered at once. Advanced techniques continue searching even while synchronization is established, so that, if synchronization is lost, by the time the loss is noticed a new frame start position has been found.\nIt is not uncommon to have multiple levels of frame synchronization, where a series of frames is assembled into a larger \"superframe\" or \"major frame\". Individual frames are then \"minor frames\" within that superframe. Each frame contains a subframe ID (often a simple counter) which identifies its position within the superframe. A second frame synchronizer establishes superframe synchronization. This allows subcommutation, where some data is sent less frequently than every frame. \nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41177", "revid": "37053484", "url": "https://en.wikipedia.org/wiki?curid=41177", "title": "Framing", "text": "Framing may refer to:\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "41178", "revid": "2255048", "url": "https://en.wikipedia.org/wiki?curid=41178", "title": "Framing bit", "text": ""}
{"id": "41179", "revid": "1309654649", "url": "https://en.wikipedia.org/wiki?curid=41179", "title": "Free-space path loss", "text": "Path loss of radio transmitted through air or vacuum\nIn telecommunications, the free-space path loss (FSPL) (also known as free-space loss, FSL) is the decrease in signal strength of a signal traveling between two antennas on a line-of-sight path through free space, which occurs because the signal spreads out as it propagates. The \"Standard Definitions of Terms for Antennas\", IEEE Std 145-1993, defines free-space loss as \"The loss between two isotropic radiators in free space, expressed as a power ratio.\"\nFree-space path loss increases with the square of the distance between the antennas because radio waves spread out following an inverse square law. It decreases with the square of the wavelength of the radio waves, and does not include any power loss in the antennas themselves due to imperfections such as resistance or losses due to interaction with the environment such as atmospheric absorption.\nThe FSPL is rarely used standalone, but rather as a part of the Friis transmission formula, which includes the gain of antennas. It is a major factor used in power link budgets to analyze radio communication systems, to ensure that sufficient radio power reaches the receiver so that the received signal is intelligible.\nFree-space path loss formula.\nThe free-space path loss (FSPL) formula derives from the Friis transmission formula. This states that in a radio system consisting of a transmitting antenna transmitting radio waves to a receiving antenna, the ratio of radio wave power received formula_1 to the power transmitted formula_2 is:\nformula_3\nwhere\nThe distance between the antennas formula_8 must be large enough that the antennas are in the far field of each other formula_9.\nThe free-space path loss is the loss factor in this equation that is due to distance and wavelength, or in other words, the ratio of power transmitted to power received assuming the antennas are isotropic and have no directivity (formula_10): \nformula_11\nSince the frequency of a radio wave formula_12 is equal to the speed of light formula_13 divided by the wavelength, the path loss can also be written in terms of frequency: \nformula_14\nBeside the assumption that the antennas are lossless, this formula assumes that the polarization of the antennas is the same, that there are no multipath effects, and that the radio wave path is sufficiently far away from obstructions that it acts as if it is in free space. This last restriction requires an ellipsoidal area around the line of sight out to 0.6 of the Fresnel zone be clear of obstructions. The Fresnel zone increases in diameter with the wavelength of the radio waves. The concept of free space path loss is often applied to radio systems that don't completely meet these requirements, but these imperfections can be accounted for by small constant power loss factors that can be included in the link budget.\nInfluence of distance and frequency.\nThe free-space loss increases with the distance between the antennas and decreases with the wavelength of the radio waves due to these factors:\nDerivation.\nThe radio waves from the transmitting antenna spread out in a spherical wavefront. The amount of power passing through any sphere centered on the transmitting antenna is equal. The surface area of a sphere of radius formula_8 is formula_18. Thus the intensity or power density of the radiation in any particular direction from the antenna is inversely proportional to the square of distance \nformula_19\n(The term formula_18 means the surface of a sphere, which has a radius formula_8. Please remember, that formula_8 here has a meaning of 'distance' between the two antennas, and does not mean the diameter of the sphere (as notation usually used in mathematics).) \nFor an isotropic antenna which radiates equal power in all directions, the power density is evenly distributed over the surface of a sphere centered on the antenna\nformula_23\nThe amount of power the receiving antenna receives from this radiation field is \nformula_24\nThe factor formula_16, called the \"effective area\" or \"aperture\" of the receiving antenna, which has the units of area, can be thought of as the amount of area perpendicular to the direction of the radio waves from which the receiving antenna captures energy. Since the linear dimensions of an antenna scale with the wavelength formula_26, the cross sectional area of an antenna and thus the aperture scales with the square of wavelength formula_27. The effective area of an isotropic antenna (for a derivation of this see antenna aperture article) is \nformula_28\nCombining the above (1) and (2), for isotropic antennas\nformula_29 \nformula_30\nFree-space path loss in decibels.\nA convenient way to express FSPL is in terms of decibels (dB):\nformula_31\nusing SI units of meters for formula_8, hertz (s\u22121) for formula_12, and meters per second (m\u22c5s\u22121) for formula_13, (where c=299 792 458 m/s in vacuum, \u2248 300 000 km/s)\nFor typical radio applications, it is common to find formula_8 measured in kilometers and formula_12 in gigahertz, in which case the FSPL equation becomes\nformula_37\nan increase of 240\u00a0dB, because the units increase by factors of 103 and 109 respectively, so:\nformula_38\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41180", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41180", "title": "Freeze frame television", "text": "Television in which fixed images are transmitted sequentially at a slow rate\nFreeze frame television is television in which the frames of the video are transmitted as a sequence of still images at a rate far too slow to be perceived as continuous motion by human vision. The receiving device typically displays each frame until the next complete frame is available.\nFor an image of specified quality, \"e.g.\", resolution and color depth, freeze-frame television has a lower bandwidth requirement than that of full-motion television. For this reason, NASA, which refers to this technique as \"sequential still video\", uses it on UHF when Ku band full-motion video signals are not available.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41181", "revid": "293907", "url": "https://en.wikipedia.org/wiki?curid=41181", "title": "F region", "text": "Layer in ionosphere\nThe F region of the ionosphere is home to the F layer of ionization, also called the Appleton\u2013Barnett layer, after the English physicist Edward Appleton and New Zealand physicist and meteorologist Miles Barnett. As with other ionospheric sectors, 'layer' implies a concentration of plasma, while 'region' is the volume that contains the said layer. The F region contains ionized gases at a height of around above sea level, placing it in the Earth's thermosphere, a hot region in the upper atmosphere, and also in the heterosphere, where chemical composition varies with height. Generally speaking, the F region has the highest concentration of free electrons and ions anywhere in the atmosphere. It may be thought of as comprising two layers, the F1 and F2 layers.\nThe F-region is located directly above the E region (formerly the Kennelly-Heaviside layer) and below the protonosphere. It acts as a dependable reflector of HF radio signals as it is not affected by atmospheric conditions, although its ionic composition varies with the sunspot cycle. It reflects normal-incident frequencies at or below the critical frequency (approximately 10\u00a0MHz) and partially absorbs waves of higher frequency.\nF1 and F2 layers.\nThe F1 layer is the lower sector of the F layer and exists from about above the surface of the Earth and only during daylight hours. It is composed of a mixture of molecular ions O2+ and NO+, and atomic ions O+. Above the F1 region, atomic oxygen becomes the dominant constituent because lighter particles tend to occupy higher altitudes above the turbopause (at ~). This atomic oxygen provides the O+ atomic ions that make up the F2 layer.\nThe F1 layer has approximately 5 \u00d7 105 e/cm3 (free electrons per cubic centimeter) at noontime and minimum sunspot activity, and increases to roughly 2 \u00d7 106 e/cm3 during maximum sunspot activity. The density falls off to below 104 e/cm3 at night.\nUsage in radio communication.\nCritical F2 layer frequencies are the frequencies that will not go through the F2 layer. Under rare atmospheric conditions, F2 propagation can occur, resulting in VHF television and FM radio signals being received over great distances, well beyond the normal reception area."}
{"id": "41183", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=41183", "title": "Frequency administration", "text": ""}
{"id": "41184", "revid": "48994643", "url": "https://en.wikipedia.org/wiki?curid=41184", "title": "Frequency averaging", "text": "In telecommunications, the term frequency averaging has the following meanings: \nIn frequency averaging, all oscillators are assigned equal weights to determine the ultimate network frequency. \nIn terms of musical note frequency, the averaging of the frequency of low or high notes in a solo instrumental piece is a technique used to match different instruments together so they may be played together. The musical note frequency calculation formula is used: F=(2^12/n)*440, where n equals the number of positive or negative steps away from the base note of A4(440 hertz), and F equals the frequency. The formula is used to calculate the frequency of each note in the piece. The values are then added together and divided by the number of notes. This is the average frequency of those notes. It is said that such techniques were used by classical composers, especially those who involved mathematics heavily in their music. "}
{"id": "41185", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41185", "title": "Frequency-change signaling", "text": "In telecommunications, frequency-change signaling is a telegraph signaling method in which one or more particular frequencies correspond to each desired signaling condition of a telegraph code. The transition from one set of frequencies to the other may be a continuous or a discontinuous change in the frequency or phase.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41186", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41186", "title": "Frequency compatibility", "text": "In telecommunications, the term frequency compatibility has the following meanings: \nReferences.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41187", "revid": "1535071", "url": "https://en.wikipedia.org/wiki?curid=41187", "title": "Frequency deviation", "text": "Concept in radio transmission\nFrequency deviation (formula_1) is used in FM radio to describe the difference between the minimum or maximum extent of a frequency modulated signal, and the nominal center or carrier frequency. The term is sometimes mistakenly used as synonymous with frequency drift, which is an unintended offset of an oscillator from its nominal frequency.\nThe frequency deviation of a radio is of particular importance in relation to bandwidth, because less deviation means that more channels can fit into the same amount of frequency spectrum. The FM broadcasting range between 87.5 and 108\u00a0MHz uses a typical channel spacing of 100 or 200\u00a0kHz, with a maximum frequency deviation of +/-75\u00a0kHz, in some cases leaving a buffer above the highest and below the lowest frequency to reduce interaction with other channels.\nThe most common FM transmitting applications use peak deviations of +/-75\u00a0kHz (100 or 200\u00a0kHz spacing), +/-5\u00a0kHz (15\u201325\u00a0kHz spacing), +/-2.5\u00a0kHz (3.75-12.5\u00a0kHz spacing), and +/-2\u00a0kHz (8.33\u00a0kHz spacing, 7.5\u00a0kHz spacing, 6.25\u00a0kHz spacing or 5\u00a0kHz spacing).\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41188", "revid": "13286072", "url": "https://en.wikipedia.org/wiki?curid=41188", "title": "Frequency-exchange signaling", "text": "In telegraphy, frequency-exchange signaling or two-source frequency keying is frequency-change signaling in which the change from one significant condition to another is accompanied by decay in amplitude of one or more frequencies and by buildup in amplitude of one or more other frequencies.\nFrequency-exchange signaling applies to supervisory signaling and user-information transmission. \nReferences.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41189", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41189", "title": "Frequency frogging", "text": "Term in telecommunication\nIn telecommunications, the term frequency frogging has the following meanings: \n\"Note:\" Frequency frogging is accomplished by having modulators, which are integrated into specially designed repeaters, translate a low-frequency group to a high-frequency group, and vice versa. A frequency channel will appear in the low group for one repeater section and will then be translated to the high group for the next section because of frequency frogging. This results in nearly constant attenuation with frequency over two successive repeater sections, and eliminates the need for large slope equalization and adjustments. Singing and crosstalk are minimized because the high-level output of a repeater is at a different frequency than the low-level input to other repeaters. It also diminishes group delay distortion. A repeater that receives on the high band from both direction and sends on the low band is called Hi-Lo; the other kind Lo-Hi.\nReferences.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41190", "revid": "808814", "url": "https://en.wikipedia.org/wiki?curid=41190", "title": "Frequency hopping", "text": ""}
{"id": "41191", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41191", "title": "Frequency sharing", "text": "Use of the same radio frequency by two or more broadcast licensees\nIn telecommunications, frequency sharing or channel sharing is the assignment to or use of the same radio frequency by two or more stations that are separated geographically or that use the frequency at different times. It reduces the potential for mutual interference where the assignment of different frequencies to each user is not practical or possible.\nChannel sharing in digital television.\nU.S. mobile data usage in 2017 was 40 times that in 2010, forcing frequencies to be reallocated. The FCC's 2016 auction allowed two or more stations to share a single 6\u00a0MHz television channel while retaining their licenses and all rights.\nNBC sold the spectrum of three of its stations in the 2017 FCC auction: WNBC New York, Telemundo WSNS-TV Chicago and WWSI Philadelphia. Other NBC stations in the market would begin channel sharing with those stations; for instance, Comcast moved Channel 28 WNBC onto Telemundo's Channel 35 WNJU, broadcasting both stations from WNJU's antenna. Stations had to either channel-share with another TV station in this way or go off the air by Jan. 23, 2018.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\n*\nExternal links.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41192", "revid": "22883165", "url": "https://en.wikipedia.org/wiki?curid=41192", "title": "Frequency shift", "text": "In the physical sciences and in telecommunication, the term frequency shift may refer to:\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "41193", "revid": "42724404", "url": "https://en.wikipedia.org/wiki?curid=41193", "title": "Frequency-shift keying", "text": "Data communications modulation protocol\nFrequency-shift keying (FSK) is a frequency modulation scheme in which digital information is encoded on a carrier signal by periodically shifting the frequency of the carrier between several discrete frequencies. The technology is used for communication systems such as telemetry, weather balloon radiosondes, caller ID, garage door openers, and low frequency radio transmission in the VLF and ELF bands. The simplest FSK is binary FSK (BFSK, which is also commonly referred to as 2FSK or 2-FSK), in which the carrier is shifted between two discrete frequencies to transmit binary (0s and 1s) information.\nModulating and demodulating.\nReference implementations of FSK modems exist and are documented in detail. The demodulation of a binary FSK signal can be done using the Goertzel algorithm very efficiently, even on low-power microcontrollers.\nVariations.\nContinuous-phase frequency-shift keying.\nIn principle FSK can be implemented by using completely independent free-running oscillators, and switching between them at the beginning of each symbol period.\nIn general, independent oscillators will not be at the same phase and therefore the same amplitude at the switch-over instant,\ncausing sudden discontinuities in the transmitted signal.\nIn practice, many FSK transmitters use only a single oscillator, and the process of switching to a different frequency at the beginning of each symbol period preserves the phase.\nThe elimination of discontinuities in the phase (and therefore elimination of sudden changes in amplitude) reduces sideband power, reducing interference with neighboring channels.\nGaussian frequency-shift keying.\nRather than directly modulating the frequency with the digital data symbols, \"instantaneously\" changing the frequency at the beginning of each symbol period, Gaussian frequency-shift keying (GFSK) filters the data pulses with a Gaussian filter to make the transitions smoother. This filter has the advantage of reducing sideband power, reducing interference with neighboring channels, at the cost of increasing intersymbol interference. It is used by Improved Layer 2 Protocol, DECT, Bluetooth, Cypress WirelessUSB, Nordic Semiconductor, Texas Instruments, IEEE 802.15.4, Z-Wave and Wavenis devices. For basic data rate Bluetooth the minimum deviation is 115\u00a0kHz.\nA GFSK modulator differs from a simple frequency-shift keying modulator in that before the baseband waveform (with levels \u22121 and +1) goes into the FSK modulator, it passed through a Gaussian filter to make the transitions smoother to limit spectral width. Gaussian filtering is a standard way to reduce spectral width; it is called \"pulse shaping\" in this application.\nIn ordinary non-filtered FSK, at a jump from \u22121 to +1 or +1 to \u22121, the modulated waveform changes rapidly, which introduces large out-of-band spectrum. If the pulse is changed going from \u22121 to +1 as \u22121, \u22120.98, \u22120.93, ..., +0.93, +0.98, +1, and this smoother pulse is used to determine the carrier frequency, the out-of-band spectrum will be reduced.\nMinimum-shift keying.\nMinimum frequency-shift keying or minimum-shift keying (MSK) is a particular spectrally efficient form of coherent FSK. In MSK, the difference between the higher and lower frequency is identical to half the bit rate. Consequently, the waveforms that represent a 0 and a 1 bit differ by exactly half a carrier period. The maximum frequency deviation is \u03b4\u00a0=\u00a00.25\u00a0\"fm\", where \"fm\" is the maximum modulating frequency. As a result, the modulation index \"m\" is 0.5. This is the smallest FSK modulation index that can be chosen such that the waveforms for 0 and 1 are orthogonal.\nGaussian minimum-shift keying.\nA variant of MSK called Gaussian minimum-shift keying (GMSK) is used in the GSM mobile phone standard.\nAudio frequency-shift keying.\n\"Audio frequency-shift keying\" (AFSK) is a modulation technique by which digital data is represented by changes in the frequency (pitch) of an audio tone, yielding an encoded signal suitable for transmission via radio or telephone. Normally, the transmitted audio alternates between two tones: one, the \"mark\", represents a binary one; the other, the \"space\", represents a binary zero.\nAFSK differs from regular frequency-shift keying in performing the modulation at baseband frequencies. In radio applications, the AFSK-modulated signal normally is being used to modulate an RF carrier (using a conventional technique, such as AM or FM) for transmission.\nAFSK is not always used for high-speed data communications, since it is far less efficient in both power and bandwidth than most other modulation modes. In addition to its simplicity, however, AFSK has the advantage that encoded signals will pass through AC-coupled links, including most equipment originally designed to carry music or speech.\nAFSK is used in the U.S.-based Emergency Alert System to notify stations of the type of emergency, locations affected, and the time of issue without actually hearing the text of the alert.\nMultilevel frequency-shift keying.\nPhase 1 radios in the Project 25 system use 4-level frequency-shift keying (4FSK).\nApplications.\nIn 1910, Reginald Fessenden invented a two-tone method of transmitting Morse code. Dots and dashes were replaced with different tones of equal length. The intent was to minimize transmission time.\nSome early Continuous Wave (CW) transmitters employed an arc converter that could not be conveniently keyed. Instead of turning the arc on and off, the key slightly changed the transmitter frequency in a technique known as the \"compensation-wave method\". The compensation-wave was not used at the receiver. Spark transmitters used for this method consumed a lot of bandwidth and caused interference, so it was discouraged by 1921.\nMost early telephone-line modems used audio frequency-shift keying (AFSK) to send and receive data at rates up to about 1200\u00a0bits per second. The Bell 103 and Bell 202 modems used this technique. Even today, North American caller ID uses 1200\u00a0baud AFSK in the form of the Bell 202 standard. Some early microcomputers used a specific form of AFSK modulation, the Kansas City standard, to store data on audio cassettes. AFSK is still widely used in amateur radio, as it allows data transmission through unmodified voiceband equipment.\nAFSK is also used in the United States' Emergency Alert System to transmit warning information. It is used at higher bitrates for Weathercopy used on Weatheradio by NOAA in the U.S.\nThe CHU shortwave radio station in Ottawa, Ontario, Canada broadcasts an exclusive digital time signal encoded using AFSK modulation.\nCaller ID and remote metering standards.\nFrequency-shift keying (FSK) is commonly used over telephone lines for caller ID (displaying callers' numbers) and remote metering applications. There are several variations of this technology.\nEuropean Telecommunications Standards Institute.\nIn some countries of Europe, the European Telecommunications Standards Institute (ETSI) standards 200 778-1 and -2 \u2013 replacing 300 778-1 &amp; -2 \u2013 allow 3 physical transport layers (Telcordia Technologies (formerly Bellcore), British Telecom (BT) and Cable Communications Association (CCA)), combined with 2 data formats Multiple Data Message Format (MDMF) &amp; Single Data Message Format (SDMF), plus the Dual-tone multi-frequency (DTMF) system and a no-ring mode for meter-reading and the like. It's more of a recognition that the different types exist than an attempt to define a single \"standard\".\nTelcordia Technologies.\nThe Telcordia Technologies (formerly Bellcore) standard is used in the United States, Canada (but see below), Australia, China, Hong Kong and Singapore. It sends the data after the first ring tone and uses the 1200 bits per second Bell 202 tone modulation. The data may be sent in SDMF \u2013 which includes the date, time and number \u2013 or in MDMF, which adds a NAME field.\nBritish Telecom.\nBritish Telecom (BT) in the United Kingdom developed their own standard, which wakes up the display with a line reversal, then sends the data as CCITT v.23 modem tones in a format similar to MDMF. It is used by BT, wireless networks like the late Ionica, and some cable companies. Details are to be found in BT Supplier Information Notes (SINs) http:// (link broken 28/7/21) and http:// (link broken 28/7/21); another useful document is http:// from the EXAR website.\nCable Communications Association.\nThe Cable Communications Association (CCA) of the United Kingdom developed their own standard which sends the information after a short first ring, as either Bell 202 or V.23 tones. They developed a new standard rather than change some \"street boxes\" (multiplexors) which couldn't cope with the BT standard. The UK cable industry use a variety of switches: most are Nortel DMS-100; some are System X; System Y; and Nokia DX220. Note that some of these use the BT standard instead of the CCA one. The data format is similar to the BT one, but the transport layer is more like Telcordia Technologies, so North American or European equipment is more likely to detect it.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41194", "revid": "41719756", "url": "https://en.wikipedia.org/wiki?curid=41194", "title": "Frequency standard", "text": "Stable oscillator used for frequency calibration or reference\nA frequency standard is a stable oscillator used for frequency calibration or reference. A frequency standard generates a fundamental frequency with a high degree of accuracy and precision. Harmonics of this fundamental frequency are used to provide reference points.\nSince time is the reciprocal of frequency, it is relatively easy to derive a time standard from a frequency standard. A standard clock comprises a frequency standard, a device to count off the cycles of the oscillation emitted by the frequency standard, and a means of displaying or outputting the result.\nFrequency standards in a network or facility are sometimes administratively designated as \"primary\" or \"secondary\". The terms \"primary\" and \"secondary\", as used in this context, should not be confused with the respective technical meanings of these words in the discipline of precise time and frequency.\nFrequency reference.\nA frequency reference is an instrument used for providing a stable frequency of some kind. There are different sorts of frequency references, acoustic ones such as tuning forks but also electrical ones that emit a signal of a certain frequency (a frequency standard).\nAmong the most stable frequency references in the world are caesium standards (including caesium fountains) and hydrogen masers. Caesium standards are widely recognized as having better long-term stability, whereas hydrogen masers can attain superior short-term performance; therefore, several national standards laboratories use ensembles of caesium standards and hydrogen masers in order to combine the best attributes of both.\nThe carrier of time signal transmitters, Loran-C transmitters and of several long wave and medium wave broadcasting stations is derived from an atomic clock and can be therefore used as frequency standard.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41195", "revid": "252195", "url": "https://en.wikipedia.org/wiki?curid=41195", "title": "Fresnel reflection", "text": ""}
{"id": "41196", "revid": "46881447", "url": "https://en.wikipedia.org/wiki?curid=41196", "title": "Fresnel zone", "text": "Region of space between a transmitting and receiving antenna\nA Fresnel zone ( ), named after physicist Augustin-Jean Fresnel, is one of a series of confocal prolate ellipsoidal regions of space between and around a transmitter and a receiver. The size of the calculated Fresnel zone at any particular distance from the transmitter and receiver predicts whether obstructions or discontinuities along the path will cause significant interference.\nBackground.\nThe primary wave will travel in a relative straight line from the transmitter to the receiver. Aberrant transmitted radio, sound, or light waves which are transmitted at the same time can follow slightly different paths before reaching a receiver, especially if there are obstructions or deflecting objects between the two. The two waves can arrive at the receiver at slightly different times and the aberrant wave may arrive out of phase with the primary wave due to the different path lengths. Depending on the magnitude of the phase difference between the two waves, the waves can interfere constructively or destructively.\nSignificance.\nIn any wave-propagated transmission between a transmitter and receiver, some amount of the radiated wave propagates off-axis (not on the line-of-sight path between transmitter and receiver). This can then deflect off objects and then radiate to the receiver. However, the direct-path wave and the deflected-path wave may arrive out of phase, leading to destructive interference when the phase difference is half an odd integer (formula_1) multiple of the period. The n-th Fresnel zone is defined as the locus of points in 3D space such that a 2-segment path from the transmitter to the receiver that deflects off a point on that surface will be between n-1 and n half-wavelengths out of phase with the straight-line path. The boundaries of these zones will be ellipsoids with foci at the transmitter and receiver. In order to ensure limited interference, such transmission paths are designed with a certain clearance distance determined by a Fresnel-zone analysis. \nThe dependence on the interference on clearance is the cause of the picket-fencing effect when either the radio transmitter or receiver is moving, and the high and low signal strength zones are above and below the receiver's cut-off threshold. The extreme variations of signal strength at the receiver can cause interruptions in the communications link, or even prevent a signal from being received at all.\nFresnel zones are seen in optics, radio communications, electrodynamics, seismology, acoustics, gravitational radiation, and other situations involving the radiation of waves and multipath propagation. Fresnel zone computations are used to anticipate obstacle clearances required when designing highly directive systems such as microwave parabolic antenna systems. Although intuitively, clear line-of-sight between transmitter and receiver may seem to be all that is required for a strong antenna system, but because of the complex nature of radio waves, obstructions within the first Fresnel zone can cause significant weakness, even if those obstructions are not blocking the apparent line-of-sight signal path. For this reason, it is valuable to do a calculation of the size of the 1st, or primary, Fresnel zone for a given antenna system. Doing this will enable the antenna installer to decide if an obstacle, such as a tree, is going to make a significant impact on signal strength. The rule of thumb is that the primary Fresnel zone would ideally be 80% clear of obstacles, but must be at least 60% clear.\nSpatial structure.\nFresnel zones are confocal prolate ellipsoidal shaped regions in space (e.g. 1, 2, 3), centered around the line of the direct transmission path (path AB on the diagram). The first region includes the ellipsoidal space which the direct line-of-sight signal passes through. If a stray component of the transmitted signal bounces off an object within this region and then arrives at the receiving antenna, the phase shift will be something less than a quarter-length wave, or less than a 90\u00ba shift (path ACB on the diagram). The effect regarding phase-shift alone will be minimal. Therefore, this bounced signal can potentially result in having a positive impact on the receiver, as it is receiving a stronger signal than it would have without the deflection, and the additional signal will potentially be mostly in-phase. However, the positive attributes of this deflection also depends on the polarization of the signal relative to the object. \nThe second region surrounds the first region but excludes it. If a reflective object is located in the second region, the stray sine-wave which has bounced from this object and has been captured by the receiver will be shifted more than 90\u00ba but less than 270\u00ba because of the increased path length, and will potentially be received out-of-phase. Generally this is unfavorable. But again, this depends on polarization. Use of same circular polarization (e.g. right) in both ends, will eliminate odd number of reflections (including one).\nThe third region surrounds the second region and deflected waves captured by the receiver will have the same effect as a wave in the first region. That is, the sine wave will have shifted more than 270\u00ba but less than 450\u00ba (ideally it would be a 360\u00ba shift) and will therefore arrive at the receiver with the same shift as a signal might arrive from the first region. A wave deflected from this region has the potential to be shifted precisely one wavelength so that it is exactly in sync with the line-of-sight wave when it arrives at the receiving antenna.\nIf unobstructed and in a perfect environment, radio waves will travel in a straight line from the transmitter to the receiver. But if there are reflective surfaces that interact with a stray transmitted wave, such as bodies of water, smooth terrain, roof tops, sides of buildings, etc., the radio waves deflecting off those surfaces may arrive either out-of-phase or in-phase with the signals that travel directly to the receiver. Sometimes this results in the counter-intuitive finding that reducing the height of an antenna increases the signal-to-noise ratio at the receiver. \nAlthough radio waves generally travel in a relative straight line, fog and even humidity can cause some of the signal in certain frequencies to scatter or bend before reaching the receiver. This means objects which are clear of the line of sight path will still potentially block parts of the signal. To maximize signal strength, one needs to minimize the effect of obstruction loss by removing obstacles from both the direct radio frequency line of sight (RF LoS) line and also the area around it within the primary Fresnel zone. The strongest signals are on the direct line between transmitter and receiver and always lie in the first Fresnel zone.\nIn the early 19th century, French scientist Augustin-Jean Fresnel created a method to calculate where the zones are \u2014 that is, whether a given obstacle will cause mostly in-phase or mostly out-of-phase deflections between the transmitter and the receiver.\nClearance calculation.\nThe concept of Fresnel zone clearance may be used to analyze interference by obstacles near the path of a radio beam. The first zone must be kept largely free from obstructions to avoid interfering with the radio reception. However, some obstruction of the Fresnel zones can often be tolerated. As a rule of thumb the maximum obstruction allowable is 40%, but the recommended obstruction is 20% or less.\nFor establishing Fresnel zones, first determine the RF line of sight (RF LoS), which in simple terms is a straight line between the transmitting and receiving antennas. Now the zone surrounding the RF LoS is said to be the Fresnel zone.\nThe cross sectional radius of each Fresnel zone is the longest at the midpoint of the RF LoS, shrinking to a point at each vertex, behind the antennas.\nFormulation.\nConsider an arbitrary point \"P\" in the LoS, at a distance formula_2 and formula_3 with respect to each of the two antennas.\nTo obtain the radius formula_4 of zone formula_5, note that the volume of the zone is delimited by all points for which the difference in distances, between the reflected wave (formula_6) and the direct wave (formula_7) is the constant formula_8 (multiples of half a wavelength). This effectively defines an ellipsoid with the major axis along formula_9 and foci at the antennas (points A and B). So:\nformula_10\nRe-writing the expression with the coordinates of point formula_11 and the distance between antennas formula_12, it gives:\nformula_13\nformula_14\nAssuming the distances between the antennas and the point formula_11 are much larger than the radius and applying the binomial approximation for the square root, formula_16 (for \"x\"\u226a1), the expression simplifies to:\nformula_17\nwhich can be solved for formula_4:\nformula_19\nFor a satellite-to-Earth link, it further simplifies to:\nformula_20\nMaximum clearance.\nFor practical applications, it is often useful to know the maximum radius of the first Fresnel zone. Using formula_21, formula_22, and formula_23 in the above formula gives\nformula_24\nwhere\nformula_12 is the distance between the two antennas,\nformula_26 is the frequency of the transmitted signal,\nformula_27 \u2248 is the speed of light in the air.\nSubstitution of the numeric value for formula_27 followed by a unit conversion results in an easy way to calculate the radius of the first Fresnel zone formula_29, knowing the distance between the two antennas formula_12 and the frequency of the transmitted signal formula_26:\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41197", "revid": "140154", "url": "https://en.wikipedia.org/wiki?curid=41197", "title": "Front-to-back ratio", "text": "In telecommunication, the term front-to-back ratio (\"also known as front-to-rear ratio\") can mean:\nThe ratio compares the antenna gain in a specified direction, \"i.e.\", azimuth, usually that of maximum gain, to the gain in a direction 180\u00b0 from the specified azimuth. A front-to-back ratio is usually expressed in decibels (dB). \nIn point-to-point microwave antennas, a \"high performance\" antenna usually has a higher front to back ratio than other antennas. For example, an unshrouded 38 GHz microwave dish may have a front to back ratio of 64 dB, while the same size reflector equipped with a shroud would have a front to back ratio of 70 dB. Other factors affecting the front to back ratio of a parabolic microwave antenna include the material of the dish and the precision with which the reflector itself was formed.\nIn other electrical engineering the front to back ratio is a ratio of parameters used to characterize rectifiers or other devices, in which electric current, signal strength, resistance, or other parameters, in one direction is compared with that in the opposite direction.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41199", "revid": "28438779", "url": "https://en.wikipedia.org/wiki?curid=41199", "title": "FTS2000", "text": "Federal Telecommunications System 2000 (FTS2000) is a long distance telecommunications service for the United States federal government, including services such as switched voice service for voice or data up to 4.8\u00a0kbit/s, switched data at 56\u00a0kbit/s and 64\u00a0kbit/s, switched digital integrated service for voice, data, image, and video up to 1.544\u00a0Mbit/s, packet switched service for data in packet form, video transmission for both compressed and wideband video, and dedicated point-to-point private line for voice and data. \n\"Note 1:\" Use of FTS2000 contract services is mandatory for use by U.S. Government agencies for all acquisitions subject to 40 U.S.C. 759. \n\"Note 2:\" No U.S. Government information processing equipment or customer premises equipment other than that which are required to provide an FTS2000 service are furnished. \n\"Note 3:\" The FTS2000 contractors will be required to provide service directly to an agency's terminal equipment interface. For example, the FTS2000 contractor might provide a terminal adapter to an agency location in order to connect FTS2000 ISDN services to the agency's terminal equipment. \n\"Note 4:\" GSA awarded two 10-year, fixed-price contracts covering FTS2000 services on December 7, 1988. \n\"Note 5:\" The Warner Amendment excludes the mandatory use of FTS2000 in instances related to maximum security.\nFTS2000 was completed in 2000, then replaced by FTS2001, and thereafter, in 2008, by Networx.\nReferences.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41200", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41200", "title": "Full width at half maximum", "text": "Concept in statistics and wave theory\nIn a distribution, full width at half maximum (FWHM) is the difference between the two values of the independent variable at which the dependent variable is equal to half of its maximum value. In other words, it is the width of a spectrum curve measured between those points on the \"y\"-axis which are half the maximum amplitude.\nHalf width at half maximum (HWHM) is half of the FWHM if the function is symmetric.\nThe term full duration at half maximum (FDHM) is preferred when the independent variable is time.\nFWHM is applied to such phenomena as the duration of pulse waveforms and the spectral width of sources used for optical communications and the resolution of spectrometers.\nThe convention of \"width\" meaning \"half maximum\" is also widely used in signal processing to define bandwidth as \"width of frequency range where less than half the signal's power is attenuated\", i.e., the power is at least half the maximum. In signal processing terms, this is at most \u22123\u00a0dB of attenuation, called \"half-power point\" or, more specifically, \"half-power bandwidth\".\nWhen half-power point is applied to antenna beam width, it is called \"half-power beam width\".\nSpecific distributions.\nNormal distribution.\nIf the considered function is the density of a normal distribution of the form\nformula_1\nwhere \"\u03c3\" is the standard deviation and \"x\"0 is the expected value, then the relationship between FWHM and the standard deviation is\nformula_2\nThe FWHM does not depend on the expected value \"x\"0; it is invariant under translations.\nThe area within this FWHM is approximately 76% of the total area under the function.\nOther distributions.\nIn spectroscopy half the width at half maximum (here \"\u03b3\"), HWHM, is in common use. For example, a Lorentzian/Cauchy distribution of height can be defined by\nformula_3\nAnother important distribution function, related to solitons in optics, is the hyperbolic secant:\nformula_4\nAny translating element was omitted, since it does not affect the FWHM. For this impulse we have:\nformula_5\nwhere arcsch is the inverse hyperbolic secant.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41201", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41201", "title": "Functional profile", "text": "In telecommunications, a functional profile is a standardization document that characterizes the requirements of a standard or group of standards, and specifies how the options and ambiguities in the standard(s) should be interpreted or implemented to (a) provide a particular information technology function, (b) provide for the development of uniform, recognized tests, and (c) promote interoperability among different network elements and terminal equipment that implement a specific profile.\nSources.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41202", "revid": "10808929", "url": "https://en.wikipedia.org/wiki?curid=41202", "title": "Fuse", "text": "Fuse or FUSE may refer to:\n&lt;templatestyles src=\"Template:TOC_right/styles.css\" /&gt;\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "41203", "revid": "27335766", "url": "https://en.wikipedia.org/wiki?curid=41203", "title": "Garble", "text": "Garble may refer to:\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "41204", "revid": "7098284", "url": "https://en.wikipedia.org/wiki?curid=41204", "title": "Gateway", "text": "Gateway often refers to a gate or portal.\nGateway or The Gateway may also refer to:\n&lt;templatestyles src=\"Template:TOC_right/styles.css\" /&gt;\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "41205", "revid": "20466351", "url": "https://en.wikipedia.org/wiki?curid=41205", "title": "Gating", "text": "Gating may refer to:\n&lt;templatestyles src=\"Template:TOC_right/styles.css\" /&gt;\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "41206", "revid": "50666884", "url": "https://en.wikipedia.org/wiki?curid=41206", "title": "Gaussian beam", "text": "Monochrome light beam whose amplitude envelope is a Gaussian function\nIn optics, a Gaussian beam is an idealized beam of electromagnetic radiation whose amplitude envelope in the transverse plane is given by a Gaussian function; this also implies a Gaussian intensity (irradiance) profile. This fundamental (or TEM00) transverse Gaussian mode describes the intended output of many lasers, as such a beam diverges less and can be focused better than any other. When a Gaussian beam is refocused by an ideal lens, a new Gaussian beam is produced. The electric and magnetic field amplitude profiles along a circular Gaussian beam of a given wavelength and polarization are determined by two parameters: the waist \"w\"0, which is a measure of the width of the beam at its narrowest point, and the position z relative to the waist.\nSince the Gaussian function is infinite in extent, perfect Gaussian beams do not exist in nature, and the edges of any such beam would be cut off by any finite lens or mirror. However, the Gaussian is a useful approximation to a real-world beam for cases where lenses or mirrors in the beam are significantly larger than the spot size \"w\"(\"z\") of the beam.\nFundamentally, the Gaussian is a solution of the paraxial Helmholtz equation, the wave equation for an electromagnetic field. Although there exist other solutions, the Gaussian families of solutions are useful for problems involving compact beams.\nMathematical form.\nThe equations below assume a beam with a circular cross-section at all values of z; this can be seen by noting that a single transverse dimension, r, appears. Beams with elliptical cross-sections, or with waists at different positions in z for the two transverse dimensions (astigmatic beams) can also be described as Gaussian beams, but with distinct values of \"w\"0 and of the \"z\" = 0 location for the two transverse dimensions x and y.\nThe Gaussian beam is a transverse electromagnetic (TEM) mode. The mathematical expression for the electric field amplitude is a solution to the paraxial Helmholtz equation. Assuming polarization in the x direction and propagation in the +\"z\" direction, the electric field in phasor (complex) notation is given by:\nformula_1\nwhere\nThe physical electric field is obtained from the phasor field amplitude given above by taking the real part of the amplitude times a time factor:\nformula_2\nwhere formula_3 is the angular frequency of the light and t is time. The time factor involves an arbitrary sign convention, as discussed at .\nSince this solution relies on the paraxial approximation, it is not accurate for very strongly diverging beams. The above form is valid in most practical cases, where \"w\"0 \u226b \"\u03bb\"/\"n\".\nThe corresponding intensity (or irradiance) distribution is given by\nformula_4\nwhere the constant \u03b7 is the wave impedance of the medium in which the beam is propagating. For free space, \"\u03b7\" = \"\u03b7\"0 \u2248 377 \u03a9. \"I\"0 = |\"E\"0|2/2\"\u03b7\" is the intensity at the center of the beam at its waist.\nIf \"P\"0 is the total power of the beam,\nformula_5\nEvolving beam width.\nAt a position z along the beam (measured from the focus), the spot size parameter w is given by a hyperbolic relation:\nformula_6\nwhere\nformula_7\nis called the Rayleigh range as further discussed below, and formula_8 is the refractive index of the medium.\nThe radius of the beam \"w\"(\"z\"), at any position z along the beam, is related to the full width at half maximum (FWHM) of the intensity distribution at that position according to:\nformula_9\nWavefront curvature.\nThe wavefronts have zero curvature (radius = \u221e) at the waist. Wavefront curvature increases in magnitude away from the waist, reaching an extremum at the Rayleigh distance, \"z\" = \u00b1\"z\"R (maximum for \"z\" = +\"z\"R, minimum for \"z\" = -\"z\"R). Beyond the Rayleigh distance, , the curvature again decreases in magnitude, approaching zero as \"z\" \u2192 \u00b1\u221e. The curvature is often expressed in terms of its reciprocal, R, the \"radius of curvature\"; for a fundamental Gaussian beam the curvature at position z is given by:\nformula_10\nso the radius of curvature \"R\"(\"z\") is \nformula_11\nBeing the reciprocal of the curvature, the radius of curvature reverses sign and is infinite at the beam waist where the curvature goes through zero.\nElliptical and astigmatic beams.\nMany laser beams have an elliptical cross-section. Also common are beams with waist positions which are different for the two transverse dimensions, called astigmatic beams. These beams can be dealt with using the above two evolution equations, but with distinct values of each parameter for x and y and distinct definitions of the \"z\" = 0 point. The Gouy phase is a single value calculated correctly by summing the contribution from each dimension, with a Gouy phase within the range \u00b1\"\u03c0\"/4 contributed by each dimension.\nAn elliptical beam will invert its ellipticity ratio as it propagates from the far field to the waist. The dimension which was the larger far from the waist, will be the smaller near the waist.\nGaussian as a decomposition into modes.\nArbitrary solutions of the paraxial Helmholtz equation can be decomposed as the sum of Hermite\u2013Gaussian modes (whose amplitude profiles are separable in x and y using Cartesian coordinates), Laguerre\u2013Gaussian modes (whose amplitude profiles are separable in r and \u03b8 using cylindrical coordinates) or similarly as combinations of Ince\u2013Gaussian modes (whose amplitude profiles are separable in \u03be and \u03b7 using elliptical coordinates). At any point along the beam z these modes include the same Gaussian factor as the fundamental Gaussian mode multiplying the additional geometrical factors for the specified mode. However different modes propagate with a different Gouy phase which is why the net transverse profile due to a superposition of modes evolves in z, whereas the propagation of any \"single\" Hermite\u2013Gaussian (or Laguerre\u2013Gaussian) mode retains the same form along a beam.\nAlthough there are other modal decompositions, Gaussians are useful for problems involving compact beams, that is, where the optical power is rather closely confined along an axis. Even when a laser is \"not\" operating in the fundamental Gaussian mode, its power will generally be found among the lowest-order modes using these decompositions, as the spatial extent of higher order modes will tend to exceed the bounds of a laser's resonator (cavity). \"Gaussian beam\" normally implies radiation confined to the fundamental (TEM00) Gaussian mode.\nBeam parameters.\nThe geometric dependence of the fields of a Gaussian beam are governed by the light's wavelength \u03bb (\"in\" the dielectric medium, if not free space) and the following beam parameters, all of which are connected as detailed in the following sections.\nBeam waist.\nThe shape of a Gaussian beam of a given wavelength \u03bb is governed solely by one parameter, the \"beam waist\" \"w\"0. This is a measure of the beam size at the point of its focus (\"z\" = 0 in the above equations) where the beam width \"w\"(\"z\") (as defined above) is the smallest (and likewise where the intensity on-axis (\"r\" = 0) is the largest). From this parameter the other parameters describing the beam geometry are determined. This includes the Rayleigh range \"z\"R and asymptotic beam divergence \u03b8, as detailed below.\nRayleigh range and confocal parameter.\nThe \"Rayleigh distance\" or \"Rayleigh range\" \"z\"R is determined given a Gaussian beam's waist size:\nformula_12\nHere \u03bb is the wavelength of the light, n is the index of refraction. At a distance from the waist equal to the Rayleigh range \"z\"R, the width w of the beam is larger than it is at the focus where \"w\" = \"w\"0, the beam waist. That also implies that the on-axis (\"r\" = 0) intensity there is one half of the peak intensity (at \"z\" = 0). That point along the beam also happens to be where the wavefront curvature (1/\"R\") is greatest.\nThe distance between the two points \"z\" = \u00b1\"z\"R is called the \"confocal parameter\" or \"depth of focus\" of the beam.\nBeam divergence.\nAlthough the tails of a Gaussian function never actually reach zero, for the purposes of the following discussion the \"edge\" of a beam is considered to be the radius where \"r\" = \"w\"(\"z\"). That is where the intensity has dropped to 1/\"e\"2 of its on-axis value. Now, for \"z\" \u226b \"z\"R the parameter \"w\"(\"z\") increases linearly with z. This means that far from the waist, the beam \"edge\" (in the above sense) is cone-shaped. The angle between that cone (whose \"r\" = \"w\"(\"z\")) and the beam axis (\"r\" \n 0) defines the \"divergence\" of the beam:\nformula_13\nIn the paraxial case, as we have been considering, \u03b8 (in radians) is then approximately\nformula_14\nwhere n is the refractive index of the medium the beam propagates through, and \u03bb is the free-space wavelength. The total angular spread of the diverging beam, or \"apex angle\" of the above-described cone, is then given by\nformula_15\nThat cone then contains 86% of the Gaussian beam's total power.\nBecause the divergence is inversely proportional to the spot size, for a given wavelength \u03bb, a Gaussian beam that is focused to a small spot diverges rapidly as it propagates away from the focus. Conversely, to \"minimize\" the divergence of a laser beam in the far field (and increase its peak intensity at large distances) it must have a large cross-section (\"w\"0) at the waist (and thus a large diameter where it is launched, since \"w\"(\"z\") is never less than \"w\"0). This relationship between beam width and divergence is a fundamental characteristic of diffraction, and of the Fourier transform which describes Fraunhofer diffraction. A beam with any specified amplitude profile also obeys this inverse relationship, but the fundamental Gaussian mode is a special case where the product of beam size at focus and far-field divergence is smaller than for any other case.\nSince the Gaussian beam model uses the paraxial approximation, it fails when wavefronts are tilted by more than about 30\u00b0 from the axis of the beam. From the above expression for divergence, this means the Gaussian beam model is only accurate for beams with waists larger than about 2\"\u03bb\"/\"\u03c0\".\nLaser beam quality is quantified by the beam parameter product (BPP). For a Gaussian beam, the BPP is the product of the beam's divergence and waist size \"w\"0. The BPP of a real beam is obtained by measuring the beam's minimum diameter and far-field divergence, and taking their product. The ratio of the BPP of the real beam to that of an ideal Gaussian beam at the same wavelength is known as \"M\"2 (\"M squared\"). The \"M\"2 for a Gaussian beam is one. All real laser beams have \"M\"2 values greater than one, although very high quality beams can have values very close to one.\nThe numerical aperture of a Gaussian beam is defined to be NA = \"n\" sin \"\u03b8\", where n is the index of refraction of the medium through which the beam propagates. This means that the Rayleigh range is related to the numerical aperture by \nformula_16\nGouy phase.\nThe \"Gouy phase\" is a phase shift gradually acquired by a beam around the focal region. At position z the Gouy phase of a fundamental Gaussian beam is given by\nformula_17\nThe Gouy phase results in an increase in the apparent wavelength near the waist (\"z\" \u2248 0). Thus the phase velocity in that region formally exceeds the speed of light. That paradoxical behavior must be understood as a near-field phenomenon where the departure from the phase velocity of light (as would apply exactly to a plane wave) is very small except in the case of a beam with large numerical aperture, in which case the wavefronts' curvature (see previous section) changes substantially over the distance of a single wavelength. In all cases the wave equation is satisfied at every position.\nThe sign of the Gouy phase depends on the sign convention chosen for the electric field phasor. With \"e\"\"i\u03c9t\" dependence, the Gouy phase changes from -\"\u03c0\"/2 to +\"\u03c0\"/2, while with \"e\"-\"i\u03c9t\" dependence it changes from +\"\u03c0\"/2 to -\"\u03c0\"/2 along the axis.\nFor a fundamental Gaussian beam, the Gouy phase results in a net phase discrepancy with respect to the speed of light amounting to \u03c0 radians (thus a phase reversal) as one moves from the far field on one side of the waist to the far field on the other side. This phase variation is not observable in most experiments. It is, however, of theoretical importance and takes on a greater range for higher-order Gaussian modes.\nPower through an aperture.\nIf a Gaussian beam is centered on a circular aperture of radius r at distance z from the beam waist, the power P that passes through the aperture is\nformula_18\nFor a circle of radius \"r\" = \"w\"(\"z\"), the fraction of power transmitted through the circle is\nformula_19\nSimilarly, about 90% of the beam's power will flow through a circle of radius \"r\" = 1.07 \u00d7 \"w\"(\"z\"), 95% through a circle of radius \"r\" = 1.224 \u00d7 \"w\"(\"z\"), and 99% through a circle of radius \"r\" = 1.52 \u00d7 \"w\"(\"z\").\nComplex beam parameter.\nThe spot size and curvature of a Gaussian beam as a function of z along the beam can also be encoded in the complex beam parameter \"q\"(\"z\") given by:\nformula_20\nThe reciprocal of \"q\"(\"z\") contains the wavefront curvature and relative on-axis intensity in its real and imaginary parts, respectively:\nformula_21\nThe complex beam parameter simplifies the mathematical analysis of Gaussian beam propagation, and especially in the analysis of optical resonator cavities using ray transfer matrices.\nThen using this form, the earlier equation for the electric (or magnetic) field is greatly simplified. If we call u the relative field strength of an elliptical Gaussian beam (with the elliptical axes in the x and y directions) then it can be separated in x and y according to:\nformula_22\nwhere \nformula_23\nwhere \"q\"\"x\"(\"z\") and \"q\"\"y\"(\"z\") are the complex beam parameters in the x and y directions.\nFor the common case of a circular beam profile, \"q\"\"x\"(\"z\") \n \"q\"\"y\"(\"z\") \n \"q\"(\"z\") and \"x\"2 + \"y\"2 = \"r\"2, which yields\nformula_24\nBeam optics.\nWhen a gaussian beam propagates through a thin lens, the outgoing beam is also a (different) gaussian beam, provided that the beam travels along the cylindrical symmetry axis of the lens, and that the lens is larger than the width of the beam. The focal length of the lens formula_25, the beam waist radius formula_26, and beam waist position formula_27 of the incoming beam can be used to determine the beam waist radius formula_28 and position formula_29 of the outgoing beam.\nLens equation.\nAs derived by Saleh and Teich, the relationship between the ingoing and outgoing beams can be found by considering the phase that is added to each point formula_30 of the gaussian beam as it travels through the lens. An alternative approach due to Self is to consider the effect of a thin lens on the gaussian beam wavefronts.\nThe exact solution to the above problem is expressed simply in terms of the magnification formula_31\nformula_32\nThe magnification, which depends on formula_26 and formula_27, is given by\nformula_35\nwhere\nformula_36\nAn equivalent expression for the beam position formula_29 is\nformula_38\nThis last expression makes clear that the ray optics thin lens equation is recovered in the limit that formula_39. It can also be noted that if formula_40 then the incoming beam is \"well collimated\" so that formula_41.\nBeam focusing.\nIn some applications it is desirable to use a converging lens to focus a laser beam to a very small spot. Mathematically, this implies minimization of the magnification formula_31. If the beam size is constrained by the size of available optics, this is typically best achieved by sending the largest possible collimated beam through a small focal length lens, i.e. by maximizing formula_43 and minimizing formula_25. In this situation, it is justifiable to make the approximation formula_45, implying that formula_46 and yielding the result formula_47. This result is often presented in the form\nformula_48\nwhere\nformula_49\nwhich is found after assuming that the medium has index of refraction formula_50 and substituting formula_51. The factors of 2 are introduced because of a common preference to represent beam size by the beam waist diameters formula_52 and formula_53, rather than the waist radii formula_28 and formula_26.\nWave equation.\nAs a special case of electromagnetic radiation, Gaussian beams (and the higher-order Gaussian modes detailed below) are solutions to the wave equation for an electromagnetic field in free space or in a homogeneous dielectric medium, obtained by combining Maxwell's equations for the curl of E and the curl of H, resulting in: \nformula_56\nwhere c is the speed of light \"in the medium\", and U could either refer to the electric or magnetic field vector, as any specific solution for either determines the other. The Gaussian beam solution is valid only in the paraxial approximation, that is, where wave propagation is limited to directions within a small angle of an axis. Without loss of generality let us take that direction to be the +\"z\" direction in which case the solution U can generally be written in terms of u which has no time dependence and varies relatively smoothly in space, with the main variation spatially corresponding to the wavenumber k in the z direction:\nformula_57\nUsing this form along with the paraxial approximation, \u22022\"u\"/\u2202\"z\"2 can then be essentially neglected. Since solutions of the electromagnetic wave equation only hold for polarizations which are orthogonal to the direction of propagation (z), we have without loss of generality considered the polarization to be in the x direction so that we now solve a scalar equation for \"u\"(\"x\", \"y\", \"z\").\nSubstituting this solution into the wave equation above yields the paraxial approximation to the scalar wave equation:\nformula_58\nWriting the wave equations in the light-cone coordinates returns this equation without utilizing any approximation. Gaussian beams of any beam waist \"w\"0 satisfy the paraxial approximation to the scalar wave equation; this is most easily verified by expressing the wave at z in terms of the complex beam parameter \"q\"(\"z\") as defined above. There are many other solutions. As solutions to a linear system, any combination of solutions (using addition or multiplication by a constant) is also a solution. The fundamental Gaussian happens to be the one that minimizes the product of minimum spot size and far-field divergence, as noted above. In seeking paraxial solutions, and in particular ones that would describe laser radiation that is \"not\" in the fundamental Gaussian mode, we will look for families of solutions with gradually increasing products of their divergences and minimum spot sizes. Two important orthogonal decompositions of this sort are the Hermite\u2013Gaussian or Laguerre-Gaussian modes, corresponding to rectangular and circular symmetry respectively, as detailed in the next section. With both of these, the fundamental Gaussian beam we have been considering is the lowest order mode.\nHigher-order modes.\nHermite-Gaussian modes.\nIt is possible to decompose a coherent paraxial beam using the orthogonal set of so-called \"Hermite-Gaussian modes\", any of which are given by the product of a factor in x and a factor in y. Such a solution is possible due to the separability in x and y in the paraxial Helmholtz equation as written in Cartesian coordinates. Thus given a mode of order (\"l\", \"m\") referring to the x and y directions, the electric field amplitude at \"x\", \"y\", \"z\" may be given by:\nformula_59 \nwhere the factors for the x and y dependence are each given by:\nformula_60\nwhere \"Lpl\" are the generalized Laguerre polynomials. \"C\" is a required normalization constant:\nformula_61.\n\"w\"(\"z\") and \"R\"(\"z\") have the same definitions as above. As with the higher-order Hermite-Gaussian modes the magnitude of the Laguerre-Gaussian modes' Gouy phase shift is exaggerated by the factor \"N\" + 1:\nformula_62\nwhere in this case the combined mode number \"N\" = |\"l\"| + 2\"p\". As before, the transverse amplitude variations are contained in the last two factors on the upper line of the equation, which again includes the basic Gaussian drop off in r but now multiplied by a Laguerre polynomial. The effect of the rotational mode number l, in addition to affecting the Laguerre polynomial, is mainly contained in the \"phase\" factor exp(\u2212\"il\u03c6\"), in which the beam profile is advanced (or retarded) by l complete 2\"\u03c0\" phases in one rotation around the beam (in \u03c6). This is an example of an optical vortex of topological charge l, and can be associated with the orbital angular momentum of light in that mode.\nInce-Gaussian modes.\nIn elliptic coordinates, one can write the higher-order modes using Ince polynomials. The even and odd Ince-Gaussian modes are given by\nformula_63\nwhere \u03be and \u03b7 are the radial and angular elliptic coordinates defined by\nformula_64\n\"C\"(\"\u03b7\", \"\u03b5\") are the even Ince polynomials of order p and degree m where \u03b5 is the ellipticity parameter. The Hermite-Gaussian and Laguerre-Gaussian modes are a special case of the Ince-Gaussian modes for \"\u03b5\" = \u221e and \"\u03b5\" = 0 respectively.\nHypergeometric-Gaussian modes.\nThere is another important class of paraxial wave modes in cylindrical coordinates in which the complex amplitude is proportional to a confluent hypergeometric function.\nThese modes have a singular phase profile and are eigenfunctions of the photon orbital angular momentum. Their intensity profiles are characterized by a single brilliant ring; like Laguerre\u2013Gaussian modes, their intensities fall to zero at the center (on the optical axis) except for the fundamental (0,0) mode. A mode's complex amplitude can be written in terms of the normalized (dimensionless) radial coordinate \"\u03c1\" = \"r\"/\"w\"0 and the normalized longitudinal coordinate \"\u0396\" = \"z\"/\"z\"R as follows:\nformula_65"}
{"id": "41207", "revid": "7611264", "url": "https://en.wikipedia.org/wiki?curid=41207", "title": "Gel", "text": "Highly viscous liquid exhibiting a kind of semi-solid behavior\nA gel is a semi-solid that can have properties ranging from soft and weak to hard and tough. Gels are defined as a substantially dilute cross-linked system, which exhibits no flow when in the steady state, although the liquid phase may still diffuse through this system.\nGels are mostly liquid by mass, yet they behave like solids because of a three-dimensional cross-linked network within the liquid. It is the cross-linking within the fluid that gives a gel its structure (hardness) and contributes to the adhesive stick (tack). In this way, gels are a dispersion of molecules of a liquid within a solid medium. The word \"gel\" was coined by 19th-century Scottish chemist Thomas Graham by clipping from \"gelatine\".\nThe process of forming a gel is called gelation.\nComposition.\nGels consist of a solid three-dimensional network that spans the volume of a liquid medium and ensnares it through surface tension effects. This internal network structure may result from physical bonds such as polymer chain entanglements (see polymers) (physical gels) or chemical bonds such as disulfide bonds (see thiomers) (chemical gels), as well as crystallites or other junctions that remain intact within the extending fluid. Virtually any fluid can be used as an extender including water (hydrogels), oil, and air (aerogel). Both by weight and volume, gels are mostly fluid in composition and thus exhibit densities similar to those of their constituent liquids. Edible jelly is a common example of a hydrogel and has approximately the density of water.\nPolyionic polymers.\nPolyionic polymers are polymers with an ionic functional group. The ionic charges prevent the formation of tightly coiled polymer chains. This allows them to contribute more to viscosity in their stretched state, because the stretched-out polymer takes up more space. This is also the reason gel hardens. See polyelectrolyte for more information.\nTypes.\nColloidal gels.\nA colloidal gel consists of a percolated network of particles in a fluid medium, providing mechanical properties, in particular the emergence of elastic behaviour. The particles can show attractive interactions through osmotic depletion or through polymeric links.\nColloidal gels have three phases in their lifespan: gelation, aging and collapse. The gel is initially formed by the assembly of particles into a space-spanning network, leading to a phase arrest. In the aging phase, the particles slowly rearrange to form thicker strands, increasing the elasticity of the material. Gels can also be collapsed and separated by external fields such as gravity. Colloidal gels show linear response rheology at low amplitudes. These materials have been explored as candidates for a drug release matrix.\nHydrogels.\nA hydrogel is a network of polymer chains that are hydrophilic, sometimes found as a colloidal gel in which water is the dispersion medium. A three-dimensional solid results from the hydrophilic polymer chains being held together by cross-links. Because of the inherent cross-links, the structural integrity of the hydrogel network does not dissolve from the high concentration of water. Hydrogels are highly absorbent (they can contain over 90% water) natural or synthetic polymeric networks.\nHydrogels also possess a degree of flexibility very similar to natural tissue, due to their significant water content. As responsive \"smart materials,\" hydrogels can encapsulate chemical systems which upon stimulation by external factors such as a change of pH may cause specific compounds such as glucose to be liberated to the environment, in most cases by a gel-sol transition to the liquid state. Chemomechanical polymers are mostly also hydrogels, which upon stimulation change their volume and can serve as actuators or sensors. The first appearance of the term 'hydrogel' in the literature was in 1894.\nOrganogels.\nAn organogel is a non-crystalline, non-glassy thermoreversible (thermoplastic) solid material composed of a liquid organic phase entrapped in a three-dimensionally cross-linked network. The liquid can be, for example, an organic solvent, mineral oil, or vegetable oil. The solubility and particle dimensions of the structurant are important characteristics for the elastic properties and firmness of the organogel. Often, these systems are based on self-assembly of the structurant molecules. (An example of formation of an undesired thermoreversible network is the occurrence of wax crystallization in petroleum.)\nOrganogels have potential for use in a number of applications, such as in pharmaceuticals, cosmetics, art conservation, and food.\nXerogels.\nA xerogel is a solid formed from a gel by drying with unhindered shrinkage. Xerogels usually retain high porosity (15\u201350%) and enormous surface area (150\u2013900 m2/g), along with very small pore size (1\u201310\u00a0nm). When solvent removal occurs under supercritical conditions, the network does not shrink and a highly porous, low-density material known as an \"aerogel\" is produced. Heat treatment of a xerogel at elevated temperature produces viscous sintering (shrinkage of the xerogel due to a small amount of viscous flow) which results in a denser and more robust solid, the density and porosity achieved depend on the sintering conditions.\nNanocomposite hydrogels.\nNanocomposite hydrogels or hybrid hydrogels, are highly hydrated polymeric networks, either physically or covalently crosslinked with each other and/or with nanoparticles or nanostructures. Nanocomposite hydrogels can mimic native tissue properties, structure and microenvironment due to their hydrated and interconnected porous structure. A wide range of nanoparticles, such as carbon-based, polymeric, ceramic, and metallic nanomaterials can be incorporated within the hydrogel structure to obtain nanocomposites with tailored functionality. Nanocomposite hydrogels can be engineered to possess superior physical, chemical, electrical, thermal, and biological properties.\nProperties.\nMany gels display thixotropy \u2013 they become fluid when agitated, but resolidify when resting.\nIn general, gels are apparently solid, jelly-like materials. It is a type of non-Newtonian fluid.\nBy replacing the liquid with gas it is possible to prepare aerogels, materials with exceptional properties including very low density, high specific surface areas, and excellent thermal insulation properties.\nThermodynamics of gel deformation.\nA gel is in essence the mixture of a polymer network and a solvent phase. Upon stretching, the network crosslinks are moved further apart from each other. Due to the polymer strands between crosslinks acting as entropic springs, gels demonstrate elasticity like rubber (which is just a polymer network, without solvent). This is so because the free energy penalty to stretch an ideal polymer segment formula_1 monomers of size formula_2 between crosslinks to an end-to-end distance formula_3 is approximately given by\n formula_4\nThis is the origin of both gel and rubber elasticity. But one key difference is that gel contains an additional solvent phase and hence is capable of having significant volume changes under deformation by taking in and out solvent. For example, a gel could swell to several times its initial volume after being immersed in a solvent after equilibrium is reached. This is the phenomenon of gel swelling. On the contrary, if we take the swollen gel out and allow the solvent to evaporate, the gel would shrink to roughly its original size. This gel volume change can alternatively be introduced by applying external forces. If a uniaxial compressive stress is applied to a gel, some solvent contained in the gel would be squeezed out and the gel shrinks in the applied-stress direction.\nTo study the gel mechanical state in equilibrium, a good starting point is to consider a cubic gel of volume formula_5 that is stretched by factors formula_6, formula_7 and formula_8 in the three orthogonal directions during swelling after being immersed in a solvent phase of initial volume formula_9. The final deformed volume of gel is then formula_10 and the total volume of the system is formula_11, that is assumed constant during the swelling process for simplicity of treatment. The swollen state of the gel is now completely characterized by stretch factors formula_6, formula_7 and formula_8 and hence it is of interest to derive the deformation free energy as a function of them, denoted as formula_15. For analogy to the historical treatment of rubber elasticity and mixing free energy, formula_15 is most often defined as the free energy difference after and before the swelling normalized by the initial gel volume formula_5, that is, a free energy difference density. The form of formula_15 naturally assumes two contributions of radically different physical origins, one associated with the elastic deformation of the polymer network, and the other with the mixing of the network with the solvent. Hence, we write\n formula_19\nWe now consider the two contributions separately. The polymer elastic deformation term is independent of the solvent phase and has the same expression as a rubber, as derived in the Kuhn's theory of rubber elasticity:\n formula_20\nwhere formula_21 denotes the shear modulus of the initial state. On the other hand, the mixing term formula_22 is usually treated by the Flory-Huggins free energy of concentrated polymer solutions formula_23, where formula_24 is polymer volume fraction. Suppose the initial gel has a polymer volume fraction of formula_25, the polymer volume fraction after swelling would be formula_26 since the number of monomers remains the same while the gel volume has increased by a factor of formula_27. As the polymer volume fraction decreases from formula_25 to formula_24, a polymer solution of concentration formula_25 and volume formula_5 is mixed with a pure solvent of volume formula_32 to become a solution with polymer concentration formula_24 and volume formula_10. The free energy density change in this mixing step is given as\n formula_35\nwhere on the right-hand side, the first term is the Flory\u2013Huggins energy density of the final swollen gel, the second is associated with the initial gel and the third is of the pure solvent prior to mixing. Substitution of formula_36 leads to\n formula_37\nNote that the second term is independent of the stretching factors formula_6, formula_7 and formula_8 and hence can be dropped in subsequent analysis. Now we make use of the Flory-Huggins free energy for a polymer-solvent solution that reads\n formula_41\nwhere formula_42 is monomer volume, formula_1 is polymer strand length and formula_44 is the Flory-Huggins energy parameter. Because in a network, the polymer length is effectively infinite, we can take the limit formula_45 and formula_23 reduces to\n formula_47\nSubstitution of this expression into formula_22 and addition of the network contribution leads to\n formula_49\nThis provides the starting point to examining the swelling equilibrium of a gel network immersed in solvent. It can be shown that gel swelling is the competition between two forces, one is the osmotic pressure of the polymer solution that favors the take in of solvent and expansion, the other is the restoring force of the polymer network elasticity that favors shrinkage. At equilibrium, the two effects exactly cancel each other in principle and the associated formula_6, formula_7 and formula_8 define the equilibrium gel volume. In solving the force balance equation, graphical solutions are often preferred.\nIn an alternative, scaling approach, suppose an isotropic gel is stretch by a factor of formula_53 in all three directions. Under the affine network approximation, the mean-square end-to-end distance in the gel increases from initial formula_54 to formula_55 and the elastic energy of one stand can be written as\n formula_56\nwhere formula_57 is the mean-square fluctuation in end-to-end distance of one strand. The modulus of the gel is then this single-strand elastic energy multiplied by strand number density formula_58 to give\n formula_59\nThis modulus can then be equated to osmotic pressure (through differentiation of the free energy) to give the same equation as we found above.\nModified Donnan equilibrium of polyelectrolyte gels.\nConsider a hydrogel made of polyelectrolytes decorated with weak acid groups that can ionize according to the reaction\n formula_60\nis immersed in a salt solution of physiological concentration. The degree of ionization of the polyelectrolytes is then controlled by the formula_61 and due to the charged nature of formula_62 and formula_63, electrostatic interactions with other ions in the systems. This is effectively a reacting system governed by acid-base equilibrium modulated by electrostatic effects, and is relevant in drug delivery, sea water desalination and dialysis technologies. Due to the elastic nature of the gel, the dispersion of formula_63 in the system is constrained and hence, there will be a partitioning of salts ions and formula_62 inside and outside the gel, which is intimately coupled to the polyelectrolyte degree of ionization. This ion partitioning inside and outside the gel is analogous to the partitioning of ions across a semipemerable membrane in classical Donnan theory, but a membrane is not needed here because the gel volume constraint imposed by network elasticity effectively acts its role, in preventing the macroions to pass through the fictitious membrane while allowing ions to pass.\nThe coupling between the ion partitioning and polyelectrolyte ionization degree is only partially by the classical Donnan theory. As a starting point we can neglect the electrostatic interactions among ions. Then at equilibrium, some of the weak acid sites in the gel would dissociate to form formula_63that electrostatically attracts positive charged formula_62 and salt cations leading to a relatively high concentration of formula_62 and salt cations inside the gel. But because the concentration of formula_62 is locally higher, it suppresses the further ionization of the acid sites. This phenomenon is the prediction of the classical Donnan theory. However, with electrostatic interactions, there are further complications to the picture. Consider the case of two adjacent, initially uncharged acid sites formula_70 are both dissociated to form formula_63. Since the two sites are both negatively charged, there will be a charge-charge repulsion along the backbone of the polymer than tends to stretch the chain. This energy cost is high both elastically and electrostatically and hence suppress ionization. Even though this ionization suppression is qualitatively similar to that of Donnan prediction, it is absent without electrostatic consideration and present irrespective of ion partitioning. The combination of both effects as well as gel elasticity determines the volume of the gel at equilibrium. Due to the complexity of the coupled acid-base equilibrium, electrostatics and network elasticity, only recently has such system been correctly recreated in computer simulations.\nAnimal-produced gels.\nSome species secrete gels that are effective in parasite control. For example, the long-finned pilot whale secretes an enzymatic gel that rests on the outer surface of this animal and helps prevent other organisms from establishing colonies on the surface of these whales' bodies.\nHydrogels existing naturally in the body include mucus, the vitreous humor of the eye, cartilage, tendons and blood clots. Their viscoelastic nature results in the soft tissue component of the body, disparate from the mineral-based hard tissue of the skeletal system. Researchers are actively developing synthetically derived tissue replacement technologies derived from hydrogels, for both temporary implants (degradable) and permanent implants (non-degradable). A review article on the subject discusses the use of hydrogels for nucleus pulposus replacement, cartilage replacement, and synthetic tissue models.\nApplications.\nMany substances can form gels when a suitable thickener or gelling agent is added to their formula. This approach is common in the manufacture of a wide range of products, from foods to paints and adhesives.\nIn fiber optic communications, a soft gel resembling hair gel in viscosity is used to fill the plastic tubes containing the fibers. The main purpose of the gel is to prevent water intrusion if the buffer tube is breached, but the gel also buffers the fibers against mechanical damage when the tube is bent around corners during installation, or flexed. Additionally, the gel acts as a processing aid when the cable is being constructed, keeping the fibers central whilst the tube material is extruded around it.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41208", "revid": "1395162", "url": "https://en.wikipedia.org/wiki?curid=41208", "title": "General purpose computer", "text": ""}
{"id": "41209", "revid": "20836525", "url": "https://en.wikipedia.org/wiki?curid=41209", "title": "Geometric optics", "text": ""}
{"id": "41210", "revid": "31025970", "url": "https://en.wikipedia.org/wiki?curid=41210", "title": "Geostationary orbit", "text": "Circular orbit above Earth's Equator and following the direction of Earth's rotation\nA geostationary orbit, also referred to as a (GEO), is a circular geosynchronous orbit in altitude above Earth's equator, in radius from Earth's center, and following the direction of Earth's rotation.\nAn object in such an orbit has an orbital period equal to Earth's rotational period, one sidereal day, and so to ground observers it appears motionless, in a fixed position in the sky. The concept of a geostationary orbit was popularised by the science fiction writer Arthur C. Clarke in the 1940s as a way to revolutionise telecommunications, and the first satellite to be placed in this kind of orbit was launched in 1963.\nCommunications satellites are often placed in a geostationary orbit so that Earth-based satellite antennas do not have to rotate to track them but can be pointed permanently at the position in the sky where the satellites are located. Weather satellites are also placed in this orbit for real-time monitoring and data collection, as are navigation satellites in order to provide a known calibration point and enhance GPS accuracy.\nGeostationary satellites are launched via a temporary orbit, and then placed in a \"slot\" above a particular point on the Earth's surface. The satellite requires periodic station-keeping to maintain its position. Modern retired geostationary satellites are placed in a higher graveyard orbit to avoid collisions.\nHistory.\nIn 1929, Herman Poto\u010dnik described both geosynchronous orbits in general and the special case of the geostationary Earth orbit in particular as useful orbits for space stations. The first appearance of a geostationary orbit in popular literature was in October 1942, in the first Venus Equilateral story by George O. Smith, but Smith did not go into details. British science fiction author Arthur C. Clarke popularised and expanded the concept in a 1945 paper entitled \"Extra-Terrestrial Relays \u2013 Can Rocket Stations Give Worldwide Radio Coverage?\", published in \"Wireless World\" magazine. Clarke acknowledged the connection in his introduction to \"The Complete Venus Equilateral\". The orbit, which Clarke first described as useful for broadcast and relay communications satellites, is sometimes called the Clarke orbit. Similarly, the collection of artificial satellites in this orbit is known as the Clarke Belt.\nIn technical terminology the orbit is referred to as either a geostationary or geosynchronous equatorial orbit, with the terms used somewhat interchangeably.\nThe first geostationary satellite was designed by Harold Rosen while he was working at Hughes Aircraft in 1959. Inspired by Sputnik 1, he wanted to use a geostationary satellite to globalise communications. Telecommunications between the US and Europe was then possible between just 136 people at a time, and reliant on high frequency radios and an undersea cable.\nConventional wisdom at the time was that it would require too much rocket power to place a satellite in a geostationary orbit and it would not survive long enough to justify the expense, so early efforts were put towards constellations of satellites in low or medium Earth orbit. The first of these were the passive Echo balloon satellites in 1960, followed by Telstar 1 in 1962. Although these projects had difficulties with signal strength and tracking, issues that could be solved using geostationary orbits, the concept was seen as impractical, so Hughes often withheld funds and support.\nBy 1961, Rosen and his team had produced a cylindrical prototype with a diameter of , height of , weighing , light and small enough to be placed into orbit. It was spin stabilised with a dipole antenna producing a pancake shaped beam. In August 1961, they were contracted to begin building the real satellite. They lost Syncom 1 to electronics failure, but Syncom 2 was successfully placed into a geosynchronous orbit in 1963. Although its inclined orbit still required moving antennas, it was able to relay TV transmissions, and allowed for US President John F. Kennedy in Washington D.C., to phone Nigerian prime minister Abubakar Tafawa Balewa aboard the USNS Kingsport docked in Lagos on August 23, 1963.\nThe first satellite placed in a geostationary orbit was Syncom 3, which was launched by a Delta D rocket in 1964. With its increased bandwidth, this satellite was able to transmit live coverage of the Summer Olympics from Japan to America. Geostationary orbits have been in common use ever since, in particular for satellite television.\nToday there are hundreds of geostationary satellites providing remote sensing and communications.\nAlthough most populated land locations on the planet now have terrestrial communications facilities (microwave, fiber-optic), with telephone access covering 96% of the population and internet access 90%, some rural and remote areas in developed countries are still reliant on satellite communications.\nUses.\nMost commercial communications satellites, broadcast satellites and SBAS satellites operate in geostationary orbits.\nCommunications.\nGeostationary communication satellites are useful because they are visible from a large area of the earth's surface, extending 81\u00b0 away in latitude and 77\u00b0 in longitude. They appear stationary in the sky, which eliminates the need for ground stations to have movable antennas. This means that Earth-based observers can erect small, cheap and stationary antennas that are always directed at the desired satellite. However, latency becomes significant as it takes about 240\u00a0ms for a signal to pass from a ground based transmitter on the equator to the satellite and back again. This delay presents problems for latency-sensitive applications such as voice communication, so geostationary communication satellites are primarily used for unidirectional entertainment and applications where low latency alternatives are not available.\nGeostationary satellites are directly overhead at the equator and appear lower in the sky to an observer nearer the poles. As the observer's latitude increases, communication becomes more difficult due to factors such as atmospheric refraction, Earth's thermal emission, line-of-sight obstructions, and signal reflections from the ground or nearby structures. At latitudes above about 81\u00b0, geostationary satellites are below the horizon and cannot be seen at all. Because of this, some Russian communication satellites have used elliptical Molniya and Tundra orbits, which have excellent visibility at high latitudes.\nMeteorology.\nA worldwide network of operational geostationary meteorological satellites is used to provide visible and infrared images of Earth's surface and atmosphere for weather observation, oceanography, and atmospheric tracking. As of 2019 there are 19 satellites in either operation or stand-by. These satellite systems include:\nThese satellites typically capture images in the visual and infrared spectrum with a spatial resolution between 0.5 and 4 square kilometres. The coverage is typically 70\u00b0, and in some cases less.\nGeostationary satellite imagery has been used for tracking volcanic ash, measuring cloud top temperatures and water vapour, oceanography, measuring land temperature and vegetation coverage, facilitating cyclone path prediction, and providing real time cloud coverage and other tracking data. Some information has been incorporated into meteorological prediction models, but due to their wide field of view, full-time monitoring and lower resolution, geostationary weather satellite images are primarily used for short-term and real-time forecasting.\nNavigation.\nGeostationary satellites can be used to augment GNSS systems by relaying clock, ephemeris and ionospheric error corrections (calculated from ground stations of a known position) and providing an additional reference signal. This improves position accuracy from approximately 5m to 1m or less.\nPast and current navigation systems that use geostationary satellites include:\nImplementation.\nLaunch.\nGeostationary satellites are launched to the east into a prograde orbit that matches the rotation rate of the equator. The smallest inclination that a satellite can be launched into is that of the launch site's latitude, so launching the satellite from close to the equator limits the amount of inclination change needed later. Additionally, launching from close to the equator allows the speed of the Earth's rotation to give the satellite a boost. A launch site should have water or deserts to the east, so any failed rockets do not fall on a populated area.\nMost launch vehicles place geostationary satellites directly into a geostationary transfer orbit (GTO), an elliptical orbit with an apogee at GEO height and a low perigee. On-board satellite propulsion is then used to raise the perigee, circularise and reach GEO.\nOrbit allocation.\nSatellites in geostationary orbit must all occupy a single ring above the equator. The requirement to space these satellites apart, to avoid harmful radio-frequency interference during operations, means that there are a limited number of orbital slots available, and thus only a limited number of satellites can be operated in geostationary orbit. This has led to conflict between different countries wishing access to the same orbital slots (countries near the same longitude but differing latitudes) and radio frequencies. These disputes are addressed through the International Telecommunication Union's allocation mechanism under the Radio Regulations. In the 1976 Bogota Declaration, eight countries located on the Earth's equator claimed sovereignty over the geostationary orbits above their territory, but the claims gained no international recognition.\nStatite proposal.\nA statite is a hypothetical satellite that uses radiation pressure from the sun against a solar sail to modify its orbit.\nIt would hold its location over the dark side of the Earth at a latitude of approximately 30 degrees. A statite is stationary relative to the Earth and Sun system rather than compared to surface of the Earth, and could ease congestion in the geostationary ring.\nRetired satellites.\nGeostationary satellites require some station keeping to keep their position, and once they run out of thruster fuel they are generally retired. The transponders and other onboard systems often outlive the thruster fuel and by allowing the satellite to move naturally into an inclined geosynchronous orbit some satellites can remain in use, or else be elevated to a graveyard orbit. This process is becoming increasingly regulated and satellites must have a 90% chance of moving over 200\u00a0km above the geostationary belt at end of life.\nSpace debris.\nSpace debris at geostationary orbits typically has a lower collision speed than at low Earth orbit (LEO) since all GEO satellites orbit in the same plane, altitude and speed; however, the presence of satellites in eccentric orbits allows for collisions at up to . Although a collision is comparatively unlikely, GEO satellites have a limited ability to avoid any debris.\nAt geosynchronous altitude, objects less than 10\u00a0cm in diameter cannot be seen from the Earth, making it difficult to assess their prevalence.\nDespite efforts to reduce risk, spacecraft collisions have occurred. The European Space Agency telecom satellite Olympus-1 was struck by a meteoroid on August 11, 1993, and eventually moved to a graveyard orbit, and in 2006 the Russian Express-AM11 communications satellite was struck by an unknown object and rendered inoperable, although its engineers had enough contact time with the satellite to send it into a graveyard orbit. In 2017, both AMC-9 and Telkom-1 broke apart from an unknown cause.\nProperties.\nA typical geostationary orbit has the following properties:\nInclination.\nAn inclination of zero ensures that the orbit remains over the equator at all times, making it stationary with respect to latitude from the point of view of a ground observer (and in the Earth-centered Earth-fixed reference frame).\nPeriod.\nThe orbital period is equal to exactly one sidereal day. This means that the satellite will return to the same point above the Earth's surface every (sidereal) day, regardless of other orbital properties. For a geostationary orbit in particular, it ensures that it holds the same longitude over time. This orbital period, \"T\", is directly related to the semi-major axis of the orbit through Kepler's Third Law:\n formula_1\nwhere:\nEccentricity.\nThe eccentricity is zero, which produces a circular orbit. This ensures that the satellite does not move closer or further away from the Earth, which would cause it to track backwards and forwards across the sky.\nStability.\nA geostationary orbit can be achieved only at an altitude very close to and directly above the equator. This equates to an orbital speed of and an orbital period of 1,436 minutes, one sidereal day. This ensures that the satellite will match the Earth's rotational period and has a stationary footprint on the ground. All geostationary satellites have to be located on this ring.\nA combination of lunar gravity, solar gravity, and the flattening of the Earth at its poles causes a precession motion of the orbital plane of any geostationary object, with an orbital period of about 53 years and an initial inclination gradient of about 0.85\u00b0 per year, achieving a maximal inclination of 15\u00b0 after 26.5 years. To correct for this perturbation, regular orbital stationkeeping maneuvers are necessary, amounting to a delta-v of approximately 50\u00a0m/s per year.\nA second effect to be taken into account is the longitudinal drift, caused by the asymmetry of the Earth \u2013 the equator is slightly elliptical (equatorial eccentricity). There are two stable equilibrium points sometimes called \"gravitational wells\" (at 75.3\u00b0E and 108\u00b0W) and two corresponding unstable points (at 165.3\u00b0E and 14.7\u00b0W). Any geostationary object placed between the equilibrium points would (without any action) be slowly accelerated towards the stable equilibrium position, causing a periodic longitude variation. The correction of this effect requires station-keeping maneuvers with a maximal delta-v of about 2\u00a0m/s per year, depending on the desired longitude.\nSolar wind and radiation pressure also exert small forces on satellites: over time, these cause them to slowly drift away from their prescribed orbits.\nIn the absence of servicing missions from the Earth or a renewable propulsion method, the consumption of thruster propellant for station-keeping places a limitation on the lifetime of the satellite. Hall-effect thrusters, which are currently in use, have the potential to prolong the service life of a satellite by providing high-efficiency electric propulsion.\nDerivation.\nFor circular orbits around a body, the centripetal force required to maintain the orbit (\"F\"c) is equal to the gravitational force acting on the satellite (\"F\"g):\n formula_2\nFrom Isaac Newton's universal law of gravitation,\nformula_3,\nwhere \"F\"g is the gravitational force acting between two objects, \"M\"E is the mass of the Earth, , \"m\"s is the mass of the satellite, \"r\" is the distance between the centers of their masses, and \"G\" is the gravitational constant, .\nThe magnitude of the acceleration, \"a\", of a body moving in a circle is given by:\nformula_4\nwhere \"v\" is the magnitude of the velocity (i.e. the speed) of the satellite. From Newton's second law of motion, the centripetal force \"F\"c is given by:\nformula_5.\nAs \"F\"c = \"F\"g,\nformula_6,\nso that\nformula_7\nReplacing \"v\" with the equation for the speed of an object moving around a circle produces:\nformula_8\nwhere \"T\" is the orbital period (i.e. one sidereal day), and is equal to . This gives an equation for \"r\":\nformula_9\nThe product \"GM\"E is known with much greater precision than either factor alone; it is known as the geocentric gravitational constant \"\u03bc\" = . Hence\nformula_10\nThe resulting orbital radius is . Subtracting the Earth's equatorial radius, , gives the altitude of .\nThe orbital speed is calculated by multiplying the angular speed by the orbital radius:\n formula_11\nIn other planets.\nBy the same method, we can determine the orbital altitude for any similar pair of bodies, including the areostationary orbit of an object in relation to Mars, if it is assumed that it is spherical (which it is not entirely). The gravitational constant \"GM\" (\"\u03bc\") for Mars has the value of , its equatorial radius is and the known rotational period (\"T\") of the planet is (). Using these values, Mars' orbital altitude is equal to .\nExplanatory notes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41211", "revid": "19929991", "url": "https://en.wikipedia.org/wiki?curid=41211", "title": "Graded-index fiber", "text": "Optical fiber whose core has a varying refractive index\nA graded-index fiber, or gradient-index fiber, is an optical fiber whose core has a refractive index that decreases \"continuously\" with increasing radial distance from the optical axis of the fiber, as opposed to a step-index fiber, which has a uniform index of refraction in the core, and a lower index in the surrounding cladding.\nBecause parts of the core closer to the fiber axis have a higher refractive index than the parts near the cladding, light rays follow sinusoidal paths down the fiber. The most common refractive index profile for a graded-index fiber is very nearly parabolic. The parabolic profile results in continual refocusing of the rays in the core, and minimizes modal dispersion.\nMulti-mode optical fiber can be built with either a graded-index or a step-index profile. The advantage of graded-index multi-mode fiber compared to step-index fiber is a considerable decrease in modal dispersion. This means that the trip time of light traversing the fiber is more consistent, allowing shorter and more frequent pulses of light to be discerned by the receiver. Modal dispersion can be further decreased by selecting a smaller core size (less than 10\u00a0\u03bcm) and forming a single-mode step index fiber.\nThis type of fiber is normalized by the International Telecommunication Union ITU-T in recommendation G.651.1.\nPulse dispersion.\nPulse dispersion in a graded-index optical fiber is given by\nformula_1\nwhere\nformula_2 is the difference in refractive indices of core and cladding,\nformula_3 is the refractive index of the cladding,\nformula_4 is the length of the fiber taken for observing the pulse dispersion,\nformula_5 is the speed of light, and\nformula_6 is the constant of graded index profile.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41212", "revid": "50024674", "url": "https://en.wikipedia.org/wiki?curid=41212", "title": "Grade of service", "text": "Concept in telecommunications engineering\nIn telecommunications engineering, and in particular teletraffic engineering, the quality of voice service is specified by two measures: the grade of service (GoS) and the quality of service (QoS).\nGrade of service is the probability of a call in a circuit \"group\" being blocked or delayed for more than a specified interval, expressed as a vulgar fraction or decimal fraction. This is always with reference to the busy hour when the traffic intensity is the greatest. Grade of service may be viewed independently from the perspective of incoming versus outgoing calls, and is not necessarily equal in each direction or between different source-destination pairs. \"Grade of Service\" sometimes means a measure of inbound call center traffic to verify adherence to conditions to measure the success of customers served.\nOn the other hand, the quality of service which a \"single\" circuit is designed or conditioned to provide, e.g. voice grade or program grade is called the quality of service. Quality criteria for such circuits may include equalization for amplitude over a specified band of frequencies, or in the case of digital data transported via analogue circuits, may include equalization for phase. Criteria for mobile quality of service in cellular telephone circuits include the probability of abnormal termination of the call.\nWhat is Grade of Service and how is it measured?\nWhen a user attempts to make a telephone call, the routing equipment handling the call has to determine whether to accept the call, reroute the call to alternative equipment, or reject the call entirely. Rejected calls occur as a result of heavy traffic loads (congestion) on the system and can result in the call either being delayed or lost. If a call is delayed, the user simply has to wait for the traffic to decrease, however if a call is lost then it is removed from the system.\nThe Grade of Service is one aspect of the quality a customer can expect to experience when making a telephone call. In a Loss System, the Grade of Service is described as that proportion of calls that are lost due to congestion in the busy hour. \nFor a Lost Call system, the Grade of Service can be measured using \"Equation 1\".\nformula_1\nFor a delayed call system, the Grade of Service is measured using three separate terms:\nThe Grade of Service can be measured using different sections of a network. When a call is routed from one end to another, it will pass through several exchanges. If the Grade of Service is calculated based on the number of calls rejected by the final circuit group, then the Grade of Service is determined by the final circuit group blocking criteria. If the Grade of Service is calculated based on the number of rejected calls between exchanges, then the Grade of Service is determined by the exchange-to-exchange blocking criteria.\nThe Grade of Service should be calculated using both the access networks and the core networks as it is these networks that allow a user to complete an end-to-end connection. Furthermore, the Grade of Service should be calculated from the average of the busy hour traffic intensities of the 30 busiest traffic days of the year. This will cater for most scenarios as the traffic intensity will seldom exceed the reference level.\nThe grade of service is a measure of the ability of a user to access a trunk system during the busiest hour. The busy is based upon customer demand at the busiest hour during a week month or year.\nClass of Service.\nDifferent telecommunications applications require different Qualities of Service. For example, if a telecommunications service provider decides to offer different qualities of voice connection, then a premium voice connection will require a better connection quality compared to an ordinary voice connection. Thus different Qualities of Service are appropriate, depending on the intended use. To help telecommunications service providers to market their different services, each service is placed into a specific class. Each Class of Service determines the level of service required.\nTo identify the Class of Service for a specific service, the network's switches and routers examine the call based on several factors. Such factors can include:\nQuality of Service in broadband networks.\nIn broadband networks, the Quality of Service is measured using two criteria. The first criterion is the probability of packet losses or delays in already accepted calls. The second criterion refers to the probability that a new incoming call will be rejected or blocked. To avoid the former, broadband networks limit the number of active calls so that packets from established calls will not be lost due to new calls arriving. As in circuit-switched networks, the Grade of Service can be calculated for individual switches or for the whole network.\nMaintaining a Grade of Service.\nThe telecommunications provider is usually aware of the required Grade of Service for a particular product. To achieve and maintain a given Grade of Service, the operator must ensure that sufficient telecommunications circuits or routes are available to meet a specific level of demand. It should also be kept in mind that too many circuits will create a situation where the operator is providing excess capacity which may never be used, or at the very least may be severely underutilized. This adds costs which must be borne by other parts of the network. To determine the correct number of circuits that are required, telecommunications service providers make use of Traffic Tables. An example of a Traffic Table can be viewed in \"Figure 1\". It follows that in order for a telecommunications network to continue to offer a given Grade of Service, the number of circuits provided in a circuit group must increase (non-linearly) if the traffic intensity increases.\nErlang's lost call assumptions.\nTo calculate the Grade of Service of a specified group of circuits or routes, Agner Krarup Erlang used a set of assumptions that relied on the network losing calls when all circuits in a group were busy. These assumptions are:\nFrom these assumptions Erlang developed the Erlang-B formula which describes the probability of congestion in a circuit group. The probability of congestion gives the Grade of Service experienced.\nCalculating the Grade of Service.\nTo determine the Grade of Service of a network when the traffic load and number of circuits are known, telecommunications network operators make use of \"Equation 2\", which is the Erlang-B equation.\nformula_4\n\"A\" = Expected traffic intensity in Erlangs,\n\"N\" = Number of circuits in group.\nThis equation allows operators to determine whether each of their circuit groups meet the required Grade of Service, simply by monitoring the reference traffic intensity."}
{"id": "41213", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=41213", "title": "Grandfathered system", "text": ""}
{"id": "41214", "revid": "38516105", "url": "https://en.wikipedia.org/wiki?curid=41214", "title": "Graphic character", "text": "Encoded character that is associated with one or more glyphs\nA graphic character, also known as a printing character or a printable character, is a character intended to be rendered in a form that can be read by a human. In other words, it is any encoded character that is associated with one or more glyphs.\nISO/IEC 646.\nIn ASCII (specified in ISO/IEC 646), graphic characters are contained in rows 2 through 7 of the code table. However, two of the characters in these rows, namely the space character SP at row 2 column 0 and the delete character\u00a0DEL (also called the rubout character) at row 7 column 15, require special mention.\nThe space is considered to be \"both\" a graphic character and a control character in ISO 646. It can be considered as a character with a visible form or, in contexts such as teleprinters, a control character that advances the print head without printing a character.\nThe delete character is strictly a control character, not a graphic character. This is true not only in ISO 646, but also in all related standards including Unicode. However, many other character sets deviate from ISO 646, and as a result a graphic character might occupy the position originally reserved for the delete character.\nUnicode.\nIn Unicode, Graphic characters are those with General Category Letter, Mark, Number, Punctuation, Symbol or Zs=space. Other code points (General categories Control, Zl=line separator, Zp=paragraph separator) are Format, Control, Private Use, Surrogate, Noncharacter or Reserved (unassigned).\nSpacing and non-spacing characters.\nMost graphic characters are spacing characters, which means that each instance of a spacing character has to occupy some area in a graphic representation. For a teletype or a typewriter this implies moving of the carriage after typing of a character. In the context of text mode display, each spacing character occupies one rectangular character box of equal sizes. Or maybe two adjacent ones, for non-alphabetic characters of East Asian languages. If a text is rendered using proportional fonts, widths of character boxes are not equal, but are positive.\nThere exist also \"non-spacing\" graphic characters. Most of non-spacing characters are \"modifiers\", also called combining characters in Unicode, such as diacritical marks. Although non-spacing graphic characters are uncommon in traditional code pages, there are many such in Unicode. A combining character has its distinct glyph, but it applies to a character box of another character, a spacing one. In some historical systems such as line printers this was implemented as overstrike.\nNote that not all modifiers are non-spacing\u00a0\u2013 there exists Spacing Modifier Letters Unicode block.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41215", "revid": "46512407", "url": "https://en.wikipedia.org/wiki?curid=41215", "title": "Ground (electricity)", "text": "Reference point in an electrical circuit from which voltages are measured\nIn electrical engineering, ground or earth may refer to reference ground \u2013 a reference point in an electrical circuit from which voltages are measured, earth ground \u2013 a direct connection to the physical ground, or common ground \u2013 a common return path for electric current. Common ground is almost identical to neutral \u2013 a return path for electric current, with an added requirement that common ground has to be a \"common\" return path. To ground or to earth an object is to electrically connect the object to earth ground or common ground.\nEarth wire, or ground wire, is a wire that connects an electrical equipment from its conductive but normally-unenergized parts to earth ground or common ground.\nElectrical circuits may be connected to ground for several reasons. Exposed conductive parts of electrical equipment are connected to ground to protect users from electrical shock hazards. If internal insulation fails, dangerous voltages may appear on the exposed conductive parts. Connecting exposed conductive parts to a \"ground\" wire which provides a low-impedance path for current to flow back to the incoming neutral (which is also connected to ground, close to the point of entry) will allow circuit breakers (or RCDs) to interrupt power supply in the event of a fault. In electric power distribution systems, a protective earth (PE) conductor is an essential part of the safety provided by the earthing system.\nConnection to ground also limits the build-up of static electricity when handling flammable products or electrostatic-sensitive devices. In some telegraph and power transmission circuits, the ground itself can be used as one conductor of the circuit, saving the cost of installing a separate return conductor (see single-wire earth return and earth-return telegraph).\nFor measurement purposes, the Earth serves as a (reasonably) constant potential reference against which other potentials can be measured. An electrical ground system should have an appropriate current-carrying capability to serve as an adequate zero-voltage reference level. In electronic circuit theory, a \"ground\" is usually idealized as an infinite source or sink for charge, which can absorb an unlimited amount of current without changing its potential. Where a real ground connection has a significant resistance, the approximation of zero potential is no longer valid. Stray voltages or earth potential rise effects will occur, which may create noise in signals or produce an electric shock hazard if large enough.\nThe use of the term ground (or earth) is so common in electrical and electronics applications that circuits in portable electronic devices, such as cell phones and media players, as well as circuits in vehicles, may be spoken of as having a \"ground\" or chassis ground connection without any actual connection to the Earth, despite \"common\" being a more appropriate term for such a connection. That is usually a large conductor attached to one side of the power supply (such as the \"ground plane\" on a printed circuit board), which serves as the common return path for current from many different components in the circuit.\nHistory.\nLong-distance electromagnetic telegraph systems from 1820 onwards\nused two or more wires to carry the signal and return currents. It was discovered by German scientist C.A. von Steinheil in 1836\u20131837, that the ground could be used as the return path to complete the circuit, making the return wire unnecessary.\nSteinheil was not the first to do this, but he was not aware of earlier experimental work, and he was the first to do it on an in-service telegraph, thus making the principle known to telegraph engineers generally. However, there were problems with this system, exemplified by the transcontinental telegraph line constructed in 1861 by the Western Union Company between St. Joseph, Missouri, and Sacramento, California. During dry weather, the ground connection often developed a high resistance, requiring water to be poured on the ground rod to enable the telegraph to work or phones to ring.\nIn the late nineteenth century, when telephony began to replace telegraphy, it was found that the currents in the earth induced by power systems, electric railways, other telephone and telegraph circuits, and natural sources including lightning caused unacceptable interference to the audio signals, and the two-wire or 'metallic circuit' system was reintroduced around 1883.\nBuilding wiring installations.\nElectrical power distribution systems are often connected to earth ground to limit the voltage that can appear on distribution circuits. A distribution system insulated from earth ground may attain a high potential due to transient voltages caused by static electricity or accidental contact with higher potential circuits. An earth ground connection of the system dissipates such potentials and limits the rise in voltage of the grounded system.\nIn a mains electricity (AC power) wiring installation, the term \"ground conductor\" typically refers to two different conductors or conductor systems as listed below:\n\"Equipment bonding conductors\" or \"equipment ground conductors\" (EGC) provide a low-impedance path between normally non-current-carrying metallic parts of equipment and one of the conductors of that electrical system's source. If any exposed metal part should become energized (fault), such as by a frayed or damaged insulator, it creates a short circuit, causing the overcurrent device (circuit breaker or fuse) to open, clearing (disconnecting) the fault. It is important to note this action occurs regardless of whether there is a connection to the physical ground; the physical ground itself has no role in this fault-clearing process since current must return to its source; however, the sources are very frequently connected to the physical ground. (see Kirchhoff's circuit laws). By bonding (interconnecting) all exposed non-current carrying metal objects together, as well as to other metallic objects such as pipes or structural steel, they should remain near the same voltage potential, thus reducing the chance of a shock. This is especially important in bathrooms where one may be in contact with several different metallic systems such as supply and drain pipes and appliance frames. When a conductive system is to be electrically connected to the physical ground, one puts the equipment bonding conductor and the grounding electrode conductor at the same potential (for example, see \"\u00a7Metal water pipe as grounding electrode\" below).\nA &lt;templatestyles src=\"Template:Visible anchor/styles.css\" /&gt;grounding electrode conductor (GEC) is used to connect the system grounded (\"neutral\") conductor, or the equipment to a grounding electrode, or a point on the grounding electrode system. This is called \"system grounding\" and most electrical systems are required to be grounded. The U.S. NEC and the UK's BS 7671 list systems that are required to be grounded. According to the NEC, the purpose of connecting an electrical system to the physical ground is to limit the voltage imposed by lightning events and contact with higher voltage lines. In the past, water supply pipes were used as grounding electrodes, but due to the increased use of plastic pipes, which are poor conductors, the use of a specific grounding electrode is often mandated by regulating authorities. The same type of ground applies to radio antennas and to lightning protection systems.\nPermanently installed electrical equipment, unless not required to, has permanently connected grounding conductors. Portable electrical devices with metal cases may have them connected to earth ground by a pin on the attachment plug (see AC power plugs and sockets). The size of power grounding conductors is usually regulated by local or national wiring regulations.\nBonding.\nStrictly speaking, the terms \"grounding\" or \"earthing\" are meant to refer to an electrical connection to ground. Bonding is the practice of intentionally electrically connecting metallic items not designed to carry electricity. This brings all the bonded items to the same electrical potential as a protection from electrical shock. The bonded items can then be connected to ground to eliminate foreign voltages.\nEarthing systems.\nIn electricity supply systems, an \"earthing system\" or \"grounding system\" defines the electrical potential of the conductors relative to that of the Earth's conductive surface. The choice of earthing system has implications for the safety and electromagnetic compatibility of the power supply. Regulations for earthing systems vary considerably between different countries.\nA functional earth connection serves more than protecting against electrical shock, as such a connection may carry current during the normal operation of a device. Such devices include surge suppression, electromagnetic-compatibility filters, some types of antennas, and various measurement instruments. Generally the protective earth system is also used as a functional earth, though this requires care.\nImpedance grounding.\nDistribution power systems may be solidly grounded, with one circuit conductor directly connected to an earth grounding electrode system. Alternatively, some amount of electrical impedance may be connected between the distribution system and ground, to limit the current that can flow to earth. The impedance may be a resistor, or an inductor (coil). In a high-impedance grounded system, the fault current is limited to a few amperes (exact values depend on the voltage class of the system); a low-impedance grounded system will permit several hundred amperes to flow on a fault. A large solidly grounded distribution system may have tens of thousands of amperes of ground fault current.\nIn a polyphase AC system, the instantaneous vector sum of the phases is zero. This neutral point is commonly used to refer the phase voltages to earth ground instead of connecting one of the phase conductors to earth. Any \u0394-Y (delta-wye) connected transformer may be used for the purpose. A nine winding transformer (a \"zig zag\" transformer) may be used to balance the phase currents of a delta connected source with an unbalanced load.\nLow-resistance grounding systems use a neutral grounding resistor (NGR) to limit the fault current to 25\u00a0A or greater. Low resistance grounding systems will have a time rating (say, 10 seconds) that indicates how long the resistor can carry the fault current before overheating. A ground fault protection relay must trip the breaker to protect the circuit before overheating of the resistor occurs.\nHigh-resistance grounding (HRG) systems use an NGR to limit the fault current to 25\u00a0A or less. They have a continuous rating, and are designed to operate with a single-ground fault. This means that the system will not immediately trip on the first ground fault. If a second ground fault occurs, a ground fault protection relay must trip the breaker to protect the circuit. On an HRG system, a sensing resistor is used to continuously monitor system continuity. If an open-circuit is detected (e.g., due to a broken weld on the NGR), the monitoring device will sense voltage through the sensing resistor and trip the breaker. Without a sensing resistor, the system could continue to operate without ground protection (since an open circuit condition would mask the ground fault) and transient overvoltages could occur.\nUngrounded systems.\nWhere the danger of electric shock is high, special ungrounded power systems may be used to minimize possible leakage current to ground. Examples of such installations include patient care areas in hospitals, where medical equipment is directly connected to a patient and must not permit any power-line current to pass into the patient's body. Medical systems include monitoring devices to warn of any increase of leakage current. On wet construction sites or in shipyards, isolation transformers may be provided so that a fault in a power tool or its cable does not expose users to shock hazard.\nCircuits used to feed sensitive audio/video production equipment or measurement instruments may be fed from an isolated ungrounded technical power system to limit the injection of noise from the power system.\nPower transmission.\nIn single-wire earth return (SWER) AC electrical distribution systems, costs are saved by using just a single high voltage conductor for the power grid, while routing the AC return current through the earth. This system is mostly used in rural areas where large earth currents will not otherwise cause hazards.\nSome high-voltage direct-current (HVDC) power transmission systems use the ground as second conductor. This is especially common in schemes with submarine cables, as sea water is a good conductor. Buried grounding electrodes are used to make the connection to the earth. The site of these electrodes must be chosen carefully to prevent electrochemical corrosion on underground structures.\nA particular concern in design of electrical substations is earth potential rise. When very large fault currents are injected into the earth, the area around the point of injection may rise to a high potential with respect to points distant from it. This is due to the limited finite conductivity of the layers of soil in the earth of the substation. The gradient of the voltage (the change in voltage across the distance to the injection point) may be so high that two points on the ground may be at significantly different potentials. This gradient creates a hazard to anyone standing on the earth in an area of the electrical substation that is insufficiently insulated from ground. Pipes, rails, or communication wires entering a substation may see different ground potentials inside and outside the substation, creating a dangerous touch voltage for unsuspecting persons who might touch those pipes, rails, or wires. This problem is alleviated by creating a low-impedance equipotential bonding plane installed in accordance with IEEE 80, within the substation. This plane eliminates voltage gradients and ensures that any fault is cleared within three voltage cycles.\nElectronics.\nGround symbols\nSignal grounds serve as return paths for signals and power (at extra-low voltages, less than about 50 V) within equipment, and on the signal interconnections between equipment. Many electronic designs feature a single return that acts as a reference for all signals. Power and signal grounds often get connected, usually through the metal case of the equipment. Designers of printed circuit boards must take care in the layout of electronic systems so that high-power or rapidly switching currents in one part of a system do not inject noise into low-level sensitive parts of a system due to some common impedance in the grounding traces of the layout.\nCircuit ground versus earth.\nVoltage is defined as the difference of electric potentials between points in an electric field. A voltmeter is used to measure the potential difference between some point and a convenient, but otherwise arbitrary reference point. This common reference point is denoted \"ground\" and is designated as having a nominal zero potential. Signals are defined with respect to signal ground, which may be connected to a power ground. A system where the system ground is not connected to another circuit or to earth (in which there may still be AC coupling between those circuits) is often referred to as a floating ground, and may correspond to Class 0 or Class II appliances.\nFunctional grounds.\nSome devices require a connection to the mass of earth to function correctly, as distinct from any purely protective role. Such a connection is known as a functional earth\u00a0\u2013 for example some long wavelength antenna structures require a functional earth connection, which generally should not be indiscriminately connected to the supply protective earth, as the introduction of transmitted radio frequencies into the electrical distribution network is both illegal and potentially dangerous. Because of this separation, a purely functional ground should not normally be relied upon to perform a protective function.\nTo avoid accidents, such functional grounds are normally wired in white, cream or pink cable, and not green or green/yellow.\nSeparating low signal ground from a noisy ground.\nIn television stations, recording studios, and other installations where signal quality is critical, a special signal ground known as a \"technical ground\" (or \"technical earth\", \"special earth\", and \"audio earth\") is often installed, to prevent ground loops. This is basically the same thing as an AC power ground, but no general appliance ground wires are allowed any connection to it, as they may carry electrical interference. For example, only audio equipment is connected to the technical ground in a recording studio. In most cases, the studio's metal equipment racks are all joined with heavy copper cables (or flattened copper tubing or busbars) and similar connections are made to the technical ground. Great care is taken that no general chassis grounded appliances are placed on the racks, as a single AC ground connection to the technical ground will destroy its effectiveness. For particularly demanding applications, the main technical ground may consist of a heavy copper pipe, if necessary fitted by drilling through several concrete floors, such that all technical grounds may be connected by the shortest possible path to a grounding rod in the basement.\nRadio frequency ground.\nCertain types of radio antennas (or their feedlines) require a connection to ground that functions adequately at radio frequencies. The required caliber of grounding system is called a \"radio frequency ground\". In general, a radio transmitter, its power source, and its antenna will require three functionally different grounds:\nAlthough some of these grounds might be combined, and should be connected at exactly one point, only the last type of ground is covered in this section. Lightning safety grounding (1) is covered in the following section, not here. The electrical safety ground (2) was discussed in previous sections and is unsuitable for radio purposes, although required for the power supply. The radio frequency ground (3) is the topic of this section.\nSince the radio frequencies of the current in antennas are far higher than the 50 or 60\u00a0Hz frequency of the power line, radio grounding systems use different principles than AC power grounding. The \"protective earth\" (PE) safety ground wires in AC utility building wiring were not designed for, and cannot be used as an adequate substitute for an RF ground. The long utility ground wires have high impedance at certain frequencies. In the case of a transmitter, the RF current flowing through the ground wires can radiate radio frequency interference and induce hazardous voltages on grounded metal parts of other appliances, so separate ground systems are used.\nMonopole antennas operating at lower frequencies, below 20\u00a0MHz, use the surface of the Earth as a part of the antenna, as a conductive plane to reflect the radio waves and provide a return path for electric fields extending from the antenna. The monopoles include the mast radiator used by AM radio stations, and the 'T' and inverted\u00a0'L' antenna, and umbrella antenna. The feedline from the transmitter is connected between the antenna and ground, so it requires a grounding (earthing) system under the antenna to make contact with the soil to collect the return current. The ground system also functions as a capacitor plate, to receive the displacement current from the antenna and return it to the ground side of the transmitter's feedline, so it is preferably located directly under the antenna. In receivers and low efficiency / low power transmitters, the ground connection can be as simple as one or several metal rods or stakes driven into the soil, or an electrical connection to a building's metal water piping which extends into the earth. However, in transmitting antennas the ground system carries the full output current of the transmitter, so the resistance of an inadequate ground contact can be a major loss of transmitter power.\nMedium to high power transmitters usually have an extensive ground system consisting of bare copper cables buried in the earth under the antenna, to lower resistance.\nSince for the omnidirectional antennas used on these bands the Earth currents travel radially toward the ground point from all directions, the grounding system usually consists of a radial pattern of buried cables extending outward under the antenna in all directions, connected together to the ground side of the transmitter's feedline at a terminal next to the base of the antenna called a radial ground system.\nThe transmitter power lost in the ground resistance, and so the efficiency of the antenna, depends on the soil conductivity. This varies widely; marshy ground or ponds, particularly salt water, provide the lowest resistance ground, while dry rocky or sandy soil are the highest. The power loss per square meter in the ground is proportional to the square of the transmitter current density flowing in the earth. The current density, and power dissipated, increases the closer one gets to the ground terminal at the base of the antenna, so the radial ground system can be thought of as providing a higher conductivity medium, copper, for the ground current to flow through, in the parts of the ground carrying high current density, to reduce power losses.\nDesign.\nA standard ground system widely used for mast radiator broadcasting antennas operating in the MF and LF bands consists of 120\u00a0equally-spaced, buried, radial ground wires extending out one quarter of a wavelength (formula_1, or 90\u00a0electrical degrees) from the antenna base.\nAWG\u00a08 to AWG\u00a010 soft-drawn copper wire is typically used, buried 4\u201310\u00a0inches deep. For AM broadcast band antennas this requires a circular land area extending from the mast . This is usually planted with grass, which is kept mowed short, as tall grass can increase power loss in certain circumstances. If the land area available is too limited for such long radials, they can in many cases be adequately replaced by a greater number of shorter radials, or a smaller number of longer radials.\nIn transmitting antennas a second cause of power wastage is dielectric power losses of the electric field (displacement current) of the antenna passing through the earth to reach the ground wires. For antennas near a half-wavelength high (180\u00a0electrical degrees) the antenna has a voltage maximum (antinode) near its base, which results in strong electric fields in the earth above the ground wires near the mast where the displacement current enters the ground. To reduce this loss these antennas often use a conductive copper ground screen under the antenna connected to the buried ground wires, either lying on the ground or elevated a few feet, to shield the ground from the electric field.\nIn a few cases where rocky or sandy soil has too high a resistance for a buried ground, a counterpoise is used. This is a radial network of wires similar to that in a buried ground system, but lying on the surface or suspended a few feet above the ground. It acts as a capacitor plate, capacitively coupling the feedline to conductive layers of the soil.\nElectrically short antennas.\nAt lower frequencies the resistance of the ground system is a more critical factor because of the small radiation resistance of the antenna. In the LF and VLF bands, construction height limitations require that electrically short antennas be used, shorter than the fundamental resonant length of one quarter of a wavelength (formula_1). A quarter wave monopole has a radiation resistance of around 25~36\u00a0ohms, but below formula_1 the resistance decreases with the square of the ratio of height to wavelength. The power fed to an antenna is split between the radiation resistance, which represents power emitted as radio waves, the desired function of the antenna, and the ohmic resistance of the ground system, which results in power wasted as heat. As the wavelength gets longer in relation to antenna height, the radiation resistance of the antenna decreases so the ground resistance constitutes a larger proportion of the input resistance of the antenna and consumes more of the transmitter power. Antennas in the VLF band often have a resistance of less than 1\u00a0ohm, and even with extremely low resistance ground systems 50% to 90% of the transmitter power may be wasted in the ground system.\nLightning protection systems.\nLightning protection systems are designed to mitigate the effects of lightning through connection to extensive grounding systems that provide a large surface area connection to earth. The large area is required to dissipate the high current of a lightning strike without damaging the system conductors by excess heat. Since lightning strikes are pulses of energy with very high frequency components, grounding systems for lightning protection tend to use short straight runs of conductors to reduce the self-inductance and skin effect.\nGround mat.\nIn an electrical substation a \"ground mat\", \"grounding mat\", \"earth mat\" or \"earthing mat\" is a mesh of conductive material installed at places where a person would stand to operate a switch or other apparatus; it is bonded to the local supporting metal structure and to the handle of the switchgear, so that the operator will not be exposed to a high differential voltage due to a fault in the substation.\nIn the vicinity of electrostatic sensitive devices, a ground mat is used to ground static electricity generated by people and moving equipment. There are two types used in static control: Static Dissipative Mats, and Conductive Mats.\nA static dissipative mat that rests on a conductive surface (commonly the case in military facilities) are typically made of 3 layers (3-ply) with static dissipative vinyl layers surrounding a conductive substrate which is electrically attached to ground. For commercial uses, static dissipative rubber mats are traditionally used that are made of 2 layers (2-ply) with a tough solder resistant top static dissipative layer that makes them last longer than the vinyl mats, and a conductive rubber bottom. Conductive mats are made of carbon and used only on floors for the purpose of drawing static electricity to ground as quickly as possible. Normally conductive mats are made with cushioning for standing and are referred to as \"anti-fatigue\" mats.\nFor a static dissipative mat to be reliably grounded it must be attached to a path to ground. Normally, both the mat and the wrist strap are connected to ground by using a common point ground system (CPGS).\nIn computer repair shops and electronics manufacturing, workers must be grounded before working on devices sensitive to voltages capable of being generated by humans. For that reason static dissipative mats can be and are also used on production assembly floors as \"floor runner\" along the assembly line to draw static generated by people walking up and down.\nIsolation.\nIsolation is a mechanism that defeats grounding. It is frequently used with low-power consumer devices, and when engineers, hobbyists, or repairmen are working on circuits that would normally be operated using the power line voltage. Isolation can be accomplished by simply placing a \"1:1 wire ratio\" transformer with an equal number of turns between the device and the regular power service, but applies to any type of transformer using two or more coils electrically insulated from each other.\nFor an isolated device, touching a single powered conductor does not cause a severe shock, because there is no path back to the other conductor through the ground. However, shocks and electrocution may still occur if both poles of the transformer are contacted by bare skin. Previously it was suggested that repairmen \"work with one hand behind their back\" to avoid touching two parts of the device under test at the same time, thereby preventing a current from crossing through the chest and interrupting cardiac rhythms or causing cardiac arrest.\nGenerally every AC power line transformer acts as an isolation transformer, and every step up or down has the potential to form an isolated circuit. However, this isolation would prevent failed devices from blowing fuses when shorted to their ground conductor. The isolation that could be created by each transformer is defeated by always having one leg of the transformers grounded, on both sides of the input and output transformer coils. Power lines also typically ground one specific wire at every pole, to ensure current equalization from pole to pole if a short to ground is occurring.\nIn the past, grounded appliances have been designed with internal isolation to a degree that allowed the simple disconnection of ground by cheater plugs without apparent problem (a dangerous practice, since the safety of the resulting floating equipment relies on the insulation in its power transformer). Modern appliances however often include power entry modules which are designed with deliberate capacitive coupling between the AC power lines and chassis, to suppress electromagnetic interference. This results in a significant leakage current from the power lines to ground. If the ground is disconnected by a cheater plug or by accident, the resulting leakage current can cause mild shocks, even without any fault in the equipment. Even small leakage currents are a significant concern in medical settings, as the accidental disconnection of ground can introduce these currents into sensitive parts of the human body. As a result, medical power supplies are designed to have low capacitance.\nClass II appliances and power supplies (such as cell phone chargers) do not provide any ground connection, and are designed to isolate the output from input. Safety is ensured by double-insulation, so that two failures of insulation are required to cause a shock.\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41216", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41216", "title": "Ground constants", "text": "In telecommunications, ground constants are the electrical parameters of earth: electrical conductivity, \u03c3, electrical permittivity, \u03b5, and magnetic permeability, \u03bc.\nThe values of these parameters vary with the local chemical composition and density of the Earth. For a propagating electromagnetic wave, such as a surface wave propagating along the surface of the Earth, these parameters vary with frequency and direction. "}
{"id": "41217", "revid": "22883165", "url": "https://en.wikipedia.org/wiki?curid=41217", "title": "Ground loop", "text": "Ground loop may refer to:\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "41218", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41218", "title": "Ground plane", "text": "Electrically conductive surface, usually connected to electrical ground\nIn electrical engineering, a ground plane is an electrically conductive surface, usually connected to electrical ground. Ground planes are typically made of copper or aluminum, and they are often located on the bottom of printed circuit boards (PCBs).\nThe term has two different meanings in separate areas of electrical engineering.\nRadio antenna theory.\nIn Telecommunications, a ground plane is a flat or nearly flat horizontal conducting surface that serves as part of an antenna, to reflect the radio waves from the other antenna elements. In monopole antennas the ground plane is connected to one side of the antenna feedline, usually the shield conductor of a coaxial cable, and the other side is connected to the monopole element itself. Ground plane shape and size play major roles in determining its radiation characteristics including gain.\nTo function as a ground plane, the conducting surface must be at least a quarter of the wavelength of the radio waves in radius. In lower frequency antennas, such as the mast radiators used for broadcast antennas, the Earth itself (or a body of water such as a salt marsh or ocean) is used as a ground plane. For higher frequency antennas, in the VHF or UHF range, the ground plane can be smaller, and metal disks, screens and wires are used as ground planes. At upper VHF and UHF, the metal skin of a car or aircraft can serve as a ground plane for whip antennas projecting from it. In microstrip antennas and printed monopole antennas an area of copper foil on the opposite side of a printed circuit board serves as a ground plane. The ground plane does not need to be a continuous surface. In the \"ground plane antenna\" style whip antenna, the plane consists of several wires \u00a0\u03bb long radiating from the base of a quarter-wave whip antenna.\nThe radio waves from an antenna element that reflect off a ground plane appear to come from a mirror image of the antenna located on the other side of the ground plane. In a monopole antenna, the radiation pattern of the monopole plus the virtual image antenna make it appear as a two-element center-fed dipole antenna. So a monopole mounted over an ideal ground plane has a radiation pattern identical to a dipole antenna. The feedline from the transmitter or receiver is connected between the bottom end of the monopole element and the ground plane. The ground plane must have good conductivity; any resistance in the ground plane is in series with the antenna and serves to dissipate power from the transmitter.\nAntennas usually need ground planes as they serves as a stabilizing factor for the signal, ensuring that antennas have a consistent baseline from which to transmit and receive information. It also helps create a specific radiation pattern by reflecting radio waves, ensuring that the antenna radiates efficiently. There are also some cases that ground planes can improve the overall performance of the antenna by ensuring better signal propagation.\nPrinted circuit boards.\nA \"ground plane\" on a printed circuit board (PCB) is a large area or layer of copper foil connected to the circuit's ground point, usually one terminal of the power supply. It serves as the return path for current from many different components.\nA ground plane is often made as large as possible, covering most of the area of the PCB which is not occupied by circuit traces. In multilayer PCBs, it is often a separate layer covering the entire board. This serves to make circuit layout easier, allowing the designer to ground any component without having to run additional traces; component leads needing grounding are routed directly through a hole in the board to the ground plane on another layer. The large area of copper also conducts the large return currents from many components without significant voltage drops, ensuring that the ground connection of all the components are at the same reference potential.\nIn digital and radio frequency PCBs, the major reason for using large ground planes is to reduce electrical noise and interference through ground loops and to prevent crosstalk between adjacent circuit traces. When digital circuits switch state, large current pulses flow from the active devices (transistors or integrated circuits) through the ground circuit. If the power supply and ground traces have significant impedance, the voltage drop across them may create noise voltage pulses that disturb other parts of the circuit (ground bounce). The large conducting area of the ground plane has much lower impedance than a circuit trace, so the current pulses cause less disturbance.\nIn addition, a ground plane under printed circuit traces can reduce crosstalk between adjacent traces. When two traces run parallel, an electrical signal in one can be coupled into the other through electromagnetic induction by magnetic field lines from one linking the other; this is called crosstalk. When a ground plane layer is present underneath, it forms a transmission line with the trace. The oppositely-directed return currents flow through the ground plane directly beneath the trace. This confines most of the electromagnetic fields to the area near the trace and consequently reduces crosstalk.\nA power plane is often used in addition to a ground plane in a multilayer circuit board, to distribute DC power to the active devices. The two facing areas of copper create a large parallel plate decoupling capacitor that prevents noise from being coupled from one circuit to another through the power supply.\nGround planes are sometimes split and then connected by a thin trace. This allows the separation of analog and digital sections of a board or the inputs and outputs of amplifiers. The thin trace has low enough impedance to keep the two sides very close to the same potential while keeping the ground currents of one side from coupling into the other side, causing ground loop.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41219", "revid": "16364713", "url": "https://en.wikipedia.org/wiki?curid=41219", "title": "Ground wave", "text": "Radio wave propagating along the Earth's surface\nGround wave is a mode of radio propagation that consists of currents traveling through the earth. Ground waves propagate parallel to and adjacent to the surface of the Earth, and are capable of covering long distances by diffracting around the Earth's curvature. This radiation is also known as the Norton surface wave, or more properly the Norton ground wave, because ground waves in radio propagation are not confined to the surface. Groundwave contrasts with line-of-sight propagation that requires no medium, and skywave via the ionosphere. \nGround wave is important for radio signals below 30 MHz, but is generally insignificant at higher frequencies where line-of-sight propagation dominates. AM and longwave broadcasting, navigation systems such as LORAN, low-frequency time signals, non-directional beacons, and short-range HF communications all make use of it. Range depends on frequency and ground conductivity, with lower frequencies and higher ground conductivity permitting longer distances. \nOverview.\nLower frequency radio waves, below 3\u00a0MHz, travel efficiently as ground waves. As losses increase with frequency, high frequency transmissions between 3 and 30\u00a0MHz have more modest groundwave range and groundwave is unimportant above 30\u00a0MHz. Surface conductivity affects the propagation of ground waves, with highly conductive surfaces such as sea water providing the best propagation, and dry ground and ice performing the worst.\nAs the distance increases, ground waves spread out according to the inverse-square law. The imperfect conductivity of the ground tilts the waves forward, dissipating energy into the ground. The long wavelengths of these signals allow them to diffract over the horizon, but this leads to further losses. Signal strength tends to fall exponentially with distance once the Earth's curvature is significant. Above about 10 kHz, atmospheric refraction helps bend waves downward. Only vertically polarized waves travel well; horizontally polarized signals are heavily attenuated.\nGroundwave signals are relatively immune to fading but changes in the ground can cause variation in signal strength. Attenuation over land is lowest in the winter in temperate climates and higher over water when seas are rough. Hills, mountains, urban areas, and forests can create areas of reduced signal strength. The penetration depth of ground waves varies, reaching tens of meters at medium frequencies over dry ground and even more at lower frequencies. Propagation predictions thus require knowing the electrical properties of subsurface layers, which are best measured from groundwave attenuation.\nApplications.\nMost low-frequency radio communication is via groundwave propagation. Groundwave is also the primary mode for medium frequencies during the day when skywave is absent, and can be useful at high frequencies at short ranges. Uses include navigation signals, low-frequency time signals, longwave radio, and AM radio. The increased effectiveness of groundwave at lower frequencies gives AM radio stations more coverage at the low end of the band. High frequency over-the-horizon radar may use groundwave at moderate ranges but skywave at longer distances. Military communications in the very low and low frequency range uses ground wave, especially to reach ships and submarines, as groundwaves at these long wavelengths penetrate well below the sea surface.\nIn the development of radio, ground waves were used extensively. Early commercial and professional radio services relied exclusively on long wave, low frequencies and ground-wave propagation. To prevent interference with these services, amateur and experimental transmitters were restricted to the high frequencies (HF), felt to be useless since their ground-wave range was limited. Upon discovery of the other propagation modes possible at medium wave and short wave frequencies, the advantages of HF for commercial and military purposes became apparent. Amateur experimentation was then confined to only authorized frequencies in the range.\nModeling.\nIn the 1930s, Alfred Norton was the first author to accurately describe groundwave mathematically, deriving an equation for field strength over a flat earth. Van der Pol and Bremmer published calculations for a spherical Earth from 1937 to 1939. Later work focused on paths with variable conductivity, the effects of terrain and objects on the ground, and computer modeling.\nRelated terms.\nMediumwave and shortwave reflect off the ionosphere at night, which is known as skywave. During daylight hours, the lower D layer of the ionosphere forms and absorbs lower frequency energy. This prevents skywave propagation from being very effective on mediumwave frequencies in daylight hours. At night, when the D layer dissipates, mediumwave transmissions travel better by skywave. Ground waves \"do not\" include ionospheric and tropospheric waves\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41220", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41220", "title": "Group alerting and dispatching system", "text": "In telecommunications, a group alerting and dispatching system is a service feature that (a) enables a controlling telephone to place a call to a specified number of telephones simultaneously, (b) enables the call to be recorded, (c) if any of the called lines is busy, enables the equipment to camp on until the busy line is free, and (d) rings the free line and plays the recorded message.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41222", "revid": "50588183", "url": "https://en.wikipedia.org/wiki?curid=41222", "title": "Group delay and phase delay", "text": "Delays experienced through a linear time-invariant system\nIn signal processing, group delay and phase delay are functions that describe in different ways the delay times experienced by a signal\u2019s various sinusoidal frequency components as they pass through a linear time-invariant (LTI) system (such as a microphone, coaxial cable, amplifier, loudspeaker, communications system, ethernet cable, digital filter, or analog filter).\nThese delays are sometimes frequency dependent, which means that different sinusoid frequency components experience different time delays. As a result, the signal's waveform experiences distortion as it passes through the system. This distortion can cause problems such as poor fidelity in analog video and analog audio, or a high bit-error rate in a digital bit stream.\nBackground.\nFrequency components of a signal.\nFourier analysis reveals how signals in time can alternatively be expressed as the sum of sinusoidal frequency components, each based on the trigonometric function formula_1 with a fixed amplitude and phase and no beginning and no end.\nLinear time-invariant systems process each sinusoidal component independently; the property of linearity means they satisfy the superposition principle.\nIntroduction.\nThe group delay and phase delay properties of a linear time-invariant (LTI) system are functions of frequency, giving the time from when a frequency component of a time varying physical quantity\u2014for example a voltage signal\u2014appears at the LTI system input, to the time when a copy of that same frequency component\u2014perhaps of a different physical phenomenon\u2014appears at the LTI system output.\nA varying phase response as a function of frequency, from which group delay and phase delay can be calculated, typically occurs in devices such as microphones, amplifiers, loudspeakers, magnetic recorders, headphones, coaxial cables, and antialiasing filters. All frequency components of a signal are delayed when passed through such devices, or when propagating through space or a medium, such as air or water.\nWhile a phase response describes phase shift in angular units (such as degrees or radians), the phase delay is in units of time and equals the negative of the phase shift at each frequency divided by the value of that frequency. Group delay is the negative derivative of phase shift with respect to frequency.\nPhase delay.\nA linear time-invariant system or device has a phase response property and a phase delay property, where one can be calculated exactly from the other. Phase delay directly measures the device or system time delay of individual \"sinusoidal\" frequency components. If the phase delay function at any given frequency\u2014within a frequency range of interest\u2014has the same constant of proportionality between the phase at a selected frequency and the selected frequency itself, the system/device will have the ideal of a flat phase delay property, a.k.a. linear phase. Since phase delay is a function of frequency giving time delay, a departure from the flatness of its function graph can reveal time delay differences among the signal\u2019s various sinusoidal frequency components, in which case those differences will contribute to signal distortion, which is manifested as the output signal waveform shape being different from that of the input signal.\nThe phase delay property in general does not give useful information if the device input is a modulated signal. For that, group delay must be used.\nGroup delay.\nThe group delay is a convenient measure of the linearity of the phase with respect to frequency in a modulation system. For a modulation signal (passband signal), the information carried by the signal is carried exclusively in the wave envelope. Group delay therefore operates only with the frequency components derived from the envelope.\nBasic modulation system.\nA device's group delay can be exactly calculated from the device's phase response, \"but not the other way around\". The simplest use case for group delay is illustrated in Figure 1 which shows a conceptual modulation system, which is itself an LTI system with a baseband output that is ideally an accurate copy of the baseband signal input. This system as a whole is referred to here as the outer LTI system/device, which contains an inner (red block) LTI system/device. As is often the case for a radio system, the inner red LTI system in Fig 1 can represent two LTI systems in cascade, for example an amplifier driving a transmitting antenna at the sending end and the other an antenna and amplifier at the receiving end.\nAmplitude Modulation.\nAmplitude modulation creates the passband signal by shifting the baseband frequency components to a much higher frequency range. Although the frequencies are different, the passband signal carries the same information as the baseband signal. The demodulator does the inverse, shifting the passband frequencies back down to the original baseband frequency range. Ideally, the output (baseband) signal is a time delayed version of the input (baseband) signal where the waveform shape of the output is identical to that of the input.\nIn Figure 1, the outer system phase delay is the meaningful performance metric. \"For amplitude modulation, the inner red LTI device group delay becomes the outer LTI device phase delay\". If the inner red device group delay is completely flat in the frequency range of interest, the outer device will have the ideal of a phase delay that is also completely flat, where the contribution of distortion due to the outer LTI device's phase response\u2014determined entirely by the inner device's possibly different phase response\u2014is eliminated. In that case, the group delay of the inner red device and the phase delay of the outer device give the same time delay figure for the signal as a whole, from the baseband input to the baseband output. \"It is significant to note that it is possible for the inner (red) device to have a very non-flat phase delay (but flat group delay), while the outer device has the ideal of a perfectly flat phase delay.\" This is fortunate because in LTI device design, a flat group delay is easier to achieve than a flat phase delay.\nAngle Modulation.\nIn an angle-modulation system\u2014such as with frequency modulation (FM) or phase modulation (PM)\u2014the (FM or PM) passband signal applied to an LTI system input can be analyzed as two separate passband signals, an in-phase (I) amplitude modulation AM passband signal and a quadrature-phase (Q) amplitude modulation AM passband signal, where their sum exactly reconstructs the original angle-modulation (FM or PM) passband signal. While the (FM/PM) passband signal is not amplitude modulation, and therefore has no apparent outer envelope, the I and Q passband signals do indeed each have their own separate amplitude modulation envelopes. (However, unlike with regular amplitude modulation, the I and Q envelopes do not resemble the wave shape of the baseband signals, even though 100 percent of the baseband signal is represented in a complex manner by those I and Q envelopes.) So, for each of the I and Q passband signals, a flat group delay ensures that neither the I pass band envelope nor the Q passband envelope will have wave shape distortion, so when the I passband signal and the Q passband signal are added back together, the sum is the original FM/PM passband signal, which will also be unaltered.\nTheory.\nAccording to LTI system theory (used in control theory and digital or analog signal processing), the output signal formula_2 of an LTI system can be determined by convolving the time-domain impulse response formula_3 of the LTI system with the input signal formula_4. expresses this relationship as:\n formula_5\nwhere formula_6 denotes the convolution operation, formula_7 and formula_8 are the Laplace transforms of the input formula_4 and impulse response formula_3, respectively, s is the complex frequency, and formula_11 is the inverse Laplace transform. formula_8 is called the transfer function of the LTI system and, like the impulse response formula_3, \"fully\" defines the input-output characteristics of the LTI system. This convolution can be evaluated by using the integral expression in the time domain, or (according to the rightmost expression) by using multiplication in the Laplace domain and then applying the inverse transform to return to time domain.\nLTI system response to wave packet.\nSuppose that such a system is driven by a wave packet formed by a sinusoid multiplied by an amplitude envelope formula_14, so the input formula_4 can be expressed in the following form:\n formula_16\nAlso suppose that the envelope formula_17 is slowly changing relative to the sinusoid's frequency formula_18. This condition can be expressed mathematically as:\n formula_19\nApplying the earlier convolution equation would reveal that the output of such an LTI system is very well approximated as:\n formula_20\nHere formula_21 is the group delay and formula_22 is the phase delay, and they are given by the expressions below (and potentially are functions of the angular frequency formula_18). The phase of the sinusoid, as indicated by the positions of the zero crossings, is delayed in time by an amount equal to the phase delay, formula_22. The envelope of the sinusoid is delayed in time by the group delay, formula_21.\nMathematical definition of group delay and phase delay.\nThe group delay, formula_21, and phase delay, formula_22, are (potentially) frequency-dependent and can be computed from the unwrapped phase shift formula_28. The phase delay at each frequency equals the negative of the phase shift at that frequency divided by the value of that frequency:\n formula_29\nThe group delay at each frequency equals the negative of the \"slope\" (i.e. the derivative with respect to frequency) of the phase at that frequency:\n formula_30\nIn a linear phase system (with non-inverting gain), both formula_21 and formula_22 are constant (i.e., independent of formula_18) and equal, and their common value equals the overall delay of the system; and the unwrapped phase shift of the system (namely formula_34) is negative, with magnitude increasing linearly with frequency formula_18.\nLTI system response to complex sinusoid.\nMore generally, it can be shown that for an LTI system with transfer function formula_8 driven by a complex sinusoid of unit amplitude,\n formula_37\nthe output is\n formula_38\nwhere the phase shift formula_39 is\n formula_40\n1st order low- or high-pass RC filter example.\nThe phase of a 1st-order low-pass filter formed by a RC circuit with cutoff frequency formula_41 is:\nformula_42\nSimilarly, the phase for a 1st-order RC high-pass filter is:\nformula_43\nTaking the negative derivative with respect to formula_44 for either this low-pass or high-pass filter yields the same group delay of:\nformula_45\nFor frequencies significantly lower than the cutoff frequency, the phase response is approximately linear (arctan for small inputs can be approximated as a line), so the group delay simplifies to a constant value of:\nformula_46\nSimilarly, right at the cutoff frequency, formula_47\nAs frequencies get even larger, the group delay decreases with the inverse square of the frequency and approaches zero as frequency approaches infinity.\nNegative group delay.\nFilters will have \"negative\" group delay over frequency ranges where its phase response is positively-sloped. If a signal is band-limited within some maximum frequency B, then it is predictable to a small degree (within time periods smaller than &lt;templatestyles src=\"Fraction/styles.css\" /&gt;1\u2044B). A filter whose group delay is negative over that signal's entire frequency range is able to use the signal's predictability to provide an illusion of a non-causal time advance. However, if the signal contains an unpredictable event (such as an abrupt change which makes the signal's spectrum exceed its band-limit), then the illusion breaks down. Circuits with negative group delay (e.g., Figure 2) are possible, though causality is not violated.\nNegative group delay filters can be made in both digital and analog domains. Applications include compensating for the inherent delay of low-pass filters, to create \"zero phase\" filters, which can be used to quickly detect changes in the trends of sensor data or stock prices.\nGroup delay in audio.\nGroup delay has some importance in the audio field and especially in the sound reproduction field. \nThreshold of audibility.\nMany components of an audio reproduction chain, notably loudspeakers and multiway loudspeaker crossover networks, introduce group delay in the audio signal. It is therefore important to know the threshold of audibility of group delay with respect to frequency, especially if the audio chain is supposed to provide high fidelity reproduction. The best thresholds of audibility table has been provided by Blauert and Laws.\nFlanagan, Moore and Stone conclude that at 1, 2 and 4\u00a0kHz, a group delay of about 1.6\u00a0ms is audible with headphones in a non-reverberant condition. Other experimental results suggest that when the group delay in the frequency range from 300\u00a0Hz to 1\u00a0kHz is below 1.0\u00a0ms, it is inaudible.\nIt is possible to use digital signal processing techniques to correct the group delay distortion that arises due to the use of crossover networks in multi-way loudspeaker systems. This involves considerable computational modeling of loudspeaker systems in order to successfully apply delay equalization, using the Parks-McClellan FIR equiripple filter design algorithm.\nCalculating minimum amplifier bandwidths for a specified phase linearity.\nAs a criterion for calculating amplifier bandwidths for a specified phase linearity, Leach introduced the concept of \"differential time-delay distortion\", defined as: \n formula_48 ,\nwhich is the difference between the phase delay and the \"group delay,\" where the phase delay refers to the time delay of a single relatively higher frequency sinusoid, and the group delay refers to the much lower frequencies \"derived\" from the envelope of the amplitude modulation applied to that sinusoid, resulting in what is perceptually a narrow band musical note. However, exhibiting zero or negligible differential time-delay distortion is a necessary but not sufficient property of an ideal system which must have flat phase delay. That's because when two narrow band amplitude modulated notes, one much higher up in the human audio range than the other so their frequency spectrums do not overlap, the results being presented separately are perceptionally ideal, because it's not likely that a human can detect that the begin time of one note took longer from device input to device output than that of the other note. However, in the same system, if both notes are presented together at the device input, then, at the device output, one note will take longer to begin than the other, and can alter the perception of the resulting combination note if the begin times at the output are separated too much in time.\nGroup delay in optics.\nGroup delay is important in physics, and in particular in optics.\nIn an optical fiber, group delay is the transit time required for optical power, traveling at a given mode's group velocity, to travel a given distance. For optical fiber dispersion measurement purposes, the quantity of interest is group delay per unit length, which is the reciprocal of the group velocity of a particular mode. The measured group delay of a signal through an optical fiber exhibits a wavelength dependence due to the various dispersion mechanisms present in the fiber.\nIt is often desirable for the group delay to be constant across all frequencies; otherwise there is temporal smearing of the signal. Because group delay is formula_49, it therefore follows that a constant group delay can be achieved if the transfer function of the device or medium has a linear phase response (i.e., formula_50 where the group delay formula_51 is a constant). The degree of nonlinearity of the phase indicates the deviation of the group delay from a constant value.\nThe differential group delay is the difference in propagation time between the two eigenmodes \"X\" and \"Y\" polarizations. Consider two eigenmodes that are the 0\u00b0 and 90\u00b0 linear polarization states. If the state of polarization of the input signal is the linear state at 45\u00b0 between the two eigenmodes, the input signal is divided equally into the two eigenmodes. The power of the transmitted signal \"E\"\"T\",total is the combination of the transmitted signals of both \"x\" and \"y\" modes.\n formula_52\nThe differential group delay \"D\"\"t\" is defined as the difference in propagation time between the eigenmodes: \"D\"\"t\"\u00a0=\u00a0|\"t\"\"t\",\"x\"\u00a0\u2212\u00a0\"t\"\"t\",\"y\"|.\nTrue time delay.\nA transmitting apparatus is said to have \"true time delay\" (TTD) if the time delay is independent of the frequency of the electrical signal. TTD allows for a wide instantaneous signal bandwidth with virtually no signal distortion such as pulse broadening during pulsed operation.\nTTD is an important characteristic of lossless and low-loss, dispersion free, transmission lines. reveals that signals propagate through them at a speed of formula_53 for a distributed inductance L and capacitance C. Hence, any signal's propagation delay through the line simply equals the length of the line divided by this speed.\nGroup delay from transfer function polynomials.\nIf a transfer function or Sij of a scattering parameter, is in a polynomial Laplace transform form, then the mathematical definition for group delay above may be solved analytically in closed form. A polynomial transfer function formula_54 may be taken along the formula_55 axis and defined as formula_56. formula_57 may be determined from formula_56, and then the group delay may be determined by solving for formula_59.\nto determine formula_57 from formula_56, use the definition of formula_62. Given that formula_63 is always real, and formula_64 is always imaginary, formula_57 may be redefined as formula_66 where \"even\" and \"odd\" refer to the polynomials that contain only the even or odd order coefficients respectively. The formula_67 in the numerator merely converts the imaginary formula_68 numerator to a real value, since formula_68 by itself is purely imaginary.\nformula_70\nThe above expressions contain four terms to calculate:\nformula_71\nThe equations above may be used to determine the group delay of polynomial formula_54 in closed form, shown below after the equations have been reduced to a simplified form.\nformula_73\nPolynomial ratio.\nA polynomial ratio of the form formula_74, such as that typically found in the definition of filter designs, may have the group delay determined by taking advantage of the phase relation, formula_75.\nformula_76\nSimple filter example.\nA four pole Legendre filter transfer function used in the is shown below.\nformula_77\nThe numerator group delay by inspection is zero, so only the denominator group delay need be determined.\nformula_78\nEvaluating at formula_79 = 1 rad/sec:\nformula_80\nformula_81\nThe group delay calculation procedure and results may be confirmed to be correct by comparing them to the results derived from the digital derivative of the phase angle, formula_57, using a small delta formula_83 of +/-1.e-04 rad/sec.\nformula_84\nSince the group delay calculated by the digital derivative using a small delta is within 7 digits of accuracy when compared to the precise analytical calculation, the group delay calculation procedure and results are confirmed to be correct.\nDeviation from Linear Phase.\nDeviation from Linear Phase, formula_85, sometimes referred to as just, \"phase deviation\", is the difference between the phase response, formula_57, and the linear portion of the phase response formula_87, and is a useful measurement to determine the linearity of formula_57.\nA convenient means to measure formula_85 is to take the simple linear regression of formula_57 sampled over a frequency range of interest, and subtract it from the actual formula_57. The formula_85 of an ideal linear phase response would be expected to have a value of 0 across the frequency range of interest (such as the pass band of a filter), while the formula_85 of a real-world approximately linear phase response may deviate from 0 by a small finite amount across the frequency range of interest.\nAdvantage over group delay.\nAn advantage of measuring or calculating formula_85 over measuring or calculating group delay, formula_95, is formula_85 always converges to 0 as the phase becomes linear, whereas formula_95 converges on a finite quantity that may not be known ahead of time. Given this, a linear phase optimizing function may more easily be executed with a formula_98 goal than with a formula_99 goal when the value for formula_100 is not necessarily already known.\nReferences.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41223", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41223", "title": "Guided ray", "text": "A guided ray (also bound ray or trapped ray) is a ray of light in a multi-mode optical fiber, which is confined by the core.\nFor step index fiber, light entering the fiber will be guided if it falls within the acceptance cone of the fiber, that is if it makes an angle with the fiber axis that is less than the acceptance angle,\nformula_1 ,\nwhere\n\"\u03b8\" is the angle the ray makes with the fiber axis, \"before\" entering the fiber,\n\"n\"0 is the refractive index along the central axis (core) of the fiber, and\n\"n\"c is the refractive index of the cladding.\nThe quantity formula_2 is the numerical aperture of the fiber. The quantity formula_3 is sometimes called the \"total acceptance angle\" of the fiber.\nThis result can be derived from Snell's law by considering the critical angle. Light that enters the core with an angle below the acceptance angle strikes the core-cladding boundary at an angle above the critical angle, and experiences total internal reflection. This repeats on every bounce within the fiber core, and so the light is confined to the core. The confinement of light by the fiber can also be described in terms of bound modes or guided modes. This treatment is necessary when considering single-mode fiber, since the ray model does not accurately describe the propagation of light in this type of fiber.\n* http://\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41224", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41224", "title": "Hagelbarger code", "text": "Error-correcting code\nIn telecommunications, a Hagelbarger code is a convolutional code that enables error bursts to be corrected provided that there are relatively long error-free intervals between the error bursts. \nIn the Hagelbarger code, inserted parity check bits are spread out in time so that an error burst is not likely to affect more than one of the groups in which parity is checked.\nReferences.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41225", "revid": "41294494", "url": "https://en.wikipedia.org/wiki?curid=41225", "title": "Halftone characteristic", "text": "Characteristic in facsimile systems\nIn a facsimile system the halftone characteristic is either:\nReferences.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41226", "revid": "1319758292", "url": "https://en.wikipedia.org/wiki?curid=41226", "title": "Hamming code", "text": "Family of linear error-correcting codes\nIn computer science and telecommunications, Hamming codes are a family of linear error-correcting codes. Hamming codes can detect one-bit and two-bit errors, or correct one-bit errors without detection of uncorrected errors. By contrast, the simple parity code cannot correct errors, and can detect only an odd number of bits in error. Hamming codes are perfect codes, that is, they achieve the highest possible rate for codes with their block length and minimum distance of three.\nRichard W. Hamming invented Hamming codes in 1950 as a way of automatically correcting errors introduced by punched card readers. In his original paper, Hamming elaborated his general idea, but specifically focused on the Hamming(7,4) code which adds three parity bits to four bits of data.\nIn mathematical terms, Hamming codes are a class of binary linear code. For each integer \"r\" \u2265 2 there is a code-word with block length \"n\" \n 2\"r\" \u2212 1 and message length \"k\" \n 2\"r\" \u2212 \"r\" \u2212 1. Hence the rate of Hamming codes is \"R\" \n \"k\" / \"n\" \n 1 \u2212 \"r\" / (2\"r\" \u2212 1), which is the highest possible for codes with minimum distance of three (i.e., the minimal number of bit changes needed to go from any code word to any other code word is three) and block length 2\"r\" \u2212 1. The parity-check matrix of a Hamming code is constructed by listing all columns of length \"r\" that are non-zero, which means that the dual code of the Hamming code is the shortened Hadamard code, also known as a Simplex code. The parity-check matrix has the property that any two columns are pairwise linearly independent.\nDue to the limited redundancy that Hamming codes add to the data, they can only detect and correct errors when the error rate is low. This is the case in computer memory (usually RAM), where bit errors are extremely rare and Hamming codes are widely used. Memory with this correction system is known as ECC memory. In this context, an extended Hamming code having one extra parity bit is often used. Extended Hamming codes achieve a Hamming distance of four, which allows the decoder to distinguish between when at most one one-bit error occurs and when any two-bit errors occur. In this sense, extended Hamming codes are single-error correcting and double-error detecting, abbreviated as SECDED.\nHistory.\nRichard Hamming, the inventor of Hamming codes, worked at Bell Labs in the late 1940s on the Bell Model V computer, an electromechanical relay-based machine with cycle times in seconds. Input was fed in on punched paper tape, seven-eighths of an inch wide, which had up to six holes per row. During weekdays, when errors in the relays were detected, the machine would stop and flash lights so that the operators could correct the problem. During after-hours periods and on weekends, when there were no operators, the machine simply moved on to the next job.\nHamming worked on weekends, and grew increasingly frustrated with having to restart his programs from scratch due to detected errors. In a taped interview, Hamming said, \"And so I said, 'Damn it, if the machine can detect an error, why can't it locate the position of the error and correct it?'\". Over the next few years, he worked on the problem of error-correction, developing an increasingly powerful array of algorithms. In 1950, he published what is now known as Hamming code, which remains in use today in applications such as ECC memory.\nCodes predating Hamming.\nA number of simple error-detecting codes were used before Hamming codes, but none were as effective as Hamming codes in the same overhead of space.\nParity.\nParity adds a single bit that indicates whether the number of ones (bit-positions with values of one) in the preceding data was even or odd. If an odd number of bits is changed in transmission, the message will change parity and the error can be detected at this point; however, the bit that changed may have been the parity bit itself. The most common convention is that a parity value of one indicates that there is an odd number of ones in the data, and a parity value of zero indicates that there is an even number of ones. If the number of bits changed is even, the check bit will be valid and the error will not be detected.\nMoreover, parity does not indicate which bit contained the error, even when it can detect it. The data must be discarded entirely and re-transmitted from scratch. On a noisy transmission medium, a successful transmission could take a long time or may never occur. However, while the quality of parity checking is poor, since it uses only a single bit, this method results in the least overhead.\nTwo-out-of-five code.\nA two-out-of-five code is an encoding scheme which uses five bits consisting of exactly three 0s and two 1s. This provides formula_1 possible combinations, enough to represent the digits 0\u20139. This scheme can detect all single bit-errors, all odd numbered bit-errors and some even numbered bit-errors (for example the flipping of both 1-bits). However it still cannot correct any of these errors.\nRepetition.\nAnother code in use at the time repeated every data bit multiple times in order to ensure that it was sent correctly. For instance, if the data bit to be sent is a 1, an \"n\" = 3 \"repetition code\" will send 111. If the three bits received are not identical, an error occurred during transmission. If the channel is clean enough, most of the time only one bit will change in each triple. Therefore, 001, 010, and 100 each correspond to a 0 bit, while 110, 101, and 011 correspond to a 1 bit, with the greater quantity of digits that are the same ('0' or a '1') indicating what the data bit should be. A code with this ability to reconstruct the original message in the presence of errors is known as an \"error-correcting\" code. This triple repetition code is a Hamming code with \"m\" = 2, since there are two parity bits, and 22 \u2212 2 \u2212 1 = 1 data bit.\nSuch codes cannot correctly repair all errors, however. In our example, if the channel flips two bits and the receiver gets 001, the system will detect the error, but conclude that the original bit is 0, which is incorrect. If we increase the size of the bit string to four, we can detect all two-bit errors but cannot correct them (the quantity of parity bits is even); at five bits, we can both detect and correct all two-bit errors, but not all three-bit errors.\nMoreover, increasing the size of the parity bit string is inefficient, reducing throughput by three times in our original case, and the efficiency drops drastically as we increase the number of times each bit is duplicated in order to detect and correct more errors.\nDescription.\nIf more error-correcting bits are included with a message, and if those bits can be arranged such that different incorrect bits produce different error results, then bad bits could be identified. In a seven-bit message, there are seven possible single bit errors, so three error control bits could potentially specify not only that an error occurred but also which bit caused the error.\nHamming studied the existing coding schemes, including two-of-five, and generalized their concepts. To start with, he developed a nomenclature to describe the system, including the number of data bits and error-correction bits in a block. For instance, parity includes a single bit for any data word, so assuming ASCII words with seven bits, Hamming described this as an \"(8,7)\" code, with eight bits in total, of which seven are data. The repetition example would be \"(3,1)\", following the same logic. The code rate is the second number divided by the first, for our repetition example, 1/3.\nHamming also noticed the problems with flipping two or more bits, and described this as the \"distance\" (it is now called the \"Hamming distance\", after him). Parity has a distance of 2, so one bit flip can be detected but not corrected, and any two bit flips will be invisible. The (3,1) repetition has a distance of 3, as three bits need to be flipped in the same triple to obtain another code word with no visible errors. It can correct one-bit errors or it can detect - but not correct - two-bit errors. A (4,1) repetition (each bit is repeated four times) has a distance of 4, so flipping three bits can be detected, but not corrected. When three bits flip in the same group there can be situations where attempting to correct will produce the wrong code word. In general, a code with distance \"k\" can detect but not correct \"k\" \u2212 1 errors.\nHamming was interested in two problems at once: increasing the distance as much as possible, while at the same time increasing the code rate as much as possible. During the 1940s he developed several encoding schemes that were dramatic improvements on existing codes. The key to all of his systems was to have the parity bits overlap, such that they managed to check each other as well as the data.\nGeneral algorithm.\nThe following general algorithm generates a single-error correcting (SEC) code for any number of bits. The main idea is to choose the error-correcting bits such that the index-XOR (the XOR of all the bit positions containing a 1) is 0. We use positions 1, 10, 100, etc. (in binary) as the error-correcting bits, which guarantees it is possible to set the error-correcting bits so that the index-XOR of the whole message is 0. If the receiver receives a string with index-XOR 0, they can conclude there were no corruptions, and otherwise, the index-XOR indicates the index of the corrupted bit.\nAn algorithm can be deduced from the following description:\nIf a byte of data to be encoded is 10011010, then the data word (using _ to represent the parity bits) would be __1_001_1010, and the code word is 011100101010.\nThe choice of the parity, even or odd, is irrelevant but the same choice must be used for both encoding and decoding.\nThis general rule can be shown visually:\nShown are only 20 encoded bits (5 parity, 15 data) but the pattern continues indefinitely. The key thing about Hamming codes that can be seen from visual inspection is that any given bit is included in a unique set of parity bits. To check for errors, check all of the parity bits. The pattern of errors, called the error syndrome, identifies the bit in error. If all parity bits are correct, there is no error. Otherwise, the sum of the positions of the erroneous parity bits identifies the erroneous bit. For example, if the parity bits in positions 1, 2 and 8 indicate an error, then bit 1+2+8=11 is in error. If only one parity bit indicates an error, the parity bit itself is in error.\nWith m parity bits, bits from 1 up to formula_2 can be covered. After discounting the parity bits, formula_3 bits remain for use as data. As m varies, we get all the possible Hamming codes:\nHamming codes with additional parity (SECDED).\nHamming codes have a minimum distance of 3, which means that the decoder can detect and correct a single error, but it cannot distinguish a double bit error of some codeword from a single bit error of a different codeword. Thus, some double-bit errors will be incorrectly decoded as if they were single bit errors and therefore go undetected, unless no correction is attempted.\nTo remedy this shortcoming, Hamming codes can be extended by an extra parity bit. This way, it is possible to increase the minimum distance of the Hamming code to 4, which allows the decoder to distinguish between single bit errors and two-bit errors. Thus the decoder can detect and correct a single error and at the same time detect (but not correct) a double error. If the decoder does not attempt to correct errors, it can reliably detect triple bit errors. If the decoder does correct errors, some triple errors will be mistaken for single errors and \"corrected\" to the wrong value. Error correction is therefore a trade-off between certainty (the ability to reliably detect triple bit errors) and resiliency (the ability to keep functioning in the face of single bit errors).\nFor \"k\" data bits, a SECDED scheme requires:\nThis extended Hamming code was popular in computer memory systems, starting with IBM 7030 Stretch in 1961, where it is known as \"SECDED\" (or SEC-DED, abbreviated from \"single error correction, double error detection\"). Common forms for memory systems include (39,32) and (72,64). (While it is more efficient to use a codeword length in the form of 2\"m\" - 1, existing computer data word sizes being powers of 2 preclude this choice, though communication and data storage system do take advantage.) Server computers in 21st century, while typically keeping the SECDED level of protection, no longer use Hamming's method, relying instead on the designs with longer codewords (128 to 256 bits of data) and modified balanced parity-check trees. The (72,64) Hamming code is still popular in some hardware designs, including Xilinx FPGA families.\n[7,4] Hamming code.\nIn 1950, Hamming introduced the [7,4] Hamming code. It encodes four data bits into seven bits by adding three parity bits. As explained earlier, it can either detect and correct single-bit errors or it can detect (but not correct) both single and double-bit errors.\nWith the addition of an overall parity bit, it becomes the [8,4] extended Hamming code and can both detect and correct single-bit errors and detect (but not correct) double-bit errors.\nConstruction of G and H.\nThe matrix \nformula_4 is called a (canonical) generator matrix of a linear (\"n\",\"k\") code,\nand formula_5 is called a parity-check matrix.\nThis is the construction of G and H in standard (or systematic) form. Regardless of form, G and H for linear block codes must satisfy\nformula_6, an all-zeros matrix.\nSince [7,\u00a04,\u00a03] =\u00a0[\"n\",\u00a0\"k\",\u00a0\"d\"] =\u00a0[2\"m\"\u00a0\u2212\u00a01, 2\"m\"\u00a0\u2212\u00a01\u00a0\u2212\u00a0\"m\",\u00a03]. The parity-check matrix H of a Hamming code is constructed by listing all columns of length \"m\" that are pair-wise independent.\nThus H is a matrix whose left side is all of the nonzero \"n\"-tuples where order of the \"n\"-tuples in the columns of matrix does not matter. The right hand side is just the (\"n\"\u00a0\u2212\u00a0\"k\")-identity matrix.\nSo G can be obtained from H by taking the transpose of the left hand side of H with the identity \"k\"-identity matrix on the left hand side of\u00a0G.\nThe code generator matrix formula_7 and the parity-check matrix formula_8 are:\nformula_9\nand\nformula_10\nFinally, these matrices can be mutated into equivalent non-systematic codes by the following operations:\nEncoding.\nFrom the above matrix we have 2k = 24 = 16 codewords.\nLet formula_11 be a row vector of binary data bits, formula_12. The codeword formula_13 for any of the 16 possible data vectors formula_14 is given by the standard matrix product formula_15 where the summing operation is done modulo-2.\nFor example, let formula_16. Using the generator matrix formula_17 from above, we have (after applying modulo 2, to the sum),\nformula_18\n[8,4] Hamming code with an additional parity bit.\nThe [7,4] Hamming code can easily be extended to an [8,4] code by adding an extra parity bit on top of the (7,4) encoded word (see Hamming(7,4)).\nThis can be summed up with the revised matrices:\nformula_19\nand\nformula_20\nNote that H is not in standard form. To obtain G, elementary row operations can be used to obtain an equivalent matrix to H in systematic form:\nformula_21\nFor example, the first row in this matrix is the sum of the second and third rows of H in non-systematic form. Using the systematic construction for Hamming codes from above, the matrix A is apparent and the systematic form of G is written as\nformula_22\nThe non-systematic form of G can be row reduced (using elementary row operations) to match this matrix.\nThe addition of the fourth row effectively computes the sum of all the codeword bits (data and parity) as the fourth parity bit.\nFor example, 1011 is encoded (using the non-systematic form of G at the start of this section) into 01100110 where blue digits are data; red digits are parity bits from the [7,4] Hamming code; and the green digit is the parity bit added by the [8,4] code.\nThe green digit makes the parity of the [7,4] codewords even.\nFinally, it can be shown that the minimum distance has increased from 3, in the [7,4] code, to 4 in the [8,4] code. Therefore, the code can be defined as [8,4] Hamming code. \nTo decode the [8,4] Hamming code, first check the parity bit. If the parity bit indicates an error, single error correction (the [7,4] Hamming code) will indicate the error location, with \"no error\" indicating the parity bit. If the parity bit is correct, then single error correction will indicate the (bitwise) exclusive-or of two error locations. If the locations are equal (\"no error\") then a double bit error either has not occurred, or has cancelled itself out. Otherwise, a double bit error has occurred.\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "41227", "revid": "1320319980", "url": "https://en.wikipedia.org/wiki?curid=41227", "title": "Hamming distance", "text": "Number of bits that differ between two strings\nIn information theory, the Hamming distance between two strings or vectors of equal length is the number of positions at which the corresponding symbols are different. In other words, it measures the minimum number of \"substitutions\" required to change one string into the other, or equivalently, the minimum number of \"errors\" that could have transformed one string into the other. In a more general context, the Hamming distance is one of several string metrics for measuring the edit distance between two sequences. It is named after the American mathematician Richard Hamming.\nA major application is in coding theory, more specifically to block codes, in which the equal-length strings are vectors over a finite field.\nDefinition.\nThe Hamming distance between two equal-length strings of symbols is the number of positions at which the corresponding symbols are different.\nExamples.\nThe symbols may be letters, bits, or decimal digits, among other possibilities. For example, the Hamming distance between:\nProperties.\nFor a fixed length \"n\", the Hamming distance is a metric on the set of the words of length \"n\" (also known as a Hamming space), as it fulfills the conditions of non-negativity, symmetry, the Hamming distance of two words is 0 if and only if the two words are identical, and it satisfies the triangle inequality as well: Indeed, if we fix three words \"a\", \"b\" and \"c\", then whenever there is a difference between the \"i\"th letter of \"a\" and the \"i\"th letter of \"c\", then there must be a difference between the \"i\"th letter of \"a\" and \"i\"th letter of \"b\", or between the \"i\"th letter of \"b\" and the \"i\"th letter of \"c\". Hence the Hamming distance between \"a\" and \"c\" is not larger than the sum of the Hamming distances between \"a\" and \"b\" and between \"b\" and \"c\". The Hamming distance between two words \"a\" and \"b\" can also be seen as the Hamming weight of \"a\" \u2212 \"b\" for an appropriate choice of the \u2212 operator, much as the difference between two integers can be seen as a distance from zero on the number line.\nFor binary strings \"a\" and \"b\" the Hamming distance is equal to the number of ones (population count) in \"a\" XOR \"b\". The metric space of length-\"n\" binary strings, with the Hamming distance, is known as the \"Hamming cube\"; it is equivalent as a metric space to the set of distances between vertices in a hypercube graph. One can also view a binary string of length \"n\" as a vector in formula_1 by treating each symbol in the string as a real coordinate; with this embedding, the strings form the vertices of an \"n\"-dimensional hypercube, and the Hamming distance of the strings is equivalent to the Manhattan distance between the vertices.\nError detection and error correction.\nThe minimum Hamming distance or minimum distance (usually denoted by \"dmin\") is used to define some essential notions in coding theory, such as error detecting and error correcting codes. In particular, a code \"C\" is said to be \"k\" error detecting if, and only if, the minimum Hamming distance between any two of its codewords is at least \"k\"+1.\nFor example, consider a code consisting of two codewords \"000\" and \"111\". The Hamming distance between these two words is 3, and therefore it is \"k\"=2 error detecting. This means that if one bit is flipped or two bits are flipped, the error can be detected. If three bits are flipped, then \"000\" becomes \"111\" and the error cannot be detected.\nA code \"C\" is said to be \"k-error correcting\" if, for every word \"w\" in the underlying Hamming space \"H\", there exists at most one codeword \"c\" (from \"C\") such that the Hamming distance between \"w\" and \"c\" is at most \"k\". In other words, a code is \"k\"-errors correcting if the minimum Hamming distance between any two of its codewords is at least 2\"k\"+1. This is also understood geometrically as any closed balls of radius \"k\" centered on distinct codewords being disjoint. These balls are also called \"Hamming spheres\" in this context.\nFor example, consider the same 3-bit code consisting of the two codewords \"000\" and \"111\". The Hamming space consists of 8 words 000, 001, 010, 011, 100, 101, 110 and 111. The codeword \"000\" and the single bit error words \"001\",\"010\",\"100\" are all less than or equal to the Hamming distance of 1 to \"000\". Likewise, codeword \"111\" and its single bit error words \"110\",\"101\" and \"011\" are all within 1 Hamming distance of the original \"111\". In this code, a single bit error is always within 1 Hamming distance of the original codes, and the code can be \"1-error correcting\", that is \"k=1\". Since the Hamming distance between \"000\" and \"111\" is 3, and those comprise the entire set of codewords in the code, the minimum Hamming distance is 3, which satisfies \"2k+1 = 3\".\nThus a code with minimum Hamming distance \"d\" between its codewords can detect at most \"d\"-1 errors and can correct \u230a(\"d\"-1)/2\u230b errors. The latter number is also called the \"packing radius\" or the \"error-correcting capability\" of the code.\nHistory and applications.\nThe Hamming distance is named after Richard Hamming, who introduced the concept in his fundamental paper on Hamming codes, \"Error detecting and error correcting codes\", in 1950. Hamming weight analysis of bits is used in several disciplines including information theory, coding theory, and cryptography.\nIt is used in telecommunication to count the number of flipped bits in a fixed-length binary word as an estimate of error, and therefore is sometimes called the signal distance. For \"q\"-ary strings over an alphabet of size \"q\"\u00a0\u2265\u00a02 the Hamming distance is applied in case of the q-ary symmetric channel, while the Lee distance is used for phase-shift keying or more generally channels susceptible to synchronization errors because the Lee distance accounts for errors of \u00b11. If formula_2 or formula_3 both distances coincide because any pair of elements from formula_4 or formula_5 differ by 1, but the distances are different for larger formula_6.\nThe Hamming distance is also used in systematics as a measure of genetic distance.\nHowever, for comparing strings of different lengths, or strings where not just substitutions but also insertions or deletions have to be expected, a more sophisticated metric such as the Levenshtein distance may be more appropriate.\nAlgorithm example.\nThe following function, written in Python 3, returns the Hamming distance between two strings:\ndef hamming_distance(string1: str, string2: str) -&gt; int:\n \"\"\"Return the Hamming distance between two strings.\"\"\"\n if len(string1) != len(string2):\n raise ValueError(\"Strings must be of equal length.\")\n dist_counter = 0\n for n in range(len(string1)):\n if string1[n] != string2[n]:\n dist_counter += 1\n return dist_counter\nThe following C function will compute the Hamming distance of two integers (considered as binary values, that is, as sequences of bits). The running time of this procedure is proportional to the Hamming distance rather than to the number of bits in the inputs. It computes the bitwise exclusive or of the two inputs, and then finds the Hamming weight of the result (the number of nonzero bits) using an algorithm of that repeatedly finds and clears the lowest-order nonzero bit. Some compilers support the __builtin_popcount function which can calculate this using specialized processor hardware where available.\nint hamming_distance(unsigned x, unsigned y)\n int dist = 0;\n // The ^ operators sets to 1 only the bits that are different\n for (unsigned val = x ^ y; val &gt; 0; ++dist)\n // We then count the bit set to 1 using the Peter Wegner way\n val = val &amp; (val - 1); // Set to zero val's lowest-order 1\n // Return the number of differing bits\n return dist;\nA faster alternative is to use the population count (\"popcount\") assembly instruction. Certain compilers such as GCC and Clang make it available via an intrinsic function:\n// Hamming distance for 32-bit integers\nint hamming_distance32(unsigned int x, unsigned int y)\n return __builtin_popcount(x ^ y);\n// Hamming distance for 64-bit integers\nint hamming_distance64(unsigned long long x, unsigned long long y)\n return __builtin_popcountll(x ^ y);\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41229", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41229", "title": "Handshake (computing)", "text": "Signal between two devices or programs\nIn computing, a handshake is a signal between two devices or programs, used to, e.g., authenticate, coordinate. An example is the handshaking between a hypervisor and an application in a guest virtual machine.\nIn telecommunications, a handshake is an automated process of negotiation between two participants (example \"Alice and Bob\") through the exchange of information that establishes the protocols of a communication link at the start of the communication, before full communication begins. The handshaking process usually takes place in order to establish rules for communication when a computer attempts to communicate with another device. Signals are usually exchanged between two devices to establish a communication link. For example, when a computer communicates with another device such as a modem, the two devices will signal each other that they are switched on and ready to work, as well as to agree to which protocols are being used.\nHandshaking can negotiate parameters that are acceptable to equipment and systems at both ends of the communication channel, including information transfer rate, coding alphabet, parity, interrupt procedure, and other protocol or hardware features.\nHandshaking is a technique of communication between two entities. However, within TCP/IP RFCs, the term \"handshake\" is most commonly used to reference the TCP three-way handshake. For example, the term \"handshake\" is not present in RFCs covering FTP or SMTP. One exception is Transport Layer Security, TLS, setup, FTP RFC 4217. In place of the term \"handshake\", FTP RFC 3659 substitutes the term \"conversation\" for the passing of commands.\nA simple handshaking protocol might only involve the receiver sending a message meaning \"I received your last message and I am ready for you to send me another one.\" A more complex handshaking protocol might allow the sender to ask the receiver if it is ready to receive or for the receiver to reply with a negative acknowledgement meaning \"I did not receive your last message correctly, please resend it\" (e.g., if the data was corrupted en route).\nHandshaking facilitates connecting relatively heterogeneous systems or equipment over a communication channel without the need for human intervention to set parameters.\nExample.\nTCP three-way handshake.\nEstablishing a normal TCP connection requires three separate steps:\n# The first host (Alice) sends the second host (Bob) a \"synchronize\" (SYN) message with its own sequence number formula_1, which Bob receives.\n# Bob replies with a synchronize-acknowledgment (SYN-ACK) message with its own sequence number formula_2 and acknowledgement number formula_3, which Alice receives.\n# Alice replies with an acknowledgment (ACK) message with acknowledgement number formula_4, which Bob receives and to which he doesn't need to reply.\n In this setup, the synchronize messages act as service requests from one server to the other, while the acknowledgement messages return to the requesting server to let it know the message was received.\nThe reason for the client and server not using a default sequence number such as 0 for establishing the connection is to protect against two incarnations of the same connection reusing the same sequence number too soon, which means a segment from an earlier incarnation of a connection might interfere with a later incarnation of the connection.\nSMTP.\nThe Simple Mail Transfer Protocol (SMTP) is the key Internet standard for email transmission. It includes handshaking to negotiate authentication, encryption and maximum message size. \nTLS handshake.\nWhen a Transport Layer Security (SSL or TLS) connection starts, the record encapsulates a \"control\" protocol\u2014the handshake messaging protocol (content type 22). This protocol is used to exchange all the information required by both sides for the exchange of the actual application data by TLS. It defines the messages formatting or containing this information and the order of their exchange. These may vary according to the demands of the client and server\u2014i.e., there are several possible procedures to set up the connection. This initial exchange results in a successful TLS connection (both parties ready to transfer application data with TLS) or an alert message (as specified below).\nThe protocol is used to negotiate the secure attributes of a session. (RFC 5246, p.\u00a037)\nWPA2 wireless.\nThe WPA2 standard for wireless uses a four-way handshake defined in IEEE 802.11i-2004.\nDial-up access modems.\nOne classic example of handshaking is that of dial-up modems, which typically negotiate communication parameters for a brief period when a connection is first established, and there after use those parameters to provide optimal information transfer over the channel as a function of its quality and capacity. The \"squealing\" (which is actually a sound that changes in pitch 100 times every second) noises made by some modems with speaker output immediately after a connection is established are in fact the sounds of modems at both ends engaging in a handshaking procedure; once the procedure is completed, the speaker might be silenced, depending on the settings of operating system or the application controlling the modem.\nSerial \"Hardware Handshaking\".\nThis frequently used term describes the use of RTS and CTS signals over a serial interconnection. It is, however, not quite correct; it's not a true form of handshaking, and is better described as flow control.\nMobile device charging.\nIn mobile device chargers offering special quick-charge abilities to supported devices, the charging process will switch up to a higher output voltage for increased power transfer. But this could cause serious damage to an unsupported device or even result in a fire. It is therefore very important for the device and charger to first perform a handshake to \"agree\" on mutually supported charge parameters. If such a charger can't identify the connected device or determine its compatibility, it will default to normal but much slower charge parameters within the USB standard.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41230", "revid": "1314401359", "url": "https://en.wikipedia.org/wiki?curid=41230", "title": "Hard copy", "text": "Paper or other physical form of information\nIn information handling, the U.S. Federal Standard 1037C (Glossary of Telecommunication Terms) defines a hard copy as a permanent reproduction, or copy, in the form of a physical object, of any media suitable for direct use by a person (in particular paper), of displayed or transmitted data. Examples of hard copies include teleprinter pages, continuous printed tapes, computer printouts, and radio photo prints. On the other hand, physical objects such as magnetic tapes, floppy disks, or non-printed punched paper tapes are not defined as hard copies by 1037C.\nA file that can be viewed on a screen without being printed is sometimes called a soft copy. The U.S. Federal Standard 1037C defines \"soft copy\" as \"a nonpermanent display image, for example, a cathode ray tube display.\"\nThe term \"hard copy\" predates the digital computer. In the book and newspaper printing process, \"hard copy\" refers to a manuscript or typewritten document that has been edited and proofread and is ready for typesetting or being read on-air in a radio or television broadcast. The old meaning of hard copy was mostly discarded after the information revolution.\nUse in computer security.\nOne often-overlooked use for printers is in the field of IT security. Copies of various system and server activity logs are typically stored on the local filesystem, where a remote attacker \u2013 having achieved their primary goals \u2013 can then alter or delete the contents of the logs in an attempt to \"cover their tracks\" or otherwise thwart the efforts of system administrators and security experts. However, if the log entries are simultaneously given to a printer, line-by-line, a local hard-copy record of system activity is created \u2013 which cannot be remotely altered or otherwise manipulated. Dot matrix printers are ideal for this task, as they can sequentially print each log entry, one at a time, as they are added to the log. The usual dot-matrix printer support for continuous stationery also prevents incriminating pages from being surreptitiously removed or altered without evidence of tampering.\n\"Dead-tree\" dysphemism.\nThe hacker's \"Jargon File\" defines a \"dead-tree version\" to be a paper version of an online document, where the phrase \"dead trees\" refers to paper.\nA saying from the \"Jargon File\" is that \"You can't grep dead trees\", which comes from the Unix command , which searches the contents of text files. This means that there is an advantage to keeping documents in digital form, rather than on paper, so that they can be more easily searched for specific contents. A similar entry in the \"Jargon File\" is \"tree-killer\", which may refer to either a printer or a person who wastes paper.\n\"Dead-tree edition\" refers to a printed paper version of a written work, as opposed to digital alternatives such as a web page.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nExternal links.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41231", "revid": "46639243", "url": "https://en.wikipedia.org/wiki?curid=41231", "title": "Hard sectoring", "text": "Method of sectoring drives\nHard sectoring in a magnetic or optical data storage device is a form of sectoring which uses a physical mark or hole in the recording medium to reference sector locations.\nIn older 8- and 5&lt;templatestyles src=\"Fraction/styles.css\" /&gt;1\u20444-inch floppy disks, hard sectoring was implemented by punching sector holes in the disk to mark the start\nof each sector. These were equally spaced holes, at a common radius. This was in addition to the index hole, situated between two sector holes, to mark the start of the entire track of sectors. When the index or sector hole was recognized by an optical sensor, a sector signal was generated. Timing electronics or software would use the faster timing of the index hole between sector holes, to generate an index signal. Data read and write is faster in this technique than soft sectoring as no operations are to be performed regarding the starting and ending points of tracks."}
{"id": "41232", "revid": "18779361", "url": "https://en.wikipedia.org/wiki?curid=41232", "title": "Harmonic", "text": "Wave with frequency an integer multiple of the fundamental frequency\nIn physics, acoustics, and telecommunications, a harmonic is a sinusoidal wave with a frequency that is a positive integer multiple of the \"fundamental frequency\" of a periodic signal. The fundamental frequency is also called the 1st harmonic; the other harmonics are known as higher harmonics. As all harmonics are periodic at the fundamental frequency, the sum of harmonics is also periodic at that frequency. The set of harmonics forms a \"harmonic series\".\nThe term is employed in various disciplines, including music, physics, acoustics, electronic power transmission, radio technology, and other fields. For example, if the fundamental frequency is 50\u00a0Hz, a common AC power supply frequency, the frequencies of the first three higher harmonics are 100\u00a0Hz (2nd harmonic), 150\u00a0Hz (3rd harmonic), 200\u00a0Hz (4th harmonic) and any addition of waves with these frequencies is periodic at 50\u00a0Hz.\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;\nIn music, harmonics are used on string instruments and wind instruments as a way of producing sound on the instrument, particularly to play higher notes and, with strings, obtain notes that have a unique sound quality or \"tone colour\". On strings, bowed harmonics have a \"glassy\", pure tone. On stringed instruments, harmonics are played by touching (but not fully pressing down the string) at an exact point on the string while sounding the string (plucking, bowing, etc.); this allows the harmonic to sound, a pitch which is always higher than the fundamental frequency of the string.\nTerminology.\nHarmonics may be called \"overtones\", \"partials\", or \"upper partials\", and in some music contexts, the terms \"harmonic\", \"overtone\" and \"partial\" are used fairly interchangeably. But more precisely, the term \"harmonic\" includes \"all\" pitches in a harmonic series (including the fundamental frequency) while the term \"overtone\" only includes pitches \"above\" the fundamental. \nCharacteristics.\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;\nMost acoustic instruments emit complex tones containing many individual partials (component simple tones or sinusoidal waves), but the untrained human ear typically does not perceive those partials as separate phenomena. Rather, a musical note is perceived as one sound, the quality or timbre of that sound being a result of the relative strengths of the individual partials. Many acoustic oscillators, such as the human voice or a bowed violin string, produce complex tones that are more or less periodic, and thus are composed of partials that are nearly matched to the integer multiples of fundamental frequency and therefore resemble the ideal harmonics and are called \"harmonic partials\" or simply \"harmonics\" for convenience (although it's not strictly accurate to call a \u2009\"partial\"\u2009 a \u2009\"harmonic\",\u2009 the first being actual and the second being theoretical).\nOscillators that produce harmonic partials behave somewhat like one-dimensional resonators, and are often long and thin, such as a guitar string or a column of air open at both ends (as with the metallic modern orchestral transverse flute). Wind instruments whose air column is open at only one end, such as trumpets and clarinets, also produce partials resembling harmonics. However they only produce partials matching the \"odd\" harmonics\u2014at least in theory. In practical use, no real acoustic instrument behaves as perfectly as the simplified physical models predict; for example, instruments made of non-linearly elastic wood, instead of metal, or strung with gut instead of brass or steel strings, tend to have not-quite-integer partials.\nPartials whose frequencies are not integer multiples of the fundamental are referred to as inharmonic partials. Some acoustic instruments emit a mix of harmonic and inharmonic partials but still produce an effect on the ear of having a definite fundamental pitch, such as pianos, strings plucked pizzicato, vibraphones, marimbas, and certain pure-sounding bells or chimes. Antique singing bowls are known for producing multiple harmonic partials or multiphonics.\nOther oscillators, such as cymbals, drum heads, and most percussion instruments, naturally produce an abundance of inharmonic partials and do not imply any particular pitch, and therefore cannot be used melodically or harmonically in the same way other instruments can.\nBuilding on of Sethares (2004), dynamic tonality introduces the notion of pseudo-harmonic partials, in which the frequency of each partial is aligned to match the pitch of a corresponding note in a pseudo-just tuning, thereby maximizing the consonance of that pseudo-harmonic timbre with notes of that pseudo-just tuning.\nPartials, overtones, and harmonics.\nAn overtone is any partial higher than the lowest partial in a compound tone. The relative strengths and frequency relationships of the component partials determine the timbre of an instrument. The similarity between the terms overtone and partial sometimes leads to their being loosely used interchangeably in a musical context, but they are counted differently, leading to some possible confusion. In the special case of instrumental timbres whose component partials closely match a harmonic series (such as with most strings and winds) rather than being inharmonic partials (such as with most pitched percussion instruments), it is also convenient to call the component partials \"harmonics\", but not strictly correct, because harmonics are numbered the same even when missing, while partials and overtones are only counted when present. This chart demonstrates how the three types of names (partial, overtone, and harmonic) are counted (assuming that the harmonics are present):\nIn many musical instruments, it is possible to play the upper harmonics without the fundamental note being present. In a simple case (e.g., recorder) this has the effect of making the note go up in pitch by an octave, but in more complex cases many other pitch variations are obtained. In some cases it also changes the timbre of the note. This is part of the normal method of obtaining higher notes in wind instruments, where it is called \"overblowing\". The extended technique of playing multiphonics also produces harmonics. On string instruments it is possible to produce very pure sounding notes, called harmonics or \"flageolets\" by string players, which have an eerie quality, as well as being high in pitch. Harmonics may be used to check at a unison the tuning of strings that are not tuned to the unison. For example, lightly fingering the node found halfway down the highest string of a cello produces the same pitch as lightly fingering the node of the way down the second highest string. For the human voice see Overtone singing, which uses harmonics.\nWhile it is true that electronically produced periodic tones (e.g. square waves or other non-sinusoidal waves) have \"harmonics\" that are whole number multiples of the fundamental frequency, practical instruments do not all have this characteristic. For example, higher \"harmonics\" of piano notes are not true harmonics but are \"overtones\" and can be very sharp, i.e. a higher frequency than given by a pure harmonic series. This is especially true of instruments other than strings, brass, or woodwinds. Examples of these \"other\" instruments are xylophones, drums, bells, chimes, etc.; not all of their overtone frequencies make a simple whole number ratio with the fundamental frequency. (The fundamental frequency is the reciprocal of the longest time period of the collection of vibrations in some single periodic phenomenon.)\nOn stringed instruments.\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;\"Grove's Dictionary of Music and Musicians\" (1879)\nThe following table displays the stop points on a stringed instrument at which gentle touching of a string will force it into a harmonic mode when vibrated. String harmonics (flageolet tones) are described as having a \"flutelike, silvery quality\" that can be highly effective as a special color or tone color (timbre) when used and heard in orchestration. It is unusual to encounter natural harmonics higher than the fifth partial on any stringed instrument except the double bass, on account of its much longer strings.\nArtificial harmonics.\nOccasionally a score will call for an artificial harmonic, produced by playing an overtone on an already stopped string. As a performance technique, it is accomplished by using two fingers on the fingerboard, the first to shorten the string to the desired fundamental, with the second touching the node corresponding to the appropriate harmonic.\nOther information.\nHarmonics may be either used in or considered as the basis of just intonation systems. Composer Arnold Dreyblatt is able to bring out different harmonics on the single string of his modified double bass by slightly altering his unique bowing technique halfway between hitting and bowing the strings. Composer Lawrence Ball uses harmonics to generate music electronically.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41233", "revid": "28438779", "url": "https://en.wikipedia.org/wiki?curid=41233", "title": "H channel", "text": "High-speed communication channel\nIn the Integrated Services Digital Network (ISDN), a high-speed communication channel comprising multiple aggregated low-speed channels to accommodate bandwidth-intensive applications such as file transfer, videoconferencing, and high-quality audio. An H channel is formed of multiple bearer B channels bonded together in a primary rate access (PRA) or primary rate interface (PRI) frame in support of applications with bandwidth requirements that exceed the B channel rate of 64\u00a0kbit/s. The channels, once bonded, remain so end-to-end, from transmitter to receiver, through the ISDN network. The feature is known variously as multirate ISDN, Nx64, channel aggregation, and bonding.\nH channels are implemented as:"}
{"id": "41234", "revid": "18731412", "url": "https://en.wikipedia.org/wiki?curid=41234", "title": "Heterodyne", "text": "Signal processing technique\nA heterodyne is a signal frequency that is created by combining or mixing two other frequencies using a signal processing technique called \"heterodyning\", which was invented by Canadian inventor-engineer Reginald Fessenden. Heterodyning is used to shift signals from one frequency range into another, and is also involved in the processes of modulation and demodulation. The two input frequencies are combined in a nonlinear signal-processing device such as a vacuum tube, transistor, or diode, usually called a \"mixer\", to create new frequency signals, called \"heterodynes\".\nIn the most common application, two signals at frequencies \"f\"1 and \"f\"2 are mixed, creating two heterodynes, one at the sum of the two frequencies \"f\"1\u00a0+\u00a0\"f\"2, and the other at the difference between the two frequencies \"f\"1\u00a0\u2212\u00a0\"f\"2. Typically, only one of the heterodynes is required and the other signal is filtered out of the output of the mixer. Heterodyne frequencies are related to the phenomenon of \"beats\" in acoustics.\nA major application of the heterodyne process is in the superheterodyne radio receiver circuit, which is used in virtually all modern radio receivers.\nHistory.\nIn 1901, Reginald Fessenden demonstrated a direct-conversion receiver or beat receiver as a method of making continuous wave radiotelegraphy signals audible. Fessenden's receiver did not see much application because of its local oscillator's stability problem. A stable yet inexpensive local oscillator was not available until Lee de Forest invented the triode vacuum tube oscillator. In a 1905 patent, Fessenden stated that the frequency stability of his local oscillator was one part per thousand.\nIn radio telegraphy, the characters of text messages are translated into the short duration dots and long duration dashes of Morse code that are broadcast as radio signals. Radio telegraphy was much like ordinary telegraphy. One of the problems was building high power transmitters with the technology of the day. Early transmitters were spark gap transmitters. A mechanical device would make sparks at a fixed but audible rate; the sparks would put energy into a resonant circuit that would then ring at the desired transmission frequency (which might be 100\u00a0kHz). This ringing would quickly decay, so the output of the transmitter would be a succession of damped waves. When these damped waves were received by a simple detector, the operator would hear an audible buzzing sound that could be transcribed back into alpha-numeric characters.\nWith the development of the arc converter radio transmitter in 1904, continuous wave (CW) modulation began to be used for radiotelegraphy. CW Morse code signals are not amplitude modulated, but rather consist of bursts of sinusoidal carrier frequency. When CW signals are received by an AM receiver, the operator does not hear a sound. The direct-conversion (heterodyne) detector was invented to make continuous wave radio-frequency signals audible.\nThe \"heterodyne\" or \"beat\" receiver has a local oscillator that produces a radio signal adjusted to be close in frequency to the incoming signal being received. When the two signals are mixed, a \"beat\" frequency equal to the difference between the two frequencies is created. Adjusting the local oscillator frequency correctly puts the beat frequency in the audio range, where it can be heard as a tone in the receiver's earphones whenever the transmitter signal is present. Thus the Morse code \"dots\" and \"dashes\" are audible as beeping sounds. This technique is still used in radio telegraphy, the local oscillator now being called the beat frequency oscillator or BFO. Fessenden coined the word \"heterodyne\" from the Greek roots \"hetero-\" \"different\", and \"dyn-\" \"power\" (cf. ).\nSuperheterodyne receiver.\nAn important and widely used application of the heterodyne technique is in the superheterodyne receiver (superhet). In the typical superhet, the incoming radio frequency signal from the antenna is mixed (heterodyned) with a signal from a local oscillator (LO) to produce a lower fixed frequency signal called the intermediate frequency (IF) signal. The IF signal is amplified and filtered and then applied to a detector that extracts the audio signal; the audio is ultimately sent to the receiver's loudspeaker.\nThe superheterodyne receiver has several advantages over previous receiver designs. One advantage is easier tuning; only the RF filter and the LO are tuned by the operator; the fixed-frequency IF is tuned (\"aligned\") at the factory and is not adjusted. In older designs such as the tuned radio frequency receiver (TRF), all of the receiver stages had to be simultaneously tuned. In addition, since the IF filters are fixed-tuned, the receiver's selectivity is the same across the receiver's entire frequency band. Another advantage is that the IF signal can be at a much lower frequency than the incoming radio signal, and that allows each stage of the IF amplifier to provide more gain. To first order, an amplifying device has a fixed gain-bandwidth product. If the device has a gain-bandwidth product of 60\u00a0MHz, then it can provide a voltage gain of 3 at an RF of 20\u00a0MHz or a voltage gain of 30 at an IF of 2\u00a0MHz. At a lower IF, it would take fewer gain devices to achieve the same gain. The regenerative radio receiver obtained more gain out of one gain device by using positive feedback, but it required careful adjustment by the operator; that adjustment also changed the selectivity of the regenerative receiver. The superheterodyne provides a large, stable gain and constant selectivity without troublesome adjustment.\nThe superior superheterodyne system replaced the earlier TRF and regenerative receiver designs, and since the 1930s most commercial radio receivers have been superheterodynes.\nApplications.\nHeterodyning, also called \"frequency conversion\", is used very widely in communications engineering to generate new frequencies and move information from one frequency channel to another. Besides its use in the superheterodyne circuit found in almost all radio and television receivers, it is used in radio transmitters, modems, satellite communications and set-top boxes, radar, radio telescopes, telemetry systems, cell phones, cable television converter boxes and headends, microwave relays, metal detectors, atomic clocks, and military electronic countermeasure (jamming) systems.\nUp and down converters.\nIn large scale telecommunication networks such as telephone network trunks, microwave relay networks, cable television systems, and communication satellite links, large bandwidth capacity links are shared by many individual communication channels by using heterodyning to move the frequency of the individual signals up to different frequencies, which share the channel. This is called frequency division multiplexing (FDM).\nFor example, a coaxial cable used by a cable television system can carry 500 television channels at the same time because each one is given a different frequency, so they do not interfere with one another. At the cable source or headend, electronic upconverters convert each incoming television channel to a new, higher frequency. They do this by mixing the television signal frequency, \"fCH\" with a local oscillator at a much higher frequency \"fLO\", creating a heterodyne at the sum \"fCH\"\u00a0+\u00a0\"fLO\", which is added to the cable. At the consumer's home, the cable set top box has a downconverter that mixes the incoming signal at frequency \"fCH\"\u00a0+\u00a0\"fLO\" with the same local oscillator frequency \"fLO\" creating the difference heterodyne frequency, converting the television channel back to its original frequency: (\"fCH\"\u00a0+\u00a0\"fLO\")\u00a0\u2212\u00a0\"fLO\" =\u00a0\"fCH\". Each channel is moved to a different higher frequency. The original lower basic frequency of the signal is called the baseband, while the higher channel it is moved to is called the passband.\nAnalog videotape recording.\nMany analog videotape systems rely on a downconverted color subcarrier to record color information in their limited bandwidth. These systems are referred to as \"heterodyne systems\" or \"color-under systems\". For instance, for NTSC video systems, the VHS (and S-VHS) recording system converts the color subcarrier from the NTSC standard 3.58\u00a0MHz to ~629\u00a0kHz. PAL VHS color subcarrier is similarly downconverted (but from 4.43\u00a0MHz). The now-obsolete 3/4\" U-matic systems use a heterodyned ~688\u00a0kHz subcarrier for NTSC recordings (as does Sony's Betamax, which is at its basis a 1/2\u2033 consumer version of U-matic), while PAL U-matic decks came in two mutually incompatible varieties, with different subcarrier frequencies, known as Hi-Band and Low-Band. Other videotape formats with heterodyne color systems include Video-8 and Hi8.\nThe heterodyne system in these cases is used to convert quadrature phase-encoded and amplitude modulated sine waves from the broadcast frequencies to frequencies recordable in less than 1\u00a0MHz bandwidth. On playback, the recorded color information is heterodyned back to the standard subcarrier frequencies for display on televisions and for interchange with other standard video equipment.\nSome U-matic (3/4\u2033) decks feature 7-pin mini-DIN connectors to allow dubbing of tapes without conversion, as do some industrial VHS, S-VHS, and Hi8 recorders.\nMusic synthesis.\nThe theremin, an electronic musical instrument, traditionally uses the heterodyne principle to produce a variable audio frequency in response to the movement of the musician's hands in the vicinity of one or more antennae, which act as capacitor plates. The output of a fixed radio frequency oscillator is mixed with that of an oscillator whose frequency is affected by the variable capacitance between the antenna and the musician's hand as it is moved near the pitch control antenna. The difference between the two oscillator frequencies produces a tone in the audio range.\nThe ring modulator is a type of frequency mixer incorporated into some synthesizers or used as a stand-alone audio effect.\nOptical heterodyning.\nOptical heterodyne detection (an area of active research) is an extension of the heterodyning technique to higher (visible) frequencies. Guerra (1995) first published the results of what he called a \"form of optical heterodyning\" in which light patterned by a 50 nm pitch grating illuminated a second grating of pitch 50 nm, with the gratings rotated with respect to each other by the angular amount needed to achieve magnification. Although the illuminating wavelength was 650 nm, the 50 nm grating was easily resolved. This showed a nearly 5-fold improvement over the Abbe resolution limit of 232 nm that should have been the smallest obtained for the numerical aperture and wavelength used. This super-resolution microscopic imaging through optical heterodyning later came to be know by many as \"structured illumination microscopy\". \nIn addition to super-resolution optical microscopy, optical heterodyning could greatly improve optical modulators, increasing the density of information carried by optical fibers. It is also being applied in the creation of more accurate atomic clocks based on directly measuring the frequency of a laser beam.\nSince optical frequencies are far beyond the manipulation capacity of any feasible electronic circuit, all visible frequency photon detectors are inherently energy detectors not oscillating electric field detectors. However, since energy detection is inherently \"square-law\" detection, it intrinsically mixes any optical frequencies present on the detector. Thus, sensitive detection of specific optical frequencies necessitates optical heterodyne detection, in which two different (close by) wavelengths of light illuminate the detector so that the oscillating electrical output corresponds to the difference between their frequencies. This allows extremely narrow band detection (much narrower than any possible color filter can achieve) as well as precision measurements of phase and frequency of a light signal relative to a reference light source, as in a laser Doppler vibrometer.\nThis phase sensitive detection has been applied for Doppler measurements of wind speed, and imaging through dense media. The high sensitivity against background light is especially useful for lidar.\nIn optical Kerr effect (OKE) spectroscopy, optical heterodyning of the OKE signal and a small part of the probe signal produces a mixed signal consisting of probe, heterodyne OKE-probe and homodyne OKE signal. The probe and homodyne OKE signals can be filtered out, leaving the heterodyne frequency signal for detection.\nHeterodyne detection is often used in interferometry but usually confined to single point detection rather than widefield interferometry, however, widefield heterodyne interferometry is possible using a special camera. Using this technique which a reference signal extracted from a single pixel it is possible to build a highly stable widefield heterodyne interferometer by removing the piston phase component\ncaused by microphonics or vibrations of the optical components or object.\nMathematical principle.\nHeterodyning is based on the trigonometric identity:\nformula_1\nThe product on the left hand side represents the multiplication (\"mixing\") of a sine wave with another sine wave (both produced by cosine functions). The right hand side shows that the resulting signal is the sum of two sinusoidal terms, one at the sum of the two original frequencies, and one at the difference, which can be dealt with separately, since their (large) frequency difference makes it easy to cleanly filter out one signal's frequency, while leaving the other signal unchanged.\nUsing this trigonometric identity, the result of multiplying two cosine wave signals formula_2 and formula_3 at different frequencies formula_4 and formula_5 can be calculated:\nformula_6\nThe result is the sum of two sinusoidal signals, one at the sum \"f\"1\u00a0+\u00a0\"f\"2 and one at the difference \"f\"1\u00a0\u2212\u00a0\"f\"2 of the original frequencies.\nMixer.\nThe two signals are combined in a device called a \"mixer\". As seen in the previous section, an ideal mixer would be a device that multiplies the two signals. Some widely used mixer circuits, such as the Gilbert cell, operate in this way, but they are limited to lower frequencies. However, any \"nonlinear\" electronic component also multiplies signals applied to it, producing heterodyne frequencies in its output\u2014so a variety of nonlinear components serve as mixers. A nonlinear component is one in which the output current or voltage is a nonlinear function of its input. Most circuit elements in communications circuits are designed to be linear. This means they obey the superposition principle; if formula_7 is the output of a linear element with an input of formula_8:\nformula_9\nSo if two sine wave signals at frequencies \"f\"1 and \"f\"2 are applied to a linear device, the output is simply the sum of the outputs when the two signals are applied separately with no product terms. Thus, the function formula_10 must be nonlinear to create mixer products. A perfect multiplier only produces mixer products at the sum and difference frequencies (\"f\"1\u00a0\u00b1\u00a0\"f\"2), but more general nonlinear functions produce higher order mixer products: \"n\"\u22c5\"f\"1\u00a0+\u00a0\"m\"\u22c5\"f\"2 for integers \"n\" and \"m\". Some mixer designs, such as double-balanced mixers, suppress some high order undesired products, while other designs, such as harmonic mixers exploit high order differences.\nExamples of nonlinear components that are used as mixers are vacuum tubes and transistors biased near cutoff (class C), and diodes. Ferromagnetic core inductors driven into saturation can also be used at lower frequencies. In nonlinear optics, crystals that have nonlinear characteristics are used to mix laser light beams to create optical heterodyne frequencies.\nOutput of a mixer.\nTo demonstrate mathematically how a nonlinear component can multiply signals and generate heterodyne frequencies, the nonlinear function formula_10 can be expanded in a power series (MacLaurin series):\nformula_12\nTo simplify the math, the higher order terms above \"\u03b1\"2 are indicated by an ellipsis (formula_13) and only the first terms are shown. Applying the two sine waves at frequencies \"\u03c9\"1 =\u00a02\u03c0\"f\"1 and \"\u03c9\"2 =\u00a02\u03c0\"f\"2 to this device:\nformula_14\nformula_15\nformula_16\nIt can be seen that the second term above contains a product of the two sine waves. Simplifying with trigonometric identities:\nformula_17\nWhich leaves the two heterodyne frequencies among the many terms:\nformula_18\nalong with many other terms not shown.\nIn addition to components with frequencies at the sum \"\u03c9\"1\u00a0+\u00a0\"\u03c9\"2 and difference \"\u03c9\"1\u00a0\u2212\u00a0\"\u03c9\"2 of the two original frequencies, shown above, the output also contains sinusoidal terms at the original frequencies and terms at multiples of the original frequencies etc., called \"harmonics\". It also contains much more complicated terms at frequencies of called intermodulation products. These unwanted frequencies, along with the unwanted heterodyne frequency, must be removed from the mixer output by an electronic filter, to leave the desired heterodyne frequency.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\nCitations.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41235", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=41235", "title": "Heterodyne repeater", "text": ""}
{"id": "41236", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41236", "title": "Heuristic routing", "text": "Heuristic routing is a system used to describe how deliveries are made when problems in a network topology arise. Heuristic is an adjective used in relation to methods of learning, discovery, or problem solving. Routing is the process of selecting paths to specific destinations. Heuristic routing is used for traffic in the telecommunications networks and transport networks of the world.\nHeuristic routing is achieved using specific algorithms to determine a better, although not always optimal, path to a destination. When an interruption in a network topology occurs, the software running on the networking electronics can calculate another route to the desired destination via an alternate available path.\nAccording to :\nThe heuristic approach to problem solving consists of applying human intelligence, experience, common sense and certain rules of thumb (or heuristics) to develop an acceptable, but not necessarily an optimum, solution to a problem. Of course, determining what constitutes an acceptable solution is part of the task of deciding which approach to use; but broadly defined, an acceptable solution is one that is both reasonably good (close to optimum) and derived within reasonable effort, time, and cost constraints. Often the effort (manpower, computer, and other resources) required, the time limits on when the solution is needed, and the cost to compile, process, and analyze all the data required for deterministic or other complicated procedures preclude their usefulness or favor the faster, simpler heuristic approach. Thus, the heuristic approach is generally used when deterministic techniques or are not available, economical, or practical.\nHeuristic routing allows a measure of route optimization in telecommunications networks based on recent empirical knowledge of the state of the network. Data, such as time delay, may be extracted from incoming messages, during specified periods and over different routes, and used to determine the optimum routing for transmitting data back to the sources. \nIP routing.\nThe IP routing protocols in use today are based on one of two algorithms: \"distance vector\" or \"link state\". Distance vector algorithms broadcast routing information to all neighboring routers. Link state routing protocols build a topographical map of the entire network based on updates from neighbor routers, and then use the Dijkstra algorithm to compute the shortest path to each destination. Metrics used are based on the number of hops, delay, throughput, traffic, and reliability.\nReferences.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41237", "revid": "3606755", "url": "https://en.wikipedia.org/wiki?curid=41237", "title": "Hierarchical routing", "text": "Network routing based on hierarchical addressing\nHierarchical routing is a method of routing in networks that is based on hierarchical addressing.\nBackground.\nMost Transmission Control Protocol/Internet Protocol (TCP/IP) routing is based on a two-level hierarchical routing in which an IP address is divided into a network portion and a host portion. Gateways use only the network portion until an IP datagram reaches a gateway that can deliver it directly. Additional levels of hierarchical routing are introduced by the addition of subnetworks.\nDescription.\nHierarchical routing is the procedure of arranging routers in a hierarchical manner. A good example would be to consider a corporate intranet. Most corporate intranets consist of a high speed backbone network. Connected to this backbone are routers which are in turn connected to a particular workgroup. These workgroups occupy a unique LAN. The reason this is a good arrangement is because even though there might be dozens of different workgroups, the span (maximum hop count to get from one host to any other host on the network) is 2. Even if the workgroups divided their LAN network into smaller partitions, the span could only increase to 4 in this particular example.\nConsidering alternative solutions with every router connected to every other router, or if every router was connected to 2 routers, shows the convenience of hierarchical routing. It decreases the complexity of network topology, increases routing efficiency, and causes much less congestion because of fewer routing advertisements. With hierarchical routing, only core routers connected to the backbone are aware of all routes. Routers that lie within a LAN only know about routes in the LAN. Unrecognized destinations are passed to the default route."}
{"id": "41239", "revid": "32942831", "url": "https://en.wikipedia.org/wiki?curid=41239", "title": "High-performance equipment", "text": "Telecommunications equipment\nHigh-performance equipment describes telecommunications equipment that\n(a) has the performance characteristics required for use in trunks or links,\n(b) is designed primarily for use in global and tactical systems, and\n(c) sufficiently withstands electromagnetic interference when operating in a variety of network or point-to-point circuits.\n\"Note:\" Requirements for global and tactical high-performance equipment may differ.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41240", "revid": "38004052", "url": "https://en.wikipedia.org/wiki?curid=41240", "title": "Hop", "text": "A hop is a type of jump.\nHop or hops may also refer to:\n&lt;templatestyles src=\"Template:TOC_right/styles.css\" /&gt;\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "41241", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=41241", "title": "Hop count", "text": ""}
{"id": "41242", "revid": "11009441", "url": "https://en.wikipedia.org/wiki?curid=41242", "title": "Horn", "text": "Horn may refer to:\n&lt;templatestyles src=\"Template:TOC_right/styles.css\" /&gt;\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "41243", "revid": "50252746", "url": "https://en.wikipedia.org/wiki?curid=41243", "title": "Hotline", "text": "Automatically directed point-to-point communications link\nA hotline is a point-to-point communications link in which a call is automatically directed to the preselected destination without any additional action by the user when the end instrument goes off-hook. An example would be a phone that automatically connects to emergency services on picking up the receiver. Therefore, dedicated hotline phones do not need a rotary dial or keypad. A hotline can also be called an automatic signaling, ringdown, or off-hook service.\nFor crises and service.\nTrue hotlines cannot be used to originate calls other than to preselected destinations. However, in common or colloquial usage, a \"hotline\" often refers to a call center reachable by dialing a standard telephone number, or sometimes the phone numbers themselves.\nThis is especially the case with 24-hour, noncommercial numbers, such as police tip hotlines or suicide crisis hotlines, which are staffed around the clock and thereby give the appearance of real hotlines. Increasingly, however, the term is found being applied to any customer service telephone number.\nBetween states.\nRussia\u2013United States.\nThe most famous hotline between states is the Moscow\u2013Washington hotline, also known as the \"red telephone\", although telephones have never been used in this capacity. This direct communications link was established on June 20, 1963, in the wake of the Cuban Missile Crisis, which convinced both sides of the need for better communications. It was used for the first time by U.S. President John F. Kennedy on August 30, 1963 and utilized teletypewriter technology, later replaced by telecopier and then by electronic mail.\nUnited Kingdom\u2013United States.\nDuring World War II\u2014two decades before the Washington\u2013Moscow hotline was established\u2014there was a hotline between No. 10 Downing Street, the Cabinet War Room bunker under the Treasury, Whitehall, and the White House in Washington, D.C. From 1943 to 1946, this link was made secure by using the first voice encryption machine, called SIGSALY.\nChina\u2013Russia.\nA hotline connection between Beijing and Moscow was used during the 1969 frontier confrontation between the two countries. The Chinese refused the Russian peace attempts and ended the communications link. After a reconciliation between the two countries, the hotline between China and Russia was revived in 1996.\nFrance\u2013Russia.\nOn his visit to the Soviet Union in 1966, French President Charles de Gaulle announced that a hotline would be established between Paris and Moscow. The line was upgraded from a telex to a high-speed fax machine in 1989.\nRussia\u2013United Kingdom.\nA London\u2013Moscow hotline was not formally established until a treaty of friendship between the two countries in 1992. An upgrade was announced when Foreign Secretary William Hague visited Moscow in 2011.\nIndia\u2013Pakistan.\nOn 20 June 2004, both India and Pakistan agreed to extend a nuclear testing ban and to set up an Islamabad\u2013New Delhi hotline between their foreign secretaries aimed at preventing misunderstandings that might lead to nuclear war.\nChina\u2013United States.\nThe United States and China set up a defense hotline in 2008, but it has rarely been used in crises.\nChina\u2013India.\nIndia and China announced a hotline for the foreign ministers of both countries while reiterating their commitment to strengthening ties and building \"mutual political trust\". As of August 2015 the hotline was yet to be made operational.\nChina\u2013Japan.\nIn February 2013, the Senkaku Islands dispute gave renewed impetus to a China\u2013Japan hotline, which had been agreed to but due to rising tensions had not been established.\nNorth and South Korea.\nBetween North and South Korea there are over 40 direct phone lines, the first of which was opened in September 1971. Most of these hotlines run through the Panmunjeom Joint Security Area (JSA) and are maintained by the Red Cross. Since 1971, North Korea has deactivated the hotlines seven times, the last time in February 2016. After Kim Jong-un's New Years address, the border hotline was reopened on January 3, 2018.\nIndia\u2013United States.\nIn August 2015 the hotline between the White House and New Delhi became operational. The decision of establishing this hotline was taken during President Barack Obama's visit to India in January 2015. This is the first hotline connecting an Indian Prime Minister to a head of state."}
{"id": "41244", "revid": "50997882", "url": "https://en.wikipedia.org/wiki?curid=41244", "title": "Hybrid (biology)", "text": "Offspring of cross-species reproduction\nIn biology, a hybrid is the offspring resulting from combining the qualities of two organisms of different varieties, subspecies, species or genera through sexual reproduction. Generally, it means that each cell has genetic material from two different organisms, whereas an individual where some cells are derived from a different organism is called a chimera. Hybrids are not always intermediates between their parents such as in blending inheritance (a now discredited theory in modern genetics by particulate inheritance), but can show hybrid vigor, sometimes growing larger or taller than either parent. The concept of a hybrid is interpreted differently in animal and plant breeding, where there is interest in the individual parentage. In genetics, attention is focused on the numbers of chromosomes. In taxonomy, a key question is how closely related the parent species are.\nSpecies are reproductively isolated by strong barriers to hybridization, which include genetic and morphological differences, differing times of fertility, mating behaviors and cues, and physiological rejection of sperm cells or the developing embryo. Some act before fertilization and others after it. Similar barriers exist in plants, with differences in flowering times, pollen vectors, inhibition of pollen tube growth, somatoplastic sterility, cytoplasmic-genic male sterility and the structure of the chromosomes. A few animal species and many plant species, however, are the result of hybrid speciation, including important crop plants such as wheat, where the number of chromosomes has been doubled.\nA form of often intentional human-mediated hybridization is the crossing of wild and domesticated species. This is common in both traditional horticulture and modern agriculture; many commercially useful fruits, flowers, garden herbs, and trees have been produced by hybridization. One such flower, \"Oenothera lamarckiana\", was central to early genetics research into mutationism and polyploidy. It is also more occasionally done in the livestock and pet trades; some well-known wild \u00d7 domestic hybrids are beefalo and wolfdogs. Human selective breeding of domesticated animals and plants has also resulted in the development of distinct breeds (usually called cultivars in reference to plants); crossbreeds between them (without any wild stock) are sometimes also imprecisely referred to as \"hybrids\". \nHybrid humans existed in prehistory. For example, Neanderthals and anatomically modern humans are thought to have interbred as recently as 40,000 years ago.\nMythological hybrids appear in human culture in forms as diverse as the Minotaur, blends of animals, humans and mythical beasts such as centaurs and sphinxes, and the Nephilim of the Biblical apocrypha described as the wicked sons of fallen angels and attractive women.\n&lt;templatestyles src=\"Template:TOC limit/styles.css\" /&gt;\nSignificance.\nIn evolution.\nHybridization between species plays an important role in evolution, though there is much debate about its significance. Roughly 25% of plants and 10% of animals are known to form hybrids with at least one other species. One example of an adaptive benefit to hybridization is that hybrid individuals can form a \"bridge\" transmitting potentially helpful genes from one species to another when the hybrid backcrosses with one of its parent species, a process called introgression. Hybrids can also cause speciation, either because the hybrids are genetically incompatible with their parents and not each other, or because the hybrids occupy a different niche than either parent. \nHybridization is a particularly common mechanism for speciation in plants, and is now known to be fundamental to the evolutionary history of plants. Plants frequently form polyploids, individuals with more than two copies of each chromosome. Whole genome doubling has occurred repeatedly in plant evolution. When two plant species hybridize, the hybrid may double its chromosome count by incorporating the entire nuclear genome of both parents, resulting in offspring that are reproductively incompatible with either parent because of different chromosome counts.\nIn conservation.\nHuman impact on the environment has resulted in an increase in the interbreeding between regional species, and the proliferation of introduced species worldwide has also resulted in an increase in hybridization. This has been referred to as genetic pollution out of concern that it may threaten many species with extinction. Similarly, genetic erosion from monoculture in crop plants may be damaging the gene pools of many species for future breeding. \nThe conservation impacts of hybridization between species are highly debated. While hybridization could potentially threaten rare species or lineages by \"swamping\" the genetically \"pure\" individuals with hybrids, hybridization could also save a rare lineage from extinction by introducing genetic diversity. It has been proposed that hybridization could be a useful tool to conserve biodiversity by allowing organisms to adapt, and that efforts to preserve the separateness of a \"pure\" lineage could harm conservation by lowering the organisms' genetic diversity and adaptive potential, particularly in species with low populations. While endangered species are often protected by law, hybrids are often excluded from protection, resulting in challenges to conservation. \nEtymology.\nThe term hybrid is derived from Latin \"\", used for crosses such as of a tame sow and a wild boar. The term came into popular use in English in the 19th century, though examples of its use have been found from the early 17th century.\nConspicuous hybrids are popularly named with portmanteau words, starting in the 1920s with the breeding of tiger\u2013lion hybrids (liger and tigon). Examples of this include the cama, pumapard, sturddlefish, and wholphin.\nAs seen by different disciplines.\nAnimal and plant breeding.\nFrom the point of view of animal and plant breeders, there are several kinds of hybrid formed from crosses within a species, such as between different breeds. Single cross hybrids result from the cross between two true-breeding organisms which produces an F1 hybrid (first filial generation). The cross between two different homozygous lines produces an F1 hybrid that is heterozygous; having two alleles, one contributed by each parent and typically one is dominant and the other recessive. Typically, the F1 generation is also phenotypically homogeneous, producing offspring that are all similar to each other.\nDouble cross hybrids result from the cross between two different F1 hybrids (i.e., there are four unrelated grandparents).\nThree-way cross hybrids result from the cross between an F1 hybrid and an inbred line. Triple cross hybrids result from the crossing of two different three-way cross hybrids. Top cross (or \"topcross\") hybrids result from the crossing of a top quality or pure-bred male and a lower quality female, intended to improve the quality of the offspring, on average.\n Population hybrids result from the crossing of plants or animals in one population with those of another population. These include interspecific hybrids or crosses between different breeds. In biology, the result of crossing of two populations is called a synthetic population.\nIn horticulture, the term stable hybrid is used to describe an annual plant that, if grown and bred in a small monoculture free of external pollen (e.g., an air-filtered greenhouse) produces offspring that are \"true to type\" with respect to phenotype; i.e., a true-breeding organism.\nBiogeography.\nHybridization can occur in the hybrid zones where the geographical ranges of species, subspecies, or distinct genetic lineages overlap. For example, the butterfly \"Limenitis arthemis\" has two major subspecies in North America, \"L.\u00a0a.\u00a0arthemis\" (the white admiral) and \"L.\u00a0a.\u00a0astyanax\" (the red-spotted purple). The white admiral has a bright, white band on its wings, while the red-spotted purple has cooler blue-green shades. Hybridization occurs between a narrow area across New England, southern Ontario, and the Great Lakes, the \"suture region\". It is at these regions that the subspecies were formed. Other hybrid zones have formed between described species of plants and animals.\nGenetics.\nFrom the point of view of genetics, several different kinds of hybrid can be distinguished.\nA genetic hybrid carries two different alleles of the same gene, where for instance one allele may code for a lighter coat colour than the other. A structural hybrid results from the fusion of gametes that have differing structure in at least one chromosome, as a result of structural abnormalities. A numerical hybrid results from the fusion of gametes having different haploid numbers of chromosomes. A permanent hybrid results when only the heterozygous genotype occurs, as in \"Oenothera lamarckiana\", because all homozygous combinations are lethal. In the early history of genetics, Hugo de Vries supposed these were caused by mutation.\nGenetic complementation.\nGenetic complementation is a hybridization test widely used in genetics to determine whether two separately isolated mutants that have the same (or similar) phenotype are defective in the same gene or in different genes (see complementation). If a hybrid organism containing the genomes of two different mutant parental organisms displays a wild type phenotype, it is ordinarily considered that the two parental mutant organisms are defective in different genes. If the hybrid organism displays a distinctly mutant phenotype, the two mutant parental organisms are considered to be defective in the same gene. However, in some cases the hybrid organism may display a phenotype that is only weakly (or partially) wild-type, and this may reflect intragenic (interallelic) complementation.\nTaxonomy.\nFrom the point of view of taxonomy, hybrids differ according to their parentage.\nHybrids between different subspecies (such as between the dog and Eurasian wolf) are called intra-specific hybrids. Interspecific hybrids are the offspring from interspecies mating; these sometimes result in hybrid speciation. Intergeneric hybrids result from matings between different genera, such as between sheep and goats. Interfamilial hybrids, such as between chickens and guineafowl or pheasants, are reliably described but extremely rare. Interordinal hybrids (between different orders) are few, but have been engineered between the sea urchin \"Strongylocentrotus purpuratus\" (female) and the sand dollar \"Dendraster excentricus\" (male).\nBiology.\nExpression of parental traits.\nWhen two distinct types of organisms breed with each other, the resulting hybrids typically have intermediate traits (e.g., one plant parent has red flowers, the other has white, and the hybrid, pink flowers). Commonly, hybrids also combine traits seen only separately in one parent or the other (e.g., a bird hybrid might combine the yellow head of one parent with the orange belly of the other).\nMechanisms of reproductive isolation.\nInterspecific hybrids are bred by mating individuals from two species, normally from within the same genus. The offspring display traits and characteristics of both parents, but are often sterile, preventing gene flow between the species. Sterility is often attributed to the different number of chromosomes between the two species. For example, donkeys have 62 chromosomes, horses have 64 chromosomes, and mules or hinnies have 63 chromosomes. Mules, hinnies, and other normally sterile interspecific hybrids cannot produce viable gametes, because differences in chromosome structure prevent appropriate pairing and segregation during meiosis, meiosis is disrupted, and viable sperm and eggs are not formed. However, fertility in female mules has been reported with a donkey as the father.\nA variety of mechanisms limit the success of hybridization, including the large genetic difference between most species. Barriers include morphological differences, differing times of fertility, mating behaviors and cues, and physiological rejection of sperm cells or the developing embryo. Some act before fertilization; others after it.\nIn plants, some barriers to hybridization include blooming period differences, different pollinator vectors, inhibition of pollen tube growth, somatoplastic sterility, cytoplasmic-genic male sterility and structural differences of the chromosomes.\nSpeciation.\nA few animal species are the result of hybridization. The Lonicera fly is a natural hybrid. The American red wolf appears to be a hybrid of the gray wolf and the coyote, although its taxonomic status has been a subject of controversy. The European edible frog is a semi-permanent hybrid between pool frogs and marsh frogs; its population requires the continued presence of at least one of the parent species. Cave paintings indicate that the European bison is a natural hybrid of the aurochs and the steppe bison.\nPlant hybridization is more commonplace compared to animal hybridization. Many crop species are hybrids, including notably the polyploid wheats: some have four sets of chromosomes (tetraploid) or six (hexaploid), while other wheat species have (like most eukaryotic organisms) two sets (diploid), so hybridization events likely involved the doubling of chromosome sets, causing immediate genetic isolation.\nHybridization may be important in speciation in some plant groups. However, homoploid hybrid speciation (not increasing the number of sets of chromosomes) may be rare: by 1997, only eight natural examples had been fully described. Experimental studies suggest that hybridization offers a rapid route to speciation, a prediction confirmed by the fact that early generation hybrids and ancient hybrid species have matching genomes, meaning that once hybridization has occurred, the new hybrid genome can remain stable.\nMany hybrid zones are known where the ranges of two species meet, and hybrids are continually produced in great numbers. These hybrid zones are useful as biological model systems for studying the mechanisms of speciation. Recently DNA analysis of a bear shot by a hunter in the Northwest Territories confirmed the existence of naturally occurring and fertile grizzly\u2013polar bear hybrids.\nHybrid vigour.\nHybridization between reproductively isolated species often results in hybrid offspring with lower fitness than either parental. However, hybrids are not, as might be expected, always intermediate between their parents (as if there were blending inheritance), but are sometimes stronger or perform better than either parental lineage or variety, a phenomenon called heterosis, hybrid vigour, or heterozygote advantage. This is most common with plant hybrids. A transgressive phenotype is a phenotype that displays more extreme characteristics than either of the parent lines. Plant breeders use several techniques to produce hybrids, including line breeding and the formation of complex hybrids. An economically important example is hybrid maize (corn), which provides a considerable seed yield advantage over open pollinated varieties. Hybrid seed dominates the commercial maize seed market in the United States, Canada and many other major maize-producing countries.\nIn a hybrid, any trait that falls outside the range of parental variation (and is thus not simply intermediate between its parents) is considered heterotic. \"Positive heterosis\" produces more robust hybrids, they might be stronger or bigger; while the term \"negative heterosis\" refers to weaker or smaller hybrids. Heterosis is common in both animal and plant hybrids. For example, hybrids between a lion and a tigress (\"ligers\") are much larger than either of the two progenitors, while \"tigons\" (lioness \u00d7 tiger) are smaller. Similarly, the hybrids between the common pheasant (\"Phasianus colchicus\") and domestic fowl (\"Gallus gallus\") are larger than either of their parents, as are those produced between the common pheasant and hen golden pheasant (\"Chrysolophus pictus\"). Spurs are absent in hybrids of the former type, although present in both parents.\nHuman influence.\nAnthropogenic hybridization.\nHybridization is greatly influenced by human impact on the environment, through effects such as habitat fragmentation and species introductions. Such impacts make it difficult to conserve the genetics of populations undergoing introgressive hybridization. Humans have introduced species worldwide to environments for a long time, both intentionally for purposes such as biological control, and unintentionally, as with accidental escapes of individuals. Introductions can drastically affect populations, including through hybridization.\nManagement.\nThere is a kind of continuum with three semi-distinct categories dealing with anthropogenic hybridization: hybridization without introgression, hybridization with widespread introgression (backcrossing with one of the parent species), and hybrid swarms (highly variable populations with much interbreeding as well as backcrossing with the parent species). Depending on where a population falls along this continuum, the management plans for that population will change. Hybridization is currently an area of great discussion within wildlife management and habitat management. Global climate change is creating other changes such as difference in population distributions which are indirect causes for an increase in anthropogenic hybridization.\nConservationists disagree on when is the proper time to give up on a population that is becoming a hybrid swarm, or to try and save the still existing pure individuals. Once a population becomes a complete mixture, the goal becomes to conserve those hybrids to avoid their loss. Conservationists treat each case on its merits, depending on detecting hybrids within the population. It is nearly impossible to formulate a uniform hybridization policy, because hybridization can occur beneficially when it occurs \"naturally\", and when hybrid swarms are the only remaining evidence of prior species, they need to be conserved as well.\nGenetic mixing and extinction.\nRegionally developed ecotypes can be threatened with extinction when new alleles or genes are introduced that alter that ecotype. This is sometimes called genetic mixing. Hybridization and introgression, which can happen in natural and hybrid populations, of new genetic material can lead to the replacement of local genotypes if the hybrids are more fit and have breeding advantages over the indigenous ecotype or species. These hybridization events can result from the introduction of non-native genotypes by humans or through habitat modification, bringing previously isolated species into contact. Genetic mixing can be especially detrimental for rare species in isolated habitats, ultimately affecting the population to such a degree that none of the originally genetically distinct population remains.\nEffect on biodiversity and food security.\nIn agriculture and animal husbandry, the Green Revolution's use of conventional hybridization increased yields by breeding high-yielding varieties. The replacement of locally indigenous breeds, compounded with unintentional cross-pollination and crossbreeding (genetic mixing), has reduced the gene pools of various wild and indigenous breeds resulting in the loss of genetic diversity. Since the indigenous breeds are often well-adapted to local extremes in climate and have immunity to local pathogens, this can be a significant genetic erosion of the gene pool for future breeding. Therefore, commercial plant geneticists strive to breed \"widely adapted\" cultivars to counteract this tendency.\nDifferent taxa.\nIn animals.\nMammals.\nFamiliar examples of equid hybrids are the mule, a cross between a female horse and a male donkey, and the hinny, a cross between a female donkey and a male horse. Pairs of complementary types like the mule and hinny are called reciprocal hybrids. Polar bears and brown bears are another case of a hybridizing species pairs, and introgression among non-sister species of bears appears to have shaped the Ursidae family tree. Among many other mammal crosses are hybrid camels, crosses between a bactrian camel and a dromedary. There are many examples of felid hybrids, including the liger. The oldest-known animal hybrid bred by humans is the kunga equid hybrid produced as a draft animal and status symbol 4,500 years ago in Umm el-Marra, present-day Syria.\nThe first known instance of hybrid speciation in marine mammals was discovered in 2014. The clymene dolphin (\"Stenella clymene\") is a hybrid of two Atlantic species, the spinner and striped dolphins. In 2019, scientists confirmed that a skull found 30 years earlier was a hybrid between the beluga whale and narwhal, dubbed the narluga.\nBirds.\nHybridization between species is common in birds. Hybrid birds are purposefully bred by humans, but hybridization is also common in the wild. Waterfowl have a particularly high incidence of hybridization, with at least 60% of species known to produce hybrids with another species. Among ducks, mallards widely hybridize with many other species, and the genetic relationships between ducks are further complicated by the widespread gene flow between wild and domestic mallards.\nOne of the most common interspecific hybrids in geese occurs between Greylag and Canada geese (Anser anser x Branta canadensis). One potential mechanism for the occurrence of hybrids in these geese is interspecific nest parasitism, where an egg is laid in the nest of another species to be raised by non-biological parents. The chick imprints upon and eventually seeks a mate among the species that raised it, instead of the species of its biological parents. \nCagebird breeders sometimes breed bird hybrids known as mules between species of finch, such as goldfinch \u00d7 canary.\nAmphibians.\nAmong amphibians, Japanese giant salamanders and Chinese giant salamanders have created hybrids that threaten the survival of Japanese giant salamanders because of competition for similar resources in Japan.\nFish.\nAmong fish, a group of about 50 natural hybrids between Australian blacktip shark and the larger common blacktip shark was found by Australia's eastern coast in 2012.\nRussian sturgeon and American paddlefish were hybridized in captivity when sperm from the paddlefish and eggs from the sturgeon were combined, unexpectedly resulting in viable offspring. This hybrid is called a sturddlefish.\nCephalochordates.\nThe two genera \"Asymmetron\" and \"Branchiostoma\" are able to produce viable hybrid offspring, even if none have lived into adulthood so far, despite the parents' common ancestor living tens of millions of years ago.\nInsects.\nAmong insects, so-called killer bees were accidentally created during an attempt to breed a strain of bees that would both produce more honey and be better adapted to tropical conditions. It was done by crossing a European honey bee and an African bee.\nThe \"Colias eurytheme\" and \"C. philodice\" butterflies have retained enough genetic compatibility to produce viable hybrid offspring. Hybrid speciation may have produced the diverse \"Heliconius\" butterflies, but that is disputed.\nThe two closely related harvester ant species \"Pogonomyrmex barbatus\" and \"Pogonomyrmex rugosus\" have evolved to depend on hybridization. When a queen fertilizes her eggs with sperm from males of her own species, the offspring is always new queens. And when she fertilizes the eggs with sperm from males of the other species, the offspring is always sterile worker ants (and because ants are haplodiploid, unfertilized eggs become males). Without mating with males of the other species, the queens are unable to produce workers, and will fail to establish a colony of their own.\nIn plants.\nPlant species hybridize more readily than animal species, and the resulting hybrids are fertile more often. Many plant species are the result of hybridization, combined with polyploidy, which duplicates the chromosomes. Chromosome duplication allows orderly meiosis and so viable seed can be produced.\nPlant hybrids are generally given names that include an \"\u00d7\" (not in italics), such as \"Platanus\" \u00d7 \"hispanica\" for the London plane, a natural hybrid of \"P. orientalis\" (oriental plane) and \"P. occidentalis\" (American sycamore). The parent's names may be kept in their entirety, as seen in \"Prunus persica\" \u00d7 \"Prunus americana\", with the female parent's name given first, or if not known, the parent's names given alphabetically.\nPlant species that are genetically compatible may not hybridize in nature for various reasons, including geographical isolation, differences in flowering period, or differences in pollinators. Species that are brought together by humans in gardens may hybridize naturally, or hybridization can be facilitated by human efforts, such as altered flowering period or artificial pollination. Hybrids are sometimes created by humans to produce improved plants that have some of the characteristics of each of the parent species. Much work is now being done with hybrids between crops and their wild relatives to improve disease resistance or climate resilience for both agricultural and horticultural crops.\nSome crop plants are hybrids from different genera (intergeneric hybrids), such as Triticale, \u00d7 \"Triticosecale\", a wheat\u2013rye hybrid. Most modern and ancient wheat breeds are themselves hybrids; bread wheat, \"Triticum aestivum\", is a hexaploid hybrid of three wild grasses. Several commercial fruits including loganberry (\"Rubus\" \u00d7 \"loganobaccus\") and grapefruit (\"Citrus\" \u00d7 \"paradisi\") are hybrids, as are garden herbs such as peppermint (\"Mentha\" \u00d7 \"piperita\"), and trees such as the London plane (\"Platanus\" \u00d7 \"hispanica\"). Among many natural plant hybrids is \"Iris albicans\", a sterile hybrid that spreads by rhizome division, and \"Oenothera lamarckiana\", a flower that was the subject of important experiments by Hugo de Vries that produced an understanding of polyploidy.\nSterility in a non-polyploid hybrid is often a result of chromosome number; if parents are of differing chromosome pair number, the offspring will have an odd number of chromosomes, which leaves them unable to produce chromosomally balanced gametes. While that is undesirable in a crop such as wheat, for which growing a crop that produces no seeds would be pointless, it is an attractive attribute in some fruits. Triploid bananas and watermelons are intentionally bred because they produce no seeds and are also parthenocarpic.\nIn fungi.\nHybridization between fungal species is common and well established, particularly in yeast. Yeast hybrids are widely found and used in human-related activities, such as brewing and winemaking. The production of lager beers for instance are known to be carried out by the yeast \"Saccharomyces pastorianus\", a cryotolerant hybrid between \"Saccharomyces cerevisiae\" and \"Saccharomyces eubayanus\", which allows fermentation at low temperatures. \nIn humans.\nThere is evidence of hybridization between modern humans and other species of the genus \"Homo\". In 2010, the Neanderthal genome project showed that 1\u20134% of DNA from all people living today, apart from most Sub-Saharan Africans, is of Neanderthal heritage. Analyzing the genomes of 600 Europeans and East Asians found that combining them covered 20% of the Neanderthal genome that is in the modern human population. Ancient human populations lived and interbred with Neanderthals, Denisovans, and at least one other extinct \"Homo\" species. Thus, Neanderthal and Denisovan DNA has been incorporated into human DNA by introgression.\nIn 1998, a complete prehistorical skeleton found in Portugal, the Lapedo child, had features of both anatomically modern humans and Neanderthals. Some ancient human skulls with especially large nasal cavities and unusually shaped braincases represent human-Neanderthal hybrids. A 37,000- to 42,000-year-old human jawbone found in Romania's Oase cave contains traces of Neanderthal ancestry from only four to six generations earlier. All genes from Neanderthals in the current human population are descended from Neanderthal fathers and human mothers.\nMythology.\nFolk tales and myths sometimes contain mythological hybrids; the Minotaur was the offspring of a human, Pasipha\u00eb, and a white bull. More often, they are composites of the physical attributes of two or more kinds of animals, mythical beasts, and humans, with no suggestion that they are the result of interbreeding, as in the centaur (man/horse), chimera (goat/lion/snake), hippocamp (fish/horse), and sphinx (woman/lion). The Old Testament mentions a first generation of half-human hybrid giants, the Nephilim, while the apocryphal Book of Enoch describes the Nephilim as the wicked sons of fallen angels and attractive women.\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41245", "revid": "6127189", "url": "https://en.wikipedia.org/wiki?curid=41245", "title": "Hybrid balance", "text": "Telecommunications specification\nIn telecommunications, a hybrid balance is an expression of the degree of electrical symmetry between two impedances connected to two conjugate sides of a hybrid coil or resistance hybrid. It is usually expressed in dB. \nIf the respective impedances of the branches of the hybrid that are connected to the conjugate sides of the hybrid are known, hybrid balance may be computed by the formula for return loss. \nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41246", "revid": "76", "url": "https://en.wikipedia.org/wiki?curid=41246", "title": "Hybrid transformer", "text": "Type of electrical transformer\nA hybrid transformer (also known as a bridge transformer, hybrid coil, or just hybrid) is a type of directional coupler which is designed to be configured as a circuit having four ports that are conjugate in pairs, implemented using one or more transformers. It is a particular case of the more general concept of a hybrid coupler.\nA signal arriving at one port is divided equally between the two adjacent ports but does not appear at the opposite port. In the schematic diagram, the signal into W splits between X and Z, and no signal passes to Y. Similarly, signals into X split to W and Y with none to Z, etc.\nCorrect operation requires matched characteristic impedance at all four ports. Forms of hybrid other than transformer coils are possible; any format of directional coupler can be designed to be a hybrid. These formats include transmission lines and waveguides.\nMotivation.\nThe primary use of a voiceband hybrid transformer is to convert between 2-wire and 4-wire operation in sequential sections of a communications circuit, for example in a four-wire terminating set. Such conversion was necessary when repeaters were introduced in a 2-wire circuit, a frequent practice at early 20th century telephony. Without hybrids, the output of one amplifier feeds directly into the input of the other, resulting in uncontrollable feedback oscillation (upper diagram). By using hybrids, the outputs and inputs are isolated, resulting in correct 2-wire repeater operation. Late in the century, this practice became rare but hybrids continued in use in line cards.\nImplementations.\nHybrids are realized using transformers. Two versions of transformer hybrids were used, the single transformer version providing unbalanced outputs with one end grounded, and the double transformer version providing balanced ports. \nSingle transformer.\nFor use in 2-wire repeaters, the single transformer version suffices, since amplifiers in the repeaters have grounded inputs and outputs. X, Y, and Z share a common ground. As shown at left, signal into W, the 2-wire port, will appear at X and Z. But since Y is bridged from center of coil to center of X and Z, no signal appears. Signal into X will appear at W and Y. But signal at Z is the difference of what appears at Y and, through the transformer coil, at W, which is zero. Similar reasoning proves both pairs, W &amp; Y, X &amp; Z, are conjugates.\nDouble transformer.\nWhen both the 2-wire and the 4-wire circuits must be balanced, double transformer hybrids are used, as shown at right. Signal into port W splits between X and Z, but due to reversed connection to the windings, cancel at port Y. Signal into port X goes to W and Y. But due to reversed connection to ports W and Y, Z gets no signal. Thus the pairs, W &amp; Y, X &amp; Z, are conjugates.\nApplications.\nTelephone hybrids are used in telephone exchanges to convert the 4-wire appearance to the 2-wire last mile connection to the subscriber's telephone. A different kind of hybrid is used in telephone handsets to convert the four wires of the transmitter (earpiece) and receiver (microphone) to the 2-wire line connection. This kind of hybrid is more commonly called an \"induction coil\" due to its derivation from high-voltage induction coils. It does not produce a high voltage, but like the high-voltage variety, it is a step-up transformer in order to impedance match the low-impedance carbon button transmitter to the higher impedance parts of the system. The simple induction coil later evolved into a form of hybrid as a sidetone reduction measure, or volume of microphone output that was fed back to the earpiece. Without this, the phone user's own voice would be louder in the earpiece than the other party's. Today, the transformer version of the hybrid has been replaced by resistor networks and compact IC versions, which use integrated circuit electronics to do the job of the hybrid coil.\nRadio-frequency hybrids are used to split radio signals, including television. The splitter divides the antenna signal to feed multiple receivers.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nExternal links.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41247", "revid": "31530", "url": "https://en.wikipedia.org/wiki?curid=41247", "title": "Hybrid routing", "text": ""}
{"id": "41248", "revid": "28481209", "url": "https://en.wikipedia.org/wiki?curid=41248", "title": "Hydroxyl ion absorption", "text": "Transmission loss in optical fibers\nHydroxyl ion absorption is the absorption in optical fibers of electromagnetic radiation, including the near-infrared, due to the presence of trapped hydroxyl ions remaining from water as a contaminant. \nThe hydroxyl (OH\u2212) ion can penetrate glass during or after product fabrication, resulting in significant attenuation of discrete optical wavelengths, \"e.g.\", centred at 1.383 \u03bcm, used for communications via optical fibres.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41249", "revid": "253891", "url": "https://en.wikipedia.org/wiki?curid=41249", "title": "Identification, friend or foe", "text": ""}
{"id": "41250", "revid": "43446663", "url": "https://en.wikipedia.org/wiki?curid=41250", "title": "Identifier", "text": "Name that identifies a specific entity\nAn identifier is a name that identifies (that is, labels the identity of) either a unique object or a unique \"class\" of objects, where the \"object\" or class may be an idea, person, physical countable object (or class thereof), or physical noncountable substance (or class thereof). The abbreviation ID often refers to identity, identification (the process of identifying), or an identifier (that is, an instance of identification). An identifier may be a word, number, letter, symbol, or any combination of those.\nThe words, numbers, letters, or symbols may follow an encoding system (wherein letters, digits, words, or symbols \"stand for\" [represent] ideas or longer names) or they may simply be arbitrary. When an identifier follows an encoding system, it is often referred to as a code or ID\u00a0code. For instance the ISO/IEC 11179 metadata registry standard defines a code as \"system of valid symbols that substitute for longer values\" in contrast to identifiers without symbolic meaning. Identifiers that do not follow any encoding scheme are often said to be arbitrary\u00a0IDs; they are arbitrarily assigned and have no greater meaning. (Sometimes identifiers are called \"codes\" even when they are actually arbitrary, whether because the speaker believes that they have deeper meaning or simply because they are speaking casually and imprecisely.)\nThe unique identifier (UID) is an identifier that refers to \"only one instance\"\u2014only one particular object in the universe. A part number is an identifier, but it is not a \"unique\" identifier\u2014for that, a serial number is needed, to identify \"each instance\" of the part design. Thus the \"identifier\" \"Model\u00a0T\" identifies the \"class\" \"(model)\" of automobiles that Ford's Model\u00a0T comprises; whereas the \"unique identifier\" \"Model\u00a0T\u00a0Serial Number\u00a0159,862\" identifies one specific member of that class\u2014that is, one particular Model\u00a0T car, owned by one specific person.\nThe concepts of \"name\" and \"identifier\" are denotatively equal, and the terms are thus denotatively synonymous; but they are not always connotatively synonymous, because code names and ID numbers are often connotatively distinguished from names in the sense of traditional natural language naming. For example, both \"Jamie Zawinski\" and \"Netscape employee number 20\" are identifiers for the same specific human being; but normal English-language connotation may consider \"Jamie Zawinski\" a \"name\" and not an \"identifier\", whereas it considers \"Netscape\u00a0employee\u00a0number\u00a020\" an \"identifier\" but not a \"name.\" This is an emic indistinction rather than an etic one.\nMetadata.\nIn metadata, an identifier is a language-independent label, sign or token that uniquely identifies an object within an identification scheme. The suffix \"identifier\" is also used as a representation term when naming a data element.\nID codes may inherently carry metadata along with them. For example, when you know that the food package in front of you has the identifier \"2011-09-25T15:42Z-MFR5-P02-243-45\", you not only have that data, you also have the metadata that tells you that it was packaged on September 25, 2011, at 3:42pm UTC, manufactured by Licensed Vendor Number 5, at the Peoria, IL, USA plant, in Building 2, and was the 243rd package off the line in that shift, and was inspected by Inspector Number 45.\nArbitrary identifiers might lack metadata. For example, if a food package just says 100054678214, its ID may not tell anything except identity\u2014no date, manufacturer name, production sequence rank, or inspector number. In some cases, arbitrary identifiers such as sequential serial numbers leak information (i.e. the German tank problem). Opaque identifiers\u2014identifiers designed to avoid leaking even that small amount of information\u2014include \"really opaque pointers\" and Version 4 UUIDs.\nIn computer science.\nIn computer science, identifiers (IDs) are lexical tokens that name entities. Identifiers are used extensively in virtually all information processing systems. Identifying entities makes it possible to refer to them, which is essential for any kind of symbolic processing.\nIn computer languages.\nIn computer languages, identifiers are tokens (also called symbols) which name language entities. Some of the kinds of entities an identifier might denote include variables, types, labels, subroutines, and packages.\nAmbiguity.\nIdentifiers (IDs) versus Unique identifiers (UIDs).\nA resource may carry multiple identifiers. Typical examples are:\nThe inverse is also possible, where multiple resources are represented with the same identifier (discussed below).\nImplicit context and namespace conflicts.\nMany codes and nomenclatural systems originate within a small namespace. Over the years, some of them bleed into larger namespaces (as people interact in ways they formerly had not, e.g., cross-border trade, scientific collaboration, military alliance, and general cultural interconnection or assimilation). When such dissemination happens, the limitations of the original naming convention, which had formerly been latent and moot, become painfully apparent, often necessitating retronymy, synonymity,\ntranslation/transcoding, and so on. Such limitations generally accompany the shift away from the original context to the broader one. Typically the system shows implicit context (context was formerly assumed, and narrow), lack of capacity (e.g., low number of possible IDs, reflecting the outmoded narrow context), lack of extensibility (no features defined and reserved against future needs), and lack of specificity and disambiguating capability (related to the context shift, where longstanding uniqueness encounters novel nonuniqueness). Within computer science, this problem is called naming collision. The story of the origination and expansion of the CODEN system provides a good case example in a recent-decades, technical-nomenclature context. The capitalization variations seen with specific designators reveals an instance of this problem occurring in natural languages, where the proper\u00a0noun/common\u00a0noun distinction (and its complications) must be dealt with. A universe in which every object had a UID would not need any namespaces, which is to say that it would constitute one gigantic namespace; but human minds could never keep track of, or semantically interrelate, so many UIDs.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41251", "revid": "73235", "url": "https://en.wikipedia.org/wiki?curid=41251", "title": "Image antenna", "text": "In telecommunications and antenna design, an image antenna is an electrical mirror-image of an antenna element formed by the radio waves reflecting from a conductive surface called a ground plane, such as the surface of the earth. It is used as a geometrical technique in calculating the radiation pattern of the antenna.\nWhen a radio antenna is mounted near a conductive surface such as the Earth or a flat metal plate or screen, the radio waves directed toward the surface reflect off it. The radiation received at a distant point is the sum of two contributions: the waves that travel directly from the antenna to the point, and the waves that reach the point after reflecting off the ground plane. Because of the reflection, these second waves appear to come from a second antenna behind the plane, just as a visible object in front of a flat mirror forms a virtual image that seems to lie behind the mirror. The radiation pattern of the antenna is exactly the same as it would be if the ground plane were replaced by a mirror image of the antenna, located an equal distance behind the plane. This second apparent source of radio waves is the image antenna.\nThe image antenna is used in calculating electric field vectors, magnetic field vectors, and electromagnetic fields emanating from the real antenna, particularly in the vicinity of the antenna and along the ground. Each charge and current in the real antenna has its counterpart in the image, and may also be considered as a source of radiation. \nTo form an image of the antenna, the ground plane need not be grounded to the Earth. Many antenna types, such as reflective array antennas, use flat surfaces of metal or metal screen to reflect radio waves from the antenna elements, and these can be analyzed using image antennas. If there is more than one reflective surface in the antenna, as in a corner reflector antenna, each surface forms its own image of the antenna elements. In order to form an image, the ground plane surface must generally have dimensions of at least a quarter-wavelength of the radio waves used.\nSee also.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41252", "revid": "1234701", "url": "https://en.wikipedia.org/wiki?curid=41252", "title": "Image frequency", "text": ""}
{"id": "41253", "revid": "139104", "url": "https://en.wikipedia.org/wiki?curid=41253", "title": "Image rejection ratio", "text": ""}
{"id": "41254", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41254", "title": "Improved-definition television", "text": "Improved-definition television (IDTV) or enhanced-quality television transmitters and receivers exceed the performance requirements of the NTSC standard, while remaining within the general parameters of NTSC emissions standards. \nIDTV improvements may be made at the television transmitter or receiver. Improvements include enhancements in encoding, digital filtering, scan interpolation, interlaced line scanning, and ghost cancellation. \nIDTV improvements must allow the TV signal to be transmitted and received in the standard 4:3 aspect ratio.\nThe only relevant implementation of IDTV for NTSC-based broadcasts before the introduction of full-digital TV distribution (DTV) was the Japanese Clear-Vision. In European countries, PALplus and MAC had a similar role.\nThe more commonly used term for advanced display technology before the advent of high-definition television (HDTV) was enhanced-definition television (EDTV), used for instance for plasma TV sets with a in the early 2000s.\nReferences.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41255", "revid": "31530", "url": "https://en.wikipedia.org/wiki?curid=41255", "title": "Independent clock", "text": ""}
{"id": "41256", "revid": "28481209", "url": "https://en.wikipedia.org/wiki?curid=41256", "title": "Index-matching material", "text": "In optics, an index-matching material is a substance, usually a liquid, cement (adhesive), or gel, which has an index of refraction that closely approximates that of another object (such as a lens, material, fiber-optic, etc.).\nWhen two substances with the same index are in contact, light passes from one to the other with neither reflection nor refraction. As such, they are used for various purposes in science, engineering, and art.\nFor example, in a popular home experiment, a glass rod is made almost invisible by immersing it in an index-matched transparent fluid such as mineral spirits.\nIn microscopy.\nIn light microscopy, oil immersion is a technique used to increase the resolution of a microscope. This is achieved by immersing both the objective lens and the specimen in a transparent oil of high refractive index, thereby increasing the numerical aperture of the objective lens.\nImmersion oils are transparent oils that have specific optical and viscosity characteristics necessary for use in microscopy. Typical oils used have an index of refraction around 1.515. An oil immersion objective is an objective lens specially designed to be used in this way. The index of the oil is typically chosen to match the index of the microscope lens glass, and of the cover slip.\nFor more details, see the main article, oil immersion. Some microscopes also use other index-matching materials besides oil; see water immersion objective and solid immersion lens.\nIn fiber optics.\nIn fiber optics and telecommunications, an index-matching material may be used in conjunction with pairs of mated connectors or with mechanical splices to reduce signal reflected in the guided mode (known as return loss) (see Optical fiber connector). Without the use of an index-matching material, Fresnel reflections will occur at the smooth end faces of a fiber unless there is no fiber-air interface or other significant mismatch in refractive index. These reflections may be as high as \u221214\u00a0dB (i.e., 14\u00a0dB below the optical power of the incident signal). When the reflected signal returns to the transmitting end, it may be reflected again and return to the receiving end at a level that is 28\u00a0dB plus twice the fiber loss below the direct signal. The reflected signal will also be delayed by twice the delay time introduced by the fiber. The twice-reflected, delayed signal superimposed on the direct signal may noticeably degrade an analog baseband intensity-modulated video signal. Conversely, for digital transmission, the reflected signal will often have no practical effect on the detected signal seen at the decision point of the digital optical receiver except in marginal cases where bit-error ratio is significant. However, certain digital transmitters such as those employing a Distributed Feedback Laser may be affected by back reflection and then fall outside specifications such as Side Mode Suppression Ratio, potentially degrading system bit error ratio, so networking standards intended for DFB lasers may specify a back-reflection tolerance such as \u221210\u00a0dB for transmitters so that they remain within specification even without index matching. This back-reflection tolerance might be achieved using an optical isolator or by way of reduced coupling efficiency.\nFor some applications, instead of standard polished connectors (e.g. FC/PC), angle polished connectors (e.g. FC/APC) may be used, whereby the non-perpendicular polish angle greatly reduces the ratio of reflected signal launched into the guided mode even in the case of a fiber-air interface.\nIn experimental fluid dynamics.\nIndex matching is used in liquid-liquid and liquid-solid (Multiphase flow) experimental systems to minimise the distortions that occur in these systems, this is particularly important for systems with many interfaces which become optically inaccessible. Matching the refractive index minimises reflection, refraction, diffraction and rotations that occurs at the interfaces allowing access to regions that would otherwise be inaccessible to optical measurements. This is particularly important for advanced optical measurements like Laser-induced fluorescence, Particle image velocimetry and Particle tracking velocimetry to name a few.\nIn art conservation.\nIf a sculpture is broken into several pieces, art conservators may reattach the pieces using an adhesive such as Paraloid B-72 or epoxy. If the sculpture is made of a transparent or semitransparent material (such as glass), the seam where the pieces are attached will usually be much less noticeable if the refractive index of the adhesive matches the refractive index of the surrounding object. Therefore, art conservators may measure the index of objects and then use an index-matched adhesive. Similarly, losses (missing sections) in transparent or semitransparent objects are often filled using an index-matched material.\nIn optical component adhesives.\nCertain optical components, such as a Wollaston prism or Nicol prism, are made of multiple transparent pieces that are directly attached to each other. The adhesive is usually index-matched to the pieces. Historically, Canada balsam was used in this application, but it is now more common to use epoxy or other synthetic adhesives.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41258", "revid": "13505698", "url": "https://en.wikipedia.org/wiki?curid=41258", "title": "Inductive coupling", "text": "Electrical circuit coupling using induction\nIn electrical engineering, two conductors are said to be inductively coupled or magnetically coupled when they are configured in a way such that change in current through one wire induces a voltage across the ends of the other wire through electromagnetic induction. A changing current through the first wire creates a changing magnetic field around it by Ampere's circuital law. The changing magnetic field induces an electromotive force (EMF) voltage in the second wire by Faraday's law of induction. The amount of inductive coupling between two conductors is measured by their mutual inductance. \nThe coupling between two wires can be increased by winding them into coils and placing them close together on a common axis, so the magnetic field of one coil passes through the other coil. Coupling can also be increased by a magnetic core of a ferromagnetic material like iron or ferrite in the coils, which increases the magnetic flux. The two coils may be physically contained in a single unit, as in the primary and secondary windings of a transformer, or may be separated. Coupling may be intentional or unintentional. Unintentional inductive coupling can cause signals from one circuit to be induced into a nearby circuit, this is called cross-talk, and is a form of electromagnetic interference.\nAn inductively coupled transponder consists of a solid state transceiver chip connected to a large coil that functions as an antenna. When brought within the oscillating magnetic field of a reader unit, the transceiver is powered up by energy inductively coupled into its antenna and transfers data back to the reader unit inductively. \nMagnetic coupling between two magnets can also be used to mechanically transfer power without contact, as in the magnetic gear.\nUses.\nInductive coupling is widely used throughout electrical technology; examples include:\nLow-frequency induction.\nLow-frequency induction can be a dangerous form of inductive coupling when it happens inadvertently. For example, if a long-distance metal pipeline is installed along a right of way in parallel with a high-voltage power line, the power line can induce current on the pipe. Since the pipe is a conductor, insulated by its protective coating from the earth, it acts as a secondary winding for a long, drawn out transformer whose primary winding is the power line. Voltages induced on the pipe are then a hazard to people operating valves or otherwise touching metal parts of the metal pipeline.\nReducing low-frequency magnetic fields may be necessary when dealing with electronics, as sensitive circuits in close proximity to an instrument with a power transformer may pickup the mains frequency. Twisted wires (e.g. in networking cables) are an effective way of reducing the interference as signals induced in the successive twists cancel. Magnetic shielding is also an effective way of reducing unwanted inductive coupling, though moving the source of the magnetic field away from sensitive electronics is the simplest solution if possible.\nAlthough induced currents can be harmful, they can also be helpful. Electrical distribution line engineers use inductive coupling to tap power for cameras on towers and at substations that allow remote monitoring of the facilities. Using this they can watch from anywhere and not need to worry about changing camera batteries or solar panel maintenance.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41259", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41259", "title": "Information-bearer channel", "text": "Type of communication channel\nIn telecommunications, an information-bearer channel is one of: "}
{"id": "41260", "revid": "119438", "url": "https://en.wikipedia.org/wiki?curid=41260", "title": "Information system (2nd version)", "text": ""}
{"id": "41261", "revid": "15936761", "url": "https://en.wikipedia.org/wiki?curid=41261", "title": "Information systems security", "text": ""}
{"id": "41262", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41262", "title": "Information-transfer transaction", "text": "A transaction is a change of state, an information-transfer transaction is a transaction in which one of the following changes occurs: content, ownership, location, format, etc. An information-transfer transaction usually consists of three consecutive phases called the access phase, the information transfer phase, and the disengagement phase. Examples of these consecutive phases are the copying and transporting of information. Once a transaction occurs there are also costs to consider, which are associated with that certain transaction. When it comes to the transfer of information some transaction costs include time and means (money).\nHistory of Information-transfer transactions.\nThere are many social systems and devices that have contributed to information-transfer transactions; starting from people writing letters using postal systems to emailing using information technology. Two main examples of information-transfer transactions technology development is the copying and transportation of information.\nHistory of Copying.\nCopying is the process of duplicating information with the change of location or format of the original information. The transfer transaction of information through copying has been going on for ages and there has been many advances in technology to decrease the time it takes to make copies of said information. The art of copying started with people having to write a copy out by hand, then the printing press, all the way to digital copying with ICTs. These developments lead to quicker information-transfer transactions in the form of distributing copies of original information to others through a changes of location or format.\nHistory of Transporting.\nTransporting is the movement of information with the change of location or ownership of the original information. The transfer transaction of information through transporting has been going on for ages and there has been social and technological developments to decrease the time it takes for information to change ownership or location. The transportation of information started with people sending letters by foot, then by horse, the public and international postal service, all the way down to technology networks. It is these developments which led to the ability to send information further and quicker through information-transfer transactions.\nTransaction Costs.\nEvery time a transaction occurs, there are always costs to be considered. In the case of information-transfer transactions, one most consider the costs of time and means (money). Both of these costs are corollated with one another in that to decrease one, you must increase the other. For example, say Person #1 sends a letter through the mail, while Person #2 sends letters through email. For Person #1 to send their letter they had to buy paper, means of writing, envelopes, stamps, etc., while Person #2 needed to buy a source of electricity, internet, computer technology, etc. to send an email. It seems like Person #1 has lower transaction costs then Person #2 in terms of means; however, when you look at both information-transfer transactions in terms of time that is a different story. For Person #1 although they had little costs in sending their letter, the time it takes for the transfer of that letter is about 3+ days, while Person #2's transfer through email happens in less than minutes, but they endured high mean costs. Therefore, for information-transfer transaction times to decrease, the costs of means have to increase and vice versa.\nTelecommunication.\nIn telecommunications, an information-transfer transaction is a coordinated sequence of user and communications system actions that cause information present at a source user to become present at a destination user.\nReferences.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41263", "revid": "4635", "url": "https://en.wikipedia.org/wiki?curid=41263", "title": "Injection laser diode", "text": ""}
{"id": "41264", "revid": "46034679", "url": "https://en.wikipedia.org/wiki?curid=41264", "title": "Input", "text": "Input may refer to:\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "41265", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41265", "title": "Insertion gain", "text": "In telecommunications, insertion gain is the gain resulting from the insertion of a device in a transmission line, expressed as the ratio of the signal power delivered to that part of the line following the device to the signal power delivered to that same part before insertion. Gains less than unity indicate \"insertion loss\". Incident power is made of two part, the reflection from the device and the power absorbed by the device.\nInsertion gain is usually expressed in decibels.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41266", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41266", "title": "Insertion loss", "text": "Loss of signal transmission power when devices are connected to a transmission line\nIn telecommunications, insertion loss is the loss of signal power resulting from the insertion of a device in a transmission line or optical fiber and is usually expressed in decibels (dB). \nIf the power transmitted to the load before insertion is \"P\"T and the power received by the load after insertion is \"P\"R, then the insertion loss in decibels is given by,\nformula_1\nElectronic filters.\nInsertion loss is a figure of merit for an electronic filter and this data is generally specified with a filter. Insertion loss is defined as a ratio of the signal level in a test configuration without the filter installed (formula_2) to the signal level with the filter installed (formula_3). This ratio is described in decibels by the following equation:\nformula_4\nFor passive filters, formula_3 will be smaller than formula_2. In this case, the insertion loss is positive and measures how much smaller the signal is after adding the filter.\nLink with scattering parameters.\nIn case the two measurement ports use the same reference impedance, the insertion loss (formula_7) is defined as: \nformula_8.\nHere formula_9 is one of the scattering parameters. Insertion loss is the extra loss produced by the introduction of the DUT between the 2 reference planes of the measurement. The extra loss can be introduced by intrinsic loss in the DUT and/or mismatch. In case of extra loss the insertion loss is defined to be positive."}
{"id": "41267", "revid": "10689882", "url": "https://en.wikipedia.org/wiki?curid=41267", "title": "Inside plant", "text": "In telecommunications, the term inside plant has the following meanings:\nAround the turn of the 21st century, DSLAMs became an important part of telephone company inside plant. Inside plant will also have distribution frames and other equipment including passive optical network (name depends on the Service Provider).\nPower.\nA typical power system for a switching office in an inside plant consists of the elements listed below: \nFor safety and reliability reasons, it is desirable that all telecommunications loads be DC powered with minimal AC-powered devices used. Telcordia http://, contains detailed industry requirements for using power in an inside plant.\nBoth integrated and isolated bonding networks as per Telcordia http://, are a technically viable means to ground and bond the equipment in a safe and effective manner. However, the integrated or mesh bonding schemes are preferred over isolated bonding networks because of the added costs and efforts required to manage, control, and maintain the isolation for the equipment, particularly during equipment upgrade and modifications to the plant. This preference is based on a pragmatic desire for lower costs and ease of management, and to simplify operations during plant modifications/upgrades.\nFor a comprehensive analysis of the energy efficiency and environmental soundness of a power system, one should ideally consider a wider range of factors than strict energy conversion AC-to-DC power efficiency. These environmental factors cover a wider vision known as Industrial Ecology within which each manufacturing step of the products need to be considered from a Design for Environment (DfE) factors standpoint."}
{"id": "41268", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41268", "title": "Intelligent Network", "text": "Standard network architecture for telecom networks\nThe Intelligent Network (IN) is the standard network architecture specified in the ITU-T Q.1200 series recommendations. It is intended for fixed as well as mobile telecom networks. It allows operators to differentiate themselves by providing value-added services in addition to the standard telecom services such as PSTN, ISDN on fixed networks, and GSM services on mobile phones or other mobile devices.\nThe intelligence is provided by network nodes on the service layer, distinct from the switching layer of the core network, as opposed to solutions based on intelligence in the core switches or equipment. The IN nodes are typically owned by telecommunications service providers such as a telephone company or mobile phone operator.\nIN is supported by the Signaling System #7 (SS7) protocol between network switching centers and other network nodes owned by network operators.\nHistory and key concepts.\nThe IN concepts, architecture and protocols were originally developed as standards by the ITU-T which is the standardization committee of the International Telecommunication Union; prior to this a number of telecommunications providers had proprietary implementations. The primary aim of the IN was to enhance the core telephony services offered by traditional telecommunications networks, which usually amounted to making and receiving voice calls, sometimes with call divert. This core would then provide a basis upon which operators could build services in addition to those already present on a standard telephone exchange.\nA complete description of the IN emerged in a set of ITU-T standards named http:// to http://, or Capability Set One (CS-1) as they became known. The standards defined a complete architecture including the architectural view, state machines, physical implementation and protocols. They were universally embraced by telecom suppliers and operators, although many variants were derived for use in different parts of the world (see Variants below).\nFollowing the success of CS-1, further enhancements followed in the form of CS-2. Although the standards were completed, they were not as widely implemented as CS-1, partly because of the increasing power of the variants, but also partly because they addressed issues which pushed traditional telephone exchanges to their limits.\nThe major driver behind the development of the IN was the need for a more flexible way of adding sophisticated services to the existing network. Before the IN was developed, all new features and/or services had to be implemented directly in the core switch systems. This made for long release cycles as the software testing had to be extensive and thorough to prevent the network from failing. With the advent of the IN, most of these services (such as toll-free numbers and geographical number portability) were moved out of the core switch systems and into self-contained nodes, creating a modular and more secure network that allowed the service providers themselves to develop variations and value-added services to their networks without submitting a request to the core switch manufacturer and waiting for the long development process. The initial use of IN technology was for number translation services, e.g. when translating toll-free numbers to regular PSTN numbers; much more complex services have since been built on the IN, such as Custom Local Area Signaling Services (CLASS) and prepaid telephone calls.\nSS7 architecture.\nThe main concepts (functional view) surrounding IN services or architecture are connected with SS7 architecture:\nProtocols.\nThe core elements described above use standard protocols to communicate with each other. The use of standard protocols allows different manufacturers to concentrate on different parts of the architecture and be confident that they will all work together in any combination.\nThe interfaces between the SSP and the SCP are SS7 based and have similarities with TCP/IP protocols. The SS7 protocols implement much of the OSI seven-layer model. This means that the IN standards only had to define the application layer, which is called the Intelligent Networks Application Part or INAP. The INAP messages are encoded using ASN.1.\nThe interface between the SCP and the SDP is defined in the standards to be an X.500 Directory Access Protocol or DAP. A more lightweight interface called LDAP has emerged from the IETF which is considerably simpler to implement, so many SCPs have implemented that instead.\nVariants.\nThe core CS-1 specifications were adopted and extended by other standards bodies. European flavours were developed by ETSI, American flavours were developed by ANSI, and Japanese variants also exist. The main reasons for producing variants in each region was to ensure interoperability between equipment manufactured and deployed locally (for example different versions of the underlying SS7 protocols exist between the regions).\nNew functionality was also added which meant that variants diverged from each other and the main ITU-T standard. The biggest variant was called Customised Applications for Mobile networks Enhanced Logic, or CAMEL for short. This allowed for extensions to be made for the mobile phone environment, and allowed mobile phone operators to offer the same IN services to subscribers while they are roaming as they receive in the home network.\nCAMEL has become a major standard in its own right and is currently maintained by 3GPP. The last major release of the standard was CAMEL phase 4. It is the only IN standard currently being actively worked on.\nBellcore (subsequently Telcordia Technologies) developed the Advanced Intelligent Network (AIN) as the variant of Intelligent Network for North America, and performed the standardization of the AIN on behalf of the major US operators. The original goal of AIN was AIN 1.0, which was specified in the early 1990s (\"AIN Release 1\", Bellcore SR-NWT-002247, 1993). AIN 1.0 proved technically infeasible to implement, which led to the definition of simplified AIN 0.1 and AIN 0.2 specifications. In North America, Telcordia SR-3511 (originally known as TA-1129+) and GR-1129-CORE protocols serve to link switches with the IN systems such as Service Control Points (SCPs) or Service Nodes. SR-3511 details a TCP/IP-based protocol which directly connects the SCP and Service Node. GR-1129-CORE provides generic requirements for an ISDN-based protocol which connects the SCP to the Service Node via the SSP.\nFuture.\nWhile activity in development of IN standards has declined in recent years, there are many systems deployed across the world which use this technology. The architecture has proved to be not only stable, but also a continuing source of revenue with new services added all the time. Manufacturers continue to support the equipment and obsolescence is not an issue.\nNevertheless, new technologies and architectures have emerged, especially in the area of VoIP and SIP. More attention is being paid to the use of APIs in preference to protocols like INAP, and new standards have emerged in the form of JAIN and Parlay. From a technical viewpoint, the SCE began to move away from its proprietary graphical origins towards a Java application server environment.\nThe meaning of \"intelligent network\" is evolving in time, largely driven by breakthroughs in computation and algorithms. From networks enhanced by more flexible algorithms and more advanced protocols, to networks designed using data-driven models to AI enabled networks.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41269", "revid": "7903804", "url": "https://en.wikipedia.org/wiki?curid=41269", "title": "Intensity modulation", "text": "In optical communications, intensity modulation (IM) is a form of modulation in which the optical power output of a source is varied in accordance with some characteristic of the modulating signal. The envelope of the modulated optical signal is an analog of the modulating signal in the sense that the instantaneous power of the envelope is an analog of the characteristic of interest in the modulating signal. \nThe recovery of the modulating signal is typically achieved by direct detection, not heterodyning. However, optical heterodyne detection is possible and has been actively studied since 1979. Bell Laboratories had a working, but impractical, system in 1969. Heterodyne and homodyne systems are of interest because they are expected to produce an increase in sensitivity of up to 20 dB allowing longer hops between islands for instance. Such systems also have the important advantage of very narrow channel spacing in optical frequency-division multiplexing (OFDM) systems. OFDM is a step beyond wavelength-division multiplexing (WDM). Normal WDM using direct detection does not achieve anything like the close channel spacing of radio frequency FDM.\nIntensity modulation with direct detection.\nIntensity Modulation / Direct Detection (IM/DD) is a scheme is simple and cost-effective in fiber optic communication, making it a suitable for various optical communication applications. It involves modulating the optical power of the carrier signal to represent the transmitted data. This modulation can be achieved using techniques, such as on-off keying (OOK). The intensity-modulated optical signal is generated by modulating the amplitude or the current of the light source, typically a laser diode with one or two cavity designs such as Fabry-Perot or distributed feedback (DFB).\nAt the receiver end, direct detection (DD) is used to recover the modulated signal. The modulated optical signal is detected by a photodetector (most commonly PIN or APD photodiode), which converts the optical power variations into corresponding electrical current or voltage variations. The output of the photodetector is then processed and decoded to retrieve the original information.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41270", "revid": "33467233", "url": "https://en.wikipedia.org/wiki?curid=41270", "title": "Intercept", "text": "Intercept may refer to:\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "41271", "revid": "28040267", "url": "https://en.wikipedia.org/wiki?curid=41271", "title": "Interchangeability", "text": "Interchangeability can refer to:\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "41272", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41272", "title": "Interchange circuit", "text": "Circuit that facilitates the exchange of data and signaling information\nIn telecommunications, an interchange circuit is a circuit that facilitates the exchange of data and signaling information between data terminal equipment (DTE) and data circuit-terminating equipment (DCE). \nAn interchange circuit can carry many types of signals and provide many types of service features, such as control signals, timing signals, and common return functions.\nReferences.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41273", "revid": "42522270", "url": "https://en.wikipedia.org/wiki?curid=41273", "title": "Intercharacter interval", "text": "Time interval telecommunications technique\nIn telecommunications, the intercharacter interval is the time interval between the end of the stop signal of one character and the beginning of the start signal of the next character of an asynchronous transmission. \nThe intercharacter interval may be of any duration. The signal sense of the intercharacter interval is always the same as the sense of the stop element, \"i.e.\", \"1\" or \"mark.\"\nReferences.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41274", "revid": "6289403", "url": "https://en.wikipedia.org/wiki?curid=41274", "title": "Interconnect facility", "text": "Interconnect facility: In a communications network, one or more communications links that (a) are used to provide local area communications service among several locations and (b) collectively form a node in the network. \nAn interconnect facility may include network control and administrative circuits as well as the primary traffic circuits. \nAn interconnect facility may use any medium available and may be redundant.\nReferences.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41275", "revid": "44062", "url": "https://en.wikipedia.org/wiki?curid=41275", "title": "Interface", "text": "Interface or interfacing may refer to:\n&lt;templatestyles src=\"Template:TOC_right/styles.css\" /&gt;\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "41276", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41276", "title": "Interface functionality", "text": "Characteristic of interfaces that allows operators to support transmission\nIn telephony, interface functionality is the characteristic of interfaces that allows operators to support transmission, switching, and signaling functions identical to those used in the enhanced services provided by the carrier. \nAs part of its comparably efficient interconnection (CEI) offering, the carrier must make available standardized telephone networking hardware and software interfaces that are able to support transmission, switching, and signaling functions identical to those used in the enhanced services provided by the carrier.\nReferences.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41277", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41277", "title": "Interface standard", "text": "In telecommunications, an interface standard is a standard that describes one or more functional characteristics (such as code conversion, line assignments, or protocol compliance) or physical characteristics (such as electrical, mechanical, or optical characteristics) necessary to allow the exchange of information between two or more (usually different) systems or pieces of equipment. Communications protocols are an example. \nAn interface standard may include operational characteristics and acceptable levels of performance. \nIn the military community, interface standards permit command and control functions to be performed using communication and computer systems.\nTelephones.\nThere are many interface standards between analog telephone central office equipment and customer-premises equipment. Single voice paths generally include analog audio connections, either a two-wire circuit or four-wire circuit plus signaling paths to indicate call progress and status information, such as ringing, answer supervision, etc. Some of the known interface types are:"}
{"id": "41278", "revid": "47762007", "url": "https://en.wikipedia.org/wiki?curid=41278", "title": "Interference filter", "text": "Wavelength-selective optical filter\nAn interference filter, dichroic filter, or thin-film filter is an optical filter that reflects some wavelengths (colors) of light and transmits others, with almost no absorption for all wavelengths of interest. An interference filter may be high-pass, low-pass, bandpass, or band-rejection. They are used in scientific applications, as well as in architectural and theatrical lighting.\nAn interference filter consists of multiple thin layers of dielectric material having different refractive indices. There may also be metallic layers. Interference filters are wavelength-selective by virtue of the interference effects that take place between the incident and reflected waves at the thin-film boundaries. The principle of operation is similar to a Fabry-Perot etalon.\nDichroic mirrors and dichroic reflectors are the same type of device, but are characterized by the colors of light that they reflect, rather than the colors they pass. Dielectric mirrors operate on the same principle, but focus exclusively on reflection.\nTheory.\nDichroic filters use the principle of thin-film interference, and produce colors in the same way as oil films on water. When light strikes an oil film at an angle, some of the light is reflected from the top surface of the oil, and some is reflected from the bottom surface where it is in contact with the water. Because the light reflecting from the bottom travels a slightly longer path, some light wavelengths are reinforced by this delay, while others tend to be canceled, producing the colors seen. The color transmitted by the filter exhibits a blue shift with increasing angle of incidence, see Dielectric mirror.\nIn a dichroic mirror or filter, instead of using an oil film to produce the interference, alternating layers of optical coatings with different refractive indices are built up upon a glass substrate. The interfaces between the layers of different refractive index produce phased reflections, selectively reinforcing certain wavelengths of light and interfering with other wavelengths. The layers are usually added by vacuum deposition. By controlling the thickness and number of the layers, the frequency of the passband of the filter can be tuned and made as wide or narrow as desired. Because unwanted wavelengths are reflected rather than absorbed, dichroic filters do not absorb this unwanted energy during operation and so do not become nearly as hot as the equivalent conventional filter (which attempts to absorb all energy except for that in the passband). (See Fabry\u2013P\u00e9rot interferometer for a mathematical description of the effect.)\nWhere white light is being deliberately separated into various color bands (for example, within a color video projector or color television camera), the similar dichroic prism is used instead. For cameras, however, it is now more common to have an absorption filter array to filter individual pixels on a single CCD array.\nApplications.\nDichroic filters can filter light from a white light source to produce light that is perceived by humans to be highly saturated in color. Such filters are popular in architectural and theatrical applications.\nDichroic reflectors known as cold mirrors are commonly used behind a light source to reflect visible light forward while allowing the invisible infrared light to pass out of the rear of the fixture. Such an arrangement allows intense illumination with less heating of the illuminated object. Many quartz-halogen lamps have an integrated dichroic reflector for this purpose, being originally designed for use in slide projectors to avoid melting the slides, but now widely used for interior home and commercial lighting. This improves whiteness by removing excess red; however, it poses a serious fire hazard if used in recessed or enclosed luminaires by allowing infrared radiation into those luminaires. For these applications non-cool-beam (ALU or Silverback) lamps must be used. Recessed or enclosed luminaires that are unsuitable for use with dichroic reflector lights can be identified by the IEC 60598 No Cool Beam symbol.\nIn fluorescence microscopy, dichroic filters are used as beam splitters to direct illumination of an excitation frequency toward the sample and then at an analyzer to reject that same excitation frequency but pass a particular emission frequency.\nSome LCD projectors use dichroic filters instead of prisms to split the white light from the lamp into the three colours before passing it through the three LCD units.\nOlder DLP projectors typically transmit a white light source through a color wheel which uses dichroic filters to rapidly switch colors sent through the (monochrome) Digital micromirror device. Newer projectors may use laser or LED light sources to directly emit the desired light wavelengths.\nThey are used as laser harmonic separators. They separate the various harmonic components of frequency doubled laser systems by selective spectral reflection and transmission.\nDichroic filters are also used to create gobos for high-power lighting products. Pictures are made by overlapping up to four colored dichroic filters.\nPhotographic enlarger color heads use dichroic filters to adjust the color balance in the print.\nArtistic glass jewelry is occasionally fabricated to behave as a dichroic filter. Because the wavelength of light selected by the filter varies with the angle of incidence of the light, such jewelry often has an iridescent effect, changing color as the (for example) earrings swing. Another interesting application of dichroic filters is spatial filtering.\nWith a technique licensed from Infitec, Dolby Labs uses dichroic filters for screening 3D movies. The left lens of the Dolby 3D glasses transmits specific narrow bands of red, green and blue frequencies, while the right lens transmits a different set of red, green and blue frequencies. The projector uses matching filters to display the images meant for the left and right eyes.\nLong-pass dichroic filters applied to ordinary lighting can prevent it from attracting insects. In some cases, such filters can prevent attraction of other wildlife, reducing adverse environmental impact.\nAdvantages.\nDichroic filters have a much longer life than conventional filters; the color is intrinsic in the construction of the hard microscopic layers and cannot \"bleach out\" over the lifetime of the filter (unlike for example, gel filters). They can be fabricated to pass any passband frequency and block a selected amount of the stopband frequencies. Because light in the stopband is reflected rather than absorbed, there is much less heating of the dichroic filter than with conventional filters. Dichroics are capable of achieving extremely high laser damage thresholds, and are used for all the mirrors on the world's most powerful laser, the National Ignition Facility.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41279", "revid": "214427", "url": "https://en.wikipedia.org/wiki?curid=41279", "title": "Interlaced scanning", "text": ""}
{"id": "41280", "revid": "23939382", "url": "https://en.wikipedia.org/wiki?curid=41280", "title": "Intermediate distribution frame", "text": "Cable rack for telecommunications wiring\nAn intermediate distribution frame (IDF) is a distribution frame in a central office or customer premises, which cross connects the user cable media to individual user line circuits and may serve as a distribution point for multipair cables from the main distribution frame (MDF) or combined distribution frame (CDF) to individual cables connected to equipment in areas remote from these frames.\nIDFs are used for telephone exchange central office, customer-premises equipment, wide area network (WAN), and local area network (LAN) environments, among others.\nIn central office environments the IDF may contain circuit termination equipment from various auxiliary components. In WAN and LAN environments IDFs can hold devices of different types including backup systems (hard drives or other media as self-contained, or as RAIDs, CD-ROMs, etc.), networking (switches, hubs, routers), and connections (fiber optics, coaxial, category cables) and so on."}
{"id": "41281", "revid": "20483999", "url": "https://en.wikipedia.org/wiki?curid=41281", "title": "Intermediate-field region", "text": "In antenna theory, intermediate-field region (also known as intermediate field, intermediate zone or transition zone) refers to the transition region lying between the near-field region and the far-field region in which the field strength of an electromagnetic wave is dependent upon the inverse distance, inverse square of the distance, and the inverse cube of the distance from the antenna. For an antenna that is small compared to the wavelength in question, the intermediate-field region is considered to exist at all distances between 0.1 wavelength and 1.0 wavelength from the antenna.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41282", "revid": "668752", "url": "https://en.wikipedia.org/wiki?curid=41282", "title": "Intermodulation distortion", "text": ""}
{"id": "41283", "revid": "2135234", "url": "https://en.wikipedia.org/wiki?curid=41283", "title": "Internal memory", "text": ""}
{"id": "41284", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41284", "title": "IP address spoofing", "text": "Creating IP packets using a false IP address\nIn computer networking, IP address spoofing or IP spoofing is the creation of Internet Protocol (IP) packets with a false source IP address, for the purpose of impersonating another computing system.\nBackground.\nThe basic protocol for sending data over the Internet network and many other computer networks is the Internet Protocol (IP). The protocol specifies that each IP packet must have a header which contains (among other things) the IP address of the sender of the packet. The source IP address is normally the address that the packet was sent from, but the sender's address in the header can be altered, so that to the recipient it appears that the packet came from another source.\nThe protocol requires the receiving computer to send back a response to the source IP address; therefore, spoofing is mainly used when the sender can anticipate the network response or does not care about the response.\nThe source IP address provides only limited information about the sender. It may provide general information on the region, city and town when on the packet was sent. It does not provide information on the identity of the sender or the computer being used.\nApplications.\nIP address spoofing involving the use of a trusted IP address can be used by network intruders to overcome network security measures, such as authentication based on IP addresses. This type of attack is most effective where trust relationships exist between machines. For example, it is common on some corporate networks to have internal systems trust each other, so that users can log in without a username or password, provided they are connecting from another machine on the internal network \u2013 which would require them already being logged in. By spoofing a connection from a trusted machine, an attacker on the same network may be able to access the target machine without authentication.\nIP address spoofing is most frequently used in denial-of-service attacks, where the objective is to flood the target with an overwhelming volume of traffic, and the attacker does not care about receiving responses to the attack packets. Packets with spoofed IP addresses are more difficult to filter since each spoofed packet appears to come from a different address, and they hide the true source of the attack. Denial of service attacks that use spoofing typically randomly choose addresses from the entire IP address space, though more sophisticated spoofing mechanisms might avoid non-routable addresses or unused portions of the IP address space. The proliferation of large botnets makes spoofing less important in denial-of-service attacks, but attackers typically have spoofing available as a tool, if they want to use it, so defenses against denial-of-service attacks that rely on the validity of the source IP address in attack packets might have trouble with spoofed packets.\nIn DDoS attacks, the attacker may decide to spoof the IP source address to randomly generated addresses, so the victim machine cannot distinguish between the spoofed packets and legitimate packets. The replies would then be sent to random addresses that do not end up anywhere in particular. Such packages-to-nowhere are called the backscatter, and there are network telescopes monitoring backscatter to measure the statistical intensity of DDoS attacks on the internet over time.\nLegitimate uses.\nThe use of packets with a false source IP address is not always evidence of malicious intent. For example, in performance testing of websites, hundreds or even thousands of \"vusers\" (virtual users) may be created, each executing a test script against the website under test, in order to simulate what will happen when the system goes \"live\" and a large number of users log in simultaneously.\nSince each user will normally have its own IP address, commercial testing products (such as HP LoadRunner, WebLOAD, and others) can use IP spoofing, allowing each user its own \"return address\" as well.\nIP spoofing is also used in some server-side load balancing. It lets the load balancer spray incoming traffic, but not need to be in the return path from the servers to the client. This saves a networking hop through switches and the load balancer as well as outbound message processing load on the load balancer. Output usually has more packets and bytes, so the savings are significant.\nServices vulnerable to IP spoofing.\nConfiguration and services that are vulnerable to IP spoofing:\nDefense against spoofing attacks.\nPacket filtering is one defense against IP spoofing attacks. The gateway to a network usually performs ingress filtering, which is blocking of packets from outside the network with a source address inside the network. This prevents an outside attacker spoofing the address of an internal machine. Ideally, the gateway would also perform egress filtering on outgoing packets, which is blocking of packets from inside the network with a source address that is not inside. This prevents an attacker within the network performing filtering from launching IP spoofing attacks against external machines. An intrusion detection system (IDS) is a common use of packet filtering, which has been used to secure the environments for sharing data over network and host-based IDS approaches.\nIt is also recommended to design network protocols and services so that they do not rely on the source IP address for authentication.\nUpper layers.\nSome upper layer protocols have their own defense against IP spoofing attacks. For example, Transmission Control Protocol (TCP) uses sequence numbers negotiated with the remote machine to ensure that arriving packets are part of an established connection. Since the attacker normally cannot see any reply packets, the sequence number must be guessed in order to hijack the connection. The poor implementation in many older operating systems and network devices, however, means that TCP sequence numbers can be predicted.\nOther definitions.\nThe term spoofing is also sometimes used to refer to \"header forgery\", the insertion of false or misleading information in e-mail or netnews headers. Falsified headers are used to mislead the recipient or network applications as to the origin of a message. This is a common technique of spammers and sporgers, who wish to conceal the origin of their messages to avoid being tracked.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41285", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41285", "title": "Interoperability", "text": "Ability of systems to work with each other\nInteroperability is a characteristic of a product or system to work with other products or systems. While the term was initially defined for information technology or systems engineering services to allow for information exchange, a broader definition takes into account social, political, and organizational factors that impact system-to-system performance.\nTypes of interoperability include syntactic interoperability, where two systems can communicate with each other, and cross-domain interoperability, where multiple organizations work together and exchange information.\nTypes.\nIf two or more systems use common data formats and communication protocols, then they are capable of communicating with each other, and they exhibit \"syntactic interoperability\". XML and SQL are examples of common data formats and protocols. Low-level data formats also contribute to syntactic interoperability, ensuring that alphabetical characters are stored in the same ASCII or a Unicode format in all the communicating systems.\nBeyond the ability of two or more computer systems to exchange information, semantic interoperability is the ability to automatically interpret the information exchanged meaningfully and accurately in order to produce useful results as defined by the end users of both systems. To achieve semantic interoperability, both sides must refer to a common information exchange reference model. The content of the information exchange requests are unambiguously defined: what is sent is the same as what is understood.\nCross-domain interoperability involves multiple social, organizational, political, legal entities working together for a common interest or information exchange.\nInteroperability and open standards.\nInteroperability implies exchanges between a range of products, or similar products from several different vendors, or even between past and future revisions of the same product. Interoperability may be developed \"post-facto\", as a special measure between two products, while excluding the rest, by using open standards. When a vendor is forced to adapt its system to a dominant system that is not based on open standards, it is compatibility, not interoperability.\nOpen standards.\nOpen standards rely on a broadly consultative and inclusive group including representatives from vendors, academics and others holding a stake in the development that discusses and debates the technical and economic merits, demerits and feasibility of a proposed common protocol. After the doubts and reservations of all members are addressed, the resulting common document is endorsed as a \"common standard\". This document may be subsequently released to the public, and henceforth becomes an \"open standard\". It is usually published and is available freely or at a nominal cost to any and all comers, with \"no further encumbrances\". Various vendors and individuals (even those who were not part of the original group) can use the standards document to make products that implement the common protocol defined in the standard and are thus \"interoperable by design\", with no specific liability or advantage for customers for choosing one product over another on the basis of standardized features. The vendors' products compete on the quality of their implementation, user interface, ease of use, performance, price, and a host of other factors, while keeping the customer's data intact and transferable even if they choose to switch to another competing product for business reasons.\n\"Post facto\" interoperability.\n\"Post facto\" interoperability may be the result of the absolute market dominance of a particular product in contravention of any applicable standards, or if any effective standards were not present at the time of that product's introduction. The vendor behind that product can then choose to \"ignore\" any forthcoming standards and not cooperate in any standardization process at all, using its near-monopoly to insist that its product sets the \"de facto\" standard by its very market dominance. This is not a problem if the product's implementation is open \"and\" minimally encumbered, but it may well be both closed and heavily encumbered (e.g., by patent claims). Because of the network effect, achieving interoperability with such a product is both critical for any other vendor if it wishes to remain relevant in the market, and difficult to accomplish because of lack of cooperation on equal terms with the original vendor, who may well see the new vendor as a potential competitor and threat. The newer implementations often rely on clean-room reverse engineering in the absence of technical data to achieve interoperability. The original vendors may provide such technical data to others, often in the name of \"encouraging competition\", but such data is invariably encumbered and may be of limited use. Availability of such data is \"not\" equivalent to an open standard, because:\nGovernment.\neGovernment.\nSpeaking from an e-government perspective, interoperability refers to the collaboration ability of cross-border services for citizens, businesses and public administrations. Exchanging data can be a challenge due to language barriers, different specifications of formats, varieties of categorizations and other hindrances.\nIf data is interpreted differently, collaboration is limited, takes longer and is inefficient. For instance, if a citizen of country A wants to purchase land in country B, the person will be asked to submit the proper address data. Address data in both countries includes full name details, street name and number as well as a postal code. The order of the address details might vary. In the same language, it is not an obstacle to order the provided address data, but across language barriers, it becomes difficult. If the language uses a different writing system it is almost impossible if no translation tools are available.\nFlood risk management.\nInteroperability is used by researchers in the context of urban flood risk management. Cities and urban areas worldwide are expanding, which creates complex spaces with many interactions between the environment, infrastructure and people. To address this complexity and manage water in urban areas appropriately, a system of systems approach to water and flood control is necessary. In this context, interoperability is important to facilitate system-of-systems thinking, and is defined as: \"the ability of any water management system to redirect water and make use of other system(s) to maintain or enhance its performance function during water exceedance events.\" By assessing the complex properties of urban infrastructure systems, particularly the interoperability between the drainage systems and other urban systems (e.g. infrastructure such as transport), it could be possible to expand the capacity of the overall system to manage flood water towards achieving improved urban flood resilience.\nMilitary forces.\n\"Force interoperability\" is defined in NATO as the ability of the forces of two or more nations to train, exercise and operate effectively together in the execution of assigned missions and tasks. Additionally NATO defines interoperability more generally as the ability to act together coherently, effectively and efficiently to achieve Allied tactical, operational and strategic objectives.\nAt the strategic level, interoperability is an enabler for coalition building. It facilitates meaningful contributions by coalition partners. At this level, interoperability issues center on harmonizing world views, strategies, doctrines, and force structures. Interoperability is an element of coalition willingness to work together over the long term to achieve and maintain shared interests against common threats. Interoperability at the operational and tactical levels is where strategic interoperability and technological interoperability come together to help allies shape the environment, manage crises, and win wars. The benefits of interoperability at the operational and tactical levels generally derive from the interchangeability of force elements and units. \"Technological interoperability\" reflects the interfaces between organizations and systems. It focuses on communications and computers but also involves the technical capabilities of systems and the resulting mission compatibility between the systems and data of coalition partners. At the technological level, the benefits of interoperability come primarily from their impacts at the operational and tactical levels in terms of enhancing flexibility.\nPublic safety.\nBecause first responders need to be able to communicate during wide-scale emergencies, interoperability is an important issue for law enforcement, firefighting, emergency medical services, and other public health and safety departments. It has been a major area of investment and research over the last 12 years. Widely disparate and incompatible hardware impedes the exchange of information between agencies. Agencies' information systems, such as computer-aided dispatch systems and records management systems, functioned largely in isolation, in so-called \"information islands\". Agencies tried to bridge this isolation with inefficient, stopgap methods while large agencies began implementing limited interoperable systems. These approaches were inadequate and, in the US, the lack of interoperability in the public safety realm became evident during the 9/11 attacks on the Pentagon and World Trade Center structures. Further evidence of a lack of interoperability surfaced when agencies tackled the aftermath of Hurricane Katrina.\nIn contrast to the overall national picture, some states, including Utah, have already made great strides forward. The Utah Highway Patrol and other departments in Utah have created a statewide data sharing network.\nThe Commonwealth of Virginia is one of the leading states in the United States in improving interoperability. The Interoperability Coordinator leverages a regional structure to better allocate grant funding around the Commonwealth so that all areas have an opportunity to improve communications interoperability. Virginia's strategic plan for communications is updated yearly to include new initiatives for the Commonwealth\u00a0\u2013 all projects and efforts are tied to this plan, which is aligned with the National Emergency Communications Plan, authored by the Department of Homeland Security's Office of Emergency Communications.\nThe State of Washington seeks to enhance interoperability statewide. The State Interoperability Executive Committee (SIEC), established by the legislature in 2003, works to assist emergency responder agencies (police, fire, sheriff, medical, hazmat, etc.) at all levels of government (city, county, state, tribal, federal) to define interoperability for their local region. \nWashington recognizes that collaborating on system design and development for wireless radio systems enables emergency responder agencies to efficiently provide additional services, increase interoperability, and reduce long-term costs. This work saves the lives of emergency personnel and the citizens they serve.\nThe U.S. government is making an effort to overcome the nation's lack of public safety interoperability. The Department of Homeland Security's Office for Interoperability and Compatibility (OIC) is pursuing the SAFECOM and CADIP and Project 25 programs, which are designed to help agencies as they integrate their CAD and other IT systems.\nThe OIC launched CADIP in August 2007. This project will partner the OIC with agencies in several locations, including Silicon Valley. This program will use case studies to identify the best practices and challenges associated with linking CAD systems across jurisdictional boundaries. These lessons will create the tools and resources public safety agencies can use to build interoperable CAD systems and communicate across local, state, and federal boundaries.\nAs regulator for interoperability.\nGovernance entities can increase interoperability through their legislative and executive powers. For instance, in 2021 the European Commission, after commissioning two impact assessment studies and a technology analysis study, proposed the implementation of a standardization \u2013 for iterations of USB-C \u2013 of phone charger products, which may increase interoperability along with convergence and convenience for consumers while decreasing resource needs, redundancy and electronic waste.\nConversely, government-mandated interoperability has been heavily criticized as leading to monopolies that become too big to fail. For example, the United States Securities and Exchange Commission's implementation of 1975 amendments to the Securities Exchange Act of 1934 that were intended to ensure interoperability was blamed for driving all regional clearinghouses and depositories out of business in the United States. As a result, the National Securities Clearing Corporation is the sole clearinghouse; the Depository Trust Company is the sole repository; and their parent, the Depository Trust &amp; Clearing Corporation, has enormous market power over central counterparty clearing in the United States. In contrast, the federal government of the United States did not attempt to mandate or regulate credit card interoperability. This allowed credit card networks to naturally develop interoperability (in the sense that almost every payment terminal can automatically accept almost every credit card), so that Visa Inc. was not left as the last credit card network standing.\nCommerce and industries.\nInformation technology and computers.\nDesktop.\nDesktop interoperability is a subset of software interoperability. In the early days, the focus of interoperability was to integrate web applications with other web applications. Over time, open-system containers were developed to create a virtual desktop environment in which these applications could be registered and then communicate with each other using simple publish\u2013subscribe patterns. Rudimentary UI capabilities were also supported, allowing windows to be grouped with other windows. Today, desktop interoperability has evolved into full-service platforms that include container support, basic exchange between web and web, but also native support for other application types and advanced window management. The very latest interop platforms also include application services such as universal search, notifications, user permissions and preferences, 3rd party application connectors and language adapters for in-house applications.\nInformation search.\nSearch interoperability refers to the ability of two or more information collections to be searched by a single query.\nSpecifically related to web-based search, the challenge of interoperability stems from the fact that designers of web resources typically have little or no need to concern themselves with exchanging information with other web resources. Federated Search technology, which does not place format requirements on the data owner, has emerged as one solution to search interoperability challenges. In addition, standards, such as Open Archives Initiative Protocol for Metadata Harvesting, Resource Description Framework, and SPARQL, have emerged that also help address the issue of search interoperability related to web resources. Such standards also address broader topics of interoperability, such as allowing data mining.\nSoftware.\nWith respect to software, the term \"interoperability\" is used to describe the capability of different programs to exchange data via a common set of exchange formats, to read and write the same file formats, and to use the same communication protocols. The lack of interoperability can be a consequence of a lack of attention to standardization during the design of a program. Indeed, interoperability is not taken for granted in the non-standards-based portion of the computing world.\nAccording to ISO/IEC 2382-01, \"Information Technology Vocabulary, Fundamental Terms\", interoperability is defined as follows: \"The capability to communicate, execute programs, or transfer data among various functional units in a manner that requires the user to have little or no knowledge of the unique characteristics of those units\".\nStandards-developing organizations provide open public software specifications to facilitate interoperability; examples include the Oasis-Open organization and buildingSMART (formerly the International Alliance for Interoperability). Another example of a neutral party is the RFC documents from the Internet Engineering Task Force (IETF).\nThe Open Service for Lifecycle Collaboration community is working on finding a common standard in order that software tools can share and exchange data e.g. bugs, tasks, requirements etc. The final goal is to agree on an open standard for interoperability of open source application lifecycle management tools.\nJava is an example of an interoperable programming language that allows for programs to be written once and run anywhere with a Java virtual machine. A program in Java, so long as it does not use system-specific functionality, will maintain interoperability with all systems that have a Java virtual machine available. Applications will maintain compatibility because, while the implementation is different, the underlying language interfaces are the same.\nAchieving software.\nSoftware interoperability is achieved through five interrelated ways:\nEach of these has an important role in reducing variability in intercommunication software and enhancing a common understanding of the end goal to be achieved.\nMarket dominance and power.\nInteroperability tends to be regarded as an issue for experts and its implications for daily living are sometimes underrated. The European Union Microsoft competition case shows how interoperability concerns important questions of power relationships. In 2004, the European Commission found that Microsoft had abused its market power by deliberately restricting interoperability between Windows Workgroup servers and non-Microsoft work group servers. By doing so, Microsoft was able to protect its dominant market position for work group server operating systems, the heart of corporate IT networks. Microsoft was ordered to disclose complete and accurate interface documentation, which could enable rival vendors to compete on an equal footing (\"the interoperability remedy\").\nInteroperability has also surfaced in the software patent debate in the European Parliament (June\u2013July 2005). Critics claim that because patents on techniques required for interoperability are kept under RAND (reasonable and non-discriminatory licensing) conditions, customers will have to pay license fees twice: once for the product and, in the appropriate case, once for the patent-protected program the product uses.\nBusiness processes.\nInteroperability is often more of an organizational issue. Interoperability can have a significant impact on the organizations concerned, raising issues of ownership (do people want to share their data? or are they dealing with information silos?), labor relations (are people prepared to undergo training?) and usability. In this context, a more apt definition is captured in the term \"business process interoperability\".\nInteroperability can have important economic consequences; for example, research has estimated the cost of inadequate interoperability in the US capital facilities industry to be $15.8 billion a year. If competitors' products are not interoperable (due to causes such as patents, trade secrets or coordination failures), the result may well be monopoly or market failure. For this reason, it may be prudent for user communities or governments to take steps to encourage interoperability in various situations. At least 30 international bodies and countries have implemented eGovernment-based interoperability framework initiatives called e-GIF, while in the US, there is the NIEM initiative.\nMedical industry.\nThe need for \"plug-and-play\" interoperability\u00a0\u2013 the ability to take a medical device out of its box and easily make it work with one's other devices\u00a0\u2013 has attracted great attention from both healthcare providers and industry.\nIncreasingly, medical devices like incubators and imaging systems feature software that integrates at the point of care and with electronic systems, such as electronic medical records. At the 2016 Regulatory Affairs Professionals Society (RAPS) meeting, experts in the field like Angela N. Johnson with GE Healthcare and Jeff Shuren of the United States Food and Drug Administration, provided practical seminars on how companies developing new medical devices, and hospitals installing them, can work more effectively to align interoperable software systems.\nRailways.\nRailways have greater or lesser interoperability depending on conforming to standards of gauge, couplings, brakes, signalling, loading gauge, and structure gauge to mention a few parameters. For passenger rail service, different railway platform height and width clearance standards may also affect interoperability.\nNorth American freight and intercity passenger railroads are highly interoperable, but systems in Europe, Asia, Africa, Central and South America, and Australia are much less so. The parameter most difficult to overcome (at reasonable cost) is incompatibility of gauge, though variable gauge axle systems can be used on rolling stock.\nTelecommunications.\nIn telecommunications, the term can be defined as:\nIn two-way radio, interoperability is composed of three dimensions:\nOrganizations dedicated to interoperability.\nMany organizations are dedicated to interoperability. Some concentrate on eGovernment, eBusiness or data exchange in general.\nGlobal.\nInternationally, Network Centric Operations Industry Consortium facilitates global interoperability across borders, language and technical barriers. In the built environment, the International Alliance for Interoperability started in 1994, and was renamed buildingSMART in 2005.\nEurope.\nIn Europe, the European Commission and its IDABC program issue the European Interoperability Framework. IDABC was succeeded by the Interoperability Solutions for European Public Administrations (ISA) program. They also initiated the Semantic Interoperability Centre Europe (SEMIC.EU). A European Land Information Service (EULIS) was established in 2006, as a consortium of European National Land Registers. The aim of the service is to establish a single portal through which customers are provided with access to information about individual properties, about land and property registration services, and about the associated legal environment.\nThe European Interoperability Framework (EIF) considered four kinds of interoperability: legal interoperability, organizational interoperability, semantic interoperability, and technical interoperability.\nIn the European Research Cluster on the Internet of Things (IERC) and IoT Semantic Interoperability Best Practices; four kinds of interoperability are distinguished: syntactical interoperability, technical interoperability, semantic interoperability, and organizational interoperability.\nUS.\nIn the United States, the General Services Administration Component Organization and Registration Environment (CORE.GOV) initiative provided a collaboration environment for component development, sharing, registration, and reuse in the early 2000s. A related initiative is the ongoing National Information Exchange Model (NIEM) work and component repository. The National Institute of Standards and Technology serves as an agency for measurement standards.\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41286", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41286", "title": "Interposition trunk", "text": "In telecommunications, the term interposition trunk has the following meanings: \n1. A single direct communication channel, \"e.g.,\" voice-frequency circuit, between two positions of a large switchboard to facilitate the interconnection of other circuits appearing at the respective switchboard positions. \n2. Within a technical control facility, a single direct transmission circuit, between positions in a testboard or patch bay, which circuit facilitates testing or patching between the respective positions. \n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41287", "revid": "20542576", "url": "https://en.wikipedia.org/wiki?curid=41287", "title": "Intersymbol interference", "text": "Form of distortion affecting communication reliability\nIn telecommunications, intersymbol interference (ISI) is a form of distortion of a signal in which one symbol interferes with subsequent symbols. This is an unwanted phenomenon as the previous symbols have a similar effect as noise, thus making the communication less reliable. The spreading of the pulse beyond its allotted time interval causes it to interfere with neighboring pulses. ISI is usually caused by multipath propagation or the inherent linear or non-linear frequency response of a communication channel causing successive symbols to blur together.\nThe presence of ISI in the system introduces errors in the decision device at the receiver output. Therefore, in the design of the transmitting and receiving filters, the objective is to minimize the effects of ISI, and thereby deliver the digital data to its destination with the smallest error rate possible.\nWays to alleviate intersymbol interference include adaptive equalization and error correcting codes.\nCauses.\nMultipath propagation.\nOne of the causes of intersymbol interference is multipath propagation in which a wireless signal from a transmitter reaches the receiver via multiple paths. The causes of this include reflection (for instance, the signal may bounce off buildings), refraction (such as through the foliage of a tree) and atmospheric effects such as atmospheric ducting and ionospheric reflection. Since the various paths can be of different lengths, this results in the different versions of the signal arriving at the receiver at different times. These delays mean that part or all of a given symbol will be spread into the subsequent symbols, thereby interfering with the correct detection of those symbols. Additionally, the various paths often distort the amplitude and/or phase of the signal, thereby causing further interference with the received signal.\nBandlimited channels.\nAnother cause of intersymbol interference is the transmission of a signal through a bandlimited channel, i.e., one where the frequency response is zero above a certain frequency (the cutoff frequency). Passing a signal through such a channel results in the removal of frequency components above this cutoff frequency. In addition, components of the frequency below the cutoff frequency may also be attenuated by the channel.\nThis filtering of the transmitted signal affects the shape of the pulse that arrives at the receiver. The effects of filtering a rectangular pulse not only change the shape of the pulse within the first symbol period, but it is also spread out over the subsequent symbol periods. When a message is transmitted through such a channel, the spread pulse of each individual symbol will interfere with following symbols.\nBandlimited channels are present in both wired and wireless communications. The limitation is often imposed by the desire to operate multiple independent signals through the same area/cable; due to this, each system is typically allocated a piece of the total bandwidth available. For wireless systems, they may be allocated a slice of the electromagnetic spectrum to transmit in (for example, FM radio is often broadcast in the 87.5\u2013108\u00a0MHz range). This allocation is usually administered by a government agency; in the case of the United States this is the Federal Communications Commission (FCC). In a wired system, such as an optical fiber cable, the allocation will be decided by the owner of the cable.\nThe bandlimiting can also be due to the physical properties of the medium - for instance, the cable being used in a wired system may have a cutoff frequency above which practically none of the transmitted signal will propagate.\nCommunication systems that transmit data over bandlimited channels usually implement pulse shaping to avoid interference caused by the bandwidth limitation. If the channel frequency response is flat and the shaping filter has a finite bandwidth, it is possible to communicate with no ISI at all. Often the channel response is not known beforehand, and an adaptive equalizer is used to compensate the frequency response.\nEffects on eye patterns.\nOne way to study ISI in a PCM or data transmission system experimentally is to apply the received wave to the vertical deflection plates of an oscilloscope and to apply a sawtooth wave at the transmitted symbol rate R (R = 1/T) to the horizontal deflection plates. The resulting display is called an eye pattern because of its resemblance to the human eye for binary waves. The interior region of the eye pattern is called the eye opening. An eye pattern provides a great deal of information about the performance of the pertinent system.\nAn eye pattern, which overlays many samples of a signal, can give a graphical representation of the\nsignal characteristics. The first image above is the eye pattern for a binary phase-shift keying (PSK) system in which a one is represented by an amplitude of \u22121 and a zero by an amplitude of +1. The current sampling time is at the center of the image and the previous and next sampling times are at the edges of the image. The various transitions from one sampling time to another (such as one-to-zero, one-to-one and so forth) can clearly be seen on the diagram.\nThe noise margin - the amount of noise required to cause the receiver to get an error - is given by the distance between the signal and the zero amplitude point at the sampling time; in other words, the further from zero at the sampling time the signal is the better. For the signal to be correctly interpreted, it must be sampled somewhere between the two points where the zero-to-one and one-to-zero transitions cross. Again, the further apart these points are the better, as this means the signal will be less sensitive to errors in the timing of the samples at the receiver.\nThe effects of ISI are shown in the second image which is an eye pattern of the same system when operating over a multipath channel. The effects of receiving delayed and distorted versions of the signal can be seen in the loss of definition of the signal transitions. It also reduces both the noise margin and the window in which the signal can be sampled, which shows that the performance of the system will be worse (i.e. it will have a greater bit error ratio).\nCountering ISI.\nThere are several techniques in telecommunications and data storage that try to work around the problem of intersymbol interference.\nIntentional intersymbol interference.\nCoded modulation systems also exist that intentionally build a controlled amount of ISI into the system at the transmitter side, known as faster-than-Nyquist signaling. Such a design trades a computational complexity penalty at the receiver against a Shannon capacity gain of the overall transceiver system.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41288", "revid": "14660971", "url": "https://en.wikipedia.org/wiki?curid=41288", "title": "Inverse-square law", "text": "Physical law\nIn physical science, an inverse-square law is any scientific law stating that the observed \"intensity\" of a specified physical quantity (being nothing more than the value of the physical quantity) is inversely proportional to the square of the distance from the source of that physical quantity. The fundamental cause for this can be understood as geometric dilution corresponding to point-source radiation into three-dimensional space.\nRadar energy expands during both the signal transmission and the reflected return, so the inverse square for both paths means that the radar will receive energy according to the inverse fourth power of the range.\nTo prevent dilution of energy while propagating a signal, certain methods can be used such as a waveguide, which acts like a canal does for water, or how a gun barrel restricts hot gas expansion to one dimension in order to prevent loss of energy transfer to a bullet.\nFormula.\nIn mathematical notation the inverse square law can be expressed as an intensity (I) varying as a function of distance (d) from some centre. The intensity is proportional (see \u221d) to the reciprocal of the square of the distance thus: \nformula_1\nIt can also be mathematically expressed as :\nformula_2\nor as the formulation of a constant quantity: \nformula_3\nThe divergence of a vector field which is the resultant of radial inverse-square law fields with respect to one or more sources is proportional to the strength of the local sources, and hence zero outside sources. Newton's law of universal gravitation follows an inverse-square law, as do the effects of electric, light, sound, and radiation phenomena.\nJustification.\nThe inverse-square law generally applies when some force, energy, or other conserved quantity is evenly radiated outward from a point source in three-dimensional space. Since the surface area of a sphere (which is\u00a04\u03c0\"r\"2) is proportional to the square of the radius, as the emitted radiation gets farther from the source, it is spread out over an area that is increasing in proportion to the square of the distance from the source. Hence, the intensity of radiation passing through any unit area (directly facing the point source) is inversely proportional to the square of the distance from the point source. Gauss's law for gravity is similarly applicable, and can be used with any physical quantity that acts in accordance with the inverse-square relationship.\nOccurrences.\nGravitation.\nGravitation is the attraction between objects that have mass. Newton's law states:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;\nformula_4\nIf the distribution of matter in each body is spherically symmetric, then the objects can be treated as point masses without approximation, as shown in the shell theorem. Otherwise, if we want to calculate the attraction between massive bodies, we need to add all the point-point attraction forces vectorially and the net attraction might not be exact inverse square. However, if the separation between the massive bodies is much larger compared to their sizes, then to a good approximation, it is reasonable to treat the masses as a point mass located at the object's center of mass while calculating the gravitational force.\nAs the law of gravitation, this law was suggested in 1645 by Isma\u00ebl Bullialdus. But Bullialdus did not accept Kepler's second and third laws, nor did he appreciate Christiaan Huygens's solution for circular motion (motion in a straight line pulled aside by the central force). Indeed, Bullialdus maintained the sun's force was attractive at aphelion and repulsive at perihelion. Robert Hooke and Giovanni Alfonso Borelli both expounded gravitation in 1666 as an attractive force. Hooke's lecture \"On gravity\" was at the Royal Society, in London, on 21 March. Borelli's \"Theory of the Planets\" was published later in 1666. Hooke's 1670 Gresham lecture explained that gravitation applied to \"all celestiall bodys\" and added the principles that the gravitating power decreases with distance and that in the absence of any such power bodies move in straight lines. By 1679, Hooke thought gravitation had inverse square dependence and communicated this in a letter to Isaac Newton: \n\"my supposition is that the attraction always is in duplicate proportion to the distance from the center reciprocall\".\nHooke remained bitter about Newton claiming the invention of this principle, even though Newton's 1686 \"Principia\" acknowledged that Hooke, along with Wren and Halley, had separately appreciated the inverse square law in the Solar System, as well as giving some credit to Bullialdus.\nElectrostatics.\nThe force of attraction or repulsion between two electrically charged particles, in addition to being directly proportional to the product of the electric charges, is inversely proportional to the square of the distance between them; this is known as Coulomb's law. The deviation of the exponent from 2 is less than one part in 1015.\nformula_5\nLight and other electromagnetic radiation.\nThe intensity (or illuminance or irradiance) of light or other linear waves radiating from a point source (energy per unit of area perpendicular to the source) is inversely proportional to the square of the distance from the source, so an object (of the same size) twice as far away receives only one-quarter the energy (in the same time period).\nMore generally, the irradiance, \"i.e.,\" the intensity (or power per unit area in the direction of propagation), of a spherical wavefront varies inversely with the square of the distance from the source (assuming there are no losses caused by absorption or scattering).\nFor example, the intensity of radiation from the Sun is 9126 watts per square meter at the distance of Mercury (0.387 AU) but only 1367 watts per square meter at the distance of Earth (1 AU)\u2014an approximate threefold increase in distance results in an approximate ninefold decrease in intensity of radiation.\nFor non-isotropic radiators such as parabolic antennas, headlights, and lasers, the effective origin is located far behind the beam aperture. If you are close to the origin, you don't have to go far to double the radius, so the signal drops quickly. When you are far from the origin and still have a strong signal, like with a laser, you have to travel very far to double the radius and reduce the signal. This means you have a stronger signal or have antenna gain in the direction of the narrow beam relative to a wide beam in all directions of an isotropic antenna.\nIn photography and stage lighting, the inverse-square law is used to determine the \u201cfall off\u201d or the difference in illumination on a subject as it moves closer to or further from the light source. For quick approximations, it is enough to remember that doubling the distance reduces illumination to one quarter; or similarly, to halve the illumination increase the distance by a factor of 1.4 (the square root of 2), and to double illumination, reduce the distance to 0.7 (square root of 1/2). When the illuminant is not a point source, the inverse square rule is often still a useful approximation; when the size of the light source is less than one-fifth of the distance to the subject, the calculation error is less than 1%.\nThe fractional reduction in electromagnetic fluence (\u03a6) for indirectly ionizing radiation with increasing distance from a point source can be calculated using the inverse-square law. Since emissions from a point source have radial directions, they intercept at a perpendicular incidence. The area of such a shell is 4\u03c0\"r\" 2 where \"r\" is the radial distance from the center. The law is particularly important in diagnostic radiography and radiotherapy treatment planning, though this proportionality does not hold in practical situations unless source dimensions are much smaller than the distance. As stated in Fourier theory of heat \u201cas the point source is magnification by distances, its radiation is dilute proportional to the sin of the angle, of the increasing circumference arc from the point of origin\u201d.\nExample.\nLet \"P\"\u00a0 be the total power radiated from a point source (for example, an omnidirectional isotropic radiator). At large distances from the source (compared to the size of the source), this power is distributed over larger and larger spherical surfaces as the distance from the source increases. Since the surface area of a sphere of radius \"r\" is \"A\"\u00a0=\u00a04\"\u03c0r\"\u00a02, the intensity \"I\" (power per unit area) of radiation at distance \"r\" is\nformula_6\nThe energy or intensity decreases (divided by\u00a04) as the distance \"r\" is doubled; if measured in dB would decrease by 6.02\u00a0dB per doubling of distance. When referring to measurements of power quantities, a ratio can be expressed as a level in decibels by evaluating ten times the base-10 logarithm of the ratio of the measured quantity to the reference value.\nSound in a gas.\nIn acoustics, the sound pressure of a spherical wavefront radiating from a point source decreases by 50% as the distance \"r\" is doubled; measured in dB, the decrease is still 6.02\u00a0dB, since dB represents an intensity ratio. The pressure ratio (as opposed to power ratio) is not inverse-square, but is inverse-proportional (inverse distance law):\nformula_7\nThe same is true for the component of particle velocity formula_8 that is in-phase with the instantaneous sound pressure formula_9:\nformula_10\nIn the near field is a quadrature component of the particle velocity that is 90\u00b0 out of phase with the sound pressure and does not contribute to the time-averaged energy or the intensity of the sound. The sound intensity is the product of the RMS sound pressure and the \"in-phase\" component of the RMS particle velocity, both of which are inverse-proportional. Accordingly, the intensity follows an inverse-square behaviour:\nformula_11\nField theory interpretation.\nFor an irrotational vector field in three-dimensional space, the inverse-square law corresponds to the property that the divergence is zero outside the source. This can be generalized to higher dimensions. Generally, for an irrotational vector field in \"n\"-dimensional Euclidean space, the intensity \"I\" of the vector field falls off with the distance \"r\" following the inverse (\"n\"\u00a0\u2212\u00a01)th power law\nformula_12\ngiven that the space outside the source is divergence free. \nNon-Euclidean implications.\nThe inverse-square law, fundamental in Euclidean spaces, also applies to non-Euclidean geometries, including hyperbolic space. The curvature present in these spaces alters physical laws, influencing a variety of fields such as cosmology, general relativity, and string theory.\nJohn D. Barrow, in his 2020 paper \"Non-Euclidean Newtonian Cosmology,\" expands on the behavior of force (F) and potential (\u03a6) within hyperbolic 3-space (H3). He explains that F and \u03a6 obey the relationships F \u221d 1 / R\u00b2 sinh\u00b2(r/R) and \u03a6 \u221d coth(r/R), where R represents the curvature radius and r represents the distance from the focal point.\nThe concept of spatial dimensionality, first proposed by Immanuel Kant, remains a topic of debate concerning the inverse-square law. Dimitria Electra Gatzia and Rex D. Ramsier, in their 2021 paper, contend that the inverse-square law is more closely related to force distribution symmetry than to the dimensionality of space.\nIn the context of non-Euclidean geometries and general relativity, deviations from the inverse-square law do not arise from the law itself but rather from the assumption that the force between two bodies is instantaneous, which contradicts special relativity. General relativity reinterprets gravity as the curvature of spacetime, leading particles to move along geodesics in this curved spacetime.\nHistory.\nJohn Dumbleton of the 14th-century Oxford Calculators, was one of the first to express functional relationships in graphical form. He gave a proof of the mean speed theorem stating that \"the latitude of a uniformly difform movement corresponds to the degree of the midpoint\" and used this method to study the quantitative decrease in intensity of illumination in his \"Summa logic\u00e6 et philosophi\u00e6 naturalis\" (ca. 1349), stating that it was not linearly proportional to the distance, but was unable to expose the Inverse-square law.\nIn proposition 9 of Book 1 in his book \"Ad Vitellionem paralipomena, quibus astronomiae pars optica traditur\" (1604), the astronomer Johannes Kepler argued that the spreading of light from a point source obeys an inverse square law:\n&lt;templatestyles src=\"Verse translation/styles.css\" /&gt;\n&lt;templatestyles src=\"Screen reader-only/styles.css\" /&gt;Translation:\nIn 1645, in his book \"Astronomia Philolaica\" ..., the French astronomer Isma\u00ebl Bullialdus (1605\u20131694) refuted Johannes Kepler's suggestion that \"gravity\" weakens as the inverse of the distance; instead, Bullialdus argued, \"gravity\" weakens as the inverse square of the distance:\n&lt;templatestyles src=\"Verse translation/styles.css\" /&gt;\n&lt;templatestyles src=\"Screen reader-only/styles.css\" /&gt;Translation:\nIn England, the Anglican bishop Seth Ward (1617\u20131689) publicized the ideas of Bullialdus in his critique \"In Ismaelis Bullialdi astronomiae philolaicae fundamenta inquisitio brevis\" (1653) and publicized the planetary astronomy of Kepler in his book \"Astronomia geometrica\" (1656).\nIn 1663\u20131664, the English scientist Robert Hooke was writing his book \"Micrographia\" (1666) in which he discussed, among other things, the relation between the height of the atmosphere and the barometric pressure at the surface. Since the atmosphere surrounds the Earth, which itself is a sphere, the volume of atmosphere bearing on any unit area of the Earth's surface is a truncated cone (which extends from the Earth's center to the vacuum of space; obviously only the section of the cone from the Earth's surface to space bears on the Earth's surface). Although the volume of a cone is proportional to the cube of its height, Hooke argued that the air's pressure at the Earth's surface is instead proportional to the height of the atmosphere because gravity diminishes with altitude. Although Hooke did not explicitly state so, the relation that he proposed would be true only if gravity decreases as the inverse square of the distance from the Earth's center.\nNewton went up to independently develop and derive the inverse-square law for gravity in his \"Principia\" (1686). This later led to a priority debate between Newton and Hooke through correspondence.\nReferences.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41289", "revid": "768664306", "url": "https://en.wikipedia.org/wiki?curid=41289", "title": "Ionospheric reflection", "text": ""}
{"id": "41290", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41290", "title": "Ionospheric sounding", "text": "Radio technique\nIn telecommunications and radio science, an ionospheric sounding is a technique that provides real-time data on high-frequency ionospheric-dependent radio propagation, using a basic system consisting of a synchronized transmitter and receiver. \nThe time delay between transmission and reception is translated into effective ionospheric layer altitude. Vertical incident sounding uses a collocated transmitter and receiver and involves directing a range of frequencies vertically to the ionosphere and measuring the values of the reflected returned signals to determine the effective ionosphere layer altitude. This technique is also used to determine the critical frequency. Oblique sounders use a transmitter at one end of a given propagation path, and a synchronized receiver, usually with an oscilloscope-type display (ionogram), at the other end. The transmitter emits a stepped- or swept-frequency signal which is displayed or measured at the receiver. The measurement converts time delay to effective altitude of the ionospheric layer. The ionogram display shows the effective altitude of the ionospheric layer as a function of frequency."}
{"id": "41291", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41291", "title": "Isochronous timing", "text": "Events occurring regularly, or at equal time intervals\nA sequence of events is isochronous if the events occur regularly, or at equal time intervals. The term \"isochronous\" is used in several technical contexts, but usually refers to the primary subject maintaining a constant period or interval (the reciprocal of frequency), despite variations in other measurable factors in the same system. Isochronous timing is a characteristic of a repeating event, whereas synchronous timing refers to the relationship between two or more events.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41292", "revid": "7903804", "url": "https://en.wikipedia.org/wiki?curid=41292", "title": "Isochronous burst transmission", "text": "Method of transmitting data at a lower rate than the bearer signalling rate\nIsochronous burst transmission is a method of transmission. In a data network where the information-bearer channel rate is higher than the input data signaling rate, transmission is performed by interrupting, at controlled intervals, the data stream being transmitted.\n\"Note 1:\" Burst transmission in isochronous form enables communication between data terminal equipment (DTE) and data networks that operate at dissimilar data signaling rates, such as when the information-bearer channel rate is higher than the DTE output data signaling rate.\n\"Note 2:\" The binary digits are transferred at the information-bearer channel rate. The data transfer is interrupted at intervals in order to produce the required average data signaling rate.\n\"Note 3:\" The interruption is always for an integral number of unit intervals.\n\"Note 4:\" Isochronous burst transmission has particular application where envelopes are being transferred between data circuit terminating equipment (DCE) and only the bytes contained within the envelopes are being transferred between the DCE and the DTE. \"Synonyms\": burst isochronous \"(deprecated)\", interrupted isochronous transmission.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41293", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41293", "title": "Isochronous signal", "text": "Signal occurring regularly, or at equal time intervals\nIn telecommunications, an isochronous signal is a signal in which the time interval separating any two significant instants is equal to the unit interval or a multiple of the unit interval. Variations in the time intervals are constrained within specified limits.\n\"Isochronous\" is a characteristic of one signal, while \"synchronous\" indicates a relationship between two or more signals.\nReferences.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41294", "revid": "1754504", "url": "https://en.wikipedia.org/wiki?curid=41294", "title": "Jam signal", "text": ""}
{"id": "41295", "revid": "46628330", "url": "https://en.wikipedia.org/wiki?curid=41295", "title": "Jerkiness", "text": "Perception of all frames while watching a film\nJerkiness (sometimes called strobing or choppy footage) describes the perception of individual still images while watching a motion picture.\nDescription.\nMotion pictures are made from still images shown in rapid sequence. Provided there is sufficient continuity between the images and provided the sequence is shown fast enough, the central nervous system interprets the sequence as continuous motion. However, some technologies cannot process or carry data fast enough for sufficiently high frame rates. For example, viewing motion pictures by Internet connection generally necessitates a greatly reduced frame rate, making jerkiness clearly apparent.\nIn conventional cinematography, the images are filmed and displayed at 24 frames per second, at which speed jerkiness is not normally discernible. Television screens refresh at even higher frequencies. PAL and S\u00c9CAM television (the standards in Europe) refresh at 25 or 50 (HDTV) frames per second. NTSC television displays (the standard in North America) refresh at 29.97 frames per second.\nAnimated cartoon films are typically made at reduced frame rates (accomplished by shooting several film frames of the individual drawings) so as to limit production costs, with the result that jerkiness tends to be apparent, especially on older limited animation features.\nOther uses.\nStrobing can also refer to cross colour and Moir\u00e9 patterning. Cross colour refers to when any high frequency luminance content of the picture, close to the TV system's colour sub-carrier frequency, is interpreted by the analogue receiver's decoder as colour information. Moir\u00e9 patterning is where an interference pattern is produced by fine scene detail beating with the line (or even pixel) structure of the device used to analyse or display the scene.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41296", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41296", "title": "Jitter", "text": "Clock deviation from perfect periodicity\nIn electronics and telecommunications, jitter is the deviation from true periodicity of a presumably periodic signal, often in relation to a reference clock signal. In clock recovery applications it is called timing jitter. Jitter is a significant, and usually undesired, factor in the design of almost all communications links.\nJitter can be quantified in the same terms as all time-varying signals, e.g., root mean square (RMS), or peak-to-peak displacement. Also, like other time-varying signals, jitter can be expressed in terms of spectral density.\nJitter period is the interval between two times of maximum effect (or minimum effect) of a signal characteristic that varies regularly with time. Jitter frequency, the more commonly quoted figure, is its inverse. ITU-T G.810 classifies deviation lower frequencies below 10\u00a0Hz as wander and higher frequencies at or above 10\u00a0Hz as \"jitter\".\nJitter may be caused by electromagnetic interference and crosstalk with carriers of other signals. Jitter can cause a display monitor to flicker, affect the performance of processors in personal computers, introduce clicks or other undesired effects in audio signals, and cause loss of transmitted data between network devices. The amount of tolerable jitter depends on the affected application.\nMetrics.\nFor clock jitter, there are three commonly used metrics:\nThe absolute difference in the position of a clock's edge from where it would ideally be.\nMaximum error committed by a clock under test in measuring a time interval for a given period of time.\nThe difference between any one clock period and the ideal or average clock period. Period jitter tends to be important in synchronous circuitry such as digital state machines, where the error-free operation of the circuitry is limited by the shortest possible clock period (average period less maximum cycle jitter), and the performance of the circuitry is set by the average clock period. Hence, synchronous circuitry benefits from minimizing period jitter, so that the shortest clock period approaches the average clock period.\nThe difference in duration of any two adjacent clock periods. It can be important for some types of clock generation circuitry used in microprocessors and RAM interfaces.\nIn telecommunications, the unit used for the above types of jitter is usually the unit interval (UI) which quantifies the jitter in terms of a fraction of the transmission unit period. This unit is useful because it scales with clock frequency and thus allows relatively slow interconnects such as T1 to be compared to higher-speed internet backbone links such as OC-192. Absolute units such as \"picoseconds\" are more common in microprocessor applications. Units of \"degrees\" and \"radians\" are also used.\nIf jitter has a Gaussian distribution, it is usually quantified using the standard deviation of this distribution. This translates to an RMS measurement for a zero-mean distribution. Often, jitter distribution is significantly non-Gaussian. This can occur if the jitter is caused by external sources such as power supply noise. In these cases, \"peak-to-peak\" measurements may be more useful. Many efforts have been made to meaningfully quantify distributions that are neither Gaussian nor have a meaningful peak level. All have shortcomings but most tend to be good enough for the purposes of engineering work.\nIn computer networking, \"jitter\" can refer to packet delay variation, the variation (statistical dispersion) in the delay of the packets.\nTypes.\nOne of the main differences between random and deterministic jitter is that deterministic jitter is bounded and random jitter is unbounded.\nRandom jitter.\nRandom jitter, also called Gaussian jitter, is unpredictable electronic timing noise. Random jitter typically follows a normal distribution due to being caused by thermal noise in an electrical circuit.\nDeterministic jitter.\nDeterministic jitter is a type of clock or data signal jitter that is predictable and reproducible. The peak-to-peak value of this jitter is bounded, and the bounds can easily be observed and predicted. Deterministic jitter has a known non-normal distribution. Deterministic jitter can either be correlated to the data stream (data-dependent jitter) or uncorrelated to the data stream (bounded uncorrelated jitter). Examples of data-dependent jitter are duty-cycle dependent jitter (also known as duty-cycle distortion) and intersymbol interference.\nTotal jitter.\nTotal jitter (\"T\") is the combination of random jitter (\"R\") and deterministic jitter (\"D\") and is computed in the context to a required bit error rate (BER) for the system:\n\"T\" = \"D\"peak-to-peak + 2\"nR\"rms,\nin which the value of \"n\" is based on the BER required of the link.\nA common BER used in communication standards such as Ethernet is 10\u221212.\nExamples.\nSampling jitter.\nIn analog-to-digital and digital-to-analog conversion of signals, the sampling is normally assumed to be periodic with a fixed period\u2014the time between every two samples is the same. If there is jitter present on the clock signal to the analog-to-digital converter or a digital-to-analog converter, the time between samples varies and instantaneous signal error arises. The error is proportional to the slew rate of the desired signal and the absolute value of the clock error. The effect of jitter on the signal depends on the nature of the jitter. Random jitter tends to add broadband noise while periodic jitter tends to add errant spectral components, \"birdys\". In some conditions, less than a nanosecond of jitter can reduce the effective bit resolution of a converter with a Nyquist frequency of 22\u00a0kHz to 14 bits.\nSampling jitter is an important consideration in high-frequency signal conversion, or where the clock signal is especially prone to interference.\nIn digital antenna arrays ADC and DAC jitters are the important factors determining the direction of arrival estimation accuracy and the depth of jammers suppression.\nPacket jitter in computer networks.\nIn the context of computer networks, packet jitter or packet delay variation (PDV) is the variation in latency as measured in the variability over time of the end-to-end delay across a network. A network with constant delay has no packet jitter. Packet jitter is expressed as an average of the deviation from the network mean delay. PDV is an important quality of service factor in assessment of network performance.\nTransmitting a burst of traffic at a high rate followed by an interval or period of lower or zero rate transmission may also be seen as a form of jitter, as it represents a deviation from the average transmission rate. However, unlike the jitter caused by variation in latency, transmitting in bursts may be seen as a desirable feature, e.g. in variable bitrate transmissions.\nVideo and image jitter.\nVideo or image jitter occurs when the horizontal lines of video image frames are randomly displaced due to the corruption of synchronization signals or electromagnetic interference during video transmission. Model-based dejittering study has been carried out under the framework of digital image and video restoration.\nTesting.\nJitter in serial bus architectures is measured by means of eye patterns. There are standards for jitter measurement in serial bus architectures. The standards cover jitter tolerance, jitter transfer function and jitter generation, with the required values for these attributes varying among different applications. Where applicable, compliant systems are required to conform to these standards.\nTesting for jitter and its measurement is of growing importance to electronics engineers because of increased clock frequencies in digital electronic circuitry to achieve higher device performance. Higher clock frequencies have commensurately smaller eye openings and thus impose tighter tolerances on jitter. For example, modern computer motherboards have serial bus architectures with eye openings of 160 picoseconds or less. This is extremely small compared to parallel bus architectures with equivalent performance, which may have eye openings on the order of 1000 picoseconds.\nJitter is measured and evaluated in various ways depending on the type of circuit under test. In all cases, the goal of jitter measurement is to verify that the jitter will not disrupt normal operation of the circuit.\nTesting of device performance for jitter tolerance may involve injection of jitter into electronic components with specialized test equipment.\nA less direct approach\u2014in which analog waveforms are digitized and the resulting data stream analyzed\u2014is employed when measuring pixel jitter in frame grabbers.\nMitigation.\nAnti-jitter circuits.\nAnti-jitter circuits (AJCs) are a class of electronic circuits designed to reduce the level of jitter in a clock signal. AJCs operate by re-timing the output pulses so they align more closely to an idealized clock. They are widely used in clock and data recovery circuits in digital communications, as well as for data sampling systems such as the analog-to-digital converter and digital-to-analog converter. Examples of anti-jitter circuits include phase-locked loop and delay-locked loop.\nJitter buffers.\nJitter buffers or de-jitter buffers are buffers used to counter jitter introduced by queuing in packet-switched networks to ensure continuous playout of an audio or video media stream transmitted over the network. The maximum jitter that can be countered by a de-jitter buffer is equal to the buffering delay introduced before starting the play-out of the media stream. In the context of packet-switched networks, the term \"packet delay variation\" is often preferred over \"jitter\".\nSome systems use sophisticated delay-optimal de-jitter buffers that are capable of adapting the buffering delay to changing network characteristics. The adaptation logic is based on the jitter estimates computed from the arrival characteristics of the media packets. Adjustments associated with adaptive de-jittering involve introducing discontinuities in the media play-out, which may be noticeable to the listener or viewer. Adaptive de-jittering is usually carried out for audio play-outs that include voice activity detection, which allows the lengths of the silence periods to be adjusted, thus minimizing the perceptual impact of the adaptation.\nDejitterizer.\nA dejitterizer is a device that reduces jitter in a digital signal. A dejitterizer usually consists of an elastic buffer in which the signal is temporarily stored and then retransmitted at a rate based on the average rate of the incoming signal. A dejitterizer may not be effective in removing low-frequency jitter (wander).\nFiltering and decomposition.\nA filter can be designed to minimize the effect of sampling jitter.\nA jitter signal can be decomposed into intrinsic mode functions (IMFs), which can be further applied for filtering or dejittering.\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41297", "revid": "11994166", "url": "https://en.wikipedia.org/wiki?curid=41297", "title": "Joint multichannel trunking and switching system", "text": "Composite multichannel trunking and switching system formed from assets of the Services\nThe Joint multichannel trunking and switching system is that composite multichannel trunking and switching system formed from assets of the Services, the Defense Communications System, other available systems, and/or assets controlled by the Joint Chiefs of Staff to provide an operationally responsive, survivable communication system, preferably in a mobile/transportable/recoverable configuration, for the joint force commander in an area of operations. "}
{"id": "41299", "revid": "11555324", "url": "https://en.wikipedia.org/wiki?curid=41299", "title": "Justify", "text": "Justify may refer to:\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "41300", "revid": "43283345", "url": "https://en.wikipedia.org/wiki?curid=41300", "title": "Kendall effect", "text": "Spurious pattern or other distortion in a facsimile\nIn telecommunications the Kendall effect is a spurious pattern or other distortion in a facsimile.\nIt is caused by unwanted modulation products which arise from the transmission of the carrier signal, and appear in the form of a rectified baseband that interferes with the lower sideband of the carrier. \nThe Kendall effect occurs principally when the single-sideband width is greater than half of the facsimile carrier frequency.\nReferences.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41301", "revid": "194203", "url": "https://en.wikipedia.org/wiki?curid=41301", "title": "Kerr electro-optic effect", "text": ""}
{"id": "41302", "revid": "8066546", "url": "https://en.wikipedia.org/wiki?curid=41302", "title": "Key pulsing", "text": ""}
{"id": "41303", "revid": "1313470772", "url": "https://en.wikipedia.org/wiki?curid=41303", "title": "K-factor", "text": "K-factor or K factor may refer to:\n&lt;templatestyles src=\"Template:TOC_right/styles.css\" /&gt;\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "41304", "revid": "6727347", "url": "https://en.wikipedia.org/wiki?curid=41304", "title": "Knife-edge effect", "text": ""}
{"id": "41305", "revid": "7098284", "url": "https://en.wikipedia.org/wiki?curid=41305", "title": "Label (disambiguation)", "text": "A label is any kind of tag attached to something so as to identify the object or its content. It may refer to:\n&lt;templatestyles src=\"Template:TOC_right/styles.css\" /&gt;\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "41306", "revid": "35936988", "url": "https://en.wikipedia.org/wiki?curid=41306", "title": "Lambert's cosine law", "text": "Description in optics of the angular dependency of the radiant intensity of a radiant surface\nIn optics, Lambert's cosine law says that the observed radiant intensity or luminous intensity from an ideal diffusely reflecting surface or ideal diffuse radiator is directly proportional to the cosine of the angle \"\u03b8\" between the observer's line of sight and the surface normal; \"I\" = \"I\"0 cos \"\u03b8\". The law is also known as the cosine emission law or Lambert's emission law. It is named after Johann Heinrich Lambert, from his \"Photometria\", published in 1760.\nA surface which obeys Lambert's law is said to be \"Lambertian\", and exhibits Lambertian reflectance. Such a surface has a constant radiance/luminance, regardless of the angle from which it is observed; a single human eye perceives such a surface as having a constant brightness, regardless of the angle from which the eye observes the surface. It has the same radiance because, although the emitted power from a given area element is reduced by the cosine of the emission angle, the solid angle, subtended by surface visible to the viewer, is reduced by the very same amount. Because the ratio between power and solid angle is constant, radiance (power per unit solid angle per unit projected source area) stays the same.\nLambertian scatterers and radiators.\nWhen an area element is radiating as a result of being illuminated by an external source, the irradiance (energy or photons /time/area) landing on that area element will be proportional to the cosine of the angle between the illuminating source and the normal. A Lambertian scatterer will then scatter this light according to the same cosine law as a Lambertian emitter. This means that although the radiance of the surface depends on the angle from the normal to the illuminating source, it will not depend on the angle from the normal to the observer. For example, if the moon were a Lambertian scatterer, one would expect to see its scattered brightness appreciably diminish towards the terminator due to the increased angle at which sunlight hit the surface. The fact that it does not diminish illustrates that the moon is not a Lambertian scatterer, and in fact tends to scatter more light into the oblique angles than a Lambertian scatterer.\nThe emission of a Lambertian radiator does not depend on the amount of incident radiation, but rather from radiation originating in the emitting body itself. For example, if the sun were a Lambertian radiator, one would expect to see a constant brightness across the entire solar disc. The fact that the sun exhibits limb darkening in the visible region illustrates that it is not a Lambertian radiator. A black body is an example of a Lambertian radiator. According to Tatum, \"In discussing the properties of reflecting surfaces, one often distinguishes between two extreme cases. At the one hand is the perfectly diffusing lambertian surface; blotting paper is sometimes cited as a near lambertian example. The other extreme is the perfectly reflecting surface, or specular reflection (Latin speculum, a mirror), in which the angle of reflection equals the angle of incidence.\"\nDetails of equal brightness effect.\nThe situation for a Lambertian surface (emitting or scattering) is illustrated in Figures 1 and 2. For conceptual clarity we will think in terms of photons rather than energy or luminous energy. The wedges in the circle each represent an equal angle \"d\"\u03a9, of an arbitrarily chosen size, and for a Lambertian surface, the number of photons per second emitted into each wedge is proportional to the area of the wedge.\nThe length of each wedge is the product of the diameter of the circle and cos(\"\u03b8\"). The maximum rate of photon emission per unit solid angle is along the normal, and diminishes to zero for \"\u03b8\" = 90\u00b0. In mathematical terms, the radiance along the normal is \"I\"\u00a0photons/(s\u00b7m2\u00b7sr) and the number of photons per second emitted into the vertical wedge is \"I\" \"d\"\u03a9 \"dA\". The number of photons per second emitted into the wedge at angle \"\u03b8\" is \"I\" cos(\"\u03b8\") \"d\"\u03a9 \"dA\".\nFigure 2 represents what an observer sees. The observer directly above the area element will be seeing the scene through an aperture of area \"dA\"0 and the area element \"dA\" will subtend a (solid) angle of \"d\"\u03a90, which is a portion of the observer's total angular field-of-view of the scene. Since the wedge size \"d\"\u03a9 was chosen arbitrarily, for convenience we may assume without loss of generality that it coincides with the solid angle subtended by the aperture when \"viewed\" from the locus of the emitting area element dA. Thus the normal observer will then be recording the same \"I\" \"d\"\u03a9 \"dA\" photons per second emission derived above and will measure a radiance of\nformula_1 photons/(s\u00b7m2\u00b7sr).\nThe observer at angle \"\u03b8\" to the normal will be seeing the scene through the same aperture of area \"dA\"0 (still corresponding to a \"d\"\u03a9 wedge) and from this oblique vantage the area element \"dA\" is foreshortened and will subtend a (solid) angle of \"d\"\u03a90\u00a0cos(\"\u03b8\"). This observer will be recording \"I\" cos(\"\u03b8\") \"d\"\u03a9 \"dA\" photons per second, and so will be measuring a radiance of\nformula_2 photons/(s\u00b7m2\u00b7sr),\nwhich is the same as the normal observer. In the words of Tatum, \"Thus the radiance of a lambertian radiating surface is independent of the angle from which it is viewed. Lambertian surfaces radiate isotropically. For a reflecting surface to be lambertian, it is required that the radiance be independent not only of the angle from which it is viewed, but also of the angle from which it is irradiated (or illuminated).\" In the words of Yeo, \"To put it in lay terms, the brightness of a\nLambertian (or perfect diffuse reflector) remains constant as you view it from different angles. This is because, the change in intensity with angle (the cosine relationship) is countered by an equal but opposite change in the projected surface area that you view (also a cosine relationship). Thus the Lambertian surface will possess the same brightness (luminance or radiance) regardless of the angle that you view it from.\"\nRelating peak luminous intensity and luminous flux.\nIn general, the luminous intensity of a point on a surface varies by direction; for a Lambertian surface, that distribution is defined by the cosine law, with peak luminous intensity in the normal direction. Thus when the Lambertian assumption holds, we can calculate the total luminous flux, formula_3, from the peak luminous intensity, formula_4, by integrating the cosine law:\nformula_5\nand so\nformula_6\nwhere formula_7 is the determinant of the Jacobian matrix for the unit sphere, and realizing that formula_4 is luminous flux per steradian. According to Tatum, \"These relations apply equally to subscripted flux and intensity and to luminous flux and luminous intensity.\"\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41307", "revid": "2732624", "url": "https://en.wikipedia.org/wiki?curid=41307", "title": "Landscape mode", "text": ""}
{"id": "41308", "revid": "1951353", "url": "https://en.wikipedia.org/wiki?curid=41308", "title": "Launch angle", "text": "In fiber optic telecommunications, the launch angle has the following meanings:\nIn baseball, the \"launch angle\" is the angle, with respect to the ground level at home plate, at which a batted ball leaves the bat.\nReferences.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41309", "revid": "1951353", "url": "https://en.wikipedia.org/wiki?curid=41309", "title": "Launch numerical aperture", "text": "Numerical aperture of an optical system used to couple power into an optical fiber\nIn telecommunications, launch numerical aperture (LNA) is the numerical aperture of an optical system used to couple (launch) power into an optical fiber.\nLNA may differ from the stated NA of a final focusing element if, for example, that element is underfilled or the focus is other than that for which the element is specified. \nLNA is one of the parameters that determine the initial distribution of power among the modes of an optical fiber.\nReferences.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41310", "revid": "1303623714", "url": "https://en.wikipedia.org/wiki?curid=41310", "title": "Layer", "text": "Layer or layered may refer to:\n&lt;templatestyles src=\"Template:TOC_right/styles.css\" /&gt;\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "41311", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41311", "title": "Layered system", "text": "In telecommunications, a layered system is a system in which components are grouped, \"i.e.\", layered, in a hierarchical arrangement, such that lower layers provide functions and services that support the functions and services of higher layers. \nSystems of ever-increasing complexity and capability can be built by adding or changing the layers to improve overall system capability while using the components that are still in place.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41312", "revid": "1461430", "url": "https://en.wikipedia.org/wiki?curid=41312", "title": "Leaky mode", "text": "A leaky mode or tunneling mode in an optical fiber or other waveguide is a mode having an electric field that decays monotonically for a finite distance in the transverse direction but becomes oscillatory everywhere beyond that finite distance. Such a mode gradually \"leaks\" out of the waveguide as it travels down it, producing attenuation even if the waveguide is perfect in every respect. In order for a leaky mode to be definable as a mode, the relative amplitude of the oscillatory part (the leakage rate) must be sufficiently small that the mode substantially maintains its shape as it decays.\nLeaky modes correspond to leaky rays in the terminology of geometric optics.\nThe propagation of light through optical fibre can take place via meridional rays or skew rays. These skew rays suffer only partial reflection while meridional rays are completely guided. Thus the modes allowing propagation of skew rays are called leaky modes. \nSome optical power is lost into clad due to these modes.\nReferences.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41313", "revid": "194203", "url": "https://en.wikipedia.org/wiki?curid=41313", "title": "Leaky ray", "text": ""}
{"id": "41314", "revid": "1152308", "url": "https://en.wikipedia.org/wiki?curid=41314", "title": "Limiting", "text": ""}
{"id": "41316", "revid": "43624274", "url": "https://en.wikipedia.org/wiki?curid=41316", "title": "Linear polarization", "text": "Electromagnetic radiation special case\nIn electrodynamics, linear polarization or plane polarization of electromagnetic radiation is a confinement of the electric field vector or magnetic field vector to a given plane along the direction of propagation. The term \"linear polarization\" (French: \"polarisation rectiligne\") was coined by Augustin-Jean Fresnel in 1822. See \"polarization\" and \"plane of polarization\" for more information.\nThe orientation of a linearly polarized electromagnetic wave is defined by the direction of the electric field vector. For example, if the electric field vector is vertical (alternately up and down as the wave travels) the radiation is said to be vertically polarized.\nMathematical description.\nThe classical sinusoidal plane wave solution of the electromagnetic wave equation for the electric and magnetic fields is (cgs units)\nformula_1\nformula_2\nfor the magnetic field, where k is the wavenumber,\nformula_3\nis the angular frequency of the wave, and formula_4 is the speed of light.\nHere formula_5 is the amplitude of the field and\nformula_6\nis the Jones vector in the x-y plane.\nThe wave is linearly polarized when the phase angles formula_7 are equal,\nformula_8.\nThis represents a wave polarized at an angle formula_9 with respect to the x axis. In that case, the Jones vector can be written\nformula_10.\nThe state vectors for linear polarization in x or y are special cases of this state vector.\nIf unit vectors are defined such that\nformula_11\nand\nformula_12\nthen the polarization state can be written in the \"x-y basis\" as\nformula_13.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nExternal links.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41317", "revid": "54234", "url": "https://en.wikipedia.org/wiki?curid=41317", "title": "Line code", "text": "Pattern used within a communications system to represent digital data\n&lt;templatestyles src=\"Stack/styles.css\"/&gt;\nIn telecommunications, a line code is a pattern of voltage, current, or photons used to represent digital data transmitted down a communication channel or written to a storage medium. This repertoire of signals is usually called a constrained code in data storage systems. \nSome signals are more prone to error than others as the physics of the communication channel or storage medium constrains the repertoire of signals that can be used reliably.\nCommon line encodings are unipolar, polar, bipolar, and Manchester code.\nTransmission and storage.\nAfter line coding, the signal is put through a physical communication channel, either a transmission medium or data storage medium. The most common physical channels are:\nSome of the more common binary line codes include:\nEach line code has advantages and disadvantages. Line codes are chosen to meet one or more of the following criteria:\nDisparity.\nMost long-distance communication channels cannot reliably transport a DC component. The DC component is also called the \"disparity\", the \"bias\", or the DC coefficient. The disparity of a bit pattern is the difference in the number of one bits vs the number of zero bits. The \"running disparity\" is the running total of the disparity of all previously transmitted bits. The simplest possible line code, unipolar, gives too many errors on such systems, because it has an unbounded DC component.\nMost line codes eliminate the DC component\u00a0\u2013 such codes are called DC-balanced, zero-DC, or DC-free. There are three ways of eliminating the DC component:\nPolarity.\nBipolar line codes have two polarities, are generally implemented as RZ, and have a radix of three since there are three distinct output levels (negative, positive and zero). One of the principal advantages of this type of code is that it can eliminate any DC component. This is important if the signal must pass through a transformer or a long transmission line.\nUnfortunately, several long-distance communication channels have polarity ambiguity. Polarity-insensitive line codes compensate in these channels.\nThere are three ways of providing unambiguous reception of 0 and 1 bits over such channels:\nRun-length limited codes.\nFor reliable clock recovery at the receiver, a run-length limitation may be imposed on the generated channel sequence, i.e., the maximum number of consecutive ones or zeros is bounded to a reasonable number. A clock period is recovered by observing transitions in the received sequence, so that a maximum run length guarantees sufficient transitions to assure clock recovery quality.\nRLL codes are defined by four main parameters: \"m\", \"n\", \"d\", \"k\". The first two, \"m\"/\"n\", refer to the rate of the code, while the remaining two specify the minimal \"d\" and maximal \"k\" number of zeroes between consecutive ones. This is used in both telecommunications and storage systems that move a medium past a fixed recording head.\nSpecifically, RLL bounds the length of stretches (runs) of repeated bits during which the signal does not change. If the runs are too long, clock recovery is difficult; if they are too short, the high frequencies might be attenuated by the communications channel. By modulating the data, RLL reduces the timing uncertainty in decoding the stored data, which would lead to the possible erroneous insertion or removal of bits when reading the data back. This mechanism ensures that the boundaries between bits can always be accurately found (preventing bit slip), while efficiently using the media to reliably store the maximal amount of data in a given space.\nEarly disk drives used very simple encoding schemes, such as RLL (0,1) FM code, followed by RLL (1,3) MFM code which were widely used in hard disk drives until the mid-1980s and are still used in digital optical discs such as CD, DVD, MD, Hi-MD and Blu-ray using EFM and EFMPLus codes. Higher density RLL (2,7) and RLL (1,7) codes became the de facto standards for hard disks by the early 1990s.\nSynchronization.\nLine coding should make it possible for the receiver to synchronize itself to the phase of the received signal. If the clock recovery is not ideal, then the signal to be decoded will not be sampled at the optimal times. This will increase the probability of error in the received data.\nBiphase line codes require at least one transition per bit time. This makes it easier to synchronize the transceivers and detect errors, however, the baud rate is greater than that of NRZ codes.\nOther considerations.\nA line code will typically reflect technical requirements of the transmission medium, such as optical fiber or shielded twisted pair. These requirements are unique for each medium, because each one has different behavior related to interference, distortion, capacitance and attenuation.\nCommon line codes.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41318", "revid": "208308", "url": "https://en.wikipedia.org/wiki?curid=41318", "title": "Network repeaters", "text": ""}
{"id": "41319", "revid": "26074453", "url": "https://en.wikipedia.org/wiki?curid=41319", "title": "Link", "text": "Link or Links may refer to:\n&lt;templatestyles src=\"Template:TOC_right/styles.css\" /&gt;\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "41320", "revid": "46895798", "url": "https://en.wikipedia.org/wiki?curid=41320", "title": "Link level", "text": "In computer networking, in the hierarchical structure of a primary or secondary station, link level is the conceptual level of control or data processing logic that controls the data link. \nLink-level functions provide an interface between the station high-level logic and the data link. Link-level functions include (a) transmit bit injection and receive bit extraction, (b) address and control field interpretation, (c) command response generation, transmission and interpretation, and (d) frame check sequence computation and interpretation.\nReferences.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41321", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41321", "title": "Link quality analysis", "text": "Overall process in adaptive high-frequency (HF) radio\nIn adaptive high-frequency (HF) radio, link quality analysis (LQA) is the overall process by which measurements of radio signal quality are made, assessed, and analyzed.\nIn LQA, signal quality is determined by measuring, assessing, and analyzing link parameters, such as bit error ratio (BER), and the levels of the ratio of signal-plus-noise-plus-distortion to noise-plus-distortion (SINAD). Measurements are stored at\u2014and exchanged between\u2014stations, for use in making decisions about link establishment.\nFor adaptive HF radio, LQA is automatically performed and is usually based on analyses of pseudo-BERs and SINAD readings.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "41322", "revid": "37052726", "url": "https://en.wikipedia.org/wiki?curid=41322", "title": "Lip synchronization", "text": ""}
{"id": "41323", "revid": "635492", "url": "https://en.wikipedia.org/wiki?curid=41323", "title": "Load", "text": "Load or LOAD may refer to:\n&lt;templatestyles src=\"Template:TOC_right/styles.css\" /&gt;\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "41325", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41325", "title": "Loading characteristic", "text": "In multichannel telephone systems, the loading characteristic is a plot, for the busy hour, of the equivalent mean power and the peak power as a function of the number of voice channels. \nThe equivalent power of a multichannel signal referred to the zero transmission level point is a function of the number of channels and has for its basis a specified voice channel mean power.\nReferences.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41326", "revid": "1317128396", "url": "https://en.wikipedia.org/wiki?curid=41326", "title": "Loading coil", "text": "Inductor in a transmission line\nA loading coil, or load coil, is an inductor that is inserted into an electronic circuit to increase its inductance. The term originated in the 19th century for inductors used to prevent signal distortion in long-distance telegraph transmission cables. The term is also used for inductors in radio antennas, or between the antenna and its feedline, to make an electrically short antenna resonant at its operating frequency.\nThe concept of loading coils was discovered by Oliver Heaviside in studying the problem of slow signalling speed of the first transatlantic telegraph cable in the 1860s. He concluded additional inductance was required to prevent amplitude and time delay distortion of the transmitted signal. The mathematical condition for distortion-free transmission is known as the Heaviside condition. Previous telegraph lines were overland or shorter and hence had less delay, and the need for extra inductance was not as great. Submarine communications cables are particularly subject to the problem, but early 20th century installations using balanced pairs were often continuously loaded with iron wire or tape rather than discretely with loading coils, which avoided the sealing problem.\nLoading coils are historically also known as Pupin coils after Mihajlo Pupin, especially when used for the Heaviside condition and the process of inserting them is sometimes called \"pupinization\".\nApplications.\nTelephone lines.\nA common application of loading coils is to improve the voice-frequency amplitude response characteristics of the twisted balanced pairs in a telephone cable. Because twisted pair is a balanced format, half the loading coil must be inserted in each leg of the pair to maintain the balance. It is common for both these windings to be formed on the same core. This increases the flux linkages, without which the number of turns on the coil would need to be increased. Despite the use of common cores, such loading coils do not comprise transformers, as they do not provide coupling to other circuits.\nLoading coils inserted periodically in series with a pair of wires reduce the attenuation at the higher voice frequencies up to the cutoff frequency of the low-pass filter formed by the inductance of the coils (plus the distributed inductance of the wires) and the distributed capacitance between the wires. Above the cutoff frequency, attenuation increases rapidly. The shorter the distance between the coils, the higher the cut-off frequency. The cutoff effect is an artifact of using lumped inductors. With loading methods using continuous distributed inductance there is no cutoff.\nWithout loading coils, the line response is dominated by the resistance and capacitance of the line with the attenuation gently increasing with frequency. With loading coils of exactly the right inductance, neither capacitance nor inductance dominate: the response is flat, waveforms are undistorted and the characteristic impedance is resistive up to the cutoff frequency. The coincidental formation of an audio frequency filter is also beneficial in that noise is reduced.\nDSL.\nWith loading coils, signal attenuation of a circuit remains low for signals within the passband of the transmission line but increases rapidly for frequencies above the audio cutoff frequency. If the telephone line is subsequently reused to support applications that require higher frequencies, such as in analog or digital carrier systems or digital subscriber line (DSL), loading coils must be removed or replaced. Using coils with parallel capacitors forms a filter with the topology of an m-derived filter and a band of frequencies above the cut-off is also passed. Without removal, for subscribers at an extended distance, e.g., over 4 miles (6.4\u00a0km) from the central office, DSL cannot be supported.\nCarrier systems.\nAmerican early and middle 20th century telephone cables had load coils at intervals of a mile (1.61\u00a0km), usually in coil cases holding many. The coils had to be removed to pass higher frequencies, but the coil cases provided convenient places for repeaters of digital T-carrier systems, which could then transmit a 1.5\u00a0Mbit/s signal that distance. Due to narrower streets and higher cost of copper, European cables had thinner wires and used closer spacing. Intervals of a kilometer allowed European systems to carry 2\u00a0Mbit/s.\nRadio antenna.\nAnother type of loading coil is used in radio antennas. Monopole and dipole radio antennas are designed to act as resonators for radio waves; the power from the transmitter, applied to the antenna through the antenna's transmission line, excites standing waves of voltage and current in the antenna element. To be \"naturally\" resonant, the antenna must have a physical length of one quarter of the wavelength of the radio waves used (or a multiple of that length, with odd multiples usually preferred). At resonance, the antenna acts electrically as a pure resistance, absorbing all the power applied to it from the transmitter.\nIn many cases, for practical reasons, it is necessary to make the antenna shorter than the resonant length, this is called an electrically short antenna. An antenna shorter than a quarter wavelength presents capacitive reactance to the transmission line. Some of the applied power is reflected back into the transmission line and travels back toward the transmitter . The two currents at the same frequency running in opposite directions causes standing waves on the transmission line , measured as a standing wave ratio (SWR) greater than one. The elevated currents waste energy by heating the wire, and can even overheat the transmitter.\nTo make an electrically short antenna resonant, a loading coil is inserted in series with the antenna. The coil is built to have an inductive reactance equal and opposite to the capacitive reactance of the short antenna, so the combination of reactances cancels. When so loaded the antenna presents a pure resistance to the transmission line, preventing energy from being reflected. The loading coil is often placed at the base of the antenna, between it and the transmission line (\"base loading\"), but for more efficient radiation, it is sometimes inserted near the midpoint of the antenna element (\"center loading\").\nLoading coils for powerful transmitters can have challenging design requirements, especially at low frequencies. The radiation resistance of short antennas can be very low, as low a few ohms in the LF or VLF bands, where antennas are commonly short and inductive loading is most needed. Because resistance in the coil winding is comparable to, or exceeds the radiation resistance, loading coils for extremely electrically short antennas must have extremely low AC resistance at the operating frequency. To reduce skin effect losses, the coil is often made of tubing or Litz wire, with single layer windings, with turns spaced apart to reduce proximity effect resistance. They must often handle high voltages. To reduce power lost in dielectric losses, the coil is often suspended in air supported on thin ceramic strips. The capacitively loaded antennas used at low frequencies have extremely narrow bandwidths, and therefore if the frequency is changed the loading coil must be adjustable to tune the antenna to resonance with the new transmitter frequency. Variometers are often used.\nBulk power transmission.\nTo reduce losses due to high capacitance on long-distance bulk power transmission lines, inductance can be introduced to the circuit with a flexible AC transmission system (FACTS), a static VAR compensator, or a static synchronous series compensator. Series compensation can be thought of as an inductor connected to the circuit in series if it is supplying inductance to the circuit.\nCampbell equation.\nThe Campbell equation is a relationship due to George Ashley Campbell for predicting the propagation constant of a loaded line. It is stated as;\nformula_1\nwhere,\nformula_2 is the propagation constant of the unloaded line\nformula_3 is the propagation constant of the loaded line\nformula_4 is the interval between coils on the loaded line\nformula_5 is the impedance of a loading coil and\nformula_6 is the characteristic impedance of the unloaded line.\nA more engineer friendly rule of thumb is that the approximate requirement for spacing loading coils is ten coils per wavelength of the maximum frequency being transmitted. This approximation can be arrived at by treating the loaded line as a constant k filter and applying image filter theory to it. From basic image filter theory the angular cutoff frequency and the characteristic impedance of a low-pass constant k filter are given by;\nformula_7 \u00a0and,\u00a0 formula_8\nwhere formula_9 and formula_10 are the half section element values.\nFrom these basic equations the necessary loading coil inductance and coil spacing can be found;\nformula_11 \u00a0and,\u00a0 formula_12\nwhere C is the capacitance per unit length of the line.\nExpressing this in terms of number of coils per cutoff wavelength yields;\nformula_13\nwhere \"v\" is the velocity of propagation of the cable in question.\nSince formula_14 then\nformula_15.\nCampbell arrived at this expression by analogy with a mechanical line periodically loaded with weights described by Charles Godfrey in 1898 who obtained a similar result. Mechanical loaded lines of this sort were first studied by Joseph-Louis Lagrange (1736\u20131813).\nThe phenomenon of cutoff whereby frequencies above the cutoff frequency are not transmitted is an undesirable side effect of loading coils (although it proved highly useful in the development of filters). Cutoff is avoided by the use of continuous loading since it arises from the lumped nature of the loading coils.\nHistory.\nOliver Heaviside.\nThe origin of the loading coil can be found in the work of Oliver Heaviside on the theory of transmission lines. Heaviside (1881) represented the line as a network of infinitesimally small circuit elements. By applying his operational calculus to the analysis of this network he discovered (1887) what has become known as the Heaviside condition. This is the condition that must be fulfilled in order for a transmission line to be free from distortion. The Heaviside condition is that the series impedance, Z, must be proportional to the shunt admittance, Y, at all frequencies. In terms of the primary line coefficients the condition is:\nformula_16\nwhere:\nformula_17 is the series resistance of the line per unit length\nformula_18 is the series self-inductance of the line per unit length\nformula_19 is the shunt leakage conductance of the line insulator per unit length\nformula_20 is the shunt capacitance between the line conductors per unit length\nHeaviside was aware that this condition was not met in the practical telegraph cables in use in his day. In general, a real cable would have,\nformula_21\nThis is mainly due to the low value of leakage through the cable insulator, which is even more pronounced in modern cables which have better insulators than in Heaviside's day. In order to meet the condition, the choices are therefore to try to increase G or L or to decrease R or C. Decreasing R requires larger conductors. Copper was already in use in telegraph cables and this is the very best conductor available short of using silver. Decreasing R means using more copper and a more expensive cable. Decreasing C would also mean a larger cable (although not necessarily more copper). Increasing G is highly undesirable; while it would reduce distortion, it would at the same time increase the signal loss. Heaviside considered, but rejected, this possibility which left him with the strategy of increasing L as the way to reduce distortion.\nHeaviside immediately (1887) proposed several methods of increasing the inductance, including spacing the conductors further apart and loading the insulator with iron dust. Finally, Heaviside made the proposal (1893) to use discrete inductors at intervals along the line. However, he never succeeded in persuading the British GPO to take up the idea. Brittain attributes this to Heaviside's failure to provide engineering details on the size and spacing of the coils for particular cable parameters. Heaviside's eccentric character and setting himself apart from the establishment may also have played a part in their ignoring of him.\nJohn Stone.\nJohn S. Stone worked for the American Telephone &amp; Telegraph Company (AT&amp;T) and was the first to attempt to apply Heaviside's ideas to real telecommunications. Stone's idea (1896) was to use a bimetallic iron-copper cable which he had patented. This cable of Stone's would increase the line inductance due to the iron content and had the potential to meet the Heaviside condition. However, Stone left the company in 1899 and the idea was never implemented. Stone's cable was an example of continuous loading, a principle that was eventually put into practice in other forms, see for instance Krarup cable later in this article.\nGeorge Campbell.\nGeorge Campbell was another AT&amp;T engineer working in their Boston facility. Campbell was tasked with continuing the investigation into Stone's bimetallic cable, but soon abandoned it in favour of the loading coil. His was an independent discovery: Campbell was aware of Heaviside's work in discovering the Heaviside condition, but unaware of Heaviside's suggestion of using loading coils to enable a line to meet it. The motivation for the change of direction was Campbell's limited budget.\nCampbell was struggling to set up a practical demonstration over a real telephone route with the budget he had been allocated. After considering that his artificial line simulators used lumped components rather than the distributed quantities found in a real line, he wondered if he could not insert the inductance with lumped components instead of using Stone's distributed line. When his calculations showed that the manholes on telephone routes were sufficiently close together to be able to insert the loading coils without the expense of either having to dig up the route or lay in new cables he changed to this new plan. The very first demonstration of loading coils on a telephone cable was on a 46-mile length of the so-called Pittsburgh cable (the test was actually in Boston, the cable had previously been used for testing in Pittsburgh) on 6 September 1899 carried out by Campbell himself and his assistant. The first telephone cable using loaded lines put into public service was between Jamaica Plain and West Newton in Boston on 18 May 1900.\nCampbell's work on loading coils provided the theoretical basis for his subsequent work on filters which proved to be so important for frequency-division multiplexing. The cut-off phenomena of loading coils, an undesirable side-effect, can be exploited to produce a desirable filter frequency response.\nMichael Pupin.\nMichael Pupin, inventor and Serbian immigrant to the US, also played a part in the story of loading coils. Pupin filed a rival patent to the one of Campbell's. This patent of Pupin's dates from 1899. There is an earlier patent (1894, filed December 1893) which is sometimes cited as Pupin's loading coil patent but is, in fact, something different. The confusion is easy to understand, Pupin himself claims that he first thought of the idea of loading coils while climbing a mountain in 1894, although there is nothing from him published at that time.\nPupin's 1894 patent \"loads\" the line with capacitors rather than inductors, a scheme that has been criticised as being theoretically flawed and never put into practice. To add to the confusion, one variant of the capacitor scheme proposed by Pupin does indeed have coils. However, these are not intended to compensate the line in any way. They are there merely to restore DC continuity to the line so that it may be tested with standard equipment. Pupin states that the inductance is to be so large that it blocks all AC signals above 50\u00a0Hz. Consequently, only the capacitor is adding any significant impedance to the line and \"the coils will not exercise any material influence on the results before noted\".\nLegal battle.\nHeaviside never patented his idea; indeed, he took no commercial advantage of any of his work. Despite the legal disputes surrounding this invention, it is unquestionable that Campbell was the first to actually construct a telephone circuit using loading coils. There also can be little doubt that Heaviside was the first to publish and many would dispute Pupin's priority.\nAT&amp;T fought a legal battle with Pupin over his claim. Pupin was first to patent but Campbell had already conducted practical demonstrations before Pupin had even filed his patent (December 1899). Campbell's delay in filing was due to the slow internal machinations of AT&amp;T.\nHowever, AT&amp;T foolishly deleted from Campbell's proposed patent application all the tables and graphs detailing the exact value of inductance that would be required before the patent was submitted. Since Pupin's patent contained a (less accurate) formula, AT&amp;T was open to claims of incomplete disclosure. Fearing that there was a risk that the battle would end with the invention being declared unpatentable due to Heaviside's prior publication, they decided to desist from the challenge and buy an option on Pupin's patent for a yearly fee so that AT&amp;T would control both patents. By January 1901 Pupin had been paid $200,000 () and by 1917, when the AT&amp;T monopoly ended and payments ceased, he had received a total of $455,000 ().\nBenefit to AT&amp;T.\nThe invention was of enormous value to AT&amp;T. Telephone cables could now be used to twice the distance previously possible, or alternatively, a cable of half the previous quality (and cost) could be used over the same distance. When considering whether to allow Campbell to go ahead with the demonstration, their engineers had estimated that they stood to save $700,000 in new installation costs in New York and New Jersey alone. It has been estimated that AT&amp;T saved $100 million in the first quarter of the 20th century. Heaviside, who began it all, came away with nothing. He was offered a token payment but would not accept, wanting the credit for his work. He remarked ironically that if his prior publication had been admitted it would \"interfere ... with the flow of dollars in the proper direction ...\".\nSubmarine cables.\nSignal distortion is a particular problem for submarine communication cables, partly because their great length allows more distortion to build up, but also because they are more susceptible to distortion than open wires on poles due to the characteristics of the insulating material. Different wavelengths of the signal travel at different velocities in the material causing dispersion. It was this problem on the first transatlantic telegraph cable that motivated Heaviside to study the problem and find the solution. Loading coils solve the dispersion problem, and the first use of them on a submarine cable was in 1906 by Siemens and Halske in a cable across Lake Constance.\nThere are a number of difficulties using loading coils with heavy submarine cables. The bulge of the loading coils could not easily pass through the cable laying apparatus of cable ships and the ship had to slow down during the laying of a loading coil. Discontinuities where the coils were installed caused stresses in the cable during laying. Without great care, the cable might part and would be difficult to repair. A further problem was that the material science of the time had difficulties sealing the joint between coil and cable against ingress of seawater. When this occurred the cable was ruined. Continuous loading was developed to overcome these problems, which also has the benefit of not having a cutoff frequency.\nKrarup cable.\nA Danish engineer, Carl Emil Krarup, invented a form of continuously loaded cable which solved the problems of discrete loading coils. Krarup cable has iron wires continuously wound around the central copper conductor with adjacent turns in contact with each other. This cable was the first use of continuous loading on any telecommunication cable. In 1902, Krarup both wrote his paper on this subject and saw the installation of the first cable between Helsing\u00f8r (Denmark) and Helsingborg (Sweden).\nPermalloy cable.\nEven though the Krarup cable added inductance to the line, this was insufficient to meet the Heaviside condition. AT&amp;T searched for a better material with higher magnetic permeability. In 1914, Gustav Elmen discovered permalloy, a magnetic nickel-iron annealed alloy. In c. 1915, Oliver E. Buckley, H. D. Arnold, and Elmen, all at Bell Labs, greatly improved transmission speeds by suggesting a method of constructing submarine communications cable using permalloy tape wrapped around the copper conductors.\nThe cable was tested in a trial in Bermuda in 1923. The first permalloy cable placed in service connected New York City and Horta (Azores) in September 1924. Permalloy cable enabled signalling speed on submarine telegraph cables to be increased to 400 words/min at a time when 40 words/min was considered good. The first transatlantic cable achieved only two words/min.\nMu-metal cable.\nMu-metal has magnetic properties similar to those of permalloy, but the addition of copper to the alloy increases the ductility and allows the metal to be drawn into wire. Mu-metal cable is easier to construct than permalloy cable, the mu-metal being wound around the core copper conductor in much the same way as the iron wire in Krarup cable. A further advantage with mu-metal cable is that the construction lends itself to a variable loading profile whereby the loading is tapered towards the ends.\nMu-metal was invented in 1923 by the Telegraph Construction and Maintenance Company, London, who made the cable, initially, for the Western Union Telegraph Co. Western Union were in competition with AT&amp;T and the Western Electric Company who were using permalloy. The patent for permalloy was held by Western Electric which prevented Western Union from using it.\nPatch loading.\nContinuous loading of cables is expensive and hence is only done when absolutely necessary. Lumped loading with coils is cheaper but has the disadvantages of difficult seals and a definite cutoff frequency. A compromise scheme is patch loading whereby the cable is continuously loaded in repeated sections. The intervening sections are left unloaded.\nCurrent practice.\nLoaded cable is no longer a useful technology for submarine communication cables, having first been superseded by co-axial cable using electrically powered in-line repeaters and then by fibre-optic cable. Manufacture of loaded cable declined in the 1930s and was then superseded by other technologies post-World War II. Loading coils can still be found in some telephone landlines today but new installations use more modern technology.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41327", "revid": "1286970", "url": "https://en.wikipedia.org/wiki?curid=41327", "title": "Lobe", "text": "Lobe may refer to:\n&lt;templatestyles src=\"Template:TOC_right/styles.css\" /&gt;\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "41328", "revid": "36729223", "url": "https://en.wikipedia.org/wiki?curid=41328", "title": "Local Access and Transport Area", "text": "Geographical area of the United States, used in telephone service\nLocal Access And Transport Area (LATA) is a term defined in United States telecommunications regulation. A LATA is a geographical area of the United States under the terms of the Modification of Final Judgment (MFJ) entered by the United States District Court for the District of Columbia in Civil Action number 82-0192 or any other geographic area designated as a LATA in the National Exchange Carrier Association, Inc., Tariff FCC No. 4, that precipitated the breakup of the original AT&amp;T into the \"Baby Bells\" or created since that time for wireline regulation.\nGenerally, a LATA represents an area within which a divested Regional Bell Operating Company (RBOC) is permitted to offer exchange telecommunications and exchange access services. Under the terms of the MFJ, the RBOCs are generally prohibited from providing services that originate in one LATA and terminate in another.\nLATA boundaries tend to be drawn around markets, and not necessarily along existing state or area code borders. Some LATAs cross over state boundaries, such as those for the New York metropolitan area and Greenwich, Connecticut; Chicago, Illinois; Portland, Oregon; and areas between Maryland, Virginia, and West Virginia. Area codes and LATAs do not necessarily share boundaries; many LATAs exist in multiple area codes, and many area codes exist in multiple LATAs.\nOriginally, the LATAs were grouped into regions within which one particular RBOC was allowed to provide services. The LATAs in each of these regions are numbered beginning with the same digit. Generally, the LATAs were associated with carriers or other indications in the following manner:\nIn addition to this list, two local carriers were made independent: Cincinnati Bell in the Cincinnati area, and SNET (a former unit of AT&amp;T, sold to Frontier) in Connecticut. These were assigned LATAs in the 9xx range.\nSince the breakup of the Bell System in 1984, however, some amount of deregulation, as well as a number of phone company mergers, have blurred the significance of these regions. A number of new LATAs have been formed within these regions since their inception, most beginning with the digit 9.\nLATAs contribute to an often confusing aspect of long-distance telephone service. Due to the various and overlapping regulatory limitations and inter-business arrangements, phone companies typically provide differing types of \u201clong distance\u201d service, each with potentially different rates:\nGiven the complexity of the legal and financial issues involved in each distinction, many long-distance companies tend to not explain the details of these different rates, which can lead to billing questions from surprised customers.\nLocal carriers have various alternative terms for LATAs such as \u201cService Area\u201d by Pacific Bell in California, or \u201cRegional Calling Area\u201d by Verizon in Maryland.\nTo facilitate the sharing of Telcordia telephone routing databases between countries, LATAs were later defined for the provinces of Canada, the other countries and territories of the North American Numbering Plan, and Mexico. Aside from U.S. territories, LATAs have no regulatory purpose in these areas. In 2000, the Canadian Radio-television and Telecommunications Commission eliminated all Canadian provincial LATAs in favor of a single LATA for Canada (888).\nNo LATAs exist with a second digit of 0 or 1, which distinguished them from traditional area codes.\n&lt;templatestyles src=\"Template:TOC_right/styles.css\" /&gt;\nList of LATAs.\nUS state LATAs.\nThe city or place name given with some LATAs is the name given to identify the LATA, not the limit of its boundary. Generally this is the most significant metropolitan area in the LATA. In some cases, a LATA is named after the largest phone exchange in the LATA that was historically served by an RBOC. For example, the largest city in the Pahrump LATA in Nevada is Las Vegas. Since Las Vegas was not historically served by an RBOC, the LATA is named after the smaller town of Pahrump, which was historically served by Nevada Bell (now AT&amp;T Inc.). Also, listing under a state does not necessarily limit the LATA's territory to that state; there may be overlaps as well as enclaves. Areas that include notable portions of other states are explained, but not all LATA state overlaps may be detailed.\nLATA boundaries are not always solidly defined. Inter-carrier agreements, change proposals to the Federal Communications Commission (FCC), and new wiring developments into rural areas can and do often alter the effective borders between LATAs. Many sources on LATA boundary information conflict with each other at detailed levels. Telcordia data may provide the most up-to-date details of LATA inclusions.\nCanada.\nAs LATAs exist for US regulatory purposes, where they serve as a demarcation between intra-LATA calls (handled by regional Bell operating companies) and inter-LATA calls (handled by interstate long-distance carriers such as AT&amp;T), they have no legal significance in Canada.\nAs of 2000, all of Canada (except for non-geographic numbers) is identified as LATA 888.\nThe use of this LATA set to identify individual provinces is therefore deprecated:\nLocal interconnection region.\nCanada does define local interconnection regions (LIR's), which determine where points of interconnection (POI) must be provided by competing local exchange and mobile carriers to provide local number portability. A Canadian LIR is geographically smaller than a US LATA, typically comparable in size to a small city's flat-rate local calling area or to an entire large regional municipality. In areas where a small-city Digital Multiplex System controls a group of remote switching centres, one for each surrounding village, the local interconnect region normally includes each exchange in the city plus all downstream remotes of those exchanges. In a Toronto-sized city, the LIR will include only the city itself.\nWhile the LIRs resemble local calling areas in geographic size, there are some key differences:\nExample: The tiny unincorporated village of Beebe Plain, divided by the Quebec-Vermont border, is served by +1-819-876 Rock Island, Quebec, Canada (a remote station controlled from Magog) and +1-802-873 Derby Line, Vermont, USA (a remote station controlled from St. Johnsbury). Magog and St. Johnsbury are both a long-distance call from anywhere in Beebe Plain, even though Canadian subscribers can place local calls to Sherbrooke, US subscribers can locally call Newport and an international call within the village is local. An LIR assignment which follows network topology places the Canadian remote station in Magog's LIR, not Sherbrooke's LIR.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41329", "revid": "4052843", "url": "https://en.wikipedia.org/wiki?curid=41329", "title": "Local battery", "text": ""}
{"id": "41330", "revid": "2325890", "url": "https://en.wikipedia.org/wiki?curid=41330", "title": "Local call", "text": "Term used to describe telephone calls\nIn telephony, the term local call has the following meanings:\nTypically, local calls have shorter numbers than long-distance calls, as the area code may not be required. However, this is not true in parts of the United States and Canada that are subject to overlay plans or many countries in Europe that require closed dialing plans.\nToll free (e.g. \"800\" numbers in the United States) are not necessarily local calls; despite being free to the caller, any charge due for the distance of the connection is charged to the \"called\" party.\nCommercial users who make or accept many long-distance calls to or from a particular distant place may make them as local calls by use of a foreign exchange service. Such an \"FX\" line also allows people in the distant place to call by using a telephone number local to them."}
{"id": "41332", "revid": "10202399", "url": "https://en.wikipedia.org/wiki?curid=41332", "title": "Log-periodic antenna", "text": "Multi-element, directional antenna useable over a wide band of frequencies\nA log-periodic antenna (LP), also known as a log-periodic array or log-periodic aerial, is a multi-element, directional antenna designed to operate over a wide band of frequencies. It was invented by John Dunlavy in 1952.\nThe most common form of log-periodic antenna is the log-periodic dipole array or LPDA, The LPDA consists of a number of half-wave dipole driven elements of gradually increasing length, each consisting of a pair of metal rods. The dipoles are mounted close together in a line, connected in parallel to the feedline with alternating phase. Electrically, it simulates a series of two- or three-element Yagi\u2013Uda antennas connected together, each set tuned to a different frequency.\nLPDA antennas look somewhat similar to Yagi antennas, in that they both consist of dipole rod elements mounted in a line along a support boom, but they work in very different ways. Adding elements to a Yagi increases its directionality, or gain, while adding elements to an LPDA increases its frequency response, or bandwidth. \nOne large application for LPDAs is in rooftop terrestrial television antennas, since they may require large bandwidth to cover various frequencies in the VHF and/or UHF bands. One widely used design for television reception combined a Yagi for UHF reception in front of a larger LPDA for VHF.\nBasic concept.\nThe LPDA normally consists of a series of half wave dipole \"elements\" each consisting of a pair of metal rods, positioned along a support boom lying along the antenna axis. The elements are spaced at intervals following a logarithmic function of the frequency, known as \"d\" or \"sigma\". The length of the successive elements and the spacing between them gradually decrease along the boom. The relationship between the lengths is a function known as \"tau\". \"Sigma\" and \"tau\" are the key design elements of the LPDA design. The radiation pattern of the antenna is unidirectional, with the main lobe along the axis of the boom, off the end with the shortest elements. Each dipole element is resonant at a wavelength approximately equal to twice its length. The bandwidth of the antenna, the frequency range over which it has near-maximum gain, is approximately between the resonant frequencies of the longest and shortest elements.\nEvery element in the LPDA antenna is a driven element, that is, connected electrically to the feedline. A parallel wire transmission line usually runs along the central boom, and each successive element is connected in \"opposite\" phase to it. The feedline can often be seen zig-zagging across the support boom holding the elements. Another common construction method is to use two parallel central support booms that also acts as the transmission line, mounting the dipoles on the alternate booms. Other forms of the log-periodic design replace the dipoles with the transmission line itself, forming the log-periodic zig-zag antenna. Many other forms using the transmission wire as the active element also exist.\nThe Yagi and the LPDA designs look very similar at first glance, as they both consist of a number of dipole elements mounted along a support boom. The Yagi, however, has only a single driven element connected to the transmission line, usually the second one from the back of the array, the remaining elements are parasitic. The Yagi antenna differs from the LPDA in having a very narrow bandwidth.\nIn general terms, at any given frequency the log-periodic design operates somewhat similar to a three-element Yagi antenna; the dipole element closest to resonant at the operating frequency acts as a driven element, with the two adjacent elements on either side as director and reflector to increase the gain, the shorter element in front acting as a director and the longer element behind as a reflector. However, the system is somewhat more complex than that, and all the elements contribute to some degree, so the gain for any given frequency is higher than a Yagi of the same dimensions as any one section of the log-periodic. However, a Yagi with the same number of elements as a log-periodic would have \"far\" higher gain, as all of those elements are improving the gain of a single driven element. In its use as a television antenna, it was common to combine a log-periodic design for VHF with a Yagi for UHF, with both halves being roughly equal in size. This resulted in much higher gain for UHF, typically on the order of 10 to 14\u00a0dB on the Yagi side and 6.5\u00a0dB for the log-periodic. But this extra gain was needed anyway in order to make up for a number of problems with UHF signals.\nThe log-periodic shape, according to the IEEE definition, does not align with broadband property for antennas. The broadband property of log-periodic antennas comes from its self-similarity. A planar log-periodic antenna can also be made self-complementary, such as logarithmic spiral antennas (which are not classified as log-periodic \"per se\" but among the frequency independent antennas that are also self-similar) or the log-periodic toothed design. Y. Mushiake found, for what he termed \"the simplest self-complementary planar antenna,\" a driving point impedance of \u03b70/2=188.4\u00a0\u03a9 at frequencies well within its bandwidth limits.\nHistory.\nJohn Dunlavy invented the log-periodic antenna in 1952 while working for the United States Air Force but was not credited with it due to its \"Secret\" classification. The University of Illinois at Urbana\u2013Champaign had patented the Isbell and Mayes\u2013Carrel antennas and licensed the design as a package exclusively to JFD Electronics in New York. Channel Master and Blonder Tongue Labs ignored the patents and produced a wide range of antennas based on that design. Lawsuits regarding the antenna patent, which the U.I. Foundation lost, evolved into the 1971 Blonder-Tongue Doctrine. This precedent governs patent litigation.\nShort wave broadcast antennas.\nThe log-periodic is commonly used as a transmitting antenna in high power shortwave broadcasting stations because its broad bandwidth allows a single antenna to transmit on frequencies in multiple bands. The log-periodic zig-zag design with up to 16 sections has been used. These large antennas are typically designed to cover 6 to 26\u00a0MHz but even larger ones have been built which operate as low as 2\u00a0MHz. Power ratings are available up to 500\u00a0kW. Instead of the elements being driven in parallel, attached to a central transmission line, the elements are driven in series, adjacent elements connected at the outer edges. The antenna shown here would have about 14\u00a0dBi gain. An antenna array consisting of two such antennas, one above the other and driven in phase has a gain of up to 17\u00a0dBi. Being log-periodic, the antenna's main characteristics (radiation pattern, gain, driving point impedance) are almost constant over its entire frequency range, with the match to a 300\u00a0\u03a9 feed line achieving a standing wave ratio of better than 2:1 over that range.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nNotes.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41333", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41333", "title": "Long-haul communications", "text": "In telecommunications, the term long-haul communications has the following meanings:\n1. In public switched networks, pertaining to circuits that span large distances, such as the circuits in inter-LATA, interstate, and international communications. See also Long line (telecommunications)\n2. In the military community, communications among users on a national or worldwide basis.\n\"Note 1:\" Compared to tactical communications, long-haul communications are characterized by (a) higher levels of users, such as the US National Command Authority, (b) more stringent performance requirements, such as higher quality circuits, (c) longer distances between users, including worldwide distances, (d) higher traffic volumes and densities, (e) larger switches and trunk cross sections, and (f) fixed and recoverable assets.\n\"Note 2:\" \"Long-haul communications\" usually pertains to the U.S. Defense Communications System.\n\" Note 3:\" \"Long-haul telecommunications technicians\" can be translated into many fields of IT work within the corporate industry (Information Technology, Network Technician, Telecommunication Specialist, It Support, and so on). While the term is used in military most career fields that are in communications such as 3D1X2 - Cyber Transport Systems (the career field has been renamed so many times over the course of many years but essentially it is the same job (Network Infrastructure Tech., Systems Control Technician, and Cyber Transport Systems)) or may work in areas that require the \"in between\" (cloud networking) for networks (MSPP, ATM, Routers, Switches), phones (VOIP, DS0 - DS4 or higher, and so on), encryption (configuring encryption devices or monitoring), and video support data transfers. The \"bulk data transfer\" or aggregation networking.\nThe Long-haul telecommunication technicians is considered a \"jack of all\" but it is much in the technician's interest to gather greater education with certifications to qualify for certain jobs outside the military. The Military provides an avenue but does not make the individual a master of the career field. The technician will find that the job out look outside of military requires many things that aren't required of them within the career field while in the military. So it is best to find the job that is similar to the AFSC and also view the companies description of the qualification to fit that job. Also at least get an associate degree, over 5 years experience, and all of the required \"certs\" (Network +, Security +, CCNA, CCNP and so on) to acquire the job or at least an interview. The best time to apply or get a guaranteed job is the last three months before you leave the military. Military personnel that are within the career field 3D1X2 require a Secret, TS, or TS with SCI clearance in order to do the job.\nSee also.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41334", "revid": "27823944", "url": "https://en.wikipedia.org/wiki?curid=41334", "title": "Longitudinal redundancy check", "text": "Error detection number calculated over a serial data stream\nIn telecommunication, a longitudinal redundancy check (LRC), or horizontal redundancy check, is a form of redundancy check that is applied independently to each of a parallel group of bit streams. The data must be divided into transmission blocks, to which the additional check data is added.\nThe term usually applies to a single parity bit per bit stream, calculated independently of all the other bit streams (BIP-8).\nThis \"extra\" LRC word at the end of a block of data is very similar to checksum and cyclic redundancy check (CRC).\nOptimal rectangular code.\nWhile simple longitudinal parity can only detect errors, it can be combined with additional error-control coding, such as a transverse redundancy check (TRC), to correct errors. The transverse redundancy check is stored on a dedicated \"parity track\".\nWhenever any single-bit error occurs in a transmission block of data, such two-dimensional parity checking, or \"two-coordinate parity checking\",\nenables the receiver to use the TRC to detect which byte the error occurred in, and the LRC to detect exactly which track the error occurred in, to discover exactly which bit is in error, and then correct that bit by flipping it.\nPseudocode.\nInternational standard ISO 1155 states that a longitudinal redundancy check for a sequence of bytes may be computed in software by the following algorithm:\n \"lrc\" := 0\n for each byte \"b\" in the buffer do\n \"lrc\" := (\"lrc\" + \"b\") and 0xFF\n \"lrc\" := (((\"lrc\" XOR 0xFF) + 1) and 0xFF)\nwhich can be expressed as \"the 8-bit two's-complement value of the sum of all bytes modulo 28\" (codice_1 is equivalent to codice_2).\nOther forms.\nMany protocols use an XOR-based longitudinal redundancy check byte (often called block check character or BCC), including the serial line interface protocol (SLIP, not to be confused with the later and well-known Serial Line Internet Protocol),\nthe IEC 62056-21 standard for electrical-meter reading, smart cards as defined in ISO/IEC 7816, and the ACCESS.bus protocol.\nAn 8-bit LRC such as this is equivalent to a cyclic redundancy check using the polynomial \"x\"8\u2009+\u20091, but the independence of the bit streams is less clear when looked at in that way.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41335", "revid": "252195", "url": "https://en.wikipedia.org/wiki?curid=41335", "title": "Longitudinal voltage", "text": ""}
{"id": "41336", "revid": "461895", "url": "https://en.wikipedia.org/wiki?curid=41336", "title": "Long-term stability", "text": "In electronics, the long-term stability of an oscillator is the degree of uniformity of frequency over time, when the frequency is measured under identical environmental conditions, such as supply voltage, load, and temperature. Long-term frequency changes are caused by changes in the oscillator elements that determine frequency, such as crystal drift, inductance changes, and capacitance changes. \n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41337", "revid": "375502", "url": "https://en.wikipedia.org/wiki?curid=41337", "title": "Loop-back", "text": ""}
{"id": "41338", "revid": "11677590", "url": "https://en.wikipedia.org/wiki?curid=41338", "title": "Loop gain", "text": ""}
{"id": "41340", "revid": "12396222", "url": "https://en.wikipedia.org/wiki?curid=41340", "title": "LPD433", "text": "European radio band for license-free devices\nLPD433 (low power device 433\u00a0MHz) is a UHF band in which license free communication devices are allowed to operate in some regions. The frequencies correspond with the ITU region 1 ISM band of 433.050\u00a0MHz to 434.790\u00a0MHz. The frequencies used are within the 70-centimeter band, which is currently otherwise reserved for government and amateur radio operations in the United States and most nations worldwide.\nLPD hand-held radios are authorized for licence-free voice communications use in most of Europe using analog frequency modulation (FM) as part of short range device regulations, with 25 kHz channel spacing, for a total of 69 channels. In some countries, LPD devices may only be used with an integral and non-removable antenna with a maximum legal power output of 10 mW.\nVoice communication in the LPD band was introduced to reduce the burden on the eight (now sixteen) PMR446 channels over shorter ranges (less than 1\u00a0km).\nLPD is also used in vehicle key-less entry device, garage or gate openers and some outdoor home weather station products.\nUsage by country.\nITU Region 1 (Europe).\nUnited Kingdom.\nIn the UK, LPD433 equipment that meets the respective Ofcom Interface Requirement can be used for model control, analogue/digitised voice and remote keyless entry systems. There is significant scope for interference however, both on frequency and on adjacent frequencies, as the band is far from free. The frequencies from 430 to 440\u00a0MHz are allocated on a secondary basis to licensed radio amateurs who are allowed to use up to 40\u00a0W (16\u00a0dBW) between 430 and 432\u00a0MHz and 400\u00a0W (26\u00a0dBW) between 432 and 440\u00a0MHz. Channels 1 to 14 are UK amateur repeater outputs and channels 62 to 69 are UK amateur repeater inputs. This band is shared on a secondary basis for both licensed and licence exempt users, with the primary user being the Ministry of Defence.\nOfcom, together with the RSGB Emerging Technology Co-ordination Committee have produced guidelines to help mitigate the side effects of interference to an extent.\nSwitzerland.\nSwitzerland permits the use of all 69 LPD433 channels with a maximum power output of 10\u00a0mW.\nSpain.\nAccording to a recently published (June 2021) resolution of the Spanish government, where it defines 'interface IR-266', non-specific mobile short-range devices may be used without authorization for voice applications with 'advanced mitigation techniques' (such as listening before talking) from 434.040 to 434.790 MHz, with channels narrower than 25 kHz and with a maximum 'apparent radiated power' of 10 mW. This would make the use of LPD433 channels 40 to 69 possible in Spain.\nOther European countries.\nEuropean remote keyless entry systems often use the 433\u00a0MHz band, although, as in all of Europe, these frequencies are within the 70-centimeter band allocated to amateur radio, and interference results. In Germany, before the end of 2008, radio control enthusiasts were able to use frequencies from channel 03 through 67 for radio control of any form of model (air or ground-based), all with odd channel numbers (03, 05, etc. up to ch. 67), with each sanctioned frequency having 50\u00a0kHz of bandwidth separation between each adjacent channel.\nITU Region 2 (America).\nIn ITU region 2 (the Americas), the frequencies that LPD433 uses are also within the 70-centimeter band allocated to amateur radio. In the United States LPD433 radios can only be used under FCC amateur regulations by properly licensed amateur radio operators.\nITU Region 3.\nMalaysia.\nIn Malaysia, this band is also within the 70-centimeter band (430.000 \u2013 440.000 MHz) allocated to amateur radio. Class B amateur radio holders are permitted to transmit up to 50 watts PEP power level. There is no licence requirement for LPD as long as it complies with requirement regulated by Malaysian Communications And Multimedia Commission (MCMC). As regulated by MCMC in Technical Code for Short Range Devices, remote control and security device are allowed up to 50 mW ERP and up to 100 mW ERP for Short Range Communication (SRC) devices. RFID are allowed up to 100 mW EIRP.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41341", "revid": "13503327", "url": "https://en.wikipedia.org/wiki?curid=41341", "title": "Machine-readable medium and data", "text": "Medium capable of storing data in a format readable by a machine\nIn communications and computing, a machine-readable medium (or computer-readable medium) is a medium capable of storing data in a format easily readable by a digital computer or a sensor. \nIt contrasts with \"human-readable\" medium and data.\nThe result is called machine-readable data or computer-readable data, and the data itself can be described as having machine-readability.\nData.\nMachine-readable data must be structured data.\nAttempts to create machine-readable data occurred as early as the 1960s. At the same time that seminal developments in machine-reading and natural-language processing were releasing (like Weizenbaum's ELIZA), people were anticipating the success of machine-readable functionality and attempting to create machine-readable documents. One such example was musicologist Nancy B. Reich's creation of a machine-readable catalog of composer William Jay Sydeman's works in 1966.\nIn the United States, the OPEN Government Data Act of 14 January 2019 defines machine-readable data as \"data in a format that can be easily processed by a computer without human intervention while ensuring no semantic meaning is lost.\" The law directs U.S. federal agencies to publish public data in such a manner, ensuring that \"any public data asset of the agency is machine-readable\".\nMachine-readable data may be classified into two groups: human-readable data that is marked up so that it can also be read by machines (e.g. microformats, RDFa, HTML), and data file formats intended principally for processing by machines (CSV, RDF, XML, JSON). These formats are only machine readable if the data contained within them is formally structured; exporting a CSV file from a badly structured spreadsheet does not meet the definition.\n\"Machine readable\" is not synonymous with \"digitally accessible\". A digitally accessible document may be online, making it easier for humans to access via computers, but its content is much harder to extract, transform, and process via computer programming logic if it is not machine-readable.\nExtensible Markup Language (XML) is designed to be both human- and machine-readable, and Extensible Stylesheet Language Transformations (XSLT) is used to improve the presentation of the data for human readability. For example, XSLT can be used to automatically render XML in Portable Document Format (PDF). Machine-readable data can be automatically transformed for human-readability but, generally speaking, the reverse is not true.\nFor purposes of implementation of the Government Performance and Results Act (GPRA) Modernization Act, the Office of Management and Budget (OMB) defines \"machine readable format\" as follows: \"Format in a standard computer language (not English text) that can be read automatically by a web browser or computer system. (e.g.; xml). Traditional word processing documents and portable document format (PDF) files are easily read by humans but typically are difficult for machines to interpret. Other formats such as extensible markup language (XML), (JSON), or spreadsheets with header columns that can be exported as comma separated values (CSV) are machine readable formats. As HTML is a structural markup language, discreetly labeling parts of the document, computers are able to gather document components to assemble tables of contents, outlines, literature search bibliographies, etc. It is possible to make traditional word processing documents and other formats machine readable but the documents must include enhanced structural elements.\"\nMedia.\nExamples of machine-readable media include magnetic media such as magnetic disks, cards, tapes, and drums, punched cards and paper tapes, optical discs, barcodes and magnetic ink characters.\nCommon machine-readable technologies include magnetic recording, processing waveforms, and barcodes. Optical character recognition (OCR) can be used to enable machines to read information available to humans. Any information retrievable by any form of energy can be machine-readable.\nExamples include:\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41342", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41342", "title": "Magneto-ionic double refraction", "text": "In telecommunications, magneto-ionic double refraction is the combined effect of the Earth's magnetic field and atmospheric ionization, whereby a linearly polarized wave entering the ionosphere is split into two components called the ordinary wave and extraordinary wave.\nThe component waves follow different paths, experience different attenuations, have different phase velocities, and, in general, are elliptically polarized in opposite senses. The critical frequency of the extraordinary wave is always greater than the critical frequency of the ordinary wave (i.e. the wave in absence of the magnetic field) by the amount approximately equal to .5 times of gyro frequency . The amplitude of extraordinary wave is dependent on the earth magnetic field at that particular point . Beside splitting, the polarization of the incident radio wave is also effected by this phenomenon because the electron that were earlier in simple harmonic motion only are now in spiral motion too due to the magnetic field.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41343", "revid": "27015025", "url": "https://en.wikipedia.org/wiki?curid=41343", "title": "Magneto-optic effect", "text": "Optical phenomenon\nA magneto-optic effect is any one of a number of phenomena in which an electromagnetic wave propagates through a medium that has been altered by the presence of a quasistatic magnetic field. In such a medium, which is also called gyrotropic or gyromagnetic, left- and right-rotating elliptical polarizations can propagate at different speeds, leading to a number of important phenomena. When light is transmitted through a layer of magneto-optic material, the result is called the Faraday effect: the plane of polarization can be rotated, forming a Faraday rotator. The results of reflection from a magneto-optic material are known as the magneto-optic Kerr effect (not to be confused with the nonlinear Kerr effect).\nIn general, magneto-optic effects break time reversal symmetry locally (i.e., when only the propagation of light, and not the source of the magnetic field, is considered) as well as Lorentz reciprocity, which is a necessary condition to construct devices such as optical isolators (through which light passes in one direction but not the other).\nTwo gyrotropic materials with reversed rotation directions of the two principal polarizations, corresponding to complex-conjugate \u03b5 tensors for lossless media, are called optical isomers.\nGyrotropic permittivity.\nIn particular, in a magneto-optic material the presence of a magnetic field (either externally applied or because the material itself is ferromagnetic) can cause a change in the permittivity tensor \u03b5 of the material. The \u03b5 becomes anisotropic, a 3\u00d73 matrix, with complex off-diagonal components, depending on the frequency \u03c9 of incident light. If the absorption losses can be neglected, \u03b5 is a Hermitian matrix. The resulting principal axes become complex as well, corresponding to elliptically-polarized light where left- and right-rotating polarizations can travel at different speeds (analogous to birefringence).\nMore specifically, for the case where absorption losses can be neglected, the most general form of Hermitian \u03b5 is:\nformula_1\nor equivalently the relationship between the displacement field D and the electric field E is:\nformula_2\nwhere formula_3 is a real symmetric matrix and formula_4 is a real pseudovector called the gyration vector, whose magnitude is generally small compared to the eigenvalues of formula_3. The direction of g is called the axis of gyration of the material. To first order, g is proportional to the applied magnetic field:\nformula_6\nwhere formula_7 is the magneto-optical susceptibility (a scalar in isotropic media, but more generally a tensor). If this susceptibility itself depends upon the electric field, one can obtain a nonlinear optical effect of magneto-optical parametric generation (somewhat analogous to a Pockels effect whose strength is controlled by the applied magnetic field).\nThe simplest case to analyze is the one in which g is a principal axis (eigenvector) of formula_3, and the other two eigenvalues of formula_3 are identical. Then, if we let g lie in the \"z\" direction for simplicity, the \u03b5 tensor simplifies to the form:\nformula_10\nMost commonly, one considers light propagating in the \"z\" direction (parallel to g). In this case the solutions are elliptically polarized electromagnetic waves with phase velocities formula_11 (where \u03bc is the magnetic permeability). This difference in phase velocities leads to the Faraday effect.\nFor light propagating purely perpendicular to the axis of gyration, the properties are known as the Cotton-Mouton effect and used for a Circulator.\nKerr rotation and Kerr ellipticity.\nKerr rotation and Kerr ellipticity are changes in the polarization of incident light which comes in contact with a gyromagnetic material. Kerr rotation is a rotation in the plane of polarization of transmitted light, and Kerr ellipticity is the ratio of the major to minor axis of the ellipse traced out by elliptically polarized light on the plane through which it propagates. Changes in the orientation of polarized incident light can be quantified using these two properties.\nAccording to classical physics, the speed of light varies with the permittivity of a material:\nformula_12\nwhere formula_13 is the velocity of light through the material, formula_14 is the material permittivity, and formula_15 is the material permeability. Because the permittivity is anisotropic, polarized light of different orientations will travel at different speeds.\nThis can be better understood if we consider a wave of light that is circularly polarized (seen to the right). If this wave interacts with a material at which the horizontal component (green sinusoid) travels at a different speed than the vertical component (blue sinusoid), the two components will fall out of the 90 degree phase difference (required for circular polarization) changing the Kerr ellipticity.\nA change in Kerr rotation is most easily recognized in linearly polarized light, which can be separated into two circularly polarized components: Left-handed circular polarized (LHCP) light and right-handed circular polarized (RHCP) light. The anisotropy of the magneto-optic material permittivity causes a difference in the speed of LHCP and RHCP light, which will cause a change in the angle of polarized light. Materials that exhibit this property are known as birefringent.\nFrom this rotation, we can calculate the difference in orthogonal velocity components, find the anisotropic permittivity, find the gyration vector, and calculate the applied magnetic field formula_16.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41344", "revid": "2255048", "url": "https://en.wikipedia.org/wiki?curid=41344", "title": "Main distribution frame", "text": "Distribution frame where cables are cross-connected in telephony\nIn telephony, a main distribution frame (MDF or main frame) is a signal distribution frame for connecting equipment (inside plant) to cables and subscriber carrier equipment (outside plant).\nThe MDF is a termination point within the local telephone exchange where exchange equipment and terminations of local loops are connected by jumper wires at the MDF. All cable copper pairs supplying services through user telephone lines are terminated at the MDF and distributed through the MDF to equipment within the local exchange e.g. repeaters and DSLAM. Cables to intermediate distribution frames (IDF) terminate at the MDF. Trunk cables may terminate on the same MDF or on a separate trunk main distribution frame (TMDF).\nLike other distribution frames the MDF provides flexibility in assigning facilities, at lower cost and higher capacity than a patch panel.\nOperation.\nThe most common kind of large MDF is a long steel rack accessible from both sides. On one side, termination blocks are arranged horizontally at the front of rack shelves. Jumpers lie on the shelves and go through an insulated steel hoop to run vertically to other termination blocks that are arranged vertically. There is a hoop or ring at the intersection of each level and each vertical. Installing a jumper historically required two workers, one on either side of the MDF. The shelves are shallow enough to allow the rings to be within arm's reach, but the workers prefer to hang the jumper on a hook on a pole so their partner can pull it through the ring. A fanning strip at the back of each termination block prevents the wires from covering each other's terminals. With disciplined administration, the MDF can hold over a hundred thousand jumpers, with dozens changed every day, for decades without tangling.\nThe MDF usually holds telephone exchange protective devices including heat coils, and functions as a test point between a line and the exchange equipment.\nHistory.\nBefore 1960, MDF jumpers were generally soldered. This was reliable but slow and expensive. Wire wrap was introduced in the 1960s, and punch blocks in the 1970s. In the early 21st century most exchanges in the UK, still used soldered blocks but were being slowly phased out.\nEach jumper is a twisted pair. Middle 20th century jumper wires in the USA were 24 AWG single strand copper, with a soft polyethylene inner jacket and a cotton wrapper, impregnated to make it slightly brittle and easy to remove neatly. Late 20th century ones had a single, thicker coating of polyethylene cross-linked to provide a suitable degree of brittleness.\nSome urban telephone exchange MDFs are two stories high so they do not have to be more than a city block long. A few are three stories. Access to the upper levels can be either by a traveling ladder attached to the MDF, or by mezzanine walkways at a suitable height. By British custom the cables to the outside world are terminated on the horizontal side, and the indoors equipment on the vertical side. American usage is the opposite.\nSmaller MDFs, and some modern large ones, are single sided so one worker can install, remove or change a jumper. COSMOS and other computerized Operations Support Systems help by assigning terminals close to one another, so most jumpers need not be long and shelves on either type of MDF do not become congested. This database keeps track of all terminals and jumpers. In the early and middle 20th century these records were kept as pencil entries in ledger books. The later database method saves much labor by permitting old jumpers to be reused for new lines.\nThe adoption of distributed switching in the late 20th century diminished the need for large, active, central MDFs.\nSometimes the MDF is combined with other kinds of distribution frame in a CDF.\nThe MDF in a private branch exchange performs functions similar to those performed by the MDF in a central office.\nAutomated Main Distribution Frame (AMDF) has been a subject of experiments.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41345", "revid": "12193631", "url": "https://en.wikipedia.org/wiki?curid=41345", "title": "Main lobe", "text": "Region of the radiation pattern of an antenna\nIn a radio antenna, the main lobe or main beam is the region of the radiation pattern containing the highest power or exhibiting the greatest field strength.\nThe radiation pattern of most antennas shows a pattern of \"\"lobes\" at various directions, where the radiated signal strength reaches a local maximum, separated by \"nulls\", at which the radiation falls to zero. In a directional antenna in which the objective is to emit the radio waves in one direction, the lobe in that direction is designed to have higher field strength than the others, so on a graph of the radiation pattern it appears biggest; this is the main lobe. The other lobes are called \"sidelobes\", and usually represent unwanted radiation in undesired directions. The sidelobe in the opposite direction from the main lobe is called the \"backlobe\"\". \nThe radiation pattern referred to above is usually the horizontal radiation pattern, which is plotted as a function of azimuth about the antenna, although the vertical radiation pattern may also have a main lobe. The beamwidth of the antenna is the width of the main lobe, usually specified by the \"half power beam width\" (HPBW), the angle encompassed between the points on the side of the lobe where the power has fallen to half (-3 dB) of its maximum value.\nThe concepts of main lobe and sidelobes also apply to acoustics and optics, and are used to describe the radiation pattern of optical systems like telescopes, and acoustic transducers like microphones and loudspeakers.\nSee also.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41346", "revid": "16752040", "url": "https://en.wikipedia.org/wiki?curid=41346", "title": "Main storage", "text": ""}
{"id": "41347", "revid": "1095934", "url": "https://en.wikipedia.org/wiki?curid=41347", "title": "Maintainability", "text": "Ease of maintaining a functioning product or service\nMaintainability is the ease of maintaining or providing maintenance for a functioning product or service. Depending on the field, it can have slightly different meanings.\nUsage in different fields.\nEngineering.\nIn engineering, maintainability is the ease with which a product can be maintained to:\nIn some cases, maintainability involves a system of continuous improvement - learning from the past to improve the ability to maintain systems, or improve the reliability of systems based on maintenance experience.\nTelecommunication.\nIn telecommunications and several other engineering fields, the term maintainability has the following meanings:\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;\nSoftware.\nIn software engineering, these activities are known as software maintenance (cf. ISO/IEC 9126). Closely related concepts in the software engineering domain are evolvability, modifiability, technical debt, and code smells.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41348", "revid": "4842600", "url": "https://en.wikipedia.org/wiki?curid=41348", "title": "Maintenance (disambiguation)", "text": "Maintenance may refer to:\n&lt;templatestyles src=\"Template:TOC_right/styles.css\" /&gt;\nTechnical maintenance.\nSome kinds of technical maintenance.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "41349", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41349", "title": "Managed object", "text": "Telecommunications term\nIn network, a managed object is an abstract representation of network resources that are managed. With \"representation\", we mean not only the actual device that is managed but also the device driver, that communicates with the device. An example of a printer as a managed object is the window that shows information about the printer, such as the location, printer status, printing progress, paper choice, and printing margins.\nThe database, where all managed objects are stored, is called Management Information Base. In contrast with a CI, a managed object is \"dynamic\" and communicates with other network resources that are managed. \nA managed object may represent a physical entity, a network service, or an abstraction of a resource that exists independently of its use in management.\nIn telecommunications management, managed object can refer to a resource within the telecommunications environment that may be managed through the use of operation, administration, maintenance, and provisioning (OAMP) application protocols.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41350", "revid": "1304329861", "url": "https://en.wikipedia.org/wiki?curid=41350", "title": "Manchester code", "text": "Line code used in early magnetic data storage and Ethernet\nIn telecommunications and data storage, Manchester code (also known as phase encoding, or PE) is a line code in which the encoding of each data bit is either low then high, or high then low, for equal time. It is a self-clocking signal with no DC component. Consequently, electrical connections using a Manchester code are easily galvanically isolated.\nManchester code derives its name from its development at the University of Manchester, where the coding was used for storing data on the magnetic drums of the Manchester Mark 1 computer.\nManchester code was widely used for magnetic recording on 1600\u00a0bpi computer tapes before the introduction of 6250\u00a0bpi tapes which used the more efficient group-coded recording. Manchester code was used in early Ethernet physical layer standards and is still used in consumer IR protocols, RFID and near-field communication. It was and still is used for uploading commands to the Voyager spacecraft.\nFeatures.\nManchester coding is a special case of binary phase-shift keying (BPSK), where the data controls the phase of a square wave carrier whose frequency is the data rate. Manchester code ensures frequent line voltage transitions, directly proportional to the clock rate; this helps clock recovery.\nThe DC component of the encoded signal is not dependent on the data and therefore carries no information. Therefore connections may be inductively or capacitively coupled, allowing the signal to be conveyed conveniently by galvanically isolated media (e.g., Ethernet) using a network isolator\u2014a simple one-to-one pulse transformer which cannot convey a DC component.\nLimitations.\nManchester coding's data rate is only half that of a non-coded signal, which limits its usefulness to systems where bandwidth is not an issue, such as a local area network (LAN).\nManchester encoding introduces difficult frequency-related problems that make it unsuitable for use at higher data rates.\nThere are more complex codes, such as 8B/10B encoding, that use less bandwidth to achieve the same data rate but may be less tolerant of frequency errors and jitter in the transmitter and receiver reference clocks.\nEncoding and decoding.\nManchester code always has a transition at the middle of each bit period and may (depending on the information to be transmitted) have a transition at the start of the period also. The direction of the mid-bit transition indicates the data. Transitions at the period boundaries do not carry information. They exist only to place the signal in the correct state to allow the mid-bit transition.\nConventions for representation of data.\nThere are two opposing conventions for the representations of data.\nThe first of these was first published by G. E. Thomas in 1949 and is followed by numerous authors (e.g., Andy Tanenbaum). It specifies that for a 0 bit the signal levels will be low\u2013high (assuming an amplitude physical encoding of the data) \u2013 with a low level in the first half of the bit period, and a high level in the second half. For a 1 bit the signal levels will be high\u2013low. This is also known as Manchester II or Biphase-L code.\nThe second convention is also followed by numerous authors (e.g., William Stallings) as well as by IEEE 802.4 (token bus) and lower speed versions of IEEE 802.3 (Ethernet) standards. It states that a logic 0 is represented by a high\u2013low signal sequence and a logic 1 is represented by a low\u2013high signal sequence.\nIf a Manchester encoded signal is inverted in communication, it is transformed from one convention to the other. This ambiguity can be overcome by using differential Manchester encoding.\nDecoding.\nThe existence of guaranteed transitions allows the signal to be self-clocking, and also allows the receiver to align correctly; the receiver can identify if it is misaligned by half a bit period, as there will no longer always be a transition during each bit period. The price of these benefits is a doubling of the bandwidth requirement compared to simpler NRZ coding schemes.\nEncoding.\nEncoding conventions are as follows:\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41351", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41351", "title": "Mandrel wrapping", "text": "In multimode fibre optics, mandrel wrapping is a technique used to preferentially attenuate high-order mode power of a propagating optical signal. Consequently, if the fibre is propagating substantial energy in affected modes, the modal distribution will be changed.\nA cylindrical rod wrap consists of a specified number turns of fibre on a mandrel of specified size, depending on the fibre characteristics and the desired modal distribution. It has application in optical transmission performance tests, to create a defined mode power distribution or to prevent multimode propagation in single mode fibre. If the launch fibre is fully filled ahead of the mandrel wrap, the higher-order modes will be stripped off, leaving only lower-order modes. If the launch fibre is underfilled, for example as a consequence of being energized by a laser diode or edge-emitting LED, there will be no effect on the mode power distribution or loss measurements.\nIn multimode fibre, mandrel wrapping is used to eliminate the effect of \"transient loss\", the tendency of high-order modes to experience higher loss than lower-order modes. Numerical addition (in decibels) of the measured loss of multiple fibre segments and/or components overestimates the loss of the concatenated set if each segment or component has been measured with a full mode power distribution.\nIn single-mode optical fibre measurements, it is used to enforce true single-mode propagation at wavelengths near or below the theoretical cutoff wavelength, at which substantial power can exist in a higher-order mode group. In this use, it is commonly termed a High Order Mode Filter (HOMF).\nUltimately, the effect of mandrel wrapping on optical measurements depends on the propagating mode power distribution. An additional loss mechanism has no effect unless power is present in the affected modes.\nPrinciple of operation.\nThe effect of physically bending an optical fibre around a cylindrical form is to slightly modify the effective refractive index in the curved region, which locally reduces the effective mode volume of the fibre. This causes optical power in the highest order modes to become unguided, or so weakly guided as to be released into an unbound state, absorbed by the fibre coating or completely ejected from the fibre. The practical effect of mandrel wrapping is to attenuate optical power propagating in the highest-order modes. Lower-order modes are unaffected, experiencing neither increased loss nor conversion into other modes (mode mixing).\nDetermination of appropriate mandrel wrap conditions.\nThe mandrel diameter and number of turns are chosen to eliminate certain modes in a reproducible way. It is empirically observed that more than 5 full 360-degree wraps creates little additional loss, so 3 to 5 turns are commonly specified. The mandrel diameter affects how far into the mode volume the modal unbinding occurs. Experimentally, one plots the transmitted power from a wrapped fibre into which a uniform modal power distribution has been excited, as a function of mandrel diameter, maintaining a constant number of turns. This reveals step-like reductions in transmitted power as the diameter decreases, where each step is the point at which the mandrel is beginning to affect the next-lower mode group. For best measurement reproducibility, one would select a diameter that is not near such a transition, although this may not be possible if measurements must be performed over a range of wavelengths. Total mode volume in a fiber is a function of wavelength, so the mandrel diameter at which the mode group transitions occur will change with wavelength.\nExternal links.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41352", "revid": "7098284", "url": "https://en.wikipedia.org/wiki?curid=41352", "title": "Margin", "text": "Margin may refer to:\n&lt;templatestyles src=\"Template:TOC_right/styles.css\" /&gt;\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "41353", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41353", "title": "Maritime broadcast communications net", "text": "Telecommunications network\nIn telecommunications, a maritime broadcast communications net is a communications net that is used for international distress calling, including international lifeboat, lifecraft, and survival-craft high frequency (HF); aeronautical emergency very high frequency (VHF); survival ultra high frequency (UHF); international calling and safety very high frequency (VHF); combined scene-of-search-and-rescue; and other similar and related purposes. \n\"Note:\" Basic international distress calling is performed at either medium frequency (MF) or at high frequency (HF).\nSee also.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41354", "revid": "949717", "url": "https://en.wikipedia.org/wiki?curid=41354", "title": "Master frequency generator", "text": "A master frequency generator or master electronic oscillator, in frequency-division multiplexing (FDM), is a piece of equipment used to provide system end-to-end carrier frequency synchronization and frequency accuracy of tones. \nThe following types of oscillators are used in the Defense Communications System FDM systems:"}
{"id": "41355", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41355", "title": "Master station", "text": "Network station to control other stations\nIn telecommunications, a master station is a station that controls or coordinates the activities of other stations in the system.\nExamples:\nOperation modes.\nIn data transmission, a master station can be set to not wait for a reply from a slave station after transmitting each message or transmission block. In this case the station is said to be in \"continuous operation\".\nReferences.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41358", "revid": "1461430", "url": "https://en.wikipedia.org/wiki?curid=41358", "title": "Maximal-ratio combining", "text": "In telecommunications, maximum-ratio combining (MRC) is a method of diversity combining in which:\nIt is also known as ratio-squared combining and predetection combining. Maximum-ratio combining is the optimum combiner for independent additive white Gaussian noise channels.\nMRC can restore a signal to its original shape. The technique was invented by American engineer Leonard R. Kahn in 1954.\nMRC has also been found in the field of neuroscience, where it has been shown that neurons in the retina scale their dependence on two sources of input in proportion to the signal-to-noise ratio of the inputs.\nThis has the advantage of producing an output with acceptable SNR even when none of the individual signals are themselves acceptable.\nExample: Least Squares estimate in the case of Rx diversity.\nWe consider an example of which the receiver is endowed with N antennas. In this case, the received vector formula_1 is\n&lt;templatestyles src=\"Numbered block/styles.css\" /&gt;\nwhere formula_2 is noise vector formula_3. Following the ML detection criterion the detection procedure may be written as\nwhere formula_4 is the considered constellation of formula_5 and formula_6 is the least square solution to the above model.\n&lt;templatestyles src=\"Numbered block/styles.css\" /&gt;\nThe least square solution in this case is also known as maximum-ratio-combining (MRC). In the case of N antennas the LS can be written as\nwhich means that the signal from each antenna is rotated and weighted according to the phase and strength of the channel, such that the signals from all antennas are combined to yield the maximum ratio between signal and noise terms.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "41359", "revid": "3628267", "url": "https://en.wikipedia.org/wiki?curid=41359", "title": "Maximum usable frequency", "text": "Highest radio frequency that can be used for skywave radio transmission\nIn radio transmission, maximum usable frequency (MUF) is the highest radio frequency that can be used for transmission between two points on Earth by reflection from the ionosphere (skywave or skip) at a specified time, independent of transmitter power. This index is especially useful for shortwave transmissions.\nIn shortwave radio communication, a major mode of long distance propagation is for the radio waves to reflect off the ionized layers of the atmosphere and return diagonally back to Earth. In this way radio waves can travel beyond the horizon, around the curve of the Earth. However the refractive index of the ionosphere decreases with increasing frequency, so there is an upper limit to the frequency which can be used. Above this frequency the radio waves are not reflected by the ionosphere but are transmitted through it into space.\nThe ionization of the atmosphere varies with time of day and season as well as with solar conditions, so the upper frequency limit for skywave communication varies throughout the day. MUF is a median frequency, defined as the highest frequency at which skywave communication is possible 50% of the days in a month, as opposed to the lowest usable high frequency (LUF) which is the frequency at which communication is possible 90% of the days, and the frequency of optimum transmission (FOT).\nTypically the MUF is a predicted number. Given the maximum observed frequency (MOF) for a mode on each day of the month at a given hour, the MUF is the highest frequency for which an ionospheric communications path is predicted on 50% of the days of the month.\nOn a given day, communications may or may not succeed at the MUF. Commonly, the optimal operating frequency for a given path is estimated at 80 to 90% of the MUF. As a rule of thumb the MUF is approximately 3 times the critical frequency.\nformula_1\nwhere the critical frequency is the highest frequency reflected for a signal propagating directly upward and \u03b8 is the angle of incidence.\nOptimum Working Frequency.\nAnother important parameter used in skywave propagation is the optimum working frequency (OWF), which estimates the maximum frequency that must be used for a given critical frequency and incident angle. It is the frequency chosen to avoid the irregularities of the atmosphere.\nformula_2\nSources.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt; \n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41360", "revid": "2255048", "url": "https://en.wikipedia.org/wiki?curid=41360", "title": "Maximum user signaling rate", "text": ""}
{"id": "41363", "revid": "33324950", "url": "https://en.wikipedia.org/wiki?curid=41363", "title": "Mean time between outages", "text": "In a system the mean time between outages (MTBO) is the mean time between equipment failures that result in loss of system continuity or unacceptable degradation. \nThe MTBO is calculated by the equation,\nformula_1\nwhere MTBF is the nonredundant mean time between failures and FFAS is the fraction of failures for which the failed equipment is automatically bypassed. \nReferences.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41364", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41364", "title": "Mechanically induced modulation", "text": "Noise created in a multimode fiber by an imperfect splice\nMechanically induced modulation is an optical signal modulation induced by mechanical means.\nAn example of deleterious mechanically induced modulation is speckle noise created in a multimode fiber by an imperfect splice or imperfectly mated connectors. Mechanical disturbance of the fiber ahead of the joint will introduce changes in the modal structure, resulting in variations of joint loss. This is a subset of many mechanisms that can lead to modal noise in an optical system.\nReferences.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41365", "revid": "42522270", "url": "https://en.wikipedia.org/wiki?curid=41365", "title": "Mediation function", "text": "Telecommunications network management function\nIn telecommunications network management, a mediation function is a function that routes or acts on information passing between network elements and network operations. \nExamples of mediation functions are communications control, protocol conversion, data handling, communications of primitives, processing that includes decision-making, and data storage. \nMediation functions can be shared among network elements, mediation devices, and network operation centers.\nSources.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41367", "revid": "13249226", "url": "https://en.wikipedia.org/wiki?curid=41367", "title": "Message", "text": "Discrete unit of communication\nA message is a unit of communication that conveys information from a sender to a receiver. It can be transmitted through various forms, such as spoken or written words, signals, or electronic data, and can range from simple instructions to complex information.\nThe consumption of the message relies on how the recipient interprets the message, there are times where the recipient contradicts the intention of the message which results in a boomerang effect. Message fatigue is another outcome recipients can obtain if a message is conveyed too much by the source.\nOne example of a message is a press release, which may vary from a brief report or statement released by a public agency to commercial publicity material. Another example of a message is how they are portrayed to a consumer via an advertisement.\nRoles in human communication.\nIn communication between humans, messages can be verbal or nonverbal:\nThe phrase \"send a message\" or \"sending a message\" is also used for actions taken by a party to convey that party's attitude towards a certain thing. For example, a government that executes people who commit acts of treason is sending a message that treason will not be tolerated. Conversely, a party that appears through its actions to endorse something that it opposes can be said to be \"sending the wrong message\", while one which appears to simultaneously endorse contradictory things can be said to be sending \"mixed messages\".\nIn computer science.\nEvents vs. Messages.\nIn distributed systems, events represent a fact or state change (e.g., \"OrderPlaced\") and are typically broadcast asynchronously to multiple consumers, promoting loose coupling and scalability. While events generally don\u2019t expect an immediate response, acknowledgment mechanisms are often implemented at the infrastructure level (e.g., Kafka commit offsets, SNS delivery statuses) rather than being an inherent part of the event pattern itself.\nIn contrast, messages serve a broader role, encompassing commands (e.g., \"ProcessPayment\"), events (e.g., \"PaymentProcessed\"), and documents (e.g., \"DataPayload\"). Both events and messages can support various delivery guarantees, including at-least-once, at-most-once, and exactly-once, depending on the technology stack and implementation. However, exactly-once delivery is often achieved through idempotency mechanisms rather than true, infrastructure-level exactly-once semantics.\nDelivery patterns for both events and messages include publish/subscribe (one-to-many) and point-to-point (one-to-one). While request/reply is technically possible, it is more commonly associated with messaging patterns rather than pure event-driven systems. Events excel at state propagation and decoupled notifications, while messages are better suited for command execution, workflow orchestration, and explicit coordination.\nModern architectures commonly combine both approaches, leveraging events for distributed state change notifications and messages for targeted command execution and structured workflows based on specific timing, ordering, and delivery requirements.\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41368", "revid": "33839581", "url": "https://en.wikipedia.org/wiki?curid=41368", "title": "Message format", "text": "In telecommunications, a message format is a predetermined or prescribed spatial or time-sequential arrangement of the parts of a message that is recorded in or on a data storage medium.\nAt one time, messages prepared for electrical transmission were composed on a printed blank form with spaces for each part of the message and for administrative entries.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41369", "revid": "7903804", "url": "https://en.wikipedia.org/wiki?curid=41369", "title": "Micro-mainframe link", "text": "In telecommunications, a micro-mainframe link is a physical or logical connection established between a remote microprocessor and mainframe host computer for the express purpose of uploading, downloading, or viewing interactive data and databases on-line in real time. \n\"Note:\" A micro-mainframe link usually requires terminal emulation software on the microcomputer.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41370", "revid": "668752", "url": "https://en.wikipedia.org/wiki?curid=41370", "title": "Minimum bend radius", "text": ""}
{"id": "41371", "revid": "44062", "url": "https://en.wikipedia.org/wiki?curid=41371", "title": "Mixer", "text": "Mixer may refer to:\n&lt;templatestyles src=\"Template:TOC_right/styles.css\" /&gt;\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "41372", "revid": "10202399", "url": "https://en.wikipedia.org/wiki?curid=41372", "title": "Sprague\u2013Grundy theorem", "text": "Every impartial game position is equivalent to a position in the game of nim\nIn combinatorial game theory, the Sprague\u2013Grundy theorem states that every impartial game under the normal play convention is equivalent to a one-heap game of nim, or to an infinite generalization of nim. It can therefore be represented as a natural number, the size of the heap in its equivalent game of nim, as an ordinal number in the infinite generalization, or alternatively as a nimber, the value of that one-heap game in an algebraic system whose addition operation combines multiple heaps to form a single equivalent heap in nim.\nThe Grundy value or nim-value of any impartial game is the unique nimber that the game is equivalent to. In the case of a game whose positions are indexed by the natural numbers (like nim itself, which is indexed by its heap sizes), the sequence of nimbers for successive positions of the game is called the nim-sequence of the game.\nThe Sprague\u2013Grundy theorem and its proof encapsulate the main results of a theory discovered independently by R. P. Sprague (1936) and P. M. Grundy (1939).\nDefinitions.\nFor the purposes of the Sprague\u2013Grundy theorem, a game is a two-player sequential game of perfect information satisfying the \"ending condition\" (all games come to an end: there are no infinite lines of play) and the \"normal play condition\" (a player who cannot move loses).\nAt any given point in the game, a player's position is the set of moves they are allowed to make. As an example, we can define the \"zero game\" to be the two-player game where neither player has any legal moves. Referring to the two players as formula_1 (for Alice) and formula_2 (for Bob), we would denote their positions as formula_3, since the set of moves each player can make is empty.\nAn impartial game is one in which at any given point in the game, each player is allowed exactly the same set of moves. Normal-play nim is an example of an impartial game. In nim, there are one or more heaps of objects, and two players (we'll call them Alice and Bob), take turns choosing a heap and removing 1 or more objects from it. The winner is the player who removes the final object from the final heap. The game is impartial because for any given configuration of pile sizes, the moves Alice can make on her turn are exactly the same moves Bob would be allowed to make if it were his turn. In contrast, a game such as checkers is not impartial because, supposing Alice were playing red and Bob were playing black, for any given arrangement of pieces on the board, if it were Alice's turn, she would only be allowed to move the red pieces, and if it were Bob's turn, he would only be allowed to move the black pieces.\nNote that any configuration of an impartial game can therefore be written as a single position, because the moves will be the same no matter whose turn it is. For example, the position of the \"zero game\" can simply be written formula_4, because if it's Alice's turn, she has no moves to make, and if it's Bob's turn, he has no moves to make either.\nA move can be associated with the position it leaves the next player in. \nDoing so allows positions to be defined recursively. For example, consider the following game of Nim played by Alice and Bob.\nExample Nim Game.\n&lt;templatestyles src=\"Pre/styles.css\"/&gt;\nNimbers.\nThe special names formula_6, formula_9, and formula_17 referenced in our example game are called nimbers. In general, the nimber formula_24 corresponds to the position in a game of nim where there are exactly formula_25 objects in exactly one heap. \nFormally, nimbers are defined inductively as follows: formula_6 is formula_4, formula_28, formula_29 and for all formula_30, formula_31.\nWhile the word \"nim\"ber comes from the game \"nim\", nimbers can be used to describe the positions of any finite, impartial game, and in fact, the Sprague\u2013Grundy theorem states that every instance of a finite, impartial game can be associated with a \"single\" nimber.\nCombining Games.\nTwo games can be combined by adding their positions together.\nFor example, consider another game of nim with heaps formula_32, formula_33, and formula_34.\nExample Game 2.\n&lt;templatestyles src=\"Pre/styles.css\"/&gt;\nWe can combine it with our first example to get a combined game with six heaps: formula_1, formula_2, formula_37, formula_32, formula_33, and formula_34:\nCombined Game.\n&lt;templatestyles src=\"Pre/styles.css\"/&gt;\nTo differentiate between the two games, for the first example game, we'll label its starting position formula_41, and color it blue:\nformula_42\nFor the second example game, we'll label the starting position formula_43 and color it red:\nformula_44\nTo compute the starting position of the combined game, remember that a player can either make a move in the first game, leaving the second game untouched, or make a move in the second game, leaving the first game untouched. So the combined game's starting position is:\nformula_45\nThe explicit formula for adding positions is: formula_46, which means that addition is both commutative and associative.\nEquivalence.\nPositions in impartial games fall into two outcome classes: either the next player (the one whose turn it is) wins (an formula_47- position), or the previous player wins (a formula_48- position). So, for example, formula_6 is a formula_50-position, while formula_9 is an formula_52-position.\nTwo positions formula_53 and formula_54 are equivalent if, no matter what position formula_55 is added to them, they are always in the same outcome class. \nFormally,\nformula_56 if and only if formula_57, formula_58 is in the same outcome class as formula_59.\nTo use our running examples, notice that in both the first and second games above, we can show that on every turn, Alice has a move that forces Bob into a formula_50-position. Thus, both formula_41 and formula_43 are formula_52-positions. (Notice that in the combined game, \"Bob\" is the player with the formula_52-positions. In fact, formula_65 is a formula_50-position, which as we will see in Lemma 2, means formula_67.)\nFirst Lemma.\nAs an intermediate step to proving the main theorem, we show that for every position formula_53 and every formula_50-position formula_1, the equivalence formula_71 holds. By the above definition of equivalence, this amounts to showing that formula_72 and formula_73 share an outcome class for all formula_55.\nSuppose that formula_72 is a formula_50-position. Then the previous player has a winning strategy for formula_73: respond to moves in formula_1 according to their winning strategy for formula_1 (which exists by virtue of formula_1 being a formula_50-position), and respond to moves in formula_72 according to their winning strategy for formula_72 (which exists for the analogous reason). So formula_73 must also be a formula_50-position.\nOn the other hand, if formula_72 is an formula_52-position, then formula_73 is also an formula_52-position, because the next player has a winning strategy: choose a formula_50-position from among the formula_72 options, and we conclude from the previous paragraph that adding formula_1 to that position is still a formula_50-position. Thus, in this case, formula_73 must be a formula_52-position, just like formula_72.\nAs these are the only two cases, the lemma holds.\nSecond Lemma.\nAs a further step, we show that formula_97 if and only if formula_98 is a formula_50-position.\nIn the forward direction, suppose that formula_97. Applying the definition of equivalence with formula_101, we find that formula_102 (which is equal to formula_98 by commutativity of addition) is in the same outcome class as formula_104. But formula_104 must be a formula_50-position: for every move made in one copy of formula_53, the previous player can respond with the same move in the other copy, and so always make the last move.\nIn the reverse direction, since formula_108 is a formula_50-position by hypothesis, it follows from the first lemma, formula_110, that formula_111. Similarly, since formula_112 is also a formula_50-position, it follows from the first lemma in the form formula_114 that formula_115. By associativity and commutativity, the right-hand sides of these results are equal. Furthermore, formula_116 is an equivalence relation because equality is an equivalence relation on outcome classes. Via the transitivity of formula_116, we can conclude that formula_97.\nProof of the Sprague\u2013Grundy theorem.\nWe prove that all positions are equivalent to a nimber by structural induction. The more specific result, that the given game's initial position must be equivalent to a nimber, shows that the game is itself equivalent to a nimber.\nConsider a position formula_119. By the induction hypothesis, all of the options are equivalent to nimbers, say formula_120. So let formula_121. We will show that formula_122, where formula_123 is the mex (minimum exclusion) of the numbers formula_124, that is, the smallest non-negative integer not equal to some formula_125.\nThe first thing we need to note is that formula_126, by way of the second lemma. If formula_127 is zero, the claim is trivially true. Otherwise, consider formula_98. If the next player makes a move to formula_129 in formula_53, then the previous player can move to formula_131 in formula_54, and conversely if the next player makes a move in formula_54. After this, the position is a formula_50-position by the lemma's forward implication. Therefore, formula_98 is a formula_50-position, and, citing the lemma's reverse implication, formula_126.\nNow let us show that formula_138 is a formula_50-position, which, using the second lemma once again, means that formula_140. We do so by giving an explicit strategy for the previous player.\nSuppose that formula_54 and formula_142 are empty. Then formula_138 is the null set, clearly a formula_50-position.\nOr consider the case that the next player moves in the component formula_142 to the option formula_146 where formula_147. Because formula_123 was the \"minimum\" excluded number, the previous player can move in formula_54 to formula_146. And, as shown before, any position plus itself is a formula_50-position.\nFinally, suppose instead that the next player moves in the component formula_54 to the option formula_131. If formula_154 then the previous player moves in formula_142 to formula_131; otherwise, if formula_157, the previous player moves in formula_131 to formula_142; in either case the result is a position plus itself. (It is not possible that formula_160 because formula_123 was defined to be different from all the formula_125.)\nIn summary, we have formula_97 and formula_140. By transitivity, we conclude that formula_122, as desired.\nDevelopment.\nIf formula_53 is a position of an impartial game, the unique integer formula_123 such that formula_122 is called its Grundy value, or Grundy number, and the function that assigns this value to each such position is called the Sprague\u2013Grundy function. R. L. Sprague and P. M. Grundy independently gave an explicit definition of this function, not based on any concept of equivalence to nim positions, and showed that it had the following properties:\nIt follows straightforwardly from these results that if a position formula_53 has a Grundy value of formula_123, then formula_58 has the same Grundy value as formula_176, and therefore belongs to the same outcome class, for any position formula_55. Thus, although Sprague and Grundy never explicitly stated the theorem described in this article, it follows directly from their results and is credited to them.\nThese results have subsequently been developed into the field of combinatorial game theory, notably by Richard Guy, Elwyn Berlekamp, John Horton Conway and others, where they are now encapsulated in the Sprague\u2013Grundy theorem and its proof in the form described here. The field is presented in the books \"Winning Ways for your Mathematical Plays\" and \"On Numbers and Games\".\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41373", "revid": "12279355", "url": "https://en.wikipedia.org/wiki?curid=41373", "title": "Mode field diameter", "text": "In fiber optics, the mode field diameter (MFD) is a measure of the width of an irradiance distribution, i.e., the optical power per unit area, across the end face of a single-mode fiber. It is analogous to the formula_1 measure of the beam diameter for a beam propagating in free space.\nThe mode field diameter is defined as twice the mode field radius, and the mode field radius is equal to the distance from the center at which the electric and magnetic field strengths are reduced to formula_2 of their maximum values. Since the intensity (given by the Poynting vector) is proportional to the square of the field amplitude, the intensity drops by formula_3 or -8.69 dB at this distance from the center. For a Gaussian-shaped mode, the mode field diameter is twice the beam waist formula_4.\nThe MFD is typically slightly larger than the core of an optical fiber extending slightly into the cladding.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nExternal links.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41374", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41374", "title": "Mode partition noise", "text": "In an optical communication link, mode partition noise is phase jitter of the signal caused by the combined effects of mode hopping in the optical source and intramodal distortion in the fiber. \nMode hopping causes random wavelength changes which in turn affect the group velocity, \"i.e.\", the propagation time. Over a long length of fiber, the cumulative effect is to create jitter, \"i.e.\" mode partition noise. The variation of group velocity creates the mode partition noise.\nReferences.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41375", "revid": "40993790", "url": "https://en.wikipedia.org/wiki?curid=41375", "title": "Mode scrambler", "text": "In telecommunications, a mode scrambler or mode mixer is a device for inducing mode coupling in an optical fiber, or a device that, itself, exhibits a uniform output intensity profile independent of the input mode volume or modal excitation condition. Mode scramblers are used to provide a modal distribution that is independent of the optical source for purposes of laboratory, manufacturing, or field measurements or tests. Mode scramblers are primarily used to improve reproducibility of multimode fiber bandwidth measurements.\nOverview.\nIf multimode fiber bandwidth is measured using a laser diode directly coupled to its input, the resulting measurement can vary by as much as an order of magnitude. This measurement variability is due to the combination of differences in laser output characteristics (emitted mode power distribution) and the differential mode delay of the fiber. Differential mode delay is the difference in the time delays amongst the fiber's propagating modes caused by imperfections or nonideality of the fiber refractive index profile.\nThe primary purpose of a mode scrambler is to create a uniform, overfilled launch condition that can be easily reproduced on multiple measurement systems, so that measurement systems have essentially the same launch conditions and can measure approximately the same bandwidth despite having different laser sources. These were used for this purpose in the first U.S. NIST round-robins on multimode fiber. The overfilled launch (OFL) was created to reduce measurement variability, and improve concatenation estimates for multimode fibers, used at that time for telecom 'long haul' (e.g., 7\u201310\u00a0km 850\u00a0nm or 20\u201330\u00a0km 1300\u00a0nm) systems.\nWhen the telecom industry converted to near-exclusive use of single-mode fiber ca. 1984, multimode fiber was re-purposed for use in LANs, such as Fiber Distributed Data Interface (FDDI), then under development. The output modal power distribution of a mode scrambler is similar to the surface-emitters used in those first LAN transmitters, but this was fortuitous coincidence. On average, but not in every case, the OFL bandwidth measured using a mode scrambler is lower than that produced by excitation of a partial mode volume (restricted mode launch or RML), such as occurs with directly coupled laser diodes.\nTypes.\nThere are two common types of mode scramblers: the \"Step-Graded-Step\" (S-G-S) and the \"step index with bends\". The S-G-S mode scrambler is actually an assembly, a fusion-spliced concatenation of a step-index profile, a graded-index profile and another step-index profile fiber. Typically, each segment is approximately 1 meter long, and may use segments of unconventional size to produce the distribution required according to core size of fiber to be tested. Unconventional fiber size was not an issue, as they were developed by fiber manufacturers, but some test equipment has difficulty complying with revised qualification standards, and now use \"Step Index with Bends\" mode scramblers, which can be adjusted to purpose. Step Index with Bend mode scramblers are created simply by routing a specially designed step-index multimode fiber through a series of small radius bends, or by compressing fiber against surfaces with specific roughness. The implementations are simple, but generally less reproducible, and require care to avoid over-stressing the fiber.\nA mode scrambler can be characterized and qualified by measuring its near-field and far-field distributions, as well as by measuring one of these distributions while restricting the other. Guidelines for constructing a mode scrambler and qualifying its output can be found in the ANSI/TIA/EIA-455-54 fiber optic test procedure (FOTP).\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41376", "revid": "28481209", "url": "https://en.wikipedia.org/wiki?curid=41376", "title": "Mode volume", "text": "Number of bound modes that an optical fiber is capable of supporting\nMode volume may refer to figures of merit used either to characterise optical and microwave cavities or optical fibers.\nIn electromagnetic cavities.\nThe mode volume (or modal volume) of an optical or microwave cavity is a measure of how concentrated the electromagnetic energy of a single cavity mode is in space, expressed as an effective volume in which most of the energy associated with an electromagentic mode is confined. Various expressions may be used to estimate this volume:\nformula_1\nformula_2\nformula_3\nwhere formula_4 is the electric field strength, formula_5 is the magnetic flux density, formula_6 is the electric permittivity, formula_7 denotes the magnetic permeability, and formula_8 denotes the maximum value of its functional argument. In each definition the integral is over all space and may diverge in leaky cavities where the electromagnetic energy can radiate out to infinity and is thus not is not confined within the cavity volume. In this case modifications to the expressions above may be required to give an effective mode volume.\nThe mode volume of a cavity or resonator is of particular importance in cavity quantum electrodynamics where it determines the magnitude of the Purcell effect and coupling strength between cavity photons and atoms in the cavity. In particular, the Purcell factor is given by\n formula_9\nwhere formula_10 is the vacuum wavelength, formula_11 is the refractive index of the cavity material (so formula_12 is the wavelength inside the cavity), and formula_13 and formula_14 are the cavity quality factor and mode volume, respectively.\nIn fiber optics.\nIn fiber optics, mode volume is the number of bound modes that an optical fiber is capable of supporting. \nThe mode volume \"M\" is approximately given by formula_15 and formula_16, respectively for step-index and power-law index profile fibers, where \"g\" is the profile parameter, and \"V\" is the normalized frequency, \"which must be greater than 5 for this approximation to be valid\".\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41377", "revid": "50649061", "url": "https://en.wikipedia.org/wiki?curid=41377", "title": "Modification of Final Judgment", "text": "Legal agreement in U.S. anti-trust litigation\nIn United States telecommunication law, the Modification of Final Judgment (MFJ) is the August 24, 1982 consent decree concerning the antitrust lawsuit of January 14, 1949, \"United States vs. Western Electric Company and American Telephone and Telegraph Company\" and its \"Final Judgment\" on January 24, 1956, the latter of which it vacated. The decree was made with Harold H. Greene as presiding judge in the United States District Court for the District of Columbia.\nThe terms required the breakup of the Bell System and a reorganization of the American Telephone and Telegraph Company (AT&amp;T), including removing local telephone service from AT&amp;T control and placing business restrictions on the divested regional telephone companies in exchange for removing other longstanding restrictions on the types of business AT&amp;T could enter.\nThe MFJ also consolidated the case \"United States v. AT&amp;T\" filed on November 20, 1974.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41378", "revid": "6127189", "url": "https://en.wikipedia.org/wiki?curid=41378", "title": "Modified AMI code", "text": "Digital telecommunications technique\nModified AMI codes are a digital telecommunications technique to maintain system synchronization. Alternate mark inversion (AMI) line codes are modified by deliberate insertion of bipolar violations. There are several types of modified AMI codes, used in various T-carrier and E-carrier systems.\nOverview.\nThe clock rate of an incoming T-carrier is extracted from its bipolar line code. Each signal transition provides an opportunity for the receiver to see the transmitter's clock. The AMI code guarantees that transitions are always present before and after each mark (1 bit), but are missing between adjacent spaces (0 bits). To prevent loss of synchronization when a long string of zeros is present in the payload, deliberate bipolar violations are inserted into the line code, to create a sufficient number of transitions to maintain synchronization; this is a form of run length limited coding. The receive terminal equipment recognizes the bipolar violations and removes from the user data the marks attributable to the bipolar violations.\nT-carrier was originally developed for voice applications. When voice signals are digitized for transmission via T-carrier, the data stream always includes ample 1 bits to maintain synchronization. (To help this, the \u03bc-law algorithm for digitizing voice signals encodes silence as a continuous stream of 1 bits.) However, when used for the transmission of digital data, the conventional AMI line code may fail to have sufficient marks to permit recovery of the incoming clock, and synchronization is lost. This happens when there are too many consecutive zeros in the user data being transported. \nThe exact pattern of bipolar violations that is transmitted in any given case depends on the line rate (\"i.e.\", the level of the line code in the T-carrier hierarchy) and the polarity of the last valid mark in the user data prior to the unacceptably long string of zeros. It would not be useful to have a violation immediately following a mark, as that would not produce a transition. For this reason, all modified AMI codes include a space (0 bit) before each violation mark.\nIn the descriptions below, \"B\" denotes a balancing mark with the opposite polarity to that of the preceding mark, while \"V\" denotes a bipolar violation mark, which has the same polarity as the preceding mark. In order to preserve AMI coding's desirable absence of DC bias, the number of positive marks must equal the number of negative marks. This happens automatically for balancing (B) marks, but the line code must ensure that positive and negative violation marks balance each other.\nZero length code suppression.\nThe first technique used to ensure a minimum density of marks was zero code suppression a form of bit stuffing, which set the least significant bit of each 8-bit byte transmitted to a 1. (This bit was already unavailable due to robbed-bit signaling.) This avoided the need to modify the AMI code in any way, but limited available data rates to 56,000 bits per second per DS0 voice channel. Also, the low minimum density of ones (12.5%) sometimes led to increased clock slippage on the span.\nIncreased demand for bandwidth, and compatibility with the G.703 and ISDN PRI standards which called for 64,000 bits per second, led to this system being superseded by B8ZS.\nB8ZS (North American T1).\nCommonly used in the North American T1 (Digital Signal 1) 1.544 Mbit/s line code, bipolar with eight-zero substitution (B8ZS) replaces each string of 8 consecutive zeros with the special pattern \"000VB0VB\". Depending on the polarity of the preceding mark, that could be 000+\u22120\u2212+ or 000\u2212+0+\u2212.\nB6ZS (North American T2).\nAt the North American T2 rate (6.312\u00a0Mbit/s), bipolar violations are inserted if 6 or more consecutive zeros occur. This line code is called bipolar with six-zero substitution (B6ZS), and replaces 6 consecutive zeros with the pattern \"0VB0VB\". Depending on the polarity of the preceding mark, that could be 0+\u22120\u2212+ or 0\u2212+0+\u2212.\nHDB3 (European E-carrier).\nUsed in all levels of the European E-carrier system, the high density bipolar of order 3 (HDB3) code replaces any instance of 4 consecutive 0 bits with one of the patterns \"000V\" or \"B00V\". The choice is made to ensure that consecutive violations are of differing polarity; i.e., separated by an odd number of normal + or \u2212 marks.\nThese rules are applied on the code as it is being built from the original string. Every time there are 4 consecutive zeros in the code they will be replaced by either 000\u2212, 000+, +00+ or \u221200\u2212. To determine which pattern to use, one must count the number of pluses (+) and the number of minuses (\u2212) since the last violation bit V, then subtract one from the other. If the result is an odd number then 000\u2212 or 000+ is used. If the result is an even number then +00+ or \u221200\u2212 is used. To determine which polarity to use, one must look at the pulse preceding the four zeros. If 000V form must be used then V simply copies the polarity of last pulse, if B00V form must be used then B and V chosen will have the opposite polarity of the last pulse.\nExamples.\nHere are some examples of bit streams codes with AMI and HDB3. All assume the same starting conditions: the previous 1 bit was \u2212, and the previous violation was an even number of 1 bits ago. (E.g. the preceding bits could have been ++\u2212.)\nB3ZS (North American T3).\nAt the North American T3 rate (44.736\u00a0Mbit/s), bipolar violations are inserted if 3 or more consecutive zeros occur. This line code is called bipolar with three-zero substitution (B3ZS), and is very similar to HDB3. Each run of 3 consecutive zeros is replaced by \"00V\" or \"B0V\". The choice is made to ensure that consecutive violations are of differing polarity, i.e. separated by an odd number of normal B marks.\nSee also.\nOther line codes that have 3 states:\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41379", "revid": "285145", "url": "https://en.wikipedia.org/wiki?curid=41379", "title": "Modulation factor", "text": ""}
{"id": "41380", "revid": "2378565", "url": "https://en.wikipedia.org/wiki?curid=41380", "title": "Modulation rate", "text": ""}
{"id": "41382", "revid": "1268366746", "url": "https://en.wikipedia.org/wiki?curid=41382", "title": "\u039c-law algorithm", "text": "Audio companding algorithm\nThe \u03bc-law algorithm (sometimes written mu-law, often abbreviated as u-law) is a companding algorithm, primarily used in 8-bit PCM digital telecommunications systems in North America and Japan. It is one of the two companding algorithms in the G.711 standard from ITU-T, the other being the similar A-law. A-law is used in regions where digital telecommunication signals are carried on E-1 circuits, e.g. Europe.\nThe terms PCMU, G711u or G711MU are used for G711 \u03bc-law.\nCompanding algorithms reduce the dynamic range of an audio signal. In analog systems, this can increase the signal-to-noise ratio (SNR) achieved during transmission; in the digital domain, it can reduce the quantization error (hence increasing the signal-to-quantization-noise ratio). These SNR increases can be traded instead for reduced bandwidth for equivalent SNR.\nAt the cost of a reduced peak SNR, it can be mathematically shown that \u03bc-law's non-linear quantization effectively increases dynamic range by 33\u00a0dB or &lt;templatestyles src=\"Fraction/styles.css\" /&gt;5+1\u20442 bits over a linearly-quantized signal, hence 13.5 bits (which rounds up to 14 bits) is the most resolution required for an input digital signal to be compressed for 8-bit \u03bc-law.\nAlgorithm types.\nThe \u03bc-law algorithm may be described in an analog form and in a quantized digital form.\nContinuous.\nFor a given input x, the equation for \u03bc-law encoding is\nformula_1\nwhere \"\u03bc\" = 255 in the North American and Japanese standards, and sgn(\"x\") is the sign function. The range of this function is \u22121 to 1.\n\u03bc-law expansion is then given by the inverse equation:\nformula_2\nDiscrete.\nThe discrete form is defined in ITU-T Recommendation G.711.\nG.711 is unclear about how to code the values at the limit of a range (e.g. whether +31 codes to 0xEF or 0xF0).\nHowever, G.191 provides example code in the C language for a \u03bc-law encoder. The difference between the positive and negative ranges, e.g. the negative range corresponding to +30 to +1 is \u221231 to \u22122. This is accounted for by the use of 1's complement (simple bit inversion) rather than 2's complement to convert a negative value to a positive value during encoding.\nImplementation.\nThe \u03bc-law algorithm may be implemented in several ways:\n Use the continuous version of the \u03bc-law algorithm to calculate the companded values.\nUsage justification.\n\u03bc-law encoding is used because speech has a wide dynamic range. In analog signal transmission, in the presence of relatively constant background noise, the finer detail is lost. Given that the precision of the detail is compromised anyway, and assuming that the signal is to be perceived as audio by a human, one can take advantage of the fact that the perceived acoustic intensity level or loudness is logarithmic by compressing the signal using a logarithmic-response operational amplifier (Weber\u2013Fechner law). In telecommunications circuits, most of the noise is injected on the lines, thus after the compressor, the intended signal is perceived as significantly louder than the static, compared to an uncompressed source. This became a common solution, and thus, prior to common digital usage, the \u03bc-law specification was developed to define an interoperable standard.\nThis pre-existing algorithm had the effect of significantly lowering the amount of bits required to encode a recognizable human voice in digital systems. A sample could be effectively encoded using \u03bc-law in as little as 8 bits, which conveniently matched the symbol size of the majority of common computers.\n\u03bc-law encoding effectively reduced the dynamic range of the signal, thereby increasing the coding efficiency while biasing the signal in a way that results in a signal-to-distortion ratio that is greater than that obtained by linear encoding for a given number of bits.\nThe \u03bc-law algorithm is also used in the .au format, which dates back at least to the SPARCstation 1 by Sun Microsystems as the native method used by the /dev/audio interface, widely used as a de facto standard for sound on Unix systems. The au format is also used in various common audio APIs such as the classes in the sun.audio Java package in Java 1.1 and in some C# methods.\nThis plot illustrates how \u03bc-law concentrates sampling in the smaller (softer) values. The horizontal axis represents the byte values 0-255 and the vertical axis is the 16-bit linear decoded value of \u03bc-law encoding.\nComparison with A-law.\nThe \u03bc-law algorithm provides a slightly larger dynamic range than the A-law at the cost of worse proportional distortions for small signals. By convention, A-law is used for an international connection if at least one country uses it.\nReferences.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41383", "revid": "6702527", "url": "https://en.wikipedia.org/wiki?curid=41383", "title": "Multicast address", "text": "Logical identifier addressing a specific group of devices on a network\nA multicast address is a logical identifier for a group of hosts in a computer network that are available to process datagrams or frames intended to be multicast for a designated network service. Multicast addressing can be used in the link layer (layer 2 in the OSI model), such as Ethernet multicast, and at the internet layer (layer 3 for OSI) for Internet Protocol Version 4 (IPv4) or Version 6 (IPv6) multicast.\nIPv4.\nIPv4 multicast addresses are defined by the most-significant bit pattern of \"1110\". This originates from the classful network design of the early Internet when this group of addresses was designated as \"Class D\". The CIDR notation for this group is . The group includes the addresses from to .\nThe address range is divided into blocks each assigned a specific purpose or behavior.\nAddresses in the range of to are individually assigned by IANA and designated for multicasting on the local subnetwork only. For example, the Routing Information Protocol (RIPv2) uses , Open Shortest Path First (OSPF) uses and , and Multicast DNS uses . Routers must not forward these messages outside the subnet from which they originate.\nAddresses in the range to are individually assigned by IANA and designated as the \"internetwork control block\". This block of addresses is used for traffic that must be routed through the public Internet, such as for applications of the Network Time Protocol using .\nAddresses in three separate blocks are not individually assigned by IANA. These addresses are globally routed and are used for applications that don't fit either of the previously described purposes.\nAddresses in the reserved range are not individually assigned by IANA. Fallen out of use for security considerations, experimental Session Announcement Protocol was the primary means of supplying addresses through Session Description Protocol, which now is mostly used in the establishment of private sessions.\nThe (IPv4) and (IPv6) blocks are reserved for use by source-specific multicast.\nThe range was originally assigned as an experimental, public statically-assigned multicast address space for publishers and Internet service providers that wished to source content on the Internet. The allocation method is termed GLOP addressing and provides implementers a block of 255 addresses that is determined by their 16-bit autonomous system number (ASN) allocation. In a nutshell, the middle two octets of this block are formed from assigned ASNs, giving any operator assigned an ASN 256 globally unique multicast group addresses. The method is not applicable to the newer 32-bit ASNs. In none }}, the IETF envisioned a broader use of the range for many-to-many multicast applications. Unfortunately, with only 256 multicast addresses available to each autonomous system, GLOP is not adequate for large-scale broadcasters.\nThe range is assigned as a range of global IPv4 multicast address space provided to each organization that has or larger globally routed unicast address space allocated; one multicast address is reserved per of unicast space. A resulting advantage over GLOP is that the unicast-prefix mechanism resembles the unicast-prefix capabilities of IPv6.\nThe range is assigned for private use within an organization. Packets destined to administratively scoped IPv4 multicast addresses do not cross administratively defined organizational boundaries, and administratively scoped IPv4 multicast addresses are locally assigned and do not have to be globally unique. The range may be structured to be loosely similar to the scoped IPv6 multicast address.\nIn support of link-local multicasts which do not use IGMP, any IPv4 multicast address that falls within the and ranges will be broadcast to all ports on many Ethernet switches, even if IGMP snooping is enabled, so addresses within these ranges should be avoided on Ethernet networks where the functionality of IGMP snooping is desired.\nNotable IPv4 multicast addresses.\nThe following table is a list of notable well-known IPv4 addresses that are reserved for IP multicasting and that are registered with the Internet Assigned Numbers Authority (IANA).\nIPv6.\nMulticast addresses in IPv6 use the prefix .\nBased on the value of the flag bits, IPv6 multicast addresses can be unicast-prefix\u2013based multicast addresses, source-specific multicast addresses, or embedded\u2013rendezvous-point IPv6 multicast addresses. Each of these types of multicast addresses have their own format and follow specific rules.\nSimilar to a unicast address, the \"prefix\" of an IPv6 multicast address specifies its scope, however, the set of possible scopes for a multicast address is different. The 4-bit \"scope\" field (bits 12 to 15) is used to indicate where the address is valid and unique.\nThe service is identified in the \"group ID\" field. For example, if refers to all Network Time Protocol (NTP) servers on the local network segment, then refers to all NTP servers in an organization's networks. The \"group ID\" field may be further divided for special multicast address types.\nNotable IPv6 multicast addresses.\nThe following table is a list notable IPv6 multicast addresses that are registered with IANA.\nTo be included in some of the below multicast groups a client must send a Multicast Listener Discovery (MLD), a component of ICMPv6 suite, to join that group. For example, to listen to , a client must send a MLD report to the router, containing the multicast address, to indicate that it wants to listen to that group.\nEthernet.\nEthernet frames with a value of 1 in the least-significant bit of the first octet of the destination MAC address are treated as multicast frames and are flooded to all points on the network. While frames with ones in all bits of the destination address () are sometimes referred to as broadcasts, Ethernet generally does not distinguish between multicast and broadcast frames. Modern Ethernet controllers filter received packets to reduce CPU load, by looking up the hash of a multicast destination address in a table, initialized by software, which controls whether a multicast packet is dropped or fully received.\nThe IEEE has allocated the address block to for group addresses for use by standard protocols. Of these, the MAC group addresses in the range of to are not forwarded by 802.1D-conformant MAC bridges.\n802.11.\n802.11 wireless networks use the same MAC addresses for multicast as Ethernet.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41384", "revid": "279219", "url": "https://en.wikipedia.org/wiki?curid=41384", "title": "Multilevel precedence and preemption", "text": ""}
{"id": "41385", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41385", "title": "Multipath propagation", "text": "Concept in radio communication\nIn radio communication, multipath is the propagation phenomenon that results in radio signals reaching the receiving antenna by two or more paths. Causes of multipath include atmospheric ducting, ionospheric reflection and refraction, and reflection from water bodies and terrestrial objects such as mountains and buildings. When the same signal is received over more than one path, it can create interference and phase shifting of the signal. Destructive interference causes fading; this may cause a radio signal to become too weak in certain areas to be received adequately. For this reason, this effect is also known as multipath interference or multipath distortion.\nWhere the magnitudes of the signals arriving by the various paths have a distribution known as the Rayleigh distribution, this is known as Rayleigh fading. Where one component (often, but not necessarily, a line of sight component) dominates, a Rician distribution provides a more accurate model, and this is known as Rician fading. Where two components dominate, the behavior is best modeled with the two-wave with diffuse power (TWDP) distribution. All of these descriptions are commonly used and accepted and lead to results. However, they are generic and abstract/hide/approximate the underlying physics. \nInterference.\nMultipath interference is a phenomenon in the physics of waves whereby a wave from a source travels to a detector via two or more paths and the two (or more) components of the wave interfere constructively or destructively. Multipath interference is a common cause of \"ghosting\" in analog television broadcasts and of fading of radio waves.\nThe condition necessary is that the components of the wave remain coherent throughout the whole extent of their travel.\nThe interference will arise owing to the two (or more) components of the wave having, in general, travelled a different length (as measured by optical path length \u2013 geometric length and refraction (differing optical speed)), and thus arriving at the detector out of phase with each other.\nThe signal due to indirect paths interferes with the required signal in amplitude as well as phase which is called multipath fading.\nExamples.\nIn analog facsimile and television transmission, multipath causes jitter and ghosting, seen as a faded duplicate image to the right of the main image. Ghosts occur when transmissions bounce off a mountain or other large object, while also arriving at the antenna by a shorter, direct route, with the receiver picking up two signals separated by a delay.\nIn radar processing, multipath causes ghost targets to appear, deceiving the radar receiver. These ghosts are particularly bothersome since they move and behave like the normal targets (which they echo), and so the receiver has difficulty in isolating the correct target echo. These problems can be minimized by incorporating a ground map of the radar's surroundings and eliminating all echoes which appear to originate below the ground or above a certain height (altitude).\nIn digital radio communications (such as GSM) multipath can cause errors and affect the quality of communications. The errors are due to intersymbol interference (ISI). Equalizers are often used to correct the ISI. Alternatively, techniques such as orthogonal frequency division modulation and rake receivers may be used.\nIn a Global Positioning System receiver, multipath effects can cause a stationary receiver's output to indicate as if it were randomly jumping about or creeping. When the unit is moving the jumping or creeping may be hidden, but it still degrades the displayed accuracy of location and speed.\nIn wired media.\nMultipath propagation is similar in power line communication and in telephone local loops. In either case, impedance mismatch causes signal reflection. \nHigh-speed power line communication systems usually employ multi-carrier modulations (such as OFDM or wavelet OFDM) to avoid the intersymbol interference that multipath propagation would cause. The ITU-T G.hn standard provides a way to create a high-speed (up to 1 gigabit per second) local area network using existing home wiring (power lines, phone lines, and coaxial cables). G.hn uses OFDM with a cyclic prefix to avoid ISI. Because multipath propagation behaves differently in each kind of wire, G.hn uses different OFDM parameters (OFDM symbol duration, guard interval duration) for each media.\nDSL modems also use orthogonal frequency-division multiplexing to communicate with their DSLAM despite multipath. In this case the reflections may be caused by mixed wire gauges, but those from bridge taps are usually more intense and complex. Where OFDM training is unsatisfactory, bridge taps may be removed.\nMathematical modeling.\nThe mathematical model of the multipath can be presented using the method of the impulse response used for studying linear systems.\nSuppose you want to transmit a single, ideal Dirac pulse of electromagnetic power at time 0, i.e.\nformula_1\nAt the receiver, due to the presence of the multiple electromagnetic paths, more than one pulse will be received, and each one of them will arrive at different times. In fact, since the electromagnetic signals travel at the speed of light, and since every path has a geometrical length possibly different from that of the other ones, there are different air travelling times (consider that, in free space, the light takes 3\u00a0\u03bcs to cross a 1\u00a0km span). Thus, the received signal will be expressed by\nformula_2\nwhere formula_3 is the number of received impulses (equivalent to the number of electromagnetic paths, and possibly very large), formula_4 is the time delay of the generic formula_5 impulse, and formula_6 represent the complex amplitude (i.e., magnitude and phase) of the generic received pulse. As a consequence, formula_7 also represents the impulse response function formula_8 of the equivalent multipath model.\nMore in general, in presence of time variation of the geometrical reflection conditions, this impulse response is time varying, and as such we have\nformula_9\nformula_10\nformula_11\nVery often, just one parameter is used to denote the severity of multipath conditions: it is called the multipath time, formula_12, and it is defined as the time delay existing between the first and the last received impulses\nformula_13\nIn practical conditions and measurement, the multipath time is computed by considering as last impulse the first one which allows receiving a determined amount of the total transmitted power (scaled by the atmospheric and propagation losses), e.g. 99%.\nKeeping our aim at linear, time invariant systems, we can also characterize the multipath phenomenon by the channel transfer function formula_14, which is defined as the continuous time Fourier transform of the impulse response formula_8\nformula_16\nwhere the last right-hand term of the previous equation is easily obtained by remembering that the Fourier transform of a Dirac pulse is a complex exponential function, an eigenfunction of every linear system.\nThe obtained channel transfer characteristic has a typical appearance of a sequence of peaks and valleys (also called \"notches\"); it can be shown that, on average, the distance (in Hz) between two consecutive valleys (or two consecutive peaks), is roughly inversely proportional to the multipath time. The so-called coherence bandwidth is thus defined as\nformula_17\nFor example, with a multipath time of 3\u00a0\u03bcs (corresponding to a 1\u00a0km of added on-air travel for the last received impulse), there is a coherence bandwidth of about 330\u00a0kHz.\nReferences.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41386", "revid": "1546024", "url": "https://en.wikipedia.org/wiki?curid=41386", "title": "Multiple access", "text": ""}
{"id": "41387", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41387", "title": "Multiple homing", "text": "In telecommunications, the term multiple homing has the following meanings: \nIn military, such as Missiles and loitering munitions, it is ability of a single weapon system or projectile to select, focus and simultaneously engage multiple targets.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41388", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41388", "title": "Multiplex baseband", "text": "In telecommunications, the term multiplex baseband has the following meanings: \nFor example, the output of a group multiplexer consists of a band of frequencies from 60\u00a0kHz to 108\u00a0kHz. This is the group-level baseband that results from combining 12 voice-frequency input channels, having a bandwidth of 4\u00a0kHz each, including guard bands. In turn, 5 groups are multiplexed into a super group having a baseband of 312\u00a0kHz to 552\u00a0kHz. This baseband, however, does not represent a group-level baseband. Ten super groups are in turn multiplexed into one master group, the output of which is a baseband that may be used to modulate a microwave-frequency carrier.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41389", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41389", "title": "Multiplexing", "text": "Method of combining multiple signals into one signal over a shared medium\nIn telecommunications and computer networking, multiplexing (sometimes contracted to muxing) is a method by which multiple analog or digital signals are combined into one signal over a shared medium. The aim is to share a scarce resource\u2014a physical transmission medium. For example, in telecommunications, several telephone calls may be carried using one wire. Multiplexing originated in telegraphy in the 1870s, and is now widely applied in communications. In telephony, George Owen Squier is credited with the development of telephone carrier multiplexing in 1910.\nThe multiplexed signal is transmitted over a communication channel such as a cable. The multiplexing divides the capacity of the communication channel into several logical channels, one for each message signal or data stream to be transferred. A reverse process, known as demultiplexing, extracts the original channels on the receiver end.\nA device that performs the multiplexing is called a multiplexer (MUX), and a device that performs the reverse process is called a demultiplexer (DEMUX or DMX).\nInverse multiplexing (IMUX) has the opposite aim as multiplexing, namely to break one data stream into several streams, transfer them simultaneously over several communication channels, and recreate the original data stream.\nIn computing, I/O multiplexing can also be used to refer to the concept of processing multiple input/output events from a single event loop, with system calls like poll and select (Unix).\nTypes.\nMultiple variable bit rate digital bit streams may be transferred efficiently over a single fixed bandwidth channel by means of statistical multiplexing. This is an asynchronous mode time-domain multiplexing which is a form of time-division multiplexing.\nDigital bit streams can be transferred over an analog channel by means of code-division multiplexing techniques such as frequency-hopping spread spectrum (FHSS) and direct-sequence spread spectrum (DSSS).\nIn wireless communications, multiplexing can also be accomplished through alternating polarization (horizontal/vertical or clockwise/counterclockwise) on each adjacent channel and satellite, or through phased multi-antenna array combined with a multiple-input multiple-output communications (MIMO) scheme.\nSpace-division multiplexing.\nIn wired communication, space-division multiplexing, also known as space-division multiple access (SDMA) is the use of separate point-to-point electrical conductors for each transmitted channel. Examples include an analog stereo audio cable, with one pair of wires for the left channel and another for the right channel, and a multi-pair telephone cable, a switched star network such as a telephone access network, a switched Ethernet network, and a mesh network.\nIn wireless communication, space-division multiplexing is achieved with multiple antenna elements forming a phased array antenna. Examples are multiple-input and multiple-output (MIMO), single-input and multiple-output (SIMO) and multiple-input and single-output (MISO) multiplexing. An IEEE 802.11g wireless router with \"k\" antennas makes it in principle possible to communicate with \"k\" multiplexed channels, each with a peak bit rate of 54 Mbit/s, thus increasing the total peak bit rate by the factor \"k\". Different antennas would give different multi-path propagation (echo) signatures, making it possible for digital signal processing techniques to separate different signals from each other. These techniques may also be utilized for space diversity (improved robustness to fading) or beamforming (improved selectivity) rather than multiplexing.\nFrequency-division multiplexing.\nFrequency-division multiplexing (FDM) is inherently an analog technology. FDM achieves the combining of several signals into one medium by sending signals in several distinct frequency ranges over a single medium. In FDM the signals are electrical signals.\nOne of the most common applications for FDM is traditional radio and television broadcasting from terrestrial, mobile or satellite stations, or cable television. Only one cable reaches a customer's residential area, but the service provider can send multiple television channels or signals simultaneously over that cable to all subscribers without interference. Receivers must tune to the appropriate frequency (channel) to access the desired signal.\nA variant technology, called wavelength-division multiplexing (WDM) is used in optical communications.\nTime-division multiplexing.\nTime-division multiplexing (TDM) is a digital (or in rare cases, analog) technology that uses time, instead of space or frequency, to separate the different data streams. TDM involves sequencing groups of a few bits or bytes from each individual input stream, one after the other, and in such a way that they can be associated with the appropriate receiver. If done sufficiently quickly, the receiving devices will not detect that some of the circuit time was used to serve another logical communication path.\nConsider an application requiring four terminals at an airport to reach a central computer. Each terminal communicated at 2400 baud, so rather than acquire four individual circuits to carry such a low-speed transmission, the airline has installed a pair of multiplexers. A pair of 9600 baud modems and one dedicated analog communications circuit from the airport ticket desk back to the airline data center are also installed. Some web proxy servers (e.g. polipo) use TDM in HTTP pipelining of multiple HTTP transactions onto the same TCP/IP connection.\nCarrier-sense multiple access and multidrop communication methods are similar to time-division multiplexing in that multiple data streams are separated by time on the same medium, but because the signals have separate origins instead of being combined into a single signal, are best viewed as channel access methods, rather than a form of multiplexing.\nTD is a legacy multiplexing technology still providing the backbone of most National fixed-line telephony networks in Europe, providing the 2 Mbit/s voice and signaling ports on narrow-band telephone exchanges such as the DMS100. Each E1 or 2 Mbit/s TDM port provides either 30 or 31 speech timeslots in the case of CCITT7 signaling systems and 30 voice channels for customer-connected Q931, DASS2, DPNSS, V5 and CASS signaling systems.\nPolarization-division multiplexing.\nPolarization-division multiplexing uses the polarization of electromagnetic radiation to separate orthogonal channels. It is in practical use in both radio and optical communications, particularly in 100 Gbit/s per channel fiber-optic transmission systems.\nDifferential Cross-Polarized Wireless Communications is a novel method for polarized antenna transmission utilizing a differential technique.\nOrbital angular momentum multiplexing.\nOrbital angular momentum multiplexing is a relatively new and experimental technique for multiplexing multiple channels of signals carried using electromagnetic radiation over a single path. It can potentially be used in addition to other physical multiplexing methods to greatly expand the transmission capacity of such systems. As of 2012[ [update]] it is still in its early research phase, with small-scale laboratory demonstrations of bandwidths of up to 2.5 Tbit/s over a single light path. This is a controversial subject in the academic community, with many claiming it is not a new method of multiplexing, but rather a special case of space-division multiplexing.\nCode-division multiplexing.\nCode-division multiplexing (CDM), code-division multiple access (CDMA) or spread spectrum is a class of techniques where several channels simultaneously share the same frequency spectrum, and this spectral bandwidth is much higher than the bit rate or symbol rate. One form is frequency hopping, another is direct sequence spread spectrum. In the latter case, each channel transmits its bits as a coded channel-specific sequence of pulses called chips. Number of chips per bit, or chips per symbol, is the spreading factor. This coded transmission typically is accomplished by transmitting a unique time-dependent series of short pulses, which are placed within chip times within the larger bit time. All channels, each with a different code, can be transmitted on the same fiber or radio channel or other medium, and asynchronously demultiplexed. Advantages over conventional techniques are that variable bandwidth is possible (just as in statistical multiplexing), that the wide bandwidth allows poor signal-to-noise ratio according to Shannon\u2013Hartley theorem, and that multi-path propagation in wireless communication can be combated by rake receivers.\nA significant application of CDMA is the Global Positioning System (GPS).\nMultiple access method.\nA multiplexing technique may be further extended into a multiple access method or channel access method, for example, TDM into time-division multiple access (TDMA) and statistical multiplexing into carrier-sense multiple access (CSMA). A multiple-access method makes it possible for several transmitters connected to the same physical medium to share their capacity.\nMultiplexing is provided by the physical layer of the OSI model, while multiple access also involves a media access control protocol, which is part of the data link layer.\nThe Transport layer in the OSI model, as well as TCP/IP model, provides statistical multiplexing of several application layer data flows to/from the same computer.\nCode-division multiplexing (CDM) is a technique in which each channel transmits its bits as a coded channel-specific sequence of pulses. This coded transmission is typically accomplished by transmitting a unique time-dependent series of short pulses, which are placed within chip times within the larger bit time. All channels, each with a different code, can be transmitted on the same fiber and asynchronously demultiplexed. Other widely used multiple access techniques are time-division multiple access (TDMA) and frequency-division multiple access (FDMA).\nCode-division multiplex techniques are used as an access technology, namely code-division multiple access (CDMA), in Universal Mobile Telecommunications System (UMTS) standard for the third-generation (3G) mobile communication identified by the ITU.\nApplication areas.\nTelegraphy.\nThe earliest communication technology using electrical wires, and therefore sharing an interest in the economies afforded by multiplexing, was the electric telegraph. Early experiments allowed two separate messages to travel in opposite directions simultaneously, first using an electric battery at both ends, then at only one end.\n\u00c9mile Baudot developed a time-multiplexing system of multiple Hughes machines in the 1870s. In 1874, the quadruplex telegraph developed by Thomas Edison transmitted two messages in each direction simultaneously, for a total of four messages transiting the same wire at the same time. Several researchers were investigating acoustic telegraphy, a frequency-division multiplexing technique, which led to the invention of the telephone.\nTelephony.\nIn telephony, a customer's telephone line now typically ends at the remote concentrator box, where it is multiplexed along with other telephone lines for that neighborhood or other similar area. The multiplexed signal is then carried to the central switching office on significantly fewer wires and for much further distances than a customer's line can practically go. This is likewise also true for digital subscriber lines (DSL).\nFiber in the loop (FITL) is a common method of multiplexing, which uses optical fiber as the backbone. It not only connects POTS phone lines with the rest of the PSTN, but also replaces DSL by connecting directly to Ethernet wired into the home. Asynchronous Transfer Mode is often the communications protocol used.\nCable TV has long carried multiplexed television channels, and late in the 20th century began offering the same services as telephone companies. IPTV also depends on multiplexing.\nVideo processing.\nIn video editing and processing systems, multiplexing refers to the process of interleaving audio and video into one coherent data stream.\nIn digital video, such a transport stream is normally a feature of a container format which may include metadata and other information, such as subtitles. The audio and video streams may have variable bit rate. Software that produces such a transport stream and/or container is commonly called a multiplexer or muxer. A demuxer is software that extracts or otherwise makes available for separate processing the components of such a stream or container.\nDigital broadcasting.\nIn digital television systems, several variable bit-rate data streams are multiplexed together to a fixed bit-rate transport stream by means of statistical multiplexing. This makes it possible to transfer several video and audio channels simultaneously over the same frequency channel, together with various services. This may involve several standard-definition television (SDTV) programs (particularly on DVB-T, DVB-S2, ISDB and ATSC-C), or one HDTV, possibly with a single SDTV companion channel over one 6 to 8\u00a0MHz-wide TV channel. The device that accomplishes this is called a statistical multiplexer. In several of these systems, the multiplexing results in an MPEG transport stream. The newer DVB standards DVB-S2 and DVB-T2 has the capacity to carry several HDTV channels in one multiplex.\nIn digital radio, a multiplex (also known as an ensemble) is a number of radio stations that are grouped together. A multiplex is a stream of digital information that includes audio and other data.\nOn communications satellites which carry broadcast television networks and radio networks, this is known as multiple channel per carrier or MCPC. Where multiplexing is not practical (such as where there are different sources using a single transponder), single channel per carrier mode is used.\nAnalog broadcasting.\nIn FM broadcasting and other analog radio media, multiplexing is a term commonly given to the process of adding subcarriers to the audio signal before it enters the transmitter, where modulation occurs. (In fact, the stereo multiplex signal can be generated using time-division multiplexing, by switching between the two (left channel and right channel) input signals at an ultrasonic rate (the subcarrier), and then filtering out the higher harmonics.) Multiplexing in this sense is sometimes known as MPX, which in turn is also an old term for stereophonic FM, seen on stereo systems since the 1960s.\nOther meanings.\nIn spectroscopy the term is used to indicate that the experiment is performed with a mixture of frequencies at once and their respective response unraveled afterward using the Fourier transform principle.\nIn computer programming, it may refer to using a single in-memory resource (such as a file handle) to handle multiple external resources (such as on-disk files).\nSome electrical multiplexing techniques do not require a physical \"multiplexer\" device, they refer to a \"keyboard matrix\" or \"Charlieplexing\" design style:\nIn high-throughput DNA sequencing, the term is used to indicate that some artificial sequences (often called \"barcodes\" or \"indexes\") have been added to link given sequence reads to a given sample, and thus allow for the sequencing of multiple samples in the same reaction.\nIn sociolinguistics, multiplexity is used to describe the number of distinct connections between individuals who are part of a social network. A multiplex network is one in which members share a number of ties stemming from more than one social context, such as workmates, neighbors, or relatives.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "41390", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41390", "title": "Multiport repeater", "text": ""}
{"id": "41391", "revid": "757572", "url": "https://en.wikipedia.org/wiki?curid=41391", "title": "Narrative traffic", "text": "Narrative traffic is data communications consisting of plain or encrypted messages written in a natural language and transmitted in accordance with standard formats and procedures. \nExamples of narrative traffic include:\nReferences.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41392", "revid": "47684007", "url": "https://en.wikipedia.org/wiki?curid=41392", "title": "Narrowband modem", "text": "Modem constrained to 4 kHz voice bandwidth\nIn telecommunications, a narrowband modem is a modem whose modulated output signal has an essential frequency spectrum that is limited to that which can be wholly contained within, and faithfully transmitted through, a voice channel with a nominal 4\u00a0kHz bandwidth. \n\"Note:\" High frequency (HF) modems are limited to operation over a voice channel with a nominal 3\u00a0kHz bandwidth.\nReferences.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41394", "revid": "1211100901", "url": "https://en.wikipedia.org/wiki?curid=41394", "title": "National Communications System", "text": "The National Communications System (NCS) was an office within the United States Department of Homeland Security charged with enabling national security and emergency preparedness communications (NS/EP telecommunications) using the national telecommunications system. The NCS was disbanded by on July 6, 2012.\nBackground and history.\nThe genesis of the NCS began in 1962 after the Cuban Missile Crisis when communications problems among the United States, the Union of Soviet Socialist Republics, the North Atlantic Treaty Organization, and foreign heads of state threatened to complicate the crisis further. After the crisis, President John F. Kennedy ordered an investigation of national security communications, and the National Security Council (NSC) formed an interdepartmental committee to examine the communications networks and institute changes. This interdepartmental committee recommended the formation of a single unified communications system to serve the President, Department of Defense, diplomatic and intelligence activities, and civilian leaders. Consequently, in order to provide better communications support to critical government functions during emergencies, President Kennedy established the National Communications System by a Presidential Memorandum on August 21, 1963. The NCS mandate included linking, improving, and extending the communications facilities and components of various Federal agencies, focusing on interconnectivity and survivability.\nOn April 3, 1984, President Ronald Reagan signed which broadened the NCS' national security and emergency preparedness (NS/EP) capabilities and superseded President Kennedy's original 1963 memorandum. The NCS expanded from its original six members to an interagency group of 23 federal departments and agencies, and began coordinating and planning NS/EP telecommunications to support crises and disasters.\nWith the addition of the Office of the Director of National Intelligence (ODNI) on September 30, 2007, the NCS membership stood at 24 members.\nEach NCS member organization was represented on the NCS through the Committee of Principals (COP) \u2013 and its subordinate Council of Representatives (COR). The COP, formed as a result of Executive Order 12472, provided advice and recommendations to the NCS and the National Security Council through the President's Critical Infrastructure Protection Board on NS/EP telecommunications and its ties to other critical infrastructures. The NCS also participated in joint industry-Government planning through its work with the President's National Security Telecommunications Advisory Committee (NSTAC), with the NCS's National Coordinating Center for Telecommunications (NCC) and the NCC's subordinate Information Sharing and Analysis Center (ISAC).\nAfter nearly forty years with the Secretary of Defense serving as its Executive Agent, President George W. Bush transferred the National Communications System to the Department of Homeland Security (DHS). The NCS was one of 22 federal agencies transferred to the department on March 1, 2003, in accordance with . A revised Executive Order 12472 reflects the changes of E.O. 13286. On November 15, 2005, the NCS became part of the department's Directorate for Preparedness after nearly two years under the Information Analysis and Infrastructure Protection Directorate. In March 2007 the NCS became an entity of the National Protection and Programs Directorate. The DHS Under Secretary for National Protection and Programs Directorate served as the NCS Manager.\nOn July 6, 2012, President Barack Obama signed , which replaced Executive Order 12472, thus eliminating the NCS as a separate organization; it was merged into the Office of Emergency Communications (OEC) of DHS' National Preparedness and Programs Directorate (NPPD) which had been created in 2007. A ceremony to retire the colors of the NCS and to celebrate the legacy of the organization was held on August 30, 2012 in Arlington, VA. Upon establishment of the Cybersecurity and Infrastructure Security Agency (CISA) the OEC was renamed the Emergency Communications Division (ECD).\nThe President's National Security Telecommunications Advisory Committee helps strengthen United States national security, enhancing cybersecurity, maintaining the global communications infrastructure, assuring communications for disaster response, and addressing critical infrastructure interdependencies and dependencies. On September 29, 2017, President Donald Trump renewed several committees including the \"President's National Security Telecommunications Advisory Committee\".\nServices.\nIn fulfillment of their mission to enable emergency communications, the NCS has created a number of different services.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nSources.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;\nExternal links.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "41395", "revid": "287953", "url": "https://en.wikipedia.org/wiki?curid=41395", "title": "National Electric Code", "text": ""}
{"id": "41396", "revid": "24198", "url": "https://en.wikipedia.org/wiki?curid=41396", "title": "National Information Infrastructure", "text": "1991 USA telecommunications policy\nThe National Information Infrastructure (NII) was the product of the High Performance Computing Act of 1991. It was a telecommunications policy buzzword, which was popularized during the Clinton Administration under the leadership of Vice-President Al Gore.\nProposal.\nIt proposed to build communications networks, interactive services, interoperable computer hardware and software, computers, databases, and consumer electronics in order to put vast amounts of information available to both public and private sectors. NII was to have included more than just the physical facilities (more than the cameras, scanners, keyboards, telephones, fax machines, computers, switches, compact disks, video and audio tape, cable, wire, satellites, optical fiber transmission lines, microwave nets, switches, televisions, monitors, and printers) used to transmit, store, process, and display voice, data, and images; it was also to encompass a wide range of interactive functions, user-tailored services, and multimedia databases that were interconnected in a technology-neutral manner that will favor no one industry over any other.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41397", "revid": "252195", "url": "https://en.wikipedia.org/wiki?curid=41397", "title": "Near-field diffraction pattern", "text": ""}
{"id": "41398", "revid": "7611264", "url": "https://en.wikipedia.org/wiki?curid=41398", "title": "Near-field region", "text": ""}
{"id": "41399", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=41399", "title": "Near real-time", "text": ""}
{"id": "41400", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=41400", "title": "Negative-acknowledge character", "text": ""}
{"id": "41402", "revid": "12240634", "url": "https://en.wikipedia.org/wiki?curid=41402", "title": "Neper", "text": "Logarithmic unit for ratios of measurements of physical field and power quantities\nThe neper (symbol: Np) is a logarithmic unit for ratios of measurements of physical field and power quantities, such as gain and loss of electronic signals. The unit's name is derived from the name of John Napier, the inventor of logarithms. As is the case for the decibel and bel, the neper is a unit defined in the international standard ISO 80000. It is not part of the International System of Units (SI), but is accepted for use alongside the SI.\nDefinition.\nLike the decibel, the neper is a unit in a logarithmic scale. While the bel uses the decadic (base-10) logarithm to compute ratios, the neper uses the natural logarithm, based on Euler's number (\"e\" \u2248 2.71828). The level of a ratio of two signal amplitudes or root-power quantities, with the unit neper, is given by\nformula_1\nwhere formula_2 and formula_3 are the signal amplitudes, and ln is the natural logarithm. The level of a ratio of two power quantities, with the unit neper, is given by\nformula_4\nwhere formula_5 and formula_6 are the signal powers.\nIn the International System of Quantities, the neper is defined as 1 Np = 1.\nUnits.\nThe neper is defined in terms of ratios of field quantities \u2014 also called \"root-power quantities\" \u2014 (for example, voltage or current amplitudes in electrical circuits, or pressure in acoustics), whereas the decibel was originally defined in terms of power ratios. A power ratio 10 log \"r\" dB is equivalent to a field-quantity ratio 20 log \"r\" dB, since power in a linear system is proportional to the square (Joule's laws) of the amplitude. Hence the decibel and the neper have a fixed ratio to each other:\nformula_7\nand\nformula_8\nThe (voltage) level ratio is\nformula_9\nLike the decibel, the neper is a dimensionless unit. The International Telecommunication Union (ITU) recognizes both units. Only the neper is coherent with the SI.\nApplications.\nThe neper is a natural linear unit of relative difference, meaning in nepers (logarithmic units) relative differences add rather than multiply. This property is shared with logarithmic units in other bases, such as the bel.\nThe derived units decineper (1 dNp = 0.1 neper) and centineper (1 cNp = 0.01 neper) are also used. The centineper for root-power quantities corresponds to a log point or log percentage, see \"\".\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41403", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41403", "title": "Net gain (telecommunications)", "text": "Overall gain of a transmission circuit\nIn telecommunications, net gain is the overall gain of a transmission circuit. Net gain is measured by applying a test signal at an appropriate power level at the input port of a circuit and measuring the power delivered at the output port. The net gain in dB is calculated by taking 10 times the common logarithm of the ratio of the output power to the input power.\nThe net gain expressed in dB may be positive or negative. If the net gain expressed in dB is negative, it is also called the net loss. If the net gain is expressed as a ratio, and the ratio is less than unity, a net loss is indicated.\nThe test signal must be chosen so that its power level is within the usual operating range of the circuit being tested.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41404", "revid": "28903366", "url": "https://en.wikipedia.org/wiki?curid=41404", "title": "Net operation", "text": "Gathering of radio stations\nA radio net is three or more radio stations communicating with each other on a common channel or frequency. A net is essentially a moderated conference call conducted over two-way radio, typically in half-duplex operating conditions and commonly through a radio repeater. The use of half-duplex operation requires a very particular set of operating procedures to be followed in order to avoid inefficiencies and chaos.\nNets operate either on schedule or continuously (continuous watch). Nets operating on schedule handle traffic only at definite, prearranged times and in accordance with a prearranged schedule of intercommunication. Nets operating continuously are prepared to handle traffic at any time; they maintain operators on duty at all stations in the net at all times. When practicable, messages relating to schedules will be transmitted by a means of signal communication other than radio.\nNet operations:\nNet manager.\nA net manager is the person who supervises the creation and operation of a net over multiple sessions. This person will specify the format, date, time, participants, and the net control script. The net manager will also choose the Net Control Station for each net, and may occasionally take on that function, especially in smaller organizations.\nNet Control Station.\nRadio nets are like conference calls in that both have a moderator who initiates the group communication, who ensures all participants follow the standard procedures, and who determines and directs when each other station may talk. The moderator in a radio net is called the Net Control Station, formally abbreviated NCS, and has the following duties:\nThe Net Control Station will, for each net, appoint at least one Alternate Net Control Station, formally abbreviated ANCS (abbreviated NC2 in WWII procedures), who has the following duties:\nStructure of the net.\nNets can be described as always having a net opening and a net closing, with a roll call normally following the net opening, itself followed by regular net business, which may include announcements, official business, and message passing. Military nets will follow a very abbreviated and opaque version of the structure outlined below, but will still have the critical elements of opening, roll call, late check-ins, and closing.\nA net should always operate on the same principle as the inverted pyramid used in journalism\u2014the most important communications always come first, followed by content in ever lower levels of priority.\nEach net will typically have a main purpose, which varies according to the organization conducting the net, which occurs during the net business phase. For amateur radio nets, it's typically for the purpose of allowing stations to discuss their recent operating activities (stations worked, antennas built, etc.) or to swap equipment. For Military Auxiliary Radio System and National Traffic System nets, net business will involve mainly the passing of formal messages, known as radiograms.\nNet-control procedure words.\nU.S. Army Field Manual ACP 125(G) has the most complete set of procedure words used in radio nets:\nTypes of radio nets.\nMaritime mobile nets serve the needs of seagoing vessels.\nCivil Air Patrol nets.\nThe Civil Air Patrol defines a different set of nets:\nAmateur radio nets.\nThe International Amateur Radio Union defines six different types of nets in its IARU Emergency Telecommunications Guide:\nOther Amateur radio net types\nU.S. Military radio nets.\nThe U.S. Army Field Manual FM 6-02.53, Tactical Radio Operations, defines the following types of radio nets:\nMaritime radio nets.\nWhen boats or ships are in distress, they will operate a maritime broadcast communications net to communicate among the vessel in distress and all the other vessels, aircraft, and shore stations assisting in the distress response.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nExternal links.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41405", "revid": "107439", "url": "https://en.wikipedia.org/wiki?curid=41405", "title": "Network administration", "text": ""}
{"id": "41406", "revid": "7007500", "url": "https://en.wikipedia.org/wiki?curid=41406", "title": "Network architecture", "text": "Design of a communications network\nNetwork architecture is the design of a computer network. It is a framework for the specification of a network's physical components and their functional organization and configuration, its operational principles and procedures, as well as communication protocols used.\nIn telecommunications, the specification of a network architecture may also include a detailed description of products and services delivered via a communications network, as well as detailed rate and billing structures under which services are compensated.\nThe network architecture of the Internet is predominantly expressed by its use of the Internet protocol suite, rather than a specific model for interconnecting networks or nodes in the network, or the usage of specific types of hardware links.\nOSI model.\nThe Open Systems Interconnection model (OSI model) defines and codifies the concept of layered network architecture. Abstraction layers are used to subdivide a communications system further into smaller manageable parts. A layer is a collection of similar functions that provide services to the layer above it and receives services from the layer below it. On each layer, an instance provides services to the instances at the layer above and requests services from the layer below.\nDistributed computing.\nIn distributed computing, the \"network architecture\" often describes the structure and classification of a distributed application architecture, as the participating nodes in a distributed application are often referred to as a \"network\". For example, the applications architecture of the public switched telephone network (PSTN) has been termed the Intelligent Network. There are a number of specific classifications but all lie on a continuum between the dumb network (e.g. the Internet) and the intelligent network (e.g. the PSTN).\nA popular example of such usage of the term in distributed applications, as well as permanent virtual circuits, is the organization of nodes in peer-to-peer (P2P) services and networks. P2P networks usually implement overlay networks running over an underlying physical or logical network. These overlay networks may implement certain organizational structures of the nodes according to several distinct models, the network architecture of the system.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41407", "revid": "1255990094", "url": "https://en.wikipedia.org/wiki?curid=41407", "title": "Network engineering", "text": "Network engineering may refer to:\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "41408", "revid": "13889901", "url": "https://en.wikipedia.org/wiki?curid=41408", "title": "Network interface", "text": "Network interface may refer to:\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "41409", "revid": "49998725", "url": "https://en.wikipedia.org/wiki?curid=41409", "title": "Network interface device", "text": "Device that separates the carrier's wiring from the customer's\nIn telecommunications, a network interface device (NID; also known by several other names) is a device that serves as the demarcation point between the carrier's local loop and the customer's premises wiring. Outdoor telephone NIDs also provide the subscriber with access to the station wiring and serve as a convenient test point for verification of loop integrity and of the subscriber's inside wiring.\nNaming.\nGenerically, an NID may also be called a network interface unit (NIU), telephone network interface (TNI), system network interface (SNI), or telephone network box.\nAustralia's National Broadband Network uses the term \"network termination device\" or NTD.\nA smartjack is a type of NID with capabilities beyond simple electrical connection, such as diagnostics.\nAn optical network terminal (ONT) is a type of NID used with fiber-to-the-premises applications.\nWiring termination.\nThe simplest NIDs are essentially just a specialized set of wiring terminals. These will typically take the form of a small, weather-proof box, mounted on the outside of the building. The telephone line from the telephone company will enter the NID and be connected to one side. The customer connects their wiring to the other side. A single NID enclosure may contain termination for a single line or multiple lines.\nIn its role as the demarcation point (dividing line), the NID separates the telephone company's equipment from the customer's wiring and equipment. The telephone company owns the NID and all wiring up to it. Anything past the NID is the customer's responsibility. To facilitate this, there is typically a test jack inside the NID. Accessing the test jack disconnects the customer premises wiring from the public switched telephone network and allows the customer to plug a \"known good\" telephone into the jack to isolate trouble. If the telephone works at the test jack, the problem is the customer's wiring, and the customer is responsible for repair. If the telephone does not work, the line is faulty and the telephone company is responsible for repair.\nMost NIDs also include \"circuit protectors\", which are surge protectors for a telephone line. They protect customer wiring, equipment, and personnel from any transient energy on the line, such as from a lightning strike to a utility pole.\nSimple NIDs are \"dumb\" devices, as they contain no digital logic. They have no capabilities beyond wiring termination, circuit protection, and providing a place to connect test equipment.\nSmartjack.\nSeveral types of NIDs provide more than just a terminal for the connection of wiring. Such NIDs are colloquially called \"smartjacks\" or Intelligent Network Interface Devices (INIDs) as an indication of their built-in \"intelligence\", as opposed to a simple NID, which is just a wiring device. Smartjacks are typically used for more complicated types of telecommunications service, such as T1 lines. Plain old telephone service lines generally cannot be equipped with smartjacks.\nDespite the name, most smartjacks are much more than a simple telephone jack. One common form for a smartjack is a printed circuit board with a face plate on one edge, mounted in an enclosure.\nA smartjack may provide signal conversion, converting codes and protocols, \"e.g.\", framing types, to the type needed by the customer equipment. It may buffer and/or regenerate the signal, to compensate for signal degradation from line transmission, similar to what a repeater does.\nSmartjacks also typically provide diagnostic capabilities. A very common capability provided by a smartjack is loopback, such that the signal from the telephone company is transmitted back to the telephone company. This allows the company to test the line from the central telephone exchange, without the need to have test equipment at the customer site. The telephone company usually has the ability to remotely activate loopback, without even needing personnel at the customer site. When looped back, the customer equipment is disconnected from the line.\nAdditional smartjack diagnostic capabilities include alarm indication signal, which reports trouble at one end of the line to the far end. This helps the telephone company know if trouble is present in the line, the smartjack, or customer equipment. Indicator lights to show configuration, status, and alarms are also common.\nSmartjacks typically derive their operating power from the telephone line, rather than relying on premises electrical power, although this is not a universal rule.\nOptical network terminals.\nIn fiber-to-the-premises systems, the signal is transmitted to the customer premises using optical fiber technologies. Unlike many conventional telephone technologies, this does not provide power for premises equipment, nor is it suitable for direct connection to customer equipment. An optical network terminal (ONT, an ITU-T term), also known as an optical network unit (ONU, an IEEE term), is used to terminate the optical fiber line, demultiplex the signal into its component parts (voice telephone, television, and Internet access), and provide power to customer telephones. If the device combines all these services into one it is known as an IAD. As the ONT must derive its power from the customer premises electrical supply, many ONTs have the option for a battery backup in order to maintain service in the event of a power outage or it will go in alarm mode if disconnected and customer may be notified of this. These terminals are used in both active optical networks and passive optical networks. Typically, an ONT connects via a fiber-optic cable to an OLT to complete a connection. An ONT can work in Single Family Unit/SFU mode (modem/bridge) or Home Gateway Unit/HGU mode (router). In passive optical networks, Management is provided by the OLT via OMCI, in case of a GPON connection, and OAM in case of a EPON connection. Authentication and encryption is done via LOID for EPON and PLOAM password, GPON serial number or others for GPON.\nEnvironmental conditions.\nAccording to Telcordia GR-49 requirements for telecommunications, NIDs vary based on three categories of environmental conditions:\nService providers must decide which condition best suits their application.\nCitations.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nGeneral references.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "41410", "revid": "734812", "url": "https://en.wikipedia.org/wiki?curid=41410", "title": "Network management", "text": "Discipline of administering and managing computer networks\nNetwork management is the process of administering and managing computer networks. Services provided by this discipline include fault analysis, performance management, provisioning of networks and maintaining quality of service. Network management software is used by network administrators to help perform these functions.\nTechnologies.\nA small number of accessory methods exist to support network and network device management. Network management allows IT professionals to monitor network components within large network area. Access methods include the SNMP, command-line interface (CLI), custom XML, CMIP, Windows Management Instrumentation (WMI), Transaction Language 1 (TL1), CORBA, NETCONF, RESTCONF and the Java Management Extensions (JMX).\nSchemas include the Structure of Management Information (SMI), YANG, WBEM, the Common Information Model (CIM Schema), and MTOSI amongst others.\nValue.\nEffective network management can provide positive strategic impacts. For example, in the case of developing an infrastructure, providing participants with some interactive space allows them to collaborate with each other, thereby promoting overall benefits. At the same time, the value of network management to the strategic network is also affected by the relationship between participants. Active participation, interaction and collaboration can make them more trusting of each other and enhance cohesion.\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41411", "revid": "1318515789", "url": "https://en.wikipedia.org/wiki?curid=41411", "title": "Network operating system", "text": "Computer software for running local area networks\nA network operating system (NOS) is a specialized operating system for a network device such as a router, switch or firewall.\nHistorically operating systems with networking capabilities were described as network operating systems, because they allowed personal computers (PCs) to participate in computer networks and shared file and printer access within a local area network (LAN). This description of operating systems is now largely historical, as common operating systems include a network stack to support a client\u2013server model.\nKey functions.\nNetwork operating systems (NOS) are responsible for managing various network activities. Key functions include creating and managing user accounts, controlling access to resources such as files and printers, and facilitating communication between devices. Network operating systems also monitor network performance, addresses issues, and manages resources to ensure efficient and secure operation of the network.\nHistory.\nPacket switching networks were developed to share hardware resources, such as a mainframe computer, a printer or a large and expensive hard disk.\nHistorically, a network operating system was an operating system for a computer which implemented network capabilities. Operating systems with a network stack allowed personal computers to participate in a client-server architecture in which a server enables multiple clients to share resources, such as printers.\nThese limited client/server networks were gradually replaced by peer-to-peer networks, which used networking capabilities to share resources and files located on a variety of computers of all sizes. A peer-to-peer network sets all connected computers equal; they all share the same abilities to use resources available on the network.\nToday, distributed computing and groupware applications have become the norm. Computer operating systems include a networking stack as a matter of course. During the 1980s the need to integrate dissimilar computers with network capabilities grew and the number of networked devices grew rapidly. Partly because it allowed for multi-vendor interoperability, and could route packets globally rather than being restricted to a single building, the Internet protocol suite became almost universally adopted in network architectures. Thereafter, computer operating systems and the firmware of network devices tended to support Internet protocols.\nNetwork device operating systems.\nNetwork operating systems can be embedded in a router or hardware firewall that operates the functions in the network layer (layer 3). Notable network operating systems include:\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41413", "revid": "1934512", "url": "https://en.wikipedia.org/wiki?curid=41413", "title": "Network topology", "text": "Arrangement of a communication network\nNetwork topology is the arrangement of the elements (links, nodes, etc.) of a communication network. Network topology can be used to define or describe the arrangement of various types of telecommunication networks, including command and control radio networks, industrial fieldbusses and computer networks.\nNetwork topology is the topological structure of a network and may be depicted physically or logically. It is an application of graph theory wherein communicating devices are modeled as nodes and the connections between the devices are modeled as links or lines between the nodes. Physical topology is the placement of the various components of a network (e.g., device location and cable installation), while logical topology illustrates how data flows within a network. Distances between nodes, physical interconnections, transmission rates, or signal types may differ between two different networks, yet their logical topologies may be identical. A network's physical topology is a particular concern of the physical layer of the OSI model. \nExamples of network topologies are found in local area networks (LAN), a common computer network installation. Any given node in the LAN has one or more physical links to other devices in the network; graphically mapping these links results in a geometric shape that can be used to describe the physical topology of the network. A wide variety of physical topologies have been used in LANs, including ring, bus, mesh and star. Conversely, mapping the data flow between the components determines the logical topology of the network. In comparison, Controller Area Networks, common in vehicles, are primarily distributed control system networks of one or more controllers interconnected with sensors and actuators over, invariably, a physical bus topology.\nTopologies.\nTwo basic categories of network topologies exist: physical topologies and logical topologies.\nThe transmission medium layout used to link devices is the physical topology of the network. For conductive or fiber optical mediums, this refers to the layout of cabling, the locations of nodes, and the links between the nodes and the cabling. The physical topology of a network is determined by the capabilities of the network access devices and media, the level of control or fault tolerance desired, and the cost associated with cabling or telecommunication circuits.\nIn contrast, logical topology is the way that the signals act on the network media, or the way that the data passes through the network from one device to the next without regard to the physical interconnection of the devices. A network's logical topology is not necessarily the same as its physical topology. For example, the original twisted pair Ethernet using repeater hubs was a logical bus topology carried on a physical star topology. Token Ring is a logical ring topology, but is wired as a physical star from the media access unit. Physically, Avionics Full-Duplex Switched Ethernet (AFDX) can be a cascaded star topology of multiple dual redundant Ethernet switches; however, the AFDX virtual links are modeled as time-switched single-transmitter bus connections, thus following the safety model of a single-transmitter bus topology previously used in aircraft. Logical topologies are often closely associated with media access control methods and protocols. Some networks are able to dynamically change their logical topology through configuration changes to their routers and switches.\nLinks.\nThe transmission media (often referred to in the literature as the \"physical media\") used to link devices to form a computer network include electrical cables (Ethernet, HomePNA, power line communication, G.hn), optical fiber (fiber-optic communication), and radio waves (wireless networking). In the OSI model, these are defined at layers 1 and 2 \u2014 the physical layer and the data link layer.\nA widely adopted \"family\" of transmission media used in local area network (LAN) technology is collectively known as Ethernet. The media and protocol standards that enable communication between networked devices over Ethernet are defined by IEEE 802.3. Ethernet transmits data over both copper and fiber cables. Wireless LAN standards (e.g., those defined by IEEE 802.11) use radio waves, or others use infrared signals as a transmission medium. Power line communication uses a building's power cabling to transmit data.\nWired technologies.\nThe orders of the following wired technologies are, roughly, from slowest to fastest transmission speed.\nPrice is a main factor distinguishing wired and wireless technology options in a business. Wireless options command a price premium that can make purchasing wired computers, printers and other devices a financial benefit. Before making the decision to purchase hard-wired technology products, a review of the restrictions and limitations of the selections is necessary. Business and employee needs may override any cost considerations.\nExotic technologies.\nThere have been various attempts at transporting data over exotic media:\nBoth cases have a large round-trip delay time, which gives slow two-way communication, but does not prevent sending large amounts of information.\nNodes.\nNetwork nodes are the points of connection of the transmission medium to transmitters and receivers of the electrical, optical, or radio signals carried in the medium. Nodes may be associated with a computer, but certain types may have only a microcontroller at a node or possibly no programmable device at all. In the simplest of serial arrangements, one RS-232 transmitter can be connected by a pair of wires to one receiver, forming two nodes on one link, or a Point-to-Point topology. Some protocols permit a single node to only either transmit or receive (e.g., ARINC 429). Other protocols have nodes that can both transmit and receive into a single channel (e.g., CAN can have many transceivers connected to a single bus). While the conventional system building blocks of a computer network include network interface controllers (NICs), repeaters, hubs, bridges, switches, routers, modems, gateways, and firewalls, most address network concerns beyond the physical network topology and may be represented as single nodes on a particular physical network topology.\nNetwork interfaces.\nA network interface controller (NIC) is computer hardware that provides a computer with the ability to access the transmission media, and has the ability to process low-level network information. For example, the NIC may have a connector for accepting a cable, or an aerial for wireless transmission and reception, and the associated circuitry.\nThe NIC responds to traffic addressed to a network address for either the NIC or the computer as a whole.\nIn Ethernet networks, each network interface controller has a unique Media Access Control (MAC) address\u2014usually stored in the controller's permanent memory. To avoid address conflicts between network devices, the Institute of Electrical and Electronics Engineers (IEEE) maintains and administers MAC address uniqueness. The size of an Ethernet MAC address is six octets. The three most significant octets are reserved to identify NIC manufacturers. These manufacturers, using only their assigned prefixes, uniquely assign the three least-significant octets of every Ethernet interface they produce.\nRepeaters and hubs.\nA repeater is an electronic device that receives a network signal, cleans it of unnecessary noise and regenerates it. The signal may be reformed or retransmitted at a higher power level, to the other side of an obstruction, possibly using a different transmission medium, so that the signal can cover longer distances without degradation. Commercial repeaters have extended RS-232 segments from 15 meters to over a kilometer. In most twisted pair Ethernet configurations, repeaters are required for cable that runs longer than 100 meters. With fiber optics, repeaters can be tens or even hundreds of kilometers apart.\nRepeaters work within the physical layer of the OSI model, that is, there is no end-to-end change in the physical protocol across the repeater, or repeater pair, even if a different physical layer may be used between the ends of the repeater, or repeater pair. Repeaters require a small amount of time to regenerate the signal. This can cause a propagation delay that affects network performance and may affect proper function. As a result, many network architectures limit the number of repeaters that can be used in a row, e.g., the Ethernet 5-4-3 rule.\nA repeater with multiple ports is known as hub, an Ethernet hub in Ethernet networks, a USB hub in USB networks.\nBridges.\nA network bridge connects and filters traffic between two network segments at the data link layer (layer 2) of the OSI model to form a single network. This breaks the network's collision domain but maintains a unified broadcast domain. Network segmentation breaks down a large, congested network into an aggregation of smaller, more efficient networks.\nBridges come in three basic types:\nSwitches.\nA network switch is a device that forwards and filters OSI layer 2 datagrams (frames) between ports based on the destination MAC address in each frame.\nA switch is distinct from a hub in that it only forwards the frames to the physical ports involved in the communication rather than all ports connected. It can be thought of as a multi-port bridge. It learns to associate physical ports to MAC addresses by examining the source addresses of received frames. If an unknown destination is targeted, the switch broadcasts to all ports but the source. Switches normally have numerous ports, facilitating a star topology for devices and cascading additional switches.\nMulti-layer switches are capable of routing based on layer 3 addressing or additional logical levels. The term \"switch\" is often used loosely to include devices such as routers and bridges, as well as devices that may distribute traffic based on load or based on application content (e.g., a Web URL identifier).\nRouters.\nA router is an internetworking device that forwards packets between networks by processing the routing information included in the packet or datagram (Internet protocol information from layer 3). The routing information is often processed in conjunction with the routing table (or forwarding table). A router uses its routing table to determine where to forward packets. A destination in a routing table can include a black hole because data can go into it; however, no further processing is done for said data, i.e., the packets are dropped.\nModems.\nModems (MOdulator-DEModulator) are used to connect network nodes via wire not originally designed for digital network traffic, or for wireless. To do this, one or more carrier signals are modulated by the digital signal to produce an analog signal that can be tailored to give the required properties for transmission. Modems are commonly used for telephone lines, using a digital subscriber line technology.\nFirewalls.\nA firewall is a network device for controlling network security and access rules. Firewalls are typically configured to reject access requests from unrecognized sources while allowing actions from recognized ones. The vital role firewalls play in network security grows in parallel with the constant increase in cyber attacks.\nClassification.\nThe study of network topology recognizes eight basic topologies: point-to-point, bus, star, ring or circular, mesh, tree, hybrid, or daisy chain.\nPoint-to-point.\nThe simplest topology with a dedicated link between two endpoints. Easiest to understand of the variations of point-to-point topology is a point-to-point communication channel that appears, to the user, to be permanently associated with the two endpoints. A child's tin can telephone is one example of a \"physical dedicated\" channel.\nUsing circuit-switching or packet-switching technologies, a point-to-point circuit can be set up dynamically and dropped when no longer needed. Switched point-to-point topologies are the basic model of conventional telephony.\nThe value of a permanent point-to-point network is unimpeded communications between the two endpoints. The value of an on-demand point-to-point connection is proportional to the number of potential pairs of subscribers and has been expressed as Metcalfe's Law.\nDaisy chain.\nDaisy chaining is accomplished by connecting each computer in series to the next. If a message is intended for a computer partway down the line, each system bounces it along in sequence until it reaches the destination. A daisy-chained network can take two basic forms: linear and ring.\nBus.\nIn local area networks using bus topology, each node is connected by interface connectors to a single central cable. This is the 'bus', also referred to as the backbone, or trunk\u00a0\u2013 all data transmission between nodes in the network is transmitted over this common transmission medium and is able to be received by all nodes in the network simultaneously.\nA signal containing the address of the intended receiving machine travels from a source machine in both directions to all machines connected to the bus until it finds the intended recipient, which then accepts the data. If the machine address does not match the intended address for the data, the data portion of the signal is ignored. Since the bus topology consists of only one wire it is less expensive to implement than other topologies, but the savings are offset by the higher cost of managing the network. Additionally, since the network is dependent on the single cable, it can be the single point of failure of the network. In this topology data being transferred may be accessed by any node.\nLinear bus.\nIn a linear bus network, all of the nodes of the network are connected to a common transmission medium, which has just two endpoints. When the electrical signal reaches the end of the bus, the signal is reflected back down the line, causing unwanted interference. To prevent this, the two endpoints of the bus are normally terminated with a device called a terminator.\nDistributed bus.\nIn a distributed bus network, all of the nodes of the network are connected to a common transmission medium with more than two endpoints, created by adding branches to the main section of the transmission medium\u00a0\u2013 the physical distributed bus topology functions in exactly the same fashion as the physical linear bus topology because all nodes share a common transmission medium.\nStar.\nIn star topology (also called hub-and-spoke), every peripheral node (computer workstation or any other peripheral) is connected to a central node called a hub or switch. The hub is the server and the peripherals are the clients. The network does not necessarily have to resemble a star to be classified as a star network, but all of the peripheral nodes on the network must be connected to one central hub. All traffic that traverses the network passes through the central hub, which acts as a signal repeater. \nThe star topology is considered the easiest topology to design and implement. One advantage of the star topology is the simplicity of adding additional nodes. The primary disadvantage of the star topology is that the hub represents a single point of failure. Also, since all peripheral communication must flow through the central hub, the aggregate central bandwidth forms a network bottleneck for large clusters.\nExtended star.\nThe extended star network topology extends a physical star topology by one or more repeaters between the central node and the peripheral (or 'spoke') nodes. The repeaters are used to extend the maximum transmission distance of the physical layer, the point-to-point distance between the central node and the peripheral nodes. Repeaters allow greater transmission distance, further than would be possible using just the transmitting power of the central node. The use of repeaters can also overcome limitations from the standard upon which the physical layer is based.\nA physical extended star topology in which repeaters are replaced with hubs or switches is a type of hybrid network topology and is referred to as a physical hierarchical star topology, although some texts make no distinction between the two topologies.\nA physical hierarchical star topology can also be referred as a tier-star topology. This topology differs from a tree topology in the way star networks are connected together. A tier-star topology uses a central node, while a tree topology uses a central bus and can also be referred as a star-bus network.\nDistributed star.\nA distributed star is a network topology that is composed of individual networks that are based upon the physical star topology connected in a linear fashion\u00a0\u2013 i.e., daisy-chained\u00a0\u2013 with no central or top level connection point (e.g., two or more \"stacked\" hubs, along with their associated star connected nodes or \"spokes\").\nRing.\nA ring topology is a daisy chain in a closed loop. Data travels around the ring in one direction. When one node sends data to another, the data passes through each intermediate node on the ring until it reaches its destination. The intermediate nodes repeat (retransmit) the data to keep the signal strong. Every node is a peer; there is no hierarchical relationship of clients and servers. If one node is unable to retransmit data, it severes communication between the nodes before and after it in the bus.\nAdvantages:\nDisadvantages:\nMesh.\nThe value of fully meshed networks is proportional to the exponent of the number of subscribers, assuming that communicating groups of any two endpoints, up to and including all the endpoints, is approximated by Reed's Law.\nFully connected network.\nIn a \"fully connected network\", all nodes are interconnected. (In graph theory this is called a complete graph.) The simplest fully connected network is a two-node network. A fully connected network doesn't need to use packet switching or broadcasting. However, since the number of connections grows quadratically with the number of nodes:\nformula_1\nThis makes it impractical for large networks. This kind of topology does not trip and affect other nodes in the network.\nPartially connected network.\nIn a partially connected network, certain nodes are connected to exactly one other node, but some nodes are connected to two or more other nodes with a point-to-point link. This makes it possible to make use of some of the redundancy of mesh topology that is physically fully connected, without the expense and complexity required for a connection between every node in the network.\nHybrid.\nHybrid topology is also known as hybrid network. Hybrid networks combine two or more topologies in such a way that the resulting network does not exhibit one of the standard topologies (e.g., bus, star, ring, etc.). For example, a tree network (or \"star-bus network\") is a hybrid topology in which star networks are interconnected via bus networks. However, a tree network connected to another tree network is still topologically a tree network, not a distinct network type. A hybrid topology is always produced when two different basic network topologies are connected.\nA \"star-ring\" network consists of two or more ring networks connected using a multistation access unit (MAU) as a centralized hub.\n\"Snowflake\" topology is meshed at the core, but tree shaped at the edges.\nTwo other hybrid network types are \"hybrid mesh\" and \"hierarchical star\".\nCentralization.\nThe star topology reduces the probability of a network failure by connecting all of the peripheral nodes (computers, etc.) to a central node. When the physical star topology is applied to a logical bus network such as Ethernet, this central node (traditionally a hub) rebroadcasts all transmissions received from any peripheral node to all peripheral nodes on the network, sometimes including the originating node. All peripheral nodes may thus communicate with all others by transmitting to, and receiving from, the central node only. The failure of a transmission line linking any peripheral node to the central node will result in the isolation of that peripheral node from all others, but the remaining peripheral nodes will be unaffected. However, the disadvantage is that the failure of the central node will cause the failure of all of the peripheral nodes.\nIf the central node is \"passive\", the originating node must be able to tolerate the reception of an echo of its own transmission, delayed by the two-way round trip transmission time (i.e. to and from the central node) plus any delay generated in the central node. An \"active\" star network has an active central node that usually has the means to prevent echo-related problems.\nA tree topology (a.k.a. hierarchical topology) can be viewed as a collection of star networks arranged in a hierarchy. This tree structure has individual peripheral nodes (e.g., leaves) which are required to transmit to and receive from one other node only and are not required to act as repeaters or regenerators. Unlike the star network, the functionality of the central node may be distributed.\nAs in the conventional star network, individual nodes may thus still be isolated from the network by a single-point failure of a transmission path to the node. If a link connecting a leaf fails, that leaf is isolated; if a connection to a non-leaf node fails, an entire section of the network becomes isolated from the rest.\nTo alleviate the amount of network traffic that comes from broadcasting all signals to all nodes, more advanced central nodes were developed that are able to keep track of the identities of the nodes that are connected to the network. These network switches will \"learn\" the layout of the network by \"listening\" on each port during normal data transmission, examining the data packets and recording the address/identifier of each connected node and which port it is connected to in a lookup table held in memory. This lookup table then allows future transmissions to be forwarded to the intended destination only.\nDaisy chain topology is a way of connecting network nodes in a linear or ring structure. It is used to transmit messages from one node to the next until they reach the destination node.\nA daisy chain network can have two types: linear and ring. A linear daisy chain network is like an electrical series, where the first and last nodes are not connected. A ring daisy chain network is where the first and last nodes are connected, forming a loop.\nDecentralization.\nIn a partially connected mesh topology, there are at least two nodes with two or more paths between them to provide redundant paths in case the link providing one of the paths fails. Decentralization is often used to compensate for the single-point-failure disadvantage that is present when using a single device as a central node (e.g., in star and tree networks). A special kind of mesh, limiting the number of hops between two nodes, is a hypercube. The number of arbitrary forks in mesh networks makes them more difficult to design and implement, but their decentralized nature makes them very useful.\nThis is similar in some ways to a grid network, where a linear or ring topology is used to connect systems in multiple directions. A multidimensional ring has a toroidal topology, for instance.\nA \"fully connected network\", \"complete topology\", or \"full mesh topology\" is a network topology in which there is a direct link between all pairs of nodes. In a fully connected network with n nodes, there are formula_2 direct links. Networks designed with this topology are usually very expensive to set up, but provide a high degree of reliability due to the multiple paths for data that are provided by the large number of redundant links between nodes. This topology is mostly seen in military applications.\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41414", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41414", "title": "Neutral direct-current telegraph system", "text": "In telecommunications, a neutral direct-current telegraph system (\"single-current system\", \"single-current transmission system\", \"single-Morse system\") is a telegraph system in which (a) current flows during marking intervals and no current flows during spacing intervals for the transmission of signals over a line, and (b) the direction of current flow (electric polarity) is immaterial.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41415", "revid": "49409394", "url": "https://en.wikipedia.org/wiki?curid=41415", "title": "Noise", "text": "Unwanted sound\nNoise is sound, chiefly unwanted, unintentional, or harmful sound considered unpleasant, loud, or disruptive to mental or hearing faculties. From a physics standpoint, there is no distinction between noise and desired sound, as both are vibrations through a medium, such as air or water. The difference arises when the brain receives and perceives a sound. Acoustic noise is any sound in the acoustic domain, either deliberate (e.g., music or speech) or unintended. \nNoise may also refer to a random or unintended component of an electronic signal, whose effects may not be audible to the human ear and may require instruments for detection. It can also refer to an intentionally produced random signal or spectral noise, such as white noise or pink noise.\nIn audio engineering, noise can refer to the unwanted residual electronic noise signal that gives rise to acoustic noise heard as a hiss. This signal noise is commonly measured using A-weighting or ITU-R 468 weighting. In experimental sciences, noise can refer to any random fluctuations of data that hinders perception of a signal.\nMeasurement.\nSound is measured based on the amplitude and frequency of a sound wave. Amplitude measures how forceful the wave is. The energy in a sound wave is measured in decibels (dB), the measure of loudness, or intensity of a sound; this measurement describes the amplitude of a sound wave. Decibels are expressed in a logarithmic scale. On the other hand, pitch describes the frequency of a sound and is measured in hertz (Hz).\nThe main instrument to measure sounds in the air is the Sound Level Meter. There are many different varieties of instruments that are used to measure noise - Noise Dosimeters are often used in occupational environments, noise monitors are used to measure environmental noise and noise pollution, and recently smartphone-based sound level meter applications (apps) are being used to crowdsource and map recreational and community noise.\nA-weighting is applied to a sound spectrum to represent the sound that humans are capable of hearing at each frequency. Sound pressure is thus expressed in terms of dBA. 0 dBA is the softest level that a person can hear. Normal speaking voices are around 65 dBA. A rock concert can be about 120 dBA.\nRecording and reproduction.\nIn audio, recording, and broadcast systems, audio noise refers to the residual low-level sound (four major types: hiss, rumble, crackle, and hum) that is heard in quiet periods of program. This variation from the expected pure sound or silence can be caused by the audio recording equipment, the instrument, or ambient noise in the recording room.\nIn audio engineering it can refer either to the acoustic noise from loudspeakers or to the unwanted residual electronic noise signal that gives rise to acoustic noise heard as hiss. This signal noise is commonly measured using A-weighting or ITU-R 468 weighting\nNoise is often generated deliberately and used as a test signal for audio recording and reproduction equipment.\nEnvironmental noise.\nEnvironmental noise is the accumulation of all noise present in a specified environment. The principal sources of environmental noise are surface motor vehicles, aircraft, trains and industrial sources. These noise sources expose millions of people to noise pollution that creates not only annoyance, but also significant health consequences such as elevated incidence of hearing loss, cardiovascular disease, and many others. Urban noise is generally not of an intensity that causes hearing loss but it interrupts sleep, disturbs communication and interferes with other human activities. There are a variety of mitigation strategies and controls available to reduce sound levels including source intensity reduction, land-use planning strategies, noise barriers and sound baffles, time of day use regimens, vehicle operational controls and architectural acoustics design measures.\nRegulation.\nCertain geographic areas or specific occupations may be at a higher risk of being exposed to constantly high levels of noise; regulation may prevent negative health outcomes. Noise regulation includes statutes or guidelines relating to sound transmission established by national, state or provincial and municipal levels of government. Environmental noise is governed by laws and standards which set maximum recommended levels of noise for specific land uses, such as residential areas, areas of outstanding natural beauty, or schools. These standards usually specify measurement using a weighting filter, most often A-weighting.\nUnited States.\nIn 1972, the Noise Control Act was passed to promote a healthy living environment for all Americans, where noise does not pose a threat to human health. This policy's main objectives were: (1) establish coordination of research in the area of noise control, (2) establish federal standards on noise emission for commercial products, and (3) promote public awareness about noise emission and reduction.\nThe Quiet Communities Act of 1978 promotes noise control programs at the state and local level and developed a research program on noise control. Both laws authorized the Environmental Protection Agency to study the effects of noise and evaluate regulations regarding noise control.\nThe National Institute for Occupational Safety and Health (NIOSH) provides recommendation on noise exposure in the workplace. In 1972 (revised in 1998), NIOSH published a document outlining recommended standards relating to the occupational exposure to noise, with the purpose of reducing the risk of developing permanent hearing loss related to exposure at work. This publication set the recommended exposure limit (REL) of noise in an occupation setting to 85 dBA for 8 hours using a 3-dB exchange rate (every 3-dB increase in level, duration of exposure should be cut in half, i.e., 88 dBA for 4 hours, 91 dBA for 2 hours, 94 dBA for 1 hour, etc.). However, in 1973 the Occupational Safety and Health Administration (OSHA) maintained the requirement of an 8-hour average of 90 dBA. The following year, OSHA required employers to provide a hearing conservation program to workers exposed to 85 dBA average 8-hour workdays.\nEurope.\nThe European Environment Agency regulates noise control and surveillance within the European Union. The Environmental Noise Directive was set to determine levels of noise exposure, increase public access to information regarding environmental noise, and reduce environmental noise. Additionally, in the European Union, underwater noise is a pollutant according to the Marine Strategy Framework Directive (MSFD). The MSFD requires EU Member States to achieve or maintain Good Environmental Status, meaning that the \"introduction of energy, including underwater noise, is at levels that do not adversely affect the marine environment\".\nHealth effects.\nExposure to noise is associated with several negative health outcomes. Depending on duration and level of exposure, noise may cause or increase the likelihood of hearing loss, high blood pressure, ischemic heart disease, sleep disturbances, injuries, and even decreased school performance. When noise is prolonged, the body's stress responses can be triggered; which can include increased heartbeat, and rapid breathing. There are also causal relationships between noise and psychological effects such as annoyance, psychiatric disorders, and effects on psychosocial well-being.\nNoise exposure has increasingly been identified as a public health issue, especially in an occupational setting, as demonstrated with the creation of NIOSH's Noise and Hearing Loss Prevention program. Noise has also proven to be an occupational hazard, as it is the most common work-related pollutant. Noise-induced hearing loss, when associated with noise exposure at the workplace is also called occupational hearing loss. For example, some occupational studies have shown a relation between those who are regularly exposed to noise above 85 decibels to have higher blood pressure than those who are not exposed.\nHearing loss prevention.\nWhile noise-induced hearing loss is permanent, it is also preventable. Particularly in the workplace, regulations may exist limiting permissible exposure limit to noise. This can be especially important for professionals working in settings with consistent exposure to loud sounds, such as musicians, music teachers and audio engineers. Examples of measures taken to prevent noise-induced hearing loss in the workplace include engineering noise control, the Buy-Quiet initiative, creation of the Safe-In-Sound award, and noise surveillance.\nOSHA requires the use of hearing protection. But the HPD (without individual selection, training and fit testing) does not significantly reduce the risk of hearing loss. For example, one study covered more than 19 thousand workers, some of whom usually used hearing protective devices, and some did not use them at all. There was no statistically significant difference in the risk of noise-induced hearing loss.\nLiterary views.\nRoland Barthes distinguishes between physiological noise, which is merely heard, and psychological noise, which is actively listened to. Physiological noise is felt subconsciously as the vibrations of the noise (sound) waves physically interact with the body while psychological noise is perceived as our conscious awareness shifts its attention to that noise.\nLuigi Russolo, one of the first composers of noise music, wrote the essay \"The Art of Noises.\" He argued that any kind of noise could be used as music, as audiences become more familiar with noises caused by technological advancements; noise has become so prominent that pure sound no longer exists.\nAvant-garde composer Henry Cowell claimed that technological advancements have reduced unwanted noises from machines, but have not managed so far to eliminate them.\nFelix Urban sees noise as a result of cultural circumstances. In his comparative study on sound and noise in cities, he points out that noise regulations are only one indicator of what is considered as harmful. It is the way in which people live and behave (acoustically) that determines the way how sounds are perceived.\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41416", "revid": "44487672", "url": "https://en.wikipedia.org/wiki?curid=41416", "title": "Noise-equivalent power", "text": "Measure of the sensitivity of a photodetector\nNoise-equivalent power (NEP) is a measure of the sensitivity of a photodetector or detector system. It is defined as the signal power that gives a signal-to-noise ratio of one in a one hertz output bandwidth. An output bandwidth of one hertz is equivalent to half a second of integration time. The units of NEP are watts per square root hertz. The NEP is equal to the noise amplitude spectral density (expressed in units of formula_1 or formula_2) divided by the responsivity (expressed in units of formula_3 or formula_4, respectively). The fundamental equation is formula_5.\nA smaller NEP corresponds to a more sensitive detector. For example, a detector with an NEP of formula_6 can detect a signal power of one picowatt with a signal-to-noise ratio (SNR) of one after one half second of averaging. The SNR improves as the square root of the averaging time, and hence the SNR in this example can be improved by a factor of 10 by averaging 100-times longer, i.e. for 50 seconds.\nIf the NEP refers to the signal power absorbed in the detector, it is known as the electrical NEP. If instead it refers to the signal power incident on the detector system, it is called the optical NEP. The optical NEP is equal to the electrical NEP divided by the optical coupling efficiency of the detector system. "}
{"id": "41417", "revid": "3125232", "url": "https://en.wikipedia.org/wiki?curid=41417", "title": "Noise figure", "text": "Number used to evaluate the performance of an amplifier\nNoise figure (NF) and noise factor (\"F\") are figures of merit that indicate degradation of the signal-to-noise ratio (SNR) that is caused by components in a signal chain. These figures of merit are used to evaluate the performance of an amplifier or a radio receiver, with lower values indicating better performance.\nThe noise factor is defined as the ratio of the output noise power of a device to the portion thereof attributable to thermal noise in the input termination at standard noise temperature \"T\"0 (usually 290\u00a0K). The noise factor is thus the ratio of actual output noise to that which would remain if the device itself did not introduce noise, which is equivalent to the ratio of input SNR to output SNR.\nThe noise \"factor\" and noise \"figure\" are related, with the former being a unitless ratio and the latter being the logarithm of the noise factor, expressed in units of decibels (dB).\nGeneral.\nThe noise figure is the difference in decibel (dB) between the noise output of the actual receiver to the noise output of an \"ideal\" receiver with the same overall gain and bandwidth when the receivers are connected to matched sources at the standard noise temperature \"T\"0 (usually 290\u00a0K). The noise power from a simple load is equal to \"kTB\", where \"k\" is the Boltzmann constant, \"T\" is the absolute temperature of the load (for example a resistor), and \"B\" is the measurement bandwidth.\nThis makes the noise figure a useful figure of merit for terrestrial systems, where the antenna effective temperature is usually near the standard 290\u00a0K. In this case, one receiver with a noise figure, say 2\u00a0dB better than another, will have an output signal-to-noise ratio that is about 2\u00a0dB better than the other. However, in the case of satellite communications systems, where the receiver antenna is pointed out into cold space, the antenna effective temperature is often colder than 290\u00a0K. In these cases a 2\u00a0dB improvement in receiver noise figure will result in more than a 2\u00a0dB improvement in the output signal-to-noise ratio. For this reason, the related figure of \"effective noise temperature\" is therefore often used instead of the noise figure for characterizing satellite-communication receivers and low-noise amplifiers.\nIn heterodyne systems, output noise power includes spurious contributions from image-frequency transformation, but the portion attributable to thermal noise in the input termination at standard noise temperature includes only that which appears in the output via the principal frequency transformation of the system and excludes that which appears via the image frequency transformation.\nDefinition.\nThe noise factor \"F\" of a system is defined as\nformula_1\nwhere SNRi and SNRo are the input and output signal-to-noise ratios respectively. The SNR quantities are unitless power ratios. Note that this specific definition is only valid for an input signal of which the noise is \"Ni=kT0B\".\nThe noise figure NF is defined as the noise factor in units of decibels (dB):\nformula_2\nwhere SNRi,\u00a0dB and SNRo,\u00a0dB are in units of (dB).\nThese formulae are only valid when the input termination is at standard noise temperature \"T\"0 = 290\u00a0K, although in practice small differences in temperature do not significantly affect the values.\nThe noise factor of a device is related to its noise temperature \"T\"e:\nformula_3\nAttenuators have a noise factor \"F\" equal to their attenuation ratio \"L\" when their physical temperature equals \"T\"0. More generally, for an attenuator at a physical temperature \"T\", the noise temperature is \"T\"e = (\"L\" \u2212 1)\"T\", giving a noise factor\nformula_4\nNoise factor of cascaded devices.\nIf several devices are cascaded, the total noise factor can be found with Friis' formula:\nformula_5\nwhere \"F\"\"n\" is the noise factor for the \"n\"-th device, and \"G\"\"n\" is the power gain (linear, not in dB) of the \"n\"-th device. The first amplifier in a chain usually has the most significant effect on the total noise figure because the noise figures of the following stages are reduced by stage gains. Consequently, the first amplifier usually has a low noise figure, and the noise figure requirements of subsequent stages is usually more relaxed.\nNoise factor as a function of additional noise.\nThe noise factor may be expressed as a function of the additional output referred noise power formula_6 and the power gain formula_7 of an amplifier.\nformula_8\nDerivation.\nFrom the definition of noise factor\nformula_9\nand assuming a system which has a noisy single stage amplifier. The signal to noise ratio of this amplifier would include its own output referred noise formula_6, the amplified signal formula_11 and the amplified input noise formula_12,\nformula_13\nSubstituting the output SNR to the noise factor definition,\nformula_14\nIn cascaded systems formula_15 does not refer to the output noise of the previous component. An input termination at the standard noise temperature is still assumed for the individual component. This means that the additional noise power added by each component is independent of the other components.\nOptical noise figure.\nThe above describes noise in electrical systems. The optical noise figure is discussed in multiple sources. Electric sources generate noise with a power spectral density, or energy per mode, equal to \"kT\", where \"k\" is the Boltzmann constant and \"T\" is the absolute temperature. One mode has two quadratures, i.e. the amplitudes of cosformula_16 and sinformula_16 oscillations of voltages, currents or fields. However, there is also noise in optical systems. In these, the sources have no fundamental noise. Instead the energy quantization causes notable shot noise in the detector. In an optical receiver which can output one available mode or two available quadratures this corresponds to a noise power spectral density, or energy per mode, of \"hf\" where \"h\" is the Planck constant and \"f\" is the optical frequency. In an optical receiver with only one available quadrature the shot noise has a power spectral density, or energy per mode, of only \"hf\"/2. \nIn the 1990s, an optical noise figure has been defined. This has been called \"F\"\"pnf\" for \"p\"hoton \"n\"umber \"f\"luctuations. The powers needed for SNR and noise factor calculation are the electrical powers caused by the current in a photodiode. SNR is the square of mean photocurrent divided by variance of photocurrent. Monochromatic or sufficiently attenuated light has a Poisson distribution of detected photons. If, during a detection interval the expectation value of detected photons is \"n\" then the variance is also \"n\" and one obtains \"SNR\"\"pnf,in\" = \"n\"2/\"n\" = \"n\". Behind an optical amplifier with power gain \"G\" there will be a mean of \"Gn\" detectable signal photons. In the limit of large \"n\" the variance of photons is \"Gn\"(2\"n\"\"sp\"(\"G\"-1)+1) where \"n\"\"sp\" is the spontaneous emission factor. One obtains \"SNR\"\"pnf,out\" = \"G\"2\"n\"2/(\"Gn\"(2\"n\"\"sp\"(\"G\"-1)+1)) = \"n\"/(2\"n\"\"sp\"(1-1/\"G\")+1/\"G\"). Resulting optical noise factor is \"F\"\"pnf\" = \"SNR\"\"pnf,in\" / \"SNR\"\"pnf,out\" = 2\"n\"\"sp\"(1-1/\"G\")+1/\"G\".\n\"F\"\"pnf\" is in conceptual conflict with the \"e\"lectrical noise factor, which is now called \"F\"\"e\":\nPhotocurrent \"I\" is proportional to optical power \"P\". \"P\" is proportional to squares of a field amplitude (electric or magnetic). So, the receiver is nonlinear in amplitude. The \"Power\" needed for \"SNR\"\"pnf\" calculation is proportional to the 4th power of the signal amplitude. But for \"F\"\"e\" in the electrical domain the power is proportional to the square of the signal amplitude.\nIf \"SNR\"\"pnf\" is a noise factor then its definition must be independent of measurement apparatus and frequency. Consider the signal \"Power\" in the sense of \"SNR\"\"pnf\" definition. Behind an amplifier it is proportional to \"G\"2\"n\"2. We may replace the photodiode by a thermal power meter, and measured photocurrent \"I\" by measured temperature change formula_18. \"Power\", being proportional to \"I\"2 or \"P\"2, is also proportional to formula_192. Thermal power meters can be built at all frequencies. Hence it is possible to lower the frequency from optical (say 200 THz) to electrical (say 200\u00a0MHz). Still there, \"Power\" must be proportional to formula_192 or \"P\"2. Electrical power \"P\" is proportional to the square \"U\"2 of voltage \"U\". But \"Power\" is proportional to \"U\"4.\nThese implications are in obvious conflict with ~150 years of physics. They are compelling consequence of calling \"F\"\"pnf\" a noise factor, or noise figure when expressed in dB.\nAt any given electrical frequency, noise occurs in both quadratures, i.e. in phase (I) and in quadrature (Q) with the signal. Both these quadratures are available behind the electrical amplifier. The same holds in an optical amplifier. But the direct detection photoreceiver needed for measurement of \"SNR\"\"pnf\" takes mainly the in-phase noise into account whereas quadrature noise can be neglected for high \"n\". Also, the receiver outputs only one baseband signal, corresponding to quadrature. So, one quadrature or degree-of-freedom is lost.\nFor an optical amplifier with large \"G\" it holds \"F\"\"pnf\" \u2265 2 whereas for an \"e\"lectrical amplifier it holds \"F\"\"e\" \u2265 1.\nMoreover, today's long-haul optical fiber communication is dominated by coherent optical I&amp;Q receivers but \"F\"\"pnf\" does not describe the SNR degradation observed in these.\nAnother optical noise figure \"F\"\"ase\" for \"a\"mplified \"s\"pontaneous \"e\"mission has been defined. But the noise factor \"F\"\"ase\" is not the SNR degradation factor in any optical receiver.\nAll the above conflicts are resolved by the optical in-phase and quadrature noise factor and figure \"F\"\"o,IQ\". It can be measured using a coherent optical I&amp;Q receiver. In these, power of the output signal is proportional to the square of an optical field amplitude because they are linear in amplitude. They pass both quadratures. For an optical amplifier it holds \"F\"\"o,IQ\" = \"n\"\"sp\"(1-1/\"G\")+1/\"G\" \u2265 1. Quantity \"n\"\"sp\"(1-1/\"G\") is the input-referred number of added noise photons per mode.\n\"F\"\"o,IQ\" and \"F\"\"pnf\" can easily be converted into each other. For large \"G\" it holds \"F\"\"o,IQ\" = \"F\"\"pnf\"/2 or, when expressed in dB, \"F\"\"o,IQ\" is 3 dB less than \"F\"\"pnf\". The ideal \"F\"\"o,IQ\" in dB equals 0 dB. This describes the known fact that the sensitivity of an ideal optical I&amp;Q receiver is not improved by an ideal optical preamplifier.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nExternal links.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41418", "revid": "45032371", "url": "https://en.wikipedia.org/wiki?curid=41418", "title": "Noise level", "text": "The noise level is the level of noise. Specifically, it may refer to:\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "41419", "revid": "84591", "url": "https://en.wikipedia.org/wiki?curid=41419", "title": "Noise power", "text": "In telecommunications, the term noise power has the following meanings: \nNoise power can be calculated by multiplying the noise spectral density with the signal bandwidth\nformula_1\nwhere:\nReferences.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41420", "revid": "1217304245", "url": "https://en.wikipedia.org/wiki?curid=41420", "title": "Noise temperature", "text": "Level of noise power introduced by a component or source\nIn electronics, noise temperature is one way of expressing the level of available noise power introduced by a component or source. The power spectral density of the noise is expressed in terms of the temperature (in kelvins) that would produce that level of Johnson\u2013Nyquist noise, thus:\nformula_1\nwhere:\nThus the noise temperature is proportional to the power spectral density of the noise, formula_6. That is the power that would be absorbed from the component or source by a matched load. Noise temperature is generally a function of frequency, unlike that of an ideal resistor which is simply equal to the actual temperature of the resistor at all frequencies.\nNoise voltage and current.\nA noisy component may be modelled as a noiseless component in series with a noisy voltage source producing a voltage of \"v\"n, or as a noiseless component in parallel with a noisy current source producing a current of \"i\"n. This equivalent voltage or current corresponds to the above power spectral density formula_7, and would have a mean squared amplitude over a bandwidth B of:\nformula_8\nwhere R is the resistive part of the component's impedance or G is the conductance (real part) of the component's admittance. Speaking of noise temperature therefore offers a fair comparison between components having different impedances rather than specifying the noise voltage and qualifying that number by mentioning the component's resistance. It is also more accessible than speaking of the noise's power spectral density (in watts per hertz) since it is expressed as an ordinary temperature which can be compared to the noise level of an ideal resistor at room temperature (290\u00a0K).\nNote that one can only speak of the noise temperature of a component or source whose impedance has a substantial (and measurable) resistive component. Thus it does not make sense to talk about the noise temperature of a capacitor or of a voltage source. The noise temperature of an amplifier refers to the noise that would be added at the amplifier's \"input\" (relative to the input impedance of the amplifier) in order to account for the added noise observed following amplification.\nSystem noise temperature.\nAn RF receiver system is typically made up of an antenna and a receiver, and the transmission line(s) that connect the two together. Each of these is a source of additive noise. The additive noise in a receiving system can be of thermal origin (thermal noise) or can be from other external or internal noise-generating processes. The contributions of all noise sources are typically lumped together and regarded as a level of thermal noise. The noise power spectral density generated by any source (formula_9) can be described by assigning to the noise a temperature formula_10 as defined above:\nformula_11\nIn an RF receiver, the overall system noise temperature formula_12 equals the sum of the effective noise temperature of the receiver and transmission lines and that of the antenna.\nformula_13\nThe antenna noise temperature formula_14 gives the noise power seen at the output of the antenna. The composite noise temperature of the receiver and transmission line losses formula_15 represents the noise contribution of the rest of the receiver system. It is calculated as the effective noise that would be present at the antenna input terminals if the receiver system were perfect and created no noise. In other words, it is a cascaded system of amplifiers and losses where the internal noise temperatures are referred to the antenna input terminals. Thus, the summation of these two noise temperatures represents the noise input to a \"perfect\" receiver system.\nNoise factor and noise figure.\nOne use of noise temperature is in the definition of a system's noise factor or noise figure. The noise factor specifies the increase in noise power (referred to the input of an amplifier) due to a component or system when its input noise temperature is formula_16.\nformula_17\nformula_18 is customarily taken to be room temperature, 290\u00a0K.\nThe noise factor (a linear term) is more often expressed as the \"noise figure\" (in decibels) using the conversion:\nformula_19\nThe noise figure can also be seen as the decrease in signal-to-noise ratio (SNR) caused by passing a signal through a system if the original signal had a noise temperature of 290\u00a0K. This is a common way of expressing the noise contributed by a radio frequency amplifier regardless of the amplifier's gain. For instance, assume an amplifier has a noise temperature 870\u00a0K and thus a noise figure of 6\u00a0dB. If that amplifier is used to amplify a source having a noise temperature of about room temperature (290\u00a0K), as many sources do, then the insertion of that amplifier would reduce the SNR of a signal by 6\u00a0dB. This simple relationship is frequently applicable where the source's noise is of thermal origin since a passive transducer will often have a noise temperature similar to 290\u00a0K.\nHowever, in many cases the input source's noise temperature is much higher, such as an antenna at lower frequencies where atmospheric noise dominates. Then there will be little degradation of the SNR. On the other hand, a good satellite dish looking through the atmosphere into space (so that it sees a much lower noise temperature) would have the SNR of a signal degraded by \"more\" than 6\u00a0dB. In those cases a reference to the amplifier's noise temperature itself, rather than the noise figure defined according to room temperature, is more appropriate.\nEffective noise temperature.\nThe noise temperature of an amplifier is commonly measured using the Y-factor method. If there are multiple amplifiers in cascade, the noise temperature of the cascade can be calculated using the Friis equation:\nformula_20\nwhere\nTherefore, the amplifier chain can be modelled as a black box having a gain of formula_27 and a noise figure given by formula_28. In the usual case where the gains of the amplifier's stages are much greater than one, then it can be seen that the noise temperatures of the earlier stages have a much greater influence on the resulting noise temperature than those later in the chain. One can appreciate that the noise introduced by the first stage, for instance, is amplified by all of the stages whereas the noise introduced by later stages undergoes lesser amplification. Another way of looking at it is that the signal applied to a later stage already has a high noise level, due to amplification of noise by the previous stages, so that the noise contribution of that stage to that already amplified signal is of less significance.\nThis explains why the quality of a preamplifier or RF amplifier is of particular importance in an amplifier chain. In most cases only the noise figure of the first stage need be considered. However one must check that the noise figure of the second stage is not so high (or that the gain of the first stage is so low) that there is SNR degradation due to the second stage anyway. That will be a concern if the noise figure of the first stage plus that stage's gain (in decibels) is not much greater than the noise figure of the second stage.\nOne corollary of the Friis equation is that an attenuator prior to the first amplifier will degrade the noise figure due to the amplifier. For instance, if stage 1 represents a 6\u00a0dB attenuator so that formula_29, then formula_30. Effectively the noise temperature of the amplifier formula_31 has been quadrupled, in addition to the (smaller) contribution due to the attenuator itself formula_32 (usually room temperature if the attenuator is composed of resistors). An antenna with poor efficiency is an example of this principle, where formula_33 would represent the antenna's efficiency.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41421", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41421", "title": "Noise weighting", "text": "A noise weighting is a specific amplitude-vs.-frequency characteristic that is designed to allow subjectively valid measurement of noise. It emphasises the parts of the spectrum that are most important.\nUsually, noise means audible noise, in audio systems, broadcast systems or telephone circuits. In this case the weighting is sometimes referred to as Psophometric weighting, though this term is best avoided because, although strictly a general term, the word Psophometric is sometimes assumed to refer to a particular weighting used in telecommunications.\nA major use of noise weighting is in the measurement of residual noise in audio equipment, usually present as hiss or hum in quiet moments of programme material. The purpose of weighting here is to emphasise the parts of the audible spectrum that our ears perceive most readily, and attenuate the parts that contribute less to our perception of loudness, in order to get a measured figure that correlates well with subjective effect. \nThe ITU-R 468 noise weighting was devised specifically for this purpose, and is widely used in broadcasting, especially in the UK and Europe. A-weighting is also used, especially in the United States, though this is only really valid for the measurement of tones, not noise, and is widely incorporated into sound level meters. \nIn telecommunications, noise weightings are used by agencies concerned with public telephone service, and various standard curves are based on the characteristics of specific commercial telephone instruments, representing successive stages of technological development. The coding of commercial apparatus appears in the nomenclature of certain weightings. The same weighting nomenclature and units are used in military versions of commercial noise measuring sets.\nTelecommunication measurements are made in lines terminated either by the measuring set or an instrument of the relevant class."}
{"id": "41422", "revid": "18931", "url": "https://en.wikipedia.org/wiki?curid=41422", "title": "Noisy black", "text": ""}
{"id": "41423", "revid": "18931", "url": "https://en.wikipedia.org/wiki?curid=41423", "title": "Noisy white", "text": ""}
{"id": "41425", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41425", "title": "Non-return-to-zero", "text": "Telecommunications coding technique\nIn telecommunications, a non-return-to-zero (NRZ) line code is a binary code in which ones are represented by one significant condition, usually a positive voltage, while zeros are represented by some other significant condition, usually a negative voltage, with no other neutral or rest condition.\nFor a given data signaling rate, i.e., bit rate, the NRZ code requires only half the baseband bandwidth required by the Manchester code (the passband bandwidth is the same). The pulses in NRZ have more energy than a return-to-zero (RZ) code, which also has an additional rest state beside the conditions for ones and zeros.\nWhen used to represent data in an asynchronous communication scheme, the absence of a neutral state requires other mechanisms for bit synchronization when a separate clock signal is not available. Since NRZ is not inherently a self-clocking signal, some additional synchronization technique must be used for avoiding bit slips; examples of such techniques are a run-length-limited constraint and a parallel synchronization signal.\nVariants.\nNRZ can refer to any of the following serializer line codes:\nThe NRZ code also can be classified as a polar or non-polar, where polar refers to a mapping to voltages of +V and \u2212V, and non-polar refers to a voltage mapping of +V and 0, for the corresponding binary values of 1 and 0.\nUnipolar non-return-to-zero level.\n\"One\" is represented by a DC bias on the transmission line (conventionally positive), while \"zero\" is represented by the absence of bias \u2013 the line at 0 volts or grounded. For this reason it is also known as \"on-off keying\". In clock language, a \"one\" transitions to or remains at a biased level on the trailing clock edge of the previous bit, while \"zero\" transitions to or remains at no bias on the trailing clock edge of the previous bit. Among the disadvantages of unipolar NRZ is that it allows for long series without change, which makes synchronization difficult, although this is not unique to the unipolar case. One solution is to not send bytes without transitions. More critically, and unique to unipolar NRZ, are issues related to the presence of a transmitted DC level \u2013 the power spectrum of the transmitted signal does not approach zero at zero frequency. This leads to two significant problems: first, the transmitted DC power leads to higher power losses than other encodings, and second, the presence of a DC signal component requires that the transmission line be DC-coupled.\nBipolar non-return-to-zero level.\n\"One\" is represented by one physical level (usually a positive voltage), while \"zero\" is represented by another level (usually a negative voltage). In clock language, in bipolar NRZ-level the voltage \"swings\" from positive to negative on the trailing edge of the previous bit clock cycle.\nAn example of this is RS-232, where \"one\" is \u221212\u00a0V to \u22125\u00a0V and \"zero\" is +5\u00a0V to +12\u00a0V.\nNon-return-to-zero space.\n\"One\" is represented by no change in physical level, while \"zero\" is represented by a change in physical level. In clock language, the level transitions on the trailing clock edge of the previous bit to represent a \"zero\".\nThis \"change-on-zero\" is used by High-Level Data Link Control and USB. They both avoid long periods of no transitions (even when the data contains long sequences of 1 bits) by using zero-bit insertion. HDLC transmitters insert a 0 bit after 5 contiguous 1 bits (except when transmitting the frame delimiter 01111110). USB transmitters insert a 0 bit after 6 consecutive 1 bits. The receiver at the far end uses every transition \u2014 both from 0 bits in the data and these extra non-data 0 bits \u2014 to maintain clock synchronization. The receiver otherwise ignores these non-data 0 bits.\nNon-return-to-zero inverted.\nNon-return-to-zero, inverted (NRZI, also known as \"non-return to zero IBM\", \"inhibit code\", or \"IBM code\") was devised by Bryon E. Phelps (IBM) in 1956. It is a method of mapping a binary signal to a physical signal for transmission over some transmission medium. The two-level NRZI signal distinguishes data bits by the presence or absence of a transition at a clock boundary. The NRZI encoded signal can be decoded unambiguously after passing through a data path that doesn\u2019t preserve polarity.\n\"Which\" bit value corresponds to a transition varies in practice, NRZI applies equally to both. Magnetic storage generally uses the NRZ-M, non-return-to-zero mark convention: a logical 1 is encoded as a transition, and a logical 0 is encoded as no transition. The HDLC and Universal Serial Bus protocols use the opposite NRZ-S, non-return-to-zero space convention: a logical 0 is a transition, and a logical 1 is no transition. Neither NRZI encoding guarantees that the encoded bitstream has transitions.\nAn asynchronous receiver uses an independent bit clock that is phase synchronized by detecting bit transitions. When an asynchronous receiver decodes a block of bits without a transition longer than the period of the difference between the frequency of the transmitting and receiving bit clocks, the decoder\u2019s bit clock is either 1 bit earlier than the encoder resulting in a duplicated bit being inserted in the decoded data stream, or the decoder\u2019s bit clock is 1 bit later than the encoder resulting in a duplicated bit being removed from the decoded data stream. Both are referred to as \"bit slip\" denoting that the phase of the bit clock has slipped a bit period.\nForcing transitions at intervals shorter than the bit clock difference period allows an asynchronous receiver to be used for NRZI bit streams. Additional transitions necessarily consume some of the data channel\u2019s rate capacity. Consuming no more of the channel capacity than necessary to maintain bit clock synchronization without increasing costs related to complexity is a problem with many possible solutions.\nRun-length limited (RLL) encodings have been used for magnetic disk and tape storage devices using fixed-rate RLL codes that increase the channel data rate by a known fraction of the information data rate. HDLC and USB use bit stuffing: inserting an additional 0 bit before NRZ-S encoding to force a transition in the encoded data sequence after 5 (HDLC) or 6 (USB) consecutive 1 bits. Bit stuffing consumes channel capacity only when necessary but results in a variable information data rate.\nSynchronized non-return-to-zero.\nSynchronized NRZI (SNRZI) and \"group-coded recording\" (\"GCR\") are modified forms of NRZI. In SNRZI-M each 8-bit group is extended to 9 bits by a 1 in order to insert a transition for synchronisation.\nComparison with return-to-zero.\nReturn-to-zero describes a line code used in telecommunications in which the signal drops (returns) to zero between each pulse. This takes place even if a number of consecutive 0s or 1s occur in the signal. The signal is self-clocking. This means that a separate clock does not need to be sent alongside the signal, but suffers from using twice the bandwidth to achieve the same data-rate as compared to non-return-to-zero format.\nThe \"zero\" between each bit is a neutral or rest condition, such as a zero amplitude in pulse-amplitude modulation (PAM), zero phase shift in phase-shift keying (PSK), or mid-frequency in frequency-shift keying (FSK). That \"zero\" condition is typically halfway between the significant condition representing a 1 bit and the other significant condition representing a 0 bit.\nAlthough return-to-zero contains a provision for synchronization, it still may have a DC component resulting in \"baseline wander\" during long strings of 0 or 1 bits, just like the line code non-return-to-zero.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nExternal links.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41426", "revid": "11521989", "url": "https://en.wikipedia.org/wiki?curid=41426", "title": "Normalized frequency", "text": "Normalized frequency may refer to:\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "41427", "revid": "8524693", "url": "https://en.wikipedia.org/wiki?curid=41427", "title": "NS/EP telecommunications", "text": "Telecommunications system in USA\nNS/EP telecommunications is an abbreviation for National Security or Emergency Preparedness telecommunications of the United States. Telecommunications services that are used to maintain a state of readiness or to respond to and manage any event or crisis (local, national, or international) that causes or could cause injury or harm to the population, damage to or loss of property, or degrade or threaten the national security or emergency preparedness posture of the United States.\nNS/EP telecommunications are managed and controlled by the National Communications System using Telecommunications Service Priority through both the Government Emergency Telecommunications Service and Wireless Priority Service."}
{"id": "41428", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41428", "title": "N-entity", "text": "Active n-th layer element\nIn telecommunications, a \"n\"-entity is an active element in the \"n\"-th layer of the Open Systems Interconnection--Reference Model (OSI-RM) that (a) interacts directly with elements, \"i.e.\", entities, of the layer immediately above or below the \"n\"-th layer, (b) is defined by a unique set of rules, \"i.e.\", syntax, and information formats, including data and control formats, and (c) performs a defined set of functions. \nThe \"n\" refers to any one of the 7 layers of the OSI-RM. \nIn an existing layered open system, the \"n\" may refer to any given layer in the system. \nLayers are conventionally numbered from the lowest, \"i.e.\", the physical layer, to the highest, so that the (\"n\" + 1)-th layer is above the \"n\"-th layer and the (\"n\" \u2212 1)-th layer is below.\nReferences.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41430", "revid": "3145267", "url": "https://en.wikipedia.org/wiki?curid=41430", "title": "NTSC standard", "text": ""}
{"id": "41431", "revid": "2255048", "url": "https://en.wikipedia.org/wiki?curid=41431", "title": "Nuclear hardness", "text": ""}
{"id": "41432", "revid": "50249534", "url": "https://en.wikipedia.org/wiki?curid=41432", "title": "Numerical aperture", "text": "Characteristic of an optical system\nIn optics, the numerical aperture (NA) of an optical system is a dimensionless number that characterizes the range of angles over which the system can accept or emit light. By incorporating index of refraction in its definition, NA has the property that it is constant for a beam as it goes from one material to another, provided there is no refractive power at the interface (e.g., a flat interface). The exact definition of the term varies slightly between different areas of optics. Numerical aperture is commonly used in microscopy to describe the acceptance cone of an objective (and hence its light-gathering ability and resolution), and in fiber optics, in which it describes the range of angles within which light that is incident on the fiber will be transmitted along it.\nGeneral optics.\nIn most areas of optics, and especially in microscopy, the numerical aperture of an optical system such as an objective lens is defined by\nformula_1\nwhere \"n\" is the index of refraction of the medium in which the lens is working (1.00 for air, 1.33 for pure water, and typically 1.52 for immersion oil; see also list of refractive indices), and \"\u03b8\" is the half-angle of the maximum cone of light that can enter or exit the lens. In general, this is the angle of the real marginal ray in the system. Because the index of refraction is included, the NA of a pencil of rays is an invariant as a pencil of rays passes from one material to another through a flat surface. This can be shown by rearranging Snell's law to find that \"n\" sin \"\u03b8\" is constant across an interface; NA = \"n\"1\u2009sin \"\u03b8\"1 = \"n\"2\u2009sin \"\u03b8\"2.\nIn air, the angular aperture of the lens, formula_2, is approximately twice this value (within the paraxial approximation). The NA is generally measured with respect to a particular object or image point and will vary as that point is moved. In microscopy, NA generally refers to object-space numerical aperture unless otherwise noted.\nIn microscopy, NA is important because it indicates the resolving power of a lens. The size of the finest detail that can be resolved (the \"resolution\") is proportional to , where \"\u03bb\" is the wavelength of the light. A lens with a larger numerical aperture will be able to visualize finer details than a lens with a smaller numerical aperture. Assuming quality (diffraction-limited) optics, lenses with larger numerical apertures collect more light and will generally provide a brighter image but will provide shallower depth of field.\nNumerical aperture is used to define the \"pit size\" in optical disc formats.\nIncreasing the magnification and the numerical aperture of the objective reduces the working distance, i.e. the distance between front lens and specimen.\nNumerical aperture versus f-number.\nNumerical aperture is not typically used in photography. Instead, the angular aperture formula_2 of a lens (or an imaging mirror) is expressed by the f-number, written &lt;templatestyles src=\"F//styles.css\" /&gt;f/\"N\", where N is the f-number given by the ratio of the focal length f to the diameter of the entrance pupil \"D\":\nformula_4\nThis ratio is related to the image-space numerical aperture when the lens is focused at infinity. Based on the diagram at the right, the image-space numerical aperture of the lens is:\nformula_5\nthus \"N\" \u2248, assuming normal use in air (\"n\" \n 1).\nThe approximation holds when the numerical aperture is small, but it turns out that for well-corrected optical systems such as camera lenses, a more detailed analysis shows that \"N\" is almost exactly equal to 1/(2NAi) even at large numerical apertures. As Rudolf Kingslake explains, \"It is a common error to suppose that the ratio [\"D\"/2\"f\"] is actually equal to tan \"\u03b8\", and not sin \"\u03b8\" ... The tangent would, of course, be correct if the principal planes were really plane. However, the complete theory of the Abbe sine condition shows that if a lens is corrected for coma and spherical aberration, as all good photographic objectives must be, the second principal plane becomes a portion of a sphere of radius f centered about the focal point\". In this sense, the traditional thin-lens definition and illustration of f-number is misleading, and defining it in terms of numerical aperture may be more meaningful.\nWorking (effective) f-number.\nThe f-number describes the light-gathering ability of the lens in the case where the marginal rays on the object side are parallel to the axis of the lens. This case is commonly encountered in photography, where objects being photographed are often far from the camera. When the object is not distant from the lens, however, the image is no longer formed in the lens's focal plane, and the f-number no longer accurately describes the light-gathering ability of the lens or the image-side numerical aperture. In this case, the numerical aperture is related to what is sometimes called the \"working f-number\" or \"effective f-number\".\nThe working f-number is defined by modifying the relation above, taking into account the magnification from object to image:\nformula_6\nwhere \"N\"w is the working f-number, \"m\" is the lens's magnification for an object a particular distance away, \"P\" is the pupil magnification, and the NA is defined in terms of the angle of the marginal ray as before. The magnification here is typically negative, and the pupil magnification is most often assumed to be 1 \u2014 as Allen R. Greenleaf explains, \"Illuminance varies inversely as the square of the distance between the exit pupil of the lens and the position of the plate or film. Because the position of the exit pupil usually is unknown to the user of a lens, the rear conjugate focal distance is used instead; the resultant theoretical error so introduced is insignificant with most types of photographic lenses.\"\nIn photography, the factor is sometimes written as 1 + \"m\", where \"m\" represents the absolute value of the magnification; in either case, the correction factor is 1 or greater. The two equalities in the equation above are each taken by various authors as the definition of working f-number, as the cited sources illustrate. They are not necessarily both exact, but are often treated as if they are.\nConversely, the object-side numerical aperture is related to the f-number by way of the magnification (tending to zero for a distant object):\nformula_7\nLaser physics.\nIn laser physics, numerical aperture is defined slightly differently. Laser beams spread out as they propagate, but slowly. Far away from the narrowest part of the beam, the spread is roughly linear with distance\u2014the laser beam forms a cone of light in the \"far field\". The relation used to define the NA of the laser beam is the same as that used for an optical system,\nformula_8\nbut \"\u03b8\" is defined differently. Laser beams typically do not have sharp edges like the cone of light that passes through the aperture of a lens does. Instead, the irradiance falls off gradually away from the center of the beam. It is very common for the beam to have a Gaussian profile. Laser physicists typically choose to make \"\u03b8\" the \"divergence\" of the beam: the far-field angle between the beam axis and the distance from the axis at which the irradiance drops to \"e\"\u22122 times the on-axis irradiance. The NA of a Gaussian laser beam is then related to its minimum spot size (\"beam waist\") by\nformula_9\nwhere \"\u03bb\"0 is the vacuum wavelength of the light, and 2\"w\"0 is the diameter of the beam at its narrowest spot, measured between the \"e\"\u22122 irradiance points (\"Full width at \"e\"\u22122 maximum of the intensity\"). This means that a laser beam that is focused to a small spot will spread out quickly as it moves away from the focus, while a large-diameter laser beam can stay roughly the same size over a very long distance. See also: Gaussian beam width.\nFiber optics.\nA multi-mode optical fiber will only propagate light that enters the fiber within a certain range of angles, known as the acceptance cone of the fiber. The half-angle of this cone is called the acceptance angle, \"\u03b8\"max. For step-index multimode fiber in a given medium, the acceptance angle is determined only by the indices of refraction of the core, the cladding, and the medium:\nformula_10\nwhere \"n\" is the refractive index of the medium around the fiber, \"n\"core is the refractive index of the fiber core, and \"n\"clad is the refractive index of the cladding. While the core will accept light at higher angles, those rays will not totally reflect off the core\u2013cladding interface, and so will not be transmitted to the other end of the fiber. The derivation of this formula is given below.\nWhen a light ray is incident from a medium of refractive index n to the core of index \"n\"core at the maximum acceptance angle, Snell's law at the medium\u2013core interface gives\nformula_11\nFrom the geometry of the above figure we have:\nformula_12\nwhere\nformula_13\nis the critical angle for total internal reflection.\nSubstituting cos \"\u03b8\"\"c\" for sin \"\u03b8\"\"r\" in Snell's law we get:\nformula_14\nBy squaring both sides\nformula_15\nSolving, we find the formula stated above:\nformula_10\nThis has the same form as the numerical aperture in other optical systems, so it has become common to \"define\" the NA of any type of fiber to be\nformula_17\nwhere \"n\"core is the refractive index along the central axis of the fiber. Note that when this definition is used, the connection between the numerical aperture and the acceptance angle of the fiber becomes only an approximation. In particular, \"NA\" defined this way is not relevant for single-mode fiber. One cannot define an acceptance angle for single-mode fiber based on the indices of refraction alone.\nThe number of bound modes, the mode volume, is related to the normalized frequency and thus to the numerical aperture.\nIn multimode fibers, the term \"equilibrium numerical aperture\" is sometimes used. This refers to the numerical aperture with respect to the extreme exit angle of a ray emerging from a fiber in which equilibrium mode distribution has been established.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41434", "revid": "869314", "url": "https://en.wikipedia.org/wiki?curid=41434", "title": "Nyquist interval", "text": ""}
{"id": "41435", "revid": "42090783", "url": "https://en.wikipedia.org/wiki?curid=41435", "title": "Nyquist rate", "text": "Minimum sampling rate to avoid aliasing\n \nIn signal processing, the Nyquist rate, named after Harry Nyquist, is a value equal to twice the highest frequency (bandwidth) of a given function or signal. It has units of samples per unit time, conventionally expressed as samples per second, or hertz (Hz). When the signal is sampled at a higher sample rate &lt;templatestyles src=\"Crossreference/styles.css\" /&gt;, the resulting discrete-time sequence is said to be free of the distortion known as aliasing. Conversely, for a given sample rate the corresponding Nyquist frequency is one-half the sample rate. Note that the \"Nyquist rate\" is a property of a continuous-time signal, whereas \"Nyquist frequency\" is a property of a discrete-time system.\nThe term \"Nyquist rate\" is also used in a different context with units of symbols per second, which is actually the field in which Harry Nyquist was working. In that context it is an upper bound for the symbol rate across a bandwidth-limited baseband channel such as a telegraph line or passband channel such as a limited radio frequency band or a frequency division multiplex channel.\nRelative to sampling.\nWhen a continuous function, formula_1 is sampled at a constant rate, formula_2 \"samples/second\", there is always an unlimited number of other continuous functions that fit the same set of samples. But only one of them is bandlimited to formula_3 \"cycles/second\" (hertz), which means that its Fourier transform, formula_4 is formula_5 for all formula_6\u00a0 The mathematical algorithms that are typically used to recreate a continuous function from samples create arbitrarily good approximations to this theoretical, but infinitely long, function. It follows that if the original function, formula_1 is bandlimited to formula_8 which is called the \"Nyquist criterion\", then it is the one unique function the interpolation algorithms are approximating. In terms of a function's own bandwidth formula_9 as depicted here, the Nyquist criterion is often stated as formula_10\u00a0 And formula_11 is called the Nyquist rate for functions with bandwidth formula_12 When the Nyquist criterion is not met formula_13say, formula_14 a condition called aliasing occurs, which results in some inevitable differences between formula_15 and a reconstructed function that has less bandwidth. In most cases, the differences are viewed as distortion.\nIntentional aliasing.\nFigure 3 depicts a type of function called baseband or lowpass, because its positive-frequency range of significant energy is [0,\u00a0\"B\"). When instead, the frequency range is (\"A\",\u00a0\"A\"+\"B\"), for some \"A\"\u00a0&gt;\u00a0\"B\", it is called bandpass, and a common desire (for various reasons) is to convert it to baseband. One way to do that is frequency-mixing (heterodyne) the bandpass function down to the frequency range (0,\u00a0\"B\"). One of the possible reasons is to reduce the Nyquist rate for more efficient storage. And it turns out that one can directly achieve the same result by sampling the bandpass function at a sub-Nyquist sample-rate that is the smallest integer-sub-multiple of frequency \"A\" that meets the baseband Nyquist criterion:\u00a0 fs\u00a0&gt;\u00a02\"B\". For a more general discussion, see bandpass sampling.\nRelative to signaling.\nLong before Harry Nyquist had his name associated with sampling, the term \"Nyquist rate\" was used differently, with a meaning closer to what Nyquist actually studied. Quoting Harold S. Black's 1953 book \"Modulation Theory,\" in the section Nyquist Interval of the opening chapter \"Historical Background:\"\n\"If the essential frequency range is limited to \"B\" cycles per second, 2\"B\" was given by Nyquist as the maximum number of code elements per second that could be unambiguously resolved, assuming the peak interference is less than half a quantum step. This rate is generally referred to as signaling at the Nyquist rate and 1/(2\"B\") has been termed a \"Nyquist interval\".\" (bold added for emphasis; italics from the original)\n\"B\" in this context, related to the Nyquist ISI criterion, referring to the one-sided bandwidth rather than the total as considered in later usage.\nAccording to the OED, Black's statement regarding 2\"B\" may be the origin of the term \"Nyquist rate\".\nNyquist's famous 1928 paper was a study on how many pulses (code elements) could be transmitted per second, and recovered, through a channel of limited bandwidth.\n\"Signaling at the Nyquist rate\" meant putting as many code pulses through a telegraph channel as its bandwidth would allow. Shannon used Nyquist's approach when he proved the sampling theorem in 1948, but Nyquist did not work on sampling per se.\nBlack's later chapter on \"The Sampling Principle\" does give Nyquist some of the credit for some relevant math:\n\"Nyquist (1928) pointed out that, if the function is substantially limited to the time interval \"T\", 2\"BT\" values are sufficient to specify the function, basing his conclusions on a Fourier series representation of the function over the time interval \"T\".\"\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41436", "revid": "28478", "url": "https://en.wikipedia.org/wiki?curid=41436", "title": "Nyquist's theorem", "text": ""}
{"id": "41437", "revid": "7903804", "url": "https://en.wikipedia.org/wiki?curid=41437", "title": "Off-axis optical system", "text": "An off-axis optical system is an optical system in which the optical axis of the aperture is not coincident with the mechanical center of the aperture. The principal applications of off-axis optical systems are to avoid obstruction of the primary aperture by secondary optical elements, instrument packages, or sensors, and to provide ready access to instrument packages or sensors at the focus. The engineering tradeoff of an off-axis optical system is an increase in image aberrations.\nThere are various theoretical models for aberration in off-axis optical systems. This involves various techniques including different types of equations for ray-tracing, and a goal can be optimizing the design.\nAn example of an off-axis optical system is a three mirror design as optics for a hyperspectral imager.\nReferences.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41438", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41438", "title": "On- and off-hook", "text": "A telephone respectively off- and on-line\nIn telephony, on-hook and off-hook are two states of a communication circuit. On subscriber telephones the states are produced by placing the handset onto or off the hookswitch. Placing the circuit into the off-hook state is also called \"seizing the line\". \"Off-hook\" originally referred to the condition that prevailed when telephones had a separate earpiece (\"receiver\"), which hung from its switchhook until the user initiated a telephone call by removing it. When off hook the weight of the receiver no longer depresses the spring-loaded switchhook, thereby connecting the instrument to the telephone line.\nOff-hook.\nThe term off-hook has the following meanings:\nOn an ordinary two-wire telephone line, off-hook status is communicated to the telephone exchange by a resistance short across the pair. When an off-hook condition persists without dialing, for example because the handset has fallen off or the cable has been flooded, it is treated as a \"permanent loop\" or permanent signal.\nThe act of \"going off-hook\" is also referred to as \"seizing\" the line or channel.\nOn-hook.\nThe term on-hook has the following meanings: \nThe act of \"going on-hook\" is also referred to as \"releasing the line\" or \"channel\", and may initiate the process of clearing.\nReferences.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41439", "revid": "139669777", "url": "https://en.wikipedia.org/wiki?curid=41439", "title": "Office classification", "text": ""}
{"id": "41440", "revid": "48643156", "url": "https://en.wikipedia.org/wiki?curid=41440", "title": "Online and offline", "text": "Connected or disconnected state for equipment and services\nIn computer technology and telecommunications, online indicates a state of connectivity, and offline indicates a disconnected state. In modern terminology, this usually refers to an Internet connection, but (especially when expressed as \"on line\" or \"on the line\") could refer to any piece of equipment or functional unit that is connected to a larger system. Being online means that the equipment or subsystem is connected, or that it is ready for use.\n\"Online\" has come to describe activities and concepts that take place on the Internet, such as online identity, online predator and online shop. A similar meaning is also given by the prefixes cyber and e, as in words \"cyberspace\", \"cybercrime\", \"email\", and \"e-commerce\". In contrast, \"offline\" can refer to either computing activities performed while disconnected from the Internet, or alternatives to Internet activities (such as shopping in brick-and-mortar stores). The term \"offline\" is sometimes used interchangeably with the acronym \"IRL\", meaning \"in real life\", as well as about office meetings in person wherein a formal, high-concentration meeting may have matters taken \"offline\" to a more relaxed moment away from the \"online\" high-stress meeting.\nHistory.\nDuring the 19th century, the term \"on line\" was commonly used in both the railroad and telegraph industries. For railroads, a signal box would send messages down the line (track), via a telegraph line (cable), indicating the track's status: \"Train on line\" or \"Line clear\". Telegraph linemen would refer to sending current through a line as \"direct on line\" or \"battery on line\"; or they may refer to a problem with the circuit as being \"on line\", as opposed to the power source or end-point equipment.\nSince at least 1950, in computing, the terms \"on-line\" and \"off-line\" have been used to refer to whether machines, including computers and peripheral devices, are connected or not. Here is an excerpt from the 1950 book \"High-Speed Computing Devices\":\n The use of automatic computing equipment for large-scale reduction of data will be strikingly successful only if means are provided for the automatic transcription of these data to a form suitable for automatic entry into the machine. For some applications, of which the most prominent are those in which the reduced data are used to control the process being measured, the input must be developed for \"on-line\" operation. In on-line operation the input is communicated directly and without delay to the data-reduction device. For other applications, \"off-line\" operation, involving automatic transcription of data in a form suitable for later introduction to the machine, may be tolerated. These requirements may be compared with teleprinter operating requirements. For example, some teletype machines operate on line. Their operators are in instantaneous communication. Other teletype machines are operated off line, through the intervention of punched paper tape. The message is preserved by means of holes punched in the tape and is transmitted later by feeding the tape to another machine.\nExamples.\nOffline e-mail.\nOne example of a common use of these concepts with email is a mail user agent (MUA) that can be instructed to be in either online or offline states. One such MUA is Microsoft Outlook. When online it will attempt to connect to mail servers (to check for new mail at regular intervals, for example), and when offline it will not attempt to make any such connection. The online or offline state of the MUA does not necessarily reflect the connection status between the computer on which it is running and the Internet i.e. the computer itself may be online\u2014connected to the Internet via a cable modem or other means\u2014while Outlook is kept offline by the user, so that it makes no attempt to send or to receive messages. Similarly, a computer may be configured to employ a dial-up connection on demand (as when an application such as Outlook attempts to make a connection to a server), but the user may not wish for Outlook to trigger that call whenever it is configured to check for mail.\nOffline media playing.\nAnother example of the use of these concepts is digital audio technology. A tape recorder, digital audio editor, or other device that is online is one whose clock is under the control of the clock of a synchronization master device. When the sync master commences playback, the online device automatically synchronizes itself to the master and commences playing from the same point in the recording. A device that is offline uses no external clock reference and relies upon its own internal clock. When many devices are connected to a sync master it is often convenient, if one wants to hear just the output of one single device, to take it offline because, if the device is played back online, all synchronized devices have to locate the playback point and wait for each other device to be in synchronization. (For related discussion, see MIDI timecode, Word clock, and recording system synchronization.)\nOffline browsing.\nA third example of a common use of these concepts is a web browser that can be instructed to be in either online or offline states. The browser attempts to fetch pages from servers while only in the online state. In the offline state, or \"offline mode\", users can perform offline browsing, where pages can be browsed using local copies of those pages that have previously been downloaded while in the online state. This can be useful when the computer is offline and connection to the Internet is impossible or undesirable. The pages are downloaded either implicitly into the web browser's own cache as a result of prior online browsing by the user or explicitly by a browser configured to keep local copies of certain web pages, which are updated when the browser is in the online state, either by checking that the local copies are up-to-date at regular intervals or by checking that the local copies are up-to-date whenever the browser is switched to the online. One such web browser is Internet Explorer. When pages are added to the Favourites list, they can be marked to be \"available for offline browsing\". Internet Explorer will download local copies of both the marked page and, optionally, all of the pages that it links to. In Internet Explorer version 6, the level of direct and indirect links, the maximum amount of local disc space allowed to be consumed, and the schedule on which local copies are checked to see whether they are up-to-date, are configurable for each individual Favourites entry.\nFor communities that lack adequate Internet connectivity\u2014such as developing countries, rural areas, and prisons\u2014offline information stores such as WiderNet's eGranary Digital Library (a collection of approximately thirty million educational resources from more than two thousand web sites and hundreds of CD-ROMs) provide offline access to information. More recently, the Internet Archive announced an offline server project intended to provide access to material on inexpensive servers that can be updated using USB sticks and SD cards.\nOffline storage.\nLikewise, offline storage is computer data storage that has no connection to the other systems until a connection is deliberately made. Additionally, an otherwise online system that is powered down may be considered offline.\nOffline messages.\nWith the growing communication tools and media, the words offline and online are used very frequently. If a person is active over a messaging tool and is able to accept the messages it is termed as online message and if the person is not available and the message is left to view when the person is back, it is termed as offline message. In the same context, the person's availability is termed as online and non-availability is termed as offline.\nFile systems.\nIn the context of file systems, \"online\" and \"offline\" are synonymous with \"mounted\" and \"not mounted\". For example, in file systems' resizing capabilities, \"online grow\" and \"online shrink\" respectively mean the ability to increase or decrease the space allocated to that file system without needing to unmount it.\nGeneralisations.\nOnline and offline distinctions have been generalised from computing and telecommunication into the field of human interpersonal relationships. The distinction between what is considered online and what is considered offline has become a subject of study in the field of sociology.\nThe distinction between online and offline is conventionally seen as the distinction between computer-mediated communication and face-to-face communication (e.g., face time), respectively. Online is virtuality or cyberspace, and offline is reality (i.e., real life or \"meatspace\"). Slater states that this distinction is \"obviously far too simple\". To support his argument that the distinctions in relationships are more complex than a simple dichotomy of online versus offline, he observes that some people draw no distinction between an online relationship, such as indulging in cybersex, and an offline relationship, such as being pen pals. He argues that even the telephone can be regarded as an online experience in some circumstances, and that the blurring of the distinctions between the uses of various technologies (such as PDA versus mobile phone, internet television versus internet, and telephone versus Voice over Internet Protocol) has made it \"impossible to use the term \"online\" meaningfully in the sense that was employed by the first generation of Internet research\".\nSlater asserts that there are legal and regulatory pressures to reduce the distinction between online and offline, with a \"general tendency to assimilate online to offline and erase the distinction,\" stressing, however, that this does not mean that online relationships are being reduced to \"pre-existing\" offline relationships. He conjectures that greater legal status may be assigned to online relationships (pointing out that contractual relationships, such as business transactions, online are already seen as just as \"real\" as their offline counterparts), although he states it to be hard to imagine courts awarding palimony to people who have had a purely online sexual relationship. He also conjectures that an online/offline distinction may be seen by people as \"rather quaint and not quite comprehensible\" within 10 years.\nThis distinction between \"online\" and \"offline\" is sometimes inverted, with online concepts being used to define and to explain offline activities, rather than (as per the conventions of the desktop metaphor with its desktops, trash cans, folders, and so forth) the other way around. Several cartoons appearing in \"The New Yorker\" have satirized this. One includes Saint Peter asking for a username and a password before admitting a man into Heaven. Another illustrates \"the offline store\" where \"All items are actual size!\", shoppers may \"Take it home as soon as you pay for it!\", and \"Merchandise may be handled prior to purchase!\"\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "41442", "revid": "37195583", "url": "https://en.wikipedia.org/wiki?curid=41442", "title": "One-way trunk", "text": "Telecommunication trunk\nIn telecommunications, a one-way trunk is a trunk between two switching centers, over which traffic may be originated from one preassigned location only.\nThe traffic may consist of two-way communications; the expression \"one way\" refers only to the origin of the demand for a connection. At the originating end, the one-way trunk is known as an \"outgoing trunk\"; at the other end, it is known as an \"incoming trunk\".\nReferences.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41443", "revid": "8066546", "url": "https://en.wikipedia.org/wiki?curid=41443", "title": "On-hook", "text": ""}
{"id": "41444", "revid": "12536756", "url": "https://en.wikipedia.org/wiki?curid=41444", "title": "On-line", "text": ""}
{"id": "41445", "revid": "47695301", "url": "https://en.wikipedia.org/wiki?curid=41445", "title": "On-premises wiring", "text": "Add before construction or break through walls\nOn-premises wiring (customer premises wiring) is customer-owned transmission or distribution lines. The transmission lines may be metallic (copper) or optical fiber, and may be installed within or between buildings.\nPremises wiring may consist of horizontal wiring, vertical wiring, and backbone cabling. It may extend from the point-of-entry to user work areas. Any type of telecommunications or data wiring is considered premises wiring, including telephone, computer/data, intercom, closed-circuit television.\nPremises networks are wired worldwide, across every industry, in both small and large-scale applications. Any type or number of topologies may be used \u2013 star, bus, ring, etc. In 1989, the United States Federal Communications Commission (FCC) deregulated charges for maintaining at home \"inside wiring\"; the corresponding monthly charge was dropped January 1990.\nOwnership.\nThe ownership of on-premises wiring varies between jurisdictions: It depends on the location of the demarcation point. The location determines ownership and responsibility for maintenance and repair.\nIn the United States and Canada, most premises wiring is owned by the customer. There generally is a demarcation point \"as close to the poles\" as possible. For many installations, this is a network interface device mounted on the outside of the building. In some cases, it is a minimum-point-of-entry (MPOE) location inside the building.\nIn the United Kingdom, the demarcation point is the wall jack, and hence most of the on-premises wiring is the property of the telephone company.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41448", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41448", "title": "Open network architecture", "text": "In telecommunications, and in the context of Federal Communications Commission's (FCC) Computer Inquiry III, Open network architecture (ONA) is the overall design of a communication carrier's basic network facilities and services to permit all users of the basic network to interconnect to specific basic network functions and interfaces on an unbundled, equal-access basis.\nThe ONA concept consists of three integral components:"}
{"id": "41449", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41449", "title": "Open systems architecture", "text": "System design objective\nOpen systems architecture is a system design approach which aims to produce systems that are inherently interoperable and connectable without recourse to retrofit and redesign.\nConcept.\nSystems design is a process of defining and engineering the architecture, methods, and interfaces necessary to accomplish a goal or fulfill a set of requirements. In open systems architecture, the design includes intentional provisions to make it possible to expand or modify the system at a later stage after initial operation. There is no one specific universal OSA, but it is essential the specific OSA applicable to a system is rigorously defined and documented. For example, in information technology and telecommunication, such design principles lead to open systems.\nTelecommunications.\nIn telecommunications, open systems architecture (OSA) is a standard that describes the layered hierarchical structure, configuration, or model of a communications or distributed data processing system. It enables system description, design, development, installation, operation, improvement, and maintenance to be performed at the abstraction layers in the hierarchical structure. Each layer provides a set of accessible functions that can be controlled and used by the functions in the layer above it. Each layer can be implemented without affecting the implementation of other layers. The alteration of system performance by the modification of one or more layers may be accomplished without altering the existing equipment, procedures, and protocols at the remaining layers.\nExamples of independent alterations include the conversion from wire to optical fiber at a physical layer without affecting the data link layer or the network layer, except to provide more traffic capacity, and the altering of the operational protocols at the network level without altering the physical layer.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nSources.\n \n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41450", "revid": "11952314", "url": "https://en.wikipedia.org/wiki?curid=41450", "title": "Open Systems Interconnection Specification", "text": ""}
{"id": "41451", "revid": "15936944", "url": "https://en.wikipedia.org/wiki?curid=41451", "title": "Open Systems Interconnection--Reference Model", "text": ""}
{"id": "41453", "revid": "47183485", "url": "https://en.wikipedia.org/wiki?curid=41453", "title": "Operation", "text": "Operation or Operations may refer to:\n&lt;templatestyles src=\"Template:TOC_right/styles.css\" /&gt;\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "41455", "revid": "49124209", "url": "https://en.wikipedia.org/wiki?curid=41455", "title": "Optical attenuator", "text": "Device used to reduce the power level of an optical signal\nAn optical attenuator, or fiber optic attenuator, is a device used to reduce the power level of an optical signal, either in free space or in an optical fiber. The basic types of optical attenuators are fixed, step-wise variable, and continuously variable.\nApplications.\nOptical attenuators are commonly used in fiber-optic communications, either to test power level margins by temporarily adding a calibrated amount of signal loss, or installed permanently to properly match transmitter and receiver levels. Sharp bends stress optic fibers and can cause losses. If a received signal is too strong a temporary fix is to wrap the cable around a pencil until the desired level of attenuation is achieved. However, such arrangements are unreliable, since the stressed fiber tends to break over time.\nGenerally, multimode systems do not need attenuators as the multimode sources, rarely have enough power output to saturate receivers. Instead, single-mode systems, especially the long-haul DWDM network links, often need to use fiber optic attenuators to adjust the optical power during the transmission.\nPrinciples of operation.\nThe power reduction is done by such means as absorption, reflection, diffusion, scattering, deflection, diffraction, and dispersion, etc. Optical attenuators usually work by absorbing the light, like sunglasses absorb extra light energy. They typically have a working wavelength range in which they absorb all light energy equally. They should not reflect the light or scatter the light in an air gap, since that could cause unwanted back reflection in the fiber system. Another type of attenuator utilizes a length of high-loss optical fiber, that operates upon its input optical signal power level in such a way that its output signal power level is less than the input level.\nTypes.\nOptical attenuators can take a number of different forms and are typically classified as fixed or variable attenuators. What's more, they can be classified as LC, SC, ST, FC, MU, E2000 etc. according to the different types of connectors.\nFixed Attenuators.\nFixed optical attenuators used in fiber optic systems may use a variety of principles for their functioning. Preferred attenuators use either doped fibers, or mis-aligned splices, or total power since both of these are reliable and inexpensive.\n\"Inline\" style attenuators are incorporated into patch cables. The alternative \"build out\" style attenuator is a small male-female adapter that can be added onto other cables.\nNon-preferred attenuators often use gap loss or reflective principles. Such devices can be sensitive to: modal distribution, wavelength, contamination, vibration, temperature, damage due to power bursts, may cause back reflections, may cause signal dispersion etc.\nLoopback attenuators.\nLoopback fiber optic attenuator is designed for testing, engineering and the burn-in stage of boards or other equipment. Available in SC/UPC, SC/APC, LC/UPC, LC/APC, MTRJ, MPO for singlemode application.900\u00a0um fiber cable inside of the black shell for LC and SC type.\nNo black shell for MTRJ and MPO type.\nBuilt-in variable attenuators.\nBuilt-in variable optical attenuators may be either manually or electrically controlled. A manual device is useful for one-time set up of a system, and is a near-equivalent to a fixed attenuator, and may be referred to as an \"adjustable attenuator\". In contrast, an electrically controlled attenuator can provide adaptive power optimization.\nAttributes of merit for electrically controlled devices, include speed of response and avoiding degradation of the transmitted signal. Dynamic range is usually quite restricted, and power feedback may mean that long term stability is a relatively minor issue. Speed of response is a particularly major issue in dynamically reconfigurable systems, where a delay of one millionth of a second can result in the loss of large amounts of transmitted data. Typical technologies employed for high speed response include liquid crystal variable attenuator (LCVA), or lithium niobate devices. There is a class of built-in attenuators that is technically indistinguishable from test attenuators, except they are packaged for rack mounting, and have no test display.\nVariable optical test attenuators.\nVariable optical test attenuators generally use a variable neutral density filter. Despite relatively high cost, this arrangement has the advantages of being stable, wavelength insensitive, mode insensitive, and offering a large dynamic range. Other schemes such as LCD, variable air gap etc. have been tried over the years, but with limited success.\nThey may be either manually or motor controlled. Motor control give regular users a distinct productivity advantage, since commonly used test sequences can be run automatically.\nAttenuator instrument calibration is a major issue. The user typically would like an absolute port to port calibration. Also, calibration should usually be at a number of wavelengths and power levels, since the device is not always linear. However a number of instruments do not in fact offer these basic features, presumably in an attempt to reduce cost. The most accurate variable attenuator instruments have thousands of calibration points, resulting in excellent overall accuracy in use.\nTest automation.\nTest sequences that use variable attenuators can be very time-consuming. Therefore, automation is likely to achieve useful benefits. Both bench and handheld-style devices are available that offer such features.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41456", "revid": "48539245", "url": "https://en.wikipedia.org/wiki?curid=41456", "title": "Optical axis", "text": "Line along which there is some degree of rotational symmetry in an optical system\nAn optical axis is an imaginary line that passes through the geometrical center of an optical system such as a camera lens, microscope or telescopic sight. Lens elements often have rotational symmetry about the axis.\nThe optical axis defines the path along which light propagates through the system, up to first approximation. For a system composed of simple lenses and mirrors, the axis passes through the center of curvature of each surface, and coincides with the axis of rotational symmetry. The optical axis is often coincident with the system's mechanical axis, but not always, as in the case of off-axis optical systems.\nFor an optical fiber, the optical axis is along the center of the fiber core, and is also known as the \"fiber axis\".\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41457", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=41457", "title": "Optical density", "text": ""}
