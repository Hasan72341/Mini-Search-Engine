{"id": "40258", "revid": "50895464", "url": "https://en.wikipedia.org/wiki?curid=40258", "title": "Harmony", "text": "Aspect of music\nIn music, harmony is the concept of combining different sounds in order to create new, distinct musical ideas. Theories of harmony seek to describe or explain the effects created by distinct pitches or tones coinciding with one another; harmonic objects such as chords, textures and tonalities are identified, defined, and categorized in the development of these theories. Harmony is broadly understood to involve both a \"vertical\" dimension (frequency-space) and a \"horizontal\" dimension (time-space), and often overlaps with related musical concepts such as melody, timbre, and form.\nA particular emphasis on harmony is one of the core concepts underlying the theory and practice of Western music. The study of harmony involves the juxtaposition of individual pitches to create chords, and in turn the juxtaposition of chords to create larger chord progressions. The principles of connection that govern these structures have been the subject of centuries worth of theoretical work and vernacular practice alike.\nDrawing both from music theoretical traditions and the field of psychoacoustics, its perception in large part consists of recognizing and processing consonance, a concept whose precise definition has varied throughout history, but is often associated with simple mathematical ratios between coincident pitch frequencies. In the physiological approach, consonance is viewed as a continuous variable measuring the human brain's ability to 'decode' aural sensory input. Culturally, \"consonant\" pitch relationships are often described as sounding more pleasant, euphonious, and beautiful than \"dissonant\" pitch relationships, which can be conversely characterized as unpleasant, discordant, or rough.\nIn popular and jazz harmony, chords are named by their root plus various terms and characters indicating their qualities. In many types of music, notably baroque, romantic, modern, and jazz, chords are often augmented with \"tensions\". A tension is an additional chord member that creates a relatively dissonant interval in relation to the bass. The notion of counterpoint seeks to understand and describe the relationships between melodic lines, often in the context of a polyphonic texture of several simultaneous but independent voices. Therefore, it is sometimes seen as a type of harmonic understanding, and sometimes distinguished from harmony. \nTypically, in the classical common practice period, a dissonant chord (chord with tension) \"resolves\" to a consonant chord. Harmonization usually sounds pleasant when there is a balance between consonance and dissonance. This occurs when there is a balance between \"tense\" and \"relaxed\" moments. Dissonance is an important part of harmony when it can be resolved and contribute to the composition of music as a whole. A misplayed note or any sound that is judged to detract from the whole composition can be described as disharmonious rather than dissonant.\nEtymology and definitions.\nThe term \"harmony\" derives from the Greek \"harmonia\", meaning \"joint, agreement, concord\", from the verb \"harmoz\u014d\", \"(\u0399) fit together, join\". Aristoxenus wrote a work entitled \"Elements of Harmony\", which is thought the first work in European history written on the subject of harmony. In this book, Aristoxenus refers to previous experiments conducted by Pythagoreans to determine the relationship between small integer ratios and consonant notes (e.g., 1:2 describes an octave relationship, which is a doubling of frequency). While identifying as a Pythagorean, Aristoxenus claims that numerical ratios are not the ultimate determinant of harmony; instead, he claims that the listener's ear determines harmony.\nCurrent dictionary definitions, while attempting to give concise descriptions, often highlight the ambiguity of the term in modern use. Ambiguities tend to arise from either aesthetic considerations (for example the view that only pleasing concords may be harmonious) or from the point of view of musical texture (distinguishing between harmonic (simultaneously sounding pitches) and \"contrapuntal\" (successively sounding tones)). According to A. Whittall:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;\nThe view that modern tonal harmony in Western music began in about 1600 is commonplace in music theory. This is usually accounted for by the replacement of horizontal (or contrapuntal) composition, common in the music of the Renaissance, with a new emphasis on the vertical element of composed music. Modern theorists, however, tend to see this as an unsatisfactory generalisation. According to Carl Dahlhaus:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;It was not that counterpoint was supplanted by harmony (Bach's tonal counterpoint is surely no less polyphonic than Palestrina's modal writing) but that an older type both of counterpoint and of vertical technique was succeeded by a newer type. And harmony comprises not only the (\"vertical\") structure of chords but also their (\"horizontal\") movement. Like music as a whole, harmony is a process.\nDescriptions and definitions of harmony and harmonic practice often show bias towards European (or Western) musical traditions, although many cultures practice vertical harmony. In addition, South Asian art music (Hindustani and Carnatic music) is frequently cited as placing little emphasis on what is perceived in western practice as conventional harmony; the underlying harmonic foundation for most South Asian music is the drone, a held open fifth interval (or fourth interval) that does not alter in pitch throughout the course of a composition. Pitch simultaneity in particular is rarely a major consideration. Nevertheless, many other considerations of pitch are relevant to the music, its theory and its structure, such as the complex system of Ragas, which combines both melodic and modal considerations and codifications within it.\nSo, intricate pitch combinations that sound simultaneously do occur in Indian classical music \u2013 but they are rarely studied as teleological harmonic or contrapuntal progressions \u2013 as with notated Western music. This contrasting emphasis (with regard to Indian music in particular) manifests itself in the different methods of performance adopted: in Indian Music, improvisation takes a major role in the structural framework of a piece, whereas in Western Music improvisation has been uncommon since the end of the 19th century. Where it does occur in Western music (or has in the past), the improvisation either embellishes pre-notated music or draws from musical models previously established in notated compositions, and therefore uses familiar harmonic schemes.\nEmphasis on the precomposed in European art music and the written theory surrounding it shows considerable cultural bias. The \"Grove Dictionary of Music and Musicians\" (Oxford University Press) identifies this clearly:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;In Western culture the musics that are most dependent on improvisation, such as jazz, have traditionally been regarded as inferior to art music, in which pre-composition is considered paramount. The conception of musics that live in oral traditions as something composed with the use of improvisatory techniques separates them from the higher-standing works that use notation.\nYet the evolution of harmonic practice and language itself, in Western art music, is and was facilitated by this process of prior composition, which permitted the study and analysis by theorists and composers of individual pre-constructed works in which pitches (and to some extent rhythms) remained unchanged regardless of the nature of the performance.\nHistorical rules.\nEarly Western religious music often features parallel perfect intervals; these intervals would preserve the clarity of the original plainsong. These works were created and performed in cathedrals, and made use of the resonant modes of their respective cathedrals to create harmonies. As polyphony developed, however, the use of parallel intervals was slowly replaced by the English style of consonance that used thirds and sixths. The English style was considered to have a sweeter sound, and was better suited to polyphony in that it offered greater linear flexibility in part-writing.\nTypes.\nCarl Dahlhaus (1990) distinguishes between \"coordinate\" and \"subordinate harmony\". \"Subordinate harmony\" is the hierarchical tonality or tonal harmony well known today. \"Coordinate harmony\" is the older Medieval and Renaissance \"tonalit\u00e9 ancienne\", \"The term is meant to signify that sonorities are linked one after the other without giving rise to the impression of a goal-directed development. A first chord forms a 'progression' with a second chord, and a second with a third. But the former chord progression is independent of the later one and vice versa.\" Coordinate harmony follows direct (adjacent) relationships rather than indirect as in subordinate. Interval cycles create symmetrical harmonies, which have been extensively used by the composers Alban Berg, George Perle, Arnold Schoenberg, B\u00e9la Bart\u00f3k, and Edgard Var\u00e8se's \"Density 21.5\".\nClose harmony and open harmony use close position and open position chords, respectively. See: Voicing (music) and Close and open harmony.\nOther types of harmony are based upon the intervals of the chords used in that harmony. Most chords in western music are based on \"tertian\" harmony, or chords built with the interval of thirds. In the chord C Major7, C\u2013E is a major third; E\u2013G is a minor third; and G to B is a major third. Other types of harmony consist of quartal and quintal harmony.\nA unison is considered a harmonic interval, just like a fifth or a third, but is unique in that it is two identical notes produced together. The unison, as a component of harmony, is important, especially in orchestration. In pop music, unison singing is usually called \"doubling\", a technique The Beatles used in many of their earlier recordings. As a type of harmony, singing in unison or playing the same notes, often using different musical instruments, at the same time is commonly called monophonic harmonization.\nIntervals.\nAn interval is the relationship between two separate musical pitches. For example, in the melody \"Twinkle Twinkle Little Star\", between the first two notes (the first \"twinkle\") and the second two notes (the second \"twinkle\") is the interval of a fifth. What this means is that if the first two notes were the pitch C, the second two notes would be the pitch G\u2014four scale notes, or seven chromatic notes (a perfect fifth), above it.\nThe following are common intervals:\nWhen tuning notes using an equal temperament, such as the 12-tone equal temperament that has become ubiquitous in Western music, each interval is created using steps of the same size, producing harmonic relations marginally 'out of tune' from pure frequency ratios as explored by the ancient Greeks. 12-tone equal temperament evolved as a compromise from earlier systems where all intervals were calculated relative to a chosen root frequency, such as just intonation and well temperament. In those systems, a major third constructed up from C did not produce the same frequency as a minor third constructed up from D\u266d. Many keyboard and fretted instruments were constructed with the ability to play, for example, both of G\u266f \"and\" A\u266d without retuning. The notes of these pairs (even those where one lacks an accidental, such as E and F\u266d) were not the 'same' note in any sense.\nUsing the diatonic scale, constructing the major and minor keys with each of the 12 notes as the tonic can be achieved using only flats \"or\" sharps to spell notes within said key, never both. This is often visualized as traveling around the circle of fifths, with each step only involving a change in one note's accidental. As such, additional accidentals are free to convey more nuanced information in the context of a passage of music and the other notes that make it up. Even when working outside diatonic contexts, it is convention, if possible, to use each letter in the alphabet only once in describing a scale.\nA note spelled as F\u266d conveys different harmonic information to the reader versus a note spelled as E. In a tuning system where two notes spelled differently are tuned to the same frequency, those notes are said to be enharmonic. Even if identical in isolation, different spellings of enharmonic notes provide meaningful context when reading and analyzing music. For example, even though E and F\u266d are enharmonic, the former is considered to be a major third up from C, while F\u266d is considered to be a diminished fourth up from C. In the context of a C major tonality, the former is the third of the scale, while the latter could (as one of numerous possible justifications) be serving the harmonic function of the third of a D\u266d minor chord, a borrowed chord within the scale.\nTherefore, the combination of notes with their specific intervals\u2014a chord\u2014creates harmony. For example, in a C chord, there are three notes: C, E, and G. The note C is the root. The notes E and G provide harmony, and in a G7 (G dominant 7th) chord, the root G with each subsequent note (in this case B, D and F) provide the harmony.\nIn the musical scale, there are twelve pitches. Each pitch is referred to as a \"degree\" of the scale. The names A, B, C, D, E, F, and G are insignificant. The intervals, however, are not. Here is an example:\nAs can be seen, no note will always be the same scale degree. The \"tonic\", or first-degree note, can be any of the 12 notes (pitch classes) of the chromatic scale. All the other notes fall into place. For example, when C is the tonic, the fourth degree or subdominant is F. When D is the tonic, the fourth degree is G. While the note names remain constant, they may refer to different scale degrees, implying different intervals with respect to the tonic. The great power of this fact is that any musical work can be played or sung in any key. It is the same piece of music, as long as the intervals are the same\u2014thus transposing the melody into the corresponding key. When the intervals surpass the perfect Octave (12 semitones), these intervals are called \"compound intervals\", which include particularly the 9th, 11th, and 13th Intervals\u2014widely used in jazz and blues Music.\nCompound Intervals are formed and named as follows:\nThese numbers don't \"add\" together because intervals are numbered inclusive of the root note (e.g. one tone up is a 2nd), so the root is counted twice by adding them. Apart from this categorization, intervals can also be divided into consonant and dissonant. As explained in the following paragraphs, consonant intervals produce a sensation of relaxation and dissonant intervals a sensation of tension. In tonal music, the term consonant also means \"brings resolution\" (to some degree at least, whereas dissonance \"requires resolution\").\nThe consonant intervals are considered the perfect unison, octave, fifth, fourth and major and minor third and sixth, and their compound forms. An interval is referred to as \"perfect\" when the harmonic relationship is found in the natural overtone series (namely, the unison 1:1, octave 2:1, fifth 3:2, and fourth 4:3). The other basic intervals (second, third, sixth, and seventh) are called \"imperfect\" because the harmonic relationships are not found mathematically exact in the overtone series. In classical music the perfect fourth above the bass may be considered dissonant when its function is contrapuntal. Other intervals, the second and the seventh (and their compound forms) are considered Dissonant and require resolution (of the produced tension) and usually preparation (depending on the music style).\nThe effect of dissonance is perceived relatively within musical context: for example, a major seventh interval alone (i.e., C up to B) may be perceived as dissonant, but the same interval as part of a major seventh chord may sound relatively consonant. A tritone (the interval of the fourth step to the seventh step of the major scale, i.e., F to B) sounds very dissonant alone, but less so within the context of a dominant seventh chord (G7 or D\u266d7 in that example).\nChords and tension.\nIn the Western tradition, in music after the seventeenth century, harmony is manipulated using chords, which are combinations of pitch classes. In tertian harmony, so named after the interval of a third, the members of chords are found and named by stacking intervals of the third, starting with the \"root\", then the \"third\" above the root, and the \"fifth\" above the root (which is a third above the third), etc. (Chord members are named after their interval above the root.) Dyads, the simplest chords, contain only two members (see power chords).\nA chord with three members is called a triad because it has three members, not because it is necessarily built in thirds (see Quartal and quintal harmony for chords built with other intervals). Depending on the size of the intervals being stacked, different qualities of chords are formed. In popular and jazz harmony, chords are named by their root plus various terms and characters indicating their qualities. To keep the nomenclature as simple as possible, some defaults are accepted (not tabulated here). For example, the chord members C, E, and G, form a C Major triad, called by default simply a C chord. In an A\u266d chord (pronounced A-flat), the members are A\u266d, C, and E\u266d.\nIn many types of music, notably baroque, romantic, modern and jazz, chords are often augmented with \"tensions\". A tension is an additional chord member that creates a relatively dissonant interval in relation to the bass. Following the tertian practice of building chords by stacking thirds, the simplest first tension is added to a triad by stacking, on top of the existing root, third, and fifth, another third above the fifth, adding a new, potentially dissonant member a seventh away from the root (called the \"seventh\" of the chord) producing a four-note chord called a \"seventh chord\".\nDepending on the widths of the individual thirds stacked to build the chord, the interval between the root and the seventh of the chord may be major, minor, or diminished. (The interval of an augmented seventh reproduces the root, and is therefore left out of the chordal nomenclature.) The nomenclature allows that, by default, \"C7\" indicates a chord with a root, third, fifth, and seventh spelled C, E, G, and B\u266d. Other types of seventh chords must be named more explicitly, such as \"C Major 7\" (spelled C, E, G, B), \"C augmented 7\" (here the word augmented applies to the fifth, not the seventh, spelled C, E, G\u266f, B\u266d), etc. (For a more complete exposition of nomenclature see Chord (music).)\nContinuing to stack thirds on top of a seventh chord produces extensions, and brings in the \"extended tensions\" or \"upper tensions\" (those more than an octave above the root when stacked in thirds), the ninths, elevenths, and thirteenths. This creates the chords named after them. (Except for dyads and triads, tertian chord types are named for the interval of the largest size and magnitude in use in the stack, not for the number of chord members : thus a ninth chord has five members \"[tonic, 3rd, 5th, 7th, 9th]\", not nine.) Extensions beyond the thirteenth reproduce existing chord members and are (usually) left out of the nomenclature. Complex harmonies based on extended chords are found in abundance in jazz, late-romantic music, modern orchestral works, film music, etc.\nTypically, in the classical common practice period a dissonant chord (chord with tension) \"resolves\" to a consonant chord. Harmonization usually sounds pleasant to the ear when there is a balance between the consonant and dissonant sounds. In simple words, that occurs when there is a balance between \"tense\" and \"relaxed\" moments. For this reason, usually tension is 'prepared' and then 'resolved', where preparing tension means to place a series of consonant chords that lead smoothly to the dissonant chord. In this way the composer ensures introducing tension smoothly, without disturbing the listener. Once the piece reaches its sub-climax, the listener needs a moment of relaxation to clear up the tension, which is obtained by playing a consonant chord that resolves the tension of the previous chords. The clearing of this tension usually sounds pleasant to the listener, though this is not always the case in late-nineteenth century music, such as \"Tristan und Isolde\" by Richard Wagner.\nPerception.\nA number of features contribute to the perception of a chord's harmony.\nTonal fusion.\nTonal fusion contributes to the perceived consonance of a chord, describing the degree to which multiple pitches are heard as a single, unitary tone. Chords which have more coinciding partials (frequency components) are perceived as more consonant, such as the octave and perfect fifth. The spectra of these intervals resemble that of a uniform tone. According to this definition, a major triad fuses better than a minor triad and a major-minor seventh chord fuses better than a major-major seventh or minor-minor seventh. These differences may not be readily apparent in tempered contexts but can explain why major triads are generally more prevalent than minor triads and major-minor sevenths are generally more prevalent than other sevenths (in spite of the dissonance of the tritone interval) in mainstream tonal music.\nIn organ registers, certain harmonic interval combinations and chords are activated by a single key. The sounds produced fuse into one tone with a new timbre. This tonal fusion effect is also used in synthesizers and orchestral arrangements; for instance, in Ravel's Bolero #5 the parallel parts of flutes, horn and celesta resemble the sound of an electric organ.\nRoughness.\nWhen adjacent harmonics in complex tones interfere with one another, they create the perception of what is known as \"beating\" or \"roughness\". These precepts are closely related to the perceived dissonance of chords. To interfere, partials must lie within a critical bandwidth, which is a measure of the ear's ability to separate different frequencies. Critical bandwidth lies between 2 and 3 semitones at high frequencies and becomes larger at lower frequencies. The roughest interval in the chromatic scale is the minor second and its inversion, the major seventh. For typical spectral envelopes in the central range, the second roughest interval is the major second and minor seventh, followed by the tritone, the minor third (major sixth), the major third (minor sixth) and the perfect fourth (fifth).\nFamiliarity.\nFamiliarity also contributes to the perceived harmony of an interval. Chords that have often been heard in musical contexts tend to sound more consonant. This principle explains the gradual historical increase in harmonic complexity of Western music. For example, around 1600 unprepared seventh chords gradually became familiar and were therefore gradually perceived as more consonant.\nIndividual characteristics such as age and musical experience also have an effect on harmony perception.\nNeural correlates of harmony.\nThe inferior colliculus is a mid-brain structure which is the first site of binaural auditory integration, processing auditory information from the left and right ears. Frequency following responses (FFRs) recorded from the mid-brain exhibit peaks in activity which correspond to the frequency components of a tonal stimulus. The extent to which FFRs accurately represent the harmonic information of a chord is called neural salience, and this value is correlated with behavioral ratings of the perceived pleasantness of chords.\nIn response to harmonic intervals, cortical activity also distinguishes chords by their consonance, responding more robustly to chords with greater consonance.\nConsonance and dissonance in balance.\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;The creation and destruction of harmonic and 'statistical' tensions is essential to the maintenance of compositional drama. Any composition (or improvisation) which remains consistent and 'regular' throughout is, for me, equivalent to watching a movie with only 'good guys' in it, or eating cottage cheese.\u2014\u200a\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\nFootnotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "40259", "revid": "48736052", "url": "https://en.wikipedia.org/wiki?curid=40259", "title": "Harthacnut", "text": "King of England (1040\u201342) and Denmark (1035\u201342)\nHarthacnut (c. 1018 \u2013 8 June 1042) was King of Denmark from 1035, and King of England from 1040 until his death in 1042. He was the last monarch of the North Sea Empire, an empire consisting of England and Denmark, and was also the last monarch of the House of Kn\u00fdtlinga.\nHarthacnut was the son of King Cnut the Great, who ruled Denmark, Norway, and England, and Emma of Normandy. After Cnut's death in 1035, Harthacnut faced challenges in retaining his father's territories. Magnus I seized control of Norway, while Harthacnut succeeded as King of Denmark. In 1040, following the death of his half-brother Harold Harefoot, he became King of England. Harthacnut died suddenly in 1042 and was succeeded by Magnus in Denmark and Edward the Confessor in England. He was the last Dane to rule England.\nEarly life.\nHarthacnut was born shortly after the marriage of his parents in July or August 1017. Cnut had set aside his first wife, \u00c6lfgifu of Northampton, to marry Emma, and according to the \"Encomium Emmae Reginae\", a book she commissioned many years later, Cnut agreed that any sons from this marriage would take precedence over the sons of his first marriage. In 1023, Emma and Harthacnut played a leading role in the translation of the body of the martyr St \u00c6lfheah from London to Canterbury. Harthacnut's biographer, Ian Howard, sees this event as recognition of his position as Cnut's heir in England.\nIn the 1020s, Denmark faced threats from Norway and Sweden. In 1026, Cnut decided to strengthen its defences by sending his eight-year-old son to Denmark as the future king under a council headed by his brother-in-law, Earl Ulf. Ulf alienated Cnut by persuading the Danish provinces to acknowledge Harthacnut as king without reference to Cnut's overall authority and by failing to take vigorous measures against Norwegian and Swedish invasions, instead waiting for Cnut's assistance. In 1027, Cnut arrived with a fleet, forgave Harthacnut his insubordination in view of his youth, but had Ulf murdered. He drove the invaders out of Denmark and established his authority over Norway, returning to England in 1028 and leaving Denmark under Harthacnut's rule.\nCnut had left Norway under the rule of H\u00e5kon Eiriksson, who drowned in a shipwreck in 1029 or 1030. Cnut then appointed his son Svein to rule Norway with the assistance of \u00c6lfgifu, Cnut's first wife and Svein's mother. They made themselves unpopular through heavy taxation and by favouring Danish advisers over the Norwegian nobility. When King Magnus I of Norway, the son of former King Olaf, invaded in 1035, Svein and \u00c6lfgifu were forced to flee to Harthacnut's court. Harthacnut was a close ally of Svein, but he did not consider his resources sufficient to launch an invasion of Norway. The half-brothers sought help from their father, only to learn of his death in November 1035.\nReign.\nHarold and Denmark.\nIn 1035, Harthacnut succeeded his father on the throne of Denmark as Cnut III. He was unable to travel to England due to the situation in Denmark, and it was agreed that Svein's full brother, Harold Harefoot, should act as regent, with Emma holding Wessex on Harthacnut's behalf. By 1037, Harold was generally accepted as king, Harthacnut being, in the words of the \"Anglo-Saxon Chronicle\", \"forsaken because he was too long in Denmark\", while Emma fled to Bruges, in Flanders. In 1039, Harthacnut sailed with ten ships to meet his mother in Bruges but delayed an invasion, as it was clear Harold was sick and would soon die, which he did in March 1040. Envoys soon crossed the Channel to offer Harthacnut the throne.\nWhile the general outline of events following Cnut's death is clear, the details are obscure, and historians offer differing interpretations. The historian M. K. Lawson states that it is unclear whether Harthacnut was intended to have England as well as Denmark, but the situation was probably reflected in a formal arrangement: mints south of the Thames produced silver pennies in his name, while those to the north were almost all Harold's. There might have been a division of the kingdom if Harthacnut had appeared immediately. He probably remained in Denmark because of the threat from Magnus of Norway, but the two eventually made a treaty by which, if either died without an heir, his kingdom would pass to the other. This may have freed Harthacnut to pursue his claim to England.\nAccording to Ian Howard, Harthacnut agreed to help Svein recover Norway and planned an invasion in 1036. Svein died shortly before the expedition was to set out, but Harthacnut proceeded nonetheless. War was avoided by the treaty between Harthacnut and Magnus, which Harthacnut accepted because he had no plausible candidate to rule Norway after Svein's death and was temperamentally inclined to avoid campaigns and wars. Howard dates the treaty to 1036, whereas other historians date it to 1039 and believe it freed Harthacnut to launch an invasion of England.\nExiled in Bruges, Emma plotted to secure the English throne for her son. She sponsored the \"Encomium Emmae Reginae\", which eulogised her and criticised Harold, particularly for arranging the murder of Alfred Atheling (the younger of Emma's two sons by \u00c6thelred) in 1036. The work describes Harthacnut's horror at hearing of his half-brother's murder and, in Howard's view, was probably influential in persuading the cautious Harthacnut to finally invade England. According to a later edition of the \"Encomium\", the English took the initiative in communicating with Harthacnut in 1039, possibly upon learning that Harold had little time to live.\nReturn to England.\nHarold died on 17 March 1040 and soon afterwards Harthacnut travelled to England with his mother, landing at Sandwich on 17 June, \"seven days before Midsummer\", in a peaceful arrival despite commanding a fleet of 62 warships. Although invited to take the throne, he came prepared as a conqueror. To reward the crews for their service, he levied a geld of over 21,000 pounds, a considerable sum that made him unpopular, although it was only a quarter of the amount his father had raised under similar circumstances in 1017\u20131018.\nHarthacnut had been horrified by Harold's murder of Alfred, and his mother demanded vengeance. With the approval of Harold's former councillors, Harold's body was disinterred from its place of honour at Westminster and publicly beheaded. It was disposed of in a sewer, but later retrieved and thrown into the Thames, from which London shipmen rescued it and had it buried in a churchyard. Godwin, the powerful earl of Wessex, had been complicit in the crime, having handed over Alfred to Harold. Queen Emma charged him in a trial before Harthacnut and his council. The king allowed Godwin to escape punishment after witnesses testified he had acted on Harold's orders, but Godwin later gave Harthacnut a richly decorated ship as wergild. Bishop Lyfing of Worcester was also charged with complicity and deprived of his see, but in 1041 he reconciled with Harthacnut and was restored.\nThe English were accustomed to a king ruling in council with the advice of his chief men, but Harthacnut had ruled autocratically in Denmark and was unwilling to change, particularly as he did not fully trust the leading earls. Initially, he successfully intimidated his subjects, though his influence waned later in his short reign. He doubled the size of the English fleet from sixteen to thirty-two ships, partly to secure forces capable of handling troubles elsewhere in his empire, and to fund it he sharply increased taxation. The increase coincided with a poor harvest, causing severe hardship. In 1041, two tax collectors in and around Worcester were killed by rioting townspeople. Harthacnut responded by ordering a then-legal but highly unpopular \"harrying\": he instructed his earls to burn the town and kill the population. Few were killed, as most had fled in advance. Citizens who had taken refuge on an island in the River Severn successfully resisted Harthacnut's troops and were allowed to return to their homes without further punishment.\nThe earl of Northumbria was Siward, but Earl Eadwulf of Bamburgh ruled the northern part semi-independently, a situation displeasing to the autocratic Harthacnut. In 1041, Eadwulf offended the king for an unknown reason but sought reconciliation. Harthacnut promised safe conduct but colluded in Eadwulf's murder by Siward, who then became earl of the whole of Northumbria. The crime was widely condemned; the \"Anglo-Saxon Chronicle\" described it as \"a betrayal\" and the king as an \"oath-breaker\".\nHarthacnut was generous to the church. Very few contemporary documents survive, but a royal charter transferred land to Bishop \u00c6lfwine of Winchester, and he made several grants to Ramsey Abbey. The 12th-century \"Ramsey Chronicle\" praises his generosity and character.\nDeath.\nHarthacnut had suffered from bouts of illness even before becoming King of England. He may have suffered from tuberculosis and was likely aware that his life expectancy was limited. In 1041, he invited his half-brother Edward the Confessor\u2014his mother Emma's son by \u00c6thelred the Unready\u2014back from exile in Normandy. The \"Anglo-Saxon Chronicle\" reports that Edward was sworn in as king. Historian M. K. Lawson notes: \"This may mean that Edward was recognized as heir of Harthacnut, who had neither wife nor children, and who is said by the slightly later Norman historian William of Poitiers to have suffered from frequent illness. The likely truth of this is suggested not only by his sudden death the following year, but also because it is otherwise difficult to see why a man in his early twenties with a normal life expectancy should have acted so.\" Harthacnut may also have been influenced by Emma, who sought to preserve her power by ensuring that one of her sons was succeeded by the other. Historian John Maddicott comments that Harthacnut must have sanctioned Edward's return and may even have promoted it, but Tom Licence disputes this, suggesting that Edward was summoned by leading magnates who had lost confidence in Harthacnut and effectively forced Edward upon him. Licence adds that no contemporary source indicates that Harthacnut was dying.\nOn 8 June 1042, Harthacnut attended a wedding in Lambeth. The groom was Tovi the Proud, and the bride was Gytha, daughter of Osgod Clapa; both men had been close to Cnut. According to the \"Anglo-Saxon Chronicle\", \"Harthacnut died as he stood at his drink, and he suddenly fell to the earth with an awful convulsion; and those who were close by took hold of him, and he spoke no word afterwards\". Licence suggests that the death does not appear to have been that of a chronically ill man.\nSuccession.\nThe political agreement between Harthacnut and Magnus the Good included the appointment of Magnus as Harthacnut's heir. At the time, this arrangement applied only to the throne of Denmark. According to the \"Heimskringla\", when Harthacnut died, Magnus extended his claim to England. He reportedly sent a letter to Edward the Confessor, asserting his claim to the English throne and threatening invasion. Magnus\u2019s own heir, Harald Hardrada, would later pursue this claim as well. Both considered themselves legal successors to Harthacnut. The \"Fagrskinna\" records Magnus proclaiming: \"I will take possession of all the Danish empire or else die in the attempt.\"\nAccording to the \"Encomium Emmae Reginae\", Edward had already served as co-ruler of England since 1041. The work emphasizes Harthacnut, Edward, and Emma acting as a trinity of rulers, in emulation of the Holy Trinity. Edward, by surviving Harthacnut, would automatically inherit the kingship. The \"Heimskringla\" depicts Edward presenting himself as brother and legal heir to both Harold Harefoot and Harthacnut, noting that he had already secured \"the support of all the people of the country\". Unstated in these accounts is that Edward\u2019s eventual marriage to Edith of Wessex strengthened his claim by gaining the political backing of her father, Godwin, Earl of Wessex, and an additional connection to Cnut, as she was a niece of the king. \nThe \"Fagrskinna\" emphasizes Edward\u2019s strong familial claim: as the son of \u00c6thelred the Unready and Emma of Normandy, half-brother of Harthacnut, stepbrother of Harold Harefoot, and stepson of Cnut, he was closely tied to multiple royal lines. England\u2019s leading nobles had already acknowledged him as king, and he was consecrated by an archbishop. He was thus widely regarded as the legitimate ruler. According to the saga, Magnus was warned: \"You can never be called king in England, and you will never be granted any allegiance there before you put an end to my life.\" This reportedly caused Magnus to doubt the strength of his claim.\nThe planned marriage between Gunhilda of Denmark, Harthacnut\u2019s sister, and Henry III, Holy Roman Emperor was intended to allow their descendants to claim the Danish throne, and potentially the English throne. From Henry\u2019s perspective, it was likely designed to give the Holy Roman Empire influence over Denmark and the western Baltic region. Gunhilda died in 1038 with no known sons. Her only daughter, Beatrice I, Abbess of Quedlinburg, never married.\nReputation.\nApart from the \"Ramsey Chronicle\", medieval sources are generally hostile to Harthacnut. According to the \"Anglo-Saxon Chronicle\", he \"did nothing worthy of a king as long as he ruled\". Modern historians, however, are less dismissive. M. K. Lawson notes that Harthacnut possessed at least two key attributes of a successful medieval king: he was \"both ruthless and feared\"; had he lived longer, the Norman Conquest might not have occurred. Ian Howard praises him for maintaining peace throughout his empire, benefiting trade and merchants, and ensuring a smooth succession by inviting Edward to his court as heir. Howard suggests that, had he lived longer, Harthacnut might have become a successful king comparable to his father.\nHenry of Huntingdon (12th century) claimed that Harthacnut ordered his court's dining tables to be \"\"laid four times a day with royal sumptuousness\", which O'Brien considers likely a popular myth. Henry framed this in the context of sharing meals with his household, suggesting that Harthacnut was more generous than contemporaries, who \"through avarice, or as they pretend through disgust, ... set but one meal a day before their dependents\"\". This account contributed to Harthacnut\u2019s image as a \"very generous bon viveur\". In contrast, Ranulf Higden (14th century) viewed the practice negatively, claiming Harthacnut insisted on two dinners and two suppers daily, influencing the English to be gluttonous and extravagant. Harthacnut\u2019s association with gluttony was well known enough to appear in Walter Scott\u2019s 1819 novel \"Ivanhoe\", where Cedric remarks about Athelstane: \"The soul of Hardicanute hath taken possession of him, and he hath no pleasure save to fill, to swill, and to call for more.\"\nThe \"Kn\u00fdtlinga saga\" treats Harthacnut\u2019s death as the end of an ancient line of kings and notes that he was the last Danish king to rule England. Otherwise, he is treated as a minor figure, with far more attention given to Cnut. The \"Morkinskinna\" covers Harthacnut\u2019s death in some detail but provides almost no information about his life, suggesting a lack of memorable achievements due to his short reign.\nThe \"Brut\" Chronicle is an Anglo-Norman work covering British and English monarchs from Brut (Brutus of Troy) to the death of Henry III in 1272. It was probably composed during the reign of Edward I (reigned 1272\u20131307), although the oldest surviving manuscript dates to 1338. The text contains numerous errors, and the original author remains unknown, though several continuations extend the narrative to the Battle of Halidon Hill (1333). The material on Harthacnut is largely positive. The author portrays Harold Harefoot as lacking chivalry, courtesy, and honour, while Harthacnut is depicted as \"a noble knight and stalwart of body, and he greatly loved knighthood and all virtues\". He praises Harthacnut\u2019s generosity with food and drink, noting that his table was open \"for all who wished to come to his court to be richly served with royal dishes\". The chronicle also commends Harthacnut for accepting his mother, Emma, back to court, highlighting his loyalty as a son.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nSources.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;\nFurther reading.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "40261", "revid": "55876", "url": "https://en.wikipedia.org/wiki?curid=40261", "title": "Harold Harefoot", "text": "King of England from 1037 to 1040\nHarold I (died 17 March 1040), commonly known as Harold Harefoot, was King of England from 1037 to 1040. His nickname \"Harefoot\" is first recorded as \"Harefoh\" or \"Harefah\" in the twelfth century in the history of Ely Abbey, and according to some late medieval chroniclers it meant that he was \"fleet of foot\".\nThe son of Cnut the Great and \u00c6lfgifu of Northampton, Harold was elected regent of England following the death of his father in 1035. He initially ruled England in place of his brother Harthacnut, who was stuck in Denmark because of a rebellion in Norway that had ousted their brother Svein. Although Harold had wished to be crowned king since 1035, \u00c6thelnoth, Archbishop of Canterbury, refused to do so. It was not until 1037 that Harold, supported by earl Leofric and many others, was officially proclaimed king. The same year, Harold's two step-brothers Edward and Alfred returned to England with a considerable military force. Alfred was captured by Earl Godwin, who had him seized and delivered to an escort of men loyal to Harefoot. While en route to Ely, he was blinded and soon after died of his wounds.\nHarold died in 1040, having ruled just five years; his half-brother Harthacnut soon returned and took hold of the kingdom peacefully. Harold was originally buried in Westminster, but Harthacnut had his body dragged up and thrown into a fen adjacent to the river Thames, from where it was reportedly recovered by a fisherman and eventually reburied in a Danish cemetery in London.\nPaternity.\nThe \"Anglo-Saxon Chronicle\" reports that Harold said that he was a son of Cnut the Great and \u00c6lfgifu of Northampton, \"although it was not true\". Florence of Worcester (12th century) elaborates on the subject. Claiming that \u00c6lfgifu wanted to have a son by the king but was unable to, she secretly adopted the newborn children of strangers and pretended to have given birth to them. Harold was reportedly the son of a cobbler, while his brother Svein Knutsson was the illegitimate son of a priest. She deceived Cnut into recognizing both children as his own.\n21st-century author Harriet O'Brien doubts Cnut, the shrewd politician who \"masterminded the bloodless takeover of Norway\", could have been deceived in such a way. She suspects the tale started out as a popular myth, or intentional defamation presumably tailored by Emma of Normandy, the other wife of Cnut and rival to \u00c6lfgifu.\nHarthacnut's reign.\nUpon the death of Cnut on 12 November 1035, Harold's younger half-brother Harthacnut, the son of Cnut and his queen Emma of Normandy, was the legitimate heir to the thrones of both the Danes and the English. Harthacnut, however, was unable to travel to his coronation in England because his Danish kingdom was under threat of invasion by King Magnus I of Norway and King Anund Jacob of Sweden. England's magnates favoured the idea of installing Harold Harefoot temporarily as regent or joint monarch, because of the difficulty of Harthacnut's absence, and despite the opposition of Godwin, the Earl of Wessex, and the Queen, he eventually wore the crown. There is some dispute in primary sources (the \"Anglo-Saxon Chronicle\") about Harold's initial role. Versions E and F mention him as regent, the others as co-ruler.\nIan Howard points out that Cnut had been survived by three sons: Svein, Harold, and Harthacnut. The \"Encomium Emmae Reginae\" also describes Edward the Confessor and Alfred Aetheling as the sons of Canute, though the modern term would be step-sons. Harold could claim the regency or kingship because he was the only one of the five present in England in 1035. Harthacnut was reigning in Denmark, and Svein had joined him there following his deposition from the Norwegian throne, while Edward and Alfred were in Normandy. Harold could reign in the name of his absent brothers, with Emma rivalling him as a candidate for the regency.\nThe \"Anglo-Saxon Chronicle\" ignores the existence of Svein, or his claim to the throne, which Howard considers as evidence of the relative entries being unreliable, of failing to give a complete picture. The Heimskringla of Snorri Sturluson claims that Svein and Harthacnut had agreed to share the kingdom between them. This agreement would include Denmark and (probably) England. Snorri quotes older sources on the subject and could be preserving valuable details.\nReign.\nAssumption of the throne.\nHarold reportedly sought coronation as early as 1035. According to the \"Encomium Emmae Reginae\", however, \u00c6thelnoth, Archbishop of Canterbury, refused to crown Harold Harefoot. Coronation by the Archbishop would be a legal requirement to become a king. \u00c6thelnoth reportedly placed the sceptre and crown on the altar of a temple, possibly that of the Canterbury Cathedral. Offering to consecrate Harold without using any of the royal regalia would have been an empty honour. He refused to remove the items from the altar and forbade any other bishop from doing so. The tale goes on that Harold failed to sway \u00c6thelnoth, as both bribes and threats proved ineffectual. The despairing Harold reportedly rejected Christianity in protest. He refused to attend church services while uncrowned, preoccupying himself with hunting and trivial matters.\nThe \"Encomium\" stays silent on an event reported by the \"Anglo-Saxon Chronicle\" and other sources. Harold was accepted as monarch in a Witenagemot held at Oxford. His chief supporter in the council was Leofric, Earl of Mercia, while the opposition was led by Godwin, Earl of Wessex. There is evidence that \u00c6lfgifu of Northampton was attempting to secure her son's position through bribes to the nobles. In 1036, Gunhilda of Denmark, sister to Harthacnut and half-sister to Harold, married Henry III, King of Germany. On this occasion Immo, a priest serving at the court of the Holy Roman Empire, wrote a letter to Azecho, Bishop of Worms. It included information on the situation in England, with messengers from there reporting that \u00c6lfgifu was gaining the support of the leading aristocrats through pleas and bribery, binding them to herself and Harold by oaths of loyalty.\nInitially, the Kingdom of England was divided between the two half-brothers. Harold ruled the areas north of the River Thames, supported by the local nobility. The southern nobility under Godwin and Emma continued to be ruled in the name of the absent Harthacnut. The \"Anglo-Saxon Chronicle\" reports that Godwin and the leading men of Wessex opposed the rule of Harold for \"...as long as they could, but they could not do anything against it.\" With the north at least on Harold's side, in adherence to the terms of a deal, which Godwin was part of, Emma was settled in Winchester, with Harthacnut's huscarls. Harold soon \"sent and had taken from her all the best treasures\" of Cnut the Great.\nThe situation could not last for long, and Godwin eventually switched sides. William of Malmesbury asserts that Godwin had been overwhelmed \"in power and in numbers\" by Harold. In 1037, Emma of Normandy fled to Bruges, Flanders, and Harold \"was everywhere chosen as king\". The details behind the event are obscure. The account of the \"Anglo-Saxon Chronicle\", version E, jumps from Harold being a mere regent to Harold being the sole king. Versions C and D do not even make a distinction between the two phases. Ian Howard theorises that the death of Svein Knutsson could have strengthened Harold's position. He went from being the second surviving son of Cnut to being the eldest living, with Harthacnut still absent and unable to press his claim to the throne.\nHarold himself is somewhat obscure; the historian Frank Stenton considered it probable that his mother \u00c6lfgifu was \"the real ruler of England\" for part or all of his reign. Kelly DeVries points out that during the High Middle Ages, royal succession in Northern Europe was determined by military power. The eldest son of a king could have a superior right of inheritance but still lose the throne to a younger brother, or other junior claimant, possessing greater military support. Harold managed to win the throne against the superior claim of Harthacnut in this way. The 11th century provides other similar examples. Magnus I of Norway (reigned 1035\u20131047), who wasn't a warlord, had reigned for more than a decade when his uncle Harald Hardrada (reigned 1047\u20131066) challenged his rule. With Harald being a famous military leader, his claim would end Magnus' reign early. Baldwin VI, Count of Flanders (reigned 1067\u20131070) was effectively succeeded by his brother Robert I (reigned 1071\u20131093), rather than his own sons. Robert Curthose, Duke of Normandy (reigned 1087\u20131106) lost the throne of England to his younger brothers William II (reigned 1087\u20131100) and Henry I (reigned 1100\u20131135).\nWith the Kingdom of England practically owned by Harold, Harthacnut could not even approach without securing sufficient military strength. His decision to remain in Denmark probably points to him lacking sufficient support, though he would certainly wait for an opportunity to forcefully assert his claim and depose his half-brother. Harold reigned as sole king from 1037 to 1040. There are few surviving documents about events of his reign. The \"Anglo-Saxon Chronicle\" mostly covers church matters, such as the deaths and appointments of bishops and archbishops. There is, however, a record of a skirmish between the Anglo-Saxons and the Welsh in 1039. The named casualties were Eadwine (Edwin), brother to Leofric, Earl of Mercia, Thurkil, and \u00c6lfgeat, but there are no other details concerning this event. Also in 1039, there is mention of a great gale, again with no details.\nReturn of \u00c6lfred and Edward.\nIn 1036, \u00c6lfred \u00c6theling, son of Emma by the long-dead \u00c6thelred, returned to the kingdom from exile in the Duchy of Normandy with his brother Edward the Confessor, with some show of arms. Their motivation is uncertain. William of Poitiers claimed that they had come to claim the English throne for themselves. Frank Barlow suspected that Emma had invited them, possibly to use them against Harold. If so, it could mean that Emma had abandoned the cause of Harthacnut, probably to strengthen her own position, but that could have inspired Godwin to also abandon the lost cause.\nThe \"Encomium Emmae Reginae\" claims that Harold himself had lured them to England, having sent them a forged letter, supposedly written by Emma. The letter reportedly both decried Harold's behaviour against her and urged her estranged sons to come and protect her. Barlow and other modern historians suspect that this letter was genuine. Ian Howard argued that Emma not being involved in a major political manoeuvre would be \"\"out of character for her\", and the Encomium was probably trying to mask her responsibility for a blunder. William of Jumi\u00e8ges reports that earlier in 1036, Edward had conducted a successful raid of Southampton, managing to win a victory against the troops defending the city and then sailing back to Normandy \"richly laden with booty\"\", but the swift retreat confirms William's assessment that Edward would need a larger army to seriously claim the throne.\nWith his bodyguard, according to the \"Anglo-Saxon Chronicle\", \u00c6lfred intended to visit his mother, Emma, in Winchester, but he may have made this journey for reasons other than a family reunion. As the \"murmur was very much in favour of Harold\", on the direction of Godwin (now apparently on the side of Harold Harefoot), \u00c6lfred was captured. Godwin had him seized and delivered to an escort of men loyal to Harefoot. He was transported by ship to Ely, and blinded while on board. He died in Ely soon afterwards from his wounds, his bodyguard being similarly treated. The event would later affect the relationship between Edward and Godwin, the Confessor holding Godwin responsible for the death of his brother.\nThe failed invasion shows that Harold Harefoot, as a son and successor to Cnut, had gained the support of Anglo-Danish nobility, which violently rejected the claims of \u00c6lfred, Edward, and (by extension) the Aethelings. The House of Wessex had lost support among the nobility of the Kingdom. It might also have served as a turning point in the struggle between Harold and Emma that resulted in Emma's exile.\nDeath.\nHarold died at Oxford on 17 March 1040, just as Harthacnut was preparing an invasion force of Danes, and was buried at Westminster Abbey. His body was subsequently exhumed, beheaded, and thrown into a fen bordering the Thames when Harthacnut assumed the throne in June 1040. The body was subsequently recovered by fishermen, and resident Danes reportedly had it reburied at their local cemetery in London. The body was eventually buried in a church in the City of Westminster, which was fittingly named St. Clement Danes. A contradictory account in the \"Kn\u00fdtlinga saga\" (13th century) reports Harold buried in the city of Morstr, alongside his half-brother Harthacnut and their father Cnut. While mentioned as a great city in the text, nothing else is known of Morstr. The \"Heimskringla\" by Snorri Sturluson reports Harold Harefoot to have been buried at Winchester, again alongside Cnut and Harthacnut.\nThe cause of Harold's death is uncertain. Katherine Holman attributes the death to \"a mysterious illness\". An Anglo-Saxon charter attributes the illness to divine judgment. Harold had reportedly claimed Sandwich for himself, thereby depriving the monks of Christchurch. Harold is described as lying ill and in despair at Oxford. When monks came to him to settle the dispute over Sandwich, he \"lay and grew black as they spoke\". The context of the event was a dispute between Christchurch and St Augustine's Abbey, which took over the local toll in the name of the king. There is little attention paid to the illness of the king. Harriet O'Brien feels this is enough to indicate that Harold died of natural causes, but not to determine the nature of the disease. The Anglo-Saxons themselves would consider him elf-shot (attacked by elves), their term for any number of deadly diseases. Michael Evans points out that Harold was only one of several youthful kings of pre-Conquest England to die following short reigns. Others included Edmund I (reigned 939\u2013946, murdered at age 25), Eadred (reigned 946\u2013955, died at age 32), Eadwig (reigned 955\u2013959, died at age 19), Edmund Ironside (reigned 1016, died at age 26), and Harthacnut (reigned 1040\u20131042, who would die at age 24). Evans wonders whether the role of king was dangerous in this era, more so than in the period after the Conquest, or whether hereditary diseases were in effect since most of these kings were members of the same lineage, the House of Wessex.\nIt is unclear why a king would have been buried at Westminster Abbey. The only previous royals reportedly buried there were S\u00e6berht of Essex and his wife \u00c6thelgoda. Emma Mason speculates that Cnut had built a royal residence in the vicinity of the Abbey, or that Westminster held some significance to the Danish Kings of England, which would also explain why Harthacnut would not allow a usurper to be buried there. The lack of detail in the \"Anglo-Saxon Chronicle\" implies that, for its compilers, the main point of interest was not the burial site, but the exhumation of the body. Harriet O'Brien theorises that the choice of location might simply reflect the political affiliation of the area of Westminster and nearby London, being a power base for Harold.\nA detailed account of the exhumation appears in the writings of John of Worcester (12th century). The group tasked with the mission was reportedly led by \u00c6lfric Puttoc, Archbishop of York, and Godwin, Earl of Wessex. The involvement of such notable men would have had a significance of its own, giving the event an official nature and avoiding secrecy. Emma Mason suspects that this could also serve as a punishment for Godwin, who had served as a chief supporter of Harold, and was now charged with the gruesome task.\nOffspring.\nHarold may have had a wife, \u00c6lfgifu, and a son, \u00c6lfwine, who became a monk on the continent when he was older \u2013 his monastic name was Alboin. \u00c6lfwine/Alboin is recorded in 1060 and 1062 in charters from the Abbey Church of Saint Foy in Conques, which mention him as son of \"Heroldus rex fuit Anglorum\" (Latin: Harold, who was king of the English People). Harold Harefoot is the most likely father as the only other king Harold was Harold Godwinson, who would not rise to the throne until 1066. Either way, an underage boy would be unable to claim the throne in 1040. His possible hereditary claims would not be enough to gain the support of the leading nobles against the adult Harthacnut.\n\u00c6lfgifu of Northampton disappears with no trace after 1040. According to the \"Anglo-Saxon Chronicle\", Harold Harefoot ruled for four years and sixteen weeks, by which calculation he would have begun ruling two weeks after the death of Cnut.\nReputation.\nThe \"Prose Brut chronicle\" was an Anglo-Norman work, covering British and English monarchs from Brut (Brutus of Troy) to the death of Henry III in 1272. It was probably written during the reign of Edward I (reigned 1272\u20131307), though the oldest surviving manuscript dates to 1338. The text often includes notable errors. The original author remains unknown, but there were a number of continuations by different hands, extending the story to the Battle of Halidon Hill (1333). The material on Harold Harefoot is rather unflattering. The author considered both Harold and Harthacnut to have been sons of Cnut and Emma of Normandy. He proceeds to portray Harold as follows: \"...He went astray from the qualities and conduct of his father King Cnut, for he cared not at all for knighthood, for courtesy, or for honour, but only for his own will...\". He accuses Harold of driving his own mother Emma out of England, by the advice of Godwin, Earl of Wessex. He paints Harthacnut in a more favorable light.\nThe \"Kn\u00fdtlinga saga\" (13th century) considers Harold Harefoot to be the oldest son of Cnut and Emma of Normandy, though its author frequently misrepresents family relationships. Harthacnut and Gunhilda of Denmark are regarded in the text as his younger siblings. The narrative has Harold and Harthacnut dividing the realms of their father in an agreement. It also features Harold offering hospitality to his half-brother Edward the Confessor, but they were actually step-brothers, and Edward only settled in England following the death of Harold.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nSources.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;\nFurther reading.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "40269", "revid": "20836525", "url": "https://en.wikipedia.org/wiki?curid=40269", "title": "Derived subgroup", "text": ""}
{"id": "40270", "revid": "46503443", "url": "https://en.wikipedia.org/wiki?curid=40270", "title": "Vibrator", "text": "Vibrator may refer to:\nOther.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "40272", "revid": "12607721", "url": "https://en.wikipedia.org/wiki?curid=40272", "title": "Industrial sociology", "text": "Branch of the discipline of sociology\nIndustrial sociology, until recently a crucial research area within the field of sociology of work, examines\n\"the direction and implications of trends in technological change, globalization, labour markets, work organization, managerial practices and employment relations\" to \"the extent to which these trends are intimately related to changing patterns of inequality in modern societies and to the changing experiences of individuals and families\", and \" the ways in which workers challenge, resist and make their own contributions to the patterning of work and shaping of work institutions\".\nLabour process theory.\nOne branch of industrial sociology is labour process theory (LPT). In 1974, Harry Braverman wrote \"Labor and Monopoly Capital\", which provided a critical analysis of scientific management. This book analysed capitalist productive relations from a Marxist perspective. Following Marx, Braverman argued that work within capitalist organizations was exploitative and alienating, and therefore workers had to be coerced into servitude. For Braverman the pursuit of capitalist interests over time ultimately leads to deskilling and routinization of the worker. The Taylorist work design is the ultimate embodiment of this tendency.\nBraverman demonstrated several mechanisms of control in both the factory blue-collar and clerical white-collar labour force.\nHis key contribution is his \"deskilling\" thesis. Braverman argued that capitalist owners and managers were incessantly driven to deskill the labour force to lower production costs and ensure higher productivity. Deskilled labour is cheap and above all easy to control due to the workers' lack of direct engagement in the production process. In turn work becomes intellectually or emotionally unfulfilling; the lack of capitalist reliance on human skill reduces the need of employers to reward workers in anything but a minimal economic way.\nBraverman's contribution to the sociology of work and industry (i.e., industrial sociology) has been important and his theories of the labour process continue to inform teaching and research. Braverman's thesis has, however, been contested, by Andrew Freidman in his work \"Industry and Labour\" (1977). In it, Freidman suggests that whilst the direct control of labour is beneficial for the capitalist under certain circumstances, a degree of \"responsible autonomy\" can be granted to unionized or \"core\" workers, in order to harness their skill under controlled conditions. Also, Richard Edwards showed in 1979 that although hierarchy in organizations has remained constant, additional forms of control (such as technical control via email monitoring, call monitoring; bureaucratic control via procedures for leave, sickness etc.) have been added to gain the interests of the capitalist class versus the workers. Duncan Gallie has shown how important it is to approach the question of skill from a social class perspective. In his study, the majority of non-manual, intermediate and skilled manual workers believed that their work had come to demand a higher level of skill, but the majority of manual workers felt that the responsibility and skill needed in their work had either remained constant or declined. This implies that Braverman's claims can't be applied to all social classes.\nThe notion the particular type of technology workers were exposed to shapes their experience was most forcefully argued in a classic study by Robert Blauner. He argued that some work is alienating more than other types because of the different technologies workers use. Alienation, to Blauner, has four dimensions: powerlessness, meaninglessness, isolation, and self-estrangement. Individuals are powerless when they can't control their own actions or conditions of work; work is meaningless when it gives employees little or no sense of value, interest or worth; work is isolating when workers cannot identify with their workplace; and work is self-estranging when, at the subjective level, the worker has no sense of involvement in the job.\nBlauner's claims however fail to recognize that the same technology can be experienced in a variety of ways. Studies have shown that cultural differences with regard to management\u2013union relations, levels of hierarchical control, and reward and performance appraisal policies mean that the experience of the same kind of work can vary considerably between countries and firms. The individualization of work and the need for workers to have more flexible skills in order to respond to technological changes means that Blauner's characterization of work experience is no longer valid. Additionally, workers today may work in teams to alleviate workers' sense of alienation, since they are involved in the entire process, rather than just a small part of it. In conclusion, automative technologies and computerized work systems have typically enhanced workers' job satisfaction and skill deployment in the better-paid, secure public and private sector jobs. But, in more non-skilled manual work, they have just perpetuated job dissatisfaction, especially for the many women involved in this type of work.\nReferences.\nFootnotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nBibliography.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;\nFurther reading.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "40275", "revid": "118722", "url": "https://en.wikipedia.org/wiki?curid=40275", "title": "Kayaking", "text": "Use of a kayak on water\nKayaking is the use of a kayak for moving over water. It is distinguished from canoeing by the sitting position of the paddler and the number of blades on the paddle. A kayak is a low-to-the-water, canoe-like boat in which the paddler sits facing forward, legs in front, using a double-bladed paddle to pull front-to-back on one side and then the other in rotation. Most kayaks have closed decks, although sit-on-top and inflatable kayaks are growing in popularity as well.\nHistory.\nKayaks were created thousands of years ago by Inuit of the northern Arctic regions. They used driftwood and sometimes the skeleton of whales, to construct the frame of the kayak, and animal skin, particularly seal skin was used to create the body. The main purpose for creating the kayak, which literally translates to \"hunter's boat\" was for hunting and fishing. The kayak's stealth capabilities allowed for the hunter to sneak up behind animals on the shoreline and successfully catch their prey.\nIn the 1740s, Russian explorers led by the Danish-born Russian explorer Vitus Bering came in contact with the Aleutians, who had taken the basic kayak concept and developed multiple designs specifically for hunting, transportation, and environmental conditions. They soon recognized the Aleutians were very skillful at hunting sea otters by kayak. Because otters were a popular commodity in Europe and Asia, they would exploit and even kidnap Aleutians and keep them aboard their ships to work and hunt.\nBy the mid-1800s the kayak became increasingly popular and the Europeans became interested. German and French men began kayaking for sport. In 1931, Adolf Anderle was the first person to kayak down the Salzach\u00f6fen Gorge, which is believed to be the birthplace of modern-day white-water kayaking. In 1932, long-distance kayaker Fridel Meyer paddled from Bavaria to Westminster, England, and in 1933 she paddled from Westminster to Montrose, Scotland. Kayak races were introduced as an official medal sport at the 1936 Summer Olympics in Berlin, after being a demonstration sport at the 1924 games.\nIn the 1950s, fiberglass kayaks were developed and commonly used, until the 1980s when polyester and polyethylene plastic kayaks were introduced. Kayaking progressed as a fringe sport in the U.S. until the 1970s, when it became a mainstream popular sport. Now, more than 10 white water kayaking events are featured in the Olympics. While kayaking represents a key international watersport, few academic studies have (to date) been conducted on the role kayaking plays in the lives and activities of the public \nDesign.\nKayaks can also be classified by their design and the materials from which they are made. Each design has its specific advantages, including performance, maneuverability, stability, and paddling style. Kayaks can be made of metal, fiberglass, wood, plastic, fabrics, and inflatable fabrics such as PVC or rubber, and more recently expensive but feather-light carbon fiber. Each material also has its specific advantages, including strength, durability, portability, flexibility, resistance to ultraviolet, and storage requirements. For example, wooden kayaks can be created from kits or built by hand. Stitch and glue, plywood kayaks can be lighter than any other material except skin-on frame. Inflatable kayaks, made from lightweight fabric, can be deflated and easily transported and stored, and are considered to be remarkably tough and durable compared to some hard-sided boats.\nEquipment.\nThere are many types of kayaks used in flat water and whitewater kayaking. The sizes and shapes vary drastically depending on what type of water to be paddled on and also what the paddler would like to do. The second set of essentials for kayaking is an off-set paddle where the paddle blades are tilted to help reduce wind resistance while the other blade is being used in the water. These vary in length and also shape depending on the intended use, the height of the paddler, and the paddler's preference. Kayaks should be equipped with one or more buoyancy aid (also called flotation) which creates air space that helps prevent a kayak from sinking when filled with water. A life jacket should be worn at all times (also called a personal flotation device or PFD), and a helmet is also often required for most kayaking and is mandatory for white water kayaking. Various other pieces of safety gear include a whistle for signaling for help; throwing ropes to help rescue other kayakers; and, a diving knife and appropriate water shoes should be used depending upon the risks the water and terrain pose. Proper clothing such as a dry suit, wetsuit or spray top also help protect kayakers from cold water or air temperatures.\nTypes of kayaks.\n\"Sit on top\" kayaks place the paddler in an open, shallowly concave deck above the water level. This style is usually used for non-white water activities as most find it harder to stay inside the kayak while also preventing them from \"rolling\" which allows the user to upright themselves if they flip over. There are some benefits to sit on tops such as the ability for a \"dry hatch\" these are a compartment, that usually runs the length of the kayak, which in addition to providing more buoyancy allows for the kayaker to store various equipment. \"Sit on top\" kayaks often use \"through holes\" which allows any water that got in the boat to make it through the deck and dry hatch to drain. \"Cockpit style\" involves sitting with the legs and hips inside the kayak hull with a spray deck or \"spray skirt\" that creates a water-resistant seal around the waist. There is a wide range of \"cockpit style\" boats which usually allow for more user control of the boat as they are able to push against the walls of the boat to tip in order to complete maneuvers. A common variant of \"cockpit style\" kayaks are \"play boats\" these are usually very short kayaks in which the user does tricks and maneuvers: \"Inflatables\" are a hybrid of the two previous configurations; these boats have an open deck, but the paddler sits below the level of the deck. These boats are often subject to more instability due to the way the boat sits higher in the water. They are often used in a more commercial setting, they are often affectionately called \"Duckies\". \"Tandems\" are configured for multiple paddlers, in contrast to the single person designs featured by most kayaks. Tandems can be used by two or even three paddlers.\nActivities.\nBecause of their range and adaptability, kayaks can be useful for other outdoor activities such as diving, fishing, wilderness exploration and search and rescue during floods.\nDiving.\nKayak diving is a type of recreational diving where the divers paddle to a diving site in a kayak carrying all their gear to the place they want to dive. The range can be up to several kilometres along the coastline from the launching point to a place where access would be difficult from the shore, although the sea is sheltered. It is a considerably cheaper alternative to using a powered boat, as well as combining the experience of sea kayaking at the same time. Kayak diving gives the diver independence from dive boat operators, while allowing dives at sites which are too far to comfortably swim, but are sufficiently sheltered.\nFishing.\nKayak fishing is fishing from a kayak, long a means of transportation and stealthily approaching easily spooked fish, such as cobia and flounder. Kayak fishing has gained popularity in recent times due to its broad appeal as an environmentally friendly and healthy method of transportation, as well as its relatively low cost of entry compared to motorized boats. In addition, kayaks allow greater access by their ability to operate in shallow water, getting in and out along the shoreline, and having the ability to get away from the crowds to find a more solitary environment where boats may not have the ability to do so.\nEcotourism.\nEcotourism based on kayak trips is gaining in popularity. In warm-water vacation destinations such as Sarasota Keys, guided kayak trips take kayakers on a tour of the local ecosystem. Kayakers can watch dolphins breach and manatees eat seagrass, in shallow bay water.\nWhitewater.\nOne of the most common uses of kayaks for hobbyists is whitewater kayaking. Whitewater kayaking is when a kayaker traverses down a series of rapids. The difficulty of these rapid ranges from Class I to Class VI. The difficulty of rapids often changes with water level and debris in the river. Debris that inhibits a kayaker's path are often called \"strainers\" as they \"strain\" out the kayakers like a colander. There are often training camps as well as man-made structures to help train kayakers.\nSurf ski.\nA surfski (or: \"surf ski\", \"surf-ski\") is generally the longest of all kayaks and is a performance oriented kayak designed for speed on open water, most commonly the ocean, although it is well suited to all bodies of water and recreational paddling.\nWinter kayaking.\nWinter kayaking is inherently more dangerous than regular paddling, as cold water and air temperatures can quickly lead to hypothermia. The activity requires the use of clothing that negates the effects of the elements on the kayaker.\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "40276", "revid": "18148185", "url": "https://en.wikipedia.org/wiki?curid=40276", "title": "Blackboard bold", "text": "Typeface style used in Mathematics\nBlackboard bold is a style of writing bold symbols on a blackboard by doubling certain strokes, commonly used in mathematical lectures, and the derived style of typeface used in printed mathematical texts. The style is most commonly used to represent the number sets formula_1 (natural numbers), formula_2 (integers), formula_3 (rational numbers), formula_4 (real numbers), and formula_5 (complex numbers).\nTo imitate a bold typeface on a typewriter, a character can be typed over itself (called \"double-striking\"); symbols thus produced are called double-struck, and this name is sometimes adopted for blackboard bold symbols, for instance in Unicode grapheme names.\nIn typography, a typeface with characters that are not solid is called \"inline\", \"handtooled\", or \"open face\".\nHistory.\nTraditionally, various symbols were indicated by boldface in print but on blackboards and in manuscripts \"by wavy underscoring, or enclosure in a circle, or even by wavy overscoring\".\nMost typewriters have no dedicated bold characters at all. To produce a bold effect on a typewriter, a character can be \"double-struck\" with or without a small offset. By the mid-1960s, typewriter accessories such as the \"Doublebold\" could automatically double-strike every character while engaged. While this method makes a character bolder, and can effectively emphasize words or passages, in isolation a double-struck character is not always clearly different from its single-struck counterpart.\nBlackboard bold originated from the attempt to write bold symbols on typewriters and blackboards that were legible but distinct, perhaps starting in the late 1950s in France, and then taking hold at the Princeton University mathematics department in the early 1960s. Mathematical authors began typing faux-bold letters by double-striking them with a significant offset or over-striking them with the letter \"I\", creating new symbols such as\nIR,\nIN,\nCC,\nor ZZ;\nat the blackboard, lecturers began writing bold symbols with certain doubled strokes. The notation caught on: blackboard bold spread from classroom to classroom and is now used around the world.\nThe style made its way into print starting in the mid-1960s. Early examples include Robert Gunning and Hugo Rossi's \"Analytic Functions of Several Complex Variables\" (1965) and Lynn Loomis and Shlomo Sternberg's \"Advanced Calculus\" (1968). Initial adoption was sporadic, however, and most publishers continued using boldface. In 1979, Wiley recommended its authors avoid \"double-backed shadow or outline letters, sometimes called blackboard bold\", because they could not always be printed; in 1982, Wiley refused to include blackboard bold characters in mathematical books because the type was difficult and expensive to obtain.\nDonald Knuth preferred boldface to blackboard bold and so did not include blackboard bold in the Computer Modern typeface that he created for the TeX mathematical typesetting system he first released in 1978. When Knuth's 1984 \"The TeXbook\" needed an example of blackboard bold for the index, he produced formula_6 using the letters \"I\" and \"R\" with a negative space between; in 1988 Robert Messer extended this to a full set of \"poor man's blackboard bold\" macros, overtyping each capital letter with carefully placed \"I\" characters or vertical lines.\nNot all mathematical authors were satisfied with such workarounds. The American Mathematical Society created a simple chalk-style blackboard bold typeface in 1985 to go with the AMS-TeX package created by Michael Spivak, accessed using the codice_1 command (for \"blackboard bold\"); in 1990, the AMS released an update with a new inline-style blackboard bold font intended to better match Times. Since then, a variety of other blackboard bold typefaces have been created, some following the style of traditional inline typefaces and others closer in form to letters drawn with chalk.\nUnicode included the most common blackboard bold letters among the \"Letterlike Symbols\" in version 1.0 (1991), inherited from the Xerox Character Code Standard. Later versions of Unicode extended this set to all uppercase and lowercase Latin letters and a variety of other symbols, among the \"Mathematical Alphanumeric Symbols\".\nIn professionally typeset books, publishers and authors have gradually adopted blackboard bold, and its use is now commonplace, but some still use ordinary bold symbols. Some authors use blackboard bold letters on the blackboard or in manuscripts, but prefer an ordinary bold typeface in print; for example, Jean-Pierre Serre has used blackboard bold in lectures, but has consistently used ordinary bold for the same symbols in his published works. The \"Chicago Manual of Style\"'s recommendation has evolved over time: In 1993, for the 14th edition, it advised that \"blackboard bold should be confined to the classroom\" (13.14); In 2003, for the 15th edition, it stated that \"open-faced (blackboard) symbols are reserved for familiar systems of numbers\" (14.12). The international standard ISO 80000-2:2019 lists R as the symbol for the real numbers but notes \"the symbols IR and formula_4 are also used\", and similarly for N, Z, Q, C, and P (prime numbers).\nEncoding.\nTeX, the standard typesetting system for mathematical texts, does not contain direct support for blackboard bold symbols, but the American Mathematical Society distributes the AMSFonts collection, loaded from the codice_2 package, which includes a blackboard bold typeface for uppercase Latin letters accessed using codice_3 (e.g. codice_4 produces formula_8).\nIn Unicode, a few of the more common blackboard bold characters (\u2102, \u210d, \u2115, \u2119, \u211a, \u211d, and \u2124) are encoded in the Basic Multilingual Plane (BMP) in the \"Letterlike Symbols\" (2100\u2013214F) area, named DOUBLE-STRUCK CAPITAL C etc. The rest, however, are encoded outside the BMP, in \"Mathematical Alphanumeric Symbols\" (1D400\u20131D7FF), specifically from 1D538\u20131D550 (uppercase, excluding those encoded in the BMP), 1D552\u20131D56B (lowercase), and 1D7D8\u20131D7E1 (digits). Blackboard bold Arabic letters are encoded in Arabic Mathematical Alphabetic Symbols (1EE00\u20131EEFF), specifically 1EEA1\u20131EEBB.\nUsage.\nThe following table shows all available Unicode blackboard bold characters.\nThe first column shows the letter as typically rendered by the LaTeX markup system. The second column shows the Unicode code point. The third column shows the Unicode symbol itself (which will only display correctly on browsers that support Unicode and have access to a suitable typeface). The fourth column describes some typical usage in mathematical texts. Some of the symbols (particularly formula_9 and formula_10) are nearly universal in their interpretation, while others are more varied in use.\nIn addition, a blackboard-bold \u03bc\"n\" (not found in Unicode or LaTeX) is sometimes used by number theorists and algebraic geometers to designate the group scheme of \"n\"-th roots of unity.\nNote: only uppercase Roman letters are given LaTeX renderings because Wikipedia's implementation uses the AMSFonts blackboard bold typeface, which does not support other characters.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "40277", "revid": "5042921", "url": "https://en.wikipedia.org/wiki?curid=40277", "title": "Corpus linguistics", "text": "Branch of linguistics that studies language through examples contained in real texts\nCorpus linguistics is an empirical method for the study of language by way of a text corpus (plural \"corpora\"). Corpora are balanced, often stratified collections of authentic, \"real world\", text of speech or writing that aim to represent a given linguistic variety. Today, corpora are generally machine-readable data collections. \nCorpus linguistics proposes that a reliable analysis of a language is more feasible with corpora collected in the field\u2014the natural context (\"realia\") of that language\u2014with minimal experimental interference. Large collections of text, though corpora may also be small in terms of running words, allow linguists to run quantitative analyses on linguistic concepts that may be difficult to test in a qualitative manner.\nThe text-corpus method uses the body of texts in any natural language to derive the set of abstract rules which govern that language. Those results can be used to explore the relationships between that subject language and other languages which have undergone a similar analysis. The first such corpora were manually derived from source texts, but now that work is automated.\nCorpora have not only been used for linguistics research, they have been increasingly used to compile dictionaries (starting with \"The American Heritage Dictionary of the English Language\" in 1969) and reference grammars, with \"A Comprehensive Grammar of the English Language\", published in 1985, as a first.\nExperts in the field have differing views about the annotation of a corpus. These views range from John McHardy Sinclair, who advocates minimal annotation so texts speak for themselves, to the Survey of English Usage team (University College, London), who advocate annotation as allowing greater linguistic understanding through rigorous recording.\nHistory.\nSome of the earliest efforts at grammatical description were based at least in part on corpora of particular religious or cultural significance. For example, Pr\u0101ti\u015b\u0101khya literature described the sound patterns of Sanskrit as found in the Vedas, and \nP\u0101\u1e47ini's grammar of classical Sanskrit was based at least in part on analysis of that same corpus. Similarly, the early Arabic grammarians paid particular attention to the language of the Quran. In the Western European tradition, scholars prepared concordances to allow detailed study of the language of the Bible and other canonical texts.\nEnglish corpora.\nA landmark in modern corpus linguistics was the publication of \"Computational Analysis of Present-Day American English\" in 1967. Written by Henry Ku\u010dera and W. Nelson Francis, the work was based on an analysis of the Brown Corpus, which is a structured and balanced corpus of one million words of American English from the year 1961. The corpus comprises 2000 text samples, from a variety of genres. The Brown Corpus was the first computerized corpus designed for linguistic research. Ku\u010dera and Francis subjected the Brown Corpus to a variety of computational analyses and then combined elements of linguistics, language teaching, psychology, statistics, and sociology to create a rich and variegated opus. A further key publication was Randolph Quirk's \"Towards a description of English Usage\" in 1960 in which he introduced the Survey of English Usage. Quirk's corpus was the first modern corpus to be built with the purpose of representing the whole language.\nShortly thereafter, Boston publisher Houghton-Mifflin approached Ku\u010dera to supply a million-word, three-line citation base for its new \"American Heritage Dictionary\", the first dictionary compiled using corpus linguistics. The \"AHD\" took the innovative step of combining prescriptive elements (how language \"should\" be used) with descriptive information (how it actually \"is\" used).\nOther publishers followed suit. The British publisher Collins' COBUILD monolingual learner's dictionary, designed for users learning English as a foreign language, was compiled using the Bank of English. The Survey of English Usage Corpus was used in the development of one of the most important Corpus-based Grammars, which was written by Quirk \"et al.\" and published in 1985 as \"A Comprehensive Grammar of the English Language\".\nThe Brown Corpus has also spawned a number of similarly structured corpora: the LOB Corpus (1960s British English), Kolhapur (Indian English), Wellington (New Zealand English), Australian Corpus of English (Australian English), the Frown Corpus (early 1990s American English), and the FLOB Corpus (1990s British English). Other corpora represent many languages, varieties and modes, and include the International Corpus of English, and the British National Corpus, a 100 million word collection of a range of spoken and written texts, created in the 1990s by a consortium of publishers, universities (Oxford and Lancaster) and the British Library. For contemporary American English, work has stalled on the American National Corpus, but the 400+ million word Corpus of Contemporary American English (1990\u2013present) is now available through a web interface.\nThe first computerized corpus of transcribed spoken language was constructed in 1971 by the Montreal French Project, containing one million words, which inspired Shana Poplack's much larger corpus of spoken French in the Ottawa-Hull area.\nMultilingual corpora.\nIn the 1990s, many of the notable early successes on statistical methods in natural-language programming (NLP) occurred in the field of machine translation, due especially to work at IBM Research. These systems were able to take advantage of existing multilingual textual corpora that had been produced by the Parliament of Canada and the European Union as a result of laws calling for the translation of all governmental proceedings into all official languages of the corresponding systems of government.\nThere are corpora in non-European languages as well. For example, the National Institute for Japanese Language and Linguistics in Japan has built a number of corpora of spoken and written Japanese. Sign language corpora have also been created using video data.\nAncient languages corpora.\nBesides these corpora of living languages, computerized corpora have also been made of collections of texts in ancient languages. An example is the Andersen-Forbes database of the Hebrew Bible, developed since the 1970s, in which every clause is parsed using graphs representing up to seven levels of syntax, and every segment tagged with seven fields of information. The Quranic Arabic Corpus is an annotated corpus for the Classical Arabic language of the Quran. This is a recent project with multiple layers of annotation including morphological segmentation, part-of-speech tagging, and syntactic analysis using dependency grammar. The Digital Corpus of Sanskrit (DCS) is a \"Sandhi-split corpus of Sanskrit texts with full morphological and lexical analysis... designed for text-historical research in Sanskrit linguistics and philology.\"\nCorpora from specific fields.\nBesides pure linguistic inquiry, researchers had begun to apply corpus linguistics to other academic and professional fields, such as the emerging sub-discipline of Law and Corpus Linguistics, which seeks to understand legal texts using corpus data and tools. The DBLP Discovery Dataset concentrates on computer science, containing relevant computer science publications with sentient metadata such as author affiliations, citations, or study fields. A more focused dataset was introduced by NLP Scholar, a combination of papers of the ACL Anthology and Google Scholar metadata. Corpora can also aid in translation efforts or in teaching foreign languages.\nMethods.\nCorpus linguistics has generated a number of research methods, which attempt to trace a path from data to theory. Wallis and Nelson (2001) first introduced what they called the 3A perspective: Annotation, Abstraction and Analysis.\nMost lexical corpora today are part-of-speech-tagged (POS-tagged). However even corpus linguists who work with 'unannotated plain text' inevitably apply some method to isolate salient terms. In such situations annotation and abstraction are combined in a lexical search.\nThe advantage of publishing an annotated corpus is that other users can then perform experiments on the corpus (through corpus managers). Linguists with other interests and differing perspectives than the originators' can exploit this work. By sharing data, corpus linguists are able to treat the corpus as a locus of linguistic debate and further study.\nNotes and references.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nFurther reading.\nBook series.\nBook series in this field include:\nJournals.\nThere are several international peer-reviewed journals dedicated to corpus linguistics, for example:"}
{"id": "40282", "revid": "31192532", "url": "https://en.wikipedia.org/wiki?curid=40282", "title": "Type theory", "text": "Mathematical theory of data types\nIn mathematics and theoretical computer science, a type theory is the formal presentation of a specific type system. Type theory is the academic study of type systems. \nSome type theories serve as alternatives to set theory as a foundation of mathematics. Two influential type theories that have been proposed as foundations are: \nMost computerized proof-writing systems use a type theory for their foundation. A common one is Thierry Coquand's calculus of inductive constructions.\nHistory.\nType theory was created to avoid paradoxes in naive set theory and formal logic, such as Russell's paradox which demonstrates that, without proper axioms, it is possible to define the set of all sets that are not members of themselves; this set both contains itself and does not contain itself. Between 1902 and 1908, Bertrand Russell proposed various solutions to this problem.\nBy 1908, Russell arrived at a ramified theory of types together with an axiom of reducibility, both of which appeared in Whitehead and Russell's \"Principia Mathematica\" published in 1910, 1912, and 1913. This system avoided contradictions suggested in Russell's paradox by creating a hierarchy of types and then assigning each concrete mathematical entity to a specific type. Entities of a given type were built exclusively of subtypes of that type, thus preventing an entity from being defined using itself. This resolution of Russell's paradox is similar to approaches taken in other formal systems, such as Zermelo-Fraenkel set theory.\nType theory is particularly popular in conjunction with Alonzo Church's lambda calculus. One notable early example of type theory is Church's simply typed lambda calculus. Church's theory of types helped the formal system avoid the Kleene\u2013Rosser paradox that afflicted the original untyped lambda calculus. Church demonstrated that it could serve as a foundation of mathematics and it was referred to as a higher-order logic.\nIn the modern literature, \"type theory\" refers to a typed system based around lambda calculus. One influential system is Per Martin-L\u00f6f's intuitionistic type theory, which was proposed as a foundation for constructive mathematics. Another is Thierry Coquand's calculus of constructions, which is used as the foundation by Rocq (previously known as \"Coq\"), Lean, and other computer proof assistants. Type theory is an active area of research, one direction being the development of homotopy type theory.\nApplications.\nMathematical foundations.\nThe first computer proof assistant, called Automath, used type theory to encode mathematics on a computer. Martin-L\u00f6f specifically developed intuitionistic type theory to encode \"all\" mathematics to serve as a new foundation for mathematics. There is ongoing research into mathematical foundations using homotopy type theory.\nMathematicians working in category theory already had difficulty working with the widely accepted foundation of Zermelo\u2013Fraenkel set theory. This led to proposals such as Lawvere's Elementary Theory of the Category of Sets (ETCS). Homotopy type theory continues in this line using type theory. Researchers are exploring connections between dependent types (especially the identity type) and algebraic topology (specifically homotopy).\nProof assistants.\nMuch of the current research into type theory is driven by proof checkers, interactive proof assistants, and automated theorem provers. Most of these systems use a type theory as the mathematical foundation for encoding proofs, which is not surprising, given the close connection between type theory and programming languages:\nMany type theories are supported by LEGO and Isabelle. Isabelle also supports foundations besides type theories, such as ZFC. Mizar is an example of a proof system that only supports set theory.\nProgramming languages.\nAny static program analysis, such as the type checking algorithms in the semantic analysis phase of compiler, has a connection to type theory. A prime example is Agda, a programming language which uses UTT (Luo's Unified Theory of dependent Types) for its type system.\nThe programming language ML was developed for manipulating type theories (see LCF) and its own type system was heavily influenced by them.\nLinguistics.\nType theory is also widely used in formal theories of semantics of natural languages, especially Montague grammar and its descendants. In particular, categorial grammars and pregroup grammars extensively use type constructors to define the types (\"noun\", \"verb\", etc.) of words.\nThe most common construction takes the basic types formula_1 and formula_2 for individuals and truth-values, respectively, and defines the set of types recursively as follows:\nA complex type formula_5 is the type of functions from entities of type formula_3 to entities of type formula_4. Thus one has types like formula_9 which are interpreted as elements of the set of functions from entities to truth-values, i.e. indicator functions of sets of entities. An expression of type formula_10 is a function from sets of entities to truth-values, i.e. a (indicator function of a) set of sets. This latter type is standardly taken to be the type of natural language quantifiers, like \" everybody\" or \"nobody\" (Montague 1973, Barwise and Cooper 1981).\nType theory with records is a formal semantics representation framework, using \"records\" to express \"type theory types\". It has been used in natural language processing, principally computational semantics and dialogue systems.\nSocial sciences.\nGregory Bateson introduced a theory of logical types into the social sciences; his notions of double bind and logical levels are based on Russell's theory of types.\nLogic.\nA type theory is a mathematical logic, which is to say it is a collection of rules of inference that result in judgments. Most logics have judgments asserting \"The proposition formula_11 is true\", or \"The formula formula_11 is a well-formed formula\". A type theory has judgments that define types and assign them to a collection of formal objects, known as terms. A term and its type are often written together as formula_13.\nTerms.\nA term in logic is recursively defined as a constant symbol, variable, or a function application, where a term is applied to another term. Constant symbols could include the natural number formula_14, the Boolean value formula_15, and functions such as the successor function formula_16 and conditional operator formula_17. Thus some terms could be formula_14, formula_19, formula_20, and formula_21.\nJudgments.\nMost type theories have 4 judgments:\nJudgments may follow from assumptions. For example, one might say \"assuming formula_30 is a term of type formula_31 and formula_32 is a term of type formula_33, it follows that formula_34 is a term of type formula_33\". Such judgments are formally written with the turnstile symbol formula_36.\nformula_37\nIf there are no assumptions, there will be nothing to the left of the turnstile.\nformula_38\nThe list of assumptions on the left is the \"context\" of the judgment. Capital greek letters, such as formula_39 and formula_40, are common choices to represent some or all of the assumptions. The 4 different judgments are thus usually written as follows.\nSome textbooks use a triple equal sign formula_41 to stress that this is judgmental equality and thus an extrinsic notion of equality. The judgments enforce that every term has a type. The type will restrict which rules can be applied to a term.\nRules of inference.\nA type theory's inference rules say what judgments can be made, based on the existence of other judgments. Rules are expressed as a Gentzen-style deduction using a horizontal line, with the required input judgments above the line and the resulting judgment below the line. For example, the following inference rule states a substitution rule for judgmental equality.formula_42The rules are syntactic and work by rewriting. The metavariables formula_39, formula_40, formula_2, formula_25, and formula_26 may actually consist of complex terms and types that contain many function applications, not just single symbols.\nTo generate a particular judgment in type theory, there must be a rule to generate it, as well as rules to generate all of that rule's required inputs, and so on. The applied rules form a proof tree, where the top-most rules need no assumptions. One example of a rule that does not require any inputs is one that states the type of a constant term. For example, to assert that there is a term formula_14 of type formula_33, one would write the following.formula_50\nType inhabitation.\nGenerally, the desired conclusion of a proof in type theory is one of type inhabitation. The decision problem of type inhabitation (abbreviated by formula_51) is: \nGiven a context formula_39 and a type formula_53, decide whether there exists a term formula_2 that can be assigned the type formula_53 in the type environment formula_39.\nGirard's paradox shows that type inhabitation is strongly related to the consistency of a type system with Curry\u2013Howard correspondence. To be sound, such a system must have uninhabited types.\nA type theory usually has several rules, including ones to:\nAlso, for each \"by rule\" type, there are 4 different kinds of rules\nFor examples of rules, an interested reader may follow Appendix A.2 of the \"Homotopy Type Theory\" book, or read Martin-L\u00f6f's Intuitionistic Type Theory.\nConnections to foundations.\nThe logical framework of a type theory bears a resemblance to intuitionistic, or constructive, logic. Formally, type theory is often cited as an implementation of the Brouwer\u2013Heyting\u2013Kolmogorov interpretation of intuitionistic logic. Additionally, connections can be made to category theory and computer programs.\nIntuitionistic logic.\nWhen used as a foundation, certain types are interpreted to be propositions (statements that can be proven), and terms inhabiting the type are interpreted to be proofs of that proposition. When some types are interpreted as propositions, there is a set of common types that can be used to connect them to make a Boolean algebra out of types. However, the logic is not classical logic but intuitionistic logic, which is to say it does not have the law of excluded middle nor double negation.\nUnder this intuitionistic interpretation, there are common types that act as the logical operators:\nBecause the law of excluded middle does not hold, there is no term of type formula_57. Likewise, double negation does not hold, so there is no term of type formula_58.\nIt is possible to include the law of excluded middle and double negation into a type theory, by rule or assumption. However, terms may not compute down to canonical terms and it will interfere with the ability to determine if two terms are judgementally equal to each other.\nConstructive mathematics.\nPer Martin-L\u00f6f proposed his intuitionistic type theory as a foundation for constructive mathematics. Constructive mathematics requires when proving \"there exists an formula_30 with property formula_60\", one must construct a particular formula_30 and a proof that it has property formula_62. In type theory, existence is accomplished using the dependent product type, and its proof requires a term of that type.\nAn example of a non-constructive proof is proof by contradiction. The first step is assuming that formula_30 does not exist and refuting it by contradiction. The conclusion from that step is \"it is not the case that formula_30 does not exist\". The last step is, by double negation, concluding that formula_30 exists. Constructive mathematics does not allow the last step of removing the double negation to conclude that formula_30 exists.\nMost of the type theories proposed as foundations are constructive, and this includes most of the ones used by proof assistants. It is possible to add non-constructive features to a type theory, by rule or assumption. These include operators on continuations such as call with current continuation. However, these operators tend to break desirable properties such as canonicity and parametricity.\nCurry-Howard correspondence.\nThe Curry\u2013Howard correspondence is the observed similarity between logics and programming languages. The implication in logic, \"A formula_67 B\" resembles a function from type \"A\" to type \"B\". For a variety of logics, the rules are similar to expressions in a programming language's types. The similarity goes farther, as applications of the rules resemble programs in the programming languages. Thus, the correspondence is often summarized as \"proofs as programs\".\nThe opposition of terms and types can also be viewed as one of \"implementation\" and \"specification\". By program synthesis, (the computational counterpart of) type inhabitation can be used to construct (all or parts of) programs from the specification given in the form of type information.\nType inference.\nMany programs that work with type theory (e.g., interactive theorem provers) also do type inferencing. It lets them select the rules that the user intends, with fewer actions by the user.\nResearch areas.\nCategory theory.\nAlthough the initial motivation for category theory was far removed from foundationalism, the two fields turned out to have deep connections. As John Lane Bell writes: \"In fact categories can \"themselves\" be viewed as type theories of a certain kind; this fact alone indicates that type theory is much more closely related to category theory than it is to set theory.\" In brief, a category can be viewed as a type theory by regarding its objects as types (or \"sorts\" ), i.e. \"Roughly speaking, a category may be thought of as a type theory shorn of its syntax.\" A number of significant results follow in this way:\nThe interplay, known as categorical logic, has been a subject of active research since then; see the monograph of Jacobs (1999) for instance.\nHomotopy type theory.\nHomotopy type theory attempts to combine type theory and category theory. It focuses on equalities, especially equalities between types. Homotopy type theory differs from intuitionistic type theory mostly by its handling of the equality type. In 2016, cubical type theory was proposed, which is a homotopy type theory with normalization.\nDefinitions.\nTerms and types.\nAtomic terms.\nThe most basic types are called atoms, and a term whose type is an atom is known as an atomic term. Common atomic terms included in type theories are natural numbers, often notated with the type formula_33, Boolean logic values (formula_15/formula_70), notated with the type formula_31, and formal variables, whose type may vary. For example, the following may be atomic terms.\nFunction terms.\nIn addition to atomic terms, most modern type theories also allow for functions. Function types introduce an arrow symbol, and are defined inductively: If formula_76 and formula_53 are types, then the notation formula_78 is the type of a function which takes a parameter of type formula_76 and returns a term of type formula_53. Types of this form are known as \"simple\" types.\nSome terms may be declared directly as having a simple type, such as the following term, formula_81, which takes in two natural numbers in sequence and returns one natural number.\nformula_82\nStrictly speaking, a simple type only allows for one input and one output, so a more faithful reading of the above type is that formula_81 is a function which takes in a natural number and returns a function of the form formula_84. The parentheses clarify that formula_81 does not have the type formula_86, which would be a function which takes in a function of natural numbers and returns a natural number. The convention is that the arrow is right associative, so the parentheses may be dropped from formula_81's type.\nLambda terms.\nNew function terms may be constructed using lambda expressions, and are called lambda terms. These terms are also defined inductively: a lambda term has the form formula_88, where formula_89 is a formal variable and formula_2 is a term, and its type is notated formula_78, where formula_76 is the type of formula_89, and formula_53 is the type of formula_2. The following lambda term represents a function which doubles an input natural number.\nformula_96\nThe variable is formula_97 and (implicit from the lambda term's type) must have type formula_98. The term formula_99 has type formula_98, which is seen by applying the function application inference rule twice. Thus, the lambda term has type formula_84, which means it is a function taking a natural number as an argument and returning a natural number.\nA lambda term is often referred to as an anonymous function because it lacks a name. The concept of anonymous functions appears in many programming languages.\nInference Rules.\nFunction application.\nThe power of type theories is in specifying how terms may be combined by way of inference rules. Type theories which have functions also have the inference rule of function application: if formula_2 is a term of type formula_78, and formula_104 is a term of type formula_76, then the application of formula_2 to formula_104, often written formula_108, has type formula_53. For example, if one knows the type notations formula_110, formula_111, and formula_112, then the following type notations can be deduced from function application.\nParentheses indicate the order of operations; however, by convention, function application is left associative, so parentheses can be dropped where appropriate. In the case of the three examples above, all parentheses could be omitted from the first two, and the third may simplified to formula_116.\nReductions.\nType theories that allow for lambda terms also include inference rules known as formula_117-reduction and formula_118-reduction. They generalize the notion of function application to lambda terms. Symbolically, they are written\nThe first reduction describes how to evaluate a lambda term: if a lambda expression formula_88 is applied to a term formula_104, one replaces every occurrence of formula_89 in formula_2 with formula_104. The second reduction makes explicit the relationship between lambda expressions and function types: if formula_130 is a lambda term, then it must be that formula_2 is a function term because it is being applied to formula_89. Therefore, the lambda expression is equivalent to just formula_2, as both take in one argument and apply formula_2 to it.\nFor example, the following term may be formula_117-reduced.\nformula_136\nIn type theories that also establish notions of equality for types and terms, there are corresponding inference rules of formula_117-equality and formula_118-equality.\nCommon terms and types.\nEmpty type.\nThe empty type has no terms. The type is usually written formula_139 or formula_140. One use for the empty type is proofs of type inhabitation. If for a type formula_3, it is consistent to derive a function of type formula_142, then formula_3 is \"uninhabited\", which is to say it has no terms.\nUnit type.\nThe unit type has exactly 1 canonical term. The type is written formula_144 or formula_145 and the single canonical term is written formula_146. The unit type is also used in proofs of type inhabitation. If for a type formula_3, it is consistent to derive a function of type formula_148, then formula_3 is \"inhabited\", which is to say it must have one or more terms.\nBoolean type.\nThe Boolean type has exactly 2 canonical terms. The type is usually written formula_150 or formula_151 or formula_152. The canonical terms are usually formula_15 and formula_70.\nNatural numbers.\nNatural numbers are usually implemented in the style of Peano Arithmetic. There is a canonical term formula_155 for zero. Canonical values larger than zero use iterated applications of a successor function formula_156.\nType constructors.\nSome type theories allow for types of complex terms, such as functions or lists, to depend on the types of its arguments; these are called type constructors. For example, a type theory could have the dependent type formula_157, which should correspond to lists of terms, where each term must have type formula_3. In this case, formula_159 has the kind formula_160, where formula_161 denotes the universe of all types in the theory. \nProduct type.\nThe product type, formula_162, depends on two types, and its terms are commonly written as ordered pairs formula_163. The pair formula_163 has the product type formula_165, where formula_76 is the type of formula_104 and formula_53 is the type of formula_2. Each product type is then usually defined with eliminator functions formula_170 and formula_171.\nBesides ordered pairs, this type is used for the concepts of logical conjunction and intersection.\nSum type.\nThe sum type is written as either formula_176 or formula_177. In programming languages, sum types may be referred to as tagged unions. Each type formula_178 is usually defined with constructors formula_179 and formula_180, which are injective, and an eliminator function formula_181 such that\nThe sum type is used for the concepts of logical disjunction and union.\nPolymorphic types.\nSome theories also allow terms to have their definitions depend on types. For instance, an identity function of any type could be written as formula_186. The function is said to be polymorphic in formula_187, or generic in formula_30.\nAs another example, consider a function formula_189, which takes in a formula_157 and a term of type formula_3, and returns the list with the element at the end. The type annotation of such a function would be formula_192, which can be read as \"for any type formula_3, pass in a formula_157 and an formula_3, and return a formula_157\". Here formula_189 is polymorphic in formula_3.\nProducts and sums.\nWith polymorphism, the eliminator functions can be defined generically for \"all\" product types as formula_199 and formula_200.\nLikewise, the sum type constructors can be defined for all valid types of sum members as formula_205 and formula_206, which are injective, and the eliminator function can be given as formula_207 such that\nDependent typing.\nSome theories also permit types to be dependent on terms instead of types. For example, a theory could have the type formula_212, where formula_213 is a term of type formula_33 encoding the length of the vector. This allows for greater specificity and type safety: functions with vector length restrictions or length matching requirements, such as the dot product, can encode this requirement as part of the type. \nThere are foundational issues that can arise from dependent types if a theory is not careful about what dependencies are allowed, such as Girard's Paradox. The logician Henk Barendegt introduced the lambda cube as a framework for studying various restrictions and levels of dependent typing.\nDependent products and sums.\nTwo common type dependencies, dependent product and dependent sum types, allow for the theory to encode BHK intuitionistic logic by acting as equivalents to universal and existential quantification; this is formalized by Curry\u2013Howard Correspondence. As they also connect to products and sums in set theory, they are often written with the symbols formula_215 and formula_216, respectively.\nSum types are seen in dependent pairs, where the second type depends on the value of the first term. This arises naturally in computer science where functions may return different types of outputs based on the input. For example, the Boolean type is usually defined with an eliminator function formula_17, which takes three arguments and behaves as follows.\nOrdinary definitions of formula_17 require formula_30 and formula_32 to have the same type. If the type theory allows for dependent types, then it is possible to define a dependent type formula_225 such that\nThe type of formula_17 may then be written as formula_231.\nIdentity type.\nFollowing the notion of Curry-Howard Correspondence, the identity type is a type introduced to mirror propositional equivalence, as opposed to the judgmental (syntactic) equivalence that type theory already provides.\nAn identity type requires two terms of the same type and is written with the symbol formula_232. For example, if formula_233 and formula_234 are terms, then formula_235 is a possible type. Canonical terms are created with a reflexivity function, formula_236. For a term formula_2, the call formula_238 returns the canonical term inhabiting the type formula_239.\nThe complexities of equality in type theory make it an active research topic; homotopy type theory is a notable area of research that mainly deals with equality in type theory.\nInductive types.\nInductive types are a general template for creating a large variety of types. In fact, all the types described above and more can be defined using the rules of inductive types. Two methods of generating inductive types are induction-recursion and induction-induction. A method that only uses lambda terms is Scott encoding.\nSome proof assistants, such as Rocq (previously known as \"Coq\") and Lean, are based on the calculus for inductive constructions, which is a calculus of constructions with inductive types.\nDifferences from set theory.\nThe most commonly accepted foundation for mathematics is first-order logic with the language and axioms of Zermelo\u2013Fraenkel set theory with the axiom of choice, abbreviated ZFC. Type theories having sufficient expressibility may also act as a foundation of mathematics. There are a number of differences between these two approaches.\nProponents of type theory will also point out its connection to constructive mathematics through the BHK interpretation, its connection to logic by the Curry\u2013Howard isomorphism, and its connections to category theory.\nProperties of type theories.\nTerms usually belong to a single type. However, there are set theories that define \"subtyping\".\nComputation takes place by repeated application of rules. Many types of theories are strongly normalizing, which means that any order of applying the rules will always end in the same result. However, some are not. In a normalizing type theory, the one-directional computation rules are called \"reduction rules\", and applying the rules \"reduces\" the term. If a rule is not one-directional, it is called a \"conversion rule\".\nSome combinations of types are equivalent to other combinations of types. When functions are considered \"exponentiation\", the combinations of types can be written similarly to algebraic identities. Thus, formula_242, formula_243, formula_244, formula_245, formula_246.\nAxioms.\nMost type theories do not have axioms. This is because a type theory is defined by its rules of inference. This is a source of confusion for people familiar with Set Theory, where a theory is defined by both the rules of inference for a logic (such as first-order logic) and axioms about sets.\nSometimes, a type theory will add a few axioms. An axiom is a judgment that is accepted without a derivation using the rules of inference. They are often added to ensure properties that cannot be added cleanly through the rules.\nAxioms can cause problems if they introduce terms without a way to compute on those terms. That is, axioms can interfere with the normalizing property of the type theory.\nSome commonly encountered axioms are:\nThe axiom of choice does not need to be added to type theory, because in most type theories it can be derived from the rules of inference. This is because of the constructive nature of type theory, where proving that a value exists requires a method to compute the value. The axiom of choice is less powerful in type theory than most set theories, because type theory's functions must be computable and, being syntax-driven, the number of terms in a type must be countable. (See .)\nFurther reading.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "40283", "revid": "7903804", "url": "https://en.wikipedia.org/wiki?curid=40283", "title": "Melting point", "text": "Temperature at which a solid turns liquid\nThe melting point (or, rarely, liquefaction point) of a substance is the temperature at which it changes state from solid to liquid. At the melting point the solid and liquid phase exist in equilibrium. The melting point of a substance depends on pressure and is usually specified at a standard pressure such as 1 atmosphere or 100 kPa.\nWhen considered as the temperature of the reverse change from liquid to solid, it is referred to as the freezing point or crystallization point. Because of the ability of substances to supercool, the freezing point can easily appear to be below its actual value. When the \"characteristic freezing point\" of a substance is determined, in fact, the actual methodology is almost always \"the principle of observing the disappearance rather than the formation of ice, that is, the melting point.\"\nExamples.\nFor most substances, melting and freezing points are approximately equal. For example, the melting and freezing points of mercury is . However, certain substances possess differing solid-liquid transition temperatures. For example, agar melts at and solidifies from ; such direction dependence is known as hysteresis. The melting point of ice at 1 atmosphere of pressure is very close to ; this is also known as the ice point. In the presence of nucleating substances, the freezing point of water is not always the same as the melting point. In the absence of nucleators water can exist as a supercooled liquid down to before freezing.\nThe metal with the highest melting point is tungsten, at ; this property makes tungsten excellent for use as electrical filaments in incandescent lamps. The often-cited carbon does not melt at ambient pressure but sublimes at about ; a liquid phase only exists above pressures of and estimated (see ). Hafnium carbonitride (HfCN) is a refractory compound with the highest known melting point of any substance to date and the only one confirmed to have a melting point above at ambient pressure. Quantum mechanical computer simulations predicted that this alloy (HfN0.38C0.51) would have a melting point of about 4,400 K. This prediction was later confirmed by experiment, though a precise measurement of its exact melting point has yet to be confirmed. At the other end of the scale, helium does not freeze at all at normal pressure even at temperatures arbitrarily close to absolute zero; a pressure of more than twenty times normal atmospheric pressure is necessary.\nMelting point measurements.\nMany laboratory techniques exist for the determination of melting points.\nA Kofler bench is a metal strip with a temperature gradient (range from room temperature to 300\u00a0\u00b0C). Any substance can be placed on a section of the strip, revealing its thermal behaviour at the temperature at that point. Differential scanning calorimetry gives information on melting point together with its enthalpy of fusion.\nA basic melting point apparatus for the analysis of crystalline solids consists of an oil bath with a transparent window (most basic design: a Thiele tube) and a simple magnifier. Several grains of a solid are placed in a thin glass tube and partially immersed in the oil bath. The oil bath is heated (and stirred) and with the aid of the magnifier (and external light source) melting of the individual crystals at a certain temperature can be observed. A metal block might be used instead of an oil bath. Some modern instruments have automatic optical detection.\nThe measurement can also be made continuously with an operating process. For instance, oil refineries measure the freeze point of diesel fuel \"online\", meaning that the sample is taken from the process and measured automatically. This allows for more frequent measurements as the sample does not have to be manually collected and taken to a remote laboratory.\nTechniques for refractory materials.\nFor refractory materials (e.g. platinum, tungsten, tantalum, some carbides and nitrides, etc.) the extremely high melting point (typically considered to be above, say, 1,800\u00a0\u00b0C) may be determined by heating the material in a black body furnace and measuring the black-body temperature with an optical pyrometer. For the highest melting materials, this may require extrapolation by several hundred degrees. The spectral radiance from an incandescent body is known to be a function of its temperature. An optical pyrometer matches the radiance of a body under study to the radiance of a source that has been previously calibrated as a function of temperature. In this way, the measurement of the absolute magnitude of the intensity of radiation is unnecessary. However, known temperatures must be used to determine the calibration of the pyrometer. For temperatures above the calibration range of the source, an extrapolation technique must be employed. This extrapolation is accomplished by using Planck's law of radiation. The constants in this equation are not known with sufficient accuracy, causing errors in the extrapolation to become larger at higher temperatures. However, standard techniques have been developed to perform this extrapolation.\nConsider the case of using gold as the source (mp = 1,063\u00a0\u00b0C). In this technique, the current through the filament of the pyrometer is adjusted until the light intensity of the filament matches that of a black-body at the melting point of gold. This establishes the primary calibration temperature and can be expressed in terms of current through the pyrometer lamp. With the same current setting, the pyrometer is sighted on another black-body at a higher temperature. An absorbing medium of known transmission is inserted between the pyrometer and this black-body. The temperature of the black-body is then adjusted until a match exists between its intensity and that of the pyrometer filament. The true higher temperature of the black-body is then determined from Planck's Law. The absorbing medium is then removed and the current through the filament is adjusted to match the filament intensity to that of the black-body. This establishes a second calibration point for the pyrometer. This step is repeated to carry the calibration to higher temperatures. Now, temperatures and their corresponding pyrometer filament currents are known and a curve of temperature versus current can be drawn. This curve can then be extrapolated to very high temperatures.\nIn determining melting points of a refractory substance by this method, it is necessary to either have black body conditions or to know the emissivity of the material being measured. The containment of the high melting material in the liquid state may introduce experimental difficulties. Melting temperatures of some refractory metals have thus been measured by observing the radiation from a black body cavity in solid metal specimens that were much longer than they were wide. To form such a cavity, a hole is drilled perpendicular to the long axis at the center of a rod of the material. These rods are then heated by passing a very large current through them, and the radiation emitted from the hole is observed with an optical pyrometer. The point of melting is indicated by the darkening of the hole when the liquid phase appears, destroying the black body conditions. Today, containerless laser heating techniques, combined with fast pyrometers and spectro-pyrometers, are employed to allow for precise control of the time for which the sample is kept at extreme temperatures. Such experiments of sub-second duration address several of the challenges associated with more traditional melting point measurements made at very high temperatures, such as sample vaporization and reaction with the container.\nThermodynamics.\nFor a solid to melt, heat is required to raise its temperature to the melting point. However, further heat needs to be supplied for the melting to take place: this is called the heat of fusion, and is an example of latent heat.\nFrom a thermodynamics point of view, at the melting point the change in Gibbs free energy (\u0394G) of the material is zero, but the enthalpy (\"H\") and the entropy (\"S\") of the material are increasing (\u0394H, \u0394S &gt; 0). Melting phenomenon happens when the Gibbs free energy of the liquid becomes lower than the solid for that material. At various pressures this happens at a specific temperature. It can also be shown that:\n formula_1\nHere \"T\", \"\u0394S\" and \"\u0394H\" are respectively the temperature at the melting point, change of entropy of melting and the change of enthalpy of melting.\nThe melting point is sensitive to extremely large changes in pressure, but generally this sensitivity is orders of magnitude less than that for the boiling point, because the solid-liquid transition represents only a small change in volume. If, as observed in most cases, a substance is more dense in the solid than in the liquid state, the melting point will increase with increases in pressure. Otherwise the reverse behavior occurs. Notably, this is the case of water, as illustrated graphically to the right, but also of Si, Ge, Ga, Bi. With extremely large changes in pressure, substantial changes to the melting point are observed. For example, the melting point of silicon at ambient pressure (0.1 MPa) is 1415\u00a0\u00b0C, but at pressures in excess of 10 GPa it decreases to 1000\u00a0\u00b0C.\nMelting points are often used to characterize organic and inorganic compounds and to ascertain their purity. The melting point of a pure substance is always higher and has a smaller range than the melting point of an impure substance or, more generally, of mixtures. The higher the quantity of other components, the lower the melting point and the broader will be the melting point range, often referred to as the \"pasty range\". The temperature at which melting begins for a mixture is known as the \"solidus\" while the temperature where melting is complete is called the \"liquidus\". Eutectics are special types of mixtures that behave like single phases. They melt sharply at a constant temperature to form a liquid of the same composition. Alternatively, on cooling a liquid with the eutectic composition will solidify as uniformly dispersed, small (fine-grained) mixed crystals with the same composition.\nIn contrast to crystalline solids, glasses do not possess a melting point;\non heating they undergo a smooth glass transition into a viscous liquid.\nUpon further heating, they gradually soften, which can be characterized by certain softening points.\nFreezing-point depression.\nThe freezing point of a solvent is depressed when another compound is added, meaning that a solution has a lower freezing point than a pure solvent. This phenomenon is used in technical applications to avoid freezing, for instance by adding salt or ethylene glycol to water.\nCarnelley's rule.\nIn organic chemistry, Carnelley's rule, established in 1882 by Thomas Carnelley, states that \"high molecular symmetry is associated with high melting point\". Carnelley based his rule on examination of 15,000 chemical compounds. For example, for three structural isomers with molecular formula C5H12 the melting point increases in the series isopentane \u2212160\u00a0\u00b0C (113 K) n-pentane \u2212129.8\u00a0\u00b0C (143 K) and neopentane \u221216.4\u00a0\u00b0C (256.8 K). Likewise in xylenes and also dichlorobenzenes the melting point increases in the order meta, ortho and then para. Pyridine has a lower symmetry than benzene hence its lower melting point but the melting point again increases with diazine and triazines. Many cage-like compounds like adamantane and cubane with high symmetry have relatively high melting points.\nA high melting point results from a high heat of fusion, a low entropy of fusion, or a combination of both. In highly symmetrical molecules the crystal phase is densely packed with many efficient intermolecular interactions resulting in a higher enthalpy change on melting.\nPredicting the melting point of substances (Lindemann's criterion).\nAn attempt to predict the bulk melting point of crystalline materials was first made in 1910 by Frederick Lindemann. The idea behind the theory was the observation that the average amplitude of thermal vibrations increases with increasing temperature. Melting initiates when the amplitude of vibration becomes large enough for adjacent atoms to partly occupy the same space. The Lindemann criterion states that melting is expected when the vibration root mean square amplitude exceeds a threshold value.\nAssuming that all atoms in a crystal vibrate with the same frequency \"\u03bd\", the average thermal energy can be estimated using the equipartition theorem as\nformula_2\nwhere \"m\" is the atomic mass, \"\u03bd\" is the frequency, \"u\" is the average vibration amplitude, \"k\"B is the Boltzmann constant, and \"T\" is the absolute temperature. If the threshold value of \"u2\" is \"c2a2\" where \"c\" is the Lindemann constant and \"a\" is the atomic spacing, then the melting point is estimated as\nformula_3\nSeveral other expressions for the estimated melting temperature can be obtained depending on the estimate of the average thermal energy. Another commonly used expression for the Lindemann criterion is\nformula_4\nFrom the expression for the Debye frequency for \"\u03bd\",\nformula_5\nwhere \"\u03b8\"D is the Debye temperature and \"h\" is the Planck constant. Values of \"c\" range from 0.15 to 0.3 for most materials.\nDatabases and automated prediction.\nIn February 2011, Alfa Aesar released over 10,000 melting points of compounds from their catalog as open data and similar data has been mined from patents. The Alfa Aesar and patent data have been summarized in (respectively) random forest and support vector machines.\nReferences.\nCitations.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nSources.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "40284", "revid": "43007828", "url": "https://en.wikipedia.org/wiki?curid=40284", "title": "Cam (mechanism)", "text": "Rotating or sliding component that transmits variable motion to a follower\nA cam is a rotating or sliding piece in a mechanical linkage used especially in transforming rotary motion into linear motion. It is often a part of a rotating wheel (e.g. an eccentric wheel) or shaft (e.g. a cylinder with an irregular shape) that strikes a lever at one or more points on its circular path. The cam can be a simple tooth, as is used to deliver pulses of power to a steam hammer, for example, or an eccentric disc or other shape that produces a smooth reciprocating (back and forth) motion in the \"follower\", which is a lever making contact with the cam. A cam timer is similar, and these were widely used for electric machine control (an electromechanical timer in a washing machine being a common example) before the advent of inexpensive electronics, microcontrollers, integrated circuits, programmable logic controllers and digital control.\nCamshaft.\nThe cam can be seen as a device that converts rotational motion to reciprocating (or sometimes oscillating) motion, see Fig. 1. A common example is the camshaft of an automobile, which takes the rotary motion of the engine and pushes shaft into the reciprocating (up and down) motion necessary to operate the intake and exhaust valves of the cylinders.\nDisplacement diagram.\nCams can be characterized by their displacement diagrams, which reflect the changing position a follower would make as the surface of the cam moves in contact with the follower. In the example shown, the cam rotates about an axis. These diagrams relate angular position, usually in degrees, to the radial displacement experienced at that position. Displacement diagrams are traditionally presented as graphs with non-negative values. A simple displacement diagram illustrates the follower motion at a constant velocity rise followed by a similar return with a dwell in between as depicted in figure 2. The rise is the motion of the follower away from the cam center, dwell is the motion where the follower is at rest, and return is the motion of the follower toward the cam center.\nA common type is in the valve actuators in internal combustion engines. Here, the cam profile is commonly symmetric and at rotational speeds generally met with, very high acceleration forces develop. Ideally, a convex curve between the onset and maximum position of lift reduces acceleration, but this requires impractically large shaft diameters relative to lift. Thus, in practice, the points at which lift begins and ends mean that a tangent to the base circle appears on the profile. This is continuous with a tangent to the tip circle. In designing the cam, the lift and the dwell angle \"\u03b8\" are given. If the profile is treated as a large base circle and a small tip circle, joined by a common tangent, giving lift \"L\", the relationship can be calculated, given the angle \"\u03c6\" between one tangent and the axis of symmetry (\"\u03c6\" being \u2212), while \"C\" is the distance between the centres of the circles (required), and \"R\" is the radius of the base (given) and \"r\" that of the tip circle (required):\n\"C\" \n and \"r\" \n \"R\" \u2212 \"L\" \nTypes by shape.\nDisc or plate.\nThe most commonly used cam is the cam plate (also known as \"disc cam\" or \"radial cam\")\nwhich is cut out of a piece of flat metal or plate. Here, the follower moves in a plane perpendicular to the axis of rotation of the camshaft. Several key terms are relevant in such a construction of plate cams: base circle, prime circle (with radius equal to the sum of the follower radius and the base circle radius), pitch curve which is the radial curve traced out by applying the radial displacements away from the prime circle across all angles, and the lobe separation angle (LSA \u2013 the angle between two adjacent intake and exhaust cam lobes).\nThe base circle is the smallest circle that can be drawn to the cam profile.\nA once common, but now outdated, application of this type of cam was automatic machine tool programming cams. Each tool movement or operation was controlled directly by one or more cams. Instructions for producing programming cams and cam generation data for the most common makes of machine, were included in engineering references well into the modern CNC era.\nThis type of cam is used in many simple electromechanical appliances controllers, such as dishwashers and clothes washing machines, to actuate mechanical switches that control the various parts.\nCylindrical.\nA cylindrical cam, or barrel cam, is a cam in which the follower rides on the surface of a cylinder. In the most common type, the follower rides in a groove cut into the surface of a cylinder. These cams are principally used to convert rotational motion to linear motion perpendicular to the rotational axis of the cylinder. A cylinder may have several grooves cut into the surface and drive several followers. Cylindrical cams can provide motions that involve more than a single rotation of the cylinder and generally provide positive positioning, removing the need for a spring or other provision to keep the follower in contact with the control surface.\nApplications include machine tool drives, such as reciprocating saws, and shift control barrels in sequential transmissions, such as on most modern motorcycles.\nA special case of this cam is a constant lead, where the position of the follower is linear with rotation, as in a lead screw. The purpose and detail of implementation influence whether this application is called a cam or a screw thread, but in some cases, the nomenclature may be ambiguous.\nCylindrical cams may also be used to reference an output to two inputs, where one input is the rotation of the cylinder and the other is the position of the follower along the cam. The output is radial to the cylinder. These were once common for special functions in control systems, such as fire control mechanisms for guns on naval vessels and mechanical analog computers.\nAn example of a cylindrical cam with two inputs is provided by a duplicating lathe, an example of which is the Klotz axe handle lathe, which cuts an axe handle to a form controlled by a pattern acting as a cam for the lathe mechanism.\nFace.\nA face cam produces motion by using a follower riding on the face of a disk. The most common type has the follower ride in a slot so that the captive follower produces radial motion with positive positioning without the need for a spring or other mechanism to keep the follower in contact with the control surface. A face cam of this type generally has only one slot for a follower on each face. In some applications, a single element, such as a gear, a barrel cam or other rotating element with a flat face, may do duty as a face cam in addition to other purposes.\nFace cams may provide repetitive motion with a groove that forms a closed curve or may provide function generation with a stopped groove. Cams used for function generation may have grooves that require several revolutions to cover the complete function, and in this case, the function generally needs to be invertible so that the groove does not self intersect, and the function output value must differ enough at corresponding rotations that there is sufficient material separating the adjacent groove segments. A common form is the constant lead cam, where the displacement of the follower is linear with rotation, such as the scroll plate in a scroll chuck. Non-invertible functions, which require the groove to self-intersect, can be implemented using special follower designs.\nA variant of the face cam provides motion parallel to the axis of cam rotation. A common example is the traditional sash window lock, where the cam is mounted to the top of the lower sash, and the follower is the hook on the upper sash. In this application, the cam is used to provide a mechanical advantage in forcing the window shut, and also provides a self-locking action, like some worm gears, due to friction.\nFace cams may also be used to reference a single output to two inputs, typically where one input is the rotation of the cam and the other is the radial position of the follower. The output is parallel to the axis of the cam. These were once common is mechanical analog computation and special functions in control systems.\nA face cam that implements three outputs for a single rotational input is the stereo phonograph, where a relatively constant lead groove guides the stylus and tonearm unit, acting as either a rocker-type (tonearm) or linear (linear tracking turntable) follower, and the stylus alone acting as the follower for two orthogonal outputs to representing the audio signals. These motions are in a plane radial to the rotation of the record and at angles of 45 degrees to the plane of the disk (normal to the groove faces). The position of the tonearm was used by some turntables as a control input, such as to turn the unit off or to load the next disk in a stack, but was ignored in simple units.\nHeart shaped.\nA cam in the form of a symmetric heart is used to return a shaft holding the cam to a set position by pressure from a roller. They were used on early models of Post Office Master clocks to synchronise the clock time with Greenwich Mean Time when the activating follower was pressed onto the cam automatically via a signal from an accurate time source.\nSnail drop.\nA snail drop cam was used for example in mechanical timekeeping clocking-in clocks to drive the day advance mechanism at precisely midnight and consisted of a follower being raised over 24 hours by the cam in a spiral path which terminated at a sharp cut off at which the follower would drop down and activate the day advance. Where timing accuracy is required as in clocking-in clocks these were typically ingeniously arranged to have a roller cam follower to raise the drop weight for most of its journey to near its full height, and only for the last portion of its travel for the weight to be taken over and supported by a solid follower with a sharp edge. This ensured that the weight dropped at a precise moment, enabling accurate timing. This was achieved by the use of two snail cams mounted coaxially with the roller initially resting on one cam and the final solid follower on the other but not in contact with its cam profile. Thus the roller cam initially carried the weight, until at the final portion of the run the profile of the non-roller cam rose more than the other causing the solid follower to take the weight.\nLinear.\nA linear cam is one in which the cam element moves in a straight line rather than rotates. The cam element is often a plate or block but may be any cross-section. The key feature is that the input is a linear motion rather than rotational. The cam profile may be cut into one or more edges of a plate or block, may be one or more slots or grooves in the face of an element, or may even be a surface profile for a cam with more than one input. The development of a linear cam is similar to, but not identical to, that of a rotating cam.\nA common example of a linear cam is a key for a pin tumbler lock. The pins act as followers. This behavior is exemplified when the key is duplicated in a key duplication machine, where the original key acts as a control cam for cutting the new key.\nHistory.\nCam mechanisms appeared in China at around 600 BC in the form of a crossbow trigger-mechanism with a cam-shaped swing arm. However, the trigger mechanism did not rotate around its own axis and traditional Chinese technology generally made little use of continuously rotating cams. Nevertheless, later research showed that such cam mechanisms did in fact rotate around their own axes. Likewise, more recent research indicates that cams were used in water-driven trip hammers by the latter half of the Western Han Dynasty (206 BC \u2013 8 AD) as recorded in the Huan Zi Xin Lun. Complex pestles were also mentioned in later records such as the Jin Zhu Gong Zan and the Tian Gong Kai Wu, amongst many other records of water-driven pestles. During the Tang dynasty, the wooden clock within the water-driven astronomical device, the spurs inside a water-driven armillary sphere, the automated alarm within a five-wheeled sand-driven clock, artificial paper figurines within a revolving lantern, all utilized cam mechanisms. The Chinese hodometer which utilized a bell and gong mechanism is also a cam, as described in the Song Shi. In the book Nongshu, the vertical wheel of a water-driven wind box is also a cam. Out of these examples, the water-driven pestle and the water driven wind box both have two cam mechanisms inside. Cams that rotated continuously and functioned as integral machine elements were built into Hellenistic water-driven automata from the 3rd century BC.\nWatermills across medieval Europe powered many cam designs as detailed by Jean Gimpel: \n\"In Europe, from the end of the tenth century the cam enabled millwrights to mechanize a whole series of industries which up to then had been operated by hand or by foot. In France, one of the first mills for making beer is mentioned in a document relating to the monastery of Saint-Sauveur at Montreuil-sur-Mer between '987 and 996; water-driven hammers seem to have been operating as early as 1010 at Schmidmiihlen in Oberpfalz in Germany. Hemp seems to have been treated mechanically in the Graisivaudan in 1040. The earliest mentioned fulling mill operating in France was in a village in Normandy about 1086. A tanning mill is mentioned in an 1138 document belonging to the chapter of Notre-Dame de Paris. Paper, which was manufactured by hand and foot for a thousand years or so following its invention by the Chinese and adoption by the Arabs, was manufactured mechanically as soon as it reached medieval Europe in the thirteenth century. This is convincing evidence of how technologically minded the Europeans of that era were. Paper had traveled nearly halfway around the world, but no culture or civilization on its route had tried to mechanize its manufacture.\" \nThe cam and camshaft later appeared in mechanisms by Al-Jazari, who used them in his automata, described in 1206. The cam and camshaft appeared in European mechanisms from the 14th century. Waldo J Kelleigh of Electrical Apparatus Company patented the adjustable cam in the United States in 1956 for its use in mechanical engineering and weaponry.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "40286", "revid": "181", "url": "https://en.wikipedia.org/wiki?curid=40286", "title": "Context free grammar", "text": ""}
{"id": "40287", "revid": "50901610", "url": "https://en.wikipedia.org/wiki?curid=40287", "title": "William B. Ogden", "text": "American politician (1805\u20131877)\nWilliam Butler Ogden (June 15, 1805 \u2013 August 3, 1877) was an American politician and railroad executive who served as the first Mayor of Chicago. He was referred to as \"the Astor of Chicago.\" He was, at one time, the city's richest citizen.\nHe brought the Galena &amp; Chicago Union RR out of insolvency and was its first president in 1847.\nHe created the Chicago &amp; North Western Railway from the failed remains of the Chicago, St.Paul, Fond du Lac and was its first president in 1859.\nHe spearheaded the 1st transcontinental railroad as the Union Pacific and was its first president in 1862, although he relinquished that position due to poor health.\nOgden serves as the namesake for Chicago's Ogden Avenue, The Bronx's Odgen Avenue, the Ogden Slip in Chicago (which was constructed by his Chicago Dock and Canal Company), and Ogden, Iowa. A 1994 survey of experts on Chicago politics assessed Ogden as one of the ten best mayors in the city's history (up to that time).\nEarly life and career in New York.\nOgden was born on June 15, 1805, in Walton, New York. He was the son of Abraham Ogden (1771\u20131825) and Abigail (n\u00e9e Weed) Ogden (1788\u20131850).\nWhen Ogden was sixteen, he took over the family real estate and lumber businesses after his father suffered a debilitating stroke. Ogden proved an adept businessman, and improved the fortunes of his family's businesses.\nWhen Ogden was eighteen, he began his military service. At the time, young men of the state were required to serve in the military. He was commissioned as an officer on his first day of duty, and on the second was assigned as the aide of Brigadier-General Frederic P. Foote. Ogden was elevated to the rank of Major, and later became a brigade inspector for several years.\nWhen Ogden was twenty, his father died. Ogden assisted Charles Butler, his brother-in-law, with business matters related to opening a new building for New York University, attending the law school for a brief period himself.\nOgden was appointed by President Andrew Jackson to serve as postmaster of Walton, New York, which was Ogden's first political position. He held this office up until moving to Chicago. By the age of 29, he had become a lawyer and was elected a member of New York State Assembly, representing Delaware County in 1835 as a member of the 58th New York State Legislature.\nDuring his career in New York politics, Ogden was a Jacksonian Democrat. However, Ogden was also an advocate of government funding for infrastructural improvements, aspiring to see the federal government financially back the construction of a railroad from New York to Chicago. He told colleagues that such a railroad would be \"the most splendid system of internal communication ever yet devised by man.\" He had been elected to the New York Senate on a platform supporting state funding for the construction of the New York and Erie Railroad. The bill he backed to accomplish this was passed.\nCareer in Chicago.\nOne of Ogden's brothers-in-law purchased a tract of land in Chicago for $100,000 in 1834. Ogden went to survey this area in 1835, and wrote back that his relative had \"been guilty of the grossest folly\" as the land held no value due to being boggy and swampy. However, he sold 1/3 of the land for more than the entire tract been purchased for after the muddy environment dried up in summer. Ogden chose to stay in Chicago rather than return to New York.\nWhile Ogden's initial concern in Chicago was based in his land interests there, he believed that he could not afford to stay out of the politics of the city, as he believed growing western towns such as Chicago were dependent on government assistance.\nIn Chicago, Ogden created a land and trust agency bearing his name, which he operated from 1836 to 1843. In 1843, he brought in William E. Jones as a partner to the growing agency which became Ogden, Jones Co. The agency would later become Ogden, Fleetwood &amp; Co.\nPolitical career in Chicago.\nShortly after moving to Chicago in 1836, Ogden joined the committee responsible for drafting the city charter to be submitted to the state legislature.\nIn 1837, he was elected the first mayor of Chicago, serving a single one-year term. From 1840 through 1841, he served on the Chicago Common Council as an alderman from the 6th Ward. From 1847 through 1848, he served as an alderman from the 9th Ward.\nOgden was a booster of Chicago both during and after his tenures in elected office. At the time he came to Chicago, its buildings were largely wood cabins, it lacked sidewalks and decent bridges, it had no paved roads, and it lacked water supply infrastructure. As a politician he advocated for the city to raise tax revenue for new roads, plank sidewalks, and bridges (which he presented designs of his own for). He also used his own wealth to fund improvements to the city's infrastructure.\nOgden later stayed removed from politics. However, he reluctantly accepted a seat in the Illinois Senate after the Republican Party selected him as a candidate. He served in the state state 1860\u201361.\nRailroad career.\nOgden was a leading promoter and investor in the Illinois and Michigan Canal, then switched his loyalty to railroads. Throughout his later life, Ogden was heavily involved in the building of several railroads. \n\"In 1847, Ogden announced a plan to build a railway out of Chicago, but no capital was forthcoming. Eastern investors were wary of Chicago's reputation for irrational boosterism, and Chicagoans did not want to divert traffic from their profitable canal works. So Ogden and his partner J. Young Scammon solicited subscriptions from the farmers and small businessmen whose land lay adjacent to the proposed rail. Farmer's wives used the money they earned from selling eggs to buy shares of stock on a monthly payment plan. By 1848, Ogden and Scammon had raised $350,000\u2014enough to begin laying track. The Galena and Chicago Union Railroad was profitable from the start and eventually extended out to Wisconsin, bringing grain from the Great Plains into the city. As president of Union Pacific, Ogden extended the reach of Chicago's rail lines to the West coast.\"\nIn 1853, the Chicago Land Company, of which Ogden was a trustee, purchased land at a bend in the Chicago River and began to cut a channel, formally known as North Branch Canal, but also referred to as Ogden's Canal. The resulting island is now known as Goose Island.\nIn 1857, Ogden created the Chicago Dock and Canal Company. Ogden designed the first swing bridge in Chicago and donated the land for Rush Medical Center. Ogden was also a founder of the Chicago Board of Trade.\nIn 1860, Ogden acquired of land Brady's Bend along the Allegheny River in Pennsylvania, which contained iron and coal mines, rolling-mills, furnaces (today known as the Bradys Bend Iron Company Furnaces), and a village with approximately 1,500 residents. Along with several acquaintances, Ogden founded the Brady\u2019s Bend Iron Company with $2 million of capital. As of 1868, the company manufactured steel rails, employing 600 workers and producing 200 tons of rails per day.\nOgden served on the board of the Mississippi and Missouri Railroad and lobbied with many others for congressional approval and funding of the transcontinental railroad. After the 1862 Pacific Railroad Act, Ogden was named as the first president of the Union Pacific Railroad. Ogden was a good choice for the first president, but his railroad experience was most likely not the primary reason he was chosen; Ogden was a clever man who had many political connections. When Ogden came to lead the Union Pacific, the railroad was not fully funded and had not yet laid a single mile of track. The railroad existed largely on paper created by an act of Congress. As part of the 1862 Pacific Railroad Act, Congress named several existing railroad companies to complete portions of the project. Several key areas needed to link the East (Chicago) to the West had none, and hence the Union Pacific was formed by Congress.\nDuring the early days of railroading Ogden had begun building Northwestern railroads connecting Chicago with cities like Janesville, Fond du Lac and St. Paul/St. Anthony. In 1856 this was the Chicago, St. Anthony and Fond du Lac Railroad but the financial panic beginning in 1857 caused the collapse of this project. Fortunately Ogden's long time personal reputation and character helped him get many supporters putting together resources to reorganize as the Chicago &amp; North Western Railway the following year of which he was president from 1859 to 1868.\nWhile his failing health precluded as active a participation as in his earlier years, his vice president, Perry Smith and Supt. George L Dunlap carried over from the Fond du Lac era, kept things progressing until 1864 when a Grand Consolidation took place with the Galena &amp; Chicago Union Railroad. This new C&amp;NW was able to cross Iowa to the Missouri River at Council Bluffs and join with Ogden's other project, the Union Pacific Transcontinental railroad in Omaha. By 1867 he could see his beloved Chicago connected by rail with California.\nIn the late-1860s, his business required him to spend time in New York. To accommodate this, he built a villa in 1866 next to the High Bridge in Fordham Heights. In the following two years, he expanded his property to and a half-mile frontage along the Harlem River. He also continued to reside much of the year at his Chicago residence, which he also built an addition to around the same time.\nOgden was a fierce supporter of the transcontinental railroad at a time of great unrest for the country and was quoted as saying:\nThis project must be carried through by even-handed wise consideration and a patriotic course of policy which shall inspire capitalists of the country with confidence. Speculation is as fatal to it as secession is to the Union. Whoever speculates will damn this project.\nAs history now shows, eventually Ogden and many others got their wish.\nLater life.\nOn October 8, 1871, Ogden lost most of his prized possessions in the Great Chicago Fire. He also owned a lumber company in Peshtigo, Wisconsin, which burned the same day.\nPersonal life.\nHe married Marianna Tuttle Arnot (1825\u20131904). Marianna was the daughter of Scottish born John Arnot and Harriet (n\u00e9e Tuttle) Arnot. In New York, he named his home in the Highbridge, Bronx (named after the bridge now called Aqueduct Bridge over the Harlem River connecting Manhattan and the Bronx) Villa Boscobel.\nOgden died at his home in the Bronx on Friday, August 3, 1877. The funeral was held August 6, 1877, with several prominent pallbearers including, Gouverneur Morris III, William A. Booth, Parke Godwin, Oswald Ottendorfer, William C. Sheldon, Martin Zborowski, and Andrew H. Green. He was interred at Woodlawn Cemetery, Bronx.\nOgden, who had no children, left behind an estate valued at $10 million in 1877. Some of the money was used to fund a graduate school of science at the Old University of Chicago. Much was left to his niece Eleanor Wheeler, who married Alexander C. McClurg.\nLegacy.\nNamesakes of William B. Ogden include a stretch of U.S. Highway 34, called Ogden Avenue in Chicago and its suburbs, Ogden International School of Chicago, which is located on Walton Street in Chicago, and Ogden Slip, a man-made harbor near the mouth of the Chicago River. Ogden Avenue in The Bronx is also named after him, as is Ogden, Iowa. The Arnot-Odgen Memorial Hospital, founded by his wife Mariana, also bears his namesake. Following his death, William B. Ogden left money to his hometown of Walton, New York, which was used for the construction of a library, completed in 1897, which bears his name, the William B. Ogden Free Library, and is still in use today.\nA 1994 survey of experts on Chicago politics saw Ogden ranked as one of the ten best mayors in the city's history (up to that time).\nFootnotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "40288", "revid": "35936988", "url": "https://en.wikipedia.org/wiki?curid=40288", "title": "John Wentworth (Illinois politician)", "text": "American newspaper editor and politician (1815\u20131888)\nJohn Wentworth (March 5, 1815 \u2013 October 16, 1888), was the editor of the \"Chicago Democrat,\" publisher of an extensive Wentworth family genealogy, a two-term mayor of Chicago, and a six-term member of the United States House of Representatives (serving tenured in that body both before and after his service as mayor).\nAfter growing up in New Hampshire, he joined the migration west and moved to the developing city of Chicago in 1836, where he made his adult life. Wentworth was affiliated with the Democratic Party until 1855; then he changed to the Republican Party. After retiring from politics, he wrote a three-volume genealogy of the Wentworth family in the United States.\nA 1994 survey of experts on Chicago politics assessed Wentworth as one of the ten best mayors in the city's history (up to that time).\nEarly life and education.\nJohn Wentworth was born in Sandwich, New Hampshire. He was educated at the New Hampton Literary Institute and at the academy of Dudley Leavitt. Known as \"Long John\" he was 6 foot 6 inches in height. He graduated from Dartmouth College in 1836.\nMigration west and career.\nLater that year, Wentworth joined a migration west and moved to Chicago, arriving in the city on October 25, 1836. He became managing editor of Chicago's first newspaper, the \"Chicago Democrat,\" eventually becoming its owner and publisher. He owned the paper for 25 years.\nWentworth was admitted to the bar in 1841.\nHe started a law practice and entered politics. He was a business partner of Illinois financier Jacob Bunn, and the two men were two of the incorporators of the Chicago Secure Depository Company.\nMarriage and family.\nIn 1844, he married Roxanna Marie Loomis. \nIn later years, his nephew Moses J. Wentworth handled his business affairs, and would eventually manage his estate as well.\nPolitical career.\nWentworth served two separate one-year terms as mayor of Chicago (1857\u20131858 and 1860\u20131861) as well as three separate stints in the United States House of Representatives totaling six terms (1843\u20131851; 1853\u20131855; 1865\u20131867). The first five of his congressional terms were as a Democrat, while the final was as a Republican. His first mayoral term was as a Republican, while his second was as a Democrat.\nEarly political career.\nWentworth started his political involvement as a Jacksonian democrat, and promoted these views in the \"Chicago Democrat\". After he supported the 1837 mayoral candidacy of William Ogden, including throwing the newspaper behind Ogden's candidacy, he was appointed by Odgen to serve in the post of city printer.\nU.S. Congressman from Illinois's 4th district (1843\u20131851).\nWentworth, having become active in Democratic politics, was elected in 1842 to represent Illinois's 4th congressional district in the U.S. House of Representatives. He took office in March 1843. He was re-elected in 1844, 1846, and 1848.\nU.S. Congressman from Illinois's 2nd district (1853\u20131855).\nWentworth returned to the U.S. House of Representatives in March 1853, having been elected in 1852 to represent Illinois's 2nd congressional district. He did not run for re-election 1854, and left office in March 1855.\nAccording to city historians in Sandwich, Illinois, Wentworth was one of the key individuals who was responsible for the city getting a railroad stop. The town, which at the time, was called \"Newark Station\", was given the station, and in turn, the town gave Wentworth the honor of naming the town, which he subsequently named after his hometown, Sandwich, New Hampshire. It is also to note that the boundary line dispute with Wisconsin would have cut through present-day Sandwich, as it straddles the northern border with neighboring LaSalle County, which would have been the State Line had Wentworth not been successful in moving the line north\nChicago mayoralties (1857\u20131858; 1860\u20131861).\nWentworth returned to Chicago and affiliated with the Republican Party. Running as a Republican, he elected the mayor of Chicago in 1857. He served two terms, 1857\u20131858 and 1860\u20131861 (being elected to his second term in the 1860 Chicago mayoral election). In his second term, he again affiliated with the Democratic Party.\nAs mayor Wentworth instituted the use of chain gangs of prisoners in the city as laborers.\nIn July 1857, while serving as mayor of Chicago, Wentworth was charged with assaulting an attorney named Charles Cameron, who was attempting to communicate with his incarcerated client. Cameron testified that Wentworth \"seized him by the coat collar and shirt bosom\" and forcibly removed him from the prison, alleging that he had resisted officers. Wentworth, after requesting the case be delayed twice, refused to appear in court. The Judge found in favor of Cameron and charged Wentworth amounts of $25 \"and costs\" of $200.\nIn his effort to clean up the city's morals, he hired spies to determine who was frequenting Chicago's brothels. In 1857, Wentworth led a raid on \"the Sands,\" Chicago's red-light district, which resulted in the burning of the area.\nIn 1862, many of the city's Republicans had hoped to nominate him as the Republican nominee for mayor, but Charles N. Holden successfully defeated these efforts and secured the nomination for himself.\nA 1994 survey of experts on Chicago politics saw Wentworth ranked as one of the ten best mayors in the city's history (up to that time).\nU.S. Congressman from Illinois's 1st district (1865\u20131867).\nIn 1864, Wentworth ran for Congress as a Republican, and was elected for his last term (serving March 4, 1865\u00a0\u2013 March 3, 1867) representing Illinois's 1st congressional district. While he was in the House, there was a controversial vote to settle a boundary issue between Wisconsin and Illinois, with Wisconsin claiming land as far as the tip of Lake Michigan. Wentworth was promised that if he voted to give the land including Chicago to Wisconsin, he would be appointed to the US Senate. Wentworth declined the offer.\nChicago Board of Education (1860\u20131863; 1867\u20131871).\nFrom 1860 until 1863, Wentworth served on the Chicago Board of Education. Wentworth again served on the Chicago Board of Education from 1867 until 1871. Per some sources, he was board president in 1869, while other sources name S.A. Briggs as having held the office that year instead.\nLater life.\nAfter retiring from Congress, beginning in 1868 Wentworth lived at his country estate at 5441 South Harlem Avenue in Chicago. He owned about of land in what is today part of the Chicago neighborhood of Garfield Ridge and suburban Summit.\nWhen an author left a manuscript of a history of Chicago with Wentworth for his suggestions, he reportedly removed what did not refer to him and returned the manuscript to its author with the note, \"Here is your expurgated and correct history of Chicago.\"\nFamily historian.\nWentworth researched and wrote \"The Wentworth Genealogy \u2013 English and American\" twice, which he published privately. The first two-volume edition, also known as the \"private edition\", published in 1871, was followed by a second, corrected, edition in 1878, which was published in three volumes, for a total of 2241 pages. The total reported cost for both editions was $40,000. The first of the 1878 volumes chronicles the ancestry of Elder William Wentworth, the first of this family in New England, and his first five generations of New World descendants. The second and third volumes discuss the \"Elder's\" many descendants and others of the name. John was a fourth great-grandson of William.\nDeath.\nWentworth died at his estate in 1888, aged 73. He was buried in Rosehill Cemetery in Chicago.\nAt his request, his tombstone was a sixty-foot tall granite obelisk that was imported from New Hampshire on two railroad cars. It was, at the time, the tallest tombstone in the west.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "40289", "revid": "125972", "url": "https://en.wikipedia.org/wiki?curid=40289", "title": "Hiram College", "text": "Private liberal arts college in Hiram, Ohio, US\nHiram College ( ) is a private liberal arts college in Hiram, Ohio, United States. It was founded in 1850 as the Western Reserve Eclectic Institute by Amos Sutton Hayden and other members of the Christian Church (Disciples of Christ). The college is nonsectarian and coeducational. It is accredited by the Higher Learning Commission. Among its alumni is the 20th president of the United States, James A. Garfield, who also served as an instructor and principal at Hiram.\nHistory.\nOn June 12, 1849, representatives of the Christian Church (Disciples of Christ) voted to establish an academic institution, which would later become Hiram College. On November 7 that year, they chose the village of Hiram as the site for the school because the founders considered this area of the Connecticut Western Reserve to be \"healthful and free of distractions\". The following month, on December 20, the founders accepted the suggestion of Isaac Errett and named the school the Western Reserve Eclectic Institute.\nThe institute's original charter was authorized by the state legislature on March 1, 1850, and the school opened several months later, on November 27. Many of the students came from the surrounding farms and villages of the Western Reserve, but Hiram soon gained a national reputation and students began arriving from other states. On February 20, 1867, the Institute incorporated as a college and changed its name to Hiram College.\nDuring the years before it was renamed Hiram College, 1850\u20131867, the school had seven principals, the equivalent of today's college presidents. The two who did the most in establishing and defining the nature of the institution were Disciple minister Amos Sutton Hayden, who led the school through its first six years, and James A. Garfield, who had been a student at the institute from 1851 to 1853 and then returned in 1856 as a teacher. As principal, Garfield expanded the institute's curriculum. He left the institute in 1861 and in 1880 was elected the 20th president of the United States.\nIn 1870, one of Garfield's best friends and former students, Burke A. Hinsdale, was appointed Hiram's president. Although there were two before him, Hinsdale is considered the college's first permanent president because the others served only briefly. President Ely V. Zollars increased enrollment, established a substantial endowment, and created a program for the construction of campus buildings.\nIn 1931, shortly before Hiram celebrated the 100th anniversary of Garfield's birth, there was a debate in the community about changing the name of the school to Garfield College. There were strong advocates on both sides of the issue. Among the 2,000 guests at the centennial celebration were three generations of Garfield's family, including two of his sons. The idea of changing the college's name was not mentioned at the event, and the idea was abandoned.\nPrincipals and presidents.\nThe following is a list of the school's leaders since its founding in 1850.\nPresidents (Hiram College).\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nJames H. Barrow Field Station.\nIn 1967, Paul and Maxine Frohring donated their recently acquired 75 acres of land in Garrettsville to Hiram College to promote wildlife research. James H. Barrow, a biology professor at the college, founded the Hiram Biological Station on the land, causing the place to later be named in his honor in 1985. Originally, the property had a beech-maple forest, a stream and a bog, but over the years it grew into over 500 acres containing forests, fields, ponds, wetlands, and more. The Frohring Forest\u2013150 acres of mature beech-maple trees\u2013Silver Creek, Eagle Creek, and the Observation Pond\u2013which has many waterfowl species, along with a pair of trumpeter swans\u2013are some of the Field Stations' most well-known natural attractions. The property also contains multiple public hiking trails, and eleven facilities for different uses such as teaching, housing animals, research, and so on. Two of the most notable buildings are the Frohring Laboratory, which uses geothermal heating and cooling and was the first LEED certified building at Hiram, and also houses a miniature indoor aquarium with different breeds of fish, amphibians, reptiles, and occasionally mammals. The other is the Endangered Waterfowl Conservation Facility, which houses multiple different breeds of endangered birds and allows students to gain valuable hands-on experience.\nA unique program established by the Field Station is the Grassland Program. Beginning in 2011, Land Stewardship Manager Emliss Ricks has been working on establishing grasslands in three locations on the property.\nAcademics.\nAs of 2025, Hiram's student body consists of 777 degree-seeking students from 31 states and 7 foreign countries. Hiram specializes in the education of undergraduate students, though the college does have a small graduate program. Hiram confers the Bachelor of Arts, Bachelor of Science in Nursing, and Master of Arts degrees. The college offers 30 majors and 38 minors for traditional undergraduates, in addition to pre-professional programs for specific fields. Interdisciplinary studies have also been a part of Hiram's curriculum for decades.\nHiram's academic program consists of four schools: Arts, Humantities &amp; Politics; Business &amp; Communication; Health, Education, Sustainability &amp; Society; and Science &amp; Technology. The college's curriculum is marketed under the name Hiram Connect, which involves four steps: First Year Colloquium/Foundations of the Liberal Arts, Declaration of Major, Experiential Learning, and a Capstone Project. Hiram has five centers for interdisciplinary studies: Center for Integrated Entrepreneurship, Center for Scientific Engagement, Center for Literature and Medicine, Garfield Institute for Public Leadership, and Lindsay-Crane Center for Writing and Literature.\nRankings.\nHiram is a member of the Annapolis Group, which has been critical of the college rankings process. Hiram is among the signatories of the \"Presidents Letter\". In the 2026 \"U.S. News &amp; World Report\" college rankings, it was ranked tenth out of 89 regional liberal arts colleges in the Midwestern United States. Hiram has regularly been included in \"The Princeton Review\" Best Colleges guide, and is one of 40 schools included in Loren Pope's book \"Colleges That Change Lives\".\nStudent life.\nAthletics.\nThe school's sports teams are called the Terriers. They participate in NCAA Division III and the North Coast Athletic Conference. In men's volleyball and men's wrestling, sports not sponsored by the NCAC, Hiram competes in the Allegheny Mountain Collegiate Conference. Stunt, an all-female cheerleading discipline included in the NCAA Emerging Sports for Women program, competes in the Great Midwest Athletic Conference.\nHiram will join the Presidents' Athletic Conference (PAC) for administrative purposes on July 1, 2024, coinciding with its move to that conference as a men's volleyball associate. It will continue to compete in other NCAC sports through the 2024\u201325 school year before fully joining the PAC. Hiram had been a full PAC member from 1971 to 1989.\nThe Hiram College basketball team won the gold medal in the collegiate division of the 1904 Summer Olympics in St. Louis. It was the first time that basketball was part of an Olympics; it was included as a demonstration sport and no foreign teams participated.\nThe Cleveland Browns held their training camp at Hiram College from 1952 through 1974, making it the longest\u2013tenured training site in the team's history.\nThe 2014 Hiram vs. Mount St. Joseph women's basketball game was named the Best Moment at the 2015 ESPY Awards. The game featured terminally ill Mount St. Joseph player Lauren Hill in the first of her four college games, which set the all-time attendance record for an NCAA women's game below the Division I level.\nStudent clubs and organizations.\nStudent Senate is the elected student governing body of the college. It serves as a liaison between students and the school's administration, and oversees all student clubs and organizations, collectively called the Associated Student Organizations (ASO). The Kennedy Center Programming Board (KCPB) falls under the auspices of Student Senate, and is responsible for planning educational, social, recreational, and cultural programs.\nHiram has close to 50 registered student clubs and organizations in eight categories: Academic, Greek Social, Musical, Political and Activism, Publications and Communications, Religious, Special Interest and Service, and Sports and Recreation. Fraternities and sororities are not permitted on campus, but there are Greek social clubs.\nSince 1971, Hiram has maintained a chapter of Phi Beta Kappa, the national honor society for the liberal arts. The school has also had a chapter of Omicron Delta Kappa (ODK), a national leadership honor society, since 1962.\nNotable alumni and faculty.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "40290", "revid": "11570102", "url": "https://en.wikipedia.org/wiki?curid=40290", "title": "Joseph Medill", "text": "American newspaper editor, publisher, and politician (1823\u20131899)\nJoseph Medill (April 6, 1823 \u2013 March 16, 1899) was a Canadian-American newspaper editor, publisher, and Republican Party politician. He was co-owner and managing editor of the \"Chicago Tribune\", and he was Mayor of Chicago from after the Great Chicago Fire of 1871 until 1873.\nEarly life.\nJoseph Medill was born April 6, 1823, in Saint John, New Brunswick, British North America, to Margaret and William Medill. His parents were Scots-Irish. In 1832, the family moved to Massillon, Ohio. He grew up on a farm and was taught English grammar, Latin, logic and philosophy from Reverend Hawkins, a clergyman of the Methodist Episcopal Church in Canton. He graduated from the Massillon Academy in 1843. He read law under Hiram Griswold and was admitted to the Ohio Bar in 1846.\nEarly career.\nAfter joining the bar, he started a law practice with George W. McIlvaine. They dissolved their practice after three years.\nPublishing career.\nIn 1855, Medill sold his interest in the \"Leader\" to Cowles and bought the \"Tribune\" in partnership with Dr. Ray and Alfred Cowles (Edwin's brother).\nPolitical activity.\nMedill was a leading Republican in Chicago. Under Medill, the \"Tribune\" became the leading Republican newspaper in Chicago. Medill was strongly anti-slavery, supporting both the Free-Soil cause and Abolitionism. Medill was a major supporter of Abraham Lincoln in the 1850s. Medill and the \"Tribune\" were instrumental in Lincoln's presidential nomination, and were equally supportive of the Union cause during the American Civil War. The \"Tribune\"'s chief adversary through this period was the \"Chicago Times\", which supported the Democrats.\nMedill was among Chicago's Protestant elites (see, WASP). His rabid anti-Irish sentiment was published daily in The Chicago Tribune. He regularly dismissed the Irish as lazy and shiftless. \u201cWho does not know that the most depraved, debased, worthless and irredeemable drunkards and sots which curse the community are Irish Catholics?\u201d This came even as Irish laborers worked feverishly to complete Chicago's stately St. Patrick's church at Adams and Desplaines Streets in the mid-1850s.\nIn 1864, Medill left the \"Tribune\" editorship for political activity, which occupied him for the next ten years. He was appointed by President Grant to the first Civil Service Commission. In 1870, he was elected as a delegate to the Illinois Constitutional convention.\nMedill joined with Samuel Snowden Hayes and Rosell Hough (prominent Chicago Democrats) in order to oppose conditions of military draft laws during the American Civil War, feeling that the government was demanding too many troops to be drafted out of Cook County. On February 23, 1865, they met with President Lincoln. On February 27, they had a meeting with both Lincoln and Secretary of War Edwin Stanton. Stanton rejected their concerns. Lincoln castigated them, particularly chewing-out Medill. Lincoln argued that Chicagoans and Medill's newspaper had been most uncompromising in their opposition to the south's stance on slavery, and therefore should muster the men demanded of them to supply the Union with troops.\nMayoralty.\nIn 1871, after the Great Chicago Fire, Medill was elected mayor of Chicago as the candidate of the emergency fusion \"Union Fireproof\" party, defeating Charles C. P. Holden, and served as mayor for two years.\nMedill was sworn in as mayor on December 4, 1871.\nAs mayor, Medill gained more power for the mayor's office, created Chicago's first public library, enforced blue laws, and reformed the police and fire departments.\nDuring his mayoralty, Medill worked successfully to have the Illinois General Assembly modify the city charter to increase mayoral authority. As mayor-elect, on December 4, 1871, he tapped Judge Murray F. Tuley to draft a \"Mayor's Bill\" to be submitted to the General Assembly in its next session. After successful lobbying by Medill and Tuley, the bill passed on March 9, 1872. It went into effect July 1, 1872, and provided the mayor with the new authority to,\nIn his first year as mayor, Medill received very little legislative resistance from the Chicago Common Council. While he vetoed what was an unprecedented eleven Common Council ordinances that year, most narrowly were involved with specific financial practices considered wasteful and none of the vetoes were overridden. He used his new powers to appoint the members of the newly constituted Chicago Board of Education and the commissioners of its constituted public library. His appointments were approved unanimously by the Common Council.\nMedill sought funding for the recovery of Chicago. Medill had strongly lobbied on behalf of the city to receive state financial aid, taking advantage of his connections with state legislators in the state capitol of Springfield, Illinois. While, at the time, state law prohibited the direct appropriation of state funds to the city, Medill was able to get the legislature to pass a special act reimbursing the city for $2.9 million the city had expended on the state-owned Illinois and Michigan Canal. Medill also sought federal financial help. Medill took advantage of his connections in Washington, D.C., to seek such aid. In his third month in office, he wrote Vice President Schuyler Colfax to urge the passage of a tariff rebate that would help increase the supply of inexpensive material for the reconstruction of the city. Despite strong opposition from lumber interests, the legislation succeeded in passing. Medill also convinced President Grant to give a personal $1,000 contribution to aid the city's reconstruction. More than $5 million in gifts and loans were collected from people and cities across the world.\nTaking Medill's lead, on February 12, 1872, the Common Council approved 26-6 an ordinance that prohibited the construction of wood frame buildings in city limits.\nMedill was a strong Republican loyalist who supported President Grant for re-election in 1872. This caused a breach with Tribune editor, Horace White after White supported the breakaway Liberal Republicans, reformists who nominated Horace Greeley for president.\nIn his second year as mayor, tensions arose as he began to further utilize the new powers given to the mayor. At the first 1873 meeting of the Common Council, Medill announced that he would be using the power to select the chairmen of members of the council committees. He appointed his loyalists to lead most important committees, while aldermen of wards consisting of immigrant populations received lesser consideration for appointments. In the first three months of 1873 alone, Medill practiced his veto power on five Common Council ordinances.\nMedill and his police superintendent Elmer Washburn cracked down on gambling.\nMedill met not only resistance from a Common Council divided over his exercise of power and aspects of his agenda, but also resistance from citizens. Anton C. Hesing derided him as \"Joseph I, Dictator\".\nThe stress of the job of mayor impaired Medill's health. In August 1873, he appointed Lester L. Bond as Acting Mayor for the remaining 3\u00bd months of his term, and went to Europe on a convalescent tour.\nPersonal life.\nMedill married Katherine \"Kitty\" Patrick on September 2, 1852, and they had three daughters, Katherine, Elinor and Josephine. Medill died on March 16, 1899, at the age of 75 in San Antonio, Texas. He was buried at Graceland Cemetery in Chicago.\nLegacy and honors.\nDuring World War II, the Liberty ship was built in Panama City, and named in his honor.\nThe Medill School of Journalism, Media, and Integrated Marketing Communications at Northwestern University is also named in his honor.\nRelations.\nThe family tree omits Medill's third daughter, Josephine, who died in 1892.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "40291", "revid": "196446", "url": "https://en.wikipedia.org/wiki?curid=40291", "title": "Carter Harrison III", "text": "American politician (1825\u20131893)\nCarter Henry Harrison III (February 15, 1825\u00a0\u2013 October 28, 1893) was an American politician who served as mayor of Chicago, Illinois, from 1879 until 1887 and from 1893 until his assassination. He previously served two terms in the United States House of Representatives, and one term on the Cook County Board of Commissioners\nHarrison was a working-class aligned populist, and attained much political support among the labor unionists and Catholic white-ethnic immigrants of the city. While wealthy himself, Harrison fell into political disfavor among many of the city's business elites in his late political career. He was the father of Carter Harrison IV, who himself served five terms as the mayor of Chicago.\nA 1994 survey of experts on Chicago politics assessed Harrison as one of the ten best mayors in the city's history (up to that time).\nEarly life, education, and career.\nCarter Henry Harrison was born on a plantation on February 15, 1825, in rural Fayette County, Kentucky near Lexington, Kentucky to Carter Henry Harrison II and Caroline Russell. He was birthed in his family's home, a log cabin (as one obituary would remark, \"he saw the light in a log hut in a canebrake in Fayette County.\") When Harrison was merely eight months old, his father died.\nHarrison's family had a long Southern lineage, dating back to early colonial Virginia. He had ancestry in the Harrison family of Virginia, the Randolph family of Virginia, Carter family of Virginia, and Cabell family of Virginia. Harrison was a descendant of Richard A. Harrison, (a lieutenant general to Oliver Cromwell during the First English Civil War who had been involved in carrying out the execution of Charles I). His great-great-grandfather was Charles Harrison (brother to Carter Henry Harrison I and founding father Benjamin Harrison V, and a cousin of president Thomas Jefferson). Carter Harrison III was a cousin of U.S. Vice President John C. Breckinridge, and was a first cousin twice removed of U.S. President William Henry Harrison (thus making him also related to U.S. President Benjamin Harrison).\nHarrison was educated by private tutors. At the age of fifteen, he began to be tutored by Louis Marshall. Harrison graduated from Yale College in 1845 as a member of Scroll and Key. He graduated from Yale in 1851. Following graduation, he traveled the world and studied in Europe from 1851 to 1853. His travels took him to England, Ireland, Scotland, and elsewhere. He also visited Egypt and accompanied Bayard Taylor to explore Syria and Asia Minor. After his world travels, he entered Transylvania College in Lexington in 1853, where he earned a law degree in 1855.\nMove to Chicago, and early career in the city.\nIn 1855, Harrison wed his first wife, Sophy Preston. While traveling North for their honeymoon, Harrison stopped in Chicago and decided to settle there. He had decided to settle in Chicago because he saw it as a land of opportunity. At the time, he inherited the Kentucky plantation and almost 100 slaves but sold it away in order to be done with slavery.\nAfter settling in Chicago, Harrison invested in real estate in Chicago, and became a millionaire. Harrison was also admitted to the bar in 1855, and commenced practice once he settled in Chicago.\nThe first property that Harrison bought in Chicago was the Adams house located at the corners of Clark and Harrison Street\nCook County Board of Commissioners (1871\u20131875).\nAfter the Great Chicago Fire, Harrison became involved in politics. In the coinciding county elections, Harrison himself was elected to the Cook County Board of Commissioners. Harrison had run on a \"union\" ticket (dubbed the \"Fireproof Ticket\") that featured members of the Democratic and Republican parties. Harrison had been the leading figure in the formation of the Union\u2013Fireproof Ticket and served as the chair of its nominating convention. The ticket ran candidates in both the Cook County and Chicago municipal elections (including in the mayoral election). The ticket's mayoral nominee, Joseph Medill, won election. Harrison had been key in convincing Medill to run for mayor. Later, during Harrison's own career in citywide politics, Medill, publisher of the \"Chicago Tribune\", would come to be a political rival of Harrison's.\nIn addition to being included on Union\u2013Fireproof ticket, Harrison was also included as a county board nominee on the joint slate nominated by both the Cook County Democratic Party and Cook County Republican Party in the 1871 county and Chicago municipal elections (who partnered due to the extenuating circumstances of the fire).\nHarrison served a single term on the county board, for three years.\nU.S. House of Representatives (1875\u20131879).\nHarrison represented Illinois's 2nd congressional district for two terms (from 1875 until 1879). During the relevant period, with exception of the 6th ward, the district represented all of Chicago's \"West Division\" wards (the 7th, 8th, 9th, 10th, 11th, 12th, 13th, 14th, and 15th wards). During his time in congress, Harrison was noted for his flamboyant oration.\nUnsuccessful 1872 campaign.\nEarly into his tenure on the county board, Harrison ran an unsuccessful campaign in 1872 as the Democratic nominee in Illinois's 2nd congressional district for election to the 43rd United States Congress. Harrison faced Republican nominee Jasper D. Ward. The district had a strong Republican lean. Harrison, while unsuccessful, managed to greatly outperform previous Democratic nominees in the district. Harrison won 42.14% of the vote to Ward's 57.86%. Harrison led his opponent in the 8th and 9th wards, but trailed him in the other wards.\nThe congressional election coincided with the 1872 United States presidential election. Harrison (the Democratic congressional nominee) was listed on a local ticket that also included Liberal Republican Party presidential nominee Horace Greeley.\nFirst term (1875\u20131877).\nDue to his strong performance in his 1872 congressional campaign, in 1874 Harrison was again nominated in the 2nd congressional district by the Democratic Party for congress, and in a re-match against Ward won election 44th United States Congress by a margin of only eight votes. The \"Chicago Tribune\" would blame local Republicans' alignment with the Citizens Union ticket in the 1873 local Chicago elections as detrimental to Ward's re-election, as in the eyes of many voters it had placed Republicans on the less popular side of the \"beer question\" (positioning them in support of enforcing Sunday temperance blue laws (laws banning the sale of alcohol on Sundays). Many voters who typically voted Republican had in 1873 voted for the People's Party instead of the Citizens Union ticket due to the \"beer question\" At the time he was elected, he had been out-of-country visiting Europe with his family (traveling to Austria, Germany, Switzerland, and the Tyrol). After learning of his election, he returned to the United States to take office.\nIn 1875, during his first term in Congress, Harrison and his family again traveled to Europe. After accompanying his family through Northern Europe, Harrison returned to the United States while the rest of his family continued their trip. However, Harrison traveled to Europe again after his first wife died there.\nSecond term (1877\u20131879).\nWhile he was out-of-country due to the death of his wife, he was re-elected in 1876 to the 45th United States Congress. He won 50.89% of the vote against Republican opponent George R. Davis.\nScandal occurred in his second term in congress when, as chairman of the Committee on Reform of the Civil Service, Harrison had pushed through the payment of benefits to four self-proclaimed Union Army veterans purporting disabilities from wartime injuries despite the fact that their claims had previously been rejected. None of these individuals had actually seen active service, and none of them had suffered serious injuries.\nin 1878, Harrison lost re-election to congress. He was defeated by Miles Kehoe for re-nomination at the district's Democratic nominating convention.\nFirst mayoralty (1879\u20131887).\nDuring his first mayoralty, Harrison was elected mayor of Chicago for four consecutive two-year terms (in 1879, 1881, 1883, and 1885).\nAfter he campaigned in 1879 with a pet eagle, he became affectionately nicknamed \"the Eagle\".\nHe was sworn in for his first term on April 28, 1879.\nDuring his first mayoralty, he surpassed his predecessor Monroe Heath's title as the longest serving mayor Chicago had had up to that time.\nLeadership and popularity.\nHarrison has been described as a practitioner of charismatic authority. He governed the city in cooperation with a fractious Democratic Party organization.\nWhile Harrison garnered both business and working class support, the evangelical middle class generally disapproved of Harrison.\nInfrastructure and public safety.\nAt the time he took office, Chicago had nearly a half-million residents. However, it was still a developing city. Harrison would later remark that, when he took office as mayor, \"there were not ten miles of paved street in the whole city over which a light vehicle could move rapidly without injury to wheel or axle.\u201d Long a booster of his adopted city, Harrison was known to refer to Chicago as his \"bride\". Harrison significantly increased the city's number of paved roads and sidewalks in its downtown and increased the size and improved the efficiency of its fire department. Harrison also forced utility companies operating in the central business district to bury their wires. Harrison fought the Illinois Central Railroad's right to the lakefront, a legal battle which was ultimately taken by the State of Illinois to Supreme Court of the United States in \"Illinois Central Railroad Co. v. Illinois\". He also worked to persuade railroads to begin elevating their tracks to eliminate level crossings. He also attempted to push measures in the City Council that would have required locomotives, steamships and tugboats to burn anthracite, which burned cleaner. He also attempted to have the city build new and longer public water system intake pipes.\nHaymarket affair.\nHarrison's first mayoral tenure was a period that saw many events which brought the city national and international attention. One such event was the Haymarket affair. Early on the evening of the Haymarket affair in 1886, Harrison had casually observed the then still peaceful demonstration of anarchists and trade unionists and advised the police to leave the demonstrators alone; he then left the scene before the riot and anarchist bomb-throwing occurred. A significant reason for his ability to attend the rally unbothered was that, while Harrison came from a Protestant background, he needed the votes of and thus made appeals to the city's large ethnic White Catholic population as well as its rapidly growing numbers of trade unionists. His administration gave the impression of being more favorable to trade unions and strikes than those of previous Chicago mayors as well as other mayors of the time, although his police force routinely put down striking workers and trade union activists when they interfered with the businesses hiring replacements.\nIn the aftermath, Harrison spoke against anti-socialist sentiments being published in the media. Harrison argued that socialists were not sympathetic with bomb throwers, and remarked that socialists were representatives of the country's \"workers, thinkers, and writers.\"\n1884 Democratic National Convention.\nHarrison was a delegate to the 1880 and 1884 Democratic National Conventions. At the 1884 convention, held in Chicago, Harrison supported the successful candidacy of Grover Cleveland, and delivered the seconding speech for Cleveland's nomination at the convention. Harrison was also alleged to have ordered the Chicago police to fill the convention hall's convention hall with as many men sympathetic to Cleveland's candidacy as they could find on the street.\n1884 gubernatorial campaign.\nHeading encouragement from other Democrats, in 1884 Harrison ran as the party's nominee for governor of Illinois. While a reluctant nominee, he conducted an energetic and effective campaign. He lost to Republican incumbent Richard J. Oglesby. The result was unsurprising, considering that the state of Illinois had a strong Republican lean at the time. However, Harrison had managed to decrease the Republican margin of victory in the gubernatorial election from the 40,000 margin of the previous election to 14,500.\nEnd of tenure.\nTowards the end of his fourth term, public approval of Harrison had significantly declined, which hurt his prospects of being re-elected to a fifth term in the 1887 mayoral election. Much of the dissatisfaction with Harrison came from disapproval of his handling of the Haymarket Riot. Harrison's handling of the Haymarket Riot had also harmed his standing with conservative business groups. Furthermore, Harrison's prospects of re-election to a fifth term was weighed down by a scandal involving criminal charges of election fraud against some of his supporters (for conduct during the previous mayoral election). Even though the charges against these individuals were did not implicate Harrison in misconduct, there was still concern that public awareness of the scandal would muddy Harrison's public image due to his proximity to the indicted individuals. Harrison's loss of public favor had led the prospect of re-nominating him to lose losing support within city's Democratic Party. Initially, Harrison maintained intention to be re-elected, and unsuccessfully attempted to persuade the United Labor Party to support him for re-election and to partner with the city Democratic Party to nominate a joint-slate. Harrison proposed a fusion nomination arrangement that would have seen the parties nominate identical tickets. Harrison's failure to persuade Chicago's United Labor Party to partner with city Democrats further harmed Harrison's support within the local Democratic party.\nRecognizing that he would have difficulty being re-nominated by the Democratic Party, Harrison decided to retire at the end of his fourth term, opting against seeking re-election in 1887. The Democratic Party voted at its convention to nominate DeWitt Clinton Cregier. However, Cregier declined the nomination, refusing to run. After this (and despite his declared intent to retire) the party voted to re-nominate Harrison. Harrison initially accepted the nomination. However, before he could begin campaigning, his wife Elizabeth died. Experiencing great grief over his wife's passing, he withdrew from the election, and instead embarked on international travels. His tenure as mayor formally ended on April 18, 1887.\nPost-mayoralty.\nOn July 26, 1887, Harrison embarked on international travels, taking a sixteen-month world tour. He concluded this trip on November 8, 1888. Harrison documented his travels in letters he wrote that were published in newspapers. He compiled his travel writings into a book, which was published under the title \"A Race With The Sun\". After returning to Chicago, Harrison continued to be withdrawn from politics for several years. During this time, he focused on his business ventures and authoring literature. In 1890, Harrison and his daughter took a vacation trip from Chicago to Yellowstone National Park and Alaska. His letters from the trip were first published in the \"Chicago Tribune\" and later compiled into an 1891 book, \"A Summer's Outing and The Old Man's Story\".\nIn 1891, Harrison became the owner and editor of the \"Chicago Times\". This marked a return to political engagement, as he used this newspaper to advocate for labor unions and the many Catholic and immigrant communities in Chicago.\nUnsuccessful 1891 mayoral campaign.\nHarrison unsuccessfully sought to stage a comeback, running in the 1891 Chicago mayoral election. Failing to receive the Democratic nomination (with incumbent mayor DeWitt Clinton Cregier being renominated instead), Harrison ran an independent campaign for mayor. The election became a four-candidate race between Harrison, Creiger, Republican nominee Hempstead Washburne, and Citizens nominee Elmer Washburn (a former head of the United States Secret Service and Chicago Police Department). Hempstead Washburne won the election, receiving only a few hundred more votes than runner-up Cregier and a few thousand more votes than Harrison. Elmer Washburn placed a more distant fourth.\nSecond mayoralty (1893).\nHarrison was again elected mayor in 1893, in time for the World's Columbian Exposition being held in the city. Harrison stated that his desire was to show the world the \"true\" Chicago during the world's fair.\nHarrison was sworn in for his fifth nonconsecutive term on April 17, 1893. Harrison's first acts after being sworn in were to immediately submit vetoes of several ordinances that the council had already passed, one which served the interests of the Midland Elevated Railway (which stockbroker James R. Keene held significant stake in) and another which would have granted the Hygeia Springs Company permission to supply water into the city (which would have advanced a controversial project by Wisconsin businessman James C. McElroy to pipe water from the famed springs in Waukesha, Wisconsin to the grounds of the world's fair). Mayor Washburne had similarly vetoed the same ordinances in his final act as mayor. All vetoes were sustained.\nHarrison appointed 1st Ward Alderman \"Bathhouse\" John Coughlin to sit on the reception committee for the world's fair. This appointment was a small part in Harrison's plan to create a centralized Democratic Party machine consisting of empowered ward committeemen and precinct captains that would answer to the local Democratic Party. The plan would not be accomplished until Anton Cermak came to power in Chicago politics in the 1920s.\nIn July, Harrison proposed the idea of building a new municipal water intake crib offshore of the city in Lake Michigan, and adjoining it to an artificial island built atop stilts anchored to the lakebottom. He envisioned such an island serving as \"a summer resort...a pleasure garden, a picnicking ground.\"\nDuring the expsoition, Harrison entertained visiting dignitaries. On June 27, he hosted a breakfast at his personal residence for an official representative of the Spanish Monarchy. Harrison also delivered speeches during a number of themed days and other events of the expositon. At one event, he generated outrage by suggesting in a speech to a Canadian audience at a Dominion Day event that Americans would invite annexation of Canada to the United States. In August, during \"British Empire Day\" at the fair, he jokingly doubled-down on this by telling a British audience, by saying that Americans would invite an annexation of Great Britain alongside Canada. In the same remarks, however, he offered praise of the influence that the British Empire had, and its influence in the world's economy, and spoke in astonishment of the vast reaches of its empire.\nOn August 8, during an assembly of military surgeons of the United States National Guard, Harrison expressed his concern about unemployment in Chicago amid the Panic of 1893. He predicted that this situation, if not addressed with federal government funds, would inspire civil unrest in the city, remarking, \"There are 200,000 people in Chicago today unemployed and almost destitute for money. If Congress does not give us plenty of money we will have riots that will shake the country.\" Indeed, significant arrest with national reverberations did ultimately arise in Chicago, during the Pullman Strike in 1894.\nAssassination.\nOn October 28, 1893, a few months into his fifth term and just two days before the close of the World's Columbian Exposition, Harrison was murdered in his home by Patrick Eugene Prendergast, an office-seeker who had supported Harrison's re-election under the idea that Harrison would reward him with an appointment to a post within his mayoral administration. Harrison was buried in Chicago's Graceland Cemetery. As part of his funeral services, Harrison lay in state in the City Hall. A celebration planned for the close of the Exposition was cancelled and replaced by a large public memorial service for Harrison. Prendergast was sentenced to death for the crime and hanged on July 13, 1894.\nWhile Harrison died at a time when the elites, Protestants, and Republicans of all kinds greatly disliked him, he never lost his core supporters of labor unions, Catholics and immigrants. He was Chicago's first mayor to be elected five times; eventually his son Carter Harrison IV was also elected mayor five times.\nHarrison's career and assassination are closely associated with the World's Columbian Exposition, and are discussed at some length as a subplot to the two main stories (about the fair and serial killer H. H. Holmes) in Erik Larson's best-selling 2003 non-fiction book \"The Devil in the White City\".\nPolitical views.\nHarrison was a populist Democrat. He was aligned with many causes that spoke to the working class in Chicago, being regarded as a voice for the concerns \"the great unwashed\" of Chicago (i.e., members of the city's non-Protestant immigrant ethnic groups, that were often derided by many of the city's business elites).\nIn the 1880s (after the Haymarket Affair), Harrison spoke in disagreement with the news media's condemnation of American socialists. He argued that (contrary to media representations) socialists were not supportive of bomb throwers, and that socialists spoke to common concerns of the United States's \"workers, thinkers, and writers\". In his final political campaign (Chicago's 1893 mayoral election), Harrison campaigned on reforming Chicago's tax assessment system to equalize the tax burden (taxing the wealthier more). While he was aligned with the working class and fell into late political disfavor the city's business elite held for him, Harrison was himself a man of significant personal wealth, and remained poker buddies with numerous members of the city's business elite up until his death.\nHarrison positioned himself as tolerant towards liquor consumption or gambling, and did not support government crackdowns on these behaviors. Rather than supporting blue laws, Harrison supported and encouraged the operation of saloons in ethnic neighborhoods, viewing them as functioning as community gathering-spaces out of which some services of further community benefit (such as post service) might also be operated.\nHarrison saw the Chicago's strength as being in its neighborhoods, and viewed it as being a city of neighborhoods.\nHailing from the Upland South and wed to a woman who hailed from the Deep South, during the American Civil War, Harrison had occasionally openly expressed sympathy towards the Confederate cause, leading him to be derided as a Copperhead. In a speech during \"Great Britain Day\" at the World's Fair, Harrison expressed wonder at the vast reaches of the British Empire, and opined that Britain's colonial rule over India was due to Britain's superiority, remarking, \"India, with her 280 million of population has to acknowledge the supremacy of the intelligence and wealth of Great Britain, with her population of only 50 million.\"\nPersonal life.\nMarriages and late engagement.\nOn April 12, 1855, Harrison married his first wife, the former Sophie Preston. She hailed from the Preston family, a distinguished southern family. Harrison and his first wife, Sophie, had ten children together. Six of their children died either in infancy or early childhood.\nHarrison became a widower after Sophie died in Europe in 1876. After being widowed, Harrison married Margarette (alternatively spelled \"Margaret\" or \"Marguerite\") E. Stearns in 1882. Stearns was a member of one of Chicago's earliest and most wealthy families, being the daughter of Chicago pioneer Marcus C. Stearns. He was widowed again when she died in 1887.\nAt the time of his assassination, Harrison was engaged to a young New Orleans heiress named Annie Howard, daughter of Louisiana State Lottery Company organizer Charles T. Howard, who had been worth an estimated $3,000,000, $700,000 of which she had inherited from him after his death. The Harrison's had originally sent out invitations for a November 7, 1893 wedding. However, realizing that November elections would be held on this day, Harrison rescheduled the wedding for November 12. Harrison was killed two weeks prior to his and Howard's planned wedding date.\nChildren.\nHarrison and his first wife, Sophie, had ten children together. Six of their children died either in infancy or early childhood.\nThe four children whom survived to adulthood were Lina, Carter IV, William Preston, and Sophie.\nLina married Heaton Owsley, and became the step-mother of Jack Owsley (who became a noted American football player and coach, as well as a successful businessman). She and Owsley had a daughter who they also named Lina Harrison Owsley. This daughter of the Owsleys (granddaughter of Mayor Harrison) performed as an opera singer, studying opera under Hermann Devri\u00e8s. In 1912, she married Paul Bartlett, a noted painter.\nCarter IV served as mayor of Chicago from 1897 to 1905 and 1911 to 1915. He married Edith Ogden (who would garner note as the author of children's fairytale books) in 1887. Together, they had three children. Their firstborn died in infancy in 1889. Their other two children survived to adulthood: son Carter V (born in 1891) and daughter Edith II (born 1896).\nOther personal details.\nIn September 1892, Harrison experienced what was described as a near-fatal accident in which he was thrown from a horse. He incurred injury, suffering a broken arm and significant bruising.\nLegacy.\nThe Carter H. Harrison Medal is one of two medals \"granted to sworn members of the fire and police departments who have performed distinguished acts of bravery in the protection of life or property\", the other being the Lambert Tree Award.\nA statue of Harrison is in Union Park on Chicago's Near West Side, about two blocks from the Ashland Avenue home where he lived and was murdered in 1893. It was erected in 1907. The plaque on the statue is a quote from Harrison's address to the World's Columbian Exposition, given hours before he died.\nA 1994 survey of experts on Chicago politics saw Harrison ranked as one of the ten best mayors in the city's history (up to that time)."}
{"id": "40292", "revid": "5718152", "url": "https://en.wikipedia.org/wiki?curid=40292", "title": "Carter Harrison IV", "text": "American politician (1860\u20131953)\nCarter Henry Harrison IV (April 23, 1860 \u2013 December 25, 1953) was an American newspaper publisher and Democratic politician who served a total of five terms as mayor of Chicago (1897\u20131905 and 1911\u20131915) but failed in his attempt to become his party's presidential nominee in 1904. Descended from aristocratic Virginia families and the son of five-term Chicago mayor Carter Harrison III, this Carter Harrison (IV) became the first native Chicagoan elected its mayor.\nA 1994 survey of experts on Chicago politics assessed Harrison as one of the ten best mayors in the city's history (up to that time).\nEarly life and career.\nHarrison was born on April 23, 1860, in Chicago at his family residence at the corner of the streets that are today known as Clark and Harrison.\nHarrison's father, Carter Harrison III, served as mayor of Chicago and was assassinated in October 1893. \nHarrison received his early education in Chicago, before finishing in Saxe-Altenburg, Germany. He attended college in Chicago at the College of St. Ignatius, graduating in its class of 1881. He attended the Yale Law School, receiving his law degree in 1883.\nHarrison practiced as a lawyer for five years, before returning to Chicago to help his brother Preston run the \"Chicago Times\" (which their father bought in 1891). Under the Harrisons the paper became a resolute supporter of the Democratic Party, and was the only local newspaper to support the Pullman strikers in the mid-1890s. Harrison served as the newspaper's managing editor, while his brother served as its business manager. The family sold the newspaper after the 1894 Pullman Strike, and Harrison stopped working there in 1895. The newspaper was soon merged with the \"Chicago Herald\" to form the \"Chicago Times-Herald\". After leaving the newspaper industry, Harrison entered the real estate industry and saw success.\nPolitical career.\nSimilarly to his father, Harrison IV won election to five terms as Chicago's mayor. \nFirst mayoralty.\nHarrison was first elected mayor in the 1897 Chicago mayoral election. He would win election to three consecutive additional two-year terms in 1899, 1901, and 1903.\nHarrison was sworn in as mayor on April 15, 1897.\nLike his father, Harrison did not believe in trying to legislate morality. As mayor, Harrison believed that Chicagoans' two major desires were to make money and to spend it. During his administrations, Chicago's vice districts blossomed, and special maps were printed to enable tourists to find their way from brothel to brothel. The name of one Chicago saloon-keeper of the time entered the English language as a term for a strong or laced drink intended to render unconsciousness: Mickey Finn.\nIn the late-1890s, Altgeld aligned himself with the free silver and William Jennings Bryan-aligned wings of the national Democratic party. Harrison was speculated as a potential candidate for his party's 1900 presidential nomination.\nHowever, Harrison was seen as more of a reformer than his father, which helped him garner the middle class votes his father had lacked. One of Harrison's biggest enemies was Charles Yerkes, whose plans to monopolize Chicago's streetcar lines were vigorously attacked by the mayor. This was the beginning of the Chicago Traction Wars, which would become a major focus of his administration. During his final term in office, Harrison established the Chicago Vice Commission and worked to close down the Levee district, starting with the Everleigh Club brothel on October 24, 1911.\nDespite prolonged and damaging international press coverage blaming his lax municipal enforcement for the 602 lives lost in the Iroquois Theatre fire on December 30, 1903 (still the deadliest single-building fire in U.S. history), Harrison hoped to become the 1904 Democratic nominee for President of the United States. However, he was unsuccessful in this effort. The nomination went to Alton B. Parker, who was soundly defeated by Theodore Roosevelt.\nHarrison declined to seek a fifth consecutive mayoral term in 1905, and was succeeded by fellow Democrat Edward Fitzsimmons Dunne on April 10, 1905.\nBetween mayoralties.\nIn 1907, attempting to stage a return to office, Harrison unsuccessfully challenged Dunne for the Democratic mayoral nomination.\nSecond mayoralty.\nIn 1911, Harrison was elected to a four-year term as mayor. He as sworn in for his fifth nonconsecutive term as mayor on April 17, 1911.\nIn 1914, Harrison convinced the city council to establish a Commission for the Encouragement of Local Art to purchase works of art by Chicago artists. Harrison personally purchased artwork from painters such as Victor Higgins and Walter Ufer.\nHarrison sought a sixth overall term as mayor in 1915, but was defeated in the Democratic primary by Robert Sweitzer, who went on to lose the general election to Republican William Hale Thompson. Harrison was succeeded in office by Thompson on April 26, 1915.\nIn 1915, when Harrison left office, Chicago had essentially reached its modern size in land area, and had a population of 2,400,000; the city was moving inexorably into its status as a major modern metropolis. He and his father had collectively been mayors of the city for 21 of the previous 36 years.\nA 1994 survey of experts on Chicago politics saw Harrison ranked as one of the ten-best mayors in the city's history (up to that time).\nPost-mayoralty.\nFrom 1933 through 1944, Harrison served as the Internal Revenue Service's collector for district of Chicago. He was appointed to this position by president Franklin D. Roosevelt on July 28, 1933.\nHarrison served as the president of a commission which advocated for local arts.\nHe published two autobiographies. One of these, a memoir entitled \"Growing Up with Chicago\", was published in 1944.\nHarrison died on December 25, 1953, at his Chicago apartment, and is buried in Graceland Cemetery.\nHis papers are held by Chicago's Newberry Library.\nAncestry and personal life.\nHarrison was a descendant of Robert Carter I, Benjamin Harrison IV, William Randolph, and Isham Randolph of Dungeness.\nHarrison's wife, Edith Ogden Harrison, was a well-known writer of children's books and fairy tales in the first two decades of the 20th century.\nHarrison as regarded to be an avid outdoorsman and sportsman. He appreciated nature, and also partook in equestrianism and horseback riding. A skilled angler, he had gone fishing in states such as Michigan and Florida as well as abroad in countries such as Egypt and Switzerland.\nHe was a member of many organizations including the Freemasons, Society of the Cincinnati, Sons of the Revolution, Sons of the American Revolution, Society of Colonial Wars, Veterans of Foreign Wars, American Legion, and the Military Order of the World Wars. He was also a member of Chicago's Century Cycling Club. In 1907 Harrison became a hereditary member of the Virginia Society of the Cincinnati.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "40293", "revid": "25004493", "url": "https://en.wikipedia.org/wiki?curid=40293", "title": "Nikkei 225", "text": "Japanese stock market index\nThe Nikkei 225, or , more commonly called the Nikkei or the Nikkei index (), is a stock market index for the Tokyo Stock Exchange (TSE). It is a price-weighted index, operating in the Japanese Yen (JP\u00a5), and its components are reviewed twice a year. The Nikkei 225 measures the performance of 225 highly capitalised and liquid publicly owned companies in Japan from a wide array of industry sectors. Since 2017, the index is calculated every five seconds. It was originally launched by the Tokyo Stock Exchange in 1950, and was taken over by the \"Nihon Keizai Shimbun\" (\"The Nikkei\") newspaper in 1970, when the Tokyo Exchange switched to the Tokyo Stock Price Index (TOPIX), which is weighed by market capitalisation rather than stock prices.\nHistory.\nThe Nikkei 225 began to be calculated on September 7, 1950, retroactively calculated back to May 16, 1949 when the average price of its component stocks was \u00a5176.21 () using a divisor of 225. Since July 2017, the index is updated every 5\u00a0seconds during trading sessions.\nThe Nikkei 225 Futures, introduced at Singapore Exchange (SGX) in 1986, the Osaka Securities Exchange (OSE) in 1988, Chicago Mercantile Exchange (CME) in 1990, is now an internationally recognized futures index.\nThe Nikkei average has deviated sharply from the textbook model of stock averages, which grow at a steady exponential rate. During the Japanese asset price bubble, the average hit its bubble-era record high on December 29th, 1989, when it reached an intraday high of 38,957.44, before closing at 38,915.87, having grown sixfold during the decade. Subsequently, it lost nearly all these gains, reaching a post-bubble intraday low of 6,994.90 on October 28th, 2008 \u2014 82% below its peak nearly 19 years earlier. The 1989 record high held for 34 years, until it was surpassed in 2024 (see below).\nOn March 15th, 2011, the second working day after the massive earthquake in the northeast part of Japan, the index dropped over 10% to finish at 8,605.15, a loss of 1,015 points. The index continued to drop throughout 2011, bottoming out at 8,160.01 on November 25th, putting it at its lowest close since March 31st, 2009. The Nikkei fell over 17% in 2011, finishing the year at 8,455.35, its lowest year-end closing value in nearly thirty years, when the index finished at 8,016.70 in 1982.\nThe Nikkei started 2013 near 10,600, hitting a peak of 15,942 in May. However, shortly afterward, it plunged by almost 10% before rebounding, making it the most volatile stock market index among the developed markets. By 2015, it had reached over 20,000 mark, marking a gain of over 10,000 in two years, making it one of the fastest growing stock market indices in the world. However, by 2018, the index growth was more moderate at around the 22,000 mark.\nThere was concern that the rise since 2013 was artificial and due to purchases by the Bank of Japan (\"BOJ\"). From a start in 2013, by end 2017, the BOJ owned circa 75% of all Japanese Exchange Traded Funds (\"ETFs\"), and were a top 10 shareholder of 90% of the Nikkei 225 constituents.\nOn February 15th, 2021, the Nikkei average breached the 30,000 benchmark, its highest level in 30 years, due to the levels of monetary stimulus and asset purchase programs executed by the Bank of Japan to mitigate the financial effects of the COVID-19 pandemic.\nOn February 22nd, 2024, the Nikkei reached an intraday high of 39,156.97 and closed at 39,098.68, finally surpassing its 1989 record high, an important milestone since the Japanese asset price bubble. On March 4th, 2024, the index surpassed 40,000 (intraday and closing) for the first time in history.\nOn August 5th, 2024, amid a global stock market decline, the Nikkei dropped by more than 4,200 points, surpassing 1987's Black Monday as its biggest single-day drop in history. The following day, it bounced back by more than 3,200 points, the largest single-day gain in history.\nWeighting.\nThe index is a price-weighted index. The index is calculated as follows:\nformula_1, then formula_2\nAs of 2025[ [update]], the divisor is 29.91719460. That is, every \u00a5100 change in the adjusted price of a constituent stock results in a 3.3426 unit movement in the index.\nAs of 2025[ [update]], the company with the largest influence on the index is Advantest (TYO: https://), at about 11% weight.\nContract specifications.\nThe Nikkei 225 is traded as a future on the Osaka exchange (OSE). The contract specifications for the Nikkei 225 (OSE ticker symbol JNK) are listed below:\nAnnual returns.\nThe following table shows the annual development of the Nikkei 225, which was calculated back to 1914.\nComponents.\n&lt;templatestyles src=\"Pie chart/styles.css\"/&gt;\nIt is not recorded how components were originally selected, or why there are 225 components. Today, components are selected by \"considering the weights of the industrial sectors\". Constituents must be highly traded (\"liquid\"). Constituents are changed either at periodic review one per year, or by \"extraordinary replacement\", for example if a company is delisted.\nAs of October 2025, the Nikkei 225 consists of the following companies (Japanese securities identification code in parentheses): Bold indicates the top ten by market capitalisation.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "40294", "revid": "7903804", "url": "https://en.wikipedia.org/wiki?curid=40294", "title": "Stephen Smale", "text": "American mathematician (born 1930)\nStephen Smale (born July 15, 1930) is an American mathematician, known for his research in topology, dynamical systems and mathematical economics. He was awarded the Fields Medal in 1966 and spent more than three decades on the mathematics faculty of the University of California, Berkeley (1960\u20131961 and 1964\u20131995), where he currently is Professor Emeritus, with research interests in algorithms, numerical analysis and global analysis.\nEducation and career.\nSmale was born in Flint, Michigan and entered the University of Michigan in 1948. Initially, he was a good student, placing into an honors calculus sequence taught by Bob Thrall and earning himself A's. However, his sophomore and junior years were marred with mediocre grades, mostly Bs, Cs and even an F in nuclear physics. Smale obtained his Bachelor of Science degree in 1952. Despite his grades, with some luck, Smale was accepted as a graduate student at the University of Michigan's mathematics department. Yet again, Smale performed poorly in his first years, earning a C average as a graduate student. When the department chair, Hildebrandt, threatened to kick Smale out, he began to take his studies more seriously. Smale finally earned his PhD in 1957, under Raoul Bott, beginning his career as an instructor at the University of Chicago.\nEarly in his career, Smale was involved in controversy over remarks he made regarding his work habits while proving the higher-dimensional Poincar\u00e9 conjecture. He said that his best work had been done \"on the beaches of Rio.\" He has been politically active in various movements in the past, such as the Free Speech movement and member of the Fair Play for Cuba Committee. In 1966, having travelled to Moscow under an NSF grant to accept the Fields Medal, he held a press conference there to denounce the American position in Vietnam, Soviet intervention in Hungary and Soviet maltreatment of intellectuals. After his return to the US, he was unable to renew the grant. At one time he was subpoenaed by the House Un-American Activities Committee.\nIn 1960, Smale received a Sloan Research Fellowship and was appointed to the Berkeley mathematics faculty, moving to a professorship at Columbia the following year. In 1964 he returned to a professorship at Berkeley, where he has spent the main part of his career. He became a professor emeritus at Berkeley in 1995 and took up a post as professor at the City University of Hong Kong. He also amassed over the years one of the finest private mineral collections in existence. Many of Smale's mineral specimens can be seen in the book \"The Smale Collection: Beauty in Natural Crystals\".\nFrom 2003 to 2012, Smale was a professor at the Toyota Technological Institute at Chicago; starting August 1, 2009, he became a Distinguished University Professor at the City University of Hong Kong.\nIn 1988, Smale was the recipient of the Chauvenet Prize of the MAA. In 2007, Smale was awarded the Wolf Prize in mathematics.\nResearch.\nSmale proved that the oriented diffeomorphism group of the two-dimensional sphere has the same homotopy type as the special orthogonal group of 3 \u00d7 3 matrices. Smale's theorem has been reproved and extended a few times, notably to higher dimensions in the form of the Smale conjecture, as well as to other topological types.\nIn another early work, he studied the immersions of the two-dimensional sphere into Euclidean space. By relating immersion theory to the algebraic topology of Stiefel manifolds, he was able to fully clarify when two immersions can be deformed into one another through a family of immersions. Directly from his results it followed that the standard immersion of the sphere into three-dimensional space can be deformed (through immersions) into its negation, which is now known as sphere eversion. He also extended his results to higher-dimensional spheres, and his doctoral student Morris Hirsch extended his work to immersions of general smooth manifolds. Along with John Nash's work on isometric immersions, the Hirsch\u2013Smale immersion theory was highly influential in Mikhael Gromov's early work on development of the h-principle, which abstracted and applied their ideas to contexts other than that of immersions.\nIn the study of dynamical systems, Smale introduced what is now known as a Morse\u2013Smale system. For these dynamical systems, Smale was able to prove Morse inequalities relating the cohomology of the underlying space to the dimensions of the (un)stable manifolds. Part of the significance of these results is from Smale's theorem asserting that the gradient flow of any Morse function can be arbitrarily well approximated by a Morse\u2013Smale system without closed orbits. Using these tools, Smale was able to construct \"self-indexing\" Morse functions, where the value of the function equals its Morse index at any critical point. Using these self-indexing Morse functions as a key tool, Smale resolved the generalized Poincar\u00e9 conjecture in every dimension greater than four. Building on these works, he also established the more powerful h-cobordism theorem the following year, together with the full classification of simply-connected smooth five-dimensional manifolds.\nSmale also introduced the horseshoe map, inspiring much subsequent research. He also outlined a research program carried out by many others. Smale is also known for injecting Morse theory into mathematical economics, as well as recent explorations of various theories of computation.\nIn 1998 he compiled a list of 18 problems in mathematics to be solved in the 21st century, known as Smale's problems. This list was compiled in the spirit of Hilbert's famous list of problems produced in 1900. In fact, Smale's list contains some of the original Hilbert problems, including the Riemann hypothesis and the second half of Hilbert's sixteenth problem, both of which are still unsolved. Other famous problems on his list include the Poincar\u00e9 conjecture (now a theorem, proved by Grigori Perelman), the P = NP problem, and the Navier\u2013Stokes equations, all of which have been designated Millennium Prize Problems by the Clay Mathematics Institute.\nImportant publications.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "40295", "revid": "10773111", "url": "https://en.wikipedia.org/wiki?curid=40295", "title": "Hawker Siddeley Nimrod", "text": "Maritime patrol aircraft family by Hawker Siddeley, later British Aerospace\nThe Hawker Siddeley Nimrod is a retired maritime patrol aircraft developed and operated by the United Kingdom. It was an extensive modification of the de Havilland Comet, the world's first operational jet airliner. It was originally designed by de Havilland's successor firm, Hawker Siddeley; further development and maintenance work was undertaken by Hawker Siddeley's own successor companies, British Aerospace and, later, BAE Systems.\nDesigned in response to a requirement issued by the Royal Air Force (RAF) to replace its fleet of ageing Avro Shackletons, the \"Nimrod MR1\"/\"MR2\"s were fixed-wing aerial platforms primarily for anti-submarine warfare (ASW) operations; secondary roles included maritime surveillance and anti-surface warfare. It served from the early 1970s until March 2010. The intended replacement was to be extensively rebuilt Nimrod MR2s, designated Nimrod MRA4. Due to considerable delays, repeated cost overruns, and financial cutbacks, the development of the MRA4 was abandoned in 2010.\nThe RAF also operated three Nimrod R1, an electronic intelligence gathering (ELINT) variant. A dedicated airborne early warning platform, the Nimrod AEW3, was in development from late 1970s to the mid-1980s; however, much like the MRA4, considerable problems were encountered in development and thus the project was cancelled in 1986 in favour of an off-the-shelf solution in the Boeing E-3 Sentry. All Nimrod variants had been retired by mid-2011.\nDevelopment.\nMR1.\nOn 4 June 1964, the British Government issued Air Staff Requirement 381, which sought a replacement for the aging Avro Shackleton maritime patrol aircraft of the Royal Air Force (RAF). Such a replacement had been necessitated by the rapidly-approaching fatigue life limitations accumulated across the Shackleton fleet. A great deal of interest in the requirement was received from both British and foreign manufacturers, who offered aircraft including the Lockheed P-3 Orion, the Breguet Atlantic and derivatives of the Hawker Siddeley Trident, BAC One-Eleven, Vickers VC10 and de Havilland Comet. On 2 February 1965, Prime Minister Harold Wilson announced the intention to order Hawker Siddeley's maritime patrol version of the Comet, the HS.801 as a replacement for Shackleton Mk 2.\nThe Nimrod design was based on the Comet 4 civil airliner which had reached the end of its commercial life (the first two prototype Nimrods, XV148 and XV147, were built from two final unfinished Comet 4C airframes). The Comet's Rolls-Royce Avon turbojet engines were replaced by Rolls-Royce Spey turbofans for better fuel efficiency, particularly at the low altitudes required for maritime patrol. Major fuselage changes were made, including an internal weapons bay, an extended nose for radar, a new tail with electronic warfare (ESM) sensors mounted in a bulky fairing, and a MAD (magnetic anomaly detector) boom. After the first flight in May 1967, the RAF ordered 46 Nimrod MR1s. The first example delivered (XV230) entered service in October 1969. A total of five squadrons using the type were established; four were permanently based in the UK and a fifth was initially based in Malta.\nR1.\nThree Nimrod aircraft were adapted for the signals intelligence role, replacing the Comet C2s and Canberras of No. 51 Squadron in May 1974. The R1 was visually distinguished from the MR2 by the lack of a MAD boom. It was fitted with an array of rotating dish aerials in the aircraft's bomb bay, with further dish aerials in the tailcone and at the front of the wing-mounted fuel tanks. It had a flight crew of four (two pilots, a flight engineer and one navigator) and up to 25 crew operating the SIGINT equipment.\nOnly since the end of the Cold War has the role of the aircraft been officially acknowledged; they were once described as \"radar calibration aircraft\". The R1s have not suffered the same rate of fatigue and corrosion as the MR2s. One R1 was lost in a flying accident since the type's introduction; this occurred in May 1995 during a flight test after major servicing, at RAF Kinloss. To replace this aircraft an MR2 was selected for conversion to R1 standard, and entered service in December 1996.\nThe Nimrod R1 was based initially at RAF Wyton, Cambridgeshire, and later at RAF Waddington, Lincolnshire, and flown by 51 Sqn. The two remaining Nimrod R1s were originally planned to be retired at the end of March 2011, but operational requirements forced the RAF to deploy one to RAF Akrotiri, Cyprus on 16 March in support of Operation Ellamy. The last flight of the type was on 28 June 2011 from RAF Waddington, in the presence of the Chief of the Air Staff, ACM Sir Stephen Dalton. XV 249, the former MR2, is now on display at the RAF Museum Cosford, West Midlands. The R1 was replaced by three Boeing RC-135W \"Rivet Joint\" aircraft, acquired under the Airseeker project; the first aircraft was delivered in late 2013.\nMR2.\nStarting in 1975, 35 aircraft were upgraded to MR2 standard, being re-delivered from August 1979. The upgrade included extensive modernisation of the aircraft's electronic suite. Changes included the replacement of the 1950s ASV Mk 21 radar used by the Shackleton and Nimrod MR1 with the new EMI Searchwater radar, a new acoustic processor (GEC-Marconi AQS-901) capable of handling more modern sonobuoys, a new mission data recorder (Hanbush) and a new Electronic Support Measures (Yellow Gate) which included new pods on the wingtips.\nProvision for in-flight refuelling was introduced during the Falklands War (as the \"MR2P\"), as well as hardpoints to allow the Nimrod to carry the AIM-9 Sidewinder missile to counter enemy Argentine Air Force maritime surveillance aircraft. In preparation for operations in the Gulf War theatre, several MR2s were fitted with new communications and ECM equipment to deal with anticipated threats; at the time these modified aircraft were given the designation \"MR2P(GM) (Gulf Mod)\".\nThe Nimrod MR2 carried out three main roles: Anti-Submarine Warfare (ASW), Anti-Surface Unit Warfare (ASUW) and Search and Rescue (SAR). Its extended range enabled the crew to monitor maritime areas far to the north of Iceland and up to out into the Western Atlantic. With Air-to-Air Refuelling (AAR), range and endurance was greatly extended. The crew consisted of two pilots and one flight engineer, two navigators (one tactical navigator and a routine navigator), one Air Electronics Officer (AEO), the sonobuoy sensor team of two Weapon System Operators (WSOp ACO) and four Weapon System Operators (WSOp EW) to manage passive and active electronic warfare systems.\nUntil 2010, the Nimrod MR2 was based at RAF Kinloss in Scotland (120, 201 and 206 Squadrons), and RAF St Mawgan in Cornwall (42 and 38(R) Squadrons). Following Options for Change, 42 Squadron was disbanded and its number reassigned to 38(R) Squadron. The Nimrod MR2 aircraft was withdrawn on 31 March 2010, a year earlier than planned, for financial reasons. The last official flight of a Nimrod MR2 took place on 26 May 2010, with XV229 flying from RAF Kinloss to Kent International Airport to be used as an evacuation training airframe at the nearby MOD Defence Fire Training and Development Centre.\nAEW3.\nIn the mid-1970s, a modified Nimrod was proposed for the Airborne Early Warning (AEW) mission \u2013 again as a replacement for the Lancaster-derived, piston-engined Shackleton AEW.2. Eleven existing Nimrod airframes were to be converted by British Aerospace to house the GEC Marconi radars in a bulbous nose and tail. The Nimrod AEW3 project was plagued by cost over-runs and problems with the GEC 4080M computer used. Eventually, the MoD recognised that the cost of developing the radar system to achieve the required level of performance was prohibitive and the probability of success very uncertain, and in December 1986 the project was cancelled. The RAF eventually received seven Boeing E-3 Sentry aircraft instead.\nMRA4.\nThe Nimrod MRA4 was intended to replace the capability provided by the MR2. It was essentially a new aircraft, with current-generation Rolls-Royce BR710 turbofan engines, a new larger wing, and fully refurbished fuselage. The project was subject to delays, cost over-runs, and contract re-negotiations. The type had been originally intended to enter service in 2003 but was cancelled in 2010 as a result of the Strategic Defence and Security Review, at which point it was \u00a3789\u00a0million over-budget; the development airframes were also scrapped. Some functions were allocated to other assets, with Hercules transport aircraft and E-3 Sentry Airborne Early Warning aircraft given some tasks, but the cancellation of the MRA4 resulted in a significant gap in long-range maritime patrol and search-and rescue capability.\nIn July 2016, the Ministry of Defence announced the purchase of nine Boeing P-8A Poseidon aircraft for the RAF. The RAF declared the P-8 had reached initial operating capability (IOC) on 1 April 2020, by which time two of the planes had been delivered. The nine aircraft will be based at RAF Lossiemouth.\nDesign.\nOverview.\nThe Nimrod was the first jet-powered MPA to enter service, being powered by the Rolls-Royce Spey turbofan engine. Aircraft in this role had been commonly propelled by piston or turboprop powerplants instead to maximise fuel economy and enable maximum patrol time on station. Advantages of the Nimrod's turbofan engines included greater speed and altitude capabilities, and it was more capable of evading detection by submarines, as propeller-driven aircraft are more detectable underwater by standard acoustic sensors. The Nimrods had a flight endurance of ten hours without aerial refuelling. The MR2s were later fitted to receive mid-air refuelling in response to demands of the Falklands War.\nAt the start of a patrol mission all four engines would normally be running; as the aircraft's weight was reduced by fuel consumption, up to two engines could be shut down, allowing the remaining engines to be operated more efficiently. Instead of relying on ram air to restart an inactive engine, compressor air could be crossfed from a live engine to a starter turbine. The crossfeed duct was later discovered to be a potential fire hazard. Similarly, the two hydraulic systems on board were designed to be powered by the two inner engines that would always be running. Electrical generation was designed to far exceed the consumption of existing equipment to accommodate additional systems installed over the Nimrod's service life.\nThe standard Nimrod fleet carried out three basic operational roles during their RAF service: Anti-Submarine Warfare duties typically involved surveillance over an allocated area of the North Atlantic to detect the presence of Soviet submarines in that area and to track their movements. In the event of war, reconnaissance information gathered during these patrols would be shared with other allied aircraft to enable coordinated strikes at both submarines and surface targets. Search and rescue (SAR) missions were another important duty of the RAF's Nimrod fleet, operating under the Air Rescue Coordination Centre at RAF Kinloss and were a common sight in both military and civil maritime incidents. Throughout the Nimrod's operational life, a minimum of one aircraft was held in a state of readiness to respond to SAR demands at all times.\nAvionics.\nThe Nimrod featured a crew of up to 25 personnel, although a typical crew numbered roughly 12, most of whom operated the various onboard sensor suites and specialist detection equipment. A significant proportion of the onboard sensor equipment was housed outside the pressure shell inside the Nimrod's distinctive pannier lower fuselage. Sensor systems included radar, ESM radar detection and sonar.The Nimrod and its detection capabilities were an important component of Britain's military defence during the height of the Cold War.\nThe Nimrod's navigational functions were computerised, and were managed from a central tactical compartment housed in the forward cabin. Various functions such as weapons control and information from sensors such as the large forward doppler radar were displayed and controlled at the tactical station. The flight systems and autopilot could be directly controlled by navigator's stations in the tactical compartment, giving the navigator nearly complete aircraft control. The navigational systems comprised digital, analogue, and electro-mechanical elements. The computers were integrated with most of the Nimrod's guidance systems such as the air data computer, astrocompass, inertial guidance and doppler radar. Navigation information could also be manually input by the operators.\nUpon entry into service, the Nimrod was hailed as possessing advanced electronic equipment such as onboard digital computers. The increased capability of these electronic systems allowed the RAF's fleet of 46 Nimrod aircraft to provide equal coverage to that of the larger fleet of retiring Avro Shackletons. The design philosophy of these computerised systems was that of a 'man-machine partnership'; while onboard computers performed much of the data sift and analysis processes, decisions and actions on the basis of that data remained in the operator's hands. To support the Nimrod's anticipated long lifespan, onboard computers were designed to be capable of integrating with various new components, systems, and sensors that could be added in future upgrades. After a mission, gathered information could be extracted for review purposes and for further analysis.\nArmaments and equipment.\nThe Nimrod featured a sizeable bomb bay in which, in addition to armaments such as torpedoes and missiles, could be housed a wide variety of specialist equipment for many purposes, such as up to 150 sonobuoys for ASW purposes or multiple air-deployed dinghies and droppable survival packs such as Lindholme Gear for SAR missions; additional fuel tanks and cargo could also be carried in the bomb bay during ferrying flights. Other armaments equippable in the bomb bay include mines, bombs, and nuclear depth charges; later munitions included the Sting Ray torpedo and Harpoon missile for increased capability.\nThe Nimrod could also be fitted with two detachable pylons mounted underneath the wings to be used with missiles such as the Martel although while planned Martel was never integrated with the Mk1.; two specialised pylons were later added to enable the equipping of AIM-9 Sidewinder missiles, used for self-defence purposes against hostile aircraft. A powerful remote-controlled searchlight was installed underneath the starboard wing for SAR operations. For reconnaissance missions, the aircraft was also equipped with a pair of downward-facing cameras suited to low and high-altitude photography. In later years a newer electro-optical camera system was installed for greater imaging quality.\nVarious new ECMs and electronic support systems were retrofitted onto the Nimrod fleet in response to new challenges and to increase the type's defensive capabilities; additional equipment also provided more effective means of identification and communication. A number of modifications were introduced during the 1991 Gulf War; a small number of MR2s were fitted with improved Link 11 datalinks, new defensive ECM equipment including the first operational use of a towed radar decoy, and a forward looking infrared turret under the starboard wing.\nOperational history.\nIntroduction to service.\nThe Nimrod first entered squadron service with the RAF at RAF St Mawgan in Cornwall in October 1969. These initial aircraft, designated as Nimrod MR1, were intended as a stop-gap measure, and thus were initially equipped with many of the same sensors and equipment as the Shackletons they were supplementing. While some improvements were implemented on the MR1 fleet to enhance their detection capabilities, the improved Nimrod MR2 variant entered service in August 1979 following a lengthy development process. The majority of the Nimrod fleet operated from RAF Kinloss in Scotland.\nOperationally, each active Nimrod would form a single piece of a complex submarine detection and monitoring mission. An emphasis on real-time intelligence sharing was paramount to these operations; upon detecting a submarine, Nimrod aircrews would inform Royal Navy frigates and other NATO-aligned vessels in pursuit in an effort to continuously monitor Soviet submarines. The safeguarding of the Royal Navy's \"Resolution\"-class ballistic missile submarines, which were the launch platform for Britain's nuclear deterrent, was viewed as being of the utmost priority.\nFalklands War.\nNimrods were first deployed to Wideawake airfield on Ascension Island on 5 April 1982, the type at first being used to fly local patrols around Ascension to guard against potential Argentine attacks, and to escort the British Task Force as it sailed south towards the Falklands, with Nimrods also being used to provide search and rescue as well as communications relay support of the Operation Black Buck bombing raids by Avro Vulcans. As the Task Force neared what would become the combat theatre and the threat from Argentine submarines rose, the more capable Nimrod MR2s took on operations initially performed by older Nimrod MR1s. Aviation author Chris Chant has claimed that the Nimrod R1 also conducted electronic intelligence missions operating from Punta Arenas in neutral Chile. The Chilean government allowed an RAF Nimrod R1 to fly signals reconnaissance sorties from the Desventuradas Islands, gathering information on Argentine Air Force movements.\nThe addition of air-to-air refuelling probes allowed operations to be carried out in the vicinity of the Falklands, while the aircraft's armament was supplemented by the addition of general-purpose bombs, BL755 cluster bombs and AIM-9 Sidewinder air-to-air missiles. The use of air-to-air refuelling allowed extremely long reconnaissance missions to be mounted, one example being a 19-hour 5-minute patrol conducted on 15 May 1982 (XV232 Airborne: 0803, Landing: 0308), which passed within of the Argentine coast to confirm that Argentine surface vessels were not at sea. Another long-range flight was carried out by an MR2 on the night of 20/21 May, covering a total of , the longest distance flight carried out during the Falklands War. In all, Nimrods flew 111 missions from Ascension in support of British operations during the Falklands War.\nGulf War.\nA detachment of three Nimrod MR2s was deployed to Seeb in Oman in August 1990 as a result of the Iraqi invasion of Kuwait, carrying out patrols over the Gulf of Oman and Persian Gulf. Due to the level of threats present in the Gulf theatre, operational Nimrods were quickly retrofitted with a Marconi towed active decoy. Once hostilities commenced, the Nimrod detachment, by now increased to five aircraft, concentrated on night patrols, with daylight patrols carried out by US Navy Lockheed P-3 Orions. Nimrods were used to guide Westland Lynx helicopters and Grumman A-6 Intruder attack aircraft against Iraqi patrol vessels, being credited with assisting in sinking or damaging 16 Iraqi vessels.\nAfter the ground offensive against Iraqi forces had ended, Britain elected to maintain an RAF presence in the region through assets such as the Nimrod and other aircraft. Nimrod R1s operated from August 1990 to March 1991 from Cyprus, providing almost continuous flying operations from the start of the ground offensive. Each R1 was retrofitted with the same Marconi towed active decoy as well as under wing chaff/flare dispensers, reportedly sourced from the Tornado fleet.\nAfghanistan and Iraq War.\nNimrods were again deployed to the Middle East as part of the British contribution to the US-led invasion of Afghanistan; missions in this theatre involved the Nimrods performing lengthy overland flights for intelligence-gathering purposes. On 2 September 2006, 14 UK military personnel were killed when a Nimrod MR2 was destroyed in a midair explosion following an onboard fire over Afghanistan. It was the single greatest loss of British military lives since the Falklands War. The outbreak of the Iraq War in March 2003 saw the RAF's Nimrods being used for operations over Iraq, using the aircraft's sensors to detect hostile forces and to direct attacks by friendly coalition forces.\nSearch and rescue.\nWhile the Nimrod MR1/MR2 was in service, one aircraft from each of the squadrons on rotation was available for search and rescue operations at one-hour standby. The standby aircraft carried two sets of Lindholme Gear in the weapons bay. Usually one other Nimrod airborne on a training mission would also carry a set of Lindholme Gear. As well as using the aircraft sensors to find aircraft or ships in distress, it was used to find survivors in the water, with a capability to search areas of up to . The main role would normally be to act as on-scene rescue coordinator to control ships, fixed-wing aircraft, and helicopters in the search area.\nThe Nimrod was most often featured in the media in relation to its search-and-rescue role, such as in the reporting of major rescue incidents. In August 1979, several Nimrods were involved in locating yachting competitors during the disaster-stricken 1979 Fastnet race and coordinated with helicopters in searches for survivors from lost vessels. In March 1980, the \"Alexander L. Kielland\", a Norwegian semi-submersible drilling rig, capsized whilst working in the Ekofisk oil field killing 123 people; six different Nimrods searched for survivors and took turns to provide rescue co-ordination, involving the control of 80 surface ships and 20 British and Norwegian helicopters. In an example of its search capabilities, in September 1977 when an attempted crossing of the North Atlantic in a Zodiac inflatable dinghy went wrong, a Nimrod found the collapsed dinghy and directed a ship to it.\nOperation Tapestry.\nThe Nimrods were often used to enforce Operation Tapestry. Tapestry is a codeword for the activities by ships and aircraft that protect the United Kingdom's Sovereign Sea Areas, including the protection of fishing rights and oil and gas extraction. Following the establishment of a Exclusive Economic Zone (EEZ) at the beginning of 1977 the Nimrod fleet was given the task of patrolling the area. The aircraft would locate, identify, and photograph vessels operating in the EEZ. The whole area was routinely patrolled. In addition to surveillance, the aircraft would communicate with all oil and gas platforms. During the Icelandic Cod Wars of 1972 and 1975\u20131976, the Nimrod fleet closely cooperated with Royal Navy surface vessels to protect British civilian fishing ships.\nAccidents and incidents.\nFive Nimrods were lost in accidents during the type's service with the RAF:\nSpecifications (MR.2).\n\"Data from\" WilsonGeneral characteristics* Crew: 13* Aspect ratio: \nPerformance* Maximum speed: Mach * Endurance: * g limits: * Roll rate: * Maximum glide ratio: \nArmament\n , with provisions to carry combinations of:** Missiles: \nSee also.\nRelated development\nAircraft of comparable role, configuration, and era\nReferences.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nCitations.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "40298", "revid": "50066621", "url": "https://en.wikipedia.org/wiki?curid=40298", "title": "Jean-Claude Killy", "text": "French alpine skier (born 1943)\nJean-Claude Killy (born 30 August 1943) is a French former World Cup alpine ski racer. He dominated the sport in the late 1960s, and was a triple Olympic champion, winning the three alpine events at the 1968 Winter Olympics, becoming the most successful athlete there. He also won the first two World Cup titles, in 1967 and 1968.\nEarly life.\nKilly was born in Saint-Cloud, a suburb of Paris, during the German occupation of World War II, but was brought up in Val-d'Is\u00e8re in the Alps, where his family had relocated in 1945 following the war. His father, Robert, was a former Spitfire pilot for the Free French, and opened a ski shop in the Savoie village, and would later operate a hotel. In 1950, his mother Madeline abandoned the family for another man, leaving Robert to raise Jean-Claude, age 7, his older sister (France), and their infant brother (Mic). Jean-Claude was sent to boarding school in Chamb\u00e9ry, down the valley, but he despised being shut up in a classroom.\nEarly career.\nKilly turned his attention to skiing rather than school. His father allowed him to drop out at age 15, and he made the French national junior team a year later. As a young racer, Killy was fast, but did not usually complete his races, and the early 1960s were not entirely successful for him.\nIn December 1961, at age 18, Killy won his first international race, a giant slalom. The event took place in his home village of Val-d'Is\u00e8re. Killy had started 39th, a position that should have been a severe disadvantage.\nThe French coach picked Killy for the giant slalom in the 1962 World Championships in Chamonix, France, away in the shadow of Mont Blanc. But Killy, unaware of his selection, was still attempting to qualify for the downhill event in northeastern Italy at Cortina d'Ampezzo. Only three weeks before the world championships, he skied in his typical reckless style. About from the finish, Killy hit a stretch of ice in a compression and went down, rose immediately, then crossed the finish on just one ski\u2014and the fastest time. Unfortunately, his other leg was broken, and he watched the 1962 World Championships on crutches.\nTwo years later, at age 20, Killy was entered in all three of the men's events at the 1964 Olympics, because his coach wanted to prepare him for 1968. Unfortunately, Killy was plagued by recurrences of amoebic dysentery and hepatitis, ailments that he had contracted in 1962 during a summer of compulsory service with the French Army in Algeria. His form was definitely off, and he fell a few yards after the start of the downhill, lost a binding in the slalom, and finished fifth in the giant slalom, in which he had been the heavy favorite. Yet a few weeks later, he dominated a giant slalom race at Garmisch-Partenkirchen, in Bavaria, counting for the prestigious Arlberg-Kandahar events, the oldest 'classic' in the sport. A year later, he also triumphed at another major competition, the slalom of the Hahnenkamm races at Kitzb\u00fchel that he clinched three times in a row until 1967.\nAlthough the first half of the decade was a relative disappointment, Killy began to strongly improve his results afterwards to become one of the best technical ski racers. In August 1966, the Frenchman, nicknamed 'Toutoune' by some of his colleagues and friends, scored his first win in a downhill race against an international field at the 1966 World Championships in Portillo, Chile, and also took gold in the combined. Killy was peaking as the first World Cup season was launched in January 1967, with the 1968 Winter Olympics in France only a year away.\nDominance \u2013 1967\u201368.\nWorld Cup results.\nRace victories.\nKilly was the first World Cup champion in 1967, winning 12 of 17 races to easily take the overall title. He also won the season standings in each of the three \"classic\" alpine disciplines; he won all five of the downhill races and four of the five giant slalom races.\nThe following year, Killy won the triple crown of alpine skiing with a sweep of all three Olympic gold medals (downhill, giant slalom, and slalom) in controversial circumstances at the 1968 Winter Olympics in Grenoble, France. By finishing first in all races, he also captured the FIS world championship title in the combined event.\nElectrical timing by Omega was accurate to one-hundredth of a second. Killy relied on his upper-body strength to hit the bar while already moving forward, giving himself a slight edge. This spectacular start appears to have helped him to beat his teammate Guy Perillat by a few hundredths in the Olympic downhill.\nWith the Olympic events included (for the only time) in the World Cup standings, Killy easily defended his title in 1968 as the overall champion, placing first in the giant slalom and second in the downhill and slalom season standings. He retired following the 1968 season, and moved to Geneva, Switzerland, in 1969.\nWorld Championship results.\n1962: injured\nPost-Olympic career.\nKilly's success in Grenoble could not have come at a more opportune time for him: the 1968 Winter Olympics were the first to be extensively televised, in color, by the American Broadcasting Company. His all-conquering success, combined with his Gallic flair and looks, made him an overnight celebrity in the United States, especially amongst young women. In May 1968, Killy signed with International Management Group, the sports management firm headed by Mark McCormack. After racing on Dynamic VR17 and Rossignol skis during the part of his career when he was dominant, Killy signed a deal with Head Ski in the fall 1968 to endorse a metal and fiberglass ski named for him, the \"Killy 800\". Head, which was acquired by AMF the following year, manufactured a line of Killy skis for at least two years.\nIn April 1969, he was awarded the Helms World Trophy.\nIn television advertisements, Killy promoted the American Express card. He also became a spokesman for Schwinn bicycles, United Airlines, and Chevrolet automobiles; the last, a role detailed by journalist Hunter S. Thompson in his 1970 article \"The Temptations of Jean-Claude Killy\" for \"Scanlan's Monthly\".\nKilly starred as a ski instructor in the 1972 crime movie \"Snow Job\", released in the UK as \"The Ski Raiders\", and US TV as \"The Great Ski Caper\". American children in the early 1970s knew Killy from a TV commercial where he introduces himself, his thick accent making his name into \"Chocolate Kitty.\" Killy played himself in the 1983 movie Copper Mountain: A Club Med Experience, starring Jim Carrey and Alan Thicke, set at Copper Mountain, Colorado. Killy also stars in the noteworthy TV movie https://, in which he performs some remarkable skiing tricks alongside the three-time ice skating World Champion Peggy Fleming.\nJean-Claude Killy also had a short career as a racing driver between 1967 and 1970, participating in several car races including at Monza. Killy entered the 24 Hours of Le Mans in 1969, partnered with Bob Wollek, another former skier turned racing driver. Killy and Wollek's car led its class for a while before pulling out of the race with only four hours to go. In team with fellow Frenchman Bernard Cahier, Killy was 7th overall in the 1967 Targa Florio in a Porsche 911 S and first in the GT classification.\nIn November 1972, Killy came out of ski racing retirement at age 29 to compete on the pro circuit in the U.S. for two seasons. After a spirited challenge from two-time defending champion Spider Sabich, Killy won the 1973 season title, taking $28,625 in race winnings and a $40,000 bonus for the championship.\nHe missed the next season, won by Hugo Nindl, due to a recurring stomach ailment, then returned in the fall of 1974. Injuries slowed him and he finished well out of the 1975 standings, won by Hank Kashiwa.\nIn addition to trying his skill as a car racer, Killy made two television series. One, \"The Killy Style\", was a thirteen-week series that showcased various ski resorts, and the other, \"The Killy Challenge\", featured him racing against celebrities, who were all given handicaps. He was also sponsored by a champagne company, Mo\u00ebt &amp; Chandon, which paid him to be seen with a bottle of their champagne on his table everywhere he went.\nIn 1974 Killy, as part of this sponsorship deal was paid to ski down the previously unskied eastern slope of Mt Ngauruhoe (Peter Jackson's \"Mt Doom\") in New Zealand. The average slope on this side of the active volcano is 35 degrees. Radar recorded his speed at over , and it took two takes, as cloud cover spoiled the first.\nIn 1975, Killy was hired to lead the new ski operations at Shawnee Mountain Ski Area, a resort in the foothills of the Pocono Mountains in northeastern Pennsylvania. In 1983, Bob Gillen wrote in \"Ski\" magazine about the growing reputation of Shawnee Mountain as a ski area. He stated, \"Some of the initial interest was stimulated by hiring Jean-Claude Killy to represent the facility, and for several seasons he spent a number of days there. The first time my wife ever skied with me, I saw Killy flash by at Shawnee\u2014he was fast and smooth and he stopped frequently to check the time on his Rolex.\"\nFrom 1977 to 1994, he was a member of the Executive board of the Alpine Skiing Committee of the FIS. Killy served as co-president of the 1992 Winter Olympics, held in Albertville, France, and as the President of the Soci\u00e9t\u00e9 du Tour de France cycling race between 1992 and 2001. From 1995 to 2014, he was a member of the International Olympic Committee and chaired the coordination committee for Turin 2006 and Sochi 2014. He has been an Honorary Member since then.\nKilly tried his hand at distance running and competed in the 1983 New York City Marathon, finishing in 3:58:33.\nThe ski area of Val-d'Is\u00e8re and Tignes in the French Alps was given the name l'Espace Killy, in his honor.\nKilly became Grand Officer of the L\u00e9gion d'honneur in 2000.\nIntrawest credits Killy with the design of a ski trail, \"Cupp Run\", at their Snowshoe resort in West Virginia.\nPersonal life.\nFrom 1973 to 1987, he was married to French actress Danielle Gaubert, until her death from cancer. Together they had a daughter, \u00c9milie; he also adopted her two children from her first marriage to Rhadam\u00e9s Trujillo, the son of Rafael Trujillo, the assassinated dictator of the Dominican Republic. Gaubert and Trujillo were divorced in 1968 and later that year she met Killy. He is known for being friends with Russian President, Vladimir Putin. In an interview for the 1972 documentary Elvis on Tour, Elvis Presley named Jean-Claude as his favorite skier.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nExternal links.\nrole=\"presentation\" class=\"wikitable succession-box noprint\" style=\"margin:0.5em auto; font-size:small;clear:both;\""}
{"id": "40300", "revid": "1158033949", "url": "https://en.wikipedia.org/wiki?curid=40300", "title": "Bow down before the porcelain god", "text": ""}
{"id": "40302", "revid": "20208822", "url": "https://en.wikipedia.org/wiki?curid=40302", "title": "William Hale Thompson", "text": "American politician (1869\u20131944)\nWilliam Hale Thompson (May 14, 1869 \u2013 March 19, 1944) was an American politician who served as mayor of Chicago from 1915 to 1923 and again from 1927 to 1931. Known as \"Big Bill\", he is the most recent Republican to have served as mayor of Chicago. Historians rank him among the most unethical mayors in American history, mainly for his open alliance with Al Capone. However, others recognize the effectiveness of his political methods and publicity-oriented campaigning, acknowledging him as a \"Political Chameleon\" and the leader of an effective political machine.\nThompson was known for his over-the-top campaigning and uncensored language that, along with his towering height and weight, earned him the nickname \"Big Bill\". Upon his reelection in 1927, Thompson had the school board suspend the Superintendent of Chicago Public Schools, William McAndrew. He was also at the forefront of the movement for the Chicago Public Library system and education officials to censor and ban many texts and historical recollections coming from the United Kingdom.\nThough Thompson was a popular figure during his career, his popularity collapsed after his death, when two safe-deposit boxes were found in his name containing over $1.8 million, which were taken as evidence of his corruption.\nEarly life.\nThompson was born in Boston, Massachusetts, to William Hale and Medora Gale Thompson, but his family moved to Chicago when he was nine days old. Despite having been born in Boston, Thompson had strong roots in Chicago. His father, Colonel William Hale Thompson Sr., was a popular businessman within Chicago and had served as colonel in the Second Illinois Guard who had come to Chicago after serving in the United States Navy during the American Civil War. His maternal grandfather, Stephen F. Gale, the first chief of the Chicago Fire Department, had played a large part in drawing up the city's corporation charter in 1837, earning him regard as a \"Chicago pioneer\" by some academic journalists.\nThompson attended Chicago Public Schools, and took supplementary courses at the Fessenden School and Metropolitan College.\nThompson was meant to attend Yale but instead moved to Wyoming at the age of 14, where he became a cowboy and cattle owner and traveled across Europe, taking up ranching in Texas and New Mexico later on in his life. The experiences influenced him to add Western touches into his campaign, including his sombrero, which became a symbol for his campaign. By the age of twenty-one, he had accumulated a stake of $30,000. He returned to Chicago in 1892 after his father's death to manage his estates. Shortly after returning to Chicago, Thompson joined the Illinois Athletic Club and the Sportsmen's Club of America and quickly was appointed director-general and captain of the Chicago Athletic Association's water polo and football teams. His six-foot frame and athletic prowess earned him the nickname \"Big Bill,\" which stuck with him throughout his career as a politician.\nEarly political career.\nThompson served on the Real Estate Board of Chicago.\nChicago City Council.\nIn 1900, Thompson narrowly won election as an alderman on the Chicago City Council from the 2nd ward, his home district. \nIn 1901, Thompson declared himself a candidate for the Republican mayoral nomination, though he ultimately did not contest at the convention.\nCook County Board of Commissioners.\nIn 1902, Thompson was elected to the Cook County Board of Commissioners, serving from 1902 through 1904. During this period, Thompson formed a political alliance with Frederick Lundin, a Republican city clerk who worked under William Lorimer, a U.S. Representative from Illinois who was known for corrupt election methods. The political duo, according to most citizens, worked very well together earning them the title the \"Gallagher and Shean of Chicago Politics\". Thompson with his outgoing and charismatic personality paired with his towering stature and gentlemanly appearance gave him an undeniable public presence, which was completed by Lundin's cunning political ideas and projects.\nFirst mayoralty (1915\u20131923).\nFirst term.\nIn 1915, Thompson was elected as the 41st Mayor of Chicago, beating County Clerk Robert M. Sweitzer, John H. Hill, Seymour Steadman, and Charles Thompson.\nThompson was sworn in on April 26, 1915. In his inaugural address, Thompson spoke of his ambitions for Chicago to become \"the greatest in the world\", but also that his acts as mayor should not be swayed by corruption. He also emphasized the importance of public safety (as enforced by the Chicago Police Department), the improvement of public transit, secure and permanently lowered gas prices, Chicago being allowed to have Home rule and more efforts being placed into Chicago's commercial interests in order to create jobs and improve the city's economy. His efforts to expand and publicly improve the streets of Chicago earned him another nickname of \"Big Bill the Builder\". In his time as mayor, he oversaw the completion of the Michigan Avenue link bridge, the Twelfth Street widening, and the extension and widening of Ogden Avenue. Along with his big dreams for Chicago's geographical expansion, he wished for Chicago to expand politically and economically. He believed that Chicago should be able to enforce laws on their own terms, particularly without what he claimed to be the interference of British government or totalitarian rule. He ended his inaugural address by declaring, &lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;I am a firm believer in the separation of the three co-ordinate branches of government \u2013 Executive, Legislative and Judicial \u2013 peculiar to our American system, and that one should not intrude upon or violate, the prerogatives of the other. I do not intend to exceed the rights and privileges of the executive nor transgress upon the legislative or judicial functions. I shall impartially execute the laws made by the proper legislative authorities and interpreted by the judiciary.\nAs Thompson entered the first term of his mayorship, he appointed Fred Lundin as chairman on the committee of patronage. Early in his mayoral career, Thompson began to amass a war chest to support an eventual run for the presidency, by charging city drivers and inspectors $3 per month.\nEarly in his mayoralty Thompson had to cut short a July 1915 trip to San Francisco in order to deal with the aftermath of the \"Eastland\" disaster. While Thompson was out of town, acting-mayor Moorhouse had turned the Chicago City Hall into a makeshift hospital for first aid and a morgue for bodies recovered from the tragedy. Once Thompson returned to Chicago he organized and heavily promoted a relief fund and ordered an investigation into the casual negligence responsible for the tragedy.\nIn 1915, a delegation of civic-oriented women, headed by Mary McDowell, urged Thompson to appoint an well-qualified woman to the city's new office of \"commissioner of public welfare\". Thompson did appoint a woman. However, instead of a woman qualified by a public welfare background, he appointed Louise Osborn Rowe, a Republican Party worker and loyalist. Within a year of her appointment, Rowe was charged with operating a kickback scheme in the department, and was forced to resign in 1916. This post would remain vacant until the mayoralty of Thompson's successor.\nThompson gained national attention and condemnation for his neutral attitude toward the events of World War I. By declining the visit of the French Mission to Chicago and refusing to control or act against anti-war or anti-conscription meetings, Thompson is \"credited with characterizing Chicago as the sixth German city of the world,\" also earning the nickname \"Kaiser\" Bill Thompson. Thompson sought to further endear himself to the city's German and Irish populations by positioning himself as anti-British. Thompson opposed sending troops into war after the United States' April 1917 declaration of war. These facts later went on to hurt his chances in his 1918 U.S. Senate campaign.\nIn 1916, he became a member of the Republican National Committee. He would continue to serve on the committee until 1920.\nSecond term.\nThompson was reelected mayor in 1919, beating out Robert Sweitzer once again along with Adolph S. Carm, John Collins, John Fitzpatrick, and Maclay Hoyne. Thompson was said to have had control of the 75,000 black voters in his day. In his campaign he claimed to be an advocate for the people against public utility companies and the rich who avoided taxes. This inspired Thompson to enforce a five-cent streetcar fare to promote his campaign, which was also used to threaten the action of streetcar companies; he also sued the Chicago \"L\" when it tried to raise fares after the inflation caused by World War I. Eventually, however, despite his protests, the fare was raised to seven cents.\nIn his second inaugural address on April 28, 1919, Thompson looked towards drastically expanding Chicago, saying that \"Chicago is greater than some nations\". This expansion included the extension and widening of streets to cross over more of the city, new post offices, freight terminals, playgrounds, bridges, and more. Also, due to the rapidly changing city, Thompson proposed a zoning bill to regulate and create commercial, industrial, and residential areas. Among the other issues he claimed he would address were telephone prices and service quality, the expansion of the Chicago Police Department, jobs for returning soldiers, lowering the cost of living, and restoring the jobs of Public School representatives who were removed by the Supreme Court.\nIn 1922, the city council voted to not spend any money to enforce the Volstead Act. Early into his second term, the city dealt with the Chicago race riot of 1919.\nAt the 1920 Republican National Convention Thompson helped to block his one-time ally Frank Lowden from capturing the nomination.\nThompson declined to run for reelection in 1923 and he was succeeded by William Emmett Dever. Thompson left office as Mayor on April 16, 1923.\nChairman of the Illinois Waterways Commission.\nWhile out of office, Thompson was appointed chairman of the Illinois Waterways Commission. He used his position to remain relevant in the media, involving himself in civic suits and campaigning for the Lakes-to-Gulf waterway project: to build a waterway from the Great Lakes to the Gulf of Mexico. Promoting both the project and himself, Thompson set off on a \"scientific\" expedition (to be extensively covered by the media), which he set off to the South Seas in order to find a tree-climbing fish on July 5, 1924. Attracting more attention, Thompson placed a $25,000 bet on his success, but no one participated.\nSecond mayoralty (1927\u20131931).\nThompson ran again in 1927 during a citywide gang war, aiming to unseat his successor, William Dever. Always a flamboyant campaigner, Thompson held a primary-election debate between himself and two live rats which he used to portray his opponents. Pledging to clean up Chicago and remove the crooks, Thompson instead turned his attention to the reformers, whom he considered the real criminals. According to Thompson, the biggest enemy the United States had was King George V of the United Kingdom. Thompson promised his supporters, many of whom were Irish, that if they ever met, Thompson would punch the king in the nose, or at other times, that he would arrest His Majesty. Upon his victory over Dever, Thompson's floating speakeasy, outwardly known as the Fish Fans Club, docked at Belmont Harbor. It was flooded with his supporters, so many so that the boat itself sank beneath the weight.\nIn his inaugural address on April 18, 1927, Thompson addressed the importance of remedying crime in Chicago, saying,\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;Our new Superintendent of Police has my positive instructions to drive the crooks and thieves and lawbreakers out of Chicago in ninety days, so that the people, their homes and their property may again be secure.\nThompson expressed his desire to remove Superintendent William McAndrew from the public schooling system, and restore what he called the \"true history of George Washington\" while exposing \"the treason and propaganda which insidiously have been injected into our schools and other educational institutions\". He also went on to enforce other issues he had addressed in previous speeches, like the issue of public transit, playgrounds, and the general upkeep and expansion of Chicago in an effort to aid property owners and increase residential income and revenue for the city as a whole. In August 1927, the Chicago Board of Education, now under Thompson's influence after he appointed a number of new members, voted to charge McAndrew with insubordination and lack of patriotism, suspending him pending an administrative hearing held by the board. The administrative hearing would last months, and the Chicago Board of Education would find McAndrew guilty. The Cook County Superior Court would later void this decision.\nAl Capone's support was pivotal to Thompson's return to the mayor's office. Capone raised over $200,000 for Thompsons's 1927 campaign.\nDuring Thompson's second term, the \"Pineapple Primary\" took place on April 10, 1928, so-called because of the hand grenades thrown at polling places to disrupt voting. The Pineapple Primary saw candidates backed by Thompson face Charles Deneen in the Republican primary election. Another infamous instance of gang activity that took place during Thompson's third term was St. Valentine's Day Massacre.\nThompson blamed Ruth Hanna McCormick's lack of support for his loss at the 1928 Republican National Convention, and he returned the favor during her 1930 campaign for the United States Senate by endorsing against her in the general election. Thompson had had a longstanding rivalry with the McCormicks. He intensely disliked Robert R. McCormick who published the \"Chicago Tribune\". U.S. Senator Medill McCormick was the publisher's brother, and Ruth Hanna McCormick was Medill McCormick's wife.\nAmid growing discontent with Thompson's leadership, particularly in the area of cleaning up Chicago's reputation as the capital of organized crime, he was defeated in 1931 by Democrat Anton Cermak. Cermak was an immigrant from Bohemia, and Thompson used this fact to belittle him with ethnic slurs such as:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;\nCermak replied, \"He doesn't like my name...It's true I didn't come over on the \"Mayflower\", but I came over as soon as I could,\" which was a sentiment to which ethnic Chicagoans (especially its large Bohemian population) could relate, so Thompson's slurs largely backfired. Al Capone reportedly donated $260,000 to Thompson's failed 1931 election.\nAfter Thompson's defeat, the \"Chicago Tribune\" wrote, &lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;\nThompson left office April 9, 1931.\nSubsequent career.\nIn 1936, Thompson ran for the office of Illinois governor on the \"Union Progressive\" ballot line against Democratic incumbent Henry Horner and Republican nominee C. Wayland Brooks. He received only three percent of the vote. In 1939, Thompson ran in the Republican primary for mayor of Chicago and was soundly defeated by a 77% to 23% margin against future Governor Dwight Green.\nPersonal life.\nIn 1901, Thompson married Mary \"Maysie\" Walker Wyse, a secretary in his father's office. The two never had children. In 1930 his wife was robbed at gunpoint by George \"Baby Face\" Nelson, taking jewellery valued at $18,000. She described her attacker, saying \"He had a baby face. He was good looking, hardly more than a boy, had dark hair and was wearing a gray topcoat and a brown felt hat, turned down brim.\" This description earned Nelson the baby face moniker.\nDeath.\nWilliam Hale Thompson died on March 19, 1944, at the Blackstone Hotel at the age of 74. He was buried in Oak Woods Cemetery in a solid bronze casket.\nDespite the fact that many had liked Thompson and enjoyed his various political antics, few people attended his funeral, and one reporter noted that there was not \"a flower nor a fern to be seen\".\nUpon Thompson's death, two safe deposit boxes in his name were discovered to contain nearly $1.84 million ($ million today) in cash. Once the money was uncovered, the Internal Revenue Service took its share in taxes, and Maysie Thompson lived off of the rest until her death in 1958.\nHistorical assessments.\nHistorians rank him among the most unethical mayors in American history, mainly for his open alliance with Al Capone. However, others recognize the effectiveness of his political methods and publicity-oriented campaigning, acknowledging him as a \"Political Chameleon\" and an effective political machine. \"Time\" magazine said in 1931, \"chief credit for creating 20th Century Politics Chicago Style\" should go to William Thompson.\nA 1985 survey of historians, political scientists and urban experts conducted by Melvin G. Holli of the University of Illinois at Chicago ranked Thompson as the second-worst big-city American mayor to have been in office since 1820. A 1993 iteration of the same survey ranked Thompson as the absolute-worst."}
{"id": "40303", "revid": "50710675", "url": "https://en.wikipedia.org/wiki?curid=40303", "title": "Anton Cermak", "text": "American politician (1873\u20131933)\nAnton Joseph Cermak (May 9, 1873\u00a0\u2013 March 6, 1933) was an American politician who served as the 44th Mayor of Chicago from April 7, 1931, until his death in 1933. He was killed by Giuseppe Zangara, whose likely target was President-elect Franklin D. Roosevelt, but Cermak was shot instead after a bystander hit the perpetrator with a purse.\nA 1994 survey of experts on Chicago politics assessed Cermak as one of the ten best mayors in the city's history (up to that time).\nEarly life.\nAnton Joseph Cermak was born to a mining family in Kladno, Austria-Hungary (now in the Czech Republic), the son of Anton\u00edn \u010cerm\u00e1k and Kate\u0159ina n\u00e9e Frank(ov\u00e1).\nHe immigrated with his parents to the United States in 1874, and grew up in the town of Braidwood, Illinois, where he was educated before beginning to work full time while still a teenager. He followed his father into coal mining, and labored at mines in Will and Grundy counties. After moving to Chicago at age 16, Cermak worked as a tow boy for the horse-drawn streetcar line, and then tended horses in the stables of Chicago's Pilsen neighborhood. During the early years of his working life, Cermak supplemented his education with evening high school and business college classes.\nEarly career.\nAfter saving enough money to buy his own horse and cart, he went into business selling firewood, and he subsequently expanded his venture into a haulage business. As he became more politically active, Cermak served in municipal government jobs, including as a clerk in the city police court, and as a bailiff for the Municipal Court of Chicago. As his political fortunes began to rise, Cermak was able to avail himself of other business opportunities, including interests in real estate, insurance, and banking.\nCermak began his political career as a Democratic Party precinct captain, and in 1902, he was elected to the Illinois House of Representatives. Seven years later, in 1909 he was elected to the Chicago City Council as an alderman of the 12th Ward. He was re-elected in 1911. In December 1912, he resigned from the city council in order to accept a position as bailiff of the Chicago Municipal Court. In 1918, Cermak unsuccessfully ran as the Democratic nominee for Cook County sheriff. He narrowly lost to Republican nominee Charles W. Peters.\nCermak was elected in 1919 to again represent the 12th ward on the city council, and was re-elected in 1921. He was elected president of the Cook County Board of Commissioners in 1922, and resigned from the city council in order to assume office as county president on December 4. He was elected the chairman of the Cook County Democratic Party in 1928. Also in 1928, he was the Democratic nominee for a seat in the United States Senate, but was defeated by Republican Otis F. Glenn, 54.46% to 44.94%.\nMayor of Chicago (1931\u20131933).\nCermak won election as mayor of Chicago in 1931. His mayoral victory came in the wake of the Great Depression, the deep resentment many Chicagoans had of Prohibition, and the increasing violence resulting from organized crime's control of Chicago\u2014typified by the St. Valentine's Day Massacre.\nThe many ethnic groups, such as Czechs, Poles, Ukrainians, Jews, Italians, and African Americans, who began to settle in Chicago in the early 1900s were mostly detached from the political system, due in part to a lack of organization, which led to underrepresentation in the City Council. As an immigrant himself, Cermak recognized Chicago's relatively new immigrants as a significant population of disenfranchised voters, which had the potential to be a large power base for Cermak and his local Democratic organization.\nBefore Cermak, the Democratic party in Cook County was run by Irish Americans. The Irish first became successful in politics since they spoke English, and because, coming from an island on the edge of Europe, they had few ancestral enemies. As the old saying went: \"A Lithuanian won\u2019t vote for a Pole, and a Pole won\u2019t vote for a Lithuanian. A German won\u2019t vote for either of them. But all three will vote for a turkey\u2014an Irishman.\" As Cermak climbed the local political ladder, the resentment of the party leadership grew. When the bosses rejected his bid to become the mayoral candidate, Cermak swore revenge. He formed his political army from the non-Irish elements.\nCermak's political and organizational skills helped create one of the most influential political organizations of his day. With support from Franklin D. Roosevelt on the national level, Cermak gradually wooed members of Chicago's growing black community into the Democratic fold. Walter Wright, the superintendent of parks and aviation for the city of Chicago, aided Cermak in stepping into office.\nWhen Cermak challenged the incumbent, William Hale \"Big Bill\" Thompson, in the 1931 mayor's race, Thompson, who represented Chicago's existing Irish-dominated power structure, responded with an ethnic slur\u2013filled ditty that ridiculed his teamster past (pushing a pushcart):\n\"I won\u2019t take a back seat to that Bohunk, Chairmock, Chermack, or whatever his name is.\"\n\"Tony, Tony, where\u2019s your pushcart at?\"\n\"Can you picture a World\u2019s Fair mayor with a name like that?\"\nCermak replied, \"He doesn\u2019t like my name\u2026 it\u2019s true I didn\u2019t come over on the \"Mayflower\", but I came over as soon as I could.\" It was a sentiment to which ethnic Chicagoans could relate, and Thompson's prejudicial insults largely backfired.\nThompson's reputation as a buffoon, many voters\u2019 disgust with the corruption of his political machine, and his inability or unwillingness to clean up organized crime in Chicago were cited as major factors in Cermak capturing 58% of the vote in the mayoral election on April 6, 1931. Cermak's victory finished Thompson as a political power, and largely ended the Republican Party's influence in Chicago; indeed, all the mayors of Chicago since 1931 have been members of the Democratic Party. For nearly his entire administration, Cermak had to deal with a major tax revolt. From 1931 to 1933, the Association of Real Estate Taxpayers mounted a \"tax strike.\"\nAt its height, the association, which was headed by John M. Pratt and James E. Bistor, had over 30,000 members. Much to Cermak's dismay, it successfully slowed down the collection of real estate taxes through litigation and the promotion of the refusal to pay. In the meantime, the city found it difficult to pay teachers and maintain services. Cermak had to meet President-elect Roosevelt to \"mend fences,\" and to request money to fund essential city services.\nA 1993 survey of historians, political scientists and urban experts conducted by Melvin G. Holli of the University of Illinois at Chicago ranked Cermak as the twenty-fifth-best American big-city mayor to have served between the years 1820 and 1993. A 1994 survey of experts on Chicago politics saw Cermak ranked as one of the ten best mayors in the city's history (up to that time).\nDeath.\nAssassination.\nOn February 15, 1933, while shaking hands with President-elect Franklin D. Roosevelt at Bayfront Park in Miami, Florida, Cermak was shot in the lung and mortally wounded by Giuseppe Zangara, who was attempting to assassinate Roosevelt. At the critical moment, Lillian Cross, a woman standing near Zangara, hit Zangara's arm with her purse, and spoiled his aim. In addition to Cermak, Zangara hit four other people: Margaret Kruis, 21, of Newark, NJ, shot through the hand; Russell Caldwell, 22, of Miami, hit squarely in the forehead by a spent bullet, which embedded itself under the skin; Mabel Gill of Miami, shot in the abdomen; and William Sinnott, a New York police detective, who received a glancing blow to the forehead and scalp. All four of those injuries were minor.\nOnce at the hospital, Cermak reportedly uttered the line that was engraved on his tomb, saying to Roosevelt, \"I'm glad it was me, not you.\" The \"Chicago Tribune\" reported the quote without attributing it to a witness, and most scholars doubt that it was ever said.\nZangara told the police that he hated rich and powerful people, but not Roosevelt personally. Later, rumors circulated that Cermak, not Roosevelt, had been the intended target, as his promise to clean up Chicago's rampant lawlessness posed a threat to Al Capone and the Chicago organized crime syndicate. One of the first people to suggest the organized crime theory was reporter Walter Winchell, who happened to be in Miami the evening of the shooting. According to Roosevelt biographer Jean Edward Smith, there is no proof for this theory.\nLong-time Chicago newsman Len O\u2019Connor offers a different view of the events surrounding the mayor's assassination. He has written that aldermen Paddy Bauler and Charlie Weber informed him that relations between Cermak and Roosevelt were strained, because Cermak fought Roosevelt's nomination at the Democratic convention in Chicago.\nAuthor Ronald Humble provides yet another perspective as to why Cermak was killed. In his book \"Frank Nitti: The True Story of Chicago\u2019s Notorious Enforcer\", Humble contends that Cermak was as corrupt as Thompson, and that the Chicago Outfit hired Zangara to kill Cermak in retaliation for Cermak's attempt to murder Frank Nitti.\nCermak died at Jackson Memorial Hospital in Miami on March 6, partly due to his wounds. On March 30, however, his personal physician, Dr. Karl A. Meyer, revealed that the primary cause of Cermak's death was ulcerative colitis, commenting, \"The mayor would have recovered from the bullet wound had it not been for the complication of colitis. The autopsy disclosed the wound had healed ... the other complications were not directly due to the bullet wound.\" Doubts were raised at the time and later concerning whether the bullet wound directly contributed to his death. A theory raised decades later contended that the bullet had actually caused damage to his colon which led to perforation which was undiagnosed by his doctors. It alleged that \"but for [the] physicians' blunders\" Cermak might have survived. This theory was refuted by a later medical analysis of the event.\nZangara was convicted of murder after Cermak's death under the law of transferred intent, and was executed in Florida's electric chair on March 20, 1933.\nFuneral and burial.\nCermak first lay in state in Miami. His remains were then transported to Chicago in a solid bronze casket placed in the baggage car of a funeral train operated by the Illinois Central. The casket was decorated with bunting and draped with an American flag. The train was transported to its destination by honorary escort.\nFamily members and associates of Cermak rode on the train. Sixteen members of the Chicago City Council served as pallbearers for the heavy casket, carrying it off the train, through a cordon of military members and American Legionnaires, and into a automobile hearse. The hearse was then followed by two fire trucks, carrying flowers that had been transported to Chicago aboard the funeral car. At least 50,000 people gathered on the streets near the Central Station in Chicago to greet the return of his remains to the city at 10 am local time on March 8. The United Press reported that along Michigan Avenue, crowds were \"aligned 10 deep...and overflowed into Grant Park.\nOn March 10, Cermak's non-sectarian funeral service was held at the Chicago Stadium arena. Crowds were described as exceeding the venue's 25,000 spectator seating capacity. The services were short, and featured eulogies by Governor Henry Horner, Reverend John Thompson (First United Methodist Church of Chicago), Father Daniel Frawley (St. Jerome Croatian Catholic Church), and Rabbi Louis Mann (Chicago Sinai Congregation). The service was followed by a procession between the arena and Chicago's Bohemian National Cemetery along a route in length, with approximately 50,000 marchers forming a marching line that measured . The procession was spectated by hundreds of thousands. Cermak was interred in a family mausoleum at Bohemian National Cemetery.\nAftermath.\nThe mayor's death was followed by a struggle for succession both to his party chairmanship, and for the mayor's office.\nA plaque honoring Cermak still lies at the site of the assassination in Miami's Bayfront Park. It is inscribed with Cermak's alleged words to Roosevelt after he was shot, \"I'm glad it was me instead of you.\" Following Cermak's death, 22nd Street\u2014a major east\u2013west artery that traversed Chicago's West Side, and the close-in suburbs of Cicero and Berwyn, areas with significant Czech populations\u2014was renamed Cermak Road. In 1943, a Liberty ship, the SS \"A. J. Cermak\" was named in Cermak's honor. It was scrapped in 1964.\nDescendants.\nCermak's son-in-law, Otto Kerner Jr., served as the 33rd Governor of Illinois, and as a federal circuit judge.\nHis grandson, Frank J. Jirka, Jr., who was with him in Miami when he was assassinated, later became an Underwater Demolition Team officer in the United States Navy. Jirka was awarded a Silver Star and Purple Heart for his actions during the Battle of Iwo Jima; the wounds he suffered led to the amputation of both legs below the knee. After World War II, he became a physician, and in 1983, was elected president of the American Medical Association. Cermak's great niece, Kajon Cermak, is a radio broadcaster. His daughter, Lillian, was married to Richey V. Graham, who served in the Illinois General Assembly.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nCitations.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "40304", "revid": "4652171", "url": "https://en.wikipedia.org/wiki?curid=40304", "title": "Jane Byrne", "text": "American politician (1933\u20132014)\nJane Margaret Byrne (n\u00e9e Burke; May 24, 1933\u00a0\u2013 November 14, 2014) was an American politician who served as the 50th mayor of Chicago from April 16, 1979, until April 29, 1983. Prior to her tenure as mayor, Byrne served as Chicago's commissioner of consumer sales from 1969 until 1977 under Mayor Richard J. Daley, the only woman in the mayoral cabinet. \nByrne won the 1979 Chicago mayoral election on April 3, 1979 becoming the first female mayor of the city, and causing an upheaval in beating the city's political machine. She was the first woman to be elected mayor of a major city in the United States, as Chicago was the second largest city in the United States at the time.\nByrne narrowly lost her bid for renomination in the Democratic primary for the 1983 Chicago mayoral election, in which she faced a long-expected challenge from Richard J. Daley's son Richard M. Daley, with both Byrne and Daley losing to Harold Washington in an upset. Washington won the general election. Byrne unsuccessfully challenged Washington for the nomination in 1987, but endorsed his re-election in the general election. Byrne made one last unsuccessful bid for the Democratic mayoral nomination in 1991, challenging Richard M. Daley (who had become mayor in a special election two years earlier).\nEarly life and career.\nByrne was born Jane Margaret Burke on May 24, 1933, at John B. Murphy Hospital in the Lake View neighborhood on the north side of Chicago, Illinois, to Katherine Marie Burke (n\u00e9e Nolan), a housewife, and William Patrick Burke, vice president of Inland Steel. Raised on the city's north side, Byrne graduated from Saint Scholastica High School and attended Saint Mary-of-the-Woods College for her first year of college. Byrne later transferred to Barat College, where she graduated with a bachelor's degree in chemistry and biology in 1955. \nByrne entered politics to volunteer in John F. Kennedy's campaign for president in 1960. During that campaign she first met then Chicago Mayor Richard J. Daley. After Daley met Byrne, he appointed her to several positions, beginning in 1964 with a job in a city anti-poverty program In June 1965, she was promoted to working with the Chicago Committee of Urban Opportunity.\nIn 1968, Byrne was appointed head of the City of Chicago's consumer affairs department. She served as a delegate to the 1972 Democratic National Convention (DNC) and chairperson of the DNC resolutions committee in 1973. In 1975, Byrne was appointed co-chairperson of the Cook County Democratic Central Committee by Daley, over the objection of a majority of Democratic leaders. The committee ousted Byrne shortly after Daley's death in late 1976. Shortly thereafter, Byrne accused the newly appointed mayor Michael Bilandic of being unfair to citizens of the city by approving an increase in regulated taxi fares, which Byrne charged was the result of a \"backroom deal\". Byrne was then dismissed from her post as head of consumer affairs by Bilandic.\nMayor of Chicago (1979\u20131983).\n1979 election.\nMonths after being fired as head of the consumer affairs department, Byrne challenged Bilandic in the 1979 Democratic mayoral primary, the real contest in the heavily Democratic Chicago. Officially announcing her mayoral campaign in August 1977, Byrne partnered with Chicago journalist and political consultant Don Rose, who served as her campaign manager. At first, political observers believed she had little chance of winning. A memorandum inside the Bilandic campaign said it should portray her as \"a shrill, charging, vindictive person\u2014and nothing makes a woman look worse\". \nNevertheless, the January Chicago Blizzard of 1979 paralyzed the city and caused Bilandic to be seen as an ineffective leader. Bilandic's ineffective leadership caused Jesse Jackson to endorse Byrne. Even many Republican voters voted in the Democratic primary to help beat Bilandic. Infuriated voters on the North Side and Northwest Side retaliated against Bilandic for the Democratic Party's slating of only South Side candidates for the mayor, clerk, and treasurer (the outgoing city clerk, John C. Marcin, was from the Northwest Side). These four factors combined to give Byrne a 51% to 49% victory over Bilandic in the primary. Byrne outperformed Bilandic in more than half of wards. She performed particularly strongly in Black-majority wards, which she swept.\nPositioning herself as a reformer, Byrne won the general election with 82.1% of the vote, which remains the largest vote share any candidate has won in a Chicago mayoral election.\nWhile Byrne had run against the Democratic political machine's candidate (Bilandic) in the primary, and her win over him harmed the machine's reputation for electoral strength, she had not run on any promise of eliminating the machine once mayor. \"The New York Times\" noted shortly after her primary election victory of Bilandic, &lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;\nTenure.\nLeadership and general politics.\nByrne made inclusive moves as mayor such as shepherding the hiring of the city's first African-American and female school superintendent Ruth B. Love, and she was the first mayor to recognize the gay community. Byrne helped to make Chicago more welcoming to the gay community. She ended the police department's practice of raiding gay bars, and declared the city's first official \"Gay Pride Parade Day\" in 1981. However, during her tenure, Byrne drifted away from many of the progressive tenets she had campaigned on. Byrne began to collaborate with aldermen Edward M. Burke and Edward Vrdolyak, whom, during her 1979 campaign, she had denounced as an \"evil cabal\".\nIn 1982, she supported the Cook County Democratic Party's replacement of its chairman, County Board President George Dunne, with her city council ally, Alderman Edward Vrdolyak.\nByrne and the Cook County Democratic Party endorsed Senator Ted Kennedy for president in the 1980 Democratic presidential primaries, but incumbent President Jimmy Carter won the Illinois Democratic Primary and even carried Cook County and the city of Chicago. Byrne's endorsement of Kennedy was later considered detrimental because of her controversial tenure, and Kennedy's loss in the city was a key moment in the 1980 Democratic Party presidential primaries because of Chicago's role in delivering his brother John F. Kennedy the 1960 Democratic presidential nomination. When Byrne and Kennedy walked in the annual Saint Patrick's Day parade they were sometimes booed by hecklers.\nSimultaneously, Byrne and the Cook County Democratic Party's candidate in the 1980 election for Cook County State's Attorney (chief local prosecutor), 14th Ward Alderman Edward M. Burke, lost in the Democratic primary to Richard M. Daley, and Daley then unseated GOP incumbent Bernard Carey in the general election.\nThe \"Chicago Sun Times\" reported that Byrne's enemies publicly mocked her as \"that crazy broad\" and \"that skinny bitch\" and worse.\nAppointments and personnel.\nIn her first year in office, significant instances of turnover in prominent city positions led critics to accuse Byrne of running a \"revolving door administration\".\nWhile Byrne initially made inclusive moves with regards to appointments as mayor: shepherding the hiring of the city's first African-American and female school superintendent Ruth B. Love which she later pivoted away from this. Among the later steps that Byrne took that upset many of the progressives and Blacks that had supported her in the 1979 mayoral campaign was replacing Black members of the Chicago Board of Education and Chicago Housing Authority board with White members, some of whom even held stances that critics viewed as racist.\nDuring the 1979 mayoral election, Byrne pledged to fire Superintendent of the Chicago Police Department James E. O'Grady, accusing him of having \"politicized\" the department. Days after her inauguration, O'Grady resigned. Later that year, she relieved interim superintendent Joseph DiLeonardi of command. She appointed Samuel Nolan interim superintendent in his place, Nolan was the first African American to serve as head of the Chicago Police Department. In January 1980, Richard J. Brzeczek took office as permanent superintendent, having been appointed by Byrne. On her last day in office, after the resignation of Brzeczek as superintdendent, Byrne appointed James E. O'Grady as interim superintendent. By this time, Byrne had rescinded her past criticisms of O'Grady. In 1980, Byrne appointed William R. Blair as Chicago fire commissioner.\nArts.\nDuring her campaign for mayor, Byrne promised to provide strong support to the performing arts. \"Chicago Tribune\" art critic, Richard Christiansen, hailed Byrne for having made, \"the arts and amusements of the city a most significant part of her\" mayoral administration.\nAs mayor, she provided $200,000 to the Lyric Opera of Chicago for the express purposes of providing family-friendly entertainment. She provided a similar amount to Auditorium Theatre for them to acquire a new lighting board. As mayor, Byrne provided for the city to partially fund the construction of the \"Mir\u00f3's Chicago\" sculpture by artist Joan Mir\u00f3. Byrne also allowed Chicago to be used as a filming location, pushing for such movies as \"The Blues Brothers\" to be shot in Chicago.\nCabrini\u2013Green.\nOn March 26, 1981, Byrne decided to move into the crime-ridden Cabrini\u2013Green Homes housing project on the near-north side of Chicago after 37 shootings resulting in 11 murders occurring during a three-month period from January to March 1981. In her 2004 memoir, Byrne reflected on her decision to move into Cabrini\u2013Green: \"How could I put Cabrini on a bigger map? ... Suddenly I knew\u2014I could move in there.\" Prior to her move to Cabrini, Byrne closed down several liquor stores in the area, citing the stores as hangout for gangs and murderers. Byrne also ordered the Chicago Housing Authority to evict tenants who were suspected of harboring gang members in their apartments, which affected approximately 800 tenants. \nByrne moved into a 4th floor apartment in a Cabrini extension building on North Sedgwick Avenue with her husband on March 31 around 8:30\u00a0p.m. after attending a dinner at the Conrad Hilton hotel. Hours after Byrne moved into the housing project, police raided the building and arrested eleven street gang members who they had learned through informants were planning to have a shootout in the mayor's building later that evening. \nByrne described her first night at Cabrini-Green as \"lovely\" and \"very quiet\". She stayed at Cabrini-Green for three weeks to bring attention to the housing project's crime and infrastructure problems. Her stay there ended on April 18, 1981, following an Easter celebration at the project which drew protests and demonstrators who claimed Byrne's move to the project was just a publicity stunt.\nFinances.\nOne of the crises that Byrne faced in her first year as mayor was a major shortage of funds in both the municipal government and by the Chicago Board of Education (the city's school board). This arose due to questionable past borrowing practices, and necessitated both budget cuts and further borrowing to resolve.\nHandgun ordinance.\nIn January 1982, Byrne proposed a controversial ordinance effectively banning new handgun registration. The ordinance was created to put a freeze on the number of legally owned handguns in Chicago and to require owners of handguns to re-register them annually. The ordinance was approved by a 6\u20131 vote in February 1982. The ordinance was struck down by the Supreme Court in the 2010 case \"McDonald v. City of Chicago\".\nHosting of special events.\nByrne also used special events, such as ChicagoFest, to revitalize Navy Pier and the downtown Chicago Theatre. ChicagoFest had first been held the year prior to her election. One of Byrne's first efforts as mayor had been an attempt to cancel future editions of the event. But, after facing complaints from citizens and unions, Byrne allowed the festival to continue as an annual event, and formally renamed it \"Mayor Jane M. Byrne's ChicagoFest\". \nFestivals inaugurated during her tenure included Taste of Chicago. Byrne held a number of smaller-scale events in neighborhoods all across the city, wording them with the prefix \"Mayor Byrne's\". As mayor, Byrne was a strong supporter of the planned Chicago 1992 World's Fair. In 1980, Byrne announced that the city would host a Championship Auto Racing Teams \"Indy Car\" automobile race at Grant Park on the 4th of July weekend of the following year. However, after facing criticism, Byrne quickly canceled these plans.\nLabor.\nIn her first year in office, she faced strikes by labor unions as the city's transit workers, public school teachers, and firefighters all went on strike.\nTransportation.\nThere had been plans under Daley and Bilandic to demolish the Loop elevated rail and replace it with a subway. Byrne appointed a commission that ultimately recommended that the Loop should be retained along with modernization. In 1981, Byrne disbanded the Chicago Transit Authority's dedicated security force, transferring its duties instead to the Chicago Police Department.\nOther matters.\nIn November 1981, the Chicago City Council approved a new redistricting map for the city's aldermanic wards which was drawn by Byrne's administration. The U.S. Court of Appeals would find, in 1984, that the map was in violation of the federal Voting Rights Act of 1965.\nOn November 11, 1981, Dan Goodwin, who had successfully climbed the Sears Tower the previous spring, battled for his life on the side of the John Hancock Center. William Blair, Chicago's fire commissioner, had ordered the Chicago Fire Department to stop Goodwin by directing a full-power fire hose at him and by using fire axes to break window glass in Goodwin's path. \nMayor Byrne rushed to the scene and ordered the fire department to stand down. Then, through a smashed out a 38th floor window, Byrne told Goodwin, who was hanging from the building's side a floor below, that though she did not agree with his climbing of the John Hancock Center, she certainly opposed the fire department knocking him to the ground below. Byrne then allowed Goodwin to continue his climb unimpeded to the top.\nByrne also initiated the idea for creating a unified lakefront museum campus, which was implemented subsequent to her tenure as Museum Campus, as well as the idea of renovating Navy Pier, also implemented subsequent to her tenure. Byrne additionally expanded O'Hare International Airport.\nBid for reelection.\nIn August 1982, Byrne decided that she would seek a second term as mayor. At the beginning of her re-election campaign, she was trailing behind Richard M. Daley, then Cook County State's Attorney, by 3% in a poll done by the \"Chicago Tribune\" in July 1982. Compared to the 1979 mayoral election in which Byrne received 59.3% of the African-American vote, Byrne had lost half of that vote. Byrne was defeated in the 1983 Democratic primary for mayor by Harold Washington, also an anti-machine politician and African-American congressman; the younger Daley ran a close third. Washington won the Democratic primary with just 36% of the vote; Byrne had 33%. Washington went on to win the general election.\nAssessments.\nA 1993 survey of historians, political scientists and urban experts conducted by Melvin G. Holli of the University of Illinois at Chicago saw Byrne ranked as the tenth-worst American big-city mayor to serve between the years 1820 and 1993. When the survey was limited only to mayors that were in office post-1960, the results saw Byrne ranked the fourth-worst.\nLater career.\nByrne ran against Washington again in the 1987 Democratic primary, but was defeated. She endorsed Washington for the general election, in which he defeated two Democrats running under other parties' banners (Edward Vrdolyak and Thomas Hynes) and a Republican.\nEarly into her 1987 campaign, in October 1985, Byrne called for a feasibility study of the potential to construct a third major airport for the city on the site of the South Works. Soon after, Governor James R. Thompson endorsed the idea of immediately planning for a third major airport to serve Chicago. This would be one of the impetuses of decades-long discussions and studies for a third major airport for the city, including the proposed Chicago south suburban airport.\nByrne next ran in the 1988 Democratic primary for Cook County Circuit Court Clerk. She faced the Democratic Party's slated candidate, Aurelia Pucinski (who was endorsed by Mayor Washington and is the daughter of then-Alderman Roman Pucinski). Pucinski defeated Byrne in the primary and Vrdolyak, by then a Republican, in the general election. Byrne's fourth run for mayor became a rematch with Daley in the 1991 primary. She received only 5.9 percent of the vote, a distant third behind Daley and Alderman Danny K. Davis.\nPersonal life.\nIn 1956, she married William P. Byrne, a U.S. Marine pilot. The couple had a daughter, Katherine C. Byrne (1957-2024). On May 31, 1959, while flying from Marine Corps Air Station Cherry Point to Naval Air Station Glenview in a Skyraider, Lt. Byrne attempted to land in a dense fog. After being waved off for landing twice, his plane's wing struck the porch of a nearby house and the plane crashed into Sunset Memorial Park, killing him. \nByrne married journalist Jay McMullen in 1978, and they remained married until his death from lung cancer in 1992. Byrne lived in the same apartment building from the 1970s until her death in 2014. She has one grandchild, Willie. Her daughter, Kathy, who died in 2024, was a lawyer with a Chicago firm. Mayor Byrne's book, \"My Chicago\" (1992) covers her life through her political career. In 2011, Byrne attended the inauguration of the city's then new mayor, Rahm Emanuel.\nDeath and legacy.\nIn 2006, a near decade before she died, Sufjan Stevens, a midwest native, released an instrumental song titled \"Inaugural Pop Music for Jane Margaret Byrne\". Stevens the year prior released an album titled and about Illinois to great acclaim. The song came out on the subsequent outtakes album, \"The Avalanche\".\nByrne had entered hospice care and died on November 14, 2014, in Chicago, aged 81, from complications of a stroke she suffered in January 2013. She was survived by her daughter Katherine and her grandson Willie. Her funeral Mass was held at St. Vincent de Paul Church on Monday, November 17, 2014. She is buried at Calvary Catholic Cemetery in Evanston, Illinois.\nIn July 2014, the Chicago City Council voted to rename the plaza surrounding the historic Chicago Water Tower on North Michigan Avenue the Jane M. Byrne Plaza in her honor. In a dedication ceremony held on August 29, 2014, Illinois Governor Pat Quinn renamed the Circle Interchange in Chicago the \"Jane Byrne Interchange\".\nByrne was also known for coining the malapropism \"fruitworthy\"."}
{"id": "40305", "revid": "49784051", "url": "https://en.wikipedia.org/wiki?curid=40305", "title": "Harold Washington", "text": "Illinois politician and 51st mayor of Chicago\nHarold Lee Washington (April 15, 1922 \u2013 November 25, 1987) was an American lawyer and politician who was the 51st mayor of Chicago. In April 1983, Washington became the first African American to be elected as the city's mayor at the age of 60. He served as mayor from April 29, 1983, until his untimely death in 1987. Born in Chicago and raised in the Bronzeville neighborhood, Washington became involved in local 3rd Ward politics under Chicago Alderman and future Congressman Ralph Metcalfe after graduating from Roosevelt University and Northwestern University School of Law. Washington was a member of the U.S. House of Representatives from 1981 to 1983, representing Illinois's first district. Washington had previously served in the Illinois House of Representatives from 1965 to 1976 and the Illinois State Senate from 1977 to 1980. A 1994 survey of experts on Chicago politics assessed Washington the third-best mayor in the city's history (up to that time).\nBiography.\nAncestry.\nThe earliest known ancestor of Harold Lee Washington, Isam/Isham Washington, was born a slave in 1832 in North Carolina. In 1864, he enlisted in the 8th United States Colored Heavy Artillery, Company L, in Paducah, Kentucky. Following his discharge in 1866, he began farming with his wife, Rebecca Neal, in Ballard County, Kentucky. Among their six children was Isam/Isom McDaniel (Mack) Washington, who was born in 1875. In 1896, Mack Washington married Arbella Weeks of Massac County, who had been born in Mississippi in 1878. In 1897, their first son, Roy L. Washington, father of Harold Washington, was born in Ballard County, Kentucky. In 1903, shortly after both families moved to Massac County, Illinois, the elder Washington died. After farming for a time, Mack Washington became a minister in the African Methodist Episcopal (A.M.E.) Church, serving numerous churches in Illinois until the death of his wife in 1952. Reverend I.M.D. Washington died in 1953.\nEarly life and education.\nHarold Lee Washington was born on April 15, 1922, at Cook County Hospital in Chicago, to Roy and Bertha Washington. While still in high school in Lawrenceville, Illinois, Roy met Bertha from nearby Carrier Mills and the two married in 1916 in Harrisburg, Illinois. At a time when most African Americans and Chicago residents supported the Republican Party, Roy Washington was one of the first black precinct captains for the Chicago Democratic Party; he also worked as a lawyer and Methodist minister.\nBertha left the family, possibly to seek her fortune as a singer, when Harold was four. Harold Washington grew up in Bronzeville, a Chicago neighborhood that was the center of black culture for the entire Midwest in the early and middle 20th century. Edward and Harold stayed with their father, while Roy Jr. and Geneva were cared for by their grandparents. After attending St. Benedict the Moor Boarding School in Milwaukee from 1928 to 1932, Washington attended DuSable High School, a new racially segregated public school, from 1936 to 1939. In a citywide track meet in 1939, Washington claimed first place in the 110-meter high hurdles and second place in the 220-meter low hurdles, playing a key role in the school's victory in the Chicago Public League Championship that year.\nWashington left high school during his senior year and joined the Civilian Conservation Corps in 1939. He later worked at a meatpacking plant before his father helped him get a job at the U.S. Treasury branch in the city. There he met Nancy Dorothy Finch, whom he married soon after; Washington was 19 and Dorothy was 17.\nMilitary service.\nIn 1942, Washington was drafted into the United States Army for the war effort and, after basic training, sent overseas as part of the 1887th Engineer Aviation Battalion, a racially segregated unit of the Air Force Engineers. Washington was part of a unit that built a bomber landing strip on Angaur island in Palau just 20 days, an action that earned the unit a Meritorious Service Unit Award. Eventually, Washington rose to the rank of First Sergeant in the Army Air Forces; he was honorably discharged in 1946 and was awarded his high school diploma from DuSable.\nRoosevelt College.\nIn the summer of 1946, Washington, aged 24 and a war veteran, enrolled at Roosevelt College (now Roosevelt University). Washington joined other groups of students not permitted to enroll in other local colleges. Local estimates place the student population of Roosevelt College at about 1/8\u00a0black and 1/2\u00a0Jewish. A full 75% of the students had enrolled because of the \"nondiscriminatory progressive principles\". Washington chaired a student fundraising drive, and then was named to a committee that supported citywide efforts to outlaw \"restrictive covenants\" in housing, the legal means by which members of ethnic minority groups (especially blacks and, to a lesser extent, Jews) were prohibited from purchasing real estate in predominantly white neighborhoods of the city.\nIn 1946, Washington was elected the third president of Roosevelt's student council; he was also the first black student to win that office. Washington saw Henry A. Wallace as an influence. Under his leadership, the student council successfully petitioned the college to have student representation on Roosevelt's faculty committees. At the first regional meeting of the newly founded National Student Association in the spring of 1948, Washington and nine other delegates proposed student representation on college faculties, and a \"Bill of Rights\" for students; both measures were roundly defeated. The next year, Washington went to the state capital at Springfield to protest Illinois legislators' coming probe of \"subversives\". The probe would outlaw the Communist Party and require \"loyalty oaths\" for teachers. Washington led students' opposition to the bills, which would pass later in 1949.\nDuring his college years, Washington came to be known for his stability. His friends said that he had a \"remarkable ability to keep cool\", reason carefully and walk a middle line. Washington intentionally avoided activities considered extremist, including street actions and sit-ins against racially segregated restaurants and businesses. Overall, Washington and other radical activists ended up sharing a mutual respect for each other, acknowledging both Washington's pragmatism and the activists' idealism. With the opportunities found only at Roosevelt College in the late 1940s, Washington's time at the Roosevelt College proved to be pivotal. Washington graduated in August 1949, with a Bachelor of Arts degree. In addition to his activities at Roosevelt, he was a member of Phi Beta Sigma fraternity.\nNorthwestern University School of Law.\nIn 1949, Washington began studies at the Northwestern University School of Law. where he was the only black student in his class (there were six women in the class, one of them being Dawn Clark Netsch). In 1951, his last year, he was elected treasurer of the Junior Bar Association (JBA). The election was largely symbolic, however, and Washington's attempts to give the JBA more authority at Northwestern were largely unsuccessful. On campus, Washington joined the Nu Beta Epsilon fraternity, largely because he and the other people who were members of ethnic minority groups which constituted the fraternity were blatantly excluded from the other fraternities on campus. Overall, Washington stayed away from the activism that defined his years at Roosevelt. During the evenings and weekends, he worked to supplement his GI Bill income. He received his JD in 1952.\nWhile attending law school, Washington divorced from his wife Dorothy in 1950; they had no children and had often lived separately during their marriage.\nEarly political career.\nAfter completing law school, Washington went into private practice with his father in 1952. His law office was near those of Ralph Metcalfe, former Olympic track athlete turned Chicago city alderman for the Third Ward. Following his father's death in 1953, Washington succeeded his father as Third Ward precinct captain in 1954 and became an assistant prosecutor with the Chicago corporation counsel office.\nRichard J. Daley was elected party chairman in 1952. Daley replaced C.C. Wimbush, an ally of William Dawson, on the party committee with Metcalfe. Under Metcalfe, the 3rd Ward was a critical factor in Daley's 1955 mayoral election victory and ranked first in the city in the size of its Democratic plurality in 1961. While working under Metcalfe, Washington began to organize the 3rd Ward's Young Democrats (YD) organization. At YD conventions, the 3rd Ward would push for numerous resolutions in the interest of blacks. Eventually, other black YD organizations would come to the 3rd Ward headquarters for advice on how to run their own organizations. Like he had at Roosevelt College, Washington avoided radicalism and preferred to work through the party to engender change.\nWhile working with the Young Democrats, Washington met Mary Ella Smith. They dated for the next 20 years, and in 1983 Washington proposed to Smith. In an interview with the Chicago Sun-Times, Smith said that she never pressed Washington for marriage because she knew Washington's first love was politics, saying, \"He was a political animal. He thrived on it, and I knew any thoughts of marriage would have to wait. I wasn't concerned about that. I just knew the day would come.\"\nIn 1959 Al Janney, Gus Savage, Lemuel Bentley, Bennett Johnson, Luster Jackson and others founded the Chicago League of Negro Voters, one of the first African-American political organizations in the city. In its first election, Bentley drew 60,000 votes for city clerk. The endorsement of the League was the deciding factor in the re-election of Leon Despres, who was an independent voice on the City Council. Washington was a close friend of the founders of the League and worked with them from time to time. In 1963 the group re-organized as Protest at the Polls. In 1967, Protest at the Polls played a key role in electing Anna Langford, William Cousins and A. A. \"Sammy\" Rayner, who were not part of the Daley machine, to seats on the City Council. In 1983, Protest at the Polls was instrumental in Washington's run for mayor. By then, the YDs were losing to independent candidates.\nLegislative career.\nIllinois House (1965\u20131976).\nAfter the state legislature failed to reapportion districts every ten years as required by the census, the 1964 election was held at-large to elect all 177 members of the Illinois House of Representatives. With the Republicans and Democrats each only running 118 candidates, independent voting groups attempted to slate candidates. The League of Negro Voters created a \"Third Slate\" of 59 candidates, announcing the creation of the slate on June 27, 1964. Shortly afterwards, Daley created a slate which included Adlai Stevenson III and Washington. The Third Slate was then thrown out by the Illinois Election Board because of \"insufficient signatures\" on the nominating petitions. In the election, Washington was elected as part of the winning Democratic slate of candidates. Washington's years in the Illinois House were marked by tension with Democratic Party leadership. In 1967, he was ranked by the Independent Voters of Illinois (IVI) as the fourth-most independent legislator in the Illinois House and named Best Legislator of the Year. His defiance of the \"idiot card\", a sheet of paper that directed legislators' votes on every issue, attracted the attention of party leaders, who moved to remove Washington from his legislative position. Daley often told Metcalfe to dump Washington as a candidate, but Metcalfe did not want to risk losing the 3rd Ward's Young Democrats, who were mostly aligned with Washington.\nWashington backed Renault Robinson, a black police officer and one of the founders of the Afro-American Patrolmen's League (AAPL). The aim of the AAPL was to fight against the racism which was directed against minority officers by the rest of the predominantly white department. Soon after the creation of the group, Robinson was written up for minor infractions, suspended, reinstated, and then placed on the graveyard shift on a single block behind central police headquarters. Robinson approached Washington and asked him to fashion a bill which would authorize the creation of a civilian review board, consisting of both patrolmen and officers, to monitor police brutality. Both black independent and white liberal legislators refused to back the bill, afraid to challenge Daley's grip on the police force.\nAfter Washington announced that he would support the AAPL, Metcalfe refused to protect him from Daley. Washington believed that he had the support of Ralph Tyler Smith, Speaker of the House. Instead, Smith criticized Washington and then allayed Daley's anger. In exchange for the party's backing, Washington would serve on the Chicago Crime Commission, the group Daley tasked with investigating the AAPL's charges. The commission promptly found the AAPL's charges \"unwarranted\". An angry and humiliated Washington admitted that on the commission, he felt like Daley's \"showcase ni***r\". In 1969, Daley removed Washington's name from the slate; only by the intervention of Cecil Partee, a party loyalist, was Washington reinstated. The Democratic Party supported Jim Taylor, a former professional boxer, Streets and Sanitation worker, over Washington. With Partee and his own ward's support, Washington defeated Taylor. His years in the House of Representatives were focused on becoming an advocate for black rights. He continued work on the Fair Housing Act, and worked to strengthen the state's Fair Employment Practices Commission (FEPC). In addition, he worked on a state Civil Rights Act, which would strengthen employment and housing provisions in the federal Civil Rights Act of 1964. In his first session, all of his bills were sent to committee or tabled. Like his time in Roosevelt College, Washington relied on parliamentary tactics (e.g., writing amendments guaranteed to fail in a vote) to enable him to bargain for more concessions.\nWashington was accused of failing to file a tax return, even though the tax was paid. He was found guilty and sentenced to 36 days in jail. (1971)\nWashington also passed bills in honor of civil rights figures. He passed a resolution in honor of Metcalfe, his mentor. He also passed a resolution in honor of James J. Reeb, a Unitarian minister who was beaten to death by a segregationist mob in Selma, Alabama. After the 1968 assassination of Martin Luther King Jr., he introduced a series of bills which were aimed at making King's birthday a state holiday. The first was tabled and later vetoed. The third bill he introduced, which was passed and signed by Gov. Richard Ogilvie, made Dr. King's birthday a commemorative day observed by Illinois public schools. It was not until 1973 that Washington was able, with Partee's help in the Senate, to have the bill enacted and signed by the governor.\n1975 speakership campaign.\nWashington ran a largely symbolic campaign for Speaker. He only received votes from himself and from Lewis A. H. Caldwell. However, with a divided Democratic caucus, this was enough to help deny Daley-backed Clyde Choate the nomination, helping to throw it to William A. Redmond after 92 rounds of voting.\nRedmond had Washington appointed as chairman of the Judiciary Committee.\nLegal issues.\nIn addition to Daley's strong-arm tactics, Washington's time in the Illinois House was also marred by problems with tax returns and allegations of not performing services owed to his clients. In her biography, Levinsohn questions whether the timing of Washington's legal troubles was politically motivated. In November 1966, Washington was re-elected to the House over Daley's strong objections; the first complaint was filed in 1964; the second was filed by January 1967. A letter asking Washington to explain the matter was sent on January 5, 1967. After failing to respond to numerous summons and subpoenas, the commission recommend a five-year suspension on March 18, 1968. A formal response to the charges did not occur until July 10, 1969. In his reply, Washington said that \"sometimes personal problems are enlarged out of proportion to the entire life picture at the time and the more important things are abandoned.\" In 1970, the Board of Managers of the Chicago Bar Association ruled that Washington's license be suspended for only one year, not the five recommended; the total amount in question between all six clients was $205.\nIn 1971, Washington was charged with failure to file tax returns for four years, although the Internal Revenue Service (IRS) claimed to have evidence for nineteen years. Judge Sam Perry noted that he was \"disturbed that this case ever made it to my courtroom\"\u2014while Washington had paid his taxes, he ended up owing the government a total of $508 as a result of not filing his returns. Typically, the IRS handled such cases in civil court, or within its bureaucracy. Washington pleaded \"no contest\" and was sentenced to forty days in Cook County Jail, a $1,000 fine, and three years of probation.\nIllinois Senate (1976\u20131980).\nCampaign for a seat on the Illinois Senate.\nIn 1975, Partee, now President of the Senate and eligible for his pension, decided to retire from the Senate. Although Daley and Taylor declined at first, at Partee's insistence, Washington was ultimately slated for the seat and he received the party's support. Daley had been displeased with Washington for having run a symbolic challenge in 1975 to Daley-backed Clyde Choate for Speaker of the Illinois House (Washington had only received two votes). Additionally, he had ultimately helped push the vote towards Redmond as a compromise candidate. The United Automobile Workers union, whose backing Washington obtained, were critical in persuading Daley to relent to back his candidacy.\nWashington defeated Anna Langford by nearly 2,000 votes in the Democratic primary. He went on to win the general election.\nHuman Rights Act of 1980.\nIn the Illinois Senate, Washington's main focus worked to pass 1980's Illinois Human Rights Act. Legislators rewrote all of the human rights laws in the state, restricting discrimination based on \"race, color, religion, sex, national origin, ancestry, age, marital status, physical or mental disability, military status, sexual orientation, or unfavorable discharge from military service in connection with employment, real estate transactions, access to financial credit, and the availability of public accommodations.\" The bill's origins began in 1970 with the rewriting of the Illinois Constitution. The new constitution required all governmental agencies and departments to be reorganized for efficiency. Republican governor James R. Thompson reorganized low-profile departments before his re-election in 1978. In 1979, during the early stages of his second term and immediately in the aftermath of the largest vote for a gubernatorial candidate in the state's history, Thompson called for human rights reorganization. The bill would consolidate and remove some agencies, eliminating a number of political jobs. Some Democratic legislators would oppose any measure backed by Washington, Thompson and Republican legislators.\nFor many years, human rights had been a campaign issue brought up and backed by Democrats. Thompson's staffers brought the bill to Washington and other black legislators before it was presented to the legislature. Washington made adjustments in anticipation of some legislators' concerns regarding the bill, before speaking for it in April 1979. On May 24, 1979, the bill passed the Senate by a vote of 59 to 1, with two voting present and six absent. The victory in the Senate was attributed by a Thompson staffer to Washington's \"calm noncombative presentation\". However, the bill stalled in the House. State Representative Susan Catania insisted on attaching an amendment to allow women guarantees in the use of credit cards. This effort was assisted by Carol Moseley Braun, a representative from Hyde Park who would later go on to serve as a U.S. Senator. State Representatives Jim Taylor and Larry Bullock introduced over one hundred amendments, including the text of the first ten amendments to the U.S. Constitution, to try to stall the bill. With Catania's amendment, the bill passed the House, but the Senate refused to accept the amendment. On June 30, 1979, the legislature adjourned.\nUnited States Representative (1981\u20131983).\nIn 1980, Washington was elected to the U.S. House of Representatives in Illinois's 1st congressional district. He defeated incumbent Representative Bennett Stewart in the Democratic primary. Anticipating that the Democratic Party would challenge him in his bid for re-nomination in 1982, Washington spent much of his first term campaigning for re-election, often travelling back to Chicago to campaign. Washington missed many House votes, an issue that would come up in his campaign for mayor in 1983. Washington's major congressional accomplishment involved legislation to extend the Voting Rights Act, legislation that opponents had argued was only necessary in an emergency. Others, including Congressman Henry Hyde, had submitted amendments designed to seriously weaken the power of the Voting Rights Act.\nAlthough he had been called \"crazy\" for railing in the House of Representatives against deep cuts to social programs, Associated Press political reporter Mike Robinson noted that Washington worked \"quietly and thoughtfully\" as the time came to pass the act. During hearings in the South regarding the Voting Rights Act, Washington asked questions that shed light on tactics used to prevent African Americans from voting (among them, closing registration early, literacy tests, and gerrymandering). After the amendments were submitted on the floor, Washington spoke from prepared speeches that avoided rhetoric and addressed the issues. As a result, the amendments were defeated, and Congress passed the Voting Rights Act Extension. By the time Washington faced re-election in 1982, he had cemented his popularity in the 1st Congressional District. Jane Byrne could not find one serious candidate to run against Washington for his re-election campaign. He had collected 250,000 signatures to get on the ballot, although only 610 signatures (0.5% of the voters in the previous election) were required. With his re-election to Congress locked up, Washington turned his attention to the next Chicago mayoral election.\nMayor of Chicago (1983\u20131987).\n1983 Chicago mayoral election.\nIn the February 22, 1983, Democratic mayoral primary, more than 100,000 new voters registered to vote led by a coalition that included the Latino reformed gang Young Lords led by Jose Cha Cha Jimenez. On the North and Northwest Sides, the incumbent mayor Jane Byrne led and future mayor Richard M. Daley, son of the late Mayor Richard J. Daley, finished a close second. Harold Washington had massive majorities on the South and West Sides. Southwest Side voters overwhelmingly supported Daley. Washington won with 37% of the vote, versus 33% for Byrne and 30% for Daley. Although winning the Democratic primary was normally considered tantamount to election in heavily Democratic Chicago, after his primary victory Washington found that his Republican opponent, former state legislator Bernard Epton (earlier considered a nominal stand-in), was supported by many high-ranking Democrats and their ward organizations, including the chairman of the Cook County Democratic Party, Alderman Edward Vrdolyak.\nEpton's campaign referred to, among other things, Washington's conviction for failure to file income tax returns (he had paid the taxes, but had not filed a return). Washington, on the other hand, stressed reforming the Chicago patronage system and the need for a jobs program in a tight economy. In the April 12, 1983, mayoral general election, Washington defeated Epton by 3.7%, 51.7% to 48.0%, to become mayor of Chicago. Washington was sworn in as mayor on April 29, 1983, and resigned his Congressional seat the following day.\nFirst term and Council Wars.\nDuring his tenure as mayor, Washington lived at the Hampton House apartments in the Hyde Park neighborhood of Chicago. He created the city's first environmental-affairs department under the management of longtime Great Lakes environmentalist Lee Botts. Washington's first term in office was characterized by conflict with the city council dubbed \"Council Wars\", referring to the then-recent \"Star Wars\" films and caused Chicago to be nicknamed \"Beirut on the Lake\". A 29-alderman City Council majority refused to enact Washington's legislation and prevented him from appointing nominees to boards and commissions. First-term challenges included city population loss and a massive decrease in ridership on the Chicago Transit Authority (CTA). Assertions that the overall crime rate increased were incorrect.\nThe 29, also known as the \"Vrdolyak 29\", were led by Vrdolyak (who was an Alderman in addition to Cook County Democratic Party chairman) and Finance Chair, Alderman Edward Burke. Parks superintendent Edmund Kelly also opposed the mayor. The three were known as \"the Eddies\" and were supported by the younger Daley (now State's Attorney), U.S. Congressmen Dan Rostenkowski and William Lipinski, and much of the Democratic Party. During his first city council meeting, Washington and the 21 supportive aldermen walked out of the meeting after a quorum had been established. Vrdolyak and the other 28 then chose committee chairmen and assigned aldermen to the various committees. Later lawsuits submitted by Washington and others were dismissed by Supreme Court Justice James C. Murray because it was determined that the appointments were legally made. Washington ruled by veto. The 29 lacked the 30th vote they needed to override Washington's veto; female and African American aldermen supported Washington despite pressure from the Eddies. Meanwhile, in the courts, Washington kept the pressure on to reverse the redistricting of city council wards that the city council had created during the Byrne years. During special elections in 1986, victorious Washington-backed candidates in the first round ensured at least 24 supporters in the city council. Six weeks later, when Marlene Carter and Lu\u00eds Guti\u00e9rrez won run-off elections, Washington had the 25 aldermen he needed. His vote as president of the City Council enabled him to break 25\u201325 tie-votes and enact his programs.\n1987 election.\nWashington defeated former mayor Jane Byrne in the February 24, 1987, Democratic mayoral primary by 7.2%, 53.5% to 46.3%, and in the April 7, 1987, mayoral general election defeated Vrdolyak (Illinois Solidarity Party) by 11.8%, 53.8% to 42.8%, with Northwestern University business professor Donald Haider (Republican) getting 4.3%, to win reelection to a second term as mayor. Cook County Assessor Thomas Hynes (Chicago First Party), a Daley ally, dropped out of the race 36 hours before the mayoral general election. During Washington's short second term, the Eddies lost much of their power: Vrdolyak became a Republican, Kelly was removed from his powerful parks post, and Burke lost his Finance Committee chairmanship.\nPolitical Education Project (PEP).\nFrom March 1984 to 1987, the Political Education Project (PEP) served as Washington's political arm, organizing both Washington's campaigns and the campaigns of his political allies. Harold Washington established the Political Education Project in 1984. This organization supported Washington's interests in electoral politics beyond the Office of the Mayor. PEP helped organize political candidates for statewide elections in 1984 and managed Washington's participation in the 1984 Democratic National Convention as a \"favorite son\" presidential candidate. PEP used its political connections to support candidates such as Luis Guti\u00e9rrez and Jes\u00fas \"Chuy\" Garc\u00eda through field operations, voter registration and Election Day poll monitoring. Once elected, these aldermen helped break the stalemate between Washington and his opponents in the city council. Due to PEP's efforts, Washington's City Council legislation gained ground and his popularity grew as the 1987 mayoral election approached. In preparation for the 1987 mayoral election, PEP formed the Committee to Re-Elect Mayor Washington. This organization carried out fundraising for the campaign, conducted campaign events, and coordinated volunteers. PEP staff members, such as Joseph Gardner and Helen Shiller, went on to play leading roles in Chicago politics.\nThe organization disbanded upon Harold Washington's death. Harold Washington's Political Education Project Records is an archival collection detailing the organization's work. It is located in the Chicago Public Library Special Collections, Harold Washington Library Center, Chicago, Illinois.\nDuSable Park.\nWashington, during his mayorship, announced a plan to redevelop a commercial site into a DuSable Park, named in honor of Jean Baptiste Point du Sable, the honorary founder of the city. The project has yet to be completed, has experienced a number of bureaucratic reconceptions and roadblocks, and is currently spearheaded by the DuSable Heritage Association.\nApproval ratings.\nDespite tumult between Washington and the City Council, Washington enjoyed positive approval among the city's residents.\nAn April 1987 \"Chicago Tribune\" poll of voters indicated that there was a significant age and gender gap in Washington's approval, with Washington being more popularly approved of by voters under the age of 55 and by male voters.\n&lt;includeonly&gt;&lt;templatestyles src=\"Chart/styles.css\"/&gt;&lt;/includeonly&gt;\nAcademic assessments.\nA 1993 survey of historians, political scientists and urban experts conducted by Melvin G. Holli of the University of Illinois at Chicago ranked Washington as the nineteenth-best American big-city mayor to have served between the years 1820 and 1993. A separate 1985 survey of experts on Chicago politics by Holli saw the then-incumbent Washington ranked eleventh-best among all Chicago mayors (up to that time). A 1994 iteration of the same survey of Chicago political experts saw Washington ranked third-best among all Chicago mayors.\nDeath and funeral.\nOn November 25, 1987, at 11:00\u00a0am, Chicago Fire Department paramedics were called to City Hall. Washington's press secretary, Alton Miller, had been discussing school board issues with the mayor when Washington suddenly slumped over on his desk, falling unconscious. After failing to revive Washington in his office, paramedics rushed him to Northwestern Memorial Hospital. Further attempts to revive him failed, and Washington was pronounced dead at 1:36\u00a0p.m.\nAt Daley Plaza, Patrick Keen, project director for the Westside Habitat for Humanity, announced Washington's official time of death to a separate gathering of Chicagoans. Initial reactions to the pronouncement of his death were of shock and sadness, as many black people believed that Washington was the only top Chicago official who would address their concerns. Following his death, President Ronald Reagan issued a statement calling Washington a \"dedicated and outspoken leader who guided one of our nation's largest cities through the 1980's\".\nThousands of Chicagoans attended his wake in the lobby of City Hall between November 27 and 29, 1987. On November 30, 1987, Reverend B. Herbert Martin officiated Washington's funeral service in Christ Universal Temple at 119th Street and Ashland Avenue in Chicago. After the service, Washington was buried in Oak Woods Cemetery on the South Side of Chicago.\nRumors.\nImmediately after Washington's death, rumors about how Washington died began to surface. On January 6, 1988, Dr. Antonio Senat, Washington's personal physician, denied \"unfounded speculations\" that Washington had cocaine in his system at the time of his death, or that foul play was involved. Cook County Medical Examiner Robert J. Stein performed an autopsy on Washington and concluded that Washington had died of a heart attack. Washington had weighed , and suffered from hypertension, high cholesterol levels, and an enlarged heart. His diet was \"heavy in fat and sodium\", wrote Gary Rivlin. On June 20, 1988, Alton Miller again indicated that drug reports on Washington had come back negative, and that Washington had not been poisoned prior to his death. Dr. Stein stated that the only drug in Washington's system had been lidocaine, which is used to stabilize the heart after a heart attack takes place. The drug was given to Washington either by paramedics or by doctors at Northwestern Memorial Hospital. Bernard Epton, Washington's opponent in the 1983 general election, died 18 days later, on December 13, 1987.\nLegacy.\nAt a party held shortly after his re-election on April 7, 1987, Washington said to a group of supporters, \"In the old days, when you told people in other countries that you were from Chicago, they would say, 'Boom-boom! Rat-a-tat-tat!' Nowadays, they say [crowd joins with him], 'How's Harold?'!\"\nIn later years, various city facilities and institutions were named or renamed after the late mayor to commemorate his legacy. The new building housing the main branch of the Chicago Public Library, located at 400 South State Street, was named the Harold Washington Library Center. The Chicago Public Library Special Collections, located on the building's 9th floor, house the Harold Washington Archives and Collections. These archives hold numerous collections related to Washington's life and political career. The building also contains Jacob Lawrence's mural \"Events in the Life of Harold Washington\".\nFive months after Washington's sudden death in office, a ceremony was held on April 19, 1988, changing the name of Loop College, one of the City Colleges of Chicago, to Harold Washington College. Harold Washington Elementary School in Chicago's Chatham neighborhood is also named after the former mayor. In August 2004, the Harold Washington Cultural Center opened to the public in the Bronzeville neighborhood. Across from the Hampton House apartments where Washington lived, a city park was renamed Harold Washington Park, which was known for \"Harold's Parakeets\", a colony of feral monk parakeets that inhabited Ash Trees in the park. A building on the campus of Chicago State University is named Harold Washington Hall.\nSix months after Washington's death, School of the Art Institute of Chicago student David Nelson painted \"Mirth &amp; Girth\", a full-length portrait depicting Washington wearing women's lingerie. The work was unveiled on May 11, 1988, opening day of SAIC's annual student exhibition. Within hours, City aldermen and members of the Chicago Police Department seized the painting. It was later returned, but with a five-inch (13\u00a0cm) gash in the canvas. Nelson, assisted by the ACLU, filed a federal lawsuit against the city, claiming that the painting's confiscation and subsequent damaging violated his First Amendment rights. The complainants eventually split a US$95,000 (1994, US$138,000 in 2008) settlement from the city."}
{"id": "40307", "revid": "31339567", "url": "https://en.wikipedia.org/wiki?curid=40307", "title": "Plasma stability", "text": "Degree to which disturbing a plasma system at equilibrium will destabilize it\nIn plasma physics, plasma stability concerns the stability properties of a plasma in equilibrium and its behavior under small perturbations. The stability of the system determines if the perturbations will grow, oscillate, or be damped out. It is an important consideration in topics such as nuclear fusion and astrophysical plasma.\nIn many cases, a plasma can be treated as a fluid and analyzed with the theory of magnetohydrodynamics (MHD). MHD stability is necessary for stable operation of magnetic confinement fusion devices and places certain operational limits. The beta limit, for example, sets the maximum achievable plasma beta in tokamaks.\nOn the other hand, small-scale plasma instabilities (typically described by kinetic theory), such as the drift wave instability, are believed to be the driving mechanism of turbulent transport in tokamaks, which leads to high rate of particle and energy transport across the confining magnetic fields. Plasma instabilities described by kinetic theory can contain aspects such as finite Larmor radius (FLR) effects and resonant wave-particle interactions, which is not captured in fluid models such as MHD.\nPlasma instabilities.\nPlasma instabilities can be divided into two general groups:\nPlasma instabilities are also categorized into different modes (e.g. with reference to a particle beam):\nList of plasma instabilities.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nMHD Instabilities.\nBeta is a ratio of the plasma pressure over the magnetic field strength.\nformula_1\nMHD stability at high beta is crucial for a compact, cost-effective magnetic fusion reactor. Fusion power density varies roughly as formula_2 at constant magnetic field, or as formula_3 at constant bootstrap fraction in configurations with externally driven plasma current. (Here formula_4 is the normalized beta.) In many cases MHD stability represents the primary limitation on beta and thus on fusion power density. MHD stability is also closely tied to issues of creation and sustainment of certain magnetic configurations, energy confinement, and steady-state operation. Critical issues include understanding and extending the stability limits through the use of a\nvariety of plasma configurations, and developing active means for reliable operation near those limits. Accurate predictive capabilities are needed, which will require the addition of new physics to existing MHD models. Although a wide range of magnetic configurations exist, the underlying MHD physics is common to all. Understanding of MHD stability gained in one configuration can benefit others, by verifying analytic theories, providing benchmarks for predictive MHD stability codes, and advancing the development of active control techniques.\nThe most fundamental and critical stability issue for magnetic fusion is simply that MHD instabilities often limit performance at high beta. In most cases the important instabilities are long wavelength, global modes, because of their ability to cause severe degradation of energy confinement or termination of the plasma. Some important examples that are common to many magnetic configurations are ideal kink modes, resistive wall modes, and neoclassical tearing modes. A possible consequence of violating stability boundaries is a disruption, a sudden loss of thermal energy often followed by termination of the discharge. The key issue thus includes understanding the nature of the beta limit in the various configurations, including the associated thermal and magnetic stresses, and finding ways to avoid the limits or mitigate the consequences. A wide range of approaches to preventing such instabilities is under investigation, including optimization of the configuration of the plasma and its confinement device, control of the internal structure of the plasma, and active control of the MHD instabilities.\nIdeal Instabilities.\nIdeal MHD instabilities driven by current or pressure gradients represent the ultimate operational limit for most configurations. The long-wavelength kink mode and short-wavelength ballooning mode limits are generally well understood and can in principle be avoided.\nIntermediate-wavelength modes (n ~ 5\u201310 modes encountered in tokamak edge plasmas, for example) are less well understood due to the computationally intensive nature of the stability calculations. The extensive beta limit database for tokamaks is consistent with ideal MHD stability limits, yielding agreement to within about 10% in beta for cases where the internal profiles of the plasma are accurately measured. This good agreement provides confidence in ideal stability calculations for other configurations and in the design of prototype fusion reactors.\nResistive Wall Modes.\nResistive wall modes (RWM) develop in plasmas that require the presence of a perfectly conducting wall for stability. RWM stability is a key issue for many magnetic configurations. Moderate beta values are possible without a nearby wall in the tokamak, stellarator, and other configurations, but a nearby conducting wall can significantly improve ideal kink mode stability in most configurations, including the tokamak, ST, reversed field pinch (RFP), spheromak, and possibly the FRC. In the advanced tokamak and ST, wall stabilization is critical for operation with a large bootstrap fraction. The spheromak requires wall stabilization to avoid the low-m, n tilt and shift modes, and possibly bending modes. However, in the presence of a non-ideal wall, the slowly growing RWM is unstable. The resistive wall mode has been a long-standing issue for the RFP, and has more recently been observed in tokamak experiments. Progress in understanding the physics of the RWM and developing the means to stabilize it could be directly applicable to all magnetic configurations. A closely related issue is to understand plasma rotation, its sources and sinks, and its role in stabilizing the RWM.\nResistive instabilities.\nResistive instabilities are an issue for all magnetic configurations, since the onset can occur at beta values well below the ideal limit. The stability of neoclassical tearing modes (NTM) is a key issue for magnetic configurations with a strong bootstrap current. The NTM is a metastable mode; in certain plasma configurations, a sufficiently large deformation of the bootstrap current produced by a \"seed island\" can contribute to the growth of the island. The NTM is already an important performance-limiting factor in many tokamak experiments, leading to degraded confinement or disruption. Although the basic mechanism is well established, the capability to predict the onset in present and future devices requires better understanding of the damping mechanisms which determine the threshold island size, and of the mode coupling by which other instabilities (such as sawteeth in tokamaks) can generate seed islands. Resistive Ballooning Mode, similar to ideal ballooning, but with finite resistivity taken into consideration, provides another example of a resistive instability.\nOpportunities for Improving MHD Stability.\nConfiguration.\nThe configuration of the plasma and its confinement device represent an\nopportunity to improve MHD stability in a robust way. The benefits of discharge shaping and low aspect ratio for ideal MHD stability have been clearly demonstrated in tokamaks and STs, and will continue to be investigated in experiments such as DIII-D, Alcator C-Mod, NSTX, and MAST. New stellarator experiments such as NCSX (proposed) will test the prediction that addition of appropriately designed helical coils can stabilize ideal kink modes at high beta, and lower-beta tests\nof ballooning stability are possible in HSX. The new ST experiments provide an opportunity to test predictions that a low aspect ratio yields improved stability to tearing modes, including neoclassical, through a large stabilizing \"Glasser effect\" term associated with a large Pfirsch-Schl\u00fcter current. Neoclassical tearing modes can be avoided by minimizing the bootstrap current in quasi-helical and quasi-omnigenous stellarator configurations. Neoclassical tearing modes are also stabilized with the appropriate relative signs of the bootstrap current and the magnetic shear; this prediction is supported by the absence of NTMs in central negative shear regions of tokamaks. Stellarator configurations such as the proposed NCSX, a quasi-axisymmetric stellarator design, can be created with negative magnetic shear and positive bootstrap current to achieve stability to the NTM. Kink mode stabilization by a resistive wall has been demonstrated in RFPs and tokamaks, and will be investigated in other configurations including STs (NSTX) and spheromaks (SSPX). A new proposal to stabilize resistive wall modes by a flowing liquid lithium wall needs further evaluation.\nInternal Structure.\nControl of the internal structure of the plasma allows more active avoidance of MHD instabilities. Maintaining the proper current density profile, for example, can help to maintain stability to tearing modes. Open-loop optimization of the pressure and current density profiles with external heating and current drive sources is routinely used in many devices. Improved diagnostic measurements along with localized heating and current drive sources, now becoming available, will allow active feedback control of the internal profiles in the near future.\nSuch work is beginning or planned in most of the large tokamaks (JET, JT\u201360U, DIII\u2013D, C\u2013Mod, and ASDEX\u2013U) using RF heating and current drive. Real-time analysis of profile data such as MSE current profile measurements and real-time identification of stability boundaries are essential components of profile control. Strong plasma rotation can stabilize resistive wall modes, as demonstrated in tokamak experiments, and rotational shear is also predicted to stabilize resistive modes. Opportunities to test these predictions are provided by configurations such as the ST, spheromak, and FRC, which have a large natural diamagnetic rotation, as well as tokamaks with rotation driven by neutral beam injection. The Electric Tokamak experiment is intended to have a very large driven rotation, approaching Alfv\u00e9nic regimes where ideal stability may also be influenced. Maintaining sufficient plasma rotation, and the possible role of the RWM in damping the rotation, are important issues that can be investigated in these experiments.\nFeedback Control.\nActive feedback control of MHD instabilities should allow operation beyond the \"passive\" stability limits. Localized RF current drive at the rational surface is predicted to reduce or eliminate neoclassical tearing mode islands. Experiments have begun in ASDEX\u2013U and COMPASS-D with promising results, and are planned for next year in DIII\u2013D. Routine use of such a technique in generalized plasma conditions will require real-time identification of the unstable mode and its radial location. If the plasma rotation needed to stabilize the resistive wall mode cannot be maintained, feedback stabilization with external coils will be required. Feedback experiments have begun in DIII\u2013D and HBT-EP, and feedback control should be explored for the RFP and other configurations. Physics understanding of these active control techniques will be directly applicable between configurations.\nDisruption Mitigation.\nThe techniques discussed above for improving MHD stability are the principal means of avoiding disruptions. However, in the event that these techniques do not prevent an instability, the effects of a disruption can be mitigated by various techniques. Experiments in JT\u201360U have demonstrated reduction of electromagnetic stresses through operation at a neutral point for vertical stability. Pre-emptive removal of the plasma energy by injection of a large gas puff or an impurity pellet has been demonstrated in tokamak experiments, and ongoing experiments in C\u2013Mod, JT\u201360U, ASDEX\u2013U, and DIII\u2013D will improve the understanding and predictive capability. Cryogenic liquid jets of helium are another proposed technique, which may be required for larger devices. Mitigation techniques developed for tokamaks will be directly applicable to other configurations.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "40310", "revid": "31339567", "url": "https://en.wikipedia.org/wiki?curid=40310", "title": "Magnetohydrodynamics", "text": "Model of electrically conducting fluids\nMagnetohydrodynamics (MHD; also called magneto-fluid dynamics or hydro\u00admagnetics) is a model of electrically conducting fluids that treats all types of charged particles together as one continuous fluid. It is primarily concerned with the low-frequency, large-scale, magnetic behavior in plasmas and liquid metals and has applications in multiple fields including space physics, geophysics, astrophysics, and engineering.\nThe word \"magnetohydrodynamics\" is derived from ' meaning magnetic field, ' meaning water, and \"\" meaning movement. The field of MHD was initiated by Hannes Alfv\u00e9n, for which he received the Nobel Prize in Physics in 1970.\nHistory.\nThe MHD description of electrically conducting fluids was first developed by Hannes Alfv\u00e9n in a 1942 paper published in \"Nature\" titled \"Existence of Electromagnetic\u2013Hydrodynamic Waves\" which outlined his discovery of what are now referred to as \"Alfv\u00e9n waves\". Alfv\u00e9n initially referred to these waves as \"electromagnetic\u2013hydrodynamic waves\"; however, in a later paper he noted, \"As the term 'electromagnetic\u2013hydrodynamic waves' is somewhat complicated, it may be convenient to call this phenomenon 'magneto\u2013hydrodynamic' waves.\"\nEquations.\nIn MHD, motion in the fluid is described using linear combinations of the mean motions of the individual species: the current density formula_1 and the center of mass velocity formula_2. In a given fluid, each species formula_3 has a number density formula_4, mass formula_5, electric charge formula_6, and a mean velocity formula_7. The fluid's total mass density is then formula_8, and the motion of the fluid can be described by the current density expressed as\nformula_9\nand the center of mass velocity expressed as:\nformula_10\nMHD can be described by a set of equations consisting of a continuity equation, an equation of motion (the Cauchy momentum equation), an equation of state, Amp\u00e8re's law, Faraday's law, and Ohm's law. As with any fluid description to a kinetic system, a closure approximation must be applied to the highest moment of the particle distribution equation. This is often accomplished with approximations to the heat flux through a condition of adiabaticity or isothermality.\nIn the adiabatic limit, that is, the assumption of an isotropic pressure formula_11 and isotropic temperature, a fluid with an adiabatic index formula_12, electrical resistivity formula_13, magnetic field formula_14, and electric field formula_15 can be described by the continuity equation\nformula_16\nthe equation of state\nformula_17\nthe equation of motion\nformula_18\nthe low-frequency Amp\u00e8re's law\nformula_19\nFaraday's law\nformula_20\nand Ohm's law\nformula_21\nTaking the curl of this equation and using Amp\u00e8re's law and Faraday's law results in the induction equation,\nformula_22\nwhere formula_23 is the magnetic diffusivity.\nIn the equation of motion, the Lorentz force term formula_24 can be expanded using Amp\u00e8re's law and a vector calculus identity to give\nformula_25\nwhere the first term on the right hand side is the magnetic tension force and the second term is the magnetic pressure force.\nIdeal MHD.\n&lt;templatestyles src=\"Template:Quote_box/styles.css\" /&gt;\nIn view of the infinite conductivity, every motion (perpendicular to the field) of the liquid in relation to the lines of force is forbidden because it would give infinite eddy currents. Thus the matter of the liquid is \"fastened\" to the lines of force...\nHannes Alfv\u00e9n\n, 1943\nThe simplest form of MHD, ideal MHD, assumes that the resistive term formula_26 in Ohm's law is small relative to the other terms such that it can be taken to be equal to zero. This occurs in the limit of large magnetic Reynolds numbers during which magnetic induction dominates over magnetic diffusion at the velocity and length scales under consideration. Consequently, processes in ideal MHD that convert magnetic energy into kinetic energy, referred to as \"ideal processes\", cannot generate heat and raise entropy.6\nA fundamental concept underlying ideal MHD is the frozen-in flux theorem which states that the bulk fluid and embedded magnetic field are constrained to move together such that one can be said to be \"tied\" or \"frozen\" to the other. Therefore, any two points that move with the bulk fluid velocity and lie on the same magnetic field line will continue to lie on the same field line even as the points are advected by fluid flows in the system.25 The connection between the fluid and magnetic field fixes the topology of the magnetic field in the fluid\u2014for example, if a set of magnetic field lines are tied into a knot, then they will remain so as long as the fluid has negligible resistivity. This difficulty in reconnecting magnetic field lines makes it possible to store energy by moving the fluid or the source of the magnetic field. The energy can then become available if the conditions for ideal MHD break down, allowing magnetic reconnection that releases the stored energy from the magnetic field.\nIdeal MHD equations.\nIn ideal MHD, the resistive term formula_26 vanishes in Ohm's law giving the ideal Ohm's law,\nformula_28\nSimilarly, the magnetic diffusion term formula_29 in the induction equation vanishes giving the ideal induction equation,23\nformula_30\nApplicability of ideal MHD to plasmas.\nIdeal MHD is only strictly applicable when:\nImportance of resistivity.\nIn an imperfectly conducting fluid the magnetic field can generally move through the fluid following a diffusion law with the resistivity of the plasma serving as a diffusion constant. This means that solutions to the ideal MHD equations are only applicable for a limited time for a region of a given size before diffusion becomes too important to ignore. One can estimate the diffusion time across a solar active region (from collisional resistivity) to be hundreds to thousands of years, much longer than the actual lifetime of a sunspot\u2014so it would seem reasonable to ignore the resistivity. By contrast, a meter-sized volume of seawater has a magnetic diffusion time measured in milliseconds.\nEven in physical systems\u2014which are large and conductive enough that simple estimates of the Lundquist number suggest that the resistivity can be ignored\u2014resistivity may still be important: many instabilities exist that can increase the effective resistivity of the plasma by factors of more than 109. The enhanced resistivity is usually the result of the formation of small scale structure like current sheets or fine scale magnetic turbulence, introducing small spatial scales into the system over which ideal MHD is broken and magnetic diffusion can occur quickly. When this happens, magnetic reconnection may occur in the plasma to release stored magnetic energy as waves, bulk mechanical acceleration of material, particle acceleration, and heat.\nMagnetic reconnection in highly conductive systems is important because it concentrates energy in time and space, so that gentle forces applied to a plasma for long periods of time can cause violent explosions and bursts of radiation.\nWhen the fluid cannot be considered as completely conductive, but the other conditions for ideal MHD are satisfied, it is possible to use an extended model called resistive MHD. This includes an extra term in Ohm's law which models the collisional resistivity. Generally MHD computer simulations are at least somewhat resistive because their computational grid introduces a numerical resistivity.\nStructures in MHD systems.\nIn many MHD systems most of the electric current is compressed into thin nearly-two-dimensional ribbons termed current sheets. These can divide the fluid into magnetic domains, inside of which the currents are relatively weak. Current sheets in\nthe solar corona are thought to be between a few meters and a few kilometers in thickness, which is quite thin compared to the magnetic domains (which are thousands to hundreds of thousands of kilometers across). Another example is in the Earth's magnetosphere, where current sheets separate topologically distinct domains, isolating most of the Earth's ionosphere from the solar wind.\nWaves.\nThe wave modes derived using the MHD equations are called magnetohydrodynamic waves or MHD waves. There are three MHD wave modes that can be derived from the linearized ideal-MHD equations for a fluid with a uniform and constant magnetic field:\nThese modes have phase velocities that are independent of the magnitude of the wavevector, so they experience no dispersion. The phase velocity depends on the angle between the wave vector k and the magnetic field B. An MHD wave propagating at an arbitrary angle \u03b8 with respect to the time independent or bulk field B0 will satisfy the dispersion relation\nformula_31\nwhere\nformula_32\nis the Alfv\u00e9n speed. This branch corresponds to the shear Alfv\u00e9n mode. Additionally the dispersion equation gives\nformula_33\nwhere\nformula_34\nis the ideal gas speed of sound. The plus branch corresponds to the fast-MHD wave mode and the minus branch corresponds to the slow-MHD wave mode. A summary of the properties of these waves is provided:\nThe MHD oscillations will be damped if the fluid is not perfectly conducting but has a finite conductivity, or if viscous effects are present.\nMHD waves and oscillations are a popular tool for the remote diagnostics of laboratory and astrophysical plasmas, for example, the corona of the Sun (Coronal seismology).\n Resistive MHD describes magnetized fluids with finite electron diffusivity (\"\u03b7\" \u2260 0). This diffusivity leads to a breaking in the magnetic topology; magnetic field lines can 'reconnect' when they collide. Usually this term is small and reconnections can be handled by thinking of them as not dissimilar to shocks; this process has been shown to be important in the Earth-Solar magnetic interactions.\n Extended MHD describes a class of phenomena in plasmas that are higher order than resistive MHD, but which can adequately be treated with a single fluid description. These include the effects of Hall physics, electron pressure gradients, finite Larmor Radii in the particle gyromotion, and electron inertia.\n Two-fluid MHD describes plasmas that include a non-negligible Hall electric field. As a result, the electron and ion momenta must be treated separately. This description is more closely tied to Maxwell's equations as an evolution equation for the electric field exists.\n In 1960, M. J. Lighthill criticized the applicability of ideal or resistive MHD theory for plasmas. It concerned the neglect of the \"Hall current term\" in Ohm's law, a frequent simplification made in magnetic fusion theory. Hall-magnetohydrodynamics (HMHD) takes into account this electric field description of magnetohydrodynamics, and Ohm's law takes the form formula_35 where formula_36 is the electron number density and formula_37 is the elementary charge. The most important difference is that in the absence of field line breaking, the magnetic field is tied to the electrons and not to the bulk fluid.\n Electron Magnetohydrodynamics (EMHD) describes small scales plasmas when electron motion is much faster than the ion one. The main effects are changes in conservation laws, additional resistivity, importance of electron inertia. Many effects of Electron MHD are similar to effects of the Two fluid MHD and the Hall MHD. EMHD is especially important for z-pinch, magnetic reconnection, ion thrusters, neutron stars, and plasma switches. \n MHD is also often used for collisionless plasmas. In that case the MHD equations are derived from the Vlasov equation.\n By using a multiscale analysis the (resistive) MHD equations can be reduced to a set of four closed scalar equations. This allows for, amongst other things, more efficient numerical calculations.\nLimitations.\nImportance of kinetic effects.\nAnother limitation of MHD (and fluid theories in general) is that they depend on the assumption that the plasma is strongly collisional (this is the first criterion listed above), so that the time scale of collisions is shorter than the other characteristic times in the system, and the particle distributions are Maxwellian. This is usually not the case in fusion, space and astrophysical plasmas. When this is not the case, or the interest is in smaller spatial scales, it may be necessary to use a kinetic model which properly accounts for the non-Maxwellian shape of the distribution function. However, because MHD is relatively simple and captures many of the important properties of plasma dynamics it is often qualitatively accurate and is therefore often the first model tried.\nEffects which are essentially kinetic and not captured by fluid models include double layers, Landau damping, a wide range of instabilities, chemical separation in space plasmas and electron runaway. In the case of ultra-high intensity laser interactions, the incredibly short timescales of energy deposition mean that hydrodynamic codes fail to capture the essential physics.\nApplications.\nGeophysics.\nBeneath the Earth's mantle lies the core, which is made up of two parts: the solid inner core and liquid outer core. Both have significant quantities of iron. The liquid outer core moves in the presence of the magnetic field and eddies are set up into the same due to the Coriolis effect. These eddies develop a magnetic field which boosts Earth's original magnetic field\u2014a process which is self-sustaining and is called the geomagnetic dynamo.\nBased on the MHD equations, Glatzmaier and Paul Roberts have made a supercomputer model of the Earth's interior. After running the simulations for thousands of years in virtual time, the changes in Earth's magnetic field can be studied. The simulation results are in good agreement with the observations as the simulations have correctly predicted that the Earth's magnetic field flips every few hundred thousand years. During the flips, the magnetic field does not vanish altogether\u2014it just gets more complex.\nEarthquakes.\nSome monitoring stations have reported that earthquakes are sometimes preceded by a spike in ultra low frequency (ULF) activity. A remarkable example of this occurred before the 1989 Loma Prieta earthquake in California, although a subsequent study indicates that this was little more than a sensor malfunction. On December 9, 2010, geoscientists announced that the DEMETER satellite observed a dramatic increase in ULF radio waves over Haiti in the month before the magnitude 7.0\u00a0Mw 2010 earthquake. Researchers are attempting to learn more about this correlation to find out whether this method can be used as part of an early warning system for earthquakes.\nSpace physics.\nThe study of space plasmas near Earth and throughout the Solar System is known as space physics. Areas researched within space physics encompass a large number of topics, ranging from the ionosphere to auroras, Earth's magnetosphere, the Solar wind, and coronal mass ejections.\nMHD forms the framework for understanding how populations of plasma interact within the local geospace environment. Researchers have developed global models using MHD to simulate phenomena within Earth's magnetosphere, such as the location of Earth's magnetopause (the boundary between the Earth's magnetic field and the solar wind), the formation of the ring current, auroral electrojets, and geomagnetically induced currents.\nOne prominent use of global MHD models is in space weather forecasting. Intense solar storms have the potential to cause extensive damage to satellites and infrastructure, thus it is crucial that such events are detected early. The Space Weather Prediction Center (SWPC) runs MHD models to predict the arrival and impacts of space weather events at Earth.\nAstrophysics.\nMHD applies to astrophysics, including stars, the interplanetary medium (space between the planets), and possibly within the interstellar medium (space between the stars) and jets. Most astrophysical systems are not in local thermal equilibrium, and therefore require an additional kinematic treatment to describe all the phenomena within the system (see Astrophysical plasma).\nSunspots are caused by the Sun's magnetic fields, as Joseph Larmor theorized in 1919. The solar wind, predicted by Eugene Parker, is also described by MHD. The differential solar rotation may be the long-term effect of magnetic drag at the poles of the Sun, an MHD phenomenon due to the Parker spiral shape assumed by the extended magnetic field of the Sun.\nPreviously, theories describing the formation of the Sun and planets could not explain how the Sun has 99.87% of the mass, yet only 0.54% of the angular momentum in the Solar System. In a closed system such as the cloud of gas and dust from which the Sun was formed, mass and angular momentum are both conserved. That conservation would imply that as the mass concentrated in the center of the cloud to form the Sun, it would spin faster, much like a skater pulling their arms in. The high speed of rotation predicted by early theories would have flung the proto-Sun apart before it could have formed. However, magnetohydrodynamic effects transfer the Sun's angular momentum into the outer solar system, slowing its rotation.\nBreakdown of ideal MHD (in the form of magnetic reconnection) is known to be the likely cause of solar flares. The magnetic field in a solar active region over a sunspot can store energy that is released suddenly as a burst of motion, X-rays, and radiation when the main current sheet collapses, reconnecting the field.\nMagnetic confinement fusion.\nMHD describes a wide range of physical phenomena occurring in fusion plasmas in devices such as tokamaks or stellarators.\nThe Grad-Shafranov equation derived from ideal MHD describes the equilibrium of axisymmetric toroidal plasma in a tokamak. In tokamak experiments, the equilibrium during each discharge is routinely calculated and reconstructed, which provides information on the shape and position of the plasma controlled by currents in external coils.\nMHD stability theory is known to govern the operational limits of tokamaks. For example, the ideal MHD kink modes provide hard limits on the achievable plasma beta (Troyon limit) and plasma current (set by the formula_38 requirement of the safety factor).\nIn a tokamak, instabilities also emerge from resistive MHD. For instance, tearing modes are instabilities arising within the framework of non-ideal MHD. This is an active field of research, since these instabilities are the starting point for disruptions.\nSensors.\nMagnetohydrodynamic sensors are used for precision measurements of angular velocities in inertial navigation systems such as in aerospace engineering. Accuracy improves with the size of the sensor. The sensor is capable of surviving in harsh environments.\nEngineering.\nMHD is related to engineering problems such as plasma confinement, liquid-metal cooling of nuclear reactors, and electromagnetic casting (among others).\nA magnetohydrodynamic drive or MHD propulsor is a method for propelling seagoing vessels using only electric and magnetic fields with no moving parts, using magnetohydrodynamics. The working principle involves electrification of the propellant (gas or water) which can then be directed by a magnetic field, pushing the vehicle in the opposite direction. Although some working prototypes exist, MHD drives remain impractical.\nThe first prototype of this kind of propulsion was built and tested in 1965 by Steward Way, a professor of mechanical engineering at the University of California, Santa Barbara. Way, on leave from his job at Westinghouse Electric, assigned his senior-year undergraduate students to develop a submarine with this new propulsion system. In the early 1990s, a foundation in Japan (Ship &amp; Ocean Foundation (Minato-ku, Tokyo)) built an experimental boat, the \"Yamato-1\", which used a magnetohydrodynamic drive incorporating a superconductor cooled by liquid helium, and could travel at 15\u00a0km/h.\nMHD power generation fueled by potassium-seeded coal combustion gas showed potential for more efficient energy conversion (the absence of solid moving parts allows operation at higher temperatures), but failed due to cost-prohibitive technical difficulties. One major engineering problem was the failure of the wall of the primary-coal combustion chamber due to abrasion.\nIn microfluidics, MHD is studied as a fluid pump for producing a continuous, nonpulsating flow in a complex microchannel design.\nMHD can be implemented in the continuous casting process of metals to suppress instabilities and control the flow.\nIndustrial MHD problems can be modeled using the open-source software EOF-Library. Two simulation examples are 3D MHD with a free surface for electromagnetic levitation melting, and liquid metal stirring by rotating permanent magnets.\nMagnetic drug targeting.\nAn important task in cancer research is developing more precise methods for delivery of medicine to affected areas. One method involves the binding of medicine to biologically compatible magnetic particles (such as ferrofluids), which are guided to the target via careful placement of permanent magnets on the external body. Magnetohydrodynamic equations and finite element analysis are used to study the interaction between the magnetic fluid particles in the bloodstream and the external magnetic field.\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "40311", "revid": "50796040", "url": "https://en.wikipedia.org/wiki?curid=40311", "title": "Great Chicago Fire", "text": "1871 conflagration in Illinois, US\nThe Great Chicago Fire was a conflagration that burned in the American city of Chicago, Illinois, during October 8\u201310, 1871. The fire killed approximately 300 people, destroyed roughly of the city including over 17,000 structures, and left more than 100,000 residents homeless. The fire began in a neighborhood southwest of the city center. A long period of hot, dry, windy conditions, and the wooden construction prevalent in the city, led to the conflagration spreading quickly. The fire leapt the south branch of the Chicago River and destroyed much of central Chicago and then crossed the main stem of the river, consuming the Near North Side.\nHelp flowed to the city from near and far after the fire. The city government improved building codes to stop the rapid spread of future fires and rebuilt rapidly to those higher standards. A donation from the United Kingdom spurred the establishment of the Chicago Public Library.\nOrigin.\nAccording to Nancy Connolly, great-great granddaughter of Catherine O'Leary, the fire is said to have started at about 8:30\u00a0p.m. on October\u00a08, in or around a small barn belonging to the O'Leary family that bordered the alley behind 137 W. DeKoven Street when Daniel \"Peg-Leg\" Sullivan accidentally knocked over a lantern while looking for beer for a party. The shed next to the barn was the first building to be consumed by the fire. City officials never determined the cause of the blaze, but the rapid spread of the fire due to a long drought in that year's summer, strong winds from the southwest, and the rapid destruction of the water pumping system explain the extensive damage of the mainly wooden city structures. There has been much speculation over the years on a single start to the fire. The most popular tale blames Mrs. O'Leary's cow, which allegedly knocked over a lantern; others state that a group of men were gambling inside the barn and knocked over a lantern. Still other speculation suggests that the blaze was related to other fires in the Midwest that day.\nThe fire's spread was aided by the city's use of wood as the predominant building material in a style called balloon frame. More than two-thirds of the structures in Chicago at the time of the fire were made entirely of wood, with most of the houses and buildings being topped with highly combustible tar or shingle roofs. All of the city's sidewalks and many roads were also made of wood. Compounding this problem, Chicago received only of rain from July\u00a04 to October\u00a09, causing severe drought conditions before the fire, while strong southwest winds helped to carry flying embers toward the heart of the city.\nIn 1871, the Chicago Fire Department had 185 firefighters with just 17 horse-drawn steam pumpers to protect the entire city. The initial response by the fire department was timely, but due to an error by the watchman, Mathias Schaffer, the firefighters were initially sent to the wrong place, allowing the fire to grow unchecked. An alarm sent from the area near the fire also failed to register at the courthouse where the fire watchmen were, while the firefighters were tired from having fought numerous small fires and one large fire in the week before. These factors combined to turn a small barn fire into a conflagration.\nSpread.\nWhen firefighters finally arrived at DeKoven Street, the fire had grown and spread to neighboring buildings and was progressing toward the central business district. Firefighters had hoped that the South Branch of the Chicago River and an area that had previously thoroughly burned would act as a natural firebreak. All along the river, however, were lumber yards, warehouses, and coal yards, as well as barges, and numerous bridges across the river. As the fire grew, the southwest wind intensified and the temperature rose, causing structures to catch fire from the heat and from burning debris blown by the wind. Around midnight, flaming debris blew across the river and landed on roofs and the South Side Gas Works.\nWith the fire across the river and moving rapidly toward the heart of the city, panic set in. About this time, Mayor Roswell B. Mason sent messages to nearby towns asking for help. When the courthouse caught fire, he ordered the building to be evacuated and the prisoners jailed in the basement to be released. At 2:30\u00a0a.m. on the 9th, the cupola of the courthouse collapsed, sending the great bell crashing down. Some witnesses reported hearing the sound from a mile (1.6\u00a0km) away.\nAs more buildings succumbed to the flames, a major contributing factor to the fire's spread was a meteorological phenomenon known as a fire whirl. As hot air rises, it comes into contact with cooler air and begins to spin, creating a tornado-like effect. These fire whirls are likely what drove flaming debris so high and so far. Such debris was blown across the main branch of the Chicago River to a railroad car carrying kerosene. The fire had jumped the river a second time and was now raging across the city's north side.\nDespite the fire spreading and growing rapidly, the city's firefighters continued to battle the blaze. A short time after the fire jumped the river, a burning piece of timber lodged on the roof of the city's waterworks. Within minutes, the interior of the building was engulfed in flames and the building was destroyed. With it, the city's water mains went dry and the city was helpless. The fire burned unchecked from building to building, block to block.\nLate in the evening of October 9, it started to rain, but the fire had already started to burn itself out. The fire had spread to the sparsely populated areas of the north side, having thoroughly consumed the densely populated areas.\nAftermath.\nOnce the fire had ended, the smoldering remains were still too hot for a survey of the damage to be completed for many days. Eventually, the city determined that the fire destroyed an area about long and averaging wide, encompassing an area of more than . Destroyed were more than of roads, of sidewalk, 2,000 lampposts, 17,500 buildings, and $222 million in property, which was about a third of the city's valuation in 1871.\nOn October 11, 1871, General Philip H. Sheridan came quickly to the aid of the city and was placed in charge by a proclamation, given by mayor Roswell B. Mason: \"The Preservation of the Good Order and Peace of the city is hereby intrusted to Lieut. General P.H. Sheridan, U.S. Army.\"To protect the city from looting and violence, the city was put under martial law for two weeks under Gen. Sheridan's command structure with a mix of regular troops, militia units, police, and a specially organized civilian group \"First Regiment of Chicago Volunteers.\" Former Lieutenant-Governor William Bross, and part owner of the \"Tribune\", later recollected his response to the arrival of Gen. Sheridan and his soldiers: \"Never did deeper emotions of joy overcome me. Thank God, those most dear to me and the city as well are safe.\"\nFor two weeks Sheridan's men patrolled the streets, guarded the relief warehouses, and enforced other regulations. On October 24 the troops were relieved of their duties and the volunteers were mustered out of service.\nOf the approximately 324,000 inhabitants of Chicago in 1871, 90,000 Chicago residents (about 28% of the population) were left homeless. 120 bodies were recovered, but the death toll may have been as high as 300. The county coroner speculated that an accurate count was impossible, as some victims may have drowned or had been incinerated, leaving no remains.\nIn the days and weeks following the fire, monetary donations flowed into Chicago from around the country and abroad, along with donations of food, clothing, and other goods. These donations came from individuals, corporations, and cities. New York City gave $450,000 along with clothing and provisions, St. Louis gave $300,000, and the Common Council of London gave 1,000 guineas, as well as \u00a37,000 from private donations. In Greenock, Scotland (pop. 40,000) a town meeting raised \u00a3518 on the spot. Cincinnati, Cleveland, and Buffalo, all commercial rivals, donated hundreds and thousands of dollars. Milwaukee, along with other nearby cities, helped by sending fire-fighting equipment. Food, clothing and books were brought by train from all over the continent. Mayor Mason placed the Chicago Relief and Aid Society in charge of the city's relief efforts.\nOperating from the First Congregational Church, city officials and aldermen began taking steps to preserve order in Chicago. Price gouging was a key concern, and in one ordinance, the city set the price of bread at 8\u00a2 for a loaf. Public buildings were opened as places of refuge, and saloons closed at 9 in the evening for the week following the fire. Many people who were left homeless after the incident were never able to get their normal lives back since all their personal papers and belongings burned in the conflagration.\nAfter the fire, A. H. Burgess of London proposed an \"English Book Donation\", to spur a free library in Chicago, in their sympathy with Chicago over the damages suffered. Libraries in Chicago had been private with membership fees. In April 1872, the City Council passed the ordinance to establish the free Chicago Public Library, starting with the donation from the United Kingdom of more than 8,000 volumes.\nThe fire also led to questions about development in the United States. Due to Chicago's rapid expansion at that time, the fire led to Americans reflecting on industrialization. Based on a religious point of view, some said that Americans should return to a more old-fashioned way of life, and that the fire was caused by people ignoring traditional morality. On the other hand, others believed that a lesson to be learned from the fire was that cities needed to improve their building techniques. Frederick Law Olmsted observed that poor building practices in Chicago were a problem:\nChicago had a weakness for \"big things\", and liked to think that it was outbuilding New York. It did a great deal of commercial advertising in its house-tops. The faults of construction as well as of art in its great showy buildings must have been numerous. Their walls were thin, and were overweighted with gross and coarse misornamentation.\nOlmsted also believed that with brick walls, and disciplined firemen and police, the deaths and damage caused would have been much less.\nAlmost immediately, the city began to rewrite its fire standards, spurred by the efforts of leading insurance executives, and fire-prevention reformers such as Arthur C. Ducat. Chicago soon developed one of the country's leading fire-fighting forces.\nBusiness owners and land speculators such as Gurdon Saltonstall Hubbard quickly set about rebuilding the city. The first load of lumber for rebuilding was delivered the day the last burning building was extinguished. By the World's Columbian Exposition 22 years later, Chicago hosted more than 21 million visitors. The Palmer House hotel burned to the ground in the fire 13 days after its grand opening. Its developer, Potter Palmer, secured a loan and rebuilt the hotel to higher standards, across the street from the original, proclaiming it to be \"The World's First Fireproof Building\".\nIn 1956, the remaining structures on the original O'Leary property at 558 W. DeKoven Street were torn down for construction of the Chicago Fire Academy, a training facility for Chicago firefighters, known as the Quinn Fire Academy or Chicago Fire Department Training Facility. A bronze sculpture of stylized flames, entitled \"Pillar of Fire\" by Egon Weiner, was erected on the point of origin in 1961.\nSurviving structures.\nThe following structures from the burned district are still standing:\nSt. Michael's Church and the Pumping Station were both gutted in the fire, but their exteriors survived, and the buildings were rebuilt using the surviving walls. Additionally, though the inhabitable portions of the building were destroyed, the bell tower of St. James Cathedral survived the fire and was incorporated into the rebuilt church. The stones near the top of the tower are still blackened from the soot and smoke.\nPrecise start.\nAlmost from the moment the fire broke out, various theories about its cause began to circulate. The most popular and enduring legend maintains that the fire began in the O'Leary barn as Mrs. O'Leary was milking her cow. The cow kicked over a lantern (or an oil lamp in some versions), setting fire to the barn. The O'Leary family denied this, stating that they were in bed before the fire started, but stories of the cow began to spread across the city. Catherine O'Leary seemed the perfect scapegoat: she was a poor, Irish Catholic immigrant. During the latter half of the 19th century, anti-Irish sentiment was strong in Chicago and throughout the United States. This was intensified as a result of the growing political power of the city's Irish population.\nFurthermore, the United States had been distrustful of Catholics (or papists, as they were often called) since its beginning, carrying over attitudes in England in the 17th century; as an Irish Catholic, Mrs. O'Leary was a target of both anti-Catholic and anti-Irish sentiment. This story was circulating in Chicago even before the flames had died out, and it was noted in the \"Chicago Tribune\"'s first post-fire issue. In 1893 the reporter Michael Ahern retracted the \"cow-and-lantern\" story, admitting it was fabricated, but even his confession was unable to put the legend to rest. Although the O'Learys were never officially charged with starting the fire, the story became so engrained in local lore that Chicago's city council officially exonerated them\u2014and the cow\u2014in 1997.\nAmateur historian Richard Bales has suggested the fire started when Daniel \"Pegleg\" Sullivan, who first reported the fire, ignited hay in the barn while trying to steal milk. Part of Bales's evidence includes an account by Sullivan, who claimed in an inquiry before the Fire Department of Chicago on November 25, 1871, that he saw the fire coming through the side of the barn and ran across DeKoven Street to free the animals from the barn, one of which included a cow owned by Sullivan's mother. Bales's account does not have consensus. The Chicago Public Library staff criticized his account in their web page on the fire. Despite this, the Chicago city council was convinced of Bales's argument and stated that the actions of Sullivan on that day should be scrutinized after the O'Leary family was exonerated in 1997.\nAnthony DeBartolo reported evidence in two articles of the \"Chicago Tribune\" (October 8, 1997, and March 3, 1998, reprinted in \"Hyde Park Media\") suggesting that Louis M. Cohn may have started the fire during a craps game. Following his death in 1942, Cohn bequeathed $35,000 which was assigned by his executors to the Medill School of Journalism at Northwestern University. The bequest was given to the school on September 28, 1944, and the dedication contained a claim by Cohn to have been present at the start of the fire. According to Cohn, on the night of the fire, he was gambling in the O'Learys' barn with one of their sons and some other neighborhood boys. When Mrs. O'Leary came out to the barn to chase the gamblers away at around 9:00, they knocked over a lantern in their flight, although Cohn states that he paused long enough to scoop up the money. The argument is not universally accepted.\nAn alternative theory, first suggested in 1882 by Ignatius L. Donnelly in \"\", is that the fire was caused by a meteor shower. This was described as a \"fringe theory\" concerning Biela's Comet. At a 2004 conference of the Aerospace Corporation and the American Institute of Aeronautics and Astronautics, engineer and physicist Robert Wood suggested that the fire began when a fragment of Biela's Comet impacted the Midwest. Biela's Comet had broken apart in 1845 and had not been observed since. Wood argued that four large fires took place, all on the same day, all on the shores of Lake Michigan (see related events), suggesting a common root cause. Eyewitnesses reported sighting spontaneous ignitions, lack of smoke, \"balls of fire\" falling from the sky, and blue flames. According to Wood, these accounts suggest that the fires were caused by the methane that is commonly found in comets.\nMeteorites are not known to start or spread fires and are cool to the touch after reaching the ground, so this theory has not found favor in the scientific community. Methane-air mixtures become flammable only when the methane concentration exceeds 5%, at which point the mixtures also become explosive, a situation unlikely to occur from meteorites. Methane gas is lighter than air and thus does not accumulate near the ground; any localized pockets of methane in the open air rapidly dissipate. Moreover, if a fragment of an icy comet were to strike the Earth, the most likely outcome, due to the low tensile strength of such bodies, would be for it to disintegrate in the upper atmosphere, leading to a meteor air burst like the Tunguska event.\nThe specific choice of Biela's Comet does not match with the dates in question, as the 6-year period of the comet's orbit did not intersect that of the Earth until 1872, one full year after the fire, when a large meteor shower was observed. A common cause for the fires in the Midwest in late 1871 is that the area had had a dry summer, so that winds from the front that moved in that evening were capable of generating rapidly expanding blazes from available ignition sources, which were plentiful in the region.\nRelated events.\nOn that hot, dry, and windy autumn day, three other major fires occurred along the shores of Lake Michigan at the same time as the Great Chicago Fire. Some to the north, the Peshtigo Fire consumed the town of Peshtigo, Wisconsin, along with a dozen other villages. It killed 1,200 to 2,500 people and charred approximately . The Peshtigo Fire remains the deadliest in American history but the remoteness of the region meant it was little noticed at the time, because one of the first things that burned were the telegraph lines to Green Bay.\nAcross the lake to the east, the town of Holland, Michigan, and other nearby areas burned to the ground. Some to the north of Holland, the lumbering community of Manistee also went up in flames in what became known as the Great Michigan Fire.\nFarther east, along the shore of Lake Huron, the Port Huron Fire swept through Port Huron, Michigan and much of Michigan's \"Thumb\". On October 9, 1871, a fire swept through the city of Urbana, Illinois, south of Chicago, destroying portions of its downtown area. Windsor, Ontario, likewise burned on October 12.\nThe city of Singapore, Michigan, provided a large portion of the lumber to rebuild Chicago. As a result, the area was so heavily deforested that the land deteriorated into barren sand dunes that buried the town, and the town had to be abandoned.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "40313", "revid": "50603748", "url": "https://en.wikipedia.org/wiki?curid=40313", "title": "Universal grammar", "text": "Theory of the biological component of the language faculty\nUniversal grammar (UG), in modern linguistics, is the theory of the innate biological component of the language faculty, usually credited to Noam Chomsky. The basic postulate of UG is that there are innate constraints on what the grammar of a possible human language could be. When linguistic stimuli are received in the course of language acquisition, children then adopt specific syntactic rules that conform to UG. The advocates of this theory emphasize and partially rely on the poverty of the stimulus (POS) argument and the existence of some universal properties of natural human languages. However, the latter has not been firmly established.\nOther linguists have opposed that notion, arguing that languages are so diverse that the postulated universality is rare. The theory of universal grammar remains a subject of debate among linguists.\nOverview.\nThe term \"universal grammar\" is placeholder for whichever domain-specific features of linguistic competence turn out to be innate. Within generative grammar, it is generally accepted that there must be some such features, and one of the goals of generative research is to formulate and test hypotheses about which aspects those are. In day-to-day generative research, the notion that universal grammar exists motivates analyses in terms of general principles. As much as possible, facts about particular languages are derived from these general principles rather than from language-specific stipulations.\nEvidence.\nThe idea that at least some aspects are innate is motivated by poverty of the stimulus arguments. For example, one famous poverty of the stimulus argument concerns the acquisition of yes\u2013no questions in English. This argument starts from the observation that children only make mistakes compatible with rules targeting hierarchical structure even though the examples which they encounter could have been generated by a simpler rule that targets linear order. In other words, children seem to ignore the possibility that the question rule is as simple as \"switch the order of the first two words\" and immediately jump to alternatives that rearrange constituents in tree structures. This is taken as evidence that children are born knowing that grammatical rules involve hierarchical structure, even though they have to figure out what those rules are.\nHistory.\nBetween 1100 and 1400, the theoretical work on matters of language significantly expanded in western Europe, its typical social context being the teaching of grammar, logic, or theology, producing a vast literature on aspects of linguistic theory, such as a 13th century theory of grammar known in modern times as modism, although no assertions were made in the texts about \"a theory of language,\" as such. While not much work was done on the evolution of languages, Dante and Roger Bacon offered perceptive observations. Bacon had a complex notion of grammar, which ranged from the teaching of elementary Latin through what he termed \"rational grammar\", to research on the so-called languages of sacred wisdom, i.e. Latin and Greek. Professor of Latin literature Raf Van Rooy quotes Bacon's \"notorious\" dictum on grammar used to denote regional linguistic variation and notes Bacon's contention that Latin and Greek, although \"one in substance,\" were each characterized by many \"idioms\" (idiomata: proprietates). Van Rooy speculates that Bacon's references to grammar concerned a \"quasi-universal nature of grammatical categories,\" whereas his assertions on Greek and Latin were applications of his \"lingua/idioma\" distinction rather than a generalizing statements on the nature of grammar. Linguistics professor Margaret Thomas acknowledges that \"intellectual commerce between ideas about Universal Grammar and [second language] acquisition is not a late-20th century invention,\" but rejects as \"convenient\" the interpretation of Bacon's dictum by generative grammarians as an assertion by the English polymath of the existence of universal grammar.\nThe concept of a generalized grammar was at the core of the 17th century projects for philosophical languages. An influential work in that time was \"Grammaire g\u00e9n\u00e9rale\" by Claude Lancelot and Antoine Arnauld. They describe a general grammar for languages, coming to the conclusion that grammar has to be universal. There is a Scottish school of universal grammarians from the 18th century that included James Beattie, Hugh Blair, James Burnett, James Harris, and Adam Smith, distinguished from the philosophical-language project.\nThe article on grammar in the first edition of the \"Encyclop\u00e6dia Britannica\" (1771) contains an extensive section titled \"Of Universal Grammar.\"\nIn the late 19th and early 20th century, Wilhelm Wundt and Otto Jespersen claimed that these earlier arguments were overly influenced by Latin and ignored the breadth of worldwide language real grammar\", but reduced it to universal syntactic categories or super-categories, such as number, tenses, etc.\nBehaviorists, after the rise of the eponymous theory, advanced the idea that language acquisition, like any other kind of learning, could be explained by a succession of trials, errors, and rewards for success. In other words, children, according to behaviorists, learn their mother tongue by simple imitation, i.e. through listening and repeating what adults say. For example, when a child says \"milk\" and the mother will smile and give milk to her child, then, as a result, the child will find this outcome rewarding, which enhances the child's language development.\nTheories of universal grammar.\nWithin generative grammar, there are a variety of theories about what universal grammar consists of. One notable hypothesis proposed by Hagit Borer holds that the fundamental syntactic operations are universal and that all variation arises from different feature-specifications in the lexicon. On the other hand, a strong hypothesis adopted in some variants of Optimality Theory holds that humans are born with a universal set of constraints, and that all variation arises from differences in how these constraints are ranked. In a 2002 paper, Noam Chomsky, Marc Hauser and W. Tecumseh Fitch proposed that universal grammar consists solely of the capacity for hierarchical phrase structure.\nMain hypotheses.\nIn an article entitled \"The Faculty of Language: What Is It, Who Has It, and How Did It Evolve?\" Hauser, Chomsky, and Fitch present the three leading hypotheses for how language evolved and brought humans to the point where they have a universal grammar.\nThe first hypothesis states that the faculty of language in the broad sense (FLb) is strictly homologous to animal communication. This means that homologous aspects of the faculty of language exist in non-human animals.\nThe second hypothesis states that the FLb is a derived and uniquely human adaptation for language. This hypothesis holds that individual traits were subject to natural selection and came to be specialized for humans.\nThe third hypothesis states that only the faculty of language in the narrow sense (FLn) is unique to humans. It holds that while mechanisms of the FLb are present in both human and non-human animals, the computational mechanism of recursion has evolved recently, and solely in humans.\nPresence of creole languages.\nThe presence of creole languages is sometimes cited as further support for this theory, especially by Bickerton's language bioprogram theory. Creole languages develop and form when disparate societies with no common language come together and are forced to devise a new system of communication. The system used by the original speakers is typically an inconsistent mix of vocabulary items, known as pidgin. As these speakers' children begin to acquire their first language, they use the pidgin input to effectively create their own original language, known as a creole language. Unlike pidgins, creole languages have native speakers (those with language acquisition from early childhood) and make use of a full, systematic grammar.\nBickerton claims the fact that certain features are shared by virtually all creole languages supports the notion of a universal grammar. For example, their default point of reference in time (expressed by bare verb stems) is not the present moment, but the past. Using pre-verbal auxiliaries, they uniformly express tense, aspect, and mood. Negative concord occurs, but it affects the verbal subject (as opposed to the object, as it does in languages like Spanish). Another similarity among creole languages can be identified in the fact that questions are created simply by changing the intonation of a declarative sentence; not its word order or content.\nOpposing this notion, the work by Carla Hudson-Kam and Elissa Newport suggests that creole languages may not support a universal grammar at all. In a series of experiments, Hudson-Kam and Newport looked at how children and adults learn artificial grammars. They found that children tend to ignore minor variations in the input when those variations are infrequent, and reproduce only the most frequent forms. In doing so, the children tend to standardize the language they hear around them. Hudson-Kam and Newport hypothesize that in a pidgin-development situation (and in the real-life situation of a deaf child whose parents are or were disfluent signers), children systematize the language they hear, based on the probability and frequency of forms, and not that which has been suggested on the basis of a universal grammar. Further, they argue, it seems to follow that creole languages would share features with the languages from which they are derived, and thus look similar in terms of grammar.\nMany researchers of universal grammar argue against the concept of relexification, i.e. that a language replaces its lexicon almost entirely with that of another. This, they argue, goes against the universalist notions of a universal grammar, which has an innate grammar.\nViews and assessments.\nRecent research has used recurrent neural network architectures (RNNs). McCoy et al. (2018) focused on a strong version of the poverty-of-the-stimulus argument, which claims that language learners require a hierarchical \"constraint\", although they report that a milder version, which only asserts that a hierarchical \"bias\" is necessary, is difficult to assess using RNNs because RNNs must possess some biases and the nature of these biases remains \"currently poorly understood.\" They go on to acknowledge that while all the architectures they used had a bias toward linear order and the GRU-with-attention architecture was the only one that overcame this linear bias sufficiently to generalize hierarchically. \"Humans certainly could have such an innate constraint.\"\nThe empirical basis of poverty-of-the-stimulus arguments has been challenged by Geoffrey Pullum and others, leading to a persistent back-and-forth debate in the language acquisition literature.\nLanguage acquisition researcher Michael Ramscar has suggested that when children erroneously expect an ungrammatical form that then never occurs, the repeated failure of expectation serves as a form of implicit negative feedback that allows them to correct their errors over time, in the way that, for example, children correct grammar generalizations like \"goed\" to \"went\" through repetitive failure.\nIn addition, it has been suggested that people learn about probabilistic patterns of word distribution in their language, rather than hard and fast rules (see Distributional hypothesis). For example, in English, children overgeneralize the past tense marker \"-ed\" and conjugate irregular verbs as if they were regular, producing forms like \"goed\" and \"eated\", and then correct this deviancy over time. It has also been hypothesized that the poverty of the stimulus problem can be largely avoided if it is assumed that children employ \"similarity-based generalization\" strategies in language learning, i.e. generalizing about the usage of new words from similar words they already know how to use.\nNeurogeneticists Simon Fisher and Sonja Vernes observe that, with human language-skills being evidently unmatched elsewhere in the world's fauna, there have been several theories about one single mutation event occurring some time in the past in our nonspeaking ancestors, as argued by e.g. Chomsky (2011), i.e. some \"lone spark that was sufficient to trigger the sudden appearance of language and culture.\" They characterize that notion as \"romantic\" and \"inconsistent with the messy mappings between genetics and cognitive processes.\" According to Fisher &amp; Vernes, the link between genes to grammar has not been consistently mapped by scientists. What has been established by research, they claim, relates primarily to speech pathologies. The arising lack of certainty, they conclude, has provided an audience for \"unconstrained speculations\" that have fed the \"myth\" of \"so-called grammar genes\".&lt;ref name=\"Fisher/Vernes\"&gt;&lt;/ref&gt;\nProfessor of Natural Language Computing Geoffrey Sampson maintains that universal grammar theories are not falsifiable and are therefore pseudoscientific. He argues that the grammatical \"rules\" linguists posit are simply post-hoc observations about existing languages, rather than predictions about what is possible in a language. Sampson claims that every one of the \"poor\" arguments used to justify the language-instinct claim is wrong. He writes that \"either the logic is fallacious, or the factual data are incorrect (or, sometimes, both),\" and the \"evidence points the other way.\" Children are good at learning languages, because people are good at learning anything that life throws at us \u2014 not because we have fixed structures of knowledge built-in. Similarly, professor of cognitive science Jeffrey Elman argues that the unlearnability of languages ostensibly assumed by universal grammar is based on a too-strict, \"worst-case\" model of grammar, which is not in keeping with any actual grammar.\nLinguist James Hurford, in his article \"Nativist and Functional Explanations in Language Acquisition,\" offers the major differences between the glossogenetic and the phylogenetic mechanisms. He states that, \"Deep aspects of the form of language are not likely to be readily identifiable with obvious specific uses, and one cannot suppose that it will be possible to attribute them directly to the recurring short-term needs of successive generations in a community. Here, nativist explanations for aspects of the form of language, appealing to an innate Language Acquisition Device (LAD), seem appropriate. But use or function can also be appealed to on the evolutionary timescale, to attempt to explain the structure of the LAD itself.\" For Hurford, biological mutations plus functional considerations constitute the \"explanans\", while the LAD itself constitutes the \"explanandum\". The LAD is part of the species' heredity, the result of mutations over a long period, he states. But, while he agrees with Chomsky that the mechanism of grammaticisation is located in \"the Chomskyan LAD\" and that Chomsky is \"entirely right in emphasising that a language (E-language) is an artifact resulting from the interplay of many factors,\" he states that this artifact should be of great interest and systematic study, and can affect grammatical competence, i.e. \"I-language.\"\nMorten H. Christiansen, professor of Psychology, and Nick Chater, professor of Psychology and Language Sciences, have argued that \"a biologically determined UG is not evolutionarily viable.\" As the processes of language change are much more rapid than processes of genetic change, they state, language constitutes a \"moving target\" both over time and across different human populations, and, hence, cannot provide a stable environment to which language genes could have adapted. In following Darwin, they view language as a complex and interdependent \"organism,\" which evolves under selectional pressures from human learning and processing mechanisms, so that \"apparently arbitrary aspects of linguistic structure may result from general learning and processing biases deriving from the structure of thought processes, perceptuo-motor factors, cognitive limitations, and pragmatics\". Professor of linguistics Norbert Hornstein countered polemically that Christiansen and Chater appear \"to have no idea what generative grammar [theory] is,\" and \"especially, but not uniquely, about the Chomsky program.\" Hornstein points out that all \"grammatically informed psycho-linguistic works done today or before\" understand that generative/universal grammar capacities are but one factor among others needed to explain real-time acquisition.\" Christiansen and Chater's observation that \"language \"use\" involves multiple interacting variables\" [italics in the original] is, essentially, a truism. It is nothing new, he argues, to state that \"much more than a competence theory will be required\" to figure out how language is deployed, acquired, produced, or parsed. The position, he concludes, that universal grammar properties are just \"probabilistic generalizations over available linguistic inputs\" belongs to the \"traditional\" and \"debunked\" view held by associationists and structuralists many decades in the past.\nIn the same vein, professor of linguistics Nicholas Evans and professor of psycholinguistics Stephen C. Levinson observe that Chomsky's notion of a Universal Grammar has been mistaken for a set of substantial research findings about what all languages have in common, while, it is, \"in fact,\" the programmatic label for \"whatever it turns out to be that all children bring to learning a language.\" For substantial findings about universals across languages, they argue, one must turn to the field of linguistic typology, which bears a \"bewildering range of diverse languages\" and in which \"generalizations are really quite hard to extract.\" Chomsky's actual views, combining, as they claim, philosophical and mathematical approaches to structure with claims about the innate endowment for language, have been \"hugely influential in the cognitive sciences.\nWolfram Hinzen, in his work \"The philosophical significance of Universal Grammar\" seeks to re-establish the epistemological significance of grammar and addresses the three main current objections to Cartesian universal grammar, i.e. that it has no coherent formulation, it cannot have evolved by standard, accepted neo-Darwinian evolutionary principles, and it goes against the variation extant at all levels of linguistic organization, which lies at the heart of human faculty of language.\nIn the domain of field research, Daniel Everett has claimed that the Pirah\u00e3 language is a counterexample to the basic tenets of universal grammar because it lacks clausal embedding. According to Everett, this trait results from Pirah\u00e3 culture emphasizing present-moment concrete matters. Nevins et al. (2007) have responded that Pirah\u00e3 does, in fact, have clausal embedding, and that, even if it did not, this would be irrelevant to current theories of universal grammar. They addressed each of Everett's claims and, using Everett's \"rich material\" data, claim to have found no evidence of a causal relation between culture and grammatical structure. Pirah\u00e3 grammar, they concluded, presents no unusual challenge, much less the \"severe\" one claimed by Everett, to the notion of a universal grammar.\nDevelopments.\nIn 2017, Chomsky and Berwick co-wrote their book titled \"Why Only Us,\" where they defined both the minimalist program and the strong minimalist thesis and its implications, to update their approach to UG theory. According to Berwick and Chomsky, \"the optimal situation would be that UG reduces to the simplest computational principles which operate in accord with conditions of computational efficiency. This conjecture is ... called the Strong Minimalist Thesis (SMT).\"\nThe significance of SMT is to shift the previous emphasis on a universal grammar to the concept that Chomsky and Berwick now call \"merge\". \"Merge\" is defined there as follows:Every computational system has embedded within it somewhere an operation that applies to two objects X and Y already formed, and constructs from them a new object Z. Call this operation Merge. SMT dictates that \"Merge will be as simple as possible: it will not modify X or Y or impose any arrangement on them; in particular, it will leave them unordered; an important fact. Merge is therefore just set formation: Merge of X and Y yields the set {X, Y}.\"\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nBibliography.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "40314", "revid": "49402000", "url": "https://en.wikipedia.org/wiki?curid=40314", "title": "Munro", "text": "Scottish peak over 3,000 ft and listed on the SMC tables\nA Munro (; ) is defined as a mountain in Scotland with a height over , and which is on the Scottish Mountaineering Club (SMC) official list of Munros; there is no explicit topographical prominence requirement. The best known Munro is Ben Nevis (Beinn Nibheis), the highest mountain in the British Isles at 4,411\u00a0ft (1,345 m).\nMunros are named after Sir Hugh Munro, 4th Baronet (1856\u20131919), who produced the first list of such hills, known as \"Munro's Tables\", in 1891. Also included were what Munro considered lesser peaks, now known as Munro Tops, which are also over 3,000 feet but are lower than the nearby primary mountain. The publication of the original list is usually considered to be the epoch event of modern peak bagging. The list has been the subject of subsequent variation and as of 2020[ [update]], the Scottish Mountaineering Club has listed 282 Munros and 226 Munro Tops.\n\"Munro bagging\" is the activity of climbing all the listed Munros. As of 2023[ [update]], 7,654 people had reported completing a round. The first continuous round was completed by Hamish Brown in 1974, whilst the record for the fastest continuous round is held by ultra runner Jamie Aarons, who completed a round in 31 days 10 hours 27\u00a0min in June 2023. Furths are mountains in England, Wales or Ireland recognized by the SMC as meeting the Munro classification.\nHistory.\nBefore the publication of \"Munro's Tables\" in 1891, there was much uncertainty about the number of Scottish peaks over 3,000 feet. Estimates ranged from 31 (in M.J.B. Baddeley's guides) to 236 (listed in Robert Hall's third edition of \"The Highland Sportsman and Tourist\", published in 1884). When the Scottish Mountaineering Club was formed in 1889, one of its aims was to remedy this by accurately documenting all of Scotland's mountains over 3,000 feet. Sir Hugh Munro, a founding member of the club, took on the task using his own experience as a mountaineer, as well as detailed study of the Ordnance Survey six-inch-to-the-mile (1:10,560) and one-inch-to-the-mile (1:63,360) maps.\nMunro researched and produced a set of tables that were published in the Scottish Mountaineering Club Journal in September 1891. The tables listed 538 summits over 3,000 feet, 282 of which were regarded as \"separate mountains\". The term \"Munro\" applies to separate mountains, while the lesser summits are known as \"Munro Tops\". Munro did not set any measure of topographic prominence by which a peak qualified as a separate mountain, so there has been much debate about how distinct two hills must be if they are to be counted as two separate Munros.\nThe Scottish Mountaineering Club has revised the tables, both in response to new height data on Ordnance Survey maps and to address the perceived inconsistency as to which peaks qualify for Munro status. In 1992, the publication of Alan Dawson's book \"Relative Hills of Britain\", showed that three Munro Tops not already considered summits, had a prominence of more than . Given this they would have qualified as Corbett summits had they been under 3,000 feet. In the 1997 tables these three Munro Tops, on Beinn Alligin, Beinn Eighe and Buachaille Etive Beag, gained full Munro summit status. Dawson's book also highlighted a number of significant Munro Tops with as much as of prominence which were not listed as Munro Tops. The 1997 tables promoted five of these to full Munro status.\nA total of 197 Munros have a topographic prominence of over and are regarded by peakbaggers as Real Munros. 130 Scottish mountains over 1000\u00a0m, with a topographic prominence of over have been termed Metric Munros.\nOther classification schemes in Scotland, such as the Corbetts and Grahams , require a peak to have a prominence of at least for inclusion. The Munros, however, lack a rigid set of criteria for inclusion, with many summits of lesser prominence listed, principally because their summits are hard to reach.\nBetween April 2007 and July 2015 the Munro Society re-surveyed twenty mountains and tops that were known to be close to the 3,000\u00a0ft figure to determine their height more accurately. On 10 September 2009 the society announced that the mountain Sg\u00f9rr nan Ceannaichean, south of Glen Carron, had a height of . Therefore, the Scottish Mountaineering Club removed the Munro status of Sg\u00f9rr nan Ceannaichean and this mountain is now a Corbett. In a Summer 2011 height survey by The Munro Society, Beinn a' Chlaidheimh was found to be and thus short of the Munro mark. On 6 September 2012, the Scottish Mountaineering Club demoted it from Munro to Corbett status. On 26 August 2020, the SMC confirmed that Beinn a' Chroin West Top at 938\u00a0m was deleted as a Munro Top and Beinn a' Chroin East Top became the new Munro Top at 940.1\u00a0m. The summit height of Beinn a' Chroin was also changed to 941.4\u00a0m. As of 10 December 2020, there were 226 Scottish Munro Tops after Stob Coire na Cloiche, a Munro Top to Parent Peak Sg\u00f9rr nan Ceathramhnan, was surveyed at 912.5\u00a0m and was deleted as a Munro Top and downgraded to a Corbett Top.\nAs of 10 December 2020, the Scottish Mountaineering Club lists 282 Munros and 226 Munro Tops. The current SMC list totals 508 summits.\nNotable peaks.\nThe most famous Munro is Ben Nevis (Beinn Nibheis) in the Lochaber area. It is the highest peak in the British Isles, with an elevation of 4,411\u00a0ft (1,345 m)\nOther well-known Munros include:\nPeak bagging.\nWhen compared to continental ranges, such as The Alps, Scottish peaks are generally lesser in height. However, walking and climbing in them can still be dangerous and difficult to navigate the recommended routes due to their latitude and exposure to Atlantic and Arctic weather systems. Even in summer, the weather can change quickly in the mountains and conditions can be atrocious; thunderstorms, thick fog, strong winds, driving rain and freezing summit temperatures close to 0\u00a0\u00b0C are not unusual.\nWinter ascents of some Munros are serious undertakings due to the unpredictable weather, the likelihood of ice and snow, and poor visibility. Each ascent becomes a test of skill, endurance, and determination, as trekkers navigate through snowdrifts, icy slopes, and unpredictable weather conditions. Some hikers try even unprepared for extreme weather on the exposed tops and fatalities are recorded every year, often resulting from slips on wet rock or ice.\nThe activity of attempting to climb every Munro is known as \"Munro bagging\". Munro-bagging is a form of peak bagging. A walker who has climbed all Munros is entitled to be called a Munroist. Descending a Munro by funicular is known as de-bagging.\nNotable completions.\nThe Scottish Mountaineering Club maintains a list of walkers who have reported completing the Munros. As of 2023[ [update]], there are 7,654 names on the list. (The club uses the spelling \"compleator\" for someone who has completed the Munros.)\nHugh Munro never completed his own list, missing out on C\u00e0rn an Fhidhleir and C\u00e0rn Cloich-mhuillin (downgraded to a Munro Top in 1981). Sir Hugh is said to have missed the Inaccessible Pinnacle of Sg\u00f9rr Dearg, on the Isle of Skye, which he never climbed. However the \"In Pinn\", as it is known colloquially within Scottish mountaineering, was only listed as a Munro Top on his list (despite being several metres higher than Sg\u00f9rr Dearg, which was listed as the main Munro Top).\nThe first \"completionist\" was to be the Reverend A. E. Robertson, in 1901, later minister at Braes of Rannoch from 1907. However, research has cast doubt on this claim, and it is not certain that he reached the summit of Ben Wyvis. Also it is known that Robertson did not climb the Inaccessible Peak of Sg\u00f9rr Dearg. If Robertson is discounted, the first Munroist is Ronald Burn, who completed in 1923. Burn is also (indisputably) the first person to climb all the Munro Tops.\nThe person with the most rounds of Munros is Steven Fallon from Edinburgh, who has completed 16 rounds as of 1 October 2019.\nChris Smith became the first Member of Parliament to complete the Munros when he reached the summit of Sg\u00f9rr nan Coireachan on 27 May 1989.\nBen Fleetwood is probably the youngest person to have completed a round. He climbed the final Munro of his round \u2013 Ben More \u2013 on 30 August 2011 at the age of 10 years and 3 months. The youngest completionist to have done the round without the presence of a parent or a guardian is probably Andy Nisbet, who finished his round in 1972 aged 18 years and 1 month.\nIn 2024 Anna Wells of Inverness became the first woman to reach the top of 282 Munros in one winter season.\nContinuous rounds.\nHamish Brown did the first continuous self-propelled round of the Munros (except for the Skye and Mull ferries) between 4 April and 24 July 1974 with of ascent and mostly walking \u2013 just were on a bicycle. The journey is fully documented in his book \"Hamish's Mountain Walk\". The average time taken to bag all the Munros is eight years.\nIn 1984 George Keeping accomplished the first continuous round of the Munros entirely on foot (and ferry) in 135 days. He went on to complete the English and Welsh 3,000-foot peaks in a further 29 days.\nThe first reported completion of all the Munros plus the Munro Tops in one continuous expedition was by Chris Townsend in 1996. His trip lasted between 18 May and 12 September (118 days), he covered a distance of ( by bicycle) with of ascent. The round was broken twice for spells at the office, which could be regarded as stretching the meaning of \"continuous\".\nThe first person to complete a winter round (all the Munros in one winter season) was Martin Moran in 1984\u201385. His journey lasted between 21 December 1984 and 13 March 1985 (83 days), he walked with of ascent. He used motor transport (campervan) to link his walk.\nIn the winter of 2005\u201306, Steve Perry completed a continuous unsupported round entirely on foot (and ferry). He is also the first person to have completed two continuous Munro rounds, having also walked Land's End to John O'Groats via every mainland 3,000\u00a0ft mountain between 18 February 2003 and 30 September 2003.\nFastest rounds.\nIn 1990, international fell runner and maths teacher Hugh Symonds of Sedbergh, Yorkshire, ran all 277 Munros starting from Ben Hope. It took him 66 days and 22 hours. This also included running the other 3,000 foot peaks in Great Britain. Having achieved this in the short time of 83 days, when his target had been a hundred, he decided to add the Republic of Ireland tops to the list and still finished all 303 peaks in 97 days.\nIn July 1992, Andrew Johnstone of Aberdeen and Rory Gibson of Edinburgh completed their mountain triathlon across the Munros, the 277 Scottish peaks over 3,000\u00a0ft, beating the existing record by five days. They began on 29 May and finished at 8.30pm on 15 July on the summit of Ben Hope, the most northerly Munro, completing a journey which began 51 days and 10 hours earlier on the Isle of Mull. After swimming lochs, cycling highland roads and running across some of the most desolate and dangerous terrain in Britain, they covered 1,400 miles.\nCharlie Campbell, a former postman from Glasgow, held the record for the fastest round of the Munros between 2000 and 2010. He completed his round in 48 days, 12 hours and 0 minutes, finishing on 16 July 2000, on Ben Hope. He cycled and swam between Munros; no motorised transport was used.\nCampbell's record was broken by Stephen Pyke of Stone, Staffordshire, in 2010 who completed the round in 39 days, 9 hours and 6 minutes. Pyke's round started on the Isle of Mull on 25 April 2010 and finished on Ben Hope in Sutherland on 3 June 2010. He cycled and kayaked between Munros; no motorised transport was used. He was backed by a support team in a motor home, but had to camp out in the more remote areas.\nOn 18 September 2011, Alex Robinson and Tom O'Connell finished a self-propelled continuous round on Ben Hope in a time of 48 days, 6 hours and 56 minutes. At the age of just 21, Alex became the youngest person to have completed a continuous round without the use of any motorised transport.\nOn 17 September 2017, the women's self-propelled, continuous record was broken by Libby Kerr and Lisa Trollope in 76 days and 10 hours. This record would later be vastly broken by Jamie Aarons on 26 June 2023 who would also break the record for both the male and female fastest ever round.\nOn 2 September 2020, Pyke's record was broken by Donnie Campbell of Inverness. He completed his round in 31 days, 23 hours and 2 minutes, starting on the Isle of Mull on 1 August 2020 and finished on Ben Hope on 2 September 2020. Campbell ran the 282 Munros and cycled and kayaked between them. On day 29, he was joined by previous record holder Stephen Pyke. Whilst ticking off M\u00f2ruisg in the cloud, he mistook the big cairn for the summit and had to head back up and so climbed the Munro twice. On day 31, he completed 18 Munros. He was supported by a crew travelling in his motorhome, who also shuttled his bike for him to follow a more linear route.\nOn 26 June 2023, Jamie Aarons of California broke the previous record of fastest ever round held by former marine Donnie Campbell by more than 12 hours by completing a self-propelled continuous round in 31 days, 10 hours and 27 minutes. She also ran, cycled and kayaked between each of the Munros, covering a total of around 932 miles (1,500 km) on foot and about the same distance by bike. She began at Ben More on Mull and ended at Ben Klibreck in Sutherland, raising \u00a314,000 for World Bicycle Relief.\nFurths.\nThe SMC recognises six peaks in England, fifteen in Wales and thirteen in Ireland that would be Munros or Munro Tops if they were in Scotland. These are referred to as Furth Munros, i.e. the Munros furth of Scotland. The first recorded Furthist is James Parker, who completed on Tryfan (Snowdonia) on 19 April 1929.\nReferences.\nCitations.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "40316", "revid": "42430230", "url": "https://en.wikipedia.org/wiki?curid=40316", "title": "Kurdish language", "text": "Northwestern Iranian dialect continuum\n&lt;templatestyles src=\"Template:Infobox/styles-images.css\" /&gt;\nKurdish (, , ) is a Northwestern Iranian language or group of languages spoken by Kurds in the region of Kurdistan, namely in southeast Turkey, northern Iraq, northwest Iran, and northern Syria. It is also spoken in northeast Iran, as well as in certain areas of Armenia and Azerbaijan. \nKurdish varieties constitute a dialect continuum, with some mutually unintelligible varieties, and collectively have 26\u00a0million native speakers. The main varieties of Kurdish are Kurmanji, Sorani, and Southern Kurdish (). The majority of the Kurds speak Kurmanji, and most Kurdish texts are written in Kurmanji and Sorani. Kurmanji is written in the Hawar alphabet, a derivation of the Latin script, and Sorani is written in the Sorani alphabet, a derivation of the Arabic script.\nA separate group of non-Kurdish Northwestern Iranian languages, the Zaza\u2013Gorani languages, are also spoken by several million ethnic Kurds.\nThe literary output in Kurdish was mostly confined to poetry until the early 20th century, when more general literature became developed. Today, the two principal written Kurdish dialects are Kurmanji and Sorani. Sorani is, along with Arabic, one of the two official languages of Iraq and is in political documents simply referred to as \"Kurdish\".\nClassification and origin.\nThe Kurdish varieties belong to the Iranian branch of the Indo-European family. They are generally classified as Northwestern Iranian languages, or by some scholars as intermediate between Northwestern and Southwestern Iranian.587 Martin van Bruinessen notes that \"Kurdish has a strong South-Western Iranian element\", whereas \"Zaza and Gurani [...] do belong to the north-west Iranian group\".\nLudwig Paul concludes that Kurdish seems to be a Northwestern Iranian language in origin, but acknowledges that it shares many traits with Southwestern Iranian languages like Persian, apparently due to longstanding and intense historical contacts.\nWindfuhr identified Kurdish dialects as Parthian, albeit with a Median substratum. Windfuhr and Frye assume an eastern origin for Kurdish and consider it as related to eastern and central Iranian dialects.\nThe present state of knowledge about Kurdish allows, at least roughly, drawing the approximate borders of the areas where the main ethnic core of the speakers of the contemporary Kurdish dialects was formed. The most argued hypothesis on the localisation of the ethnic territory of the Kurds remains D.N. Mackenzie's theory, proposed in the early 1960s (Mackenzie 1961). Developing the ideas of P. Tedesco (1921: 255) and regarding the common phonetic isoglosses shared by Kurdish, Persian, and Baluchi, Mackenzie concluded that the speakers of these three languages may once have been in closer contact.\nVarieties.\nKurdish varieties are divided into three or four groups, with varying degrees of mutual intelligibility.\nIn historical evolution terms, Kurmanji is less modified than Sorani and Pehlewani in both phonetic and morphological structure. The Sorani group has been influenced by among other things its closer cultural proximity to the other languages spoken by Kurds in the region including the Gorani language in parts of Iranian Kurdistan and Iraqi Kurdistan.\nPhilip G. Kreyenbroek, an expert writing in 1992, says:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;Since 1932 most Kurds have used the Roman script to write Kurmanji... Sorani is normally written in an adapted form of the Arabic script... Reasons for describing Kurmanji and Sorani as 'dialects' of one language are their common origin and the fact that this usage reflects the sense of ethnic identity and unity among the Kurds. From a linguistic or at least a grammatical point of view, however, Kurmanji and Sorani differ as much from each other as English and German, and it would seem appropriate to refer to them as languages. For example, Sorani has neither gender nor case-endings, whereas Kurmanji has both... Differences in vocabulary and pronunciation are not as great as between German and English, but they are still considerable.\nAccording to \"Encyclopaedia of Islam\", although Kurdish is not a unified language, its many dialects are interrelated and at the same time distinguishable from other Western Iranian languages. The same source classifies different Kurdish dialects as two main groups, northern and central. The average Kurmanji speaker does not find it easy to communicate with the inhabitants of Sulaymaniyah or Halabja.\nThe Mokriani variety of Sorani is widely spoken in Mokrian. Piranshahr and Mahabad are two principal cities of the Mokrian area.\nZazaki and Gorani.\nZaza\u2013Gorani languages, which are spoken by communities in the wider area who identify as ethnic Kurds, are not linguistically classified as Kurdish. Zaza-Gorani is classified as adjunct to Kurdish, although authorities differ in the details.589 groups Kurdish with Zaza Gorani within a \"Northwestern I\" group, while Glottolog based on \"Encyclop\u00e6dia Iranica\" prefers an areal grouping of \"Central dialects\" (or \"Kermanic\") within Northwest Iranic, with Kurdish but not Zaza-Gorani grouped with \"Kermanic\".\nGorani is distinct from Northern and Central Kurdish, yet shares vocabulary with both of them and there are some grammatical similarities with Central Kurdish. The Hawrami dialects of Gorani includes a variety that was an important literary language since the 14th century, but it was replaced by Central Kurdish in the 20th century.\nEuropean scholars have maintained that Gorani is separate from Kurdish and that Kurdish is synonymous with the Northern Kurdish group, whereas ethnic Kurds maintain that Kurdish encompasses any of the unique languages or dialects spoken by Kurds that are not spoken by neighbouring ethnic groups.\nGorani is classified as part of the Zaza\u2013Gorani branch of Indo-Iranian languages. The Zaza language, spoken mainly in Turkey, differs both grammatically and in vocabulary and is generally not understandable by Gorani speakers but it is considered related to Gorani. Almost all Zaza-speaking communities, as well as speakers of the closely related Shabaki dialect spoken in parts of Iraqi Kurdistan, identify themselves as ethnic Kurds.\nAccording to M. Leezenberg, the term \"Kurdish dialect\" could refer to either a \"dialect of the Kurdish branch of Northwestern Indo-Iranian languages\" (such as Kurmanji and Sorani), or a \"dialect spoken by people who consider themselves Kurds\" (such as Zaza and Gorani). He added that the situation of Zazas, who overwhelmingly identified as Kurds, and Goranis, who were certainly Kurds and identified as such, was opposite from the situation of Yazidis, who spoke a Kurdish language but for religious reasons often did not identify as Kurds and were not seen as Kurds by other Kurds until the rise of secular nationalism.\nGeoffrey Haig and Ergin \u00d6pengin in their recent study suggest grouping the Kurdish languages into Northern Kurdish, Central Kurdish, Southern Kurdish, Zaza, and Gorani, and avoid the subgrouping Zaza\u2013Gorani.\nProfessor Zare Yusupova has carried out much work and research into the Gorani dialect (as well as many other minority/ancient Kurdish dialects).\nHistory.\nDuring his stay in Damascus, historian Ibn Wahshiyya came across two books on agriculture written in Kurdish, one on the culture of the vine and the palm tree, and the other on water and the means of finding it out in unknown ground. He translated both from Kurdish into Arabic in the early 9th century AD.\nAmong the earliest Kurdish religious texts is the \"Yazidi Black Book\", the sacred book of Yazidi faith. It is considered to have been authored sometime in the 13th century AD by \"Hassan bin Adi\" (b. 1195 AD), the great-grandnephew of Sheikh Adi ibn Musafir (d. 1162), the founder of the faith. It contains the Yazidi account of the creation of the world, the origin of man, the story of Adam and Eve and the major prohibitions of the faith. According to \"The Cambridge History of the Kurds\", \"the first proper 'text'\" written in Kurdish is a short Christian prayer. It was written in Armenian characters, and dates from the fifteenth century. From the 15th to 17th centuries, classical Kurdish poets and writers developed a literary language. The most notable classical Kurdish poets from this period were Ali Hariri, Ahmad Khani, Malaye Jaziri and Faqi Tayran.\nThe Italian priest Maurizio Garzoni published the first Kurdish grammar titled \"Grammatica e Vocabolario della Lingua Kurda\" in Rome in 1787 after eighteen years of missionary work among the Kurds of Amadiya. This work is very important in Kurdish history as it is the first acknowledgment of the widespread use of a distinctive Kurdish language. Garzoni was given the title \"Father of Kurdology\" by later scholars. The Kurdish language was banned in a large portion of Kurdistan for some time. After the 1980 Turkish coup d'\u00e9tat until 1991 the use of the Kurdish language was illegal in Turkey.\nCurrent status.\nToday, Sorani is an official language in Iraq. In Syria, on the other hand, publishing materials in Kurdish is forbidden, though this prohibition is not enforced any more due to the Syrian civil war.\nBefore August 2002, the Turkish government placed severe restrictions on the use of Kurdish, prohibiting the language in education and broadcast media. In March 2006, Turkey allowed private television channels to begin airing programming in Kurdish. However, the Turkish government said that they must avoid showing children's cartoons, or educational programs that teach Kurdish, and could broadcast only for 45 minutes a day or four hours a week. The state-run Turkish Radio and Television Corporation (TRT) started its 24-hour Kurdish television station on 1 January 2009 with the motto \"we live under the same sky\". The Turkish prime minister sent a video message in Kurdish to the opening ceremony, which was attended by Minister of Culture and other state officials. The channel uses the \"X\", \"W\", and \"Q\" letters during broadcasting. However, most of these restrictions on private Kurdish television channels were relaxed in September 2009. In 2010, Kurdish municipalities in the southeast began printing marriage certificates, water bills, construction and road signs, as well as emergency, social and cultural notices in Kurdish alongside Turkish. Also Imams began to deliver Friday sermons in Kurdish and Esnaf price tags in Kurdish. Many mayors were tried for issuing public documents in Kurdish language. The Kurdish alphabet is not recognized in Turkey, and prior to 2013 the use of Kurdish names containing the letters \"X\", \"W\", and \"Q\", which do not exist in the Turkish alphabet, was not allowed. In 2012, Kurdish-language lessons became an elective subject in public schools. Previously, Kurdish education had only been possible in private institutions.\nIn Iran, though it is used in some local media and newspapers, it is not used in public schools. In 2005, 80 Syrian and Iranian Kurds took part in an experiment and gained scholarships to study in Kurdistan Region, Iraq, in their native tongue.\nIn Kyrgyzstan, of the Kurdish population speak Kurdish as their native language. In Kazakhstan, the corresponding percentage is 88.7%.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "40317", "revid": "122189", "url": "https://en.wikipedia.org/wiki?curid=40317", "title": "UTF-16", "text": "Variable-width encoding of Unicode, using one or two 16-bit code units\n&lt;templatestyles src=\"Mono/styles.css\" /&gt;\nUTF-16 (16-bit Unicode Transformation Format) is a character encoding that supports all 1,112,064 valid code points of Unicode. The encoding is variable-length as code points are encoded with one or two 16-bit \"code units\". UTF-16 arose from an earlier obsolete fixed-width 16-bit encoding now known as UCS-2 (for 2-byte Universal Character Set), once it became clear that more than 216 (65,536) code points were needed,&lt;ref name=\"Unicode.org/faq\"&gt;&lt;/ref&gt; including most emoji and important CJK characters such as for personal and place names.\nUTF-16 is used by the Windows API, and by many programming environments such as Java and Qt. The variable-length character of UTF-16, combined with the fact that most characters are \"not\" variable-length (so variable length is rarely tested), has led to many bugs in software, including in Windows itself.\nUTF-16 is the only encoding (still) allowed on the web that is incompatible with 8-bit ASCII. It has never gained popularity on the web, where it is declared by under 0.004% of public web pages (and even then, the web pages are most likely also using UTF-8). UTF-8, by comparison, gained dominance years ago and accounted for 99% of all web pages by 2025. The Web Hypertext Application Technology Working Group (WHATWG) considers UTF-8 \"the mandatory encoding for all [text]\" and that for security reasons browser applications should not use UTF-16.\nHistory.\nIn the late 1980s, work began on developing a uniform encoding for a \"Universal Character Set\" (UCS) that would replace earlier language-specific encodings with one coordinated system. The goal was to include all required characters from most of the world's languages, as well as symbols from technical domains such as science, mathematics, and music. The original idea was to replace the typical 256-character encodings, which required 1 byte per character, with an encoding using 65,536 (216) values, which would require 2 bytes (16 bits) per character.\nTwo groups worked on this in parallel, ISO/IEC JTC 1/SC 2 and the Unicode Consortium, the latter representing mostly manufacturers of computing equipment. The two groups attempted to synchronize their character assignments so that the developing encodings would be mutually compatible. The early 2-byte encoding was called \"UCS-2\".\nWhen it became increasingly clear that 216 characters would not suffice,&lt;ref name=\"unicode.org/faq\"&gt;&lt;/ref&gt; IEEE introduced a larger 31-bit space and an encoding (UCS-4) that would require 4 bytes per character. This was resisted by the Unicode Consortium, both because 4 bytes per character wasted a lot of memory and disk space, and because some manufacturers were already heavily invested in 2-byte-per-character technology. The UTF-16 encoding scheme was developed as a compromise and introduced with version 2.0 of the Unicode standard in July 1996. It is fully specified in RFC 2781, published in 2000 by the IETF.\nUTF-16 is specified in the latest versions of both the international standard ISO/IEC 10646 and the Unicode Standard. \"UCS-2 should now be considered obsolete. It no longer refers to an encoding form in either 10646 or the Unicode Standard.\" UTF-16 will never be extended to support a larger number of code points or to support the code points that were replaced by surrogates, as this would violate the Unicode Stability Policy with respect to general category or surrogate code points. (Any scheme that remains a self-synchronizing code would require allocating at least one Basic Multilingual Plane (BMP) code point to start a sequence. Changing the purpose of a code point is disallowed.)\nDescription.\nEach Unicode \"code point\" is encoded either as one or two 16-bit \"code units\". Code points less than 216 (\"in the BMP\") are encoded with a single 16-bit code unit equal to the numerical value of the code point, as in the older UCS-2. Code points greater than or equal to 216 (\"above the BMP\") are encoded using \"two\" 16-bit code units. These two 16-bit code units are chosen from the UTF-16 surrogate range 0xD800\u20130xDFFF which had not previously been assigned to characters. Values in this range are not used as characters, and UTF-16 provides no legal way to code them as individual code points. A UTF-16 stream, therefore, consists of single 16-bit codes outside the surrogate range, and pairs of 16-bit values that are within the surrogate range.\nU+0000 to U+D7FF and U+E000 to U+FFFF.\nBoth UTF-16 and UCS-2 encode code points in this range as single 16-bit code units that are numerically equal to the corresponding code points. These code points in the Basic Multilingual Plane (BMP) are the \"only\" code points that can be represented in UCS-2. As of Unicode 9.0, some modern non-Latin Asian, Middle-Eastern, and African scripts fall outside this range, as do most emoji characters.\nCode points from U+010000 to U+10FFFF.\nCode points from the other planes are encoded as two 16-bit \"code units\" called a \"surrogate pair\". The first code unit is a \"high surrogate\" and the second is a \"low surrogate\" (These are also known as \"leading\" and \"trailing\" surrogates, respectively, analogous to the leading and trailing bytes of UTF-8.):\nIllustrated visually, the distribution of \"U\"' between \"W1\" and \"W2\" looks like:\nU' = yyyyyyyyyyxxxxxxxxxx // U - 0x10000\nW1 = 110110yyyyyyyyyy // 0xD800 + yyyyyyyyyy\nW2 = 110111xxxxxxxxxx // 0xDC00 + xxxxxxxxxx\nSince the ranges for the \"high surrogates\" (0xD800\u20130xDBFF), \"low surrogates\" (0xDC00\u20130xDFFF), and valid BMP characters (0x0000\u20130xD7FF, 0xE000\u20130xFFFF) are disjoint, it is not possible for a surrogate to match a BMP character, or for two adjacent \"code units\" to look like a legal \"surrogate pair\". This simplifies searches a great deal. It also means that UTF-16 is \"self-synchronizing\" on 16-bit words: whether a code unit starts a character can be determined without examining earlier code units (i.e. the type of \"code unit\" can be determined by the ranges of values in which it falls). UTF-8 shares these advantages, but many earlier multi-byte encoding schemes (such as Shift JIS and other Asian multi-byte encodings) did not allow unambiguous searching and could only be synchronized by re-parsing from the start of the string. UTF-16 is not self-synchronizing if one byte is lost or if traversal starts at a random byte.\nBecause the most commonly used characters are all in the BMP, handling of surrogate pairs is often not thoroughly tested. This leads to persistent bugs and potential security holes, even in popular and well-reviewed application software (e.g. CVE-).\nU+D800 to U+DFFF (surrogates).\nThe official Unicode standard says that no UTF forms, including UTF-16, can encode the surrogate code points. Since these will never be assigned a character, there should be no reason to encode them. However, Windows allows unpaired surrogates in filenames and other places, which generally means they have to be supported by software in spite of their exclusion from the Unicode standard.\nUCS-2, UTF-8, and UTF-32 can encode these code points in trivial and obvious ways, and a large amount of software does so, even though the standard states that such arrangements should be treated as encoding errors. It is possible to unambiguously encode an \"unpaired surrogate\" (a high surrogate code point not followed by a low one, or a low one not preceded by a high one) in the format of UTF-16 by using a code unit equal to the code point. The result is not valid UTF-16, but the majority of UTF-16 encoder and decoder implementations do this when translating between encodings.\nExamples.\nTo encode U+10437 (\ud801\udc37) to UTF-16:\nTo decode U+10437 (\ud801\udc37) from UTF-16:\nThe following table summarizes this conversion, as well as others. The colors indicate how bits from the code point are distributed among the UTF-16 bytes. Additional bits added by the UTF-16 encoding process are shown in black.\nByte-order encoding schemes.\nUTF-16 and UCS-2 produce a sequence of 16-bit code units. Since most communication and storage protocols are defined for bytes, and each unit thus takes two 8-bit bytes, the order of the bytes may depend on the endianness (byte order) of the computer architecture.\nTo assist in recognizing the byte order of code units, UTF-16 allows a byte order mark (BOM), a code point with the value U+FEFF, to precede the first actual coded value. (U+FEFF is the invisible zero-width non-breaking space/ZWNBSP character). If the endian architecture of the decoder matches that of the encoder, the decoder detects the 0xFEFF value, but an opposite-endian decoder interprets the BOM as the noncharacter value U+FFFE reserved for this purpose. This incorrect result provides a hint to perform byte-swapping for the remaining values.\nIf the BOM is missing, RFC 2781 recommends that big-endian (BE) encoding be assumed. In practice, due to Windows using little-endian (LE) order by default, many applications assume little-endian encoding. It is also reliable to detect endianness by looking for null bytes, on the assumption that characters less than U+0100 are very common. If more even bytes (starting at 0) are null, then it is big-endian.\nThe standard also allows the byte order to be stated explicitly by specifying UTF-16BE or UTF-16LE as the encoding type. When the byte order is specified explicitly this way, a BOM is specifically \"not\" supposed to be prepended to the text, and a U+FEFF at the beginning should be handled as a ZWNBSP character. Most applications ignore a BOM in all cases despite this rule.\nFor Internet protocols, IANA has approved \"UTF-16\", \"UTF-16BE\", and \"UTF-16LE\" as the names for these encodings (the names are case insensitive). The aliases UTF_16 or UTF16 may be meaningful in some programming languages or software applications, but they are not standard names in Internet protocols.\nSimilar designations, UCS-2BE and UCS-2LE, are used to show versions of UCS-2.\nEfficiency.\nA \"character\" may use any number of Unicode code points and in UTF-16 a code point could use either 1 or 2 16-bit values. This means that UTF-16 in no way assists in \"counting characters\" or in \"measuring the width/length of a string\".\nUTF-16 is often claimed to be more space-efficient than UTF-8 for East Asian languages, since it uses two bytes for characters that take 3 bytes in UTF-8. Since real text contains many spaces, numbers, punctuation, markup (for e.g. web pages), and control characters, which take only one byte in UTF-8, this is only true for artificially constructed dense blocks of text. A more serious claim can be made for Devanagari and Bengali, which use multi-letter words and all the letters take 3 bytes in UTF-8 and only 2 in UTF-16. In addition the Chinese Unicode encoding standard GB 18030 always produces files the same size or smaller than UTF-16 for all languages, not just for Chinese (it does this by sacrificing self-synchronization).\nUsage.\nA method to determine what encoding a system is using internally is to ask for the \"length\" of string containing a single non-BMP character. If the length is 2 then UTF-16 is being used. 4 indicates UTF-8. 3 or 6 may indicate CESU-8. 1 \"may\" indicate UTF-32, but more likely indicates the language decodes the string to code points before measuring the \"length\".\nOperating systems.\nUTF-16 is used for text in the OS\u00a0API of all currently supported versions of Microsoft Windows (and including at least Windows CE since Windows CE 5.0 and Windows NT since Windows 2000). Windows NT prior to Windows 2000 only supported UCS-2. Windows 9x only supported UCS-2, and support of Unicode is limited to internal, such as VFAT and WDM. Since Windows 10 version 1903 (or insider build 17035) it has been possible to use UTF-8 in the API, though most software, such as Windows File Explorer, still uses UTF-16 API. Microsoft has stated that \"UTF-16 [..] is a unique burden that Windows places on code that targets multiple platforms\"\nThe IBM i operating system designates CCSID (code page) 13488 for UCS-2 encoding and CCSID 1200 for UTF-16 encoding, though the system treats them both as UTF-16.\nUTF-16 is used by the Qualcomm BREW operating systems; the .NET environments; and the Qt cross-platform graphical widget toolkit.\nFile systems.\nThe Joliet file system, used in CD-ROM media, encodes file names using UCS-2BE (up to sixty-four Unicode characters per file name). NTFS and ReFS use UTF-16 to store strings.\nMessaging.\nSMS text messaging effectively uses UTF-16. The 3GPP TS 23.038 (GSM) and IS-637 (CDMA) standards specify UCS-2, but UTF-16 is necessary for Emoji to work. Symbian OS used in Nokia S60 handsets and Sony Ericsson UIQ handsets uses UCS-2. iPhone handsets use UTF-16.\nProgramming languages.\nPython version 2.0 officially only used UCS-2 internally, but the UTF-8 decoder to \"Unicode\" produced correct UTF-16. There was also the ability to compile Python so that it used UTF-32 internally, this was sometimes done on Unix. Python 3.3 switched internal storage to use one of ISO-8859-1, UCS-2, or UTF-32 depending on the largest code point in the string. Python 3.12 drops some functionality (for CPython extensions) to make it easier to migrate to UTF-8 for all strings.\nJava originally used UCS-2, and added UTF-16 supplementary character support in J2SE 5.0. All strings in memory are UTF-16 (since Java 9, strings containing only ISO-8859-1 characters can be \"compressed\" to bytes). Java I/O uses UTF-8 or Modified UTF-8.\nJavaScript may use UCS-2 or UTF-16. As of ES2015, string methods and regular expression flags have been added to the language that permit handling strings from an encoding-agnostic perspective.\nSwift, Apple's preferred application language, used UTF-16 to store strings until version 5 which switched to UTF-8.\nQuite a few languages make the encoding part of the string object, and thus store and support a large set of encodings including UTF-16. Most consider UTF-16 and UCS-2 to be different encodings. Examples are the PHP language and MySQL.\nFirmware.\nUEFI uses UTF-16 to encode strings by default.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "40318", "revid": "40982560", "url": "https://en.wikipedia.org/wiki?curid=40318", "title": "Portland cement", "text": "Binder used as basic ingredient of concrete\nPortland cement is the most common type of cement in general use around the world as a basic ingredient of concrete, mortar, stucco, and non-specialty grout. It was developed from other types of hydraulic lime in England in the early 19th century by Joseph Aspdin, and is usually made from limestone. It is a fine powder, produced by heating limestone and clay minerals in a kiln to form clinker, and then grinding the clinker with the addition of several percent (often around 5%) gypsum. Several types of Portland cement are available. The most common, historically called ordinary Portland cement (OPC), is grey, but white Portland cement is also available.\nThe cement was so named by Joseph Aspdin, who obtained a patent for it in 1824, because, once hardened, it resembled the fine, pale limestone known as Portland stone, quarried from the windswept cliffs of the Isle of Portland in Dorset. Portland stone was prized for centuries in British architecture and used in iconic structures such as St Paul's Cathedral and the British Museum.\nHis son William Aspdin is regarded as the inventor of \"modern\" Portland cement due to his developments in the 1840s.\nThe low cost and widespread availability of the limestone, shales, and other naturally occurring materials used in Portland cement make it a relatively cheap building material. At 4.4 billion tons manufactured (in 2023), Portland cement ranks third in the list (by mass) of manufactured materials, outranked only by sand and gravel. These two are combined, with water, to make the most manufactured material, concrete. This is Portland cement's most common use.\nHistory.\nPortland cement was developed from natural cements made in Britain beginning in the middle of the 18th century. Its name is derived from its similarity to Portland stone, a type of building stone quarried on the Isle of Portland in Dorset, England. The development of modern Portland cement (sometimes called ordinary or normal Portland cement) began in 1756, when John Smeaton experimented with combinations of different limestones and additives, including trass and pozzolanas, intended for the construction of a lighthouse, now known as Smeaton's Tower. In the late 18th century, Roman cement was developed and patented in 1796 by James Parker. Roman cement quickly became popular, but was largely replaced by Portland cement in the 1850s. In 1811, James Frost produced a cement he called British cement. James Frost is reported to have erected a manufactory for making of an artificial cement in 1826. In 1811 Edgar Dobbs of Southwark patented a cement of the kind invented 7 years later by the French engineer Louis Vicat. Vicat's cement is an artificial hydraulic lime, and is considered the \"principal forerunner\" of Portland cement.\nThe name \"Portland cement\" is recorded in a directory published in 1823 being associated with a William Lockwood and possibly others. In his 1824 cement patent, Joseph Aspdin called his invention \"Portland cement\" because of its resemblance to Portland stone. Aspdin's cement was nothing like modern Portland cement, but a first step in the development of modern Portland cement, and has been called a \"proto-Portland cement\".\nWilliam Aspdin had left his father's company, to form his own cement manufactury. In the 1840s William, apparently accidentally, produced calcium silicates which are a middle step in the development of Portland cement. In 1843, he set up a manufacturing plant at Rotherhithe, southeast London, where he was soon making a cement that caused a sensation among users in London. In 1848, William further improved his cement. Then, in 1853, he moved to Germany, where he was involved in cement making. William made what could be called \"meso-Portland cement\" (a mix of Portland cement and hydraulic lime). Isaac Charles Johnson further refined the production of \"meso-Portland cement\" (middle stage of development), and claimed to be the real father of Portland cement.\nIn 1859, John Grant of the Metropolitan Board of Works, set out requirements for cement to be used in the London sewer project. This became a specification for Portland cement. The next development in the manufacture of Portland cement was the introduction of the rotary kiln, patented by Frederick Ransome in 1885 (U.K.) and 1886 (U.S.); which allowed a stronger, more homogeneous mixture and a continuous manufacturing process. The Hoffmann \"endless\" kiln which was said to give \"perfect control over combustion\" was tested in 1860 and shown to produce a superior grade of cement. This cement was made at the Portland Cementfabrik Stern at Stettin, which was the first to use a Hoffmann kiln. The Association of German Cement Manufacturers issued a standard on Portland cement in 1878.\nPortland cement had been imported into the United States from England and Germany, and in the 1870s and 1880s, it was being produced by Eagle Portland cement near Kalamazoo, Michigan. In 1875, the first Portland cement was produced in the Coplay Cement Company Kilns under the direction of David O. Saylor in Coplay, Pennsylvania, United States. By the early 20th century, American-made Portland cement had displaced most of the imported Portland cement.\nComposition.\nASTM C219 defines Portland cement as:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;a hydraulic cement produced by pulverizing clinker, consisting essentially of crystalline hydraulic calcium silicates, and usually containing one or more of the following: water, calcium sulfate, up to 5\u2009% limestone, and processing additions The European Standard EN 197-1 uses the following definition:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;Portland cement clinker is a hydraulic material which shall consist of at least two-thirds by mass of calcium silicates, (3\u00a0CaO\u00b7SiO2, and 2\u00a0CaO\u00b7SiO2), the remainder consisting of aluminium- and iron-containing clinker phases and other compounds. The ratio of CaO to SiO2 shall not be less than 2.0. The magnesium oxide content (MgO) shall not exceed 5.0% by mass.\n(The last two requirements were already set out in the German Standard, issued in 1909).\nClinkers make up more than 90% of the cement, along with a limited amount of calcium sulphate (CaSO4, which controls the set time), and up to 5% minor constituents (fillers) as allowed by various standards. Clinkers are nodules (diameters, ) of a sintered material that is produced when a raw mixture of predetermined composition is heated to high temperature. The key chemical reaction distinguishing Portland cement from other hydraulic limes occurs at these high temperatures (&gt;) as belite (Ca2SiO4) combines with calcium oxide (CaO) to form alite (Ca3SiO5).\nManufacturing.\nPortland cement clinker is made by heating, in a cement kiln, a mixture of raw materials to a calcining temperature of above and then a fusion temperature, which is about for modern cements, to sinter the materials into clinker.\nThe four mineral phases present in the cement clinker are alite (), belite (), tricalcium aluminate () and tetracalcium alumino ferrite (). The aluminium, iron and magnesium oxides are present as a flux allowing the calcium silicates to form at a lower temperature, and contribute little to the strength. For special cements, such as low heat (LH) and sulphate resistant (SR) types, it is necessary to limit the amount of tricalcium aluminate (: 3\u00a0CaO\u00b7Al2O3) formed.\nThe major raw material for the clinker-making is usually limestone (CaCO3) mixed with a second material containing clay as a source of alumino-silicate. Normally, an impure limestone which contains clay or SiO2 is used. The CaCO3 content of these limestones can be as low as 80\u00a0%. Secondary raw materials (materials in the raw mix other than limestone) depend on the purity of the limestone. Some of the materials used are clay, shale, sand, iron ore, bauxite, fly ash, and slag. When a cement kiln is fired with coal, the coal ash acts as a secondary raw material.\nCement grinding.\nTo achieve the desired setting qualities in the finished product, a quantity (2\u20138\u00a0%, but typically 5%) of calcium sulphate (usually gypsum or anhydrite) is added to the clinker, and the mixture is finely ground to form the finished cement powder. This is achieved in a cement mill. The grinding process is controlled to obtain a powder with a broad particle size range, in which typically 15\u00a0% by mass consists of particles below 5\u00a0\u03bcm diameter, and 5\u00a0% of particles above 45\u00a0\u03bcm. The measure of fineness usually used is the 'specific surface area', which is the total particle surface area of a unit mass of cement. The rate of initial reaction (up to 24\u00a0hours) of the cement on the addition of water is directly proportional to the specific surface area. Typical values are 320\u2013380\u00a0m2\u00b7kg\u22121 for general purpose cements, and 450\u2013650\u00a0m2\u00b7kg\u22121 for 'rapid hardening' cements. The cement is conveyed by belt or powder pump to a silo for storage. Cement plants normally have sufficient silo space for one to 20\u00a0weeks of production, depending upon local demand cycles. The cement is delivered to end users either in bags or as bulk powder blown from a pressure vehicle into the customer's silo. In industrial countries, 80\u00a0% or more of cement is delivered in bulk.\nSetting and hardening.\nCement sets when mixed with water by way of a complex series of chemical reactions that are still only partly understood. A brief summary is as follows:\nThe clinker phases\u2014calcium silicates and aluminates\u2014dissolve into the water that is mixed with the cement, which results in a fluid containing relatively high concentrations of dissolved ions. This reaches supersaturation with respect to specific mineral phases: usually first ettringite, and then calcium silicate hydrate (C-S-H) which precipitate as newly formed solids. The interlocking of the C-S-H (which is crystallographically disordered, and can take on needle or crumpled-foil morphologies) and the ettringite crystals gives cement its initial setting, converting the fluid into a solid, and chemically incorporating much of the water into these new phases.\nGypsum is included in the cement as an inhibitor to prevent flash (or quick) setting; if gypsum is not present, the initial formation of (needle-shaped) ettringite is not possible, and so (plate-shaped) hydrocalumite-group (\"AFm\") calcium aluminate phases form instead. The premature formation of AFm phases causes a rapid loss of flowability, which is generally undesirable because it renders the placement of the cement or concrete very difficult. \nHardening of the cement then proceeds through further C-S-H formation, as this fills in the spaces between the (still-dissolving) cement grains with newly formed solid phases. Portlandite also precipitates from the pore solution to form part of the solid microstructure. Some of the initially formed ettringite may be converted to AFm phases, releasing part of the sulfate from its structure to continue reacting with any remaining tricalcium aluminate\nGas diffusion and radon permeability.\nOrdinary Portland Cement (OPC) concretes can act as barriers to gases such as radon, with their effectiveness depending on the thickness of the concrete and its diffusion properties. Direct measurement of the radon diffusion coefficient in cement is complex, but it can be estimated from the oxygen diffusion coefficient, which is more readily obtained experimentally. The relatively low permeability of OPC to radon makes it useful in constructions where mitigation of radon exposure is required, such as in residential or underground structures.\nUse.\nThe most common use for Portland cement is in the production of concrete. Concrete is a composite material consisting of aggregate (gravel and sand), cement, and water. As a construction material, concrete can be cast in almost any shape desired, and once hardened, can become a structural (load-bearing) element. Concrete can be used in the construction of structural elements like panels, beams, and street furniture, or may be cast-\"in situ\" for superstructures like roads and dams. These may be supplied with concrete mixed on site, or may be provided with 'ready-mixed' concrete made at permanent mixing sites. Portland cement is also used in mortars (with sand and water only), for plasters and screeds, and in grouts (cement/water mixes squeezed into gaps to consolidate foundations, road-beds, etc.).\nWhen water is mixed with Portland cement, the product sets in a few hours and hardens over several weeks. These processes can vary widely, depending upon the mix used and the conditions of curing of the product, but a typical concrete sets in about 6\u00a0hours and develops a compressive strength of 8\u00a0MPa in 24\u00a0hours. The strength rises to 15\u00a0MPa at 3\u00a0days, 23\u00a0MPa at 1\u00a0week, 35\u00a0MPa at 4\u00a0weeks, and 41\u00a0MPa at 3\u00a0months. In principle, the strength continues to rise slowly as long as water is available for continued hydration, but concrete is usually allowed to dry out after a few weeks, and this causes strength growth to stop.\nTypes.\nASTM C150.\nFive types of Portland cements exist, with variations of the first three according to ASTM C150.\nType I Portland cement is known as common or general-purpose cement. It is generally assumed unless another type is specified. It is commonly used for general construction, especially when making precast and precast-prestressed concrete that is not to be in contact with soils or groundwater. The typical compound compositions of this type are:\n 55% (C3S), 19% (C2S), 10% (C3A), 7% (C4AF), 2.8% MgO, 2.9% (SO3), 1.0% ignition loss, and 1.0% free CaO (utilizing cement chemist notation).\nA limitation on the composition is that the (C3A) shall not exceed 15%.\nType II provides moderate sulphate resistance, and gives off less heat during hydration. This type of cement costs about the same as type I. Its typical compound composition is:\n 51% (C3S), 24% (C2S), 6% (C3A), 11% (C4AF), 2.9% MgO, 2.5% (SO3), 0.8% ignition loss, and 1.0% free CaO.\nA limitation on the composition is that the (C3A) shall not exceed 8%, which reduces its vulnerability to sulphates. This type is for general construction exposed to moderate sulphate attack. It is intended for use when concrete is in contact with soils and groundwater, especially in the western United States due to the high sulphur content of the soils. Because of a similar price to that of type I, type II is used much as a general-purpose cement, and the majority of Portland cement sold in North America meets this specification.\n Note: Cement meeting (among others) the specifications for types I and II has become commonly available on the world market.\nType III has relatively high early strength. Its typical compound composition is:\n 57% (C3S), 19% (C2S), 10% (C3A), 7% (C4AF), 3.0% MgO, 3.1% (SO3), 0.9% ignition loss, and 1.3% free CaO.\nThis cement is similar to type I, but ground finer. Some manufacturers make a separate clinker with higher C3S and/or C3A content, but this is increasingly rare, and the general purpose clinker is usually used, ground to a specific surface area typically 50\u201380% higher. The gypsum level may also be increased a small amount. This gives the concrete using this type of cement a three-day compressive strength equal to the seven-day compressive strength of types I and II. Its seven-day compressive strength is almost equal to the 28-day compressive strengths of types I and II. The only downside is that the six-month strength of type III is the same or slightly less than that of types I and II. Therefore, the long-term strength is sacrificed. It is usually used for precast concrete manufacture, where high one-day strength allows fast turnover of molds. It may also be used in emergency construction and repairs, and the construction of machine bases and gate installations.\nType IV Portland cement is generally known for its low heat of hydration. Its typical compound composition is:\n 28% (C3S), 49% (C2S), 4% (C3A), 12% (C4AF), 1.8% MgO, 1.9% (SO3), 0.9% ignition loss, and 0.8% free CaO.\nThe percentages of (C2S) and (C4AF) are relatively high and (C3S) and (C3A) are relatively low. A limitation on this type is that the maximum percentage of (C3A) is seven, and the maximum percentage of (C3S) is thirty-five. This causes the heat given off by the hydration reaction to develop at a slower rate. Consequently, the strength of the concrete develops slowly. After one or two years, the strength is higher than that of the other types after full curing. This cement is used for huge concrete structures, such as dams, which have a low surface-to-volume ratio. This type of cement is generally not stocked by manufacturers, but some might consider a large special order. This type of cement has not been made for many years, because Portland-pozzolan cement and ground granulated blast furnace slag addition offer a cheaper and more reliable alternative.\nType V is used where sulphate resistance is important. Its typical compound composition is:\n 38% (C3S), 43% (C2S), 4% (C3A), 9% (C4AF), 1.9% MgO, 1.8% (SO3), 0.9% ignition loss, and 0.8% free CaO.\nThis cement has a very low (C3A) composition, which accounts for its high sulphate resistance. The maximum content of (C3A) allowed is 5% for type V Portland cement. Another limitation is that the (C4AF) + 2(C3A) composition cannot exceed 20%. This type is used in concrete to be exposed to alkali soil and ground water sulphates which react with (C3A), causing disruptive expansion. It is unavailable in many places, although its use is common in the western United States and Canada. As with type IV, type V Portland cement has largely been supplanted by the use of ordinary cement with added ground granulated blast furnace slag or tertiary blended cements containing slag and fly ash.\nTypes Ia, IIa, and IIIa have the same composition as types I, II, and III. The only difference is that in Ia, IIa, and IIIa, an air-entraining agent is ground into the mix. The air-entrainment must meet the minimum and maximum optional specifications found in the ASTM manual. These types are only available in the eastern United States and Canada, on a limited basis. They are a poor approach to air-entrainment, which improves resistance to freezing under low temperatures.\nTypes II(MH) and II(MH)a have a similar composition to types II and IIa, but with a mild heat.\nEN 197 norm.\nThe European norm EN 197-1 defines five classes of common cement that comprise Portland cement as a main constituent. These classes differ from the ASTM classes.\nCSA A3000-08.\nThe Canadian standards describe six main classes of cement, four of which can also be supplied as a blend containing ground limestone (denoted by a suffix 'L' in the class names).\nWhite Portland cement.\nWhite Portland cement or white ordinary Portland cement (WOPC) is similar to ordinary grey Portland cement in all respects, except for its high degree of whiteness. Obtaining this colour requires raw materials with a sufficiently low content, and some modification to the method of manufacture, among others, a higher kiln temperature required to sinter the clinker in the absence of ferric oxides acting as a flux in normal clinker. As contributes decreasing the melting point of the clinker (normally 1450\u00a0\u00b0C), the white cement requires a higher sintering temperature (around 1600\u00a0\u00b0C). As a result, it is slightly more expensive than the grey product. The main requirement is to have a low iron content which should be less than 0.5\u00a0wt.% expressed as for white cement, and less than 0.9\u00a0wt.% for off-white cement. It also helps to have the iron oxide as ferrous oxide (FeO), which is obtained via slightly reducing conditions in the kiln, i.e., operating with zero excess oxygen at the kiln exit. This gives the clinker and cement a green tinge. Other metallic oxides such as (green), MnO (pink), (white), etc., in trace content, can also give colour tinges, so for a given project it is best to use cement from a single batch.\nSafety issues.\nBags of cement routinely have health and safety warnings printed on them, because not only is cement highly alkaline, but the setting process is also exothermic. As a result, wet cement is strongly caustic and can easily cause severe skin burns if not promptly washed off with water. Similarly, dry cement powder in contact with mucous membranes can cause severe eye or respiratory irritation. The reaction of cement dust with moisture in the sinuses and lungs can also cause a chemical burn, as well as headaches, fatigue, and lung cancer.\nThe production of comparatively low-alkalinity cements (pH &lt; 11) is an area of ongoing investigation.\nIn Scandinavia, France, and the United Kingdom, the level of chromium(VI), which is considered to be toxic and a major skin irritant, may not exceed 2 parts per million (ppm).\nIn the US, the Occupational Safety and Health Administration (OSHA) has set the legal limit (permissible exposure limit) for Portland cement exposure in the workplace as 50 mppcf (million particles per cubic foot) over an 8-hour workday. The National Institute for Occupational Safety and Health (NIOSH) has set a recommended exposure limit (REL) of 10\u00a0mg/m3 total exposure and 5\u00a0mg/m3 respiratory exposure over an 8-hour workday. At levels of 5000\u00a0mg/m3, Portland cement is immediately dangerous to life and health.\nEnvironmental effects.\nPortland cement manufacture can cause environmental impacts at all stages of the process. These include emissions of airborne pollution in the form of dust; gases; noise and vibration when operating machinery and during blasting in quarries; consumption of large quantities of fuel during manufacture; release of CO2 from the raw materials during manufacture, and damage to countryside from quarrying. Equipment to reduce dust emissions during quarrying and cement manufacturing is widely used, and equipment to trap and separate exhaust gases is gaining increased use. Environmental protection also includes the reintegration of quarries into the countryside after they have been closed, by returning them to nature or re-cultivating them.\nPortland cement is caustic, so it can cause chemical burns. The powder can cause irritation or, with severe exposure, lung cancer, and can contain several hazardous components, including crystalline silica and hexavalent chromium. Environmental concerns are the high energy consumption required to mine, manufacture, and transport the cement, and the related air pollution, including the release of the greenhouse gas carbon dioxide, dioxin, , , and particulates. Production of Portland cement contributes about 10% of world carbon dioxide emissions. The International Energy Agency has estimated that cement production will increase by between 12 and 23% by 2050 to meet the needs of the world's growing population. There are several ongoing types of research targeting a suitable replacement of Portland cement by supplementary cementitious materials.\n\"Epidemiologic Notes and Reports Sulfur Dioxide Exposure in Portland Cement Plants\", from the Centers for Disease Control, states:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;\nAn independent research effort of AEA Technology to identify critical issues for the cement industry today concluded the most important environment, health and safety performance issues facing the cement industry are atmospheric releases (including greenhouse gas emissions, dioxin, , SO2, and particulates), accidents, and worker exposure to dust.\nThe CO2 associated with Portland cement manufacture comes mainly from four sources:\nOverall, with nuclear or hydroelectric power, and efficient manufacturing, CO2 generation can be reduced to per kg cement, but can be twice as high. The thrust of innovation for the future is to reduce sources 1 and 2 by modification of the chemistry of cement, by the use of wastes, and by adopting more efficient processes. Although cement manufacturing is a huge CO2 emitter, concrete (of which cement makes up about 15%) compares quite favourably with other modern building systems in this regard.. Traditional materials such as lime based mortars as well as timber and earth based construction methods emit significantly less CO2.\nCement plants used for waste disposal or processing.\nDue to the high temperatures inside cement kilns, combined with the oxidising (oxygen-rich) atmosphere and long residence times, cement kilns are used as a processing option for various types of waste streams; indeed, they efficiently destroy many hazardous organic compounds. The waste streams also often contain combustible materials, which allow the substitution of part of the fossil fuel commonly used in the process.\nWaste materials used in cement kilns as a fuel supplement:\nPortland cement manufacture also has the potential to benefit from using industrial byproducts from the waste stream. These include in particular:\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nFurther reading.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "40319", "revid": "15881234", "url": "https://en.wikipedia.org/wiki?curid=40319", "title": "Sarah Hughes", "text": "American figure skater (born 1985)\nSarah Elizabeth Hughes (born May 2, 1985) is an American former competitive figure skater. She is the 2002 Olympic Champion and the 2001 World bronze medalist in ladies' singles.\nPersonal life.\nHughes was born in Great Neck, New York, a suburb on Long Island. Her father, John Hughes, was a Canadian of Irish descent and was one of the captains of the undefeated and untied NCAA champion 1969\u201370 Cornell University ice hockey team. Her mother, Amy Pastarnack, is Jewish and is a breast cancer survivor. This led Hughes to become an advocate for breast cancer awareness. She appeared in a commercial for General Electric promoting breast cancer awareness and research. Hughes stated, \"I always said that if I can get one person to get a mammogram, I've accomplished something.\" Among the other causes Hughes supports is Figure Skating in Harlem, which provides free ice skating lessons and academic tutoring for girls in the Harlem community in New York City. Hughes has supported this program for over ten years.\nHughes attended Great Neck North High School. In 2003, she began her studies at Yale University where she was in Timothy Dwight College. On May 25, 2009, Hughes graduated from Yale and received a bachelor's degree in American studies with a concentration in U.S. politics and communities. She graduated from the University of Pennsylvania Law School on May 15, 2018. As of May 2023, Hughes was pursuing a business degree at Stanford University.\nHughes had previously dated former New York City Mayor Rudy Giuliani's son Andrew Giuliani in 2011, whom she befriended in 2005.\nHughes is the fourth of six children. One of her younger sisters, Emily, is also a figure skater and competed at the 2006 Winter Olympics. She is the cousin of Gregg \"Opie\" Hughes, from the \"Opie and Anthony\" radio show.\nSkating career.\nHughes began skating at the age of three. Robin Wagner, who also choreographed for her from 1994, became her head coach in January 1998.\nHughes won the junior title at the 1998 U.S. Championships in the 1997\u20131998 season. The following season, she competed on the ISU Junior Grand Prix and won the silver medal at the 1998\u20131999 Junior Grand Prix Final. She also took silver at the 1999 World Junior Championships held in November 1998. At the 1999 U.S. Championships, Hughes won the pewter medal in her senior-level debut. As the fourth-place finisher, Hughes would not normally have received one of the three spots for U.S. ladies at the 1999 World Championships, however, Naomi Nari Nam, the silver medalist, was not age-eligible for the event according to ISU rules. Hughes was likewise not age-eligible, but at the time a loophole existed for skaters who had medaled at Junior Worlds. Hughes was sent to senior Worlds and finished 7th in her debut.\nIn the 1999\u20132000 season, Hughes made her Grand Prix debut, winning the bronze medal at the 1999 Troph\u00e9e Lalique. She won the bronze medal at the 2000 U.S. Championships and was credited with a triple-salchow-triple-loop combination. She placed 5th at the 2000 World Championships.\nOn September 7, 2001, at the age of 16, Hughes was invited to meet United States National Security Advisor Condoleezza Rice.\nIn the 2000\u20132001 season, Hughes won three medals on the Grand Prix circuit and won the bronze medal at the 2000\u20132001 Grand Prix of Figure Skating Final. She won the silver medal at the 2001 U.S. Championships. At the 2001 World Championships, she won the bronze medal.\nIn the 2001\u20132002 season, Hughes again competed on the Grand Prix, winning the 2001 Skate Canada International while placing second at her other two events. She won her second consecutive bronze medal at the Grand Prix Final and won the bronze medal at the 2002 U.S. Championships to qualify for the 2002 Winter Olympics.\nThe week before the opening of the 2002 Olympics, Hughes appeared on the cover of \"Time\" magazine.\nAt the 2002 Olympics, Hughes won the gold medal in what was widely considered one of the biggest upsets in figure skating history. She was the youngest skater in the competition, and was not expected to seriously challenge the favorites, teammate Michelle Kwan and Russia's Irina Slutskaya. Hughes became the first woman in Olympic history to land two triple jump-triple jump combinations in a 4-minute free skate. Kwan, Slutskaya, and Sasha Cohen (the three skaters that finished ahead of Hughes in the short program), all made significant mistakes in the free skate, clearing the way for Hughes to win gold. Her Lutz jump was flawed, but her difficult and successful jump combinations made up for it. Her artistry, above-average edge quality, and ice coverage combined to establish her as a \"strong all-around skater\" and ensured her gold-medal win.\nAfter her Olympic win, Hughes was honored with a parade in her hometown of Great Neck, attended by U.S. Senators Hillary Clinton and Chuck Schumer, as well as Governor of New York George Pataki. Clinton spoke at the event and declared it Sarah Hughes Day. She received the James E. Sullivan Award as the top amateur athlete in the U.S., becoming the third figure skater to win this award after Dick Button (1949) and Michelle Kwan (2001).\nHughes did not compete at the 2002 World Championships. In the 2002\u20132003 season, she won the silver medal at the 2003 U.S. Championships, and placed sixth at the 2003 World Championships.\nHughes took the 2004\u20132005 year off from college and skated professionally with the Smuckers Stars on Ice tour company. She was inducted into the International Jewish Sports Hall of Fame in 2005.\nRichard Krawiec wrote a biography about her, \"Sudden Champion: The Sarah Hughes Story\" (2002).\nSkating technique.\nHughes employed a variety of triple-triple jump combinations, including a triple loop-triple loop, triple salchow-triple loop, and a triple toe-triple loop. She would also perform the triple loop jump which she often completed out of and following a back spiral. She was known for her camel spin with a change of edge as well as her spiral position. Unlike most skaters, she executed jumps and spins clockwise.\nPolitics.\nOn May 15, 2023, Hughes filed paperwork to run for Congress as a Democrat in New York's 4th congressional district. She withdrew from the race on September 9.\nResults.\n\"GP: Grand Prix; JGP: Junior Grand Prix\"\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "40320", "revid": "212624", "url": "https://en.wikipedia.org/wiki?curid=40320", "title": "Magnetic fusion energy", "text": ""}
{"id": "40321", "revid": "40943757", "url": "https://en.wikipedia.org/wiki?curid=40321", "title": "Soundgarden", "text": "American rock band\nSoundgarden was an American rock band formed in Seattle, Washington, in 1984 by singer and drummer Chris Cornell, lead guitarist Kim Thayil, and bassist Hiro Yamamoto. Cornell switched to rhythm guitar in 1985, replaced on drums initially by Scott Sundquist, and later by Matt Cameron in 1986. Yamamoto left in 1989 and was replaced initially by Jason Everman and shortly thereafter by Ben Shepherd. Soundgarden disbanded in 1997 and reformed in 2010. Following Cornell's death in 2017, Thayil declared in October 2018 that Soundgarden had disbanded a second time. The surviving members of the band have continued to occasionally work together since then, including reuniting for a one-off concert in tribute to Cornell in January 2019, and again in December 2024 for a benefit concert in Seattle, with Shaina Shepherd on vocals, under the anagram Nudedragons.\nSoundgarden was one of the pioneers of grunge music, a style of alternative rock that developed in the American Pacific Northwest in the mid-1980s, and helped to popularize it in the early 1990s, alongside such Seattle contemporaries as Alice in Chains, Pearl Jam, and Nirvana. They were the first of a number of grunge bands to sign to the Seattle-based record label Sub Pop, through which they released two EPs: \"Screaming Life\" (1987) and \"Fopp\" (1988). Soundgarden's debut album, \"Ultramega OK\", was also released in 1988 by the Los Angeles-based independent label SST Records; although the album did not sell well nationally, it garnered critical acclaim and the band's first Grammy Award nomination. Soundgarden was the first grunge band to be signed to a major label when they signed to A&amp;M Records in 1988. The release of their second album, \"Louder Than Love\" (1989), was the band's first album to enter the \"Billboard\" 200 chart, peaking at number 108, and spawned two radio hits: \"Hands All Over\" and \"Loud Love\".\nSoundgarden's third album, \"Badmotorfinger\" (1991), helped usher in the mainstream success of grunge. The album was buoyed by the success of the singles \"Jesus Christ Pose\", \"Outshined\", and \"Rusty Cage\", reached number 39 on the \"Billboard\" 200 and has been certified double-platinum by the Recording Industry Association of America (RIAA). The band's fourth album, \"Superunknown\" (1994), expanded their popularity; it debuted at number one on the \"Billboard\" 200 and yielded the Grammy Award-winning singles \"Spoonman\" and \"Black Hole Sun\". Soundgarden experimented with new sonic textures on their follow-up album \"Down on the Upside\" (1996), which debuted at number two on the \"Billboard\" 200 and spawned several hit singles of its own, including \"Pretty Noose\", \"Burden in My Hand\", and \"Blow Up the Outside World\". In 1997, the band broke up due to internal strife over its creative direction and exhaustion from touring. After more than a decade of working on projects and other bands, they reunited in 2010. Republic Records released their sixth and final studio album, \"King Animal\", in 2012.\nBy 2019, Soundgarden had sold more than 14 million records in the United States and an estimated 30 million worldwide. VH1 ranked them at number 14 in their special, \"100 Greatest Artists of Hard Rock\" list. The band was inducted into the Rock and Roll Hall of Fame in 2025.\nHistory.\nFormation and early recordings (1984\u20131988).\nSoundgarden's origin is a band called the Shemps, which had performed around Seattle in the early 1980s and included bassist Hiro Yamamoto and drummer and singer Chris Cornell. Following Yamamoto's departure, the band recruited guitarist Kim Thayil as its new bassist. Thayil moved to Seattle from Park Forest, Illinois, with Yamamoto and Bruce Pavitt, who later started the independent record label Sub Pop. Cornell and Yamamoto stayed in contact, and after the Shemps broke up Cornell and Yamamoto started jamming together, and were eventually joined by Thayil.\nSoundgarden was formed in 1984 and included Cornell (drums and vocals), Yamamoto (bass), and Thayil (guitar). The band named themselves after a wind-channeling pipe sculpture titled \"A Sound Garden\", on National Oceanic and Atmospheric Administration property at 7600 Sand Point Way, next to Magnuson Park in Seattle. Cornell originally played drums while singing, but in 1985 the band enlisted Scott Sundquist to allow Cornell to concentrate on vocals. The band traveled around playing various concerts with this lineup for about a year. Their first recordings were three songs that appeared on the 1986 compilation album for C/Z Records called \"Deep Six\": \"Heretic\", \"Tears to Forget\" and \"All Your Lies\". It also featured songs by fellow grunge pioneers Green River, Skin Yard, Malfunkshun, the U-Men, and the Melvins. In 1986, Cornell's then-girlfriend and future wife, Susan Silver started managing Soundgarden. In the same year, Sundquist left the band to spend time with his family and was replaced by former Skin Yard drummer Matt Cameron.\nA Soundgarden performance one night impressed KCMU DJ Jonathan Poneman, who later said: \"I saw this band that was everything rock music should be.\" Poneman offered to fund a release by the band, so Thayil suggested he team up with Bruce Pavitt. Poneman offered to contribute $20,000 in funding for Sub Pop, effectively turning it into a full-fledged record label. Soundgarden signed to Sub Pop, and the label released \"Hunted Down\" in 1987 as the band's first single. The B-side of \"Hunted Down\", \"Nothing to Say\", appeared on the KCMU compilation tape \"Bands That Will Make Money\", which was distributed to record companies, many of whom showed interest in Soundgarden. Through Sub Pop, the band released the \"Screaming Life\" EP in 1987, and the \"Fopp\" EP in 1988, and a combination of the two, \"Screaming Life/Fopp\", in 1990.\n\"Ultramega OK\", major label signing, and \"Louder Than Love\" (1988\u20131990).\nThough major labels were courting the band, in 1988 they signed to the independent label SST Records for their debut album, \"Ultramega OK\", released on October 31, 1988. Cornell said the band \"made a huge mistake with \"Ultramega OK\"\" because they used a producer suggested by SST who \"didn't know what was happening in Seattle.\" According to Steve Huey of AllMusic, Soundgarden demonstrates a \"Stooges/MC5-meets-Zeppelin/Sabbath sound\" on the album. Mark Miremont directed the band's first music video for \"Flower\", which aired regularly on MTV's \"120 Minutes\". Soundgarden promoted \"Ultramega OK\" on a tour in the United States in the spring of 1989, and a tour in Europe which began in May of that year\u2014the band's first overseas tour. \"Ultramega OK\" earned the band a Grammy Award nomination for Best Metal Performance in 1990.\nThe band signed with A&amp;M Records in mid-1988, which had caused a rift between Soundgarden and its traditional audience. Thayil said, \"In the beginning, our fans came from the punk rock crowd. They abandoned us when they thought we sold out the punk tenets, getting on a major label and touring with Guns N' Roses. There were fashion issues and social issues, and people thought we no longer belonged to their scene, to their particular sub-culture.\" The band began work on its first album for a major label, but personnel difficulties caused a shift in the band's songwriting process. According to Cornell, \"At the time Hiro [Yamamoto] excommunicated himself from the band and there wasn't a free-flowing system as far as music went, so I ended up writing a lot of it.\" On September 5, 1989, the band released its debut major-label album, \"Louder Than Love\", which saw it take \"a step toward the metal mainstream\", according to Steve Huey of AllMusic, describing it as \"a slow, grinding, detuned mountain of Sabbath/Zeppelin riffs and Chris Cornell wailing\". Because of some of the lyrics, most notably on \"Hands All Over\" and \"Big Dumb Sex\", the band faced various retail and distribution problems upon the album's release. \"Louder Than Love\" became Soundgarden's first album to chart on the \"Billboard\" 200, peaking at number 108, and it was also the first grunge album to enter that chart. This accomplishment was aided by two singles \u2014 \"Hands All Over\" and \"Loud Love\" \u2014 that gained the band exposure on MTV's \"Headbangers Ball\", and mainstream rock radio stations outside of Seattle such as KNAC, WMMS, KRZQ, WBCN, Z Rock and KISS-FM.\nA month before touring for \"Louder Than Love\" was to begin, Yamamoto, who was becoming frustrated that he was not making much of a contribution, left the band to return to college. First the band played a few rehearsals with Jim Tillman from the U-Men, but it did not work, and soon Jason Everman, formerly of Nirvana, officially replaced Yamamoto on bass. The band toured North America from December 1989 to March 1990, opening for Voivod, who were supporting their album \"Nothingface\", with Faith No More and the Big F also as opening acts at the beginning and end of the tour. The band also toured Europe. The band fired Everman in mid-1990 after completing its promotional tour for \"Louder Than Love\"; Thayil said that \"Jason just didn't work out.\" \"Louder Than Love\" spawned the EP \"Loudest Love\" and the video compilation \"Louder Than Live\", both released in 1990.\nEstablished lineup, \"Badmotorfinger\", and rise in popularity (1990\u20131993).\nBassist Ben Shepherd replaced Everman and the new lineup recorded Soundgarden's third album in 1991. Cornell said that Shepherd brought a \"fresh and creative\" approach to the recording sessions, and the band as a whole said that his knowledge of music and writing skills redefined the band. The band released the resulting album, \"Badmotorfinger\", on October 8, 1991. Steve Huey of AllMusic said that the songwriting on \"Badmotorfinger\" \"takes a quantum leap in focus and consistency.\" He added, \"It's surprisingly cerebral and arty music for a band courting mainstream metal audiences.\" Thayil suggested that the album's lyrics are \"like reading a novel [about] man's conflict with himself and society, or the government, or his family, or the economy, or anything.\" The first single from \"Badmotorfinger\", \"Jesus Christ Pose\", garnered attention when MTV decided to ban its music video in 1991. The song and its video outraged many listeners who perceived it as anti-Christian. The band received death threats while on tour in the United Kingdom in support of the album. Cornell explained that the lyrics criticize public figures who use religion (particularly the image of Jesus Christ) to portray themselves as being persecuted. Although eclipsed at the time of its release by the sudden popularity of Nirvana's \"Nevermind\", the focus of attention brought by \"Nevermind\" to the Seattle scene helped Soundgarden gain wider attention. The singles \"Outshined\" and \"Rusty Cage\" found an audience on alternative rock radio and MTV. \"Badmotorfinger\" was nominated for a Grammy Award for Best Metal Performance in 1992, and was among the 100 top-selling albums of the year.\nFollowing the release of \"Badmotorfinger\", Soundgarden toured North American in October and November 1991. Afterward, Guns N' Roses selected the band as its opening act for their \"Use Your Illusion\" tour. The band also opened for Skid Row in North America in February 1992 on their \"Slave to the Grind\" tour, and then headed to Europe for a month-long headlining theater tour. The band returned for a tour in the United States, and then rejoined Guns N' Roses in the summer of 1992 in Europe as part of the \"Use Your Illusion\" tour along with fellow opening act Faith No More. Describing opening for Guns N' Roses, Cornell said, \"It wasn't a whole lot of fun going out in front of 40,000 people for 35 minutes every day. Most of them never heard our songs and didn't care about them. It was a bizarre thing.\" The band played the 1992 Lollapalooza tour with the Red Hot Chili Peppers, Pearl Jam, Ministry and Ice Cube among others. In anticipation of the band's appearance at Lollapalooza, they released a limited edition of \"Badmotorfinger\" in 1992 with a second disc containing the EP \"Satanoscillatemymetallicsonatas\" (a palindrome), featuring Soundgarden's cover of Black Sabbath's \"Into the Void\", titled \"Into the Void (Sealth)\", which was nominated for a Grammy Award for Best Metal Performance in 1993. The band released the video compilation \"Motorvision\", filmed at Seattle's Paramount Theatre in 1992. The band appeared in the movie \"Singles\", performing \"Birth Ritual\". The song is included on , as is a Cornell solo song, \"Seasons\".\nIn 1993, the band contributed the track \"Show Me\" to the AIDS-Benefit album \"No Alternative\", produced by the Red Hot Organization.\n\"Superunknown\" and mainstream success (1994\u20131995).\nSoundgarden began working on its fourth album after touring in support of \"Badmotorfinger\". Cornell said that while working on the album, the band allowed each other more freedom than on past records, and Thayil observed that they had spent a lot more time working on the recording of the songs than on previous records. Released on March 8, 1994, \"Superunknown\" became the band's breakthrough album, debuting at number one on the \"Billboard\" 200 album chart and being driven by the singles \"Spoonman\", \"The Day I Tried to Live\", \"Black Hole Sun\", \"My Wave\", and \"Fell on Black Days\".\nThe songs on \"Superunknown\" captured the creativity and heaviness of the band's earlier works, while showcasing the group's newly evolving style. Lyrically, the album was quite dark and mysterious, and it is often interpreted to be dealing with substance abuse, suicide, and depression. At the time, Sylvia Plath inspired Cornell's writing. The album was also more experimental than previous releases, with some songs incorporating Middle-Eastern or Indian music. J. D. Considine of \"Rolling Stone\" said \"Superunknown\" \"demonstrates far greater range than many bands manage in an entire career\". He also stated, \"At its best, \"Superunknown\" offers a more harrowing depiction of alienation and despair than anything on [Nirvana's final studio album] \"In Utero\".\" The music video for \"Black Hole Sun\" became a hit on MTV, and received the award for Best Metal/Hard Rock Video at the 1994 MTV Video Music Awards, and in 1995 the Clio Award for Alternative Music Video. Soundgarden won two Grammy Awards in 1995\u2014\"Black Hole Sun\" received the award for Best Hard Rock Performance and \"Spoonman\" received the award for Best Metal Performance. The album was nominated for a Grammy Award for Best Rock Album in 1995. \"Superunknown\" has been certified six times Platinum in the United States and remains Soundgarden's most successful album.\nThe band began touring in January 1994 in Oceania and Japan, areas where the record came out early and where the band had never toured before. This round of touring ended in February 1994. In the following month, the band moved on to Europe. They began a theater and arena tour of the United States, first with a stop on May 27, 1994, at the PNE Forum in Vancouver, with the opening acts Tad and Eleven. In late 1994, after touring in support of \"Superunknown\", doctors discovered that Cornell had severely strained his vocal cords, and Soundgarden canceled several shows to avoid causing any permanent damage. Cornell said, \"I think we kinda overdid it! We were playing five or six nights a week and my voice pretty much took a beating. Towards the end of the American tour I felt like I could still kinda sing, but I wasn't really giving the band a fair shake. You don't buy a ticket to see some guy croak for two hours! That seemed like kind of a rip off.\" The band made up the dates later in 1995. \"Superunknown\" spawned the EP \"Songs from the Superunknown\" and the CD-ROM \"Alive in the Superunknown\", both released in 1995.\n\"Down on the Upside\" and breakup (1996\u20131997).\nFollowing the worldwide tour in support of \"Superunknown\", the band began working on what would become their last studio album for over 15 years, choosing to produce it themselves. However, tensions within the group reportedly arose during the sessions, with Thayil and Cornell allegedly clashing over Cornell's desire to shift away from the heavy guitar riffing that had become the band's trademark. Cornell said, \"By the time we were finished, it felt like it had been kind of hard, like it was a long, hard haul. But there was stuff we were discovering.\" The band's fifth album, \"Down on the Upside\", was released on May 21, 1996. It was notably less heavy than the group's earlier albums, and marked a further departure from the band's grunge roots. At the time, Soundgarden explained that they wanted to experiment with other sounds, including acoustic instrumentation. David Browne of \"Entertainment Weekly\" said, \"Few bands since Led Zeppelin have so crisply mixed instruments both acoustic and electric.\" The overall mood of the album's lyrics is less dark than on previous Soundgarden albums, with Cornell describing some songs as \"self-affirming\". The album spawned several singles, including \"Pretty Noose\", \"Burden in My Hand\", and \"Blow Up the Outside World\". \"Pretty Noose\" was nominated for a Grammy Award for Best Hard Rock Performance in 1997. The album did not match the sales or critical praise of \"Superunknown\".\nThe band took a slot on the 1996 Lollapalooza tour with Metallica, who had insisted on Soundgarden's appearance on the tour. After Lollapalooza, the band embarked on a world tour, and already-existing tensions increased during it. When asked whether the band hated touring, Cornell replied: \"We really enjoy it to a point, and then it gets tedious, because it becomes repetitious. You feel like fans have paid their money and they expect you to come out and play them your songs like the first time you ever played them. That's the point where we hate touring.\" At the tour's last stop in Honolulu, Hawaii on February 9, 1997, Shepherd threw his bass into the air in frustration after suffering equipment failure, and then stormed off the stage. The band retreated, with Cornell returning to end the show with a solo encore. On April 9, 1997, the band announced they were disbanding. Thayil said, \"It was pretty obvious from everybody's general attitude over the course of the previous half year that there was some dissatisfaction.\" Cameron later said that Soundgarden was \"eaten up by the business\". The band released a greatest hits collection entitled \"A-Sides\" on November 4, 1997, composed of 17 songs, including the previously unreleased \"Bleed Together\", which was recorded during the \"Down on the Upside\" recording sessions.\nPost-breakup activities (1998\u20132009).\nCornell released a solo album in September 1999, entitled \"Euphoria Morning\", which featured Matt Cameron on the track \"Disappearing One\". By May 2001, Cornell had joined the platinum-selling supergroup Audioslave with Tom Morello, Tim Commerford and Brad Wilk, then-former members of Rage Against the Machine, which recorded three albums: \"Audioslave\" (2002), \"Out of Exile\" (2005), and \"Revelations\" (2006). Cornell left Audioslave in early 2007, resulting in the band's break-up. His second solo album, \"Carry On\", was released in June 2007, and his third solo album, \"Scream\", produced by Timbaland, was released in March 2009, both to mixed commercial and critical success. Cornell also wrote the lyrics and provided vocals for the song \"Promise\" on Slash's debut solo album \"Slash\", released in 2010.\nThayil joined forces with former Dead Kennedys singer Jello Biafra, former Nirvana bassist Krist Novoselic, and drummer Gina Mainwal for one show, performing as The No WTO Combo during the WTO ministerial conference in Seattle on December 1, 1999. Thayil contributed guitar tracks to Steve Fisk's 2001 album, \"999 Levels of Undo\", as well as Dave Grohl's 2004 side-project album, \"Probot\". In 2006, Thayil played guitar on the album \"Altar\", the collaboration between the bands Sunn O))) and Boris.\nCameron initially turned his efforts to his side-project Wellwater Conspiracy, to which both Shepherd and Thayil had contributed. He then worked briefly with the Smashing Pumpkins on the band's 1998 album, \"Adore\". That same year, he played drums for Pearl Jam's Yield Tour following Jack Irons's departure, and later joined as an official member of the band, remaining until his departure in July 2025. He recorded seven albums with Pearl Jam: \"Binaural\" (2000), \"Riot Act\" (2002), \"Pearl Jam\" (2006), \"Backspacer\" (2009), \"Lightning Bolt\" (2013), \"Gigaton\" (2020) and \"Dark Matter\" (2024). Cameron also played percussion on Geddy Lee's album \"My Favourite Headache\". In 2017, he was inducted into the Rock and Roll Hall of Fame as a member of Pearl Jam.\nShepherd was the singer on Wellwater Conspiracy's 1997 debut studio album, \"Declaration of Conformity\", but left the band after its release. He has toured with Mark Lanegan and played bass on two of Lanegan's albums, \"I'll Take Care of You\" (1999), and \"Field Songs\" (2001). Shepherd and Cameron lent a hand with recording Tony Iommi's album \"IOMMI\" (2000). While they were members of Soundgarden they were part of the side-project band Hater, and in 2005 Shepherd released the band's long-delayed second album, \"The 2nd\".\nIn a July 2009 interview with \"Rolling Stone\", Cornell shot down rumors of a reunion, saying that conversations between the band members had been limited to discussion about the release of a box set or B-sides album of Soundgarden rarities, and that there had been no discussion of a reunion at all. The band's interest in new releases emerged from a 2008 meeting about their shared properties, both financial and legal, where they realized Soundgarden lacked online presence such as a website or a Facebook page. As Thayil summed up, \"We kind of had neglected our merchandise over the last decade\". Eventually the musicians decided to create an official site handled by Pearl Jam's Ten Club, relaunch their catalog, and according to Cameron, seek \"a bunch of unreleased stuff we wanted to try to put out\". In March 2009, Thayil, Shepherd and Cameron got onstage during a concert by Tad Doyle in Seattle and played some Soundgarden songs. Cornell stated that the moment \"sort of sparked the idea: If Matt, Kim, and Ben can get in a room, rehearse a couple songs, and play, maybe we all could do that as Soundgarden.\"\nOn October 6, 2009, all the members of Soundgarden attended Night 3 of Pearl Jam's four-night stand at the Gibson Amphitheatre in Universal City, California. During an encore, Temple of the Dog reunited for the first time since Pearl Jam's show at the Santa Barbara Bowl on October 28, 2003. Chris Cornell joined the band to sing \"Hunger Strike\". It was the first public appearance of Soundgarden since their breakup in April 1997. Consequently, rumors of an impending reunion were circulating on the Internet.\nReunion, \"Telephantasm\" and \"King Animal\" (2010\u20132013).\nOn January 1, 2010, Cornell alluded to a Soundgarden reunion on his Twitter account writing: \"The 12-year break is over and school is back in session. Sign up now. Knights of the Soundtable ride again!\" The message linked to a website that featured a picture of the group performing live and a place for fans to enter their e-mail addresses to get updates on the reunion. Entering that information unlocked a video for the song \"Get on the Snake\", from 1989's \"Louder Than Love\". On March 1, 2010, Soundgarden announced to their e-mail subscribers that they would be re-releasing an old single \"Hunted Down\" with the song \"Nothing to Say\" on a 7-inch vinyl record. It was released on April 17, Record Store Day. They released \"Spoonman\" live at the Del Mar Fairgrounds in San Diego, California from 1996. Soundgarden played their first show since 1997 on April 16 at the Showbox at the Market in the band's hometown of Seattle. The band headlined Lollapalooza on August 8.\n\"Telephantasm: A Retrospective\", a new Soundgarden compilation album, was packaged with initial shipments of the \"\" video game and released on September 28, 2010, one week before the CD's availability in stores on October 5, 2010. An expanded version of \"Telephantasm\" consisting of two CDs and one DVD is available for sale. A previously unreleased Soundgarden song\u2014\"Black Rain\"\u2014debuted on the \"Guitar Hero\" video game and appears on the compilation album, which achieved platinum certification status after its first day of retail availability. \"Black Rain\" hit rock radio stations on August 10, 2010, and was the band's first single since 1997. In November 2010, Soundgarden was the second musical guest on the show \"Conan\", making their first television appearance in 13 years. The band issued a 7-inch vinyl, \"The Telephantasm\", for Black Friday Record Store Day. In March 2011, Soundgarden released their first live album, \"Live on I-5\".\nIn February 2011, Soundgarden announced on their homepage that they had started recording a new album. On March 1, 2011, Chris Cornell confirmed that Adam Kasper would produce it. Four days later, the band stated it would consist of material that was \"90 percent new\" with the rest consisting of updated versions of older ideas. They also noted that they had 12 to 14 songs that were \"kind of ready to go\". Although Cameron claimed the album would be released in 2011, the recording was prolonged as Thayil said that \"the more we enjoy it, the more our fans should end up enjoying it\". Thayil also reported that some songs sound \"similar in a sense to \"Down on the Upside\"\" and that the album would be \"picking up where we left off. There are some heavy moments, and there are some fast songs.\" The next day, Cornell reported that the new album would not be released until the spring of 2012.\nIn April 2011, Soundgarden announced a summer tour consisting of 16 dates across the US with various opening acts. The band later headlined Voodoo Experience at City Park in New Orleans on the 2011 Halloween weekend. In March 2012 a post on the band's official Facebook page said a new song, \"Live to Rise\", would be included on the soundtrack of the upcoming movie \"The Avengers\", based on the Marvel Comics franchise. It was the first newly recorded song the band had released since re-forming in 2010. \"Live to Rise\" was released as a free download on iTunes on April 17. Also in March it was announced that Soundgarden would headline the Friday night of the Hard Rock Calling Festival the following July in London, England. In April, Soundgarden announced the release of a box set titled \"Classic Album Selection\" for Europe, containing all of their studio albums except for \"Ultramega OK\", and live album \"Live on I-5\". On May 5, just before The Offspring began playing their set, the band appeared as a special guest at the 20th annual KROQ Weenie Roast in Irvine, California. Later that month, Soundgarden told \"Rolling Stone\" they were eyeing an October release for their new album. That June, the band appeared at Download Festival in Donington, England. The band released \"Been Away Too Long\", the first single from their new album \"King Animal\" on September 27; the album was released on November 13, 2012. The band released a video for \"By Crooked Steps\", directed by Dave Grohl, in early 2013. \"Halfway There\" was the third single released from the album.\n\"Echo of Miles...\" and Cornell's death (2013\u20132017).\nOn November 15, 2013, drummer Matt Cameron announced he would not be touring with Soundgarden in 2014, due to prior commitments promoting Pearl Jam's album \"Lightning Bolt\". On March 16, 2014, Soundgarden and Nine Inch Nails announced they were going to tour North America together, along with opening act Death Grips. Former Pearl Jam drummer Matt Chamberlain filled in for Cameron for live shows in South America and Europe on March 27, 2014.\nSoundgarden announced on October 28, 2014, they would release the 3-CD compilation box set, \"\", on November 24. The set includes rarities, live tracks, and unreleased material spanning the group's history. It includes previously released songs, such as \"Live to Rise\", \"Black Rain\", \"Birth Ritual\", and others, as well as a newly recorded rendition of the song \"The Storm\" from the band's pre-Matt Cameron 1985 demo, now simply titled \"Storm\", which was, like the original, produced by Jack Endino. One day before its official announcement, on October 27, the band posted a copy of \"Storm\" on YouTube.\nThayil mentioned in several interviews it was likely the band would start working on material for a new album in 2015, and in August 2015, Cornell stated they were doing so. On January 19, 2016, The Pulse Of Radio announced that Soundgarden had returned to the studio to continue working on their new album. On July 14, 2016, bassist Ben Shepherd and Cameron stated that the band had written \"six solid tunes\" for the new album, with more writing to be done in August.\nOn May 18, 2017, Cornell was found dead, \"with a band around his neck\", according to his representative, Brian Bumbery. Cornell was in his room at the MGM Grand hotel and casino in Detroit, Michigan, after performing at the Fox Theatre with Soundgarden. From the outset, the investigation into the singer's death was described by a local police spokesperson as that of a \"possible suicide\", based on unspecified details in the room where his body was discovered. Subsequently, the Wayne County Medical Examiner's Office determined the cause of death as suicide by hanging. However, Cornell's widow, Vicky, questioned whether he would deliberately end his own life, and said that the drug Ativan, which her husband was taking, might have led him to commit suicide. She said: \"I know that he loved our children and he would not hurt them by intentionally taking his own life.\"\nFollowing Cornell's death, Soundgarden canceled the rest of their 2017 tour, including headlining performances at Rock on the Range and Rocklahoma later that month.\nAftermath, disbandment and reunions (2017\u2013present).\nIn September 2017, drummer Matt Cameron told \"Billboard\" that he and the other surviving members of Soundgarden had yet to make a decision about the future of the band following Cornell's death. He was quoted as saying, \"I don't think we're ready to say anything other than ... Kim and Ben and I are certainly aware of how much our fans are hurting, and we're certainly hurting right there along with them. But we're extremely private people, and we're all still processing our grief in our own way and on our own time. But we definitely are thinking of our fans and love them very much.\"\nIn September 2018, guitarist Kim Thayil told \"Billboard\" that he and the other surviving members of Soundgarden were still unsure about the future of the band. He clarified, \"We often reference rock history and we've often commented on what other bands in similar situations have done, not as a plan or anything but just commenting on how bands have handled situations like this and what bands seem to have been graceful and dignified in how they manage their future musical endeavors and how some maybe were clumsy and callous. We think about those things. We try not to go too deep into these conversations, but stuff comes up after a few beers.\" A month later, Cameron told \"Rolling Stone\" that the surviving members of Soundgarden \"would certainly love to try to continue to do something, figure out something to do together.\" Bassist Ben Shepherd added, \"We haven't even gotten a chance to hang out, just us three, yet. We're going through natural healing, then thinking about the natural next step.\"\nIn an October 2018 interview with \"Seattle Times\", Thayil stated that the Soundgarden band name would be retired. He explained, \"I don't know really what kind of thing is possible or what we would consider in the future. It's likely nothing. The four of us were that. There were four of us and now there's three of us, so it's just not likely that there's much to be pursued other than the catalog work at this point.\" Thayil also stated that while he did not rule out the possibility of working with Cameron and Shepherd in a different capacity, writing or touring under the Soundgarden banner again was unlikely: \"No, I don't think that's anything we'd give reasonable consideration to at this point. When I say 'at this point,' I mean perhaps ever.\"\nIn January 2019, the remaining members of the band reunited in a tribute concert and fundraiser at The Forum in Inglewood, California, organized by Cornell's widow, Vicky Cornell. Members of Soundgarden, Temple of the Dog, Audioslave, Alice in Chains, Melvins, Foo Fighters, and Metallica together with other notable artists performed songs from Cornell's career. Taylor Momsen, Marcus Durant, Brandi Carlile, and Taylor Hawkins contributed vocals to Soundgarden, who performed \"Rusty Cage\", \"Flower\", \"Outshined\", \"Drawing Flies\", \"Loud Love\", \"I Awake\", \"The Day I Tried to Live\", and \"Black Hole Sun\", making this their first performance since Cornell's death.\nIn July 2019, Thayil said in an interview with \"Music Radar\" that the surviving members of Soundgarden are trying to finish and release the album they were working on with Cornell. However, the master files of Cornell's vocal recordings are currently being withheld, and when Thayil sought permission to use these files, he was denied.\nIn December 2019, Cornell's widow, Vicky Cornell, sued the surviving members of Soundgarden over seven unreleased recordings Cornell made before his death in 2017, claiming \"they have \"shamelessly conspired to wrongfully withhold hundreds of thousands of dollars indisputably owed to Chris' widow and minor children in an unlawful attempt to strong-arm Chris' Estate into turning over certain audio recordings created by Chris before he passed away.\" The lawsuit stated that Cornell made the seven recordings at his personal studio in Florida in 2017, which there was never any explicit agreement that these songs were meant for Soundgarden, and that Cornell was the only owner of tracks. In February 2020, Thayil, Cameron and Shepherd demanded Vicky to hand over the unreleased recordings, claiming that they worked jointly on these final tracks with Chris and that Vicky has no right to withhold from them what they call the \"final Soundgarden album.\" The band members pointed to interviews Chris and his bandmates made at the time confirming they were working together on what would be Soundgarden's eighth album. In March 2020, Soundgarden asked court to dismiss the lawsuit. In May 2020, Soundgarden countersued Vicky claiming that she engaged in \"fraudulent inducement\" by allegedly attempting to use the revenue from the January 2019 \"I Am the Highway: A Tribute to Chris Cornell\" concert, which was meant to go to the Chris and Vicky Cornell Foundation, for \"personal purposes for herself and her family\". The band dropped the benefit concert lawsuit in July 2020.\nOn August 10, 2020, Nile Rodgers and Merck Mercuriadis' company Hipgnosis Songs Fund acquired 100% of Chris Cornell's catalog of song rights (241 songs), including Soundgarden's catalog. Rodgers is friends with Cornell's widow.\nOn December 1, 2020, Thayil, Shepherd and Cameron performed as \"members of Soundgarden\" alongside Tad Doyle of Tad, Mike McCready and Meagan Grandallat at MoPOP Founders Award tribute to Alice in Chains.\nIn February 2021, Vicky Cornell filed another lawsuit claiming that the remaining members of Soundgarden had undervalued her share of the band, offering her \"the villainously low figure of less than $300,000.\" Vicky claimed the band offered her $300,000 despite receiving a $16 million offer from another investor for the act's master recordings. Vicky said she counter-offered $12 million for the band's collective interests, equaling $4 million per surviving member, which they denied. She then offered them $21 million for the band's interests, and that offer was also rejected. Soundgarden said in a statement that the \"buyout offer that was demanded by the estate has been grossly mischaracterized and we are confident that clarity will come out in court. All offers to buy out our interests have been unsolicited and rejected outright.\" The band also noted that they also had not had access to their social media accounts, which has resulted in \"misleading and confusing our fans\", leading the band to create new Twitter, Instagram and Facebook accounts under the name \"Nude Dragons\", an anagram for Soundgarden. On March 19, 2021, a federal judge recommended that claims the surviving band members improperly withheld \"hundreds of thousands of dollars\" and that the band's manager breached his duty to look after Vicky's interests be dismissed, citing lack of evidence of the band withholding royalties. On March 25, 2021, Soundgarden demanded the passwords for their social media and website. On June 15, 2021, the band got their website and social media accounts back in a temporary agreement with Vicky.\nOn April 17, 2023, it was officially revealed that seven final recordings with Cornell would be released after the dispute between the members and Vicky Cornell had ended. In a September 2025 interview with \"Billboard\", Cameron confirmed that he, Thayil and Shepherd were \"definitely over halfway done with\" the remaining eight songs the band worked on before Cornell's death. It was announced in November 2025 that during the album's sessions, the band reunited with producer Terry Date.\nOn December 14, 2024, the surviving members of Soundgarden, along with vocalist Shaina Shepherd, performed together under the moniker Nudedragons (previously used in 2010) for a benefit show in Seattle for the Seattle Children's Hospital. The surviving members of the band were scheduled to perform the following July for the benefit concert Back to the Beginning, but did not appear due to scheduling conflicts.\nOn November 8, 2025, Soundgarden was inducted into the Rock and Roll Hall of Fame by Jim Carrey. At the ceremony, the surviving members of the band were joined by original bassist Hiro Yamamoto for the first time in 36 years and performed a two-song set: Momsen and Mike McCready joined them for \"Rusty Cage\", while Carlile and Jerry Cantrell were the additional performers for \"Black Hole Sun\". Cornell's daughter Toni also performed at the ceremony, singing the band's \"Fell on Black Days\".\nMusical style and influences.\nSoundgarden were pioneers of the grunge music genre, which mixed elements of punk rock and metal to make a sludgy, murky sound through the use of fuzzy-sounding distortion in the guitars.\nSoundgarden cited Minutemen, the Meat Puppets, Butthole Surfers, Wire, and Joy Division as key early influences. Black Sabbath also had a significant impact on the band's sound, especially on the guitar riffs and tunings. Guitarist Kim Thayil has described the band's sound as a \"Sabbath-influenced punk\".\nSoundgarden has been frequently compared to Led Zeppelin, their early sound being described as consisting of \"gnarled neo-Zeppelinisms\". Though the band initially denied being inspired by Led Zeppelin, they would eventually embrace this influence, as detailed by Thayil:[W]e started getting [comparisons to Led Zeppelin] a lot: 'Zeppelin, Zeppelin, Zeppelin,' and we were like, OK, let's check some of this out. We were all very acquainted with it individually, but collectively we weren't sitting around the table listening them. So initially we would deny that influence. Eventually, after practice we'd be like, 'Let\u2019s check out \"Led Zeppelin IV\".' Let\u2019s listen to \"Houses of the Holy\".' Like, 'Yeah, I guess I can kind of see that a little bit.' It became very important to us, because of the comparison, so we would listen to it and start referencing it. Ultimately, we started to re-embrace the Zeppelin, Beatles, Sabbath and Pink Floyd. I think they were always there. At some point, we had to look back and say, 'This has a lot to do with our upbringing.' It's a weird story, but it may explain why, for a few years, we denied the Zeppelin-Sabbath influence.\nThough the influence of Led Zeppelin was evident, \"Q\" magazine noted that Soundgarden were \"in thrall to '70s rock, but contemptuous of the genre's overt sexism and machismo.\"\nThe Butthole Surfers' mix of punk, heavy metal and noise rock was a major influence on the early work of Soundgarden. Soundgarden, like other early grunge bands, were also influenced by British post-punk bands such as Gang of Four and Bauhaus which were popular in the early 1980s Seattle scene. The band was also influenced by the likes of the Ramones, Kiss, Accept, the Melvins, and Saint Vitus.\nThe name of the band, according to Thayil, was supposed to include the many roots of their style: that included \"a virtual plethora of cutting edge rock that spans Velvet Underground, Meat Puppets, and Killing Joke\". The band also mentioned \"Metallica Gothicism and sublime poetry. The almost ethereal flavour of the name betrays the brutality of the music but never pins Soundgarden in one corner\".\nCornell himself said: \"When Soundgarden formed we were post-punk \u2013 pretty quirky. Then somehow we found this neo-Sabbath psychedelic rock that fitted well with who we were.\"\nSoundgarden broadened its musical range with its later releases. By 1994's \"Superunknown\", the band began to incorporate more psychedelic influences into its music. Cornell also became known for his wide vocal range and his dark, existentialist lyrics.\nSoundgarden also used unorthodox time signatures; \"Fell on Black Days\" is in 6/4, \"Limo Wreck\" is played in 15/8, and \"The Day I Tried to Live\" alternates between 7/8 and 4/4 sections. The main guitar riff of \"Circle of Power\" is in 5/4. The E strings of the instruments were at times tuned even lower, such as on \"Rusty Cage\", where the lower E is tuned down to B. Some songs use more unorthodox tunings: \"Been Away Too Long\", \"My Wave\", and \"The Day I Tried to Live\" are all in a E\u2013E\u2013B\u2013B\u2013B\u2013B tuning and \"Burden in My Hand\", \"Head Down\", and \"Pretty Noose\" in a tuning of C-G-C-G-G-E\". Thayil has said Soundgarden usually did not consider the time signature of a song until after the band wrote it, and said the use of odd meters was \"a total accident\". He also used the meters as an example of the band's anti-commercial stance, saying that if Soundgarden \"were in the business of hit singles, we'd at least write songs in 4/4 so you could dance to them\".\nLegacy.\nThe development of the Seattle independent record label Sub Pop Records is tied closely to Soundgarden (due to Sub Pop co-founder Jonathan Poneman funding Soundgarden's early releases), and the success of the band resulted in the expansion of Sub Pop as a serious record label.\nNirvana frontman Kurt Cobain was a fan of Soundgarden's music, and reportedly Soundgarden's involvement with Sub Pop influenced Cobain to sign Nirvana with the label. Cobain also stated that Soundgarden was one of the only Seattle bands that he liked along with Tad and Mudhoney. In rare footage from the 2015 documentary \"\", Cobain can be seen impersonating Chris Cornell singing \"Outshined\". Alice in Chains guitarist and vocalist Jerry Cantrell stated that Soundgarden was a big influence on his band.\nSoundgarden was the first grunge band to sign to a major label when the band joined the roster of A&amp;M Records in 1988. However, Soundgarden did not achieve success initially, and only with successive album releases did the band meet with increased sales and wider attention. Bassist Ben Shepherd has not been receptive to the grunge label, saying in a 2013 interview, \"That's just marketing. It's called rock and roll, or it's called punk rock or whatever. We never were grunge, we were just a band from Seattle.\" They were ranked No.\u00a014 on VH1's \"100 Greatest Artists of Hard Rock\".\nIn 1994, Electronic Arts contacted Soundgarden's label A&amp;M Records for a bid to license the band's music for a CD-based entry in the \"Road Rash\" video game series. Although the label was initially hesitant due to the lack of precedence for licensing music for video games, Cornell and his band members expressed enthusiasm, as they were fans of the games and frequently played them on their bus while touring the country. A&amp;M then obtained the band's permission to use them as leverage to incorporate other alt-rock bands within the A&amp;M label into the game, including Monster Magnet, Paw, Swervedriver, Therapy? and Hammerbox. As a result of Soundgarden's involvement, the 3DO version of \"Road Rash\" has been credited with revolutionizing the use of licensed music in video games.\nRegarding Soundgarden's legacy, in a 2007 interview Cornell said:\nI think, and this is now with some distance in listening to the records, but on the outside looking in with all earnestness I think Soundgarden made the best records out of that scene. I think we were the most daring and experimental and genre-pushing really and I'm really proud of it. And I guess that's why I have trepidation about the idea of re-forming. I don't know what it would mean, or I guess I just have this image of who we were and I had probably a lot of anxiety during the period of being Soundgarden, as we all did, that it was responsibility and it was an important band and music and we didn't want to mess it up and we managed to not, which I feel is a great achievement.\nSoundgarden has been praised for its technical musical ability, and the expansion of its sound as the band's career progressed. \"Heavy yet ethereal, powerful yet always-in-control, Soundgarden's music was a study in contrasts,\" said Henry Wilson of \"Hit Parader\". Wilson proclaimed the band's music as \"a brilliant display of technical proficiency tempered by heart-felt emotion\".\nSoundgarden is one of the bands credited with the development of the alternative metal genre, with AllMusic's Stephen Thomas Erlewine stating that \"Soundgarden made a place for heavy metal in alternative rock.\" Ben Ratliff of \"Rolling Stone\" defined Soundgarden as the \"standard-bearers\" of the rock riff during the 1990s.\nSeveral bands and artists from different genres have cited Soundgarden as an influence, including Biffy Clyro, Stabbing Westward, the Dillinger Escape Plan, Cave In, Iceburn, The Fierce and the Dead, and Amy Lee of Evanescence. The members of Pantera have cited Soundgarden's second album \"Louder Than Love\" as one of the inspirations behind the direction change from glam metal to a more heavier groove sound on their 1990 album \"Cowboys from Hell\", which also prompted the band to hire Terry Date to produce it. Metallica guitarist Kirk Hammett has claimed that the riffs for \"Enter Sandman\" were inspired by Soundgarden.\nIn 2017, \"Metal Injection\" ranked Soundgarden at number three on their list of 10 Heaviest Grunge Bands. \"Loudwire\" recognizes Soundgarden as one of the \"big four\" bands of grunge, alongside Alice in Chains, Pearl Jam, and Nirvana.\nAwards and nominations.\nClio Awards\n&lt;templatestyles src=\"Template:Awards table/styles.css\" /&gt;\nMTV Europe Music Awards\n&lt;templatestyles src=\"Template:Awards table/styles.css\" /&gt;\n! scope=\"col\" | Year\n! scope=\"col\" | Nominee / work\n! scope=\"col\" | Award\n! scope=\"col\" | Result\nMTV Video Music Awards\n&lt;templatestyles src=\"Template:Awards table/styles.css\" /&gt;\n! scope=\"col\" | Year\n! scope=\"col\" | Nominee / work\n! scope=\"col\" | Award\n! scope=\"col\" | Result\nNorthwest Area Music Awards\n&lt;templatestyles src=\"Template:Awards table/styles.css\" /&gt;\n! scope=\"col\" | Year\n! scope=\"col\" | Nominee / work\n! scope=\"col\" | Award\n! scope=\"col\" | Result\nRevolver Music Awards\n&lt;templatestyles src=\"Template:Awards table/styles.css\" /&gt;\n! scope=\"col\" | Year\n! scope=\"col\" | Nominee / work\n! scope=\"col\" | Award\n! scope=\"col\" | Result\nRock and Roll Hall of Fame\n&lt;templatestyles src=\"Template:Awards table/styles.css\" /&gt;\n! scope=\"col\" | Year\n! scope=\"col\" | Nominee / work\n! scope=\"col\" | Award\n! scope=\"col\" | Result\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nExternal links.\n&lt;templatestyles src=\"Sister-inline/styles.css\"/&gt; Media related to at Wikimedia Commons"}
{"id": "40322", "revid": "11096", "url": "https://en.wikipedia.org/wiki?curid=40322", "title": "Theodore Beza", "text": "French Calvinist theologian, reformer and scholar (1519\u20131605)\nTheodore Beza (; or \"de Besze\"; 24 June 1519 \u2013 13 October 1605) was a French Calvinist Protestant theologian, reformer and scholar who played an important role in the Protestant Reformation. He was a disciple of John Calvin and lived most of his life in Geneva. Beza succeeded Calvin as the spiritual leader of the Republic of Geneva.\nBiography.\nEarly life.\nTheodore Beza was born on 24 June 1519 in V\u00e9zelay, in the province of Burgundy, France. His father, Pierre de B\u00e8ze, bailiff of V\u00e9zelay, descended from a Burgundian noble family; his mother, Marie Bourdelot, was known for her generosity. Beza's father had two brothers; Nicolas, who was member of the of Paris, and Claude, who was abbot of the Cistercian monastery of Froimont in the diocese of Beauvais. Nicolas, who was unmarried, during a visit to V\u00e9zelay was so pleased with Theodore that, with the permission of his parents, he took him to Paris to educate him there. \nFrom Paris, Theodore was sent to Orl\u00e9ans in December 1528 to receive instruction from the German humanist Melchior Wolmar. He was received into Wolmar's house, and the day on which this took place was afterward celebrated as a second birthday. Beza soon followed his teacher to Bourges, where the latter was called by Duchess Margaret of Angoul\u00eame, sister of King Francis I. At the time, Bourges was the focus of the Reformation movement in France. In 1534, after Francis I issued his edict against ecclesiastical innovations, Wolmar returned to Germany. Beza, in accordance with the wish of his father, went back to Orl\u00e9ans and studied law from 1535 to 1539. The pursuit of law had little attraction for him; he enjoyed more the reading of the ancient classics, especially Ovid, Catullus, and Tibullus.\nHe received a licentiate in law on 11 August 1539, and, as his father desired, went to Paris, where he began to practice. To support him, his relatives had obtained for him two benefices, the proceeds of which amounted to 700 golden crowns a year; and his uncle had promised to make him his successor. Beza spent two years in Paris and gained a prominent position in literary circles. To escape the many temptations to which he was exposed, with the knowledge of two friends, he became engaged in the year 1544 to a young girl of humble descent, Claudine Denosse, promising to publicly marry her as soon as his circumstances would allow it.\nIn 1548, Beza published a collection of Latin poetry, , which made him famous, and he was considered one of the best writers of Latin poetry of his time. But his work attracted unexpected criticism; as Philip Schaff says, \"Prurient minds\u00a0... read between his lines what he never intended to put there, and imagined offences of which he was not guilty even in thought\". Shortly after the publication of his book, Beza fell ill with plague and his illness, it is reported, revealed to him his spiritual needs. Following his recovery, Beza adhered to the Reformed faith, a decision which resulted in a condemnation from the of Paris, the loss of part of his property and the need to leave France. He then resolved to sever his connections of the time, and went to the Republic of Geneva, which, with the Genevan Reformation, had become a place of refuge for adherents of the Reformation. He arrived in Geneva with Claudine on 23 October 1548.\nTeacher at Lausanne.\nTheodore Beza was received in Geneva by John Calvin, who had met him already in Wolmar's house, and was at once married to Claudine. Beza was at a loss for immediate occupation so he went to T\u00fcbingen to see his former teacher Wolmar. On his way home, he visited Pierre Viret at Lausanne, who brought about his appointment as professor of Greek at the Academy of Lausanne in November 1549. Beza served as rector of the academy from 1552 to 1554.\nBeza found time to write a Biblical drama, \"Abraham Sacrifiant\", in which he contrasted Catholicism with Protestantism. The work was well-received. The text of some verses includes directions for musical performance, but no music survives.\nIn 1551, Beza was asked by Calvin to complete the French metrical translations of the Psalms begun by Cl\u00e9ment Marot. Thirty-four of his translations were published in the 1551 edition of the Genevan Psalter, and six more were added to later editions. About the same time he published \"Passavantius\", a satire directed against Pierre Lizet, the former president of the Parlement of Paris, and principal originator of the \"fiery chamber\" (), who, at the time (1551), was abbot of St. Victor near Paris and publishing a number of polemical writings.\nOf a more serious character were two controversies in which Beza was involved at this time: The first concerned the doctrine of predestination and the controversy of Calvin with Jerome Hermes Bolsec and the second referred to the execution of Michael Servetus at Geneva on 27 October 1553. In defense of Calvin and the Genevan magistrates, Beza published, in 1554, the work (translated into French in 1560).\nJourneys on behalf of the Protestants.\nIn 1557, Beza took a special interest in the Waldensians of Piedmont, Italy, who were being harassed by the French government. On their behalf, he went with William Farel to Bern, Z\u00fcrich, Basel, and Schaffhausen, then to Strasburg, M\u00f6mpelgard, Baden, and G\u00f6ppingen. In Baden and G\u00f6ppingen, Beza and Farel made a declaration concerning the Waldensians' views on the sacrament on 14 May 1557. The written declaration clearly stated their position and was well received by the Lutheran theologians, but was strongly disapproved of in Bern and Zurich.\nIn the autumn of 1558, Beza undertook a second journey with Farel to Worms by way of Strasburg in the hopes of bringing about an intercession by the Evangelical princes of the empire in favor of the persecuted brethren at Paris. With Melanchthon and other theologians then assembled at the Colloquy of Worms, Beza proposed a union of all Protestant Christians, but the proposal was decidedly denied by Zurich and Bern.\nFalse reports reached the German princes that the hostilities against the Huguenots in France had ceased and no embassy was sent to the court of France. As a result, Beza undertook another journey with Farel, Johannes Buddaeus, and Gaspard Carmel to Strasburg and Frankfurt, where the sending of an embassy to Paris was resolved upon.\nSettling in Geneva.\nUpon his return to Lausanne, Beza was greatly disturbed. In union with many ministers and professors in city and country, Viret at last thought of establishing a consistory and of introducing a church discipline which should apply excommunication especially at the celebration of the communion. But the Bernese, then in control of Lausanne, would have no Calvinistic church government. This caused many difficulties, and Beza thought it best in 1558 to settle at Geneva. Here he was given chair of Greek in the newly established academy, and after Calvin's death also that of theology. He was also obliged to preach.\nHe completed the revision of Pierre Olivetan's translation of the New Testament, begun some years before. In 1559, he undertook another journey in the interest of the Huguenots, this time to Heidelberg. At about the same time, he had to defend Calvin against Joachim Westphal in Hamburg and Tilemann Heshusius.\nMore important than this polemical activity was Beza's statement of his own confession. It was originally prepared for his father in justification of his actions and published in revised form to promote Evangelical knowledge among Beza's countrymen. It was printed in Latin in 1560 with a dedication to Wolmar. An English translation was published at London in 1563, 1572, and 1585. Translations into German, Dutch, and Italian were also issued.\nEvents of 1560\u20131563.\nIn the meantime, things took such shape in France that the happiest future for Protestantism seemed possible. King Antoine of Navarre, yielding to the urgent requests of Evangelical noblemen, declared his willingness to listen to a prominent teacher of the Church. Beza, a French nobleman and head of the academy in the metropolis of French Protestantism, was invited to Castle Nerac, but he could not plant the seed of Evangelical faith in the heart of the king.\nIn the following year, 1561, Beza represented the Evangelicals at the Colloquy of Poissy, and in an eloquent manner defended the principles of the Evangelical faith. The colloquy was without result, but Beza as the head and advocate of all Reformed congregations of France was revered and hated at the same time. The queen insisted upon another colloquy, which was opened at St. Germain 28 January 1562, eleven days after the proclamation of the famous January edict, which granted important privileges to those of the Reformed faith. But the colloquy was broken off when it became evident that the Catholic party was preparing (after the Massacre of Vassy, on 1 March) to overthrow Protestantism.\nBeza hastily issued a circular letter (25 March) to all Reformed congregations of the empire, and went to Orl\u00e9ans with the Huguenot leader Conde and his troops. It was necessary to proceed quickly and energetically. But there were neither soldiers nor money. At the request of Conde, Beza visited all Huguenot cities to obtain both. He also wrote a manifesto in which he argued the justice of the Reformed cause. As one of the messengers to collect soldiers and money among his coreligionists, Beza was appointed to visit England, Germany, and Switzerland. He went to Strasburg and Basel, but met with failure. He then returned to Geneva, which he reached on 4 September. He had hardly been there fourteen days when he was called once more to Orl\u00e9ans by D'Andelot. The campaign was becoming more successful; but the publication of the unfortunate edict of pacification which Conde accepted (12 March 1563) filled Beza and all Protestant France with horror.\nCalvin's successor.\nFor 22 months Beza had been absent from Geneva, and the interests of school and Church there and especially the condition of Calvin made it necessary for him to return, as there was no one to take the place of Calvin, who was sick and unable to work. Calvin and Beza arranged to perform their duties jointly in alternate weeks, but the death of Calvin occurred soon afterward (27 May 1564). As a matter of course Beza was his successor.\nUntil 1580, Beza was not only moderator of the Company of Pastors, but also the real soul of the Academy of Geneva which Calvin had founded in 1559. As long as he lived, Beza was interested in higher education. The Protestant youth for nearly forty years thronged his lecture-room to hear his theological lectures, in which he expounded the purest Calvinistic orthodoxy. As a counselor he was listened to by both magistrates and pastors. He founded the Academy's law faculty in which notable jurists such as Fran\u00e7ois Hotman, Giulio Pace, Lambert Daneau, and Denis Godefroy, lectured in turn.\nCourse of events after 1564.\nBeza successfully continued Calvin\u2019s work within the Genevan church, helped by the city authorities having largely adopted Calvin\u2019s ideas by this time with no major doctrinal disputes, though there were debates on more practical issues such as the authority of magistrates over pastors, freedom in preaching, and the balance between ministerial freedom and the authority of the Company of Pastors. Beza on his part was less protective of the independence of the church from the city authorities than Calvin had been. \nBeza did not believe it wise for the Company of Pastors to have a permanent head. He convinced the Company to petition the Small Council to have limited terms for the position of moderator. In 1580 the Council agreed to a weekly rotating presidency. He mediated between the and the magistracy; the latter continually asked his advice even in political questions. He corresponded with all the leaders of the Reformed party in Europe. After the St. Bartholomew's Day Massacre (1572), he used his influence to give to the refugees a hospitable reception at Geneva.\nIn 1574, he wrote his (Right of Magistrates), in which he emphatically protested against tyranny in religious matters, and affirmed that it is legitimate for a people to oppose an unworthy magistracy in a practical manner and if necessary to use weapons and depose them.\nWithout being a great dogmatician like his master, nor a creative genius in the ecclesiastical realm, Beza had qualities which made him famous as humanist, exegete, orator, and leader in religious and political affairs, and qualified him to be the guide of the Calvinists in all Europe. In the various controversies into which he was drawn, Beza often showed an excess of irritation and intolerance, from which Bernardino Ochino, pastor of the Italian congregation at Zurich (on account of a treatise which contained some objectionable points on polygamy), and Sebastian Castellio at Basel (on account of his Latin and French translations of the Bible) had especially to suffer.\nBeza continued to maintain the closest relations with Reformed France. He was the moderator of the general synod which met in April 1571 at La Rochelle and decided not to abolish church discipline or to acknowledge the civil government as head of the Church, as the Paris minister Jean Morel and the philosopher Pierre Ramus demanded; it also decided to confirm anew the Calvinistic doctrine of the Lord's Supper (by the expression: \"substance of the body of Christ\") against Zwinglianism, which caused a dispute between Beza and Ramus and Heinrich Bullinger.\nIn May 1572 he took an important part in the national synod at N\u00eemes. He was also interested in the controversies which concerned the Augsburg Confession in Germany, especially after 1564, on the doctrine of the Person of Christ and the sacrament, and published several works against Joachim Westphal, Tilemann Heshusius, Nikolaus Selnecker, Johannes Brenz, and Jakob Andrea. This caused him to be hated by all those who adhered to Lutheranism in opposition to Melanchthon, especially after 1571.\nThe Colloquy of Montb\u00e9liard.\nThe last polemical conflict of importance Beza encountered from the Lutherans was at the Colloquy of Montb\u00e9liard, 14\u201327 March 1586, (which is also called the Mompelgard Colloquium) to which he had been invited by the Lutheran Count Frederick of W\u00fcrttemberg at the wish of the French-speaking and Reformed residents as well as by French noblemen who had fled to Montb\u00e9liard. As a matter of course the intended union which was the purpose of the colloquy was not brought about; nevertheless it called forth serious developments within the Reformed Church.\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;On the Lutheran side appeared Andrea and Lucas Osiander, assisted by the two political counsellors, Hans Wolf von Anweil and Frederich Schiitz; on the part of the Reformed, Beza, Abraham Musculus (pastor at Berne), Anton Fajus (deacon at Geneva), Peter Hybner (professor of the Greek language at Berne), Claudius Alberius (professor of philosophy at Lausanne), and the two counsellors, Samuel Meyer, of Berne, and Anton Marisius, of Geneva.\nWhen the edition of the acts of the colloquy, as prepared by Jakob Andrea, was published, Samuel Huber, of Burg near Bern, who belonged to the Lutheranizing faction of the Swiss clergy, took so great offense at the supralapsarian doctrine of predestination propounded at Montb\u00e9liard by Beza and Musculus that he felt it to be his duty to denounce Musculus to the magistrates of Bern as an innovator in doctrine. To adjust the matter, the magistrates arranged a colloquy between Huber and Musculus (2 September 1587), in which the former represented the universalism, the latter the particularism, of grace.\nAs the colloquy was resultless, a debate was arranged at Bern on 15\u201318 April 1588, at which the defense of the accepted system of doctrine was at the start put into Beza's hands. The three delegates of the Swiss cantons who presided at the debate declared in the end that Beza had substantiated the teaching propounded at Montb\u00e9liard as the orthodox one, and Huber was dismissed from his office.\nLater years.\nAfter that time Beza's activity was confined more and more to the affairs of his home. His wife Claudine had died childless in 1588 after forty years of marriage, a few days before he went to the Bern Disputation. He contracted, on the advice of his friends, a second marriage with the widow Caterina del Piano, a Protestant refugee from Asti, Piedmont, in order to have a helpmate in his declining years. Up to his sixty-fifth year he enjoyed excellent health, but after that a gradual sinking of his vitality became perceptible. He was active in teaching until January 1597.\nThe saddest experience in his old days was the conversion of King Henry IV of France to Catholicism, in spite of his most earnest exhortations (1593). In 1596, a false report was spread by the Jesuits in Germany, France, England, and Italy that Beza and the Church of Geneva had returned into the bosom of Rome, and Beza replied in a satire which showed that he still possessed his old fire of thought and vigor of expression.\nBeza died on 13 October 1605 in Geneva. He was not buried, like Calvin, in the general cemetery at Plainpalais (for the Savoyards had threatened to abduct his body to Rome), but, at the direction of the magistrates, at Saint-Pierre Cathedral.\nLiterary works.\nHumanistic and historical writings.\nIn Beza's literary activity as well as in his life, distinction must be made between the period of the humanist (which ended with the publication of his \"Juvenilia\") and that of the ecclesiastic. Combining his pastoral and literary gifts, Beza wrote the first drama produced in French, \"Abraham Sacrifiant\"; a play that is an antecedent to the work of Racine and is still occasionally produced today. Later productions like the humanistic, biting, satirical \"Passavantius\" and his \"Complainte de Messire Pierre Lizet...\" prove that in later years he occasionally went back to his first love. In his old age he published his \"Cato censorius\" (1591), and revised his \"Poemata\", from which he purged juvenile eccentricities.\nOf his historiographical works, aside from his \"Icones\" (1580), which have only an iconographical value, mention may be made of the famous \"Histoire ecclesiastique des Eglises reformes au Royaume de France\" (1580), and his biography of Calvin, with which must be named his edition of Calvin's \"Epistolae et responsa\" (1575).\nTheological works.\nBut all these humanistic and historical studies are surpassed by his theological productions (contained in \"Tractationes theologicae\"). In these Beza appears the perfect pupil or the \"alter ego\" of Calvin. His view of life is deterministic and the basis of his religious thinking is the predestinate recognition of the necessity of all temporal existence as an effect of the absolute, eternal, and immutable will of God, so that even the fall of the human race appears to him essential to the divine plan of the world. Beza, in tabular form, thoroughly elucidates the religious views which emanated from a fundamental supralapsarian mode of thought. This he added to his highly instructive treatise \"Summa totius Christianismi.\"\nBeza's \"De vera excommunicatione et Christiano presbyterio\" (1590), written as a response to Thomas Erastus's \"Explicatio gravissimae quaestionis utrum excommunicatio\" (1589) contributed an important defense of the right of ecclesiastical authorities (rather than civil authorities) to excommunicate.\nBeza's Greek New Testament.\nOf no less importance are the contributions of Beza to Biblical scholarship. In 1565 he issued an edition of the Greek New Testament, accompanied in parallel columns by the text of the Vulgate and a translation of his own (already published as early as 1556, though our earliest extant edition dates to https://). Annotations were added, also previously published, but now he greatly enriched and enlarged them.\nIn the preparation of this edition of the Greek text, but much more in the preparation of the second edition which he brought out in 1582, Beza may have availed himself of the help of two very valuable manuscripts. One is known as the \"Codex Bezae\" or \"Cantabrigensis,\" and was later presented by Beza to the University of Cambridge, where it remains in the Cambridge University Library; the second is the \"Codex Claromontanus\", which Beza had found in Clermont (now in the Biblioth\u00e8que Nationale de France in Paris).\nIt was not, however, to these sources that Beza was chiefly indebted, but rather to the previous edition of the eminent Robert Estienne (1550), itself based in great measure upon one of the later editions of Erasmus. Beza's labours in this direction were exceedingly helpful to those who came after. The same thing may be asserted with equal truth of his Latin version and of the copious notes with which it was accompanied. The former is said to have been published over a hundred times.\nAlthough some contend that Beza's view of the doctrine of predestination exercised an overly dominant influence upon his interpretation of the Scriptures, there is no question that he added much to a clear understanding of the New Testament.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "40323", "revid": "14751539", "url": "https://en.wikipedia.org/wiki?curid=40323", "title": "Inertial confinement fusion", "text": "Branch of fusion energy research\nInertial confinement fusion (ICF) is a fusion energy process that initiates nuclear fusion reactions by compressing and heating targets filled with fuel. The targets are small pellets, typically containing deuterium (2H) and tritium (3H).\nTypically, short pulse lasers deposit energy on a hohlraum. Its inner surface vaporizes, releasing X-rays. These converge on the pellet's exterior, turning it into a plasma. This produces a reaction force in the form of shock waves that travel through the target. The waves compress and heat it. Sufficiently powerful shock waves achieve the Lawson criterion for fusion of the fuel.\nICF is one of two major branches of fusion research; the other is magnetic confinement fusion (MCF). When first proposed in the early 1970s, ICF appeared to be a practical approach to power production and the field flourished. Experiments demonstrated that the efficiency of these devices was much lower than expected. Throughout the 1980s and '90s, experiments were conducted in order to understand the interaction of high-intensity laser light and plasma. These led to the design of much larger machines that achieved ignition-generating energies. Nonetheless, MCF currently dominates power-generation approaches.\nUnlike MCF, ICF has direct dual-use applications to the study of thermonuclear weapon detonation. For nuclear states, ICF forms a component of stockpile stewardship. This allows the allocation of not only scientific but military funding.\nCalifornia's Lawrence Livermore National Laboratory has dominated ICF history, and operates the largest ICF experiment, the National Ignition Facility (NIF). In 2022, an NIF deuterium-tritium shot yielded 3.15 megajoules (MJ) from a delivered energy of 2.05 MJ, the first time that any fusion device produced an energy gain factor above one.\n&lt;templatestyles src=\"Template:TOC limit/styles.css\" /&gt;\nDescription.\nFusion basics.\nFusion reactions combine smaller atoms to form larger ones. This occurs when two atoms (or ions, atoms stripped of their electrons) come close enough to each other that the nuclear force dominates the electrostatic force that otherwise keeps them apart. Overcoming electrostatic repulsion requires kinetic energy sufficient to overcome the \"Coulomb barrier\" or \"fusion barrier\".\nLess energy is needed to cause lighter nuclei to fuse, as they have less electrical charge and thus a lower barrier energy. Thus the barrier is lowest for hydrogen. Conversely, the nuclear force increases with the number of nucleons, so isotopes of hydrogen that contain additional neutrons reduce the required energy. The easiest fuel is a mixture of 2H, and 3H, known as D-T.\nThe odds of fusion occurring are a function of the fuel density and temperature and the length of time that the density and temperature are maintained. Even under ideal conditions, the chance that a D and T pair fuse is very small. Higher density and longer times allow more encounters among the atoms. This cross section is further dependent on individual ion energies. This combination, the fusion triple product, must reach the Lawson criterion, to reach ignition.\nThermonuclear devices.\nThe first ICF devices were the hydrogen bombs invented in the early 1950s. A hydrogen bomb consists of two bombs in a single case. The first, the \"primary stage\", is a fission-powered device normally using plutonium. When it explodes it gives off a burst of thermal X-rays that fill the interior of the bomb casing. These X-rays are absorbed by a special material (like Fogbank) surrounding the \"secondary stage\", which consists of fusion fuel, sandwiched between a fission fuel sparkplug and tamper. The X-rays heat this secondary and initiate further fission. Due to Newton's third law, this causes the fuel inside to be driven inward, compressing and heating it. This causes the fusion fuel to reach the temperature and density where fusion reactions begin.\nIn the case of D-T fuel, most of the energy is released in the form of alpha particles and neutrons. Under normal conditions, an alpha can travel about 10\u00a0mm through the fuel, but in the ultra-dense conditions in the compressed fuel, they can travel about 0.01\u00a0mm before their electrical charge, interacting with the surrounding plasma, causes them to lose their speed. This means the majority of the energy released by the alphas is redeposited in the fuel. This transfer of kinetic energy heats the surrounding particles to the energies they need to undergo fusion. This process causes the fusion fuel to burn outward from the center. The electrically neutral neutrons travel longer distances in the fuel mass and do not contribute to this self-heating process. In a bomb, they are instead used to either breed tritium through reactions in a lithium-deuteride fuel, or are used to split additional fissionable fuel surrounding the secondary stage, often part of the bomb casing.\nThe requirement that the reaction has to be sparked by a fission bomb makes this method impractical for power generation. Not only would the fission triggers be expensive to produce, but the minimum size of such a bomb is large, defined roughly by the critical mass of the plutonium fuel used. Generally, it seems difficult to build efficient nuclear fission devices much smaller than about 1 kiloton in yield, and the fusion secondary would add to this yield. This makes it a difficult engineering problem to extract power from the resulting explosions. Project PACER studied solutions to the engineering issues, but also demonstrated that it was not economically feasible. The cost of the bombs was far greater than the value of the resulting electricity.\nMechanism of action.\nThe energy needed to overcome the Coulomb barrier corresponds to the energy of the average particle in a gas heated to 100 million K. The specific heat of hydrogen is about 14 Joule per gram-K, so considering a 1 milligram fuel pellet, the energy needed to raise the mass as a whole to this temperature is 1.4 megajoules (MJ).\nIn the more widely developed magnetic fusion energy (MFE) approach, confinement times are on the order of one second. However, plasmas can be sustained for minutes. In this case the confinement time represents the amount of time it takes for the energy from the reaction to be lost to the environment - through a variety of mechanisms. For a one-second confinement, the density needed to meet the Lawson criterion is about 1014 particles per cubic centimetre (cc). For comparison, air at sea level has about 2.7 x 1019 particles/cc, so the MFE approach has been described as \"a good vacuum\".\nConsidering a 1 milligram drop of D-T fuel in liquid form, the size is about 1\u00a0mm and the density is about 4 x 1020/cc. Nothing holds the fuel together. Heat created by fusion events causes it to expand at the speed of sound, which leads to a confinement time around 2 x 10\u221210 seconds. At liquid density the required confinement time is about 2 x 10\u22127s. In this case only about 0.1 percent of the fuel fuses before the drop blows apart.\nThe rate of fusion reactions is a function of density, and density can be improved through compression. If the drop is compressed from 1\u00a0mm to 0.1\u00a0mm in diameter, the confinement time drops by the same factor of 10, because the particles have less distance to travel before they escape. However, the density, which is the cube of the dimensions, increases by 1,000 times. This means the overall rate of fusion increases 1,000 times while the confinement drops by 10 times, a 100-fold improvement. In this case 10% of the fuel undergoes fusion; 10% of 1\u00a0mg of fuel produces about 30\u00a0MJ of energy, 30 times the amount needed to compress it to that density.\nThe other key concept in ICF is that the entire fuel mass does not have to be raised to 100 million K. In a fusion bomb the reaction continues because the alpha particles released in the interior heat the fuel around it. At liquid density the alphas travel about 10\u00a0mm and thus their energy escapes the fuel. In the 0.1\u00a0mm compressed fuel, the alphas have a range of about 0.016\u00a0mm, meaning that they will stop within the fuel and heat it. In this case a \"propagating burn\" can be caused by heating only the center of the fuel to the needed temperature. This requires far less energy; calculations suggested 1\u00a0kJ is enough to reach the compression goal.\nSome method is needed to heat the interior to fusion temperatures, and do so while when the fuel is compressed and the density is high enough. In modern ICF devices, the density of the compressed fuel mixture is as much as one-thousand times the density of water, or one-hundred times that of lead, around 1000 g/cm3. Much of the work since the 1970s has been on ways to create the central hot-spot that starts off the burning, and dealing with the many practical problems in reaching the desired density.\nHeating concepts.\nEarly calculations suggested that the amount of energy needed to ignite the fuel was very small, but this does not match subsequent experience.\nHot spot ignition.\nThe initial solution to the heating problem involved deliberate \"shaping\" of the energy delivery. The idea was to use an initial lower-energy pulse to vaporize the capsule and cause compression, and then a very short, very powerful pulse near the end of the compression cycle. The goal is to launch shock waves into the compressed fuel that travel inward to the center. When they reach the center they meet the waves coming in from other sides. This causes a brief period where the density in the center reaches much higher values, over 800\u00a0g/cm3.\nThe central hot spot ignition concept was the first to suggest ICF was not only a practical route to fusion, but relatively simple. This led to numerous efforts to build working systems in the early 1970s. These experiments revealed unexpected loss mechanisms. Early calculations suggested about 4.5x107\u00a0J/g would be needed, but modern calculations place it closer to 108\u00a0J/g. Greater understanding led to complex shaping of the pulse into multiple time intervals.\nDirect vs. indirect drive.\nIn the simplest method of inertial confinement, the fuel is arranged as a sphere. This allows it to be compressed uniformly from all sides. To produce the inward force, the fuel is placed within a thin capsule that absorbs energy from the driver beams, causing the capsule shell to explode outward. The capsule shell is usually made of a lightweight plastic fabricated using plasma polymerization, and the fuel is deposited as a layer on the inside by injecting or diffusing the gaseous fuel into the shell and then freezing it.\nShining the driver beams directly onto the fuel capsule is known as \"direct drive\". The implosion process must be extremely uniform in order to avoid asymmetry due to Rayleigh\u2013Taylor instability and similar effects. For a beam energy of 1\u00a0MJ, the fuel capsule cannot be larger than about 2\u00a0mm before these effects disrupt the implosion symmetry. This limits the size of the laser beams to a diameter so narrow that it is difficult to achieve in practice.\nOn the other hand, \"indirect drive\" illuminates a small cylinder of heavy metal, often gold or lead, known as a hohlraum. The beam energy heats the hohlraum until it emits X-rays. These X-rays fill the interior of the hohlraum and heat the capsule. The advantage of indirect drive is that the beams can be larger and less accurate. The disadvantage is that much of the delivered energy is used to heat the hohlraum until it is \"X-ray hot\", so the end-to-end energy efficiency is much lower than the direct drive method.\nWithin the direct inertial confinement fusion scheme, there are two alternative approaches: shock ignition and fast ignition. In both cases the compression and heating processes are separated. First, a set of driver lasers compress the fuel up to an optimal point were the plasma is condensed and found in a stagnation state, this is, it has approximately homogenous temperature and density at its core. Then, another mechanism heates the plasma up to fusion conditions. \nShock ignition.\nProposed by C. Zhou and R. Betti, after an early compression phase similar to that of the direct drive approach, an additional driver is applied (such as a laser, electron beam, or similar pulse). This create a shock wave orders of magnitude stronger. The separation of the compression process from the final heating, where ignition is achieved, offers the advantage of reducing the compression requirements and utilizing more efficient energy deposition mechanisms. Additionally, some theoretical and experimental findings claim that these approach enhances ignition conditions, as demonstrated, for instance, at the OMEGA laser at the University of Rochester. This increases the efficiency of the process while lowering the overall amount of power required.\nFast ignition.\nFast ignition is a promising alternative for achieving nuclear fusion within the inertial confinement fusion scheme. Similar to the shock ignition scheme, fast ignition divides the fusion process into two distinct steps: compression and heating, each of which can be optimized independently. After the precompression phase, a powerful particle beam is used to provide additional energy directly to the core of the fuel. It is important to note that, in fast ignition, this relies on a separate and rapid heating pulse, while shock ignition primarily employs shock waves to achieve ignition. The beam applied creates a heated volume within the plasma. If any region of such volume is able to ignite the nuclear fusion process, then, the burning will start and spread to the rest of the fuel. \nThe two types of fast ignition are the \"plasma bore-through\" method and the \"cone-in-shell\" method. In plasma bore-through, a preceding laser bores through the outer plasma of the imploding capsule (the corona), before the last beam shot. In the cone-in-shell method, the capsule is mounted on the end of a small high-z (high atomic number) cone such that the tip of the cone projects into the core. In this second method, when the capsule is imploded, the beam has a clear view of the core and does not use energy to bore through the 'corona' plasma. However, the presence of the cone affects the implosion process in significant ways that are not fully understood. In tests, this approach presents difficulties, because the laser pulse had to reach the center at a precise moment, while the center is obscured by debris and free electrons from the compression pulse. A variation of this cone approach incorporates a small pellet of fuel at the apex of the device, initiating a preliminary pre-explosion that also moves inward towards the larger fuel mass. \nRegarding the power beam, the original proposal for fast ignition involved an electron-based scheme. However, it was limited by the high electron divergences, kinetic energy constraints and sensitivity. Meanwhile, fast ignition by laser-driven ion beams, offers a much more localized energy deposition, a stiffer ion transport, with the possibility of beam focusing, and a better understood and controlled ion-plasma interaction. At first, the proposed projectiles of the beam were light ions, such as protons. However, these ions deposit most of their energy at the edge of the fuel, resulting in an asymmetrical geometry of the heated plasma. Later, heavier projectiles were suggested. Their interaction with the plasma is semi-transparent at the edge, allowing for deposition of most of their energy in the centre of the fuel, which optimises a symmetrical propagation and explosion. The ion beam used for the final ignition can be optimized, in order to achieve the desired conditions for the plasma and the burning, and to reduce system requirements. \nCurrently, several research facilities worldwide are actively experimenting with Fast Ignition nuclear fusion, notably: the High Power Laser Energy Research Facility (HiPer), located across multiple institutions in Europe. HiPer is a proposed \u00a3500 million facility by the European Union. Compared to NIF's 2\u00a0MJ UV beams, HiPER's driver was planned to be 200\u00a0kJ and heater 70\u00a0kJ, although the predicted fusion gains are higher than NIF. It was to employ diode lasers, which convert electricity into laser light with much higher efficiency and run cooler. This allows them to operate at much higher frequencies. HiPER proposed to operate at 1 MJ at 1\u00a0Hz, or alternately 100 kJ at 10\u00a0Hz. The project's final update was in 2014. It was expected to offer a higher \"Q\" with a 10x reduction in construction costs times. Several other projects are currently underway to explore fast ignition, including upgrades to the OMEGA laser at the Laboratory for Laser Energetics (LLE) in the University of Rochester and the GEKKO XII device at the Institute of Laser Engineering (ILE) in Osaka, Japan. Nonetheless, fast ignition faces its particular challenges, such as achieving an optimal deposition of energy in the target, avoiding unnecessary losses and properly transporting the fast electrons or ions through the plasma without creating divergences or instabilities.\nPolymer fuel capsule fabrication.\nFor fuel capsules constructed using glow-discharge polymer (GDP) via plasma polymerization, outer diameters can range from 900\u00a0\u03bcm (typical for the OMEGA laser system at the Laboratory for Laser Energetics) to 2mm (typical for the NIF laser at the Lawrence Livermore National Laboratory.\nThe process for producing GDP capsules begins with a bubble of poly(\u03b1-methylstyrene) (PAMS) that serves as a decomposable mandrel. Next, the bubble is coated with GDP to the desired thickness. Finally, the coated bubble is heated in an inert atmosphere. Upon reaching 300\u00a0\u00b0C, the PAMS bubble decomposes into its monomers and diffuses out of the coating, leaving only a hollow sphere of the GDP coating.\nGDP lends itself to inertial fusion capsules\u2014especially those used in direct-drive configurations\u2014due to its ability to create low-defect, uniform thin films that are permeable to deuterium and tritium fuel. Permeating the fuel into the capsule precludes the need for drilling into the capsule to facilitate fuel injection, reducing the overall fusion target complexity and asymmetry. The rigorous uniformity and sphericity requirements of direct drive fusion experiments result in GDP being favored over other capsule materials. Additionally, the GDP layers can be doped with different elements to provide diagnostic signals or prevent preheating of the fuel.\nChallenges.\nThe primary challenges with increasing ICF performance are:\nIn order to focus the shock wave on the center of the target, the target must be made with great precision and sphericity with tolerances of no more than a few micrometres over its (inner and outer) surface. The lasers must be precisely targeted in space and time. Beam timing is relatively simple and is solved by using delay lines in the beams' optical path to achieve picosecond accuracy. The other major issue is so-called \"beam-beam\" imbalance and beam anisotropy. These problems are, respectively, where the energy delivered by one beam may be higher or lower than other beams impinging and of \"hot spots\" within a beam diameter hitting a target which induces uneven compression on the target surface, thereby forming Rayleigh-Taylor instabilities in the fuel, prematurely mixing it and reducing heating efficacy at the instant of maximum compression. The Richtmyer-Meshkov instability is also formed during the process due to shock waves.\nThese problems have been mitigated by beam smoothing techniques and beam energy diagnostics; however, RT instability remains a major issue. Modern cryogenic hydrogen ice targets tend to freeze a thin layer of deuterium on the inside of the shell while irradiating it with a low power infrared laser to smooth its inner surface and monitoring it with a microscope equipped camera, thereby allowing the layer to be closely monitored. Cryogenic targets filled with D-T are \"self-smoothing\" due to the small amount of heat created by tritium decay. This is referred to as \"beta-layering\".\nIn the indirect drive approach, the absorption of thermal x-rays by the target is more efficient than the direct absorption of laser light. However, the hohlraums take up considerable energy to heat, significantly reducing energy transfer efficiency. Most often, indirect drive hohlraum targets are used to simulate thermonuclear weapons tests due to the fact that the fusion fuel in weapons is also imploded mainly by X-ray radiation.\nICF drivers are evolving. Lasers have scaled up from a few joules and kilowatts to megajoules and hundreds of terawatts, using mostly frequency doubled or tripled light from neodymium glass amplifiers.\nHeavy ion beams are particularly interesting for commercial generation, as they are easy to create, control, and focus. However, it is difficult to achieve the energy densities required to implode a target efficiently, and most ion-beam systems require the use of a hohlraum surrounding the target to smooth out the irradiation.\nHistory.\nConception.\nUnited States.\nICF history began as part of the \"Atoms For Peace\" conference in 1957. This was an international, UN-sponsored conference between the US and the Soviet Union. Some thought was given to using a hydrogen bomb to heat a water-filled cavern. The resulting steam could then be used to power conventional generators, and thereby provide electrical power.\nThis meeting led to Operation Plowshare, formed in June 1957 and formally named in 1961. It included three primary concepts; energy generation under Project PACER, the use of nuclear explosions for excavation, and for fracking in the natural gas industry. PACER was directly tested in December 1961 when the 3\u00a0kt Project Gnome device was detonated in bedded salt in New Mexico. While the press looked on, radioactive steam was released from the drill shaft, at some distance from the test site. Further studies designed engineered cavities to replace natural ones, but Plowshare turned from bad to worse, especially after the failure of 1962's Sedan which produced significant fallout. PACER continued to receive funding until 1975, when a 3rd party study demonstrated that the cost of electricity from PACER would be ten times the cost of conventional nuclear plants.\nAnother outcome of Atoms For Peace was to prompt John Nuckolls to consider what happens on the fusion side of the bomb as fuel mass is reduced. This work suggested that at sizes on the order of milligrams, little energy would be needed to ignite the fuel, much less than a fission primary. He proposed building, in effect, tiny all-fusion explosives using a tiny drop of D-T fuel suspended in the center of a hohlraum. The shell provided the same effect as the bomb casing in an H-bomb, trapping x-rays inside to irradiate the fuel. The main difference is that the X-rays would be supplied by an external device that heated the shell from the outside until it was glowing in the x-ray region. The power would be delivered by a then-unidentified pulsed power source he referred to, using bomb terminology, as the \"primary\".\nThe main advantage to this scheme is the fusion efficiency at high densities. According to the Lawson criterion, the amount of energy needed to heat the D-T fuel to break-even conditions at ambient pressure is perhaps 100 times greater than the energy needed to compress it to a pressure that would deliver the same rate of fusion. So, in theory, the ICF approach could offer dramatically more gain. This can be understood by considering the energy losses in a conventional scenario where the fuel is slowly heated, as in the case of magnetic fusion energy; the rate of energy loss to the environment is based on the temperature difference between the fuel and its surroundings, which continues to increase as the fuel temperature increases. In the ICF case, the entire hohlraum is filled with high-temperature radiation, limiting losses.\nGermany.\nIn 1956 a meeting was organized at the Max Planck Institute in Germany by fusion pioneer Carl Friedrich von Weizs\u00e4cker. At this meeting Friedwardt Winterberg proposed the non-fission ignition of a thermonuclear micro-explosion by a convergent shock wave driven with high explosives. Further reference to Winterberg's work in Germany on nuclear micro explosions (mininukes) is contained in a declassified report of the former East German Stasi (Staatsicherheitsdienst).\nIn 1964 Winterberg proposed that ignition could be achieved by an intense beam of microparticles accelerated to a speed of 1000\u00a0km/s. In 1968, he proposed to use intense electron and ion beams generated by Marx generators for the same purpose. The advantage of this proposal is that charged particle beams are not only less expensive than laser beams, but can entrap the charged fusion reaction products due to the strong self-magnetic beam field, drastically reducing the compression requirements for beam ignited cylindrical targets.\nUSSR.\nIn 1967, research fellow Gurgen Askaryan published an article proposing the use of focused laser beams in the fusion of lithium deuteride or deuterium.\nEarly research.\nThrough the late 1950s, and collaborators at Lawrence Livermore National Laboratory (LLNL) completed computer simulations of the ICF concept. In early 1960, they performed a full simulation of the implosion of 1\u00a0mg of D-T fuel inside a dense shell. The simulation suggested that a 5 MJ power input to the hohlraum would produce 50 MJ of fusion output, a gain of 10x. This was before the laser and a variety of other possible drivers were considered, including pulsed power machines, charged particle accelerators, plasma guns, and hypervelocity pellet guns.\nTwo theoretical advances advanced the field. One came from new simulations that considered the timing of the energy delivered in the pulse, known as \"pulse shaping\", leading to better implosion. The second was to make the shell much larger and thinner, forming a thin shell as opposed to an almost solid ball. These two changes dramatically increased implosion efficiency and thereby greatly lowered the required compression energy. Using these improvements, it was calculated that a driver of about 1 MJ would be needed, a five-fold reduction. Over the next two years, other theoretical advancements were proposed, notably Ray Kidder's development of an implosion system without a hohlraum, the so-called \"direct drive\" approach, and Stirling Colgate and Ron Zabawski's work on systems with as little as 1 \u03bcg of D-T fuel.\nThe introduction of the laser in 1960 at Hughes Research Laboratories in California appeared to present a perfect driver mechanism. However, the maximum power produced by these devices appeared very limited, far below what would be needed. This was addressed with Gordon Gould's introduction of the Q-switching which was applied to lasers in 1961 at Hughes Research Laboratories. Q-switching allows a laser amplifier to be pumped to very high energies without starting stimulated emission, and then triggered to release this energy in a burst by introducing a tiny seed signal. With this technique it appeared any limits to laser power were well into the region that would be useful for ICF.\nStarting in 1962, Livermore's director John S. Foster, Jr. and Edward Teller began a small ICF laser study. Even at this early stage the suitability of ICF for weapons research was well understood and was the primary reason for its funding. Over the next decade, LLNL made small experimental devices for basic laser-plasma interaction studies.\nDevelopment begins.\nIn 1967 Kip Siegel started KMS Industries. In the early 1970s he formed KMS Fusion to begin development of a laser-based ICF system. This development led to considerable opposition from the weapons labs, including LLNL, who put forth a variety of reasons that KMS should not be allowed to develop ICF in public. This opposition was funnelled through the Atomic Energy Commission, which controlled funding. Adding to the background noise were rumours of an aggressive Soviet ICF program, new higher-powered CO2 and glass lasers, the electron beam driver concept, and the energy crisis which added impetus to many energy projects.\nIn 1972 John Nuckolls wrote a paper introducing ICF and suggesting that testbed systems could be made to generate fusion with drivers in the kJ range, and high-gain systems with MJ drivers.\nIn spite of limited resources and business problems, KMS Fusion successfully demonstrated IFC fusion on 1 May 1974. This success was soon followed by Siegel's death and the end of KMS Fusion a year later. By this point several weapons labs and universities had started their own programs, notably the solid-state lasers (Nd:glass lasers) at LLNL and the University of Rochester, and krypton fluoride excimer lasers systems at Los Alamos and the Naval Research Laboratory.\n\"High-energy\" ICF.\nHigh-energy ICF experiments (multi-hundred joules per shot) began in the early 1970s, when better lasers appeared. Funding for fusion research was stimulated by energy crises produced rapid gains in performance, and inertial designs were soon reaching the same sort of \"below break-even\" conditions of the best MCF systems.\nLLNL was, in particular, well funded and started a laser fusion development program. Their Janus laser started operation in 1974, and validated the approach of using Nd:glass lasers for high power devices. Focusing problems were explored in the Long path and Cyclops lasers, which led to the larger Argus laser. None of these were intended to be practical devices, but they increased confidence that the approach was valid. It was then believed that a much larger device of the Cyclops type could both compress and heat targets, leading to ignition. This misconception was based on extrapolation of the fusion yields seen from experiments utilizing the so-called \"exploding pusher\" fuel capsule. During the late 1970s and early 1980s the estimates for laser energy on target needed to achieve ignition doubled almost yearly as plasma instabilities and laser-plasma energy coupling loss modes were increasingly understood. The realization that exploding pusher target designs and single-digit kilojoule (kJ) laser irradiation intensities would never scale to high yields led to the effort to increase laser energies to the 100 kJ level in the ultraviolet band and to the production of advanced ablator and cryogenic DT ice target designs.\nShiva and Nova.\nOne of the earliest large scale attempts at an ICF driver design was the Shiva laser, a 20-beam neodymium doped glass laser system at LLNL that started operation in 1978. Shiva was a \"proof of concept\" design intended to demonstrate compression of fusion fuel capsules to many times the liquid density of hydrogen. In this, Shiva succeeded, reaching 100 times the liquid density of deuterium. However, due to the laser's coupling with hot electrons, premature heating of the dense plasma was problematic and fusion yields were low. This failure to efficiently heat the compressed plasma pointed to the use of optical frequency multipliers as a solution that would frequency triple the infrared light from the laser into the ultraviolet at 351\u00a0nm. Schemes to efficiently triple the frequency of laser light discovered at the Laboratory for Laser Energetics in 1980 was experimented with in the 24 beam OMEGA laser and the NOVETTE laser, which was followed by the Nova laser design with 10 times Shiva's energy, the first design with the specific goal of reaching ignition.\nNova also failed, this time due to severe variation in laser intensity in its beams (and differences in intensity between beams) caused by filamentation that resulted in large non-uniformity in irradiation smoothness at the target and asymmetric implosion. The techniques pioneered earlier could not address these new issues. This failure led to a much greater understanding of the process of implosion, and the way forward again seemed clear, namely to increase the uniformity of irradiation, reduce hot-spots in the laser beams through beam smoothing techniques to reduce Rayleigh\u2013Taylor instabilities and increase laser energy on target by at least an order of magnitude. Funding was constrained in the 1980s.\nNational Ignition Facility.\nThe resulting 192-beam design, dubbed the National Ignition Facility, started construction at LLNL in 1997. NIF's main objective is to operate as the flagship experimental device of the so-called nuclear stewardship program, supporting LLNLs traditional bomb-making role. Completed in March 2009, NIF experiments set new records for power delivery by a laser. As of September 27, 2013, for the first time fusion energy generated was greater than the energy absorbed into deuterium\u2013tritium fuel. In June, 2018 NIF announced record production of 54kJ of fusion energy output. On August 8, 2021 the NIF produced 1.3MJ of output, 25x higher than the 2018 result, generating 70% of the break-even definition of ignition - when energy out equals energy in.\nAs of December 2022, the NIF claims to have become the first fusion experiment to achieve scientific breakeven on December 5, 2022, with an experiment producing 3.15 megajoules of energy from a 2.05 megajoule input of laser light (somewhat less than the energy needed to boil 1\u00a0kg of water) for an energy gain of about 1.5.\nOther projects.\nThe French Laser M\u00e9gajoule achieved its first experimental line in 2002, and its first target shots were conducted in 2014. The machine was roughly 75% complete as of 2016.\nUsing a different approach entirely is the \"z\"-pinch device. \"Z\"-pinch uses massive electric currents switched into a cylinder comprising extremely fine wires. The wires vaporize to form an electrically conductive, high current plasma. The resulting circumferential magnetic field squeezes the plasma cylinder, imploding it, generating a high-power x-ray pulse that can be used to implode a fuel capsule. Challenges to this approach include relatively low drive temperatures, resulting in slow implosion velocities and potentially large instability growth, and preheat caused by high-energy x-rays.\nShock ignition was proposed to address problems with fast ignition. Japan developed the KOYO-F design and laser inertial fusion test (LIFT) experimental reactor. In April 2017, clean energy startup Apollo Fusion began to develop a hybrid fusion-fission reactor technology.\nIn Germany, technology company Marvel Fusion is working on . The startup adopted a short-pulsed high energy laser and the aneutronic fuel pB11. It was founded in Munich 2019. It works with Siemens Energy, TRUMPF, and Thales. The company partnered with Ludwig Maximilian University of Munich in July 2022.\nIn March 2022, Australian company HB11 announced fusion using non-thermal laser pB11, at a higher than predicted rate of alpha particle creation. Other companies include NIF-like Longview Fusion and fast-ignition origned Focused Energy.\nApplications.\nElectricity generation.\nInertial fusion energy (IFE) power plants have been studied since the late 1970s. These devices were to deliver multiple targets per second into the reaction chamber, using the resulting energy to drive a conventional steam turbine.\nTechnical challenges.\nEven if the many technical challenges in reaching ignition were all to be solved, practical problems abound. Given the 1 to 1.5% efficiency of the laser amplification process and that steam-driven turbine systems are typically about 35% efficient, fusion gains would have to be on the order of 125-fold just to energetically break even.\nAn order of magnitude improvement in laser efficiency may be possible through the use of designs that replace flash lamps with laser diodes that are tuned to produce most of their energy in a frequency range that is strongly absorbed. Initial experimental devices offer efficiencies of about 10%, and it is suggested that 20% is possible.\nNIF uses about 330\u00a0MJ to produce the driver beams, producing an expected yield of about 20\u00a0MJ, with maximum credible yield of 45\u00a0MJ.\nPower extraction.\nICF systems face some of the secondary power extraction problems as MCF systems. One of the primary concerns is how to successfully remove heat from the reaction chamber without interfering with the targets and driver beams. Another concern is that the released neutrons react with the reactor structure, mechanically weakening it, and turning it intensely radioactive. Conventional metals such as steel would have a short lifetime and require frequent replacement of the core containment walls. Another concern is fusion afterdamp (debris left in the reaction chamber), which could interfere with subsequent shots, including helium ash produced by fusion, along with unburned hydrogen and other elements used in the fuel pellet. This problem is most troublesome with indirect drive systems. If the driver energy misses the fuel pellet completely and strikes the containment chamber, material could foul the interaction region, or the lenses or focusing elements.\nOne concept, as shown in the HYLIFE-II design, is to use a \"waterfall\" of FLiBe, a molten mix of fluoride salts of lithium and beryllium, which both protect the chamber from neutrons and carry away heat. The FLiBe is passed into a heat exchanger where it heats water for the turbines. The tritium produced by splitting lithium nuclei can be extracted in order to close the power plant's thermonuclear fuel cycle, a necessity for perpetual operation because tritium is rare and otherwise must be manufactured. Another concept, Sombrero, uses a reaction chamber built of carbon-fiber-reinforced polymer which has a low neutron cross section. Cooling is provided by a molten ceramic, chosen because of its ability to absorb the neutrons and its efficiency as a heat transfer agent.\nEconomic viability.\nAnother factor working against IFE is the cost of the fuel. Even as Nuckolls was developing his earliest calculations, co-workers pointed out that if an IFE machine produces 50\u00a0MJ of fusion energy, a shot could produce perhaps 10\u00a0MJ (2.8 kWh) of energy. Wholesale rates for electrical power on the grid were about 0.3 cents/kWh at the time, which meant the monetary value of the shot was perhaps one cent. In the intervening 50 years the real price of power has remained about even, and the rate in 2012 in Ontario, Canada was about 2.8 cents/kWh. Thus, in order for an IFE plant to be economically viable, fuel shots would have to cost considerably less than ten cents in 2012 dollars.\nDirect-drive systems avoid the use of a hohlraum and thereby may be less expensive in fuel terms. However, these systems still require an ablator, and the accuracy and geometrical considerations are critical. The direct-drive approach still may not be less expensive to operate.\nNuclear weapons.\nThe hot and dense conditions encountered during an ICF experiment are similar to those in a thermonuclear weapon, and have applications to nuclear weapons programs. When energy is put into the fuel pellets, the result is shock-wave explosions. With enough shock waves, the fuel pellets combine to form helium, and a free neutron and energy is released. ICF experiments might be used, for example, to help determine how warhead performance degrades as it ages, or as part of a weapons design program. Retaining knowledge and expertise inside the nuclear weapons program is another motivation for pursuing ICF. Funding for the NIF in the United States is sourced from the Nuclear Weapons Stockpile Stewardship program, whose goals are oriented accordingly. It has been argued that some aspects of ICF research violate the Comprehensive Test Ban Treaty or the Nuclear Non-Proliferation Treaty. In the long term, despite the formidable technical hurdles, ICF research could lead to the creation of a \"pure fusion weapon\".\nNeutron source.\nICF has the potential to produce orders of magnitude more neutrons than spallation. Neutrons are capable of locating hydrogen atoms in molecules, resolving atomic thermal motion and studying collective excitations of photons more effectively than X-rays. Neutron scattering studies of molecular structures could resolve problems associated with protein folding, diffusion through membranes, proton transfer mechanisms, dynamics of molecular motors, etc. by modulating thermal neutrons into beams of slow neutrons. In combination with fissile materials, neutrons produced by ICF can potentially be used in Hybrid Nuclear Fusion designs to produce electric power.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nBibliography.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "40324", "revid": "50807092", "url": "https://en.wikipedia.org/wiki?curid=40324", "title": "Quadratic programming", "text": "Solving an optimization problem with a quadratic objective function\nQuadratic programming (QP) is the process of solving certain mathematical optimization problems involving quadratic functions. Specifically, one seeks to optimize (minimize or maximize) a multivariate quadratic function subject to linear constraints on the variables. Quadratic programming is a type of nonlinear programming.\n\"Programming\" in this context refers to a formal procedure for solving mathematical problems. This usage dates to the 1940s and is not specifically tied to the more recent notion of \"computer programming.\" To avoid confusion, some practitioners prefer the term \"optimization\" \u2014 e.g., \"quadratic optimization.\"\nProblem formulation.\nThe quadratic programming problem with n variables and m constraints can be formulated as follows.\nGiven:\nthe objective of quadratic programming is to find an n-dimensional vector x, that will\nwhere xT denotes the vector transpose of x, and the notation \"Ax \u2aaf b means that every entry of the vector \"Ax is less than or equal to the corresponding entry of the vector b (component-wise inequality).\nConstrained least squares.\nAs a special case when \"Q\" is symmetric positive-definite, the cost function reduces to least squares:\nwhere \"Q\" = \"R\"T\"R\" follows from the Cholesky decomposition of \"Q\" and c = \u2212\"R\"T d. Conversely, any such constrained least squares program can be equivalently framed as a quadratic programming problem, even for a generic non-square \"R\" matrix.\nGeneralizations.\nWhen minimizing a function f in the neighborhood of some reference point \"x\"0, Q is set to its Hessian matrix H(\"f\"(x0)) and c is set to its gradient \u2207\"f\"(x0). A related programming problem, quadratically constrained quadratic programming, can be posed by adding quadratic constraints on the variables.\nSolution methods.\nFor general problems a variety of methods are commonly used, including\n*interior point,\n*active set,\n*augmented Lagrangian,\n*conjugate gradient,\n*gradient projection,\n*extensions of the simplex algorithm.\nIn the case in which Q is positive definite, the problem is a special case of the more general field of convex optimization.\nEquality constraints.\nQuadratic programming is particularly simple when Q is positive definite and there are only equality constraints; specifically, the solution process is linear. By using Lagrange multipliers and seeking the extremum of the Lagrangian, it may be readily shown that the solution to the equality constrained problem\nformula_1\nformula_2\nis given by the linear system\nformula_3\nwhere \u03bb is a set of Lagrange multipliers which come out of the solution alongside x.\nThe easiest means of approaching this system is direct solution (for example, LU factorization), which for small problems is very practical. For large problems, the system poses some unusual difficulties, most notably that the problem is never positive definite (even if Q is), making it potentially very difficult to find a good numeric approach, and there are many approaches to choose from dependent on the problem.\nIf the constraints don't couple the variables too tightly, a relatively simple attack is to change the variables so that constraints are unconditionally satisfied. For example, suppose d = 0 (generalizing to nonzero is straightforward). Looking at the constraint equations:\nformula_4\nintroduce a new variable y defined by\nformula_5\nwhere y has dimension of x minus the number of constraints. Then\nformula_6\nand if Z is chosen so that \"EZ\" = 0 the constraint equation will be always satisfied. Finding such Z entails finding the null space of E, which is more or less simple depending on the structure of E. Substituting into the quadratic form gives an unconstrained minimization problem:\nformula_7\nthe solution of which is given by:\nformula_8\nUnder certain conditions on Q, the reduced matrix \"Z\"T\"QZ\" will be positive definite. It is possible to write a variation on the conjugate gradient method which avoids the explicit calculation of Z.\nLagrangian duality.\nThe Lagrangian dual of a quadratic programming problem is also a quadratic programming problem. To see this let us focus on the case where \"c\" = 0 and Q is positive definite. We write the Lagrangian function as \nformula_9\nDefining the (Lagrangian) dual function \"g\"(\u03bb) as formula_10, we find an infimum of L, using formula_11 and positive-definiteness of Q:\nformula_12\nHence the dual function is \nformula_13\nand so the Lagrangian dual of the quadratic programming problem is\nformula_14\nBesides the Lagrangian duality theory, there are other duality pairings (e.g. Wolfe, etc.).\nRun-time complexity.\nConvex quadratic programming.\nFor positive definite Q, when the problem is convex, the ellipsoid method solves the problem in (weakly) polynomial time.\nYe and Tse present a polynomial-time algorithm, which extends Karmarkar's algorithm from linear programming to convex quadratic programming. On a system with \"n\" variables and \"L\" input bits, their algorithm requires O(L n) iterations, each of which can be done using O(L n3) arithmetic operations, for a total runtime complexity of O(\"L\"2 \"n\"4). \nKapoor and Vaidya present another algorithm, which requires O(\"L\" * log \"L\" \"* n\"3.67 * log \"n\") arithmetic operations.\nNon-convex quadratic programming.\nIf Q is indefinite, (so the problem is non-convex) then the problem is NP-hard. A simple way to see this is to consider the non-convex quadratic constraint \"xi\"2 = \"xi\". This constraint is equivalent to requiring that \"xi\" is in {0,1}, that is, \"xi\" is a binary integer variable. Therefore, such constraints can be used to model any integer program with binary variables, which is known to be NP-hard.\nMoreover, these non-convex problems might have several stationary points and local minima. In fact, even if Q has only one negative eigenvalue, the problem is (strongly) NP-hard.\nMoreover, finding a KKT point of a non-convex quadratic program is CLS-hard.\nMixed-integer quadratic programming.\nThere are some situations where one or more elements of the vector x will need to take on integer values. This leads to the formulation of a mixed-integer quadratic programming (MIQP) problem. Applications of MIQP include water resources and the construction of index funds.\nExtensions.\nPolynomial optimization is a more general framework, in which the constraints can be polynomial functions of any degree, not only 2.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nFurther reading.\n "}
{"id": "40325", "revid": "20836525", "url": "https://en.wikipedia.org/wiki?curid=40325", "title": "Positive semidefinite", "text": "In mathematics, positive semidefinite may refer to:\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n Topics referred to by the same termThis page lists mathematics articles associated with the same title. "}
{"id": "40326", "revid": "13974845", "url": "https://en.wikipedia.org/wiki?curid=40326", "title": "Definite matrix", "text": "Property of a mathematical matrix\nIn mathematics, a symmetric matrix formula_1 with real entries is positive-definite if the real number formula_2 is positive for every nonzero real column vector formula_3 where formula_4 is the row vector transpose of formula_5\nMore generally, a Hermitian matrix (that is, a complex matrix equal to its conjugate transpose) is positive-definite if the real number formula_6 is positive for every nonzero complex column vector formula_7 where formula_8 denotes the conjugate transpose of formula_9\nPositive semi-definite matrices are defined similarly, except that the scalars formula_2 and formula_6 are required to be positive \"or zero\" (that is, nonnegative). Negative-definite and negative semi-definite matrices are defined analogously. A matrix that is not positive semi-definite and not negative semi-definite is sometimes called \"indefinite\".\nSome authors use more general definitions of definiteness, permitting the matrices to be non-symmetric or non-Hermitian. The properties of these generalized definite matrices are explored in , below, but are not the main focus of this article.\nDefinitions.\nIn the following definitions, formula_4 is the transpose of formula_3 formula_8 is the conjugate transpose of formula_7 and formula_16 denotes the zero-vector.\nDefinitions for real matrices.\nAn formula_17 symmetric real matrix formula_1 is said to be positive-definite if formula_19 for all non-zero formula_20 in formula_21 Formally,\nformula_22\nAn formula_17 symmetric real matrix formula_1 is said to be positive-semidefinite or non-negative-definite if formula_25 for all formula_20 in formula_27 Formally,\nformula_28\nAn formula_17 symmetric real matrix formula_1 is said to be negative-definite if formula_31 for all non-zero formula_20 in formula_33 Formally,\nformula_34\nAn formula_17 symmetric real matrix formula_1 is said to be negative-semidefinite or non-positive-definite if formula_37 for all formula_20 in formula_27 Formally,\nformula_40\nAn formula_17 symmetric real matrix which is neither positive semidefinite nor negative semidefinite is called indefinite.\nDefinitions for complex matrices.\nThe following definitions all involve the term formula_42 Notice that this is always a real number for any Hermitian square matrix formula_43\nAn formula_17 Hermitian complex matrix formula_1 is said to be positive-definite if formula_46 for all non-zero formula_47 in formula_48 Formally,\nformula_49\nAn formula_17 Hermitian complex matrix formula_1 is said to be positive semi-definite or non-negative-definite if formula_52 for all formula_47 in formula_48 Formally,\nformula_55\nAn formula_17 Hermitian complex matrix formula_1 is said to be negative-definite if formula_58 for all non-zero formula_47 in formula_48 Formally,\nformula_61\nAn formula_17 Hermitian complex matrix formula_1 is said to be negative semi-definite or non-positive-definite if formula_64 for all formula_47 in formula_48 Formally,\nformula_67\nAn formula_17 Hermitian complex matrix which is neither positive semidefinite nor negative semidefinite is called indefinite.\nConsistency between real and complex definitions.\nSince every real matrix is also a complex matrix, the definitions of \"definiteness\" for the two classes must agree.\nFor complex matrices, the most common definition says that formula_1 is positive-definite if and only if formula_70 is real and positive for every non-zero complex column vectors formula_71 This condition implies that formula_1 is Hermitian (i.e. its transpose is equal to its conjugate), since formula_70 being real, it equals its conjugate transpose formula_74 for every formula_7 which implies formula_76 \nBy this definition, a positive-definite \"real\" matrix formula_1 is Hermitian, hence symmetric; and formula_78 is positive for all non-zero \"real\" column vectors formula_71 However the last condition alone is not sufficient for formula_1 to be positive-definite. For example, if\nformula_81\nthen for any real vector formula_47 with entries formula_83 and formula_84 we have formula_85 which is always positive if formula_47 is not zero. However, if formula_47 is the complex vector with entries 1 and &amp;NoBreak;&amp;NoBreak;, one gets\nformula_88\nwhich is not real. Therefore, formula_1 is not positive-definite.\nOn the other hand, for a \"symmetric\" real matrix formula_90 the condition \"formula_91 for all nonzero real vectors formula_47\" \"does\" imply that formula_1 is positive-definite in the complex sense.\nNotation.\nIf a Hermitian matrix formula_1 is positive semi-definite, one sometimes writes formula_95 and if formula_1 is positive-definite one writes formula_97 To denote that formula_1 is negative semi-definite one writes formula_99 and to denote that formula_1 is negative-definite one writes formula_101\nThe notion comes from functional analysis where positive semidefinite matrices define positive operators. If two matrices formula_102 and formula_103 satisfy formula_104 we can define a non-strict partial order formula_105 that is reflexive, antisymmetric, and transitive; It is not a total order, however, as formula_106 in general, may be indefinite.\nA common alternative notation is formula_107 formula_108 formula_109 and formula_110 for positive semi-definite and positive-definite, negative semi-definite and negative-definite matrices, respectively. This may be confusing, as sometimes nonnegative matrices (respectively, nonpositive matrices) are also denoted in this way.\nRamifications.\nIt follows from the above definitions that a Hermitian matrix is positive-definite if and only if it is the matrix of a positive-definite quadratic form or Hermitian form. In other words, a Hermitian matrix is positive-definite if and only if it defines an inner product.\nPositive-definite and positive-semidefinite matrices can be characterized in many ways, which may explain the importance of the concept in various parts of mathematics. A Hermitian matrix M is positive-definite if and only if it satisfies any of the following equivalent conditions.\nA matrix is positive semi-definite if it satisfies similar equivalent conditions where \"positive\" is replaced by \"nonnegative\", \"invertible matrix\" is replaced by \"matrix\", and the word \"leading\" is removed.\nPositive-definite and positive-semidefinite real matrices are at the basis of convex optimization, since, given a function of several real variables that is twice differentiable, then if its Hessian matrix (matrix of its second partial derivatives) is positive-definite at a point formula_117 then the function is convex near p, and, conversely, if the function is convex near formula_117 then the Hessian matrix is positive-semidefinite at formula_119\nThe set of positive definite matrices is an open convex cone, while the set of positive semi-definite matrices is a closed convex cone.\nEigenvalues.\nLet formula_1 be an formula_17 Hermitian matrix (this includes real symmetric matrices). All eigenvalues of formula_1 are real, and their sign characterize its definiteness:\nLet formula_128 be an eigendecomposition of formula_90 where formula_130 is a unitary complex matrix whose columns comprise an orthonormal basis of eigenvectors of formula_90 and formula_132 is a \"real\" diagonal matrix whose main diagonal contains the corresponding eigenvalues. The matrix formula_1 may be regarded as a diagonal matrix formula_132 that has been re-expressed in coordinates of the (eigenvectors) basis formula_135 Put differently, applying formula_1 to some vector formula_7 giving formula_138 is the same as changing the basis to the eigenvector coordinate system using formula_139 giving formula_140 applying the stretching transformation formula_132 to the result, giving formula_142 and then changing the basis back using formula_143 giving formula_144\nWith this in mind, the one-to-one change of variable formula_145 shows that formula_70 is real and positive for any complex vector formula_47 if and only if formula_148 is real and positive for any formula_149 in other words, if formula_132 is positive definite. For a diagonal matrix, this is true only if each element of the main diagonal \u2013 that is, every eigenvalue of formula_1 \u2013 is positive. Since the spectral theorem guarantees all eigenvalues of a Hermitian matrix to be real, the positivity of eigenvalues can be checked using Descartes' rule of alternating signs when the characteristic polynomial of a real, symmetric matrix formula_1 is available.\nDecomposition.\nLet formula_1 be an formula_17 Hermitian matrix.\nformula_1 is positive semidefinite if and only if it can be decomposed as a product\nformula_156\nof a matrix formula_103 with its conjugate transpose.\nWhen formula_1 is real, formula_103 can be real as well and the decomposition can be written as formula_160\nformula_1 is positive definite if and only if such a decomposition exists with formula_103 invertible.\nMore generally, formula_1 is positive semidefinite with rank formula_164 if and only if a decomposition exists with a formula_165 matrix formula_103 of full row rank (i.e. of rank formula_164).\nMoreover, for any decomposition formula_168 formula_169\n&lt;templatestyles src=\"Math_proof/styles.css\" /&gt;Proof\nIf formula_168 then formula_171 as the diagonal matrix whose entries are non-negative square roots of eigenvalues.\nThen formula_172 for formula_173\nIf moreover formula_1 is positive definite, then the eigenvalues are (strictly) positive, so formula_175 is invertible, and hence formula_176 is invertible as well.\nIf formula_1 has rank formula_178 then it has exactly formula_164 positive eigenvalues and the others are zero, hence in formula_176 all but formula_164 rows are all zeroed.\nCutting the zero rows gives a formula_165 matrix formula_183 such that formula_184\nThe columns formula_185 of formula_103 can be seen as vectors in the complex or real vector space formula_187 respectively.\nThen the entries of formula_1 are inner products (that is dot products, in the real case) of these vectors\nformula_189\nIn other words, a Hermitian matrix formula_1 is positive semidefinite if and only if it is the Gram matrix of some vectors formula_191\nIt is positive definite if and only if it is the Gram matrix of some linearly independent vectors.\nIn general, the rank of the Gram matrix of vectors formula_185 equals the dimension of the space spanned by these vectors.\nUniqueness up to unitary transformations.\nThe decomposition is not unique: \nif formula_193 for some formula_165 matrix formula_103 and if formula_196 is any unitary formula_197 matrix (meaning formula_198),\nthen formula_199 for formula_200\nHowever, this is the only way in which two decompositions can differ: The decomposition is unique up to unitary transformations.\nMore formally, if formula_102 is a formula_165 matrix and formula_103 is a formula_204 matrix such that formula_205\nthen there is a formula_206 matrix formula_196 with orthonormal columns (meaning formula_208) such that formula_209\nWhen formula_210 this means formula_196 is unitary.\nThis statement has an intuitive geometric interpretation in the real case:\nlet the columns of formula_102 and formula_103 be the vectors formula_214 and formula_185 in formula_216\nA real unitary matrix is an orthogonal matrix, which describes a rigid transformation (an isometry of Euclidean space formula_217) preserving the 0 point (i.e. rotations and reflections, without translations). \nTherefore, the dot products formula_218 and formula_219 are equal if and only if some rigid transformation of formula_217 transforms the vectors formula_214 to formula_222 (and 0 to 0).\nSquare root.\nA Hermitian matrix formula_1 is positive semidefinite if and only if there is a positive semidefinite matrix formula_103 (in particular formula_103 is Hermitian, so formula_226) satisfying formula_227 This matrix formula_103 is unique, is called the \"non-negative square root\" of formula_90 and is denoted with formula_230\nWhen formula_1 is positive definite, so is formula_232 hence it is also called the \"positive square root\" of formula_233\nThe non-negative square root should not be confused with other decompositions formula_116\nSome authors use the name \"square root\" and formula_235 for any such decomposition, or specifically for the Cholesky decomposition,\nor any decomposition of the form formula_236\nothers only use it for the non-negative square root.\nIf formula_237 then formula_238\nCholesky decomposition.\nA Hermitian positive semidefinite matrix formula_1 can be written as formula_240 where formula_241 is lower triangular with non-negative diagonal (equivalently formula_242 where formula_243 is upper triangular); this is the Cholesky decomposition.\nIf formula_1 is positive definite, then the diagonal of formula_241 is positive and the Cholesky decomposition is unique. Conversely if formula_241 is lower triangular with nonnegative diagonal then formula_247 is positive semidefinite. \nThe Cholesky decomposition is especially useful for efficient numerical calculations.\nA closely related decomposition is the LDL decomposition, formula_248 where formula_132 is diagonal and formula_241 is lower unitriangular.\nWilliamson theorem.\nAny formula_251 positive definite Hermitian real matrix formula_252 can be diagonalized via symplectic (real) matrices. More precisely, Williamson's theorem ensures the existence of symplectic formula_253 and diagonal real positive formula_254 such that formula_255.\nOther characterizations.\nLet formula_1 be an formula_17 real symmetric matrix, and let formula_258 be the \"unit ball\" defined by formula_43 Then we have the following\nLet formula_1 be an formula_17 Hermitian matrix. The following properties are equivalent to formula_1 being positive definite:\nA positive semidefinite matrix is positive definite if and only if it is invertible.\nA matrix formula_1 is negative (semi)definite if and only if formula_306 is positive (semi)definite.\nQuadratic forms.\nThe (purely) quadratic form associated with a real formula_17 matrix formula_1 is the function formula_309 such that formula_310 for all formula_5 formula_1 can be assumed symmetric by replacing it with formula_313 since any asymmetric part will be zeroed-out in the double-sided product.\nA symmetric matrix formula_1 is positive definite if and only if its quadratic form is a strictly convex function.\nMore generally, any quadratic function from formula_315 to formula_316 can be written as formula_317 where formula_1 is a symmetric formula_17 matrix, formula_320 is a real n\u00a0vector, and formula_321 a real constant. In the formula_322 case, this is a parabola, and just like in the formula_322 case, we have\nTheorem: This quadratic function is strictly convex, and hence has a unique finite global minimum, if and only if formula_1 is positive definite.\nProof: If formula_1 is positive definite, then the function is strictly convex. Its gradient is zero at the unique point of formula_326 which must be the global minimum since the function is strictly convex. If formula_1 is not positive definite, then there exists some vector formula_328 such that formula_329 so the function formula_330 is a line or a downward parabola, thus not strictly convex and not having a global minimum.\nFor this reason, positive definite matrices play an important role in optimization problems.\nSimultaneous diagonalization.\nOne symmetric matrix and another matrix that is both symmetric and positive definite can be simultaneously diagonalized. This is so although simultaneous diagonalization is not necessarily performed with a similarity transformation. This result does not extend to the case of three or more matrices. In this section we write for the real case. Extension to the complex case is immediate.\nLet formula_1 be a symmetric and formula_276 a symmetric and positive definite matrix. Write the generalized eigenvalue equation as formula_333 where we impose that formula_20 be normalized, i.e. formula_335 Now we use Cholesky decomposition to write the inverse of formula_276 as formula_337 Multiplying by formula_196 and letting formula_339 we get formula_340 which can be rewritten as formula_341 where formula_342 Manipulation now yields formula_343 where formula_344 is a matrix having as columns the generalized eigenvectors and formula_345 is a diagonal matrix of the generalized eigenvalues. Now premultiplication with formula_346 gives the final result: formula_347 and formula_348 but note that this is no longer an orthogonal diagonalization with respect to the inner product where formula_342 In fact, we diagonalized formula_1 with respect to the inner product induced by formula_351\nNote that this result does not contradict what is said on simultaneous diagonalization in the article Diagonalizable matrix, which refers to simultaneous diagonalization by a similarity transformation. Our result here is more akin to a simultaneous diagonalization of two quadratic forms, and is useful for optimization of one form under conditions on the other.\nProperties.\nInduced partial ordering.\nFor arbitrary square matrices formula_90 formula_276 we write formula_354 if formula_355 i.e., formula_356 is positive semi-definite. This defines a partial ordering on the set of all square matrices. One can similarly define a strict partial ordering formula_357 The ordering is called the Loewner order.\nInverse of positive definite matrix.\nEvery positive definite matrix is invertible and its inverse is also positive definite. If formula_358 then formula_359 Moreover, by the min-max theorem, the kth largest eigenvalue of formula_1 is greater than or equal to the kth largest eigenvalue of formula_351\nScaling.\nIf formula_1 is positive definite and formula_363 is a real number, then formula_364 is positive definite.\nTrace.\nThe diagonal entries formula_386 of a positive-semidefinite matrix are real and non-negative. As a consequence the trace, formula_387 Furthermore, since every principal sub-matrix (in particular, 2-by-2) is positive semidefinite,\nformula_388\nand thus, when formula_389\nformula_390\nAn formula_17 Hermitian matrix formula_1 is positive definite if it satisfies the following trace inequalities:\nformula_393\nAnother important result is that for any formula_1 and formula_276 positive-semidefinite matrices, formula_396 This follows by writing formula_397 The matrix formula_398 is positive-semidefinite and thus has non-negative eigenvalues, whose sum, the trace, is therefore also non-negative.\nHadamard product.\nIf formula_399 although formula_379 is not necessary positive semidefinite, the Hadamard product is, formula_401 (this result is often called the Schur product theorem).\nRegarding the Hadamard product of two positive semidefinite matrices formula_402 formula_403 there are two notable inequalities:\nKronecker product.\nIf formula_399 although formula_379 is not necessary positive semidefinite, the Kronecker product formula_408\nFrobenius product.\nIf formula_399 although formula_379 is not necessary positive semidefinite, the Frobenius inner product formula_411 (Lancaster\u2013Tismenetsky, \"The Theory of Matrices\", p.\u00a0218).\nConvexity.\nThe set of positive semidefinite symmetric matrices is convex. That is, if formula_1 and formula_276 are positive semidefinite, then for any formula_414 between 0 and 1, formula_415 is also positive semidefinite. For any vector formula_20:\nformula_417\nThis property guarantees that semidefinite programming problems converge to a globally optimal solution.\nRelation with cosine.\nThe positive-definiteness of a matrix formula_102 expresses that the angle formula_419 between any vector formula_20 and its image formula_421 is always formula_422\nformula_423 the angle between formula_20 and formula_425\nBlock matrices and submatrices.\nA positive formula_450 matrix may also be defined by blocks:\nformula_451\nwhere each block is formula_452 By applying the positivity condition, it immediately follows that formula_102 and formula_132 are hermitian, and formula_455\nWe have that formula_456 for all complex formula_7 and in particular for formula_458 Then\nformula_459\nA similar argument can be applied to formula_460 and thus we conclude that both formula_102 and formula_132 must be positive definite. The argument can be extended to show that any principal submatrix of formula_1 is itself positive definite.\nConverse results can be proved with stronger conditions on the blocks, for instance, using the Schur complement.\nLocal extrema.\nA general quadratic form formula_464 on formula_465 real variables formula_466 can always be written as formula_2 where formula_20 is the column vector with those variables, and formula_1 is a symmetric real matrix. Therefore, the matrix being positive definite means that formula_470 has a unique minimum (zero) when formula_20 is zero, and is strictly positive for any other formula_5\nMore generally, a twice-differentiable real function formula_470 on formula_465 real variables has local minimum at arguments formula_466 if its gradient is zero and its Hessian (the matrix of all second derivatives) is positive semi-definite at that point. Similar statements can be made for negative definite and semi-definite matrices.\nCovariance.\nIn statistics, the covariance matrix of a multivariate probability distribution is always positive semi-definite; and it is positive definite unless one variable is an exact linear function of the others. Conversely, every positive semi-definite matrix is the covariance matrix of some multivariate distribution.\nExtension for non-Hermitian square matrices.\nThe definition of positive definite can be generalized by designating any complex matrix formula_1 (e.g. real non-symmetric) as positive definite if formula_477 for all non-zero complex vectors formula_7 where formula_479 denotes the real part of a complex number formula_480 Only the Hermitian part formula_481 determines whether the matrix is positive definite, and is assessed in the narrower sense above. Similarly, if formula_20 and formula_1 are real, we have formula_484 for all real nonzero vectors formula_20 if and only if the symmetric part formula_486 is positive definite in the narrower sense. It is immediately clear that formula_487is insensitive to transposition of formula_43\nA non-symmetric real matrix with only positive eigenvalues may have a symmetric part with negative eigenvalues, in which case it will not be positive (semi)definite. For example, the matrix formula_489 has positive eigenvalues 1 and 7, yet formula_490 with the choice formula_491.\nIn summary, the distinguishing feature between the real and complex case is that, a bounded positive operator on a complex Hilbert space is necessarily Hermitian, or self adjoint. The general claim can be argued using the polarization identity. That is no longer true in the real case.\nApplications.\nHeat conductivity matrix.\nFourier's law of heat conduction, giving heat flux formula_492 in terms of the temperature gradient formula_493 is written for anisotropic media as formula_494 in which formula_495 is the thermal conductivity matrix. The negative is inserted in Fourier's law to reflect the expectation that heat will always flow from hot to cold. In other words, since the temperature gradient formula_496 always points from cold to hot, the heat flux formula_492 is expected to have a negative inner product with formula_496 so that formula_499 Substituting Fourier's law then gives this expectation as formula_500 implying that the conductivity matrix should be positive definite. Ordinarily formula_495 should be symmetric, however it becomes nonsymmetric in the presence of a magnetic field as in a thermal Hall effect.\nMore generally in thermodynamics, the flow of heat and particles is a fully coupled system as described by the Onsager reciprocal relations, and the coupling matrix is required to be positive semi-definite (possibly non-symmetric) in order that entropy production be nonnegative.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nSources.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "40327", "revid": "157812", "url": "https://en.wikipedia.org/wiki?curid=40327", "title": "Wendy's restaurant", "text": ""}
{"id": "40328", "revid": "37740198", "url": "https://en.wikipedia.org/wiki?curid=40328", "title": "Negative semidefinite", "text": ""}
{"id": "40329", "revid": "20836525", "url": "https://en.wikipedia.org/wiki?curid=40329", "title": "Negative definite", "text": ""}
{"id": "40330", "revid": "28481209", "url": "https://en.wikipedia.org/wiki?curid=40330", "title": "Magnoliaceae", "text": "Family of flowering plants\nThe Magnoliaceae () are a flowering plant family, the magnolia family, in the order Magnoliales. It consists of two genera: \"Magnolia\" and \"Liriodendron\" (tulip trees).\nUnlike most angiosperms, whose flower parts are in whorls (rings), the Magnoliaceae have their stamens and pistils in spirals on a conical receptacle. This arrangement is found in some fossil plants and is believed to be a basal or early condition for angiosperms. The flowers also have parts not distinctly differentiated into sepals and petals, while angiosperms that evolved later tend to have distinctly differentiated sepals and petals. The poorly differentiated perianth parts that occupy both positions are known as tepals.\nThe family has about 219 species and ranges across subtropical eastern North America, Mexico and Central America, the West Indies, tropical South America, southern and eastern India, Sri Lanka, Indochina, Malesia, China, Japan, and Korea.\nGenera.\nThe number of genera in Magnoliaceae is a subject of debate. Up to 17 have been recognized, including \"Alcimandra\", \"Lirianthe\", \"Manglietia\", \"Michelia\", \"Pachylarnax\", \"Parakmeria\", \"Talauma\" and \"Yulania\". However, many recent studies have opted to merge all genera within subfamily Magnolioideae into the genus \"Magnolia\". Thus, Magnoliaceae would include only two extant genera, \"Magnolia\" and \"Liriodendron\".\nDescription.\nThe monophyly of Magnoliaceae is supported by a number of shared morphological characters among the various genera in the family. Most have bisexual flowers (with the exception of \"Kmeria\" and some species of \"Magnolia\" section \"Gynopodium\"), showy, fragrant, radial, and with an elongated receptacle.\nLeaves are alternate, simple, and sometimes lobed. The inflorescence is a solitary, showy flower with indistinguishable petals and sepals. Sepals range from six to many; stamens are numerous and feature short filaments which are poorly differentiated from the anthers. Carpels are usually numerous, distinct, and on an elongated receptacle or torus. The fruit is an aggregate fruit (etaerio) of follicles which usually become closely appressed as they mature and open along the abaxial surface. Seeds have a fleshy coat, aril, and color that ranges from red to orange (except \"Liriodendron\"). Magnoliaceae flowers are beetle pollinated, except for \"Liriodendron\", which is bee pollinated. The carpels of\" Magnolia\" flowers are especially thick to avoid damage by beetles that land, crawl, and feast on them. The seeds of Magnolioideae are bird-dispersed, while the seeds of \"Liriodendron\" are wind-dispersed.\nBiogeography.\nDue to its great age, the geographical distribution of the Magnoliaceae has become disjunct or fragmented as a result of major geologic events such as ice ages, continental drift, and mountain formation. This distribution pattern has isolated some species, while keeping others in close contact.\nExtant species of the Magnoliaceae are widely distributed in temperate and tropical Asia from the Himalayas to Japan and southwest through Malaysia and New Guinea. Asia is home to about two-thirds of the species in Magnoliaceae, with the remainder of the family spread across the Americas with temperate species extending into southern Canada and tropical elements extending into Brazil and the West Indies.\nSystematics.\nFoundational Taxonomic and Systematics Research (18th-19th century).\nThe earliest botanical description of the Magnoliaceae as a family is in Antonii Laurentii de Jussieu's \"Genera Plantarum\", which describes eight genera included within the family (\"Euryandra\", \"Drymis\", Illicium, \"Michelia\", \"Magnolia\", \"Talauma, Liriodendrum\", and \"Mayna\") as well as four genera closely related to the family (\"Dillenia\", \"Curatella\", \"Ochna\", and \"Quassia\"). Bentham and Hooker's \"Genera Plantarum\", almost a century later, sorts the family's genera into three tribes: the Wintereae, including the genera \"Drimys\" and \"Illicium\", the Magnolieae, including the genera \"Talauma, Magnolia, Manglieta, Michelia\", and \"Liriodendron\", and the Schizandreae, including the genera \"Schizandra\" and \"Kadsura\". In his following work \"Adansonia\", Baillon recognizes Bentham and Hooker's changes and additions but proposes an alternative taxonomy where he sets aside the \"Tulipier\" genus and include all remaining genera under one Magnolieae tribe. From this basic separation, scholars have continued to debate the systematics of the family.\nModern Systematics Research (20th-21st century).\nDandy's taxonomic proposal in 1927 sets aside the genus \"Liriodendron\" as a part of the subfamily Liriodendreae and includes Bentham and Hooker's four genera in addition to four more (\"Kmeria, Pachylarnax, Alcimandra\", and \"Elmerrillia\") within the Magnolieae tribe. Dandy's model with eleven genera was widely accepted until molecular evidence brought it into question (Figlar, 2019). Qiu et al. analyzed molecular data in 1995 to investigate the divergences within and between East Asian and East North American species of \"Magnolia\", presenting molecular evidence which shows that Dandy's section \"Rytidospermum\" is not monophyletic. Azuma et al. employ both molecular phylogeny and parsimonious mapping of the chemistry of floral scents in 1999 to propose a phylogenetic tree where, unlike Dandy's taxonomy, they include \"Michelia\" species within the \"Magnolia\" genus as a sister group to the subgenus \"Yulania\" and also find that the section \"Rytidospermum\" is not monophyletic, placing some of its members in a clade with the section \"Oyama\".\nThe most recent research on the family continues the debate over the genera of the family. Wang et al.'s study analyzes complete chloroplast genome sequences of 86 species in the Magnoliaceae and supports a phylogeny with fifteen major clades, two subfamilies, two genera, and fifteen sections, maintaining \"Magnolia\"'s classification as one monophyletic genus. Dong et al. also place \"Magnolia\" as the sole genus of the subfamily Magnolioideae made up of fifteen sections. However, Yang et al. and Zhao et al. work with phylogenies of the Magnoliaceae that recognize several genera in the Magnolioideae.\nConsensus and Debates Today.\nAlthough phylogenetic trees of the Magnoliaceae still include anywhere from 2 to 17 genera, the broad generic concept (where one genus, \"Magnolia\", is in the Magnolioideae) is largely accepted as a practical construction upheld by molecular and morphological evidence. Even as debates over rank persist, monophyletic groups are largely established with opportunities for further research into endangered and extinct species. The family's place as early angiosperms means that research into its taxonomy and evolutionary history contributes to our broader understanding of the evolution of plant life.\nThe development of DNA sequencing at the end of the 20th century had a profound impact on the research of phylogenetic relationships within the family. The employment of \"ndh\"F and cpDNA sequences has refuted many of the traditionally accepted phylogenetic relationships within the Magnoliaceae. For example, the genera \"Magnolia\" and \"Michelia\" were shown to be paraphyletic when the remaining four genera of the Magnolioideae are split out. In fact, even many of the subgenera (\"Magnolia\" subg. \"Magnolia\", \"Magnolia\" subg. \"Talauma\") have been found to be paraphyletic. Although no completely resolved phylogeny for the family has yet been determined, these technological advances have allowed systematists to broadly circumscribe major lineages.\nEconomic significance.\nAs a whole, the Magnoliaceae are not an economically significant family. With the exception of ornamental cultivation, the economic significance of magnolias is generally confined to the use of wood from certain timber species and the use of bark and flowers from several species believed to possess medicinal qualities. The wood of the American tuliptree, \"Liriodendron tulipifera\" and the wood of the cucumbertree magnolia, \"Magnolia acuminata\", and, to a lesser degree, that of the Frasier magnolia, \"Magnolia fraseri\", are harvested and marketed collectively as \"yellow poplar.\" This is a lightweight and exceptionally fine-grained wood, lending itself to precision woodworking for purposes such as pipe organ building.\nMagnolias have a rich cultural tradition in China, where references to their healing qualities go back thousands of years. The Chinese have long used the bark of \"Magnolia officinalis\", a magnolia native to the mountains of China with large leaves and fragrant white flowers, as a remedy for cramps, abdominal pain, nausea, diarrhea, and indigestion. Certain magnolia flowers, such as the buds of \"Magnolia liliiflora\", have been used to treat chronic respiratory and sinus infections and lung congestion. Recently, magnolia bark has become incorporated into alternative medicine in the west, where tablets made from the bark of \"M. officinalis\" have been marketed as an aid for anxiety, allergies, asthma, and weight loss. Compounds found in magnolia bark might have antibacterial and antifungal properties, but no large-scale study on the health effects of magnolia bark or flowers has yet been conducted.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "40331", "revid": "4770293", "url": "https://en.wikipedia.org/wiki?curid=40331", "title": "Cannoli", "text": "Italian ricotta-filled pastry\nCannoli are Sicilian pastries consisting of a tube-shaped shell of fried pastry dough, filled with a sweet and creamy filling containing ricotta cheese. Their size ranges from . In mainland Italy, the food is commonly known as (lit.\u2009'Sicilian cannoli').\nIn culinary traditions across Sicily, regional variations in cannoli fillings reflect local preferences and ingredient availability. In Palermo, cannoli are decorated with candied orange zest, adding a citrusy sweetness to the filling. In Catania, chopped pistachios are favored, adding a distinctive nutty flavor and texture. Ramacca is known for its purple artichokes, which also feature as filling in some cannoli recipes.\nEtymology.\nItalian and Sicilian is originally a diminutive noun meaning 'little tube', from , 'cane' or 'tube'.\nHistory.\nSome food historians place the origins of cannoli in 827\u20131091 in Caltanissetta, Sicily, by the concubines of princes looking to capture their attention. This period marks the Arab rule of the island, known then as the Emirate of Sicily, giving rise to the theory that the etymology stemmed from the Arabic word \"qanaw\u0101t\", 'tubes', in reference to their tube-shaped shells. During this time, the Arabs influenced Sicilian baking with the introduction of candied fruits, pistachios, and cinnamon. They also introduced the technique of combining nuts and fruits with sugar and honey.\nGaetano Basile claims that cannoli come from the Palermo and Messina areas and were historically prepared as a treat during Carnival season, possibly as a fertility symbol. The dessert eventually became a year-round staple in Sicily.\nWhat is certain is, as Salvatore Farina explains, that, \"\"cannolo\" is a word of Latin origin \u2013 \"canneolus\" \u2013 and means the joint of a reed or cane, the artisan instrument used to roll the dough that was fried to make the characteristic shell, later filled with ricotta cream.\" Farina continues, \"Probably, long ago, in the wild days of the Saturnali and the old style Carnival, street sellers prepared cannoli in the noisy and crowded public squares, filling the shell with a ricotta and honey cream. This is a confection that comes in natural portions, ideal for eating outside just as one does today with an ice cream cone.\"\nSome similar desserts in Middle Eastern tradition include \"Zaynab's fingers\" (), which are filled with nuts, and \"qanaw\u0101t\" (), deep-fried dough tubes filled with various sweets, which were a popular pastry.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\n&lt;templatestyles src=\"Sister-inline/styles.css\"/&gt; at the Wikibooks Cookbook subproject"}
{"id": "40333", "revid": "48643156", "url": "https://en.wikipedia.org/wiki?curid=40333", "title": "Magnolia", "text": "Genus of angiosperms\nMagnolia is a large genus of about 210 to 340 flowering plant species in the subfamily Magnolioideae of the family Magnoliaceae. The natural range of \"Magnolia\" species is disjunct, with a main center in east, south and southeast Asia and a secondary center in eastern North America, Central America, the West Indies, and some species in South America.\nMagnolias are evergreen or deciduous trees or shrubs known for their large, fragrant, bowl- or star-shaped flowers with numerous spirally arranged reproductive parts, producing cone-like fruits in autumn that open to reveal seeds. The genus \"Magnolia\" was first named in 1703 by Charles Plumier, honoring Pierre Magnol, with early taxonomy refined by Linnaeus in the 18th century based on American and later Asian species. Modern molecular phylogenetic studies have revealed complex relationships leading to taxonomic debates about merging related genera like \"Michelia\" with \"Magnolia\". Magnolia species are valued horticulturally for their early and showy flowering, used culinarily in various edible forms, employed in traditional medicine for their bioactive compounds like magnolol and honokiol, and harvested for timber, with hybridization enhancing desirable traits.\n\"Magnolia\" is an ancient genus that dates back to the Cretaceous. Fossilized specimens of \"M. acuminata\" have been found dating to 20 million years ago (mya), and fossils of plants identifiably belonging to the Magnoliaceae date to 95 mya. They are theorized to have evolved to encourage pollination by beetles as they existed prior to the evolution of bees. Another aspect of \"Magnolia\" considered to represent an ancestral state is that the flower bud is enclosed in a bract rather than in sepals; the perianth parts are undifferentiated and called tepals rather than distinct sepals and petals. \"Magnolia\" shares the tepal characteristic with several other flowering plants near the base of the flowering plant lineage, such as \"Amborella\" and \"Nymphaea\" (as well as with many more recently derived plants, such as \"Lilium).\"\nMagnolias are culturally significant symbols, serving as official flowers and trees in various regions like Shanghai, Mississippi, Louisiana, North Korea, and Seoul, and are closely associated with the Southern United States. In the arts, magnolias symbolize both beauty and resilience, as seen in the play and film \"Steel Magnolias\", while also evoking the contrasting brutality of lynching in the song \"Strange Fruit\" and Southern stereotypes in political commentary.\nDescription.\nMagnolias are spreading evergreen or deciduous trees or shrubs characterised by large fragrant flowers, which may be bowl-shaped or star-shaped, in shades of white, pink, purple, green, or yellow. In deciduous species, the blooms often appear before the leaves in spring. Cone-like fruits are often produced in the autumn.\nAs with all Magnoliaceae, the perianth is undifferentiated, with 9\u201315 tepals in three or more whorls. The flowers are hermaphroditic, with numerous adnate carpels and stamens arranged in a spiral fashion on the elongated receptacle. The flowers' carpels are often damaged by pollinating beetles.\nThe fruit dehisces along the dorsal sutures of the carpels. The pollen is monocolpate, and the embryonic development is of the Polygonum type. Taxonomists, including James E. Dandy in 1927, have used differences in the fruits of Magnoliaceae as the basis for classification systems.\nTaxonomy.\nHistory.\nEarly.\nThe name \"Magnolia\" first appeared in 1703 in the \"Genera\" written by French botanist Charles Plumier, for a flowering tree from the island of Martinique (\"talauma\"). It was named after French botanist Pierre Magnol. English botanist William Sherard, who studied botany in Paris under Joseph Pitton de Tournefort, a pupil of Magnol, was most probably the first after Plumier to adopt the genus name \"Magnolia\". He was at least responsible for the taxonomic part of Johann Jacob Dillenius's \"Hortus Elthamensis\" and of Mark Catesby's \"Natural History of Carolina, Florida and the Bahama Islands\". These were the first works after Plumier's \"Genera\" that used the name \"Magnolia\", this time for some species of flowering trees from temperate North America. The species that Plumier originally named \"Magnolia\" was later described as \"Annona dodecapetala\" by Jean-Baptiste Lamarck and has since been named \"Magnolia plumieri\" and \"Talauma plumieri\" (among a number of other names), but is now known as \"Magnolia dodecapetala\".\nCarl Linnaeus, who was familiar with Plumier's \"Genera\", adopted the genus name \"Magnolia\" in 1735 in his first edition of \"Systema Naturae\", without a description but with a reference to Plumier's work. In 1753, he took up Plumier's \"Magnolia\" in the first edition of \"Species Plantarum\". He described a monotypic genus, with the sole species being \"Magnolia virginiana\". Since Linnaeus never saw a herbarium specimen (if there ever was one) of Plumier's \"Magnolia\" and had only his description and a rather poor picture at hand, he must have taken it for the same plant that was described by Mark Catesby in his 1730 \"Natural History of Carolina\". He placed it in the synonymy of \"Magnolia virginiana\" var. \"f\u0153tida\", the taxon now known as \"Magnolia grandiflora\". Under \"Magnolia virginiana\", Linnaeus described five varieties (\"glauca\", \"f\u0153tida\", \"grisea\", \"tripetala\", and \"acuminata\"). In the tenth edition of \"Systema Naturae\" (1759), he merged \"grisea\" with \"glauca\" and raised the four remaining varieties to species status.\nBy the end of the 18th century, botanists and plant hunters exploring Asia had begun to name and describe the \"Magnolia\" species from China and Japan. The first Asiatic species to be described by western botanists were \"Magnolia denudata\", \"Magnolia liliiflora\", \"Magnolia coco,\" and \"Magnolia figo\". Soon after that, in 1794, Carl Peter Thunberg collected and described \"Magnolia obovata\" from Japan, and roughly at the same time \"Magnolia kobus\" was also first collected.\nRecent.\nWith the number of species increasing, the genus was divided into two subgenera, \"Magnolia\" and \"Yulania\". \"Magnolia\" contains the American evergreen species \"M. grandiflora\", which is of horticultural importance, especially in the southeastern United States, and \"M. virginiana\", the type species. \"Yulania\" contains several deciduous Asiatic species, such as \"M. denudata\" and \"M. kobus\", which have become horticulturally important in their own right and as parents in hybrids. Classified in \"Yulania\" is also the American deciduous \"M. acuminata\" (cucumber tree), which has recently attained greater status as the parent responsible for the yellow flower color.\nRelations in the family Magnoliaceae have puzzled taxonomists for a long time. Because the family is quite old and has survived many geological events (such as ice ages, mountain formation, and continental drift), its distribution has become scattered. Some species or groups of species have been isolated for a long time, while others could stay in close contact. To create divisions in the family (or even within the genus \"Magnolia\") solely based upon morphological characters has proven to be a nearly impossible task.\nBy the end of the 20th century, DNA sequencing had become available as a method of large-scale research on phylogenetic relationships. Several studies, including studies on many species in the family Magnoliaceae, were carried out to investigate relationships. What these studies all revealed was that the genus \"Michelia\" and \"Magnolia\" subgenus \"Yulania\" were far more closely allied to each other than either one of them was to \"Magnolia\" subgenus \"Magnolia\". These phylogenetic studies were supported by morphological data.\nAs nomenclature is supposed to reflect relationships, the situation with the species names in \"Michelia\" and \"Magnolia\" subgenus \"Yulania\" was undesirable. Taxonomically, three choices are available:\n\"Magnolia\" subgenus \"Magnolia\" cannot be renamed because it contains \"M. virginiana\", the type species of the genus and of the family.\nNot many \"Michelia\" species have so far become horticulturally or economically important, apart from their wood. Both subgenus \"Magnolia\" and subgenus \"Yulania\" include species of major horticultural importance, and a change of name would be very undesirable for many people, especially in the horticultural branch. In Europe, \"Magnolia\" is even more or less a synonym for \"Yulania\", since most of the cultivated species on this continent have \"Magnolia (Yulania) denudata\" as one of their parents. Most taxonomists who acknowledge close relations between \"Yulania\" and \"Michelia\" therefore support the third option and join \"Michelia\" with \"Magnolia\".\nThe same goes, \"mutatis mutandis\", for the (former) genera \"Talauma\" and \"Dugandiodendron\", which are then placed in subgenus \"Magnolia\", and genus \"Manglietia\", which could be joined with subgenus \"Magnolia\" or may even earn the status of an extra subgenus. \"Elmerrillia\" seems to be closely related to \"Michelia\" and \"Yulania\", in which case it will most likely be treated in the same way as \"Michelia\" is now. The precise nomenclatural status of small or monospecific genera like \"Kmeria\", \"Parakmeria\", \"Pachylarnax\", \"Manglietiastrum\", \"Aromadendron\", \"Woonyoungia\", \"Alcimandra\", \"Paramichelia\", and \"Tsoongiodendron\" remains uncertain. Taxonomists who merge \"Michelia\" into \"Magnolia\" tend to merge these small genera into \"Magnolia\" s.l. as well. Botanists do not agree on whether to recognize a big \"Magnolia\" or the different small genera. For example, \"Flora of China\" offers two choices: a large genus \"Magnolia\", which includes about 300 species and everything in the Magnoliaceae except \"Liriodendron\" (tulip tree), or 16 different genera, some of them recently split out or re-recognized, each of which contains up to 50 species. The western co-author favors the big genus \"Magnolia\", whereas the Chinese recognize the different small genera.\nNew species of \"Magnolia\" are still being discovered today. In 2014, researchers discovered \"Magnolia vargasiana\" and \"Magnolia llangantensis\" in Ecuador\u2019s Cordillera Llanganates, within the R\u00edo Zu\u00f1ac Reserve at 2000 meters elevation. The R\u00edo Zu\u00f1ac Reserve is a privately protected conservation area in Ecuador, managed by the EcoMinga Foundation. This newly identified tree species grows between 11 and 26 meters tall and features sub-orbicular leaves, creamy white petals, and a pollination system involving flea beetles. Found during a vegetation survey, its limited distribution and low population density place it at risk of extinction.\nFossil record.\nFossils go back to the Late Cretaceous. Post KT fossils of \"Magnolia\" are known from the Paleogene, for example the species \"Magnolia nanningensis,\" named for mummified wood from the Oligocene of Guangxi, China, which has a close affinity to members of the modern section \"Michelia\".\nSubdivision.\nIn 2012, the Magnolia Society published on its website a classification of the genus produced by Richard B. Figlar, based on a 2004 classification by Figlar and Hans Peter Nooteboom. Species of \"Magnolia\" were listed under three subgenera, 12 sections, and 13 subsections. Subsequent molecular phylogenetic studies have led to some revisions of this system; for example, the subgenus \"Magnolia\" was found not to be monophyletic. A revised classification in 2020, based on a phylogenetic analysis of complete chloroplast genomes, abandoned subgenera and subsections, dividing \"Magnolia\" into 15 sections. The relationships among these sections are shown in the following cladogram, as is the paraphyletic status of subgenus \"Magnolia\".\nThe table below compares the 2012 and 2020 classifications. (The circumscriptions of the corresponding taxa may not be the same.)\nUses.\nHorticulture.\nIn general, the genus \"Magnolia\" has attracted horticultural interest. Some, such as the shrub \"M. stellata\" (star magnolia) and the tree \"M.\" \u00d7 \"soulangeana\" (saucer magnolia) flower quite early in the spring, before the leaves open. Others flower in late spring or early summer, including \"M. virginiana\" (sweetbay magnolia) and \"M. grandiflora\" (southern magnolia). The shape of these flowers lend themselves to the common name \"tulip tree\" that is sometimes applied to some \"Magnolia\" species.\nHybridisation has been immensely successful in combining the best aspects of different species to give plants which flower at an earlier age than the parent species, as well as having more impressive flowers. One of the most popular garden magnolias, \"M\". \u00d7 \"soulangeana\", is a hybrid of \"M. liliiflora\" and \"M. denudata\". In the eastern United States, five native species are frequently in cultivation: \"M. acuminata\" (as a shade tree), \"M. grandiflora\", \"M. virginiana\", \"M. tripetala\", and \"M. macrophylla\". The last two species must be planted where high winds are not a frequent problem because of the large size of their leaves. Young plants of the Florida endemic \"M. ashei\" start flowering at a particularly early age, even just 3\u20134 years old, making it popular.\nCulinary.\nThe flowers of many species are considered edible. In parts of England, the petals of \"M. grandiflora\" are pickled and used as a spicy condiment. In some Asian cuisines, the buds are pickled and used to flavor rice and scent tea. In Japan, the young leaves and flower buds of \"M. hypoleuca\" are broiled and eaten as a vegetable. Older leaves are made into a powder and used as seasoning; dried, whole leaves are placed on a charcoal brazier and filled with miso, leeks, daikon, and shiitake, and broiled. There is a type of miso which is seasoned with magnolia, hoba miso.\nTraditional medicine.\nThe bark and flower buds of \"M. officinalis\" have long been used in traditional Chinese medicine, where they are known as \"hou po\" (\u539a\u6734). In Japan, \"k\u014dboku\", \"M. obovata\", has been used in a similar manner.\nTimber.\nThe cucumbertree, \"M. acuminata\", grows to large size and is harvested as a timber tree in northeastern U.S. forests. Its wood is sold as \"yellow poplar\" along with that of the tuliptree, \"Liriodendron tulipifera\". The Fraser magnolia, \"M. fraseri\", also attains enough size sometimes to be harvested, as well.\nChemical compounds and bioeffects.\nThe aromatic bark contains magnolol, honokiol, 4-O-methylhonokiol, and obovatol. Magnolol and honokiol activate the nuclear receptor peroxisome proliferator-activated receptor gamma.\nPastoral scene of the gallant south\nThe bulging eyes and the twisted mouth\n\"Scent of magnolias, sweet and fresh\",\nThen the sudden smell of burning flesh.\nCulture.\nArts.\nDespite Meeropol's frequent mention of the South and magnolia trees, the horrific image which inspired his poem, Lawrence Beitler's 1930 photograph of the lynching of Thomas Shipp and Abram Smith following the robbery and murder of Claude Deteer, was taken in Marion, Indiana, where magnolia trees are less common.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nBibliography.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "40335", "revid": "40161362", "url": "https://en.wikipedia.org/wiki?curid=40335", "title": "Salzburg", "text": "Capital of the state of Salzburg, Austria\nSalzburg is the fourth-largest city in Austria. In 2020 its population was 156,852. The city lies on the Salzach River, near the border with Germany and at the foot of the Alps mountains. \nThe town occupies the site of the Roman settlement of \"Iuvavum\". Founded as an episcopal see in 696, it became a seat of the archbishop in 798. Its main sources of income were salt extraction, trade, as well as gold mining. The fortress of Hohensalzburg, one of the largest medieval fortresses in Europe, dates from the 11th century. In the 17th century, Salzburg became a centre of the Counter-Reformation, with monasteries and numerous Baroque churches built. Salzburg has an extensive cultural and educational history, being the birthplace of Wolfgang Amadeus Mozart and being home to three universities and a large student population. Today, along with Vienna and the Tyrol, Salzburg is one of Austria's most popular tourist destinations.\nSalzburg's historic center () is renowned for its Baroque architecture and is one of the best-preserved city centres north of the Alps. The historic center was listed as a UNESCO World Heritage Site in 1996. \nEtymology.\nThe name \"Salzburg\" was first recorded in the late 8th century. It is composed of two parts; the first being \"\" (German for \"salt\"), and the second being \"-burg\" from Proto-West-Germanic: \"*burg \"settlement, city\"\" and not that of the New High German: , 'fortress'.\nHistory.\nAntiquity.\nThe area of the city has been inhabited continuously since the Neolithic Age until the present. In the La T\u00e8ne period, it was an administrative centre of the Celtic Alums in the Kingdom of Noricum.\nAfter the Roman invasion in 15 BC, the various settlements on the Salzburg hills were abandoned, following the construction of the Roman city in the area of the old town. The recently created \"Municipium Claudium Iuvavum\" was awarded the status of a Roman \"municipium\" in \u00a0CE and has become one of the most important cities of the now Roman province of Noricum.\nMiddle Ages.\nWhen the province of Noricum collapsed in 488 at the beginning of the migration period, part of the Romano-Celtic population remained in the country. In the 6th century, they came under the rule of the Baiuvarii. The \"Life of Saint Rupert\" credits the 8th-century saint with the city's rebirth, when around \u00a0CE, Bishop Rupert of Salzburg received the remains of the Roman town from Duke Theodo II of Bavaria as well as a \"castrum superius\" (upper castle) on the Nonnberg Terrace as a gift. In return, he was to evangelize the east and south-east of the country of Bavaria.\nRupert reconnoitred the river for the site of his basilica and chose \"Iuvavum\". He ordained priests and annexed the manor of Piding. Rupert built a church at St. Peter on the site of today's cathedral and probably also founded the associated monastery and the Benedictine nunnery on Nonnberg for his relative Erentrude. Salzburg has been the seat of a diocesan bishop since \u00a0CE and an archbishopric since \u00a0CE. The first cathedral was built under Archbishop Virgil. The Franciscan Church existed since the beginning of the 9th century at the latest. The Marienkirche dates from 1139.\nThe first use of the German name Salzburg, meaning \"Salt-Castle\", can be traced back to \u00a0CE when the name was used in Willibald's report on the organization of the Bavarian dioceses by Saint Boniface. The name derives from the barges carrying salt on the River Salzach, which were subject to a toll in the 8th century as was customary for many communities and cities on European rivers. Hohensalzburg Fortress, the city's fortress was built on the site of a Roman fort in 1077 by Archbishop Gebhard, who made it his residence. It was greatly expanded during the following centuries. This site is not the site of the Roman \"castrum superius\", which was located on the Nonnberg nearby.\nThe state of Salzburg and its counties soon gained more and more influence and power within Bavaria due to the flourishing salt mining and the wide-ranging missionary activities. In 996 Otto III, Holy Roman Emperor rented Archbishop Hartwig the market rights and minting rights (probably also the toll law). The first part of Hohensalzburg Fortress was built in 1077. A city judge was first mentioned in a document in 1120/30. On the left bank of the Salzach, an extensive spiritual district was created with the cathedral, the bishop's residence north-west of the cathedral, the cathedral monastery on its south side, St Peter's monastery, and the Frauengarten (probably after a former women's convent that was dissolved in 1583). Only during the 12th century did the civil settlement begin to spread into the Getreidegasse, the Abtsgasse (Sigmund Haffner-Gasse), and along the quay. Around 1280, the first city fortifications were created. The oldest known city law document dates from the year 1287.\nUnder the prince-bishopric's rule.\nIndependence from Bavaria was secured in the late 14th century. Salzburg was the seat of the Archbishopric of Salzburg, a prince-bishopric of the Holy Roman Empire. As the Reformation movement gained momentum, riots broke out among peasants in the areas surrounding Salzburg. The city was occupied during the German Peasants' War, and the Archbishop had to flee to the safety of the fortress. It was besieged for three months in 1525.\nEventually, tensions were quelled, and the city's independence led to an increase in wealth and prosperity, culminating in the late 16th to 18th centuries under the Prince Archbishops Wolf Dietrich von Raitenau, Markus Sittikus, and Paris Lodron. It was in the 17th century that Italian architects (and Austrians who had studied the Baroque style) rebuilt the city center as it is today, along with many palaces.\nModern era.\nReligious conflict.\nOn 31 October 1731, the 214th anniversary of the 95 Theses, Archbishop Count Leopold Anton von Firmian signed an Edict of Expulsion, the \"Emigrationspatent\", directing all Protestant citizens to recant their non-Catholic beliefs. 21,475 citizens refused to recant their beliefs and were expelled from Salzburg. Most of them accepted an offer by King Friedrich Wilhelm I of Prussia, travelling the length and breadth of Germany to their new homes in East Prussia. The rest settled in other Protestant states in Europe and the British colonies in America.\nIlluminism.\nIn 1772\u20131803, under archbishop Hieronymus Graf von Colloredo, Salzburg was a center of late Illuminism. Colloredo is known for being one of the main employers of Wolfgang Amadeus Mozart. Colloredo often had arguments with Mozart, and he dismissed him by saying, (He should go; I don't need him!). Mozart left Salzburg for Vienna in 1781 with his family, although his father Leopold stayed behind, as he had a close relationship with Colloredo.\nElectorate of Salzburg.\nIn 1803, the archbishopric was secularised by Emperor Napoleon; he transferred the territory to Ferdinando III of Tuscany, former Grand Duke of Tuscany, as the Electorate of Salzburg.\nAustrian and Bavarian rule.\nIn 1805, Salzburg was annexed to the Austrian Empire, along with the Berchtesgaden Provostry. In 1809, the territory of Salzburg was transferred to the Kingdom of Bavaria after Austria's defeat at Wagram. After the Congress of Vienna with the Treaty of Munich (1816), Salzburg was definitively returned to Austria, but without Rupertigau and Berchtesgaden, which remained with Bavaria. Salzburg was integrated into the Province of Salzach, and Salzburgerland was ruled from Linz.\nIn 1850, Salzburg's status was restored as the capital of the Duchy of Salzburg, a crownland of the Austrian Empire. The city became part of Austria-Hungary in 1866 as the capital of a crownland of the Austrian Empire. The nostalgia of the Romantic Era led to increased tourism. In 1892, a funicular was installed to facilitate tourism to Hohensalzburg Fortress.\n20th century.\nFirst Republic.\nFollowing World War\u00a0I and the dissolution of the Austro-Hungarian Empire, Salzburg, as the capital of one of the Austro-Hungarian territories, became part of the new German Austria. In 1918, it represented the residual German-speaking territories of the Austrian heartlands. This was replaced by the First Austrian Republic in 1919, after the Treaty of Saint-Germain-en-Laye (1919).\nAnnexation by Nazi Germany.\nThe Anschluss (the occupation and annexation of Austria, including Salzburg, into Nazi Germany) took place on 12 March 1938, one day before a scheduled referendum on Austria's independence. German troops moved into the city. Political opponents, Jewish citizens and other minorities were subsequently arrested and deported to concentration camps. The synagogue was destroyed.\nWorld War II.\nAfter Germany invaded the Soviet Union, several POW camps for prisoners from the Soviet Union and other enemy nations were arranged in the city.\nDuring the Nazi occupation, a Romani camp was built in Salzburg-Maxglan. It was an Arbeitserziehungslager (work 'education' camp), which provided slave labor to local industry. It also operated as a Zwischenlager (transit camp), holding Roma before their deportation to German camps or ghettos in German-occupied territories in Eastern Europe.\nSalzburg was also the location of five subcamps of the Dachau concentration camp.\nAllied bombing destroyed 7,600 houses and killed 550 inhabitants. Fifteen air strikes destroyed 46 percent of the city's buildings, especially those around Salzburg railway station. Although the town's bridges and the dome of the cathedral were destroyed, much of its Baroque architecture remained intact. As a result, Salzburg is one of the few remaining examples of a town of its style. American troops entered the city on 5 May 1945, and it became the centre of the American-occupied area in Austria. Several displaced persons camps were established in Salzburg\u2014, among them Riedenburg, Camp Herzl (Franz-Josefs-Kaserne), Camp M\u00fclln, Bet Bialik, Bet Trumpeldor, and New Palestine.\nToday.\nAfter World War\u00a0II, Salzburg became the capital city of the Federal State of Salzburg (\"Land Salzburg\") and saw the Americans leave the area once Austria had signed a 1955 treaty re-establishing the country as a democratic and independent nation and subsequently declared its perpetual neutrality. In the 1960s, the city became the shooting location and setting of the family musical film \"The Sound of Music\". On 27 January 2006, the 250th anniversary of the birth of Wolfgang Amadeus Mozart, all 35 churches of Salzburg rang their bells after 8:00\u00a0p.m. (local time) to celebrate the occasion. Major celebrations took place throughout the year.\nAs of 2017 Salzburg had a GDP per capita of \u20ac46,100, which was greater than the average for Austria and most European countries.\nGeography.\nSalzburg is on the banks of the River Salzach, at the northern boundary of the Alps. The mountains to Salzburg's south contrast with the rolling plains to the north. The closest alpine peak, the Untersberg, is less than from the city center. The \"Altstadt\", or \"old town\", is dominated by its baroque towers and churches and the massive Hohensalzburg Fortress. This area is flanked by two smaller hills, the M\u00f6nchsberg and Kapuzinerberg, which offer green relief within the city. Salzburg is approximately east of Munich, northwest of Ljubljana, Slovenia, and west of Vienna. Salzburg has about the same latitude as Seattle.\nDue to its proximity to the Austrian-German border, the greater Salzburg urban area has sometimes (unofficially) been thought of as if it included contiguous parts of Germany: Freilassing (until 1923 known as Salzburghofen), Ainring, and Piding. Public transport planning and multiple public transport lines stretch across the border. \nClimate.\nThe K\u00f6ppen climate classification specifies Salzburg's climate as a warm-summer humid continental climate (\"Dfb\"). However, with the isotherm for the coldest month, Salzburg can be classified as having a four-season oceanic climate (\"Cfb\") with significant temperature differences between seasons. Due to the location at the northern rim of the Alps, the amount of precipitation is comparatively high, mainly in the summer months. The specific drizzle is called \"Schn\u00fcrlregen\" in the local dialect. In winter and spring, pronounced foehn winds regularly occur.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nDemography.\nHistory.\n&lt;templatestyles src=\"Module:Historical populations/styles.css\"/&gt;\nSalzburg's official population significantly increased in 1935 when the city absorbed adjacent municipalities. After World War\u00a0II, numerous refugees found a new home in the city. New residential space was constructed for American soldiers of the postwar occupation and could be used for refugees when they left. Around 1950, Salzburg passed the mark of 100,000 citizens, and in 2016, it reached the mark of 150,000 citizens.\nMigrant communities.\nSalzburg is home to large German, Bosnian, Serbian, and Romanian communities.\nLargest groups of immigrants as of 1 January 2021:\nArchitecture.\nRomanesque and Gothic.\nThe Romanesque and Gothic churches, the monasteries and the early carcass houses dominated the medieval city for a long time. The Cathedral of Archbishop Conrad of Wittelsbach was the largest basilica north of the Alps. The choir of the Franciscan Church, initiated Hans von Burghausen and completed by , is one of the most prestigious religious Gothic constructions of southern Germany. At the end of the Gothic era, Nonnberg Abbey, the Margaret Chapel in St Peter's Abbey, St George's Chapel, and the stately halls of the \"Hoher Stock\" in Hohensalzburg Fortress were constructed.\nRenaissance and baroque.\nInspired by Vincenzo Scamozzi, Prince-Archbishop Wolf Dietrich von Raitenau began to transform the medieval town into the architectural ideals of the late Renaissance. Plans for a massive cathedral by Scamozzi failed to materialize upon the fall of the archbishop. A second cathedral planned by Santino Solari rose as the first early Baroque church in Salzburg. It served as an example for many other churches in Southern Germany and Austria. and continued to rebuild the city with major projects such as Hellbrunn Palace, the prince archbishop's residence, the university buildings, fortifications, and many other buildings. Giovanni Antonio Daria managed, by order of Prince Archbishop Guido von Thun, the construction of the residential well. Giovanni , by order of the same archbishop, created the Erhard and the Kajetan church in the south of the town. The city's redesign was completed with buildings designed by Johann Bernhard Fischer von Erlach, donated by Prince Archbishop Johann Ernst von Thun.\nAfter the era of Ernst von Thun, the city's expansion came to a halt, which is the reason why there are no churches built in the Rococo style. Sigismund von Schrattenbach continued with the construction of \"Sigmundstor\" and the statue of holy Maria on the cathedral square. With the fall and division of the former \"F\u00fcrsterzbistum Salzburg\" (Archbishopric) to Upper Austria, Bavaria (Rupertigau), and Tyrol (Zillertal Matrei) began a long period of urban stagnation. This era didn't end before the period of promoterism (\"Gr\u00fcnderzeit\") brought new life into urban development. The builder dynasty and filled major positions in shaping the city in this era.\nClassical modernism and post-war modernism.\nBuildings of classical modernism and in particular, post-war modernism are frequently encountered in Salzburg. Examples are the Zahnwurzen house (a house in the Linzergasse\u00a022 in the right center of the old town), the \"Lepi\" (public baths in \"Leopoldskron\") (built 1964), and the original 1957 constructed congress-center of Salzburg, which was replaced by a new building in 2001. An important and famous example of the architecture of this era is the 1960 opening of the Gro\u00dfes Festspielhaus by Clemens Holzmeister.\nContemporary architecture.\nAdding contemporary architecture to Salzburg's old town without risking its UNESCO World Heritage status is problematic. Nevertheless, some new structures have been added: the Mozarteum at the Baroque Mirabell Garden (Architecture Robert Rechenauer), the 2001 Congress House (Architecture: Freemasons), the 2011 Unipark Nonntal (Architecture: Storch Ehlers Partners), the 2001 \"Makartsteg\" bridge (Architecture: HALLE1), and the \"Residential and Studio House\" of the architects and in the middle of Salzburg's old town (winner of the ). Other examples of contemporary architecture lie outside the old town: the Faculty of Science building (Universit\u00e4t Salzburg \u2013 Architecture ) built on the edge of free green space, the blob architecture of Red Bull Hangar-7 (Architecture: Volkmar Burgstaller) at Salzburg Airport, home to Dietrich Mateschitz's Flying Bulls and the Europark Shopping Centre. (Architecture: Massimiliano Fuksas)\nDistricts.\nSalzburg has twenty-four urban districts and three extra-urban populations.\nUrban districts (\"Stadtteile\"):\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nExtra-urban populations (\"Landschaftsr\u00e4ume\"):\nMain sights.\nSalzburg is a tourist favourite, with the number of visitors outnumbering locals by a large margin in peak times. In addition to Mozart's birthplace noted above, other notable places include:\nOld Town\nOutside the Old Town\nGreater Salzburg area\nEducation.\nSalzburg is a center of education and home to three universities, as well as several professional colleges and gymnasiums (high schools).\nTransport.\nSalzburg Hauptbahnhof is served by comprehensive rail connections, with frequent east\u2013west trains serving Vienna, Munich, Innsbruck, and Z\u00fcrich, including daily high-speed ICE services. North\u2013south rail connections also serve popular destinations such as Venice and Prague. The city acts as a hub for southbound trains through the Alps into Italy.\nSalzburg Airport has scheduled flights to European cities such as Frankfurt, Vienna, London, Rotterdam, Amsterdam, Brussels, D\u00fcsseldorf, and Z\u00fcrich, as well as Naples, Hamburg, Edinburgh and Dublin. In addition to these, there are numerous charter flights.\nIn the main city, there is the Salzburg trolleybus system and bus system with a total of more than 20 lines, and service every 10 minutes. Salzburg has an S-Bahn system with four Lines (S1, S2, S3, S11), trains depart from the main station every 30 minutes, and they are part of the \u00d6BB network. Suburb line number S1 reaches the world-famous Silent Night chapel in Oberndorf in about 25 minutes.\nPopular culture.\nIn the 1960s, \"The Sound of Music\", based on the true story of Maria von Trapp, who took up with an aristocratic family and fled the German Anschluss, used locations in Salzburg and Salzburg State as filming location.\nThe city briefly appears on the map when Indiana Jones travels through the city in the 1989 film \"Indiana Jones and the Last Crusade\".\nSalzburg is the setting for the Austrian crime series \"Stockinger\" and an Austrian-German television crime drama series \"Der Pass\".\nIn the 2010 film \"Knight &amp; Day\", Salzburg serves as the backdrop for a large portion of the film.\nSalzburg was featured as one of the mystery destinations on the NBC reality competition series \"Destination X\" in 2025.\nLanguage.\nAustrian German is widely written and differs from Germany's standard variation only in some vocabulary and a few grammar points. Salzburg belongs to the region of Austro-Bavarian dialects, in particular Central Bavarian. It is widely spoken by young and old alike although professors of linguistics from the Universit\u00e4t Salzburg, Irmgard Kaiser, and Hannes Scheutz, have seen over the past few years a reduction in the number of dialect speakers in the city. Although more and more school children are speaking standard German, Scheutz feels it has less to do with parental influence and more to do with media consumption.\nSports.\nFootball.\nThe former SV Austria Salzburg reached the UEFA Cup final in 1994. On 6 April 2005, Red Bull bought the club and changed its name to FC Red Bull Salzburg. The home stadium of Red Bull Salzburg is the Wals Siezenheim Stadium in a suburb in the agglomeration of Salzburg and was one of the venues for the 2008 European Football Championship. FC Red Bull Salzburg plays in the Austrian Bundesliga.\nAfter Red Bull had bought the SV Austria Salzburg and changed its name and team colors, some supporters of the club decided to leave and form a new club with the old name and old colors, wanting to preserve the traditions of their club. The reformed SV Austria Salzburg was founded in 2005 and at one point played in the Erste Liga, only one tier below the Bundesliga. However, in recent years, they have struggled to climb back up to the Austrian second tier, and since 2019, they have competed in the Regionalliga Salzburg in the Austrian Football third tier.\nIce hockey.\nRed Bull also sponsors the local ice hockey team, the EC Salzburg Red Bulls. The team plays in the Erste Bank Eishockey Liga, an Austria-headquartered cross-border league featuring the best teams from Austria, Hungary, Slovenia, Croatia, and Italy, as well as one Czech team.\nOther sports.\nSalzburg was a candidate city for the 2010 and 2014 Winter Olympics, but lost to Vancouver and Sochi respectively.\nInternational relations.\nTwin towns\u2014sister cities.\nSalzburg is twinned with:\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "40336", "revid": "27015025", "url": "https://en.wikipedia.org/wiki?curid=40336", "title": "Rhododendron", "text": "Genus of flowering plants\nRhododendron (; pl.: rhododendra), from Ancient Greek \u1fe5\u03cc\u03b4\u03bf\u03bd (\"rh\u00f3don\"), meaning \"rose\", and \u03b4\u03ad\u03bd\u03b4\u03c1\u03bf\u03bd (\"d\u00e9ndron\"), meaning \"tree\", is a very large genus of about 1,024 species of woody plants in the heath family (Ericaceae). They can be either evergreen or deciduous. Most species are native to eastern Asia and the Himalayan region, but smaller numbers occur elsewhere in Asia, and in North America, Europe and Australia.\nIt is the national flower of Nepal, the state flower of Washington and West Virginia in the United States, the state flower of Nagaland and Himachal Pradesh in India, the provincial flower of Jeju Province in South Korea, the provincial flower of Jiangxi in China and the state tree of Sikkim and Uttarakhand in India. Most species have brightly coloured flowers which bloom from late winter through to early summer.\nAzaleas make up two subgenera of \"Rhododendron\". They are distinguished from \"true\" rhododendrons by having only five anthers per flower.\nDescription.\n\"Rhododendron\" is a genus of shrubs and small to (rarely) large trees, the smallest species, \"R. cespitosum\" of New Guinea, growing to tall, and the largest, \"R. protistum\" var. \"giganteum\", reported to tall. The leaves are spirally arranged; leaf size can range from to over , exceptionally in \"R. sinogrande\". They may be either evergreen or deciduous. In some species, the undersides of the leaves are covered with scales (lepidote) or hairs (indumentum). Some of the best known species are noted for their many clusters of large flowers. A recently discovered species in New Guinea has flowers up to six inches (fifteen centimeters) in width, the largest in the whole genus. The accompanying photograph shows it as having seven petals. There are alpine species with small flowers and small leaves, and tropical species such as section \"Vireya\" that often grow as epiphytes. Species in this genus may be part of the heath complex in oak-heath forests in eastern North America.\nThey have frequently been divided based on the presence or absence of scales on the abaxial (lower) leaf surface (lepidote or elepidote). These scales, unique to subgenus \"Rhododendron\", are modified hairs consisting of a polygonal scale attached by a stalk.\n\"Rhododendron\" are characterised by having inflorescences with scarious (dry) perulae, a chromosome number of x=13, fruit that has a septicidal capsule, an ovary that is superior (or nearly so), stamens that have no appendages, and agglutinate (clumped) pollen.\nTaxonomy.\n\"Rhododendron\" is the largest genus in the family Ericaceae, with over 1,000 species, (though estimates vary from 850 to 1,200) and is morphologically diverse. Consequently, the taxonomy has been historically complex. It is currently the only genus accepted in the tribe Rhodoreae, though historically several subgroups have been given generic rank, including \"Azalea\", \"Ledum\" and \"Menziesia\".\nEarly history.\nAlthough Rhododendrons had been known since the description of \"Rhododendron hirsutum\" by Charles de l'\u00c9cluse (Clusius) in the sixteenth century, and were known to classical writers (Magor 1990), and referred to as \"Chamaerhododendron\" (low-growing rose tree), the genus was first formally described by Linnaeus in his \"Species Plantarum\" in 1753. He listed five species under \"Rhododendron\": \"R.\"\u00a0\"ferrugineum\" (the type species), \"R.\u00a0dauricum\", \"R.\u00a0hirsutum\", \"R.\u00a0chamaecistus\" (now \"Rhodothamnus chamaecistus\" (L.) Rchb.) and \"R.\u00a0maximum\". At that time he considered the then known six species of \"Azalea\" that he had described earlier in 1735 in his \"Systema Naturae\" as a separate genus.\nLinnaeus' six species of \"Azalea\" were \"Azalea indica\", \"A.\u00a0pontica\", \"A.\u00a0lutea\", \"A.\u00a0viscosa\", \"A.\u00a0lapponica\" and \"A.\u00a0procumbens\" (now \"Kalmia procumbens\"), which he distinguished from \"Rhododendron\" by having five stamens, as opposed to ten. As new species of what are now considered \"Rhododendron\" were discovered, they were assigned to separate genera if they seemed to differ significantly from the type species. For instance \"Rhodora\" (Linnaeus 1763) for \"Rhododendron canadense\", \"Vireya\" (Blume 1826) and \"Hymenanthes\" (Blume 1826) for \"Rhododendron metternichii\", now \"R. degronianum\". Meanwhile, other botanists such as Salisbury (1796) and Tate (1831) began to question the distinction between \"Azalea\" and \"Rhododendron\", and finally in 1836, \"Azalea\" was incorporated into \"Rhododendron\" and the genus divided into eight sections. Of these \"Tsutsutsi\" (\"Tsutsusi\"), \"Pentanthera\", \"Pogonanthum\", \"Ponticum\" and \"Rhodora\" are still used, the other sections being \"Lepipherum\", \"Booram\", and \"Chamaecistus\". This structure largely survived till recently (2004), following which the development of molecular phylogeny led to major re-examinations of traditional morphological classifications, although other authors such as Candolle, who described six sections, used slightly different numeration.\nSoon, as more species became available in the nineteenth century so did a better understanding of the characteristics necessary for the major divisions. Chief amongst these were Maximovicz's \"Rhododendreae Asiae Orientali\" and Planchon. Maximovicz used flower bud position and its relationship with leaf buds to create eight \"Sections\". Bentham and Hooker used a similar scheme, but called the divisions \"Series\". It was not until 1893 that Koehne appreciated the significance of scaling and hence the separation of lepidote and elepidote species. The large number of species that were available by the early twentieth century prompted a new approach when Balfour introduced the concept of grouping species into series. \"The Species of Rhododendron\" referred to this series concept as the Balfourian system. That system continued up to modern times in Davidian's four volume \"The Rhododendron Species\".\nModern classification.\nThe next major attempt at classification was by Sleumer who from 1934 began incorporating the Balfourian series into the older hierarchical structure of subgenera and sections, according to the International Code of Botanical Nomenclature, culminating in 1949 with his \"Ein System der Gattung \"Rhododendron\"\" and subsequent refinements. Most of the Balfourian series are represented by Sleumer as subsections, though some appear as sections or even subgenera. Sleumer based his system on the relationship of the flower buds to the leaf buds, habitat, flower structure, and whether the leaves were lepidote or non-lepidote. While Sleumer's work was widely accepted, many in the United States and the United Kingdom continued to use the simpler Balfourian system of the Edinburgh group.\nSleumer's system underwent many revisions by others, predominantly the Edinburgh group in their continuing Royal Botanic Garden Edinburgh notes. Cullen of the Edinburgh group, placing more emphasis on the lepidote characteristics of the leaves, united all of the lepidote species into subgenus \"Rhododendron\", including four of Sleumer's subgenera (\"Rhododendron\", \"Pseudoazalea\", \"Pseudorhodorastrum\", \"Rhodorastrum\"). In 1986 Philipson &amp; Philipson raised two sections of subgenus \"Aleastrum\" (\"Mumeazalea\", \"Candidastrum\") to subgenera, while reducing genus \"Therorhodion\" to a subgenus of \"Rhododendron\". In 1987 Spethmann, adding phytochemical features proposed a system with fifteen subgenera grouped into three 'chorus' subgenera.\nA number of closely related genera had been included together with \"Rhododendron\" in a former tribe, Rhodoreae. These have been progressively incorporated into \"Rhododendron\". Chamberlain and Rae moved the monotypic section \"Tsusiopsis\" together with the monotypic genus \"Tsusiophyllum\" into section \"Tsutsusi\", while Kron &amp; Judd reduced genus \"Ledum\" to a subsection of section \"Rhododendron\". Then Judd &amp; Kron moved two species (\"R.\"\u00a0\"schlippenbachii\" and \"R.\"\u00a0\"quinquefolium\") from section \"Brachybachii\", subgenus \"Tsutsusi\" and two from section \"Rhodora\", subgenus \"Pentanthera\" (\"R.\u00a0albrechtii\", \"R.\u00a0pentaphyllum\") into section \"Sciadorhodion\", subgenus \"Pentanthera\". Finally Chamberlain brought the various systems together in 1996, with 1,025 species divided into eight subgenera. Goetsch (2005) provides a comparison of the Sleumer and Chamberlain schemata (Table\u00a01).\nPhylogeny.\nThe era of molecular analysis rather than descriptive features can be dated to the work of Kurashige (1988) and Kron (1997) who used matK sequencing. Later Gao \"et al.\" (2002) used ITS sequences to determine a cladistic analysis. They confirmed that the genus \"Rhododendron\" was monophyletic, with subgenus \"Therorhodion\" in the basal position, consistent with the matK studies. Following publication of the studies of Goetsch \"et al.\" (2005) with RPB2, there began an ongoing realignment of species and groups within the genus, based on evolutionary relationships. Their work was more supportive of Sleumer's original system than the later modifications introduced by Chamberlain \"et al.\".\nThe major finding of Goetsch and colleagues was that all species examined (except \"R.\u00a0camtschaticum\", subgenus \"Therorhodion\") formed three major clades which they labelled A, B, and C, with the subgenera \"Rhododendron\" and \"Hymenanthes\" as monophyletic groups nested within clades A and B, respectively. By contrast subgenera \"Azaleastrum\" and \"Pentanthera\" were polyphyletic, while \"R.\u00a0camtschaticum\" appeared as a sister to all other rhododendrons. The small polyphyletic subgenera \"Pentanthera\" and \"Azaleastrum\" were divided between two clades. The four sections of \"Pentanthera\" between clades B and C, with two each, while \"Azaleastrum\" had one section in each of A and C.\nThus subgenera \"Azaleastrum\" and \"Pentanthera\" needed to be disassembled, and \"Rhododendron\", \"Hymenanthes\" and \"Tsutsusi\" correspondingly expanded. In addition to the two separate genera included under \"Rhododendron\" by Chamberlain (\"Ledum\", \"Tsusiophyllum\"), Goetsch \"et al.\". added \"Menziesia\" (clade C). Despite a degree of paraphyly, the subgenus \"Rhododendron\" was otherwise untouched with regard to its three sections but four other subgenera were eliminated and one new subgenus created, leaving a total of five subgenera in all, from eight in Chamberlain's scheme. The discontinued subgenera are \"Pentanthera\", \"Tsutsusi\", \"Candidastrum\" and \"Mumeazalea\", while a new subgenus was created by elevating subgenus \"Azaleastrum\" section \"Choniastrum\" to subgenus rank.\nSubgenus \"Pentanthera\" (deciduous azaleas) with its four sections was dismembered by eliminating two sections and redistributing the other two between the existing subgenera in clades B (\"Hymenanthes\") and C (\"Azaleastrum\"), although the name was retained in section \"Pentanthera\" (14\u00a0species) which was moved to subgenus \"Hymenanthes\". Of the remaining three sections, monotypic \"Viscidula\" was discontinued by moving \"R.\"\u00a0\"nipponicum\" to \"Tsutsusi\" (C), while \"Rhodora\" (2\u00a0species) was itself polyphyletic and was broken up by moving \"R.\"\u00a0\"canadense\" to section \"Pentanthera\" (B) and \"R.\"\u00a0\"vaseyi\" to section \"Sciadorhodion\", which then became a new section of subgenus \"Azaleastrum\"\u00a0(C).\nSubgenus \"Tsutsusi\" (C) was reduced to section status retaining the name, and included in subgenus \"Azaleastrum\". Of the three minor subgenera, all in C, two were discontinued. The single species of monotypic subgenus \"Candidastrum\" (\"R.\"\u00a0\"albiflorum\") was moved to subgenus \"Azaleastrum\", section \"Sciadorhodion\". Similarly the single species in monotypic subgenus \"Mumeazalea\" (\"R.\"\u00a0\"semibarbatum\") was placed in the new section \"Tsutsusi\", subgenus \"Azaleastrum\". Genus \"Menziesa\" (9\u00a0species) was also added to section \"Sciadorhodion\". The remaining small subgenus \"Therorhodion\" with its two species was left intact. Thus two subgenera, \"Hymenanthes\" and \"Azaleastrum\" were expanded at the expense of four subgenera that were eliminated, although \"Azaleastrum\" lost one section (\"Choniastrum\") as a new subgenus, since it was a distinct subclade in\u00a0A. In all, \"Hymenanthes\" increased from one to two sections, while \"Azaleastrum\", by losing one section and gaining two increased from two to three sections. (See schemata under \"Subgenera\".)\nSubsequent research has supported the revision by Goetsch, although has largely concentrated on further defining the phylogeny within the subdivisions. In 2011 the two species of \"Diplarche\" were also added to \"Rhododendron\", \"incertae sedis\".\nSubdivision.\nThis genus has been progressively subdivided into a hierarchy of subgenus, section, subsection, and species.\nSubgenera.\nTerminology from the Sleumer (1949) system is frequently found in older literature, with five subgenera and is as follows;\nIn the later traditional classification, attributed to Chamberlain (1996), and as used by horticulturalists and the American Rhododendron Society, \"Rhododendron\" has eight subgenera based on morphology, namely the presence of scales (lepidote), deciduousness of leaves, and the floral and vegetative branching patterns, after Sleumer (1980). These consist of four large and four small subgenera. The first two subgenera (\"Rhododendron\" and \"Hymenanthes\") represent the species commonly considered as 'Rhododendrons'. The next two smaller subgenera (\"Pentanthera\" and \"Tsutsusi\") represent the 'Azaleas'. The remaining four subgenera contain very few species. The largest of these is subgenus \"Rhododendron\", containing nearly half of all known species and all of the lepidote species.\nFor a comparison of the Sleumer and Chamberlain systems, see Goetsch \"et al.\" (2005) Table 1.\nThis division was based on a number of what were thought to be key morphological characteristics. These included the position of the inflorescence buds (terminal or lateral), whether lepidote or elepidote, deciduousness of leaves, and whether new foliage was derived from axils from previous year's shoots or the lowest scaly leaves.\nFollowing the cladistic analysis of Goetsch \"et al.\" (2005) this scheme was simplified, based on the discovery of three major clades (A, B, C) as follows.\nClade A\nClade B\nClade C\nSister taxon\nSections and subsections.\nThe larger subgenera are further subdivided into sections and subsections Some subgenera contain only a single section, and some sections only a single subsection. Shown here is the traditional classification, with species number after Chamberlain (1996), but this scheme is undergoing constant revision. Revisions by Goetsch \"et al.\" (2005) and by Craven \"et al.\" (2008) shown in (\"parenthetical italics\"). Older ranks such as Series (groups of species) are no longer used but may be found in the literature, but the American Rhododendron Society still uses a similar device, called Alliances\nThe system used by the World Flora Online as of December\u00a02023[ [update]] uses six subgenera, four of which are divided further:\nDistribution and habitat.\nSpecies of the genus \"Rhododendron\" are widely distributed between latitudes 80\u00b0N and 20\u00b0S and are native to areas from North America to Europe, Russia, and Asia, and from Greenland to Queensland, Australia and the Solomon Islands. The centres of diversity are in the Himalayas and Maritime Southeast Asia, with the greatest species diversity in the Sino-Himalayan region, Southwest China and northern Burma, from India \u2013 Himachal Pradesh, Uttarakhand, Sikkim and Nagaland to Nepal, northwestern Yunnan and western Sichuan and southeastern Tibet. Other significant areas of diversity are in the mountains of Korea, Japan and Taiwan. More than 90% of \"Rhododendron\" \"sensu\" Chamberlain belong to the Asian subgenera \"Rhododendron\", \"Hymenanthes\" and section \"Tsutsusi\". Of the first two of these, the species are predominantly found in the area of the Himalayas and Southwest China (Sino-Himalayan Region).\nThe 300 tropical species within the \"Vireya\" section of subgenus \"Rhododendron\" occupy the Maritime Southeast Asia from their presumed Southeast Asian origin to Northern Australia, with 55 known species in Borneo and 164 in New Guinea. The species in New Guinea are native to subalpine moist grasslands at around 3,000 metres above sea level in the Central Highlands. Subgenera \"Rhododendron\" and \"Hymenanthes\", together with section \"Pentanthera\" of subgenus \"Pentanthera\" are also represented to a lesser degree in the Mountainous areas of North America and Western Eurasia. Subgenus \"Tsutsusi\" is found in the maritime regions of East Asia (Japan, Korea, Taiwan, East China), but not in North America or Eurasia.\nIn the United States, native \"Rhododendron\" mostly occur in lowland and montane forests in the Pacific Northwest, California, the Northeast, and the Appalachian Mountains.\nEcology.\nInvasive species.\n\"Rhododendron ponticum\" has become invasive in Ireland and the United Kingdom. It is an introduced species, spreading in woodland areas and replacing the natural understory. \"R. ponticum\" is difficult to eradicate, as its roots can make new shoots.\nInsects.\nA number of insects either target rhododendrons or will opportunistically attack them. Rhododendron borers and various weevils are major pests of rhododendrons, and many caterpillars will preferentially devour them.\n\"Rhododendron\" species are used as food plants by the larvae (caterpillars) of some butterflies and moths; see List of Lepidoptera that feed on rhododendrons.\nDiseases.\nMajor diseases include \"Phytophthora\" root rot, stem and twig fungal dieback.\nRhododendron bud blast, a fungal condition that causes buds to turn brown and dry before they can open, is caused by the fungus \"Pycnostysanus azaleae\", which may be brought to the plant by the rhododendron leafhopper, \"Graphocephala fennahi\".562\nConservation.\nIn the UK the forerunner of the Rhododendron, Camellia and Magnolia Group (RCMG), The Rhododendron Society was founded in 1916. while in Scotland species are being conserved by the Rhododendron Species Conservation Group.\nCultivation.\nBoth species and hybrid rhododendrons (including azaleas) are used extensively as ornamental plants in landscaping in many parts of the world, including both temperate and subtemperate regions. Many species and cultivars are grown commercially for the nursery trade.\nRhododendrons can be propagated by air layering or stem cuttings. They can self-propagate by sending up shoots from the roots. Sometimes an attached branch that has drooped to the ground will root in damp mulch and the resulting rooted plant then can be cut off the parent rhododendron. They can also be reproduced by seed dispersal or by horticulturalists collecting the spent flower buds and saving and drying the seed for later germination and planting.\nRhododendrons are often valued in landscaping for their structure, size, flowers, and the fact that many of them are evergreen. Azaleas are frequently used around foundations and occasionally as hedges, and many larger-leafed rhododendrons lend themselves well to more informal plantings and woodland gardens, or as specimen plants. In some areas, larger rhododendrons can be pruned to encourage more tree-like form, with some species such as \"Rhododendron arboreum\" and \"R.\u00a0falconeri\" eventually growing to a height of or more.\nCommercial growing.\nRhododendrons are grown commercially in many areas for sale, and seeds were occasionally collected in the wild, a practice now rare in most areas due to the Nagoya Protocol. Larger commercial growers often ship long distances; in the United States, most of them are on the west coast (Oregon, Washington state and California). Large-scale commercial growing often selects for different characteristics than hobbyist growers might want, such as resistance to root rot when overwatered, ability to be forced into budding early, ease of rooting or other propagation, and saleability.\nHorticultural divisions.\nHorticulturally, rhododendrons may be divided into the following groups:\nPlanting and care.\nLike other ericaceous plants, most rhododendrons prefer acid soils with a pH of roughly 4.5\u20135.5; some tropical Vireyas and a few other rhododendron species grow as epiphytes and require a planting mix similar to orchids. Rhododendrons have fibrous roots and prefer well-drained soils high in organic material. In areas with poorly drained or alkaline soils, rhododendrons are often grown in raised beds using media such as composted pine bark. Mulching and careful watering are important, especially before the plant is established.\nA new calcium-tolerant stock of rhododendrons (trademarked as 'Inkarho') has been exhibited at the RHS Chelsea Flower Show in London (2011). Individual hybrids of rhododendrons have been grafted on to a rootstock on a single rhododendron plant that was found growing in a chalk quarry. The rootstock is able to grow in calcium-rich soil up to a pH of 7.5.\nHybrids.\nRhododendrons are extensively hybridized in cultivation, and natural hybrids often occur in areas where species ranges overlap. There are over 28,000 cultivars of Rhododendron in the International Rhododendron Registry held by the Royal Horticultural Society. Most have been bred for their flowers, but a few are of garden interest because of ornamental leaves and some for ornamental bark or stems. Some hybrids have fragrant flowers\u2014such as the Loderi hybrids, created by crossing \"Rhododendron fortunei\" and \"R.\u00a0griffithianum\". Other examples include the PJM hybrids, formed from a cross between \"Rhododendron carolinianum\" and \"R.\u00a0dauricum\", and named after Peter J. Mezitt of Weston Nurseries, Massachusetts.\nToxicity.\nSome species of rhododendron are poisonous to grazing animals because of a toxin called grayanotoxin in their pollen and nectar. People have been known to become ill from eating mad honey made by bees feeding on rhododendron and azalea flowers. Xenophon described the odd behaviour of Greek soldiers after having consumed honey in a village surrounded by \"Rhododendron ponticum\" during the march of the Ten Thousand in 401\u00a0BCE. Pompey's soldiers reportedly suffered lethal casualties following the consumption of honey made from \"Rhododendron\" deliberately left behind by Pontic forces in 67\u00a0BCE during the Third Mithridatic War. Later, it was recognized that honey resulting from these plants has a slightly hallucinogenic and laxative effect. The suspect rhododendrons are \"Rhododendron ponticum\" and \"Rhododendron luteum\" (formerly \"Azalea pontica\"), both found in northern Asia Minor. Eleven similar cases during the 1980s have been documented in Istanbul, Turkey. Rhododendron is extremely toxic to horses, with some animals dying within a few hours of ingesting the plant, although most horses tend to avoid it if they have access to good forage. Rhododendron, including its stems, leaves and flowers, contains toxins that, if ingested by a cat's stomach, can cause seizures and even coma and death.\nUses.\n\"Rhododendron\" species have long been used in traditional medicine.\nIn Nepal, the flower is considered edible and consumed for its sour taste. The pickled flower can last for months, and the flower juice is also marketed. The flower, fresh or dried, is added to fish curry in the belief that it will soften the bones. The juice of rhododendron flower is used to make a squash called burans (named after the flower) in the hilly regions of Uttarakhand. It is admired for its distinctive flavour and colour.\nLabrador tea.\nThe herbal tea called Labrador tea (not a true tea) is made from one of three closely related species: \nIn culture.\nIn Uttarakhand, in north India, the Buransh flower is deeply embedded in local culture, playing a significant role in festivals like Holi and weddings, where it is used in garlands and decorations to bless attendees. The flower is also utilized in making a healthful, antioxidant-rich juice that is popular during local festivities and summer months. Additionally, Buransh flowers are incorporated into local arts and crafts, where they are used to make colourful necklaces and jewelry, symbolizing the spiritual and physical prosperity of the community.\nThe rhododendron is the national flower of Nepal.\nIn the language of flowers, the rhododendron symbolizes danger and to beware.\n\"Rhododendron arboreum\" (\"lali guransh\") is the national flower of Nepal. \"R. ponticum\" is the state flower of Indian-administered Jammu and Kashmir and Pakistan-administered Azad Kashmir. \"Rhododendron niveum\" is the state tree of Sikkim in India. \"Rhododendron arboreum\" is also the state tree of the state of Uttarakhand, India. Pink Rhododendron (\"Rhododendron campanulatum\") is the state flower of Himachal Pradesh, India. Rhododendron is also the provincial flower of Jiangxi, China and the state flower of Nagaland, the 16th state of the Indian Union.\n\"Rhododendron maximum\", the most widespread rhododendron of the Appalachian Mountains, is the state flower of the US state of West Virginia, and is in the Flag of West Virginia.\n\"Rhododendron macrophyllum\", a widespread rhododendron of the Pacific Northwest, is the state flower of the US state of Washington.\nAmongst the Zomi tribes in India and Myanmar, \"Rhododendrons\" called \"Ngeisok\" is used in a poetic manner to signify a lady.\nIn media.\nThe nineteenth-century American poet and essayist Ralph Waldo Emerson in 1834 wrote a poem titled \"The Rhodora, On Being Asked, Whence Is the Flower\".\nRhododendrons play a role and are soliloquized in James Joyce's \"Ulysses\". The flowers are referenced throughout Daphne Du Maurier's novel \"Rebecca\" (1938) and in Sharon Creech's young adult novel \"Walk Two Moons\" (1994). British author Jasper Fforde also uses rhododendron as a motif throughout many of his books, e.g. the \"Thursday Next\" series and \"Shades of Grey\" (2009).\nThe effects of \"R.\u00a0ponticum\" were mentioned in the 2009 film \"Sherlock Holmes\" as a proposed way to arrange a fake execution. It was also mentioned in the third episode of Season\u00a02 of BBC's \"Sherlock\", speculated to have been a part of Sherlock's fake death scheme.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nAdditional resources.\nRecords of the Rhododendron Society of America reside at the Albert and Shirley Small Special Collections Library at the University of Virginia."}
{"id": "40337", "revid": "33924195", "url": "https://en.wikipedia.org/wiki?curid=40337", "title": "Fetchmail", "text": "Email retriever utility\nFetchmail is an open-source software utility for POSIX-compliant operating systems which is used to retrieve e-mail from a remote POP3, IMAP, or ODMR mail server to the user's local system. It was developed from the https:// program, written by Carl Harris.\nIts chief significance is perhaps that its author, Eric S. Raymond, used it as a model to discuss his theories of open-source software development in a widely read and influential essay on software development methodologies \"The Cathedral and the Bazaar\".\nDesign.\nBy design, Fetchmail's only means of delivering messages is by submitting them to the local MTA/Message transfer agent or invoking a mail delivery agent like procmail, maildrop, or sendmail; delivering directly to mail folders such as maildir is not supported.\nIt is a C program evolved by gradual mutation from an ancestor already written in C.\nDan Bernstein, getmail creator Charles Cazabon and FreeBSD developer Terry Lambert, have criticized Fetchmail's design, its number of security holes, and that it was prematurely put into \"maintenance mode\". In 2004, a new team of maintainers took over Fetchmail development, and laid out development plans that broke with design decisions that Eric Raymond had made in earlier versions.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nExternal links.\nhttps://sourceforge.net/directory/os:windows/?q=fetchmail"}
{"id": "40338", "revid": "41219559", "url": "https://en.wikipedia.org/wiki?curid=40338", "title": "Mayor of London", "text": "Head of the government of Greater London\nThe mayor of London is the chief executive of the Greater London Authority. The role was created in 2000 after the Greater London devolution referendum in 1998, and was the first directly elected mayor in the United Kingdom.\nThe current mayor is Sir Sadiq Khan, who took office on 9 May 2016. The position was held by Ken Livingstone from the creation of the role on 4 May 2000 until he was defeated in May 2008 by Boris Johnson, who then also served two terms before being succeeded by Khan.\nThe mayor is scrutinised by the London Assembly and, supported by their Mayoral Cabinet, is responsible for the strategic government of the entirety of London, including the City of London (for which there is also the Lord Mayor of the City of London). Each of the 32 London Boroughs also has a ceremonial mayor or, in Croydon, Hackney, Lewisham, Newham and Tower Hamlets, an elected mayor. The mayor of London is elected by the largest single-member electorate in the United Kingdom.\nBackground.\nThe Greater London Council, the elected government for Greater London, was abolished in 1986 by the Local Government Act 1985. Strategic functions were split off to various joint arrangements. Londoners voted in a referendum in 1998 to create a new governance structure for Greater London. The directly elected mayor of London was created by the Greater London Authority Act 1999 in 2000 as part of the reforms.\nElections.\nThe mayor is elected by the first-past-the-post system for a fixed term of four years, with elections taking place in May. Prior to the Elections Act 2022, the supplementary vote method was used. There are no limits on the number of terms a mayor may serve. The mayor is elected by the largest single-member electorate in the United Kingdom.\nAs with most elected posts in the United Kingdom, there is a deposit (in this case of \u00a310,000), which is returnable on the candidate's winning of at least 5% of votes cast.\nMost recent election.\nThe most recent London mayoral election was held on 2 May 2024. The results of the election were announced on 4 May 2024. Sadiq Khan was re-elected as mayor and became the first to be elected for three terms, beating the Conservative Susan Hall.\nPowers and functions.\nMost powers are derived from the Greater London Authority Act 1999, with additional functions coming from the Greater London Authority Act 2007, the Localism Act 2011 and Police Reform and Social Responsibility Act 2011.\nThe mayor's main functions are:\nThe remaining local government functions are performed by the London borough councils. There is some overlap; for example, the borough councils are responsible for waste management, but the mayor is required to produce a waste management strategy. In 2010, Johnson launched an initiative in partnership with the Multi-academy Trust AET to transform schools across London. This led to the establishment of London Academies Enterprise Trust (LAET) which was intended to be a group of ten academies, but it only reached a group of four before the mayor withdrew it in 2013. The mayor is a member of the Mayoral Council for England and the Council of the Nations and Regions.\nThe following is a table comparing power over services of the boroughs to the GLA and mayor.\nInitiatives.\nKen Livingstone.\nInitiatives taken by Ken Livingstone as Mayor of London included the London congestion charge on private vehicles using city centre London on weekdays, the creation of the London Climate Change Agency, the London Energy Partnership and the founding of the international Large Cities Climate Leadership Group, now known as C40 Cities Climate Leadership Group. The congestion charge led to many new buses being introduced across London. In August 2003, Livingstone oversaw the introduction of the Oyster card electronic ticketing system for Transport for London services. Livingstone supported the withdrawal of the vintage AEC Routemaster buses from regular service in London.\nLivingstone introduced the London Partnerships Register which was a voluntary scheme without legal force for same sex couples to register their partnership, and paved the way for the introduction by the United Kingdom Parliament of civil partnerships and later still, Same-sex marriage. Unlike civil partnerships, the London Partnerships Register was open to heterosexual couples who favour a public commitment other than marriage.\nAs Mayor of London, Livingstone was a supporter of the London Olympics in 2012, ultimately winning the bid to host the Games in 2005. Livingstone encouraged sport in London; especially when sport could be combined with helping charities like The London Marathon and 10K charity races. Livingstone, in a mayoral election debate on the BBC's \"Question Time\" in April 2008, stated that the primary reason he supported the Olympic bid was to secure funding for the redevelopment of the East End of London. In July 2007, he brought the Tour de France cycle race to London.\nBoris Johnson.\nIn May 2008, Boris Johnson introduced a new transport safety initiative to put 440 high visibility police officers in and around bus stations. A ban on alcohol on underground, and Docklands Light Railway, tram services and stations across the capital was introduced.\nAlso in May 2008, he announced the closure of \"The Londoner\" newspaper, saving approximately \u00a32.9\u00a0million. A percentage of this saving was to be spent on planting 10,000 new street trees.\nIn 2010, he extended the coverage of Oyster card electronic ticketing to all National Rail overground train services. Also in 2010, he opened a cycle hire scheme (originally sponsored by Barclays, now Santander) with 5,000 bicycles available for hire across London. Although initiated by his predecessor, Ken Livingstone, the scheme rapidly acquired the nickname of \"Boris Bikes\". Johnson withdrew the recently introduced high-speed high-capacity \"bendy buses\" from service in 2011 which had been bought by Livingstone, and he instead supported the development of the New Routemaster which entered service the next year.\nIn 2011, Boris Johnson set up the Outer London Fund of \u00a350\u00a0million designed to help facilitate improve local high streets. Areas in London were given the chance to submit proposals for two tranches of funding. Successful bids for Phase 1 included Enfield, Muswell Hill and Bexley town centre. As of 2011[ [update]], the recipients of phase 2 funding were still to be announced.\nIn January 2013, he appointed journalist Andrew Gilligan as the first Cycling Commissioner for London. In March 2013, Johnson announced \u00a31\u00a0billion of investment in infrastructure to make cycling safer in London, including a East to West segregated 'Crossrail for bikes'.\nAt the general election of 7 May 2015, Johnson was elected MP for Uxbridge and South Ruislip, He continued to serve as mayor until the mayoral election in May 2016, when Sadiq Khan was elected.\nSir Sadiq Khan.\nSir Sadiq Khan introduced the 'bus hopper' fare on TfL buses, which allows passengers to board a second bus within one hour for the same fare. Under Khan, paper, coin and cash transactions became obsolete. The Oyster system was expanded to include debit and credit cards. This initiative was started under his predecessor, Johnson.\nUpon election, Khan outlined a vision to make London the \"greenest city\" by investing in walking and cycling infrastructure while reducing polluting vehicles. In 2019, the \"Ultra Low Emission Zone\" scheme was launched which taxes highly polluting vehicles in its covered territory. London declared itself the world's first \"National Park City\" (effective from July 2019), reflecting its unusually high amount of green space for a city of its size.\nExtended term.\nThe Government postponed all elections due in May 2020, including for the mayor of London, for one year due to the COVID-19 pandemic. Khan had therefore served a term in office of five years rather than four, which ended in May 2021. He was re-elected in 2021 for a shortened three-year term, defeating the Conservative candidate Shaun Bailey.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "40339", "revid": "20376192", "url": "https://en.wikipedia.org/wiki?curid=40339", "title": "Lady Mayor of London", "text": "Mayor of the City of London and leader of the City of London Corporation\nThe Lady Mayor of London is the mayor of the City of London, England, and the leader of the City of London Corporation. Within the City, the Lady Mayor is accorded precedence over all individuals except the sovereign and retains various traditional powers, rights, and privileges, including the title and style \"The Right Honourable Lady Mayor of London\".\nOne of the world's oldest continuously elected civic offices, it is entirely separate from the directly elected mayor of London, a political office controlling a budget which covers the much larger area of Greater London. Dame Susan Langley serves as the 697th Lord/Lady Mayor (for 2025\u20132026).\nThe Corporation of London changed its name to the City of London Corporation in 2006, and accordingly the title Lord/Lady Mayor of the City of London was introduced, so as to avoid confusion with that of Mayor of London. The legal and commonly used title remains \"Lord/Lady Mayor of London\". The Lord/Lady mayor is elected at \"Common Hall\" each year on Michaelmas, and takes office on the Friday before the second Saturday in November, at \"The Silent Ceremony\". The Lady Mayor's Show is held on the day after taking office; the Lady Mayor, preceded by a procession, travels to the Royal Courts of Justice at the Strand to swear allegiance to the Sovereign/Monarch before the Justices of the High Court.\nCurrently, the Lady Mayor's main role is to represent and promote the businesses and residents in the City of London. Today these are mostly financial businesses, and the Lady Mayor is regarded as the champion of the entire UK-based financial sector. As leader of the Corporation of the City of London, the Lady Mayor serves as the key spokesman for the local authority and also has important ceremonial and social responsibilities. The Lady Mayor is non-affiliated politically (not a member of any political party), typically delivering hundreds of speeches and addresses per year and attending many receptions and other events in London and beyond, and usually makes overseas visits under the auspices of the FCDO. The Lady Mayor is \"ex officio\" Rector of City University of London and Admiral of the Port of London.\nThe Lady Mayor is assisted by the Mansion House Esquires including the City Marshal, Sword Bearer and Common Crier, as well as the Chaplain, Venerable Ray Pentland. The Lady Mayor has six Cadet Aides de Camp representing the Uniformed Youth Organisation branches connecting with the City Cadet Forces. The ADCs welcome inquiries to arrange new affiliations.\nHistorically known as \"Lord Mayor of London\", in 2025 the title was restyled as \"Lady Mayor of London\" following the appointment of Dame Susan Langley to the position.\nTitles and honours.\nOf the 69 cities in the United Kingdom, the City of London is among the 30 with lord mayors (or, in Scotland, lords provost). The Lord Mayor is entitled to the prefix The Right Honourable; the same privilege extends only to the lord mayors of York, Cardiff, Bristol and Belfast, and to the lords provost of Edinburgh and Glasgow. The style is used when referring to the office, not to the holder: thus, \"The Rt Hon Lord Mayor of London\" would be correct, while \"The Rt Hon Alastair King\" would be incorrect. The prefix applies personally only to privy counsellors and peers.\nWomen who have previously held the office have been known as a Lord Mayor but Alderwoman Dame Susan Langley DBE taking office in November 2025 announced her intention to adopt the style of Lady Mayor following the established precedent of having Lady Justices and indeed currently a Lady Chief Justice. The wife of a male Lord Mayor is styled as Lady Mayoress, but no equivalent title exists for the husband of a female Lord Mayor or Lady Mayor. A female Lord Mayor or an unmarried male Lord Mayor may appoint a female consort, usually a fellow member of the Corporation, to the role of Lady Mayoress. In speech, a Lord Mayor is referred to as \"My Lord Mayor\", and a Lady Mayoress as \"My Lady Mayoress\".\nIt was once customary for Lord Mayors to be appointed knights upon taking office and baronets upon retirement, unless they already held such a title. This custom was generally followed from the 16th to the 19th centuries; creations became more regular from 1889 onward. From 1964 onward, the regular creation of hereditary titles such as baronetcies was phased out, so subsequent Lord Mayors were offered knighthoods (and, until 1993, most often as Knight Grand Cross of the Order of the British Empire (GBE)). Since 1993, Lord Mayors have not automatically received a national honour upon appointment, but have been made knights bachelor upon retirement. Gordon Brown's Government broke with that tradition by making Ian Luder a CBE after his term of office in 2009, and the following year Nick Anstee declined offers of an honour. Furthermore, foreign heads of state visiting the City of London on a UK state visit, diplomatically bestow upon the Lord Mayor one of their suitable national honours. For example, in 2001 King\u00a0Abdullah\u00a0II of Jordan created Sir David Howard a Grand Cordon (First Class) of the Order of Independence. Recently Lord Mayors at the beginning of their term have been appointed knights or dames of St John by the late Queen Elizabeth II, Sovereign Head of the Order of St John until her death in 2022.\nHistory.\nThe office of Mayor was instituted in 1189, the first holder being Henry Fitz-Ailwin de Londonestone. The Mayor of the City of London has been elected by the City, rather than appointed by the sovereign, since a royal charter issued by King John in 1215. The title \"Lord Mayor\" came to be used after 1354, when it was granted to Thomas Legge by King Edward III.\nLord Mayors are elected for one-year terms, and customarily serve only one term. Numerous individuals have served multiple terms, including:\nAs Mayor\nAs Lord Mayor\nAlmost 700 people have served as Lord Mayor. \nDame Mary Donaldson, elected in 1983, and Dame Fiona Woolf, elected in 2013, are women to have already held the office of Lord Mayor. Alderwoman Dame Susan Langley has assumed the Mayoralty in 2025\u20132026, setting the precedent of taking office as Lady Mayor.\nSome Lord Mayors in the Middle Ages, such as Sir Edward Dalyngrigge (1392), did not live in London. Since 1435, the Lord Mayor has been chosen from amongst the aldermen of the City of London.\nElection.\nThe Lord Mayor is elected at Common Hall, comprising liverymen belonging to all of the City's livery companies. Common Hall is summoned by the sitting Lord Mayor; it meets at Guildhall on Michaelmas Day (29 September) or on the closest weekday. Voting is by show of hands; if any liveryman so demands, balloting is held a fortnight later.\nThe qualification to stand for election is that one must have served as a City sheriff and be a current alderman. Since 1385, prior service as sheriff has been mandatory for election to the Lord Mayoralty. Two sheriffs are selected annually by Common Hall, which meets on Midsummer's Day for this purpose. By an ordinance of 1435, the Lord Mayor must be chosen from amongst the aldermen of the City of London. Those on the electoral roll of each of the City's 25 wards select one alderman, who formerly held office for life or until resignation. Each alderman must now submit for re-election at least once in every six years.\nThe Lord Mayor is sworn in each November, on the day before the Lord Mayor's Show (\"see below\"). The ceremony is known as the \"Silent Ceremony\" because, aside from a short declaration by the incoming Lord Mayor, no speeches are made. At Guildhall, the outgoing Lord Mayor transfers the mayoral insignia \u2013 the seal, the purse, the sword and the mace \u2014 to the incoming Lord Mayor.\nLord Mayor's Show.\nThe day after being sworn into office, the Lord Mayor leads a procession from the City of London to the Royal Courts of Justice in the City of Westminster, where the Lord Mayor swears allegiance to the Crown. This pageantry has evolved into one of London's longest-running and most popular annual events, known as the \"Lord Mayor's Show\". The Lord Mayor travels in the City's state coach that was built in 1757 at a cost of \u00a31,065.0s.3d. Nowadays, this festival combines traditional British pageantry with the element of carnival, and since 1959 it has been held on the second Saturday in November. Participants include the livery companies, bands and members of the military, charities and schools. In the evening, a fireworks display is held.\nRole.\nThe Lord Mayor is a member of the City of London's governing body, the City of London Corporation (incorporated as \"The Mayor and Commonalty and Citizens of the City of London\"). The Corporation comprises the Court of Aldermen and the Court of Common Council; the former includes only the aldermen, while the latter includes both aldermen and common councilmen. The Lord Mayor belongs to and presides over both bodies.\nThe main role of the Lord Mayor is to represent, support and promote all aspects of the UK's financial service industries, including maritime. This is undertaken as head of the City of London Corporation and includes hosting visiting foreign government ministers, business people and dignitaries as well as conducting foreign visits of their own.\nBanquets hosted by the Lord Mayor serve as opportunities for senior government figures to deliver major speeches. At the Lord Mayor's Banquet (held on the Monday after the Lord Mayor's Show), the Prime Minister delivers the keynote address. At the Bankers' Dinner in June, the Chancellor of the Exchequer delivers a speech known as the \"Mansion House Speech\", which takes its name from the Lord Mayor's residence. At the Easter Banquet, also hosted each year at the Mansion House, the Foreign Secretary addresses an audience of international dignitaries.\nThe Lord Mayor takes part in major state occasions; for example, in 2013, the then-Lord Mayor, Sir Roger Gifford, carried the Mourning Sword at Margaret Thatcher's funeral, processing ahead of the Queen and Prince Philip, Duke of Edinburgh, into St Paul's Cathedral.\nThe Lord Mayor performs numerous other functions, including serving as the chief magistrate of the City of London, admiral of the Port of London, rector of City, University of London, president of Gresham College, president of City of London Reserve Forces and Cadets Association, and trustee of St Paul's Cathedral. The Lord Mayor also heads the City's Commission of Lieutenancy, which represents the sovereign in the City of London (other counties usually have lord lieutenants, as opposed to Commissions), and annually attends a meeting of the Treloar Trust (named after Sir William Treloar, Lord Mayor in 1906), in Hampshire. The Treloar Trust runs two educational sites for disabled children: a school and college.\nRights and privileges.\nThe residence of the Lord Mayor is known as the Mansion House. The establishment of the residence was considered after the Great Fire of London (1666), but construction did not commence until 1739. It was first occupied by a Lord Mayor in 1752, when Sir Crispin Gascoigne took up residence. The official car of the Lord Mayor is a Rolls-Royce Phantom VI with registration number LM 0.\nIn each of the eighteen courtrooms of the Old Bailey, the centre of the judges' bench is reserved for the Lord Mayor, as chief justice of the City of London. The presiding judge therefore sits to one side.\nIt is sometimes asserted that the Lord Mayor may exclude the monarch from the City of London. This legend is based on the misinterpretation of the ceremony observed each time the sovereign enters the City at Temple Bar, when the Lord Mayor presents the City's Pearl Sword to the sovereign as a symbol of the latter's overlordship. The monarch does not, as is often purported, wait for the Lord Mayor's permission to enter the City. When the sovereign enters the City, a short ceremony usually takes place where the Lord Mayor presents a sword to the monarch, symbolically surrendering their authority. If the sovereign is attending a service at St Paul's Cathedral this ceremony would take place there rather than at the boundary of the City, simply for convenience.\nThe importance of the office is reflected in the composition of the Accession Council, a body which proclaims the accession of new sovereigns. The Council includes the Lord Mayor and aldermen of London, as well as members of the House of Lords and privy councillors. At the coronation banquet which follows, the Lord Mayor has the right to assist the royal butler. The same privilege is held by the lord mayor of Oxford, while the mayor of Winchester may assist the royal cook. Such privileges have not been exercised since 1821, when the coronation banquet (celebrating the coronation of George IV) was last held. \nOfficial dress.\nOn formal occasions the Lord Mayor wears traditional black velvet court dress consisting of a coat, waistcoat and knee breeches with steel cut buttons. This is worn with black silk stockings, patent court shoes with steel buckles, a white shirt with lace cuffs and a large jabot stock. This form of court dress is worn by all Lord Mayors regardless of gender.\nWhen outdoors, the Lord Mayor wears a black beaver plush tricorne hat trimmed with black ostrich feathers and a steel 'loop' for the cockade. Traditionally, this has been made by Patey's, and re-commissioned by the Worshipful Company of Feltmakers for each incumbent Lord Mayor.\nSince 1545 the Lord Mayor has worn a Royal Livery Collar of Esses. The collar's origins are not royal, Sir John Alleyn, twice Lord Mayor, having bequeathed it to the next Lord Mayor and his successors \"to use and occupie yerely at and uppon principall and festivall dayes\". It was enlarged in 1567, and in its present shape has 28 Esses (the Lancastrian \u2018S\u2019), Tudor roses and the tasselled knots of the Garter (alternating) and also the Portcullis, from which hangs the Mayoral Jewel. The collar is worn over whatever the Lord Mayor may be wearing, secured onto their underdress or State Robes by means of black or white silk satin ribbons on the shoulders.\nRobes.\nAs an alderman of the City of London the Lord Mayor has a scarlet gown and a violet gown, which are identical to those worn by their fellow aldermen except that they are trained. The violet robe is worn at most formal meetings of the Corporation with the scarlet robe substituted on certain days or occasions as directed by the City Ceremonial Book.\nFor state occasions when the monarch is present, the Lord Mayor wears a crimson velvet robe of state trimmed with an ermine cape and facings, similar to a royal earl's coronation robe. It is tied with gold cordons, and dates from the reign of George IV.\nOn other ceremonial occasions a black silk damask robe trimmed with gold lace is worn, of a design the same as that of the Lord Chancellor. This is known as the Entertaining Gown.\nAt coronations, the Lord Mayor wears a special coronation robe: a mantle of scarlet superfine wool trimmed with bars of gold lace and ermine and lined with white silk satin; they also carry the Crystal Sceptre as a baton of office. After the coronation, the incumbent may keep their coronation robe as a personal token.\nA plain black gown is worn by the Lord Mayor in times of national mourning.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "40343", "revid": "37625882", "url": "https://en.wikipedia.org/wiki?curid=40343", "title": "Prehnite", "text": "Inosilicate of calcium and aluminium\nPrehnite is an inosilicate of calcium and aluminium with the formula: Ca2Al(AlSi3O10)(OH)2 with limited Fe3+ substitutes for aluminium in the structure. Prehnite crystallizes in the orthorhombic crystal system, and most often forms as stalactitic, botryoidal, reniform or globular aggregates, with only just the crests of small crystals showing any faces, which are almost always curved or composite. Very rarely will it form distinct, well-individualized crystals showing a square-like cross-section, including those found at the Jeffrey Mine in Asbestos, Quebec, Canada. Prehnite is brittle with an uneven fracture and a vitreous to pearly luster. Its hardness is 6.5, its specific gravity is 2.80\u20132.95 and its color varies from light green to yellow, but also colorless, blue, pink or white. In April 2000, rare orange prehnite was discovered in the Kalahari Manganese Fields, South Africa. Prehnite is mostly translucent, and rarely transparent.\nThough not a zeolite, prehnite is found associated with minerals such as datolite, calcite, apophyllite, epidote, stilbite, laumontite, and heulandite in veins and cavities of basaltic rocks, sometimes in granites, syenites, or gneisses. It is an indicator mineral of the prehnite-pumpellyite metamorphic facies. \nIt was first described in 1788 for an occurrence in the Karoo dolerites of Cradock, Eastern Cape Province, South Africa. It was named for Colonel Hendrik Von Prehn (1733\u20131785), commander of the military forces of the Dutch colony at the Cape of Good Hope from 1768 to 1780.\nIt is used as a gemstone. \nExtensive deposits of gem-quality prehnite occur in the basalt tableland surrounding Wave Hill Station in the central Northern Territory, of Australia.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "40344", "revid": "27823944", "url": "https://en.wikipedia.org/wiki?curid=40344", "title": "Semiconductor device", "text": "Electronic component that exploits the electronic properties of semiconductor materials\nA semiconductor device is an electronic component that relies on the electronic properties of a semiconductor material (primarily silicon, germanium, and gallium arsenide, as well as organic semiconductors) for its function. Its conductivity lies between conductors and insulators. Semiconductor devices have replaced vacuum tubes in most applications. They conduct electric current in the solid state, rather than as free electrons across a vacuum (typically liberated by thermionic emission) or as free electrons and ions through an ionized gas.\nSemiconductor devices are manufactured both as single discrete devices and as integrated circuits, which consist of two or more devices\u2014which can number from the hundreds to the billions\u2014manufactured and interconnected on a single semiconductor wafer (also called a substrate).\nSemiconductor materials are useful because their behavior can be easily manipulated by the deliberate addition of impurities, known as doping. Semiconductor conductivity can be controlled by the introduction of an electric or magnetic field, by exposure to light or heat, or by the mechanical deformation of a doped monocrystalline silicon grid; thus, semiconductors can make excellent sensors. Current conduction in a semiconductor occurs due to mobile or \"free\" electrons and electron holes, collectively known as charge carriers. Doping a semiconductor with a small proportion of an atomic impurity, such as phosphorus or boron, greatly increases the number of free electrons or holes within the semiconductor. When a doped semiconductor contains excess holes, it is called a p-type semiconductor (\"p\" for positive electric charge); when it contains excess free electrons, it is called an n-type semiconductor (\"n\" for a negative electric charge). A majority of mobile charge carriers have negative charges. The manufacture of semiconductors controls precisely the location and concentration of p- and n-type dopants. The connection of n-type and p-type semiconductors form p\u2013n junctions.\nThe most common semiconductor device in the world is the MOSFET (metal\u2013oxide\u2013semiconductor field-effect transistor), also called the MOS transistor. As of 2013, billions of MOS transistors are manufactured every day. Semiconductor devices made per year have been growing by 9.1% on average since 1978, and shipments in 2018 are predicted for the first time to exceed 1 trillion, meaning that well over 7 trillion have been made to date.\nMain types.\nDiode.\nA semiconductor diode is a device typically made from a single p\u2013n junction. At the junction of a p-type and an n-type semiconductor, there forms a depletion region where current conduction is inhibited by the lack of mobile charge carriers. When the device is \"forward biased\" (connected with the p-side, having a higher electric potential than the n-side), this depletion region is diminished, allowing for significant conduction. Contrariwise, only a very small current can be achieved when the diode is \"reverse biased\" (connected with the n-side at higher electric potential than the p-side, and thus the depletion region expanded).\nExposing a semiconductor to light can generate electron\u2013hole pairs, which increases the number of free carriers and thereby the conductivity. Diodes optimized to take advantage of this phenomenon are known as \"photodiodes\".\nCompound semiconductor diodes can also produce light, as in light-emitting diodes and laser diode\nTransistor.\nBipolar junction transistor.\nBipolar junction transistors (BJTs) are formed from two p\u2013n junctions, in either n\u2013p\u2013n or p\u2013n\u2013p configuration. The middle, or \"base\", the region between the junctions is typically very narrow. The other regions, and their associated terminals, are known as the \"emitter\" and the \"collector\". A small current injected through the junction between the base and the emitter changes the properties of the base-collector junction so that it can conduct current even though it is reverse biased. This creates a much larger current between the collector and emitter, controlled by the base-emitter current.\nField-effect transistor.\nAnother type of transistor, the field-effect transistor (FET), operates on the principle that semiconductor conductivity can be increased or decreased by the presence of an electric field. An electric field can increase the number of free electrons and holes in a semiconductor, thereby changing its conductivity. The field may be applied by a reverse-biased p\u2013n junction, forming a \"junction field-effect transistor\" (JFET) or by an electrode insulated from the bulk material by an oxide layer, forming a \"metal\u2013oxide\u2013semiconductor field-effect transistor\" (MOSFET).\nMetal-oxide-semiconductor.\nThe metal-oxide-semiconductor FET (MOSFET, or MOS transistor), a solid-state device, is by far the most used widely semiconductor device today. It accounts for at least 99.9% of all transistors, and there have been an estimated 13sextillion MOSFETs manufactured between 1960 and 2018.\nThe \"gate\" electrode is charged to produce an electric field that controls the conductivity of a \"channel\" between two terminals, called the \"source\" and \"drain\". Depending on the type of carrier in the channel, the device may be an \"n-channel\" (for electrons) or a \"p-channel\" (for holes) MOSFET. Although the MOSFET is named in part for its \"metal\" gate, in modern devices polysilicon is typically used instead.\nOther types.\n\"Two-terminal devices:\"\n\"Three-terminal devices:\"\n\"Four-terminal devices:\"\nMaterials.\nBy far, silicon (Si) is the most widely used material in semiconductor devices. Its combination of low raw material cost, relatively simple processing, and a useful temperature range makes it currently the best compromise among the various competing materials. Silicon used in semiconductor device manufacturing is currently fabricated into boules that are large enough in diameter to allow the production of 300\u00a0mm (12 in.) wafers.\nGermanium (Ge) was a widely used early semiconductor material but its thermal sensitivity makes it less useful than silicon. Today, germanium is often alloyed with silicon for use in very-high-speed SiGe devices; IBM is a major producer of such devices.\nGallium arsenide (GaAs) is also widely used in high-speed devices but so far, it has been difficult to form large-diameter boules of this material, limiting the wafer diameter to sizes significantly smaller than silicon wafers thus making mass production of GaAs devices significantly more expensive than silicon.\nGallium Nitride (GaN) is gaining popularity in high-power applications including power ICs, light-emitting diodes (LEDs), and RF components due to its high strength and thermal conductivity. Compared to silicon, GaN's band gap is more than 3 times wider at 3.4 eV and it conducts electrons 1,000 times more efficiently.\nOther less common materials are also in use or under investigation.\nSilicon carbide (SiC) is also gaining popularity in power ICs and has found some application as the raw material for blue LEDs and is being investigated for use in semiconductor devices that could withstand very high operating temperatures and environments with the presence of significant levels of ionizing radiation. IMPATT diodes have also been fabricated from SiC.\nVarious indium compounds (indium arsenide, indium antimonide, and indium phosphide) are also being used in LEDs and solid-state laser diodes. Selenium sulfide is being studied in the manufacture of photovoltaic solar cells.\nThe most common use for organic semiconductors is organic light-emitting diodes.\nApplications.\nAll transistor types can be used as the building blocks of logic gates, which are fundamental in the design of digital circuits. In digital circuits like microprocessors, transistors act as on-off switches; in the MOSFET, for instance, the voltage applied to the gate determines whether the switch is on or off.\nTransistors used for analog circuits do not act as on-off switches; rather, they respond to a continuous range of inputs with a continuous range of outputs. Common analog circuits include amplifiers and oscillators.\nCircuits that interface or translate between digital circuits and analog circuits are known as mixed-signal circuits.\nPower semiconductor devices are discrete devices or integrated circuits intended for high current or high voltage applications. Power integrated circuits combine IC technology with power semiconductor technology, these are sometimes referred to as \"smart\" power devices. Several companies specialize in manufacturing power semiconductors.\nComponent identifiers.\nThe part numbers of semiconductor devices are often manufacturer specific. Nevertheless, there have been attempts at creating standards for type codes, and a subset of devices follow those. For discrete devices, for example, there are three standards: JEDEC JESD370B in the United States, Pro Electron in Europe, and Japanese Industrial Standards (JIS).\nHistory of development.\nCat's-whisker detector.\nSemiconductors had been used in the electronics field for some time before the invention of the transistor. Around the turn of the 20th century they were quite common as detectors in radios, used in a device called a \"cat's whisker\" developed by Jagadish Chandra Bose and others. These detectors were somewhat troublesome, however, requiring the operator to move a small tungsten filament (the whisker) around the surface of a galena (lead sulfide) or carborundum (silicon carbide) crystal until it suddenly started working. Then, over a period of a few hours or days, the cat's whisker would slowly stop working and the process would have to be repeated. At the time their operation was completely mysterious. After the introduction of the more reliable and amplified vacuum tube based radios, the cat's whisker systems quickly disappeared. The \"cat's whisker\" is a primitive example of a special type of diode still popular today, called a Schottky diode.\nMetal rectifier.\nAnother early type of semiconductor device is the metal rectifier in which the semiconductor is copper oxide or selenium. Westinghouse Electric (1886) was a major manufacturer of these rectifiers.\nWorld War II.\nDuring World War II, radar research quickly pushed radar receivers to operate at ever higher frequencies about 4000\u00a0MHz and the traditional tube-based radio receivers no longer worked well. The introduction of the cavity magnetron from Britain to the United States in 1940 during the Tizard Mission resulted in a pressing need for a practical high-frequency amplifier. \nOn a whim, Russell Ohl of Bell Laboratories decided to try a cat's whisker. By this point, they had not been in use for a number of years, and no one at the labs had one. After hunting one down at a used radio store in Manhattan, he found that it worked much better than tube-based systems.\nOhl investigated why the cat's whisker functioned so well. He spent most of 1939 trying to grow more pure versions of the crystals. He soon found that with higher-quality crystals their finicky behavior went away, but so did their ability to operate as a radio detector. One day he found one of his purest crystals nevertheless worked well, and it had a clearly visible crack near the middle. However, as he moved about the room trying to test it, the detector would mysteriously work, and then stop again. After some study he found that the behavior was controlled by the light in the room \u2013 more light caused more conductance in the crystal. He invited several other people to see this crystal, and Walter Brattain immediately realized there was some sort of junction at the crack.\nFurther research cleared up the remaining mystery. The crystal had cracked because either side contained very slightly different amounts of the impurities Ohl could not remove \u2013 about 0.2%. One side of the crystal had impurities that added extra electrons (the carriers of electric current) and made it a \"conductor\". The other had impurities that wanted to bind to these electrons, making it (what he called) an \"insulator\". Because the two parts of the crystal were in contact with each other, the electrons could be pushed out of the conductive side which had extra electrons (soon to be known as the \"emitter\"), and replaced by new ones being provided (from a battery, for instance) where they would flow into the insulating portion and be collected by the whisker filament (named the \"collector\"). However, when the voltage was reversed the electrons being pushed into the collector would quickly fill up the \"holes\" (the electron-needy impurities), and conduction would stop almost instantly. This junction of the two crystals (or parts of one crystal) created a solid-state diode, and the concept soon became known as semiconduction. The mechanism of action when the diode off has to do with the separation of charge carriers around the junction. This is called a \"depletion region\".\nDevelopment of the diode.\nArmed with the knowledge of how these new diodes worked, a vigorous effort began to learn how to build them on demand. Teams at Purdue University, Bell Labs, MIT, and the University of Chicago all joined forces to build better crystals. Within a year germanium production had been perfected to the point where military-grade diodes were being used in most radar sets.\nDevelopment of the transistor.\nAfter the war, William Shockley decided to attempt the building of a triode-like semiconductor device. He secured funding and lab space, and went to work on the problem with Brattain and John Bardeen.\nThe key to the development of the transistor was the further understanding of the process of the electron mobility in a semiconductor. It was realized that if there were some way to control the flow of the electrons from the emitter to the collector of this newly discovered diode, an amplifier could be built. For instance, if contacts are placed on both sides of a single type of crystal, current will not flow between them through the crystal. However, if a third contact could then \"inject\" electrons or holes into the material, the current would flow.\nActually doing this appeared to be very difficult. If the crystal were of any reasonable size, the number of electrons (or holes) required to be injected would have to be very large, making it less than useful as an amplifier because it would require a large injection current to start with. That said, the whole idea of the crystal diode was that the crystal itself could provide the electrons over a very small distance, the depletion region. The key appeared to be to place the input and output contacts very close together on the surface of the crystal on either side of this region.\nBrattain started working on building such a device, and tantalizing hints of amplification continued to appear as the team worked on the problem. Sometimes the system would work but then stop working unexpectedly. In one instance a non-working system started working when placed in water. Ohl and Brattain eventually developed a new branch of quantum mechanics, which became known as surface physics, to account for the behavior. The electrons in any one piece of the crystal would migrate about due to nearby charges. Electrons in the emitters, or the \"holes\" in the collectors, would cluster at the surface of the crystal where they could find their opposite charge \"floating around\" in the air (or water). Yet they could be pushed away from the surface with the application of a small amount of charge from any other location on the crystal. Instead of needing a large supply of injected electrons, a very small number in the right place on the crystal would accomplish the same thing.\nTheir understanding solved the problem of needing a very small control area to some degree. Instead of needing two separate semiconductors connected by a common, but tiny, region, a single larger surface would serve. The electron-emitting and collecting leads would both be placed very close together on the top, with the control lead placed on the base of the crystal. When current flowed through this \"base\" lead, the electrons or holes would be pushed out, across the block of the semiconductor, and collect on the far surface. As long as the emitter and collector were very close together, this should allow enough electrons or holes between them to allow conduction to start.\nFirst transistor.\nThe Bell team made many attempts to build such a system with various tools but generally failed. Setups, where the contacts were close enough, were invariably as fragile as the original cat's whisker detectors had been, and would work briefly, if at all. Eventually, they had a practical breakthrough. A piece of gold foil was glued to the edge of a plastic wedge, and then the foil was sliced with a razor at the tip of the triangle. The result was two very closely spaced contacts of gold. When the wedge was pushed down onto the surface of a crystal and voltage was applied to the other side (on the base of the crystal), current started to flow from one contact to the other as the base voltage pushed the electrons away from the base towards the other side near the contacts. The point-contact transistor had been invented.\nWhile the device was constructed a week earlier, Brattain's notes describe the first demonstration to higher-ups at Bell Labs on the afternoon of 23 December 1947, often given as the birthdate of the transistor. What is now known as the \"p\u2013n\u2013p point-contact germanium transistor\" operated as a speech amplifier with a power gain of 18 in that trial. John Bardeen, Walter Houser Brattain, and William Bradford Shockley were awarded the 1956 Nobel Prize in physics for their work.\nEtymology of \"transistor\".\nBell Telephone Laboratories needed a generic name for their new invention: \"Semiconductor Triode\", \"Solid Triode\", \"Surface States Triode\" [\"sic\"], \"Crystal Triode\" and \"Iotatron\" were all considered, but \"transistor\", coined by John R. Pierce, won an internal ballot. The rationale for the name is described in the following extract from the company's Technical Memoranda (May 28, 1948) [26] calling for votes:\nTransistor. This is an abbreviated combination of the words \"transconductance\" or \"transfer\", and \"varistor\". The device logically belongs in the varistor family, and has the transconductance or transfer impedance of a device having gain, so that this combination is descriptive.\nImprovements in transistor design.\nShockley was upset about the device being credited to Brattain and Bardeen, who he felt had built it \"behind his back\" to take the glory. Matters became worse when Bell Labs lawyers found that some of Shockley's own writings on the transistor were close enough to those of an earlier 1925 patent by Julius Edgar Lilienfeld that they thought it best that his name be left off the patent application.\nShockley was incensed, and decided to demonstrate who was the real brains of the operation. A few months later he invented an entirely new, considerably more robust, bipolar junction transistor type of transistor with a layer or 'sandwich' structure, used for the vast majority of all transistors into the 1960s.\nWith the fragility problems solved, the remaining problem was purity. Making germanium of the required purity was proving to be a serious problem and limited the yield of transistors that actually worked from a given batch of material. Germanium's sensitivity to temperature also limited its usefulness. Scientists theorized that silicon would be easier to fabricate, but few investigated this possibility. Former Bell Labs scientist Gordon K. Teal was the first to develop a working silicon transistor at the nascent Texas Instruments, giving it a technological edge. From the late 1950s, most transistors were silicon-based. Within a few years transistor-based products, most notably easily portable radios, were appearing on the market. \"Zone melting\", a technique using a band of molten material moving through the crystal, further increased crystal purity.\nMetal-oxide semiconductor.\nIn 1955, Carl Frosch and Lincoln Derick accidentally grew a layer of silicon dioxide over the silicon wafer, for which they observed surface passivation effects. By 1957 Frosch and Derick, using masking and predeposition, were able to manufacture silicon dioxide field effect transistors; the first planar transistors, in which drain and source were adjacent at the same surface. They showed that silicon dioxide insulated, protected silicon wafers and prevented dopants from diffusing into the wafer. At Bell Labs, the importance of Frosch and Derick technique and transistors was immediately realized. Results of their work circulated around Bell Labs in the form of BTL memos before being published in 1957. At Shockley Semiconductor, Shockley had circulated the preprint of their article in December 1956 to all his senior staff, including Jean Hoerni, who would later invent the planar process in 1959 while at Fairchild Semiconductor.\nAfter this, J.R. Ligenza and W.G. Spitzer studied the mechanism of thermally grown oxides, fabricated a high quality Si/SiO2 stack and published their results in 1960. Following this research, Mohamed Atalla and Dawon Kahng proposed a silicon MOS transistor in 1959 and successfully demonstrated a working MOS device with their Bell Labs team in 1960. Their team included E. E. LaBate and E. I. Povilonis who fabricated the device; M. O. Thurston, L. A. D\u2019Asaro, and J. R. Ligenza who developed the diffusion processes, and H. K. Gummel and R. Lindner who characterized the device.\nWith its scalability, and much lower power consumption and higher density than bipolar junction transistors, the MOSFET became the most common type of transistor in computers, electronics, and communications technology such as smartphones. The US Patent and Trademark Office calls the MOSFET a \"groundbreaking invention that transformed life and culture around the world\".\nBardeen's 1948 inversion layer concept, forms the basis of CMOS technology today. CMOS (complementary MOS) was invented by Chih-Tang Sah and Frank Wanlass at Fairchild Semiconductor in 1963. The first report of a floating-gate MOSFET was made by Dawon Kahng and Simon Sze in 1967. FinFET (fin field-effect transistor), a type of 3D multi-gate MOSFET, was proposed by H. R. Farrah (Bendix Corporation) and R. F. Steinberg in 1967 and first built by Digh Hisamoto and his team of researchers at Hitachi Central Research Laboratory in 1989.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "40345", "revid": "44645973", "url": "https://en.wikipedia.org/wiki?curid=40345", "title": "MOSFET", "text": "Type of field-effect transistor\nIn electronics, the metal\u2013oxide\u2013semiconductor field-effect transistor (MOSFET, MOS-FET, MOS FET, or MOS transistor) is a type of field-effect transistor (FET), most commonly fabricated by the controlled oxidation of silicon. It has an insulated gate, the voltage of which determines the conductivity of the device. This ability to change conductivity with the amount of applied voltage can be used for amplifying or switching electronic signals. The term \"metal\u2013insulator\u2013semiconductor field-effect transistor\" (\"MISFET\") is almost synonymous with \"MOSFET\". Another near-synonym is \"insulated-gate field-effect transistor\" (\"IGFET\").\nThe main advantage of a MOSFET is that it requires almost no input current to control the load current under steady-state or low-frequency conditions, especially compared to bipolar junction transistors (BJTs). However, at high frequencies or when switching rapidly, a MOSFET may require significant current to charge and discharge its gate capacitance. In an \"enhancement mode\" MOSFET, voltage applied to the gate terminal increases the conductivity of the device. In \"depletion mode\" transistors, voltage applied at the gate reduces the conductivity.\nThe \"metal\" in the name MOSFET is sometimes a misnomer, because the gate material can be a layer of polysilicon (polycrystalline silicon). Similarly, \"oxide\" in the name can also be a misnomer, as different dielectric materials are used with the aim of obtaining strong channels with smaller applied voltages.\nThe MOSFET is by far the most common transistor in digital circuits, as billions may be included in a memory chip or microprocessor. As MOSFETs can be made with either a p-type or n-type channel, complementary pairs of MOS transistors can be used to make switching circuits with very low power consumption, in the form of CMOS logic.\nHistory.\nThe basic principle of the field-effect transistor was first filed as a patent by Julius Edgar Lilienfeld as a Canadian patent in 1925 and later as a U.S. patent in 1926. In 1934, inventor Oskar Heil independently patented a similar device in Europe.\nIn the 1940s, Bell Labs scientists William Shockley, John Bardeen and Walter Houser Brattain attempted to build a field-effect device which led to their discovery of the transistor effect. However, their structure failed to show the anticipated effects due to the problem of surface states: traps on the semiconductor surface that hold electrons immobile. With no surface passivation they were only able to build the BJT and thyristor transistors.\nIn 1955, Carl Frosch and Lincoln Derick accidentally grew a layer of silicon dioxide over the silicon wafer, for which they observed surface passivation effects. By 1957 Frosch and Derick, using masking and predeposition, were able to manufacture silicon dioxide field effect transistors; the first planar transistors, in which drain and source were adjacent at the same surface. They showed that silicon dioxide insulated, protected silicon wafers and prevented dopants from diffusing into the wafer. At Bell Labs, the importance of Frosch and Derick technique and transistors was immediately realized. Results of their work circulated around Bell Labs in the form of BTL memos before being published in 1957. At Shockley Semiconductor, Shockley had circulated the preprint of their article in December 1956 to all his senior staff, including Jean Hoerni, who would later invent the planar process in 1959 while at Fairchild Semiconductor.\nAfter this, J.R. Ligenza and W.G. Spitzer studied the mechanism of thermally grown oxides, fabricated a high quality Si/SiO2 stack and published their results in 1960.Following this research, Mohamed Atalla and Dawon Kahng proposed a silicon MOS transistor in 1959 and successfully demonstrated a working MOS device with their Bell Labs team in 1960. Their team included E. E. LaBate and E. I. Povilonis who fabricated the device; M. O. Thurston, L. A. D'Asaro, and J. R. Ligenza who developed the diffusion processes, and H. K. Gummel and R. Lindner who characterized the device. This was a culmination of decades of field-effect research that began with Lilienfeld.\nThe first MOS transistor at Bell Labs was about 100 times slower than contemporary bipolar transistors and was initially seen as inferior. Nevertheless, Kahng pointed out several advantages of the device, notably ease of fabrication and its application in integrated circuits.\nComposition.\nSilicon remains the primary semiconductor in CMOS devices, but to enhance carrier mobility, manufacturers\u2014most notably IBM and Intel\u2014induce strain in the silicon channel by incorporating silicon-germanium (SiGe) in nearby regions or applying stress engineering techniques, thereby improving transistor performance without using SiGe alloys as the channel material itself. \nMany semiconductors with better electrical properties than silicon, such as gallium arsenide, do not form good semiconductor-to-insulator interfaces, and thus are not suitable for MOSFETs. Research continues on creating insulators with acceptable electrical characteristics on other semiconductor materials.\nTo overcome the increase in power consumption due to gate current leakage, a high-\u03ba dielectric is used instead of silicon dioxide for the gate insulator, while polysilicon is replaced by metal gates (e.g. Intel, 2009).\nThe gate is separated from the channel by a thin insulating layer, traditionally of silicon dioxide and later of silicon oxynitride. Some companies use a high-\u03ba dielectric and metal gate combination in the 45 nanometer node.\nWhen a voltage is applied between the gate and the source, the electric field generated penetrates through the oxide and creates an \"inversion layer\" or \"channel\" at the semiconductor-insulator interface. The inversion layer provides a channel through which current can pass between source and drain terminals. Varying the voltage between the gate and body modulates the conductivity of this layer and thereby controls the current flow between drain and source. This is known as enhancement mode.\nOperation.\nMetal\u2013oxide\u2013semiconductor structure.\nThe traditional metal\u2013oxide\u2013semiconductor (MOS) structure is obtained by growing a layer of silicon dioxide (SiO2) on top of a silicon substrate, commonly by thermal oxidation and depositing a layer of metal or polycrystalline silicon (the latter is commonly used). As silicon dioxide is a dielectric material, its structure is equivalent to a planar capacitor, with one of the electrodes replaced by a semiconductor.\nWhen a voltage is applied across a MOS structure, it modifies the distribution of charges in the semiconductor. If we consider a p-type semiconductor (with \"N\"A the density of acceptors, \"p\" the density of holes; \"p = N\"A in neutral bulk), a positive voltage, \"V\"G, from gate to body (see figure) creates a depletion layer by forcing the positively charged holes away from the gate-insulator/semiconductor interface, leaving exposed a carrier-free region of immobile, negatively charged acceptor ions (see doping). If \"V\"G is high enough, a high concentration of negative charge carriers forms in an \"inversion layer\" located in a thin layer next to the interface between the semiconductor and the insulator.\nConventionally, the gate voltage at which the volume density of electrons in the inversion layer is the same as the volume density of holes in the body is called the threshold voltage. When the voltage between transistor gate and source (\"V\"G) exceeds the threshold voltage (\"V\"th), the difference is known as overdrive voltage.\nThis structure with p-type body is the basis of the n-type MOSFET, which requires the addition of n-type source and drain regions.\nMOS capacitors and band diagrams.\nThe MOS capacitor structure is the heart of the MOSFET. Consider a MOS capacitor where the silicon base is of p-type. If a positive voltage is applied at the gate, holes which are at the surface of the p-type substrate will be repelled by the electric field generated by the voltage applied. At first, the holes will simply be repelled and what will remain on the surface will be immobile (negative) atoms of the acceptor type, which creates a depletion region on the surface. A hole is created by an acceptor atom, e.g., boron, which has one less electron than a silicon atom. Holes are not actually repelled, being non-entities; electrons are attracted by the positive field, and fill these holes. This creates a depletion region where no charge carriers exist because the electron is now fixed onto the atom and immobile.\nAs the voltage at the gate increases, there will be a point at which the surface above the depletion region will be converted from p-type into n-type, as electrons from the bulk area will start to get attracted by the larger electric field. This is known as \"inversion\". The threshold voltage at which this conversion happens is one of the most important parameters in a MOSFET.\nIn the case of a p-type MOSFET, bulk inversion happens when the intrinsic energy level at the surface becomes smaller than the Fermi level at the surface. This can be seen on a band diagram. The Fermi level defines the type of semiconductor in discussion. If the Fermi level is equal to the Intrinsic level, the semiconductor is of intrinsic, or pure type. If the Fermi level lies closer to the conduction band (valence band) then the semiconductor type will be of n-type (p-type).\nWhen the gate voltage is increased in a positive sense this will shift the intrinsic energy level band so that it will curve downwards towards the valence band. If the Fermi level lies closer to the valence band (for p-type), there will be a point when the Intrinsic level will start to cross the Fermi level and when the voltage reaches the threshold voltage, the intrinsic level does cross the Fermi level, and that is what is known as inversion. At that point, the surface of the semiconductor is inverted from p-type into n-type.\nIf the Fermi level lies above the intrinsic level, the semiconductor is of n-type, therefore at inversion, when the intrinsic level reaches and crosses the Fermi level (which lies closer to the valence band), the semiconductor type changes at the surface as dictated by the relative positions of the Fermi and Intrinsic energy levels.\nStructure and channel formation.\nA MOSFET is based on the modulation of charge concentration by a MOS capacitance between a \"body\" electrode and a \"gate\" electrode located above the body and insulated from all other device regions by a gate dielectric layer. If dielectrics other than an oxide are employed, the device may be referred to as a metal-insulator-semiconductor FET (MISFET). Compared to the MOS capacitor, the MOSFET includes two additional terminals (\"source\" and \"drain\"), each connected to individual highly doped regions that are separated by the body region. These regions can be either p or n type, but they must both be of the same type, and of opposite type to the body region. The source and drain (unlike the body) are highly doped as signified by a \"+\" sign after the type of doping.\nIf the MOSFET is an n-channel or nMOS FET, then the source and drain are \"n+\" regions and the body is a \"p\" region. If the MOSFET is a p-channel or pMOS FET, then the source and drain are \"p+\" regions and the body is a \"n\" region. The source is so named because it is the source of the charge carriers (electrons for n-channel, holes for p-channel) that flow through the channel; similarly, the drain is where the charge carriers leave the channel.\nThe occupancy of the energy bands in a semiconductor is set by the position of the Fermi level relative to the semiconductor energy-band edges.\nWith sufficient gate voltage, the valence band edge is driven far from the Fermi level, and holes from the body are driven away from the gate.\nAt larger gate bias still, near the semiconductor surface the conduction band edge is brought close to the Fermi level, populating the surface with electrons in an \"inversion layer\" or \"n-channel\" at the interface between the p region and the oxide. This conducting channel extends between the source and the drain, and current is conducted through it when a voltage is applied between the two electrodes. Increasing the voltage on the gate leads to a higher electron density in the inversion layer and therefore increases the current flow between the source and drain. For gate voltages below the threshold value, the channel is lightly populated, and only a very small subthreshold leakage current can flow between the source and the drain.\nWhen a negative gate-source voltage (positive source-gate) is applied, it creates a \"p-channel\" at the surface of the n region, analogous to the n-channel case, but with opposite polarities of charges and voltages. When a voltage less negative than the threshold value (a negative voltage for the p-channel) is applied between gate and source, the channel disappears and only a very small subthreshold current can flow between the source and the drain. The device may comprise a silicon on insulator device in which a buried oxide is formed below a thin semiconductor layer. If the channel region between the gate dielectric and the buried oxide region is very thin, the channel is referred to as an ultrathin channel region with the source and drain regions formed on either side in or above the thin semiconductor layer. Other semiconductor materials may be employed. When the source and drain regions are formed above the channel in whole or in part, they are referred to as raised source/drain regions.\nModes of operation.\nThe operation of a MOSFET can be separated into three different modes, depending on the device's threshold voltage (formula_1), gate-to-source voltage (formula_2), and drain-to-source voltage (formula_3). In the following discussion, a simplified algebraic model is used. Modern MOSFET characteristics are more complex than the algebraic model presented here.\nFor an \"enhancement-mode, n-channel MOSFET\", the three operational modes are:\nCutoff, subthreshold, and weak-inversion mode.\nCriterion: formula_4\nAccording to the basic threshold model, the transistor is turned off, and there is no conduction between drain and source. A more accurate model considers the effect of thermal energy on the Fermi\u2013Dirac distribution of electron energies which allow some of the more energetic electrons at the source to enter the channel and flow to the drain. This results in a subthreshold current that is an exponential function of gate-source voltage. While the current between drain and source should ideally be zero when the transistor is being used as a turned-off switch, there is a weak-inversion current, sometimes called subthreshold leakage.\nIn weak inversion where the source is tied to bulk, the current varies exponentially with formula_2 as given approximately by:\nformula_6\nwhere:\nformula_11\nwhere formula_12 is the capacitance of the depletion layer and formula_13 is the capacitance of the oxide layer. This equation is generally used, but is only an adequate approximation for the source tied to the bulk. For the source not tied to the bulk, the subthreshold equation for drain current in saturation is\nformula_14\nIn a long-channel device, there is no drain voltage dependence of the current once formula_15, but as channel length is reduced drain-induced barrier lowering introduces drain voltage dependence that depends in a complex way upon the device geometry (for example, the channel doping, the junction doping and so on). Frequently, threshold voltage \"V\"th for this mode is defined as the gate voltage at which a selected value of current \"I\"D0 occurs, for example, \"I\"D0 = 1\u03bcA, which may not be the same \"V\"th-value used in the equations for the following modes.\nSome micropower analog circuits are designed to take advantage of subthreshold conduction. By working in the weak-inversion region, the MOSFETs in these circuits deliver the highest possible transconductance-to-current ratio, namely: formula_16, almost that of a bipolar transistor.\nThe subthreshold \"I\u2013V curve\" depends exponentially upon threshold voltage, introducing a strong dependence on any manufacturing variation that affects threshold voltage; for example: variations in oxide thickness, junction depth, or body doping that change the degree of drain-induced barrier lowering. The resulting sensitivity to fabricational variations complicates optimization for leakage and performance.\nTriode mode or linear region (also known as the ohmic mode).\nCriteria: formula_17 and formula_18\nThe transistor is turned on, and a channel has been created which allows current between the drain and the source. The MOSFET operates like a resistor, controlled by the gate voltage relative to both the source and drain voltages. The current from drain to source is modeled as:\nformula_19\nwhere formula_20 is the charge-carrier effective mobility, formula_21 is the gate width, formula_22 is the gate length and formula_13 is the gate oxide capacitance per unit area. The transition from the exponential subthreshold region to the triode region is not as sharp as the equations suggest.\nSaturation or active mode.\nCritera: formula_17 and formula_25\nThe switch is turned on, and a channel has been created, which allows current between the drain and source. Since the drain voltage is higher than the source voltage, the electrons spread out, and conduction is not through a narrow channel but through a broader, two- or three-dimensional current distribution extending away from the interface and deeper in the substrate. The onset of this region is also known as pinch-off to indicate the lack of channel region near the drain. Although the channel does not extend the full length of the device, the electric field between the drain and the channel is very high, and conduction continues. The drain current is now weakly dependent upon drain voltage and controlled primarily by the gate-source voltage, and modeled approximately as:\nformula_26\nThe additional factor involving \u03bb, the channel-length modulation parameter, models current dependence on drain voltage due to the Early effect, or channel length modulation. According to this equation, a key design parameter, the MOSFET transconductance is:\nformula_27\nwhere the combination \"V\"ov = \"V\"GS\u00a0\u2212 \"V\"th is called the overdrive voltage, and where \"V\"DSsat = \"V\"GS\u00a0\u2212 \"V\"th accounts for a small discontinuity in formula_28 which would otherwise appear at the transition between the triode and saturation regions.\nAnother key design parameter is the MOSFET output resistance \"rout\" given by:\nformula_29\nNote: \"r\"out is the inverse of \"g\"DS, where formula_30. \"I\"D is the expression in the saturation region.\nIf \u03bb is taken as zero, an infinite output resistance of the device results that leads to unrealistic circuit predictions, particularly in analog circuits.\nAs the channel length becomes very short, these equations become quite inaccurate. New physical effects arise. For example, carrier transport in the active mode may become limited by velocity saturation. When velocity saturation dominates, the saturation drain current is more nearly linear than quadratic in \"V\"GS. At even shorter lengths, carriers transport with near zero scattering, known as quasi-ballistic transport. In the ballistic regime, the carriers travel at an injection velocity that may exceed the saturation velocity and approaches the Fermi velocity at high inversion charge density. In addition, drain-induced barrier lowering increases off-state (cutoff) current and requires an increase in threshold voltage to compensate, which in turn reduces the saturation current.\nBody effect.\nThe occupancy of the energy bands in a semiconductor is set by the position of the Fermi level relative to the semiconductor energy-band edges. Application of a source-to-substrate reverse bias of the source-body pn-junction introduces a split between the Fermi levels for electrons and holes, moving the Fermi level for the channel further from the band edge, lowering the occupancy of the channel. The effect is to increase the gate voltage necessary to establish the channel, as seen in the figure. This change in channel strength by application of reverse bias is called the \"body effect\".\nUsing an nMOS example, the gate-to-body bias \"V\"GB positions the conduction-band energy levels, while the source-to-body bias VSB positions the electron Fermi level near the interface, deciding occupancy of these levels near the interface, and hence the strength of the inversion layer or channel.\nThe body effect upon the channel can be described using a modification of the threshold voltage, approximated by the following equation:\n formula_31\nwhere \"V\"TB is the threshold voltage with substrate bias present, and \"V\"T0 is the zero-\"V\"SB value of threshold voltage, formula_32 is the body effect parameter, and 2\"\u03c6\"B is the approximate potential drop between surface and bulk across the depletion layer when \"V\"SB \n 0 and gate bias is sufficient to ensure that a channel is present. As this equation shows, a reverse bias \"V\"SB &gt; 0 causes an increase in threshold voltage \"V\"TB and therefore demands a larger gate voltage before the channel populates.\nThe body can be operated as a second gate, and is sometimes referred to as the \"back gate\"; the body effect is sometimes called the \"back-gate effect\".\nCircuit symbols.\nA variety of symbols are used for the MOSFET. The basic design is generally a line for the channel with the source and drain leaving it at right angles and then bending back at right angles into the same direction as the channel. Sometimes three line segments are used for enhancement mode and a solid line for depletion mode (see depletion and enhancement modes). Another line is drawn parallel to the channel for the gate.\nThe \"bulk\" or \"body\" connection, if shown, is shown connected to the back of the channel with an arrow indicating pMOS or nMOS. This arrow always points from P to N, so an nMOS (N-channel in P-well or P-substrate) has the arrow pointing from the bulk to the channel. Typically the bulk is internally connected to the source (as is generally the case with discrete devices), in which case the bulk end of the arrow will have a line drawn between it and the source to explicitly show this connection. Some symbols will also explicitly show the MOSFET's \"intrinsic body diode\" along an outer (and sometimes dotted, as may be done for parasitic elements) path between drain and source. Note that tying the bulk to the source is by no means the only important configuration. In general, the MOSFET is a four-terminal device. In integrated circuits, many of the MOSFETs share a body connection, not necessarily connected to the source terminals of all the transistors.\nIf the bulk is not shown (as is often the case in IC design as they are generally common bulk) an inversion symbol is sometimes used to indicate pMOS. Alternatively, as is done with bipolar transistors, an arrow on the MOSFET's source may indicate the direction of conventional current: pointing out for nMOS, in for pMOS.\nIn the following symbols for JFETs, enhancement-mode MOSFETs, and depletion-mode MOSFETs, the orientation of the symbols (most significantly the position of source relative to drain) is such that more positive voltages appear higher on the page than less positive voltages, implying conventional current flowing \"down\" the page:\nIn schematics where G, S, D are not labeled, the detailed features of the symbol indicate which terminal is source and which is drain. For enhancement-mode and depletion-mode MOSFET symbols (in columns two and five), the source terminal is the one connected to the triangle. Additionally, in this diagram, the gate is shown as an \"L\" shape, whose input leg is closer to S than D, also indicating which is which. However, these symbols are often drawn with a T-shaped gate (as elsewhere on this page), so it is the triangle which must be relied upon to indicate the source terminal.\nApplications.\nDigital integrated circuits such as microprocessors and memory devices contain thousands to billions of integrated MOSFETs on each device, providing the basic switching functions required to implement logic gates and data storage. Discrete devices are widely used in applications such as switch mode power supplies, variable-frequency drives and other power electronics applications where each device may be switching thousands of watts. Radio-frequency amplifiers up to the UHF spectrum use MOSFET transistors as analog signal and power amplifiers. Radio systems also use MOSFETs as oscillators, or mixers to convert frequencies. MOSFET devices are also applied in audio-frequency power amplifiers for public address systems, sound reinforcement and home and automobile sound systems\nMOS integrated circuits.\nFollowing the development of clean rooms to reduce contamination to levels never before thought necessary, and of photolithography and the planar process to allow circuits to be made in very few steps, the Si\u2013SiO2 system possessed the technical attractions of low cost of production (on a per circuit basis) and ease of integration. Largely because of these two factors, the MOSFET has become the most widely used type of transistor in the Institution of Engineering and Technology (IET).\nGeneral Micro-electronics introduced the first commercial MOS integrated circuit in 1964. Additionally, the method of coupling two complementary MOSFETs (P-channel and N-channel) into one high/low switch, known as CMOS, means that digital circuits dissipate very little power except when actually switched.\nThe earliest microprocessors starting in 1970 were all MOS microprocessors, fabricated entirely from PMOS logic or fabricated entirely from nMOS logic. In the 1970s, MOS microprocessors were often contrasted with CMOS microprocessors and bipolar bit-slice processors.\nCMOS circuits.\nThe MOSFET is used in digital complementary metal\u2013oxide\u2013semiconductor (CMOS) logic, which uses p- and n-channel MOSFETs as building blocks. Overheating is a major concern in integrated circuits since ever more transistors are packed into ever smaller chips. CMOS logic reduces power consumption because no current flows (ideally), and thus no power is consumed, except when the inputs to logic gates are being switched. CMOS accomplishes this current reduction by complementing every nMOS FET with a pMOS FET and connecting both gates and both drains together. A high voltage on the gates will cause the nMOS FET to conduct and the pMOS FET not to conduct and a low voltage on the gates causes the reverse. During the switching time as the voltage goes from one state to another, both MOSFETs will conduct briefly. This arrangement greatly reduces power consumption and heat generation.\nDigital.\nThe growth of digital technologies like the microprocessor has provided the motivation to advance MOSFET technology faster than any other type of silicon-based transistor. A big advantage of MOSFETs for digital switching is that the oxide layer between the gate and the channel prevents DC current from flowing through the gate, further reducing power consumption and giving a very large input impedance. The insulating oxide between the gate and channel effectively isolates a MOSFET in one logic stage from earlier and later stages, which allows a single MOSFET output to drive a considerable number of MOSFET inputs. Bipolar transistor-based logic (such as TTL) does not have such a high fanout capacity. This isolation also makes it easier for the designers to ignore to some extent loading effects between logic stages independently. That extent is defined by the operating frequency: as frequencies increase, the input impedance of the MOSFETs decreases.\nAnalog.\nThe MOSFET's advantages in digital circuits do not translate into supremacy in all analog circuits. The two types of circuit draw upon different features of transistor behavior. Digital circuits switch, spending most of their time either fully on or fully off. The transition from one to the other is only of concern with regards to speed and charge required. Analog circuits depend on operation in the transition region where small changes to \"V\"gs can modulate the output (drain) current. The JFET and bipolar junction transistor (BJT) are preferred for accurate matching (of adjacent devices in integrated circuits), higher transconductance and certain temperature characteristics which simplify keeping performance predictable as circuit temperature varies.\nNevertheless, MOSFETs are widely used in many types of analog circuits because of their own advantages (zero gate current, high and adjustable output impedance and improved robustness vs. BJTs which can be permanently degraded by even lightly breaking down the emitter-base). The characteristics and performance of many analog circuits can be scaled up or down by changing the sizes (length and width) of the MOSFETs used. By comparison, bipolar transistors follow a different scaling law. MOSFETs' ideal characteristics regarding gate current (zero) and drain-source offset voltage (zero) also make them nearly ideal switch elements, and also make switched capacitor analog circuits practical. In their linear region, MOSFETs can be used as precision resistors, which can have a much higher controlled resistance than BJTs. In high power circuits, MOSFETs sometimes have the advantage of not suffering from thermal runaway as BJTs do. This means that complete analog circuits can be made on a silicon chip in a much smaller space and with simpler fabrication techniques. MOSFETS are ideally suited to switch inductive loads because of tolerance to inductive kickback.\nSome ICs combine analog and digital MOSFET circuitry on a single mixed-signal integrated circuit, making the needed board space even smaller. This creates a need to isolate the analog circuits from the digital circuits on a chip level, leading to the use of isolation rings and silicon on insulator (SOI). Since MOSFETs require more space to handle a given amount of power than a BJT, fabrication processes can incorporate BJTs and MOSFETs into a single device. Mixed-transistor devices are called bi-FETs (bipolar FETs) if they contain just one BJT-FET and BiCMOS (bipolar-CMOS) if they contain complementary BJT-FETs. Such devices have the advantages of both insulated gates and higher current density.\nAnalog switches.\nBidirectional analog switches pass analog signals when on or block them (by presenting a high impedance) when off. The MOSFETs used are typically symmetrical, such that their drain and source exchange places depending on the relative voltages of their electrodes; at any moment, the source would be the more negative side for an nMOS or the more positive side for a pMOS. All of these switches are limited on what signals they can pass or stop by their gate-source, gate-drain and source\u2013drain voltages; exceeding the voltage, current, or power limits will potentially damage the switch. See Power MOSFET subsection down below.\nSingle-type.\nThis analog switch uses a four-terminal symmetrical MOSFET of either P or N type.\nIn the case of an n-type switch, the body is connected to the most negative supply (usually GND) and the gate is used as the switch control. Whenever the gate voltage exceeds the source voltage by at least a threshold voltage, the MOSFET conducts. The higher the voltage, the more the MOSFET can conduct. An nMOS switch passes all voltages less than Vgate\u00a0\u2212\u00a0\"V\"thresold_nMOS, but passes lower voltages better than higher ones. When the switch is conducting, it typically operates in the linear (or ohmic) mode of operation, since the source and drain voltages will typically be nearly equal.\nIn the case of a pMOS, the body is connected to the most positive voltage, and the gate is brought to a lower potential to turn the switch on. The pMOS switch passes all voltages higher than Vgate\u00a0\u2212\u00a0\"V\"thresold_pMOS (note: enhancement-mode pMOS FETs have a negative threshold voltage), but passes higher voltages better than lower ones.\nDual-type (CMOS).\nThis \"complementary\" or CMOS type of switch uses one pMOS and one nMOS FET connected in parallel to counteract the limitations of the single-type switch. When used in digital logic it is called a transmission gate. The FETs have their drains and sources connected in parallel, the body of the pMOS is connected to the high potential (Vpos in diagram) and the body of the nMOS is connected to the low potential (Vneg). To turn the switch on, the gate of the pMOS is driven to the low potential and the gate of the nMOS is driven to the high potential. For voltages between Vpos\u00a0\u2212\u00a0\"V\"thresold_nMOS and Vneg\u00a0\u2212\u00a0\"V\"thresold_pMOS, both FETs conduct the signal, though with the nMOS passing lower voltages better while the pMOS passes higher voltages better. For voltages less than Vneg\u00a0\u2212\u00a0\"V\"thresold_pMOS, the nMOS conducts alone. For voltages greater than Vpos\u00a0\u2212\u00a0\"V\"thresold_nMOS, the pMOS conducts alone.\nThe voltage limits for this switch are the gate-source, gate-drain and source-drain voltage limits for both FETs. Also, the pMOS is typically two to three times wider than the nMOS, so the switch will be balanced for speed in the two directions.\nTri-state circuitry sometimes incorporates a CMOS MOSFET switch on its output to provide for a low-ohmic, full-range output when on, and a high-ohmic, mid-level signal when off.\nConstruction.\nGate material.\nThe primary criterion for the gate material is that it is a good conductor. Highly doped polycrystalline silicon is an acceptable but certainly not ideal conductor, and also suffers from some more technical deficiencies in its role as the standard gate material. Nevertheless, there are several reasons favoring use of polysilicon:\nWhile polysilicon gates have been the de facto standard for the last twenty years, they do have some disadvantages which have led to their likely future replacement by metal gates. These disadvantages include:\nPresent high performance CPUs use metal gate technology, together with high-\u03ba dielectrics, a combination known as \"high-\u03ba, metal gate\" (HKMG). The disadvantages of metal gates are overcome by a few techniques:\nInsulator.\nAs devices are made smaller, insulating layers are made thinner, often through steps of thermal oxidation or localised oxidation of silicon (LOCOS). For nano-scaled devices, at some point tunneling of carriers through the insulator from the channel to the gate electrode takes place. To reduce the resulting leakage current, the insulator can be made thinner by choosing a material with a higher dielectric constant. To see how thickness and dielectric constant are related, note that Gauss's law connects field to charge as:\n formula_33\nwith \"Q\" = charge density, \u03ba = dielectric constant, \u03b50 = permittivity of empty space and \"E\" = electric field. From this law it appears the same charge can be maintained in the channel at a lower field provided \u03ba is increased. The voltage on the gate is given by:\n formula_34\nwith \"V\"G = gate voltage, \"V\"ch = voltage at channel side of insulator, and \"t\"ins = insulator thickness. This equation shows the gate voltage will not increase when the insulator thickness increases, provided \u03ba increases to keep \"t\"ins / \u03ba = constant (see the article on high-\u03ba dielectrics for more detail, and the section in this article on gate-oxide leakage).\nThe insulator in a MOSFET is a dielectric which can in any event be silicon oxide, formed by LOCOS but many other dielectric materials are employed. The generic term for the dielectric is gate dielectric since the dielectric lies directly below the gate electrode and above the channel of the MOSFET.\nJunction design.\nThe source-to-body and drain-to-body junctions are the object of much attention because of three major factors: their design affects the current\u2013voltage (\"I\u2013V\") characteristics of the device, lowering output resistance, and also the speed of the device through the loading effect of the junction capacitances, and finally, the component of stand-by power dissipation due to junction leakage.\nThe drain induced barrier lowering of the threshold voltage and channel length modulation effects upon \"I-V\" curves are reduced by using shallow junction extensions. In addition, \"halo\" doping can be used, that is, the addition of very thin heavily doped regions of the same doping type as the body tight against the junction walls to limit the extent of depletion regions.\nThe capacitive effects are limited by using raised source and drain geometries that make most of the contact area border thick dielectric instead of silicon.\nThese various features of junction design are shown (with artistic license) in the figure.\nScaling.\nOver the past decades, the MOSFET (as used for digital logic) has continually been scaled down in size; typical MOSFET channel lengths were once several micrometres, but modern integrated circuits are incorporating MOSFETs with channel lengths of tens of nanometers. Robert Dennard's work on scaling theory was pivotal in recognising that this ongoing reduction was possible. Intel began production of a process featuring a 32\u00a0nm feature size (with the channel being even shorter) in late 2009. The semiconductor industry maintains a \"roadmap\", the ITRS, which sets the pace for MOSFET development. Historically, the difficulties with decreasing the size of the MOSFET have been associated with the semiconductor device fabrication process, the need to use very low voltages, and with poorer electrical performance necessitating circuit redesign and innovation (small MOSFETs exhibit higher leakage currents and lower output resistance).\nSmaller MOSFETs are desirable for several reasons. The main reason to make transistors smaller is to pack more and more devices in a given chip area. This results in a chip with the same functionality in a smaller area, or chips with more functionality in the same area. Since fabrication costs for a semiconductor wafer are relatively fixed, the cost per integrated circuits is mainly related to the number of chips that can be produced per wafer. Hence, smaller ICs allow more chips per wafer, reducing the price per chip. In fact, over the past 30 years the number of transistors per chip has been doubled every 2\u20133 years once a new technology node is introduced. For example, the number of MOSFETs in a microprocessor fabricated in a 45 nm technology can well be twice as many as in a 65 nm chip. This doubling of transistor density was first observed by Gordon Moore in 1965 and is commonly referred to as Moore's law. It is also expected that smaller transistors switch faster. For example, one approach to size reduction is a scaling of the MOSFET that requires all device dimensions to reduce proportionally. The main device dimensions are the channel length, channel width, and oxide thickness. When they are scaled down by equal factors, the transistor channel resistance does not change, while gate capacitance is cut by that factor. Hence, the RC delay of the transistor scales with a similar factor. While this has been traditionally the case for the older technologies, for the state-of-the-art MOSFETs reduction of the transistor dimensions does not necessarily translate to higher chip speed because the delay due to interconnections is more significant.\nProducing MOSFETs with channel lengths much smaller than a micrometre is a challenge, and the difficulties of semiconductor device fabrication are always a limiting factor in advancing integrated circuit technology. Though processes such as ALD have improved fabrication for small components, the small size of the MOSFET (less than a few tens of nanometers) has created operational problems:\nHigher subthreshold conduction.\nAs MOSFET geometries shrink, the voltage that can be applied to the gate must be reduced to maintain reliability. To maintain performance, the threshold voltage of the MOSFET has to be reduced as well. As threshold voltage is reduced, the transistor cannot be switched from complete turn-off to complete turn-on with the limited voltage swing available; the circuit design is a compromise between strong current in the \"on\" case and low current in the \"off\" case, and the application determines whether to favor one over the other. Subthreshold leakage (including subthreshold conduction, gate-oxide leakage and reverse-biased junction leakage), which was ignored in the past, now can consume upwards of half of the total power consumption of modern high-performance VLSI chips.\nIncreased gate-oxide leakage.\nThe gate oxide, which serves as insulator between the gate and channel, should be made as thin as possible to increase the channel conductivity and performance when the transistor is on and to reduce subthreshold leakage when the transistor is off. However, with current gate oxides with a thickness of around 1.2\u00a0nm (which in silicon is ~5\u00a0atoms thick) the quantum mechanical phenomenon of electron tunneling occurs between the gate and channel, leading to increased power consumption. Silicon dioxide has traditionally been used as the gate insulator. Silicon dioxide however has a modest dielectric constant. Increasing the dielectric constant of the gate dielectric allows a thicker layer while maintaining a high capacitance (capacitance is proportional to dielectric constant and inversely proportional to dielectric thickness). All else equal, a higher dielectric thickness reduces the quantum tunneling current through the dielectric between the gate and the channel.\nInsulators that have a larger dielectric constant than silicon dioxide (referred to as high-\u03ba dielectrics), such as group IVb metal silicates e.g. hafnium and zirconium silicates and oxides are being used to reduce the gate leakage from the 45 nanometer technology node onwards. On the other hand, the barrier height of the new gate insulator is an important consideration; the difference in conduction band energy between the semiconductor and the dielectric (and the corresponding difference in valence band energy) also affects leakage current level. For the traditional gate oxide, silicon dioxide, the former barrier is approximately 8 eV. For many alternative dielectrics the value is significantly lower, tending to increase the tunneling current, somewhat negating the advantage of higher dielectric constant. The maximum gate-source voltage is determined by the strength of the electric field able to be sustained by the gate dielectric before significant leakage occurs. As the insulating dielectric is made thinner, the electric field strength within it goes up for a fixed voltage. This necessitates using lower voltages with the thinner dielectric.\nIncreased junction leakage.\nTo make devices smaller, junction design has become more complex, leading to higher doping levels, shallower junctions, \"halo\" doping and so forth, all to decrease drain-induced barrier lowering (see the section on junction design). To keep these complex junctions in place, the annealing steps formerly used to remove damage and electrically active defects must be curtailed increasing junction leakage. Heavier doping is also associated with thinner depletion layers and more recombination centers that result in increased leakage current, even without lattice damage.\nDrain-induced barrier lowering and \"V\"T roll off.\nDrain-induced barrier lowering (DIBL) and \"V\"T roll off: Because of the short-channel effect, channel formation is not entirely done by the gate, but now the drain and source also affect the channel formation. As the channel length decreases, the depletion regions of the source and drain come closer together and make the threshold voltage (\"V\"T) a function of the length of the channel. This is called \"V\"T roll-off. \"V\"T also becomes function of drain to source voltage \"V\"DS. As we increase the \"V\"DS, the depletion regions increase in size, and a considerable amount of charge is depleted by the \"V\"DS. The gate voltage required to form the channel is then lowered, and thus, the \"V\"T decreases with an increase in \"V\"DS. This effect is called drain induced barrier lowering (DIBL).\nLower output resistance.\nFor analog operation, good gain requires a high MOSFET output impedance, which is to say, the MOSFET current should vary only slightly with the applied drain-to-source voltage. As devices are made smaller, the influence of the drain competes more successfully with that of the gate due to the growing proximity of these two electrodes, increasing the sensitivity of the MOSFET current to the drain voltage. To counteract the resulting decrease in output resistance, circuits are made more complex, either by requiring more devices, for example the cascode and cascade amplifiers, or by feedback circuitry using operational amplifiers, for example a circuit like that in the adjacent figure.\nLower transconductance.\nThe transconductance of the MOSFET decides its gain and is proportional to hole or electron mobility (depending on device type), at least for low drain voltages. As MOSFET size is reduced, the fields in the channel increase and the dopant impurity levels increase. Both changes reduce the carrier mobility, and hence the transconductance. As channel lengths are reduced without proportional reduction in drain voltage, raising the electric field in the channel, the result is velocity saturation of the carriers, limiting the current and the transconductance.\nInterconnect capacitance.\nTraditionally, switching time was roughly proportional to the gate capacitance of gates. However, with transistors becoming smaller and more transistors being placed on the chip, interconnect capacitance (the capacitance of the metal-layer connections between different parts of the chip) is becoming a large percentage of capacitance. Signals have to travel through the interconnect, which leads to increased delay and lower performance.\nHeat production.\nThe ever-increasing density of MOSFETs on an integrated circuit creates problems of substantial localized heat generation that can impair circuit operation. Circuits operate more slowly at high temperatures, and have reduced reliability and shorter lifetimes. Heat sinks and other cooling devices and methods are now required for many integrated circuits including microprocessors. Power MOSFETs are at risk of thermal runaway. As their on-state resistance rises with temperature, if the load is approximately a constant-current load then the power loss rises correspondingly, generating further heat. When the heatsink is not able to keep the temperature low enough, the junction temperature may rise quickly and uncontrollably, resulting in destruction of the device.\nProcess variations.\nWith MOSFETs becoming smaller, the number of atoms in the silicon that produce many of the transistor's properties is becoming fewer, with the result that control of dopant numbers and placement is more erratic. During chip manufacturing, random process variations affect all transistor dimensions: length, width, junction depths, oxide thickness \"etc.\", and become a greater percentage of overall transistor size as the transistor shrinks. The transistor characteristics become less certain, more statistical. The random nature of manufacture means we do not know which particular example MOSFETs actually will end up in a particular instance of the circuit. This uncertainty forces a less optimal design because the design must work for a great variety of possible component MOSFETs. See process variation, design for manufacturability, reliability engineering, and statistical process control.\nModeling challenges.\nModern ICs are computer-simulated with the goal of obtaining working circuits from the first manufactured lot. As devices are miniaturized, the complexity of the processing makes it difficult to predict exactly what the final devices look like, and modeling of physical processes becomes more challenging as well. In addition, microscopic variations in structure due simply to the probabilistic nature of atomic processes require statistical (not just deterministic) predictions. These factors combine to make adequate simulation and \"right the first time\" manufacture difficult.\nOther types.\nDual-gate.\nThe dual-gate MOSFET has a tetrode configuration, where both gates control the current in the device. It is commonly used for small-signal devices in radio frequency applications where biasing the drain-side gate at constant potential reduces the gain loss caused by Miller effect, replacing two separate transistors in cascode configuration. Other common uses in RF circuits include gain control and mixing (frequency conversion). The \"tetrode\" description, though accurate, does not replicate the vacuum-tube tetrode. Vacuum-tube tetrodes, using a screen grid, exhibit much lower grid-plate capacitance and much higher output impedance and voltage gains than triode vacuum tubes. These improvements are commonly an order of magnitude (10 times) or considerably more. Tetrode transistors (whether bipolar junction or field-effect) do not exhibit improvements of such a great degree.\nThe FinFET is a double-gate silicon-on-insulator device, one of a number of geometries being introduced to mitigate the effects of short channels and reduce drain-induced barrier lowering. The \"fin\" refers to the narrow channel between source and drain. A thin insulating oxide layer on either side of the fin separates it from the gate. SOI FinFETs with a thick oxide on top of the fin are called \"double-gate\" and those with a thin oxide on top as well as on the sides are called \"triple-gate\" FinFETs.\nDepletion-mode.\nThere are \"depletion-mode\" MOSFET devices, which are less commonly used than the standard \"enhancement-mode\" devices already described. These are MOSFET devices that are doped so that a channel exists even with zero voltage from gate to source. To control the channel, a negative voltage is applied to the gate (for an n-channel device), depleting the channel, which reduces the current flow through the device. In essence, the depletion-mode device is equivalent to a normally closed (on) switch, while the enhancement-mode device is equivalent to a normally open (off) switch.\nDue to their low noise figure in the RF region, and better gain, these devices are often preferred to bipolars in RF front-ends such as in TV sets.\nDepletion-mode MOSFET families include the BF960 by Siemens and Telefunken, and the BF980 in the 1980s by Philips (later to become NXP Semiconductors), whose derivatives are still used in AGC and RF mixer front-ends.\nMetal\u2013insulator\u2013semiconductor field-effect transistor (MISFET).\nMetal\u2013insulator\u2013semiconductor field-effect-transistor, or \"MISFET\", is a more general term than \"MOSFET\" and a synonym to \"insulated-gate field-effect transistor\" (IGFET). All MOSFETs are MISFETs, but not all MISFETs are MOSFETs.\nThe gate dielectric insulator in a MISFET is a substrate oxide (hence typically silicon dioxide) in a MOSFET, but other materials can also be employed. The gate dielectric lies directly below the gate electrode and above the channel of the MISFET. The term \"metal\" is historically used for the gate material, even though now it is usually highly doped polysilicon or some other non-metal.\nInsulator types may be:\nNMOS logic.\nFor devices of equal current driving capability, n-channel MOSFETs can be made smaller than p-channel MOSFETs, due to p-channel charge carriers (holes) having lower mobility than do n-channel charge carriers (electrons), and producing only one type of MOSFET on a silicon substrate is cheaper and technically simpler. These were the driving principles in the design of nMOS logic which uses n-channel MOSFETs exclusively. However, neglecting leakage current, unlike CMOS logic, nMOS logic consumes power even when no switching is taking place. With advances in technology, CMOS logic displaced nMOS logic in the mid-1980s to become the preferred process for digital chips.\nPower MOSFET.\nPower MOSFETs have a different structure. As with most power devices, the structure is vertical and not planar. Using a vertical structure, it is possible for the transistor to sustain both high blocking voltage and high current. The voltage rating of the transistor is a function of the doping and thickness of the N-epitaxial layer (see cross section), while the current rating is a function of the channel width (the wider the channel, the higher the current). In a planar structure, the current and breakdown voltage ratings are both a function of the channel dimensions (respectively width and length of the channel), resulting in inefficient use of the \"silicon estate\". With the vertical structure, the component area is roughly proportional to the current it can sustain, and the component thickness (actually the N-epitaxial layer thickness) is proportional to the breakdown voltage.\nPower MOSFETs with lateral structure are mainly used in high-end audio amplifiers and high-power PA systems. Their advantage is a better behaviour in the saturated region (corresponding to the linear region of a bipolar transistor) than the vertical MOSFETs. Vertical MOSFETs are designed for switching applications.\nDouble-diffused metal\u2013oxide\u2013semiconductor (&lt;templatestyles src=\"Template:Visible anchor/styles.css\" /&gt;DMOS).\nThere are \"LDMOS\" (lateral double-diffused metal oxide semiconductor) and \"VDMOS\" (vertical double-diffused metal oxide semiconductor). Most power MOSFETs are made using this technology.\nRadiation-hardened-by-design (RHBD).\nSemiconductor sub-micrometer and nanometer electronic circuits are the primary concern for operating within the normal tolerance in harsh radiation environments like outer space. One of the design approaches for making a radiation-hardened-by-design (RHBD) device is enclosed-layout-transistor (ELT). Normally, the gate of the MOSFET surrounds the drain, which is placed in the center of the ELT. The source of the MOSFET surrounds the gate. Another RHBD MOSFET is called H-Gate. Both of these transistors have very low leakage currents with respect to radiation. However, they are large in size and take up more space on silicon than a standard MOSFET. In older STI (shallow trench isolation) designs, radiation strikes near the silicon oxide region cause the channel inversion at the corners of the standard MOSFET due to accumulation of radiation induced trapped charges. If the charges are large enough, the accumulated charges affect STI surface edges along the channel near the channel interface (gate) of the standard MOSFET. This causes a device channel inversion to occur along the channel edges, creating an off-state leakage path. Subsequently, the device turns on; this process severely degrades the reliability of circuits. The ELT offers many advantages, including an improvement of reliability by reducing unwanted surface inversion at the gate edges which occurs in the standard MOSFET. Since the gate edges are enclosed in ELT, there is no gate oxide edge (STI at gate interface), and thus the transistor off-state leakage is reduced very much. Low-power microelectronic circuits including computers, communication devices, and monitoring systems in space shuttles and satellites are very different from what is used on earth. They are radiation (high-speed atomic particles like proton and neutron, solar flare magnetic energy dissipation in Earth's space, energetic cosmic rays like X-ray, gamma ray etc.) tolerant circuits. These special electronics are designed by applying different techniques using RHBD MOSFETs to ensure safe space journeys and safe space-walks of astronauts.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "40346", "revid": "7903804", "url": "https://en.wikipedia.org/wiki?curid=40346", "title": "JFET", "text": "Type of field-effect transistor\nThe junction field-effect transistor (JFET) is one of the simplest types of field-effect transistor. JFETs are three-terminal semiconductor devices that can be used as electronically controlled switches or resistors, or to build amplifiers. \nUnlike bipolar junction transistors, JFETs are exclusively voltage-controlled in that they do not need a biasing current. Electric charge flows through a semiconducting channel between \"source\" and \"drain\" terminals. By applying a reverse bias voltage to a \"gate\" terminal, the channel is \"pinched\", so that the electric current is impeded or switched off completely. A JFET is usually conducting when there is zero voltage between its gate and source terminals. If a potential difference of the proper polarity is applied between its gate and source terminals, the JFET will be more resistive to current flow, which means less current would flow in the channel between the source and drain terminals.\nJFETs are sometimes referred to as depletion-mode devices, as they rely on the principle of a depletion region, which is devoid of majority charge carriers. The depletion region has to be closed to enable current to flow.\nJFETs can have an n-type or p-type channel. In the n-type, if the voltage applied to the gate is negative with respect to the source, the current will be reduced (similarly in the p-type, if the voltage applied to the gate is positive with respect to the source). Because a JFET in a common source or common drain configuration has a large input impedance (sometimes on the order of 1010\u00a0ohms), little current is drawn from circuits used as input to the gate.\nHistory.\nA succession of FET-like devices was patented by Julius Lilienfeld in the 1920s and 1930s. However, materials science and fabrication technology would require decades of advances before FETs could actually be manufactured.\nJFET was first patented by Heinrich Welker in 1945. During the 1940s, researchers John Bardeen, Walter Houser Brattain, and William Shockley were trying to build a FET, but failed in their repeated attempts. They discovered the point-contact transistor in the course of trying to diagnose the reasons for their failures. Following Shockley's theoretical treatment on JFET in 1952, a working practical JFET was made in 1953 by George C. Dacey and Ian M. Ross. Japanese engineers Jun-ichi Nishizawa and Y. Watanabe applied for a patent for a similar device in 1950 termed static induction transistor (SIT). The SIT is a type of JFET with a short channel.\nHigh-speed, high-voltage switching with JFETs became technically feasible following the commercial introduction of Silicon carbide (SiC) wide-bandgap devices in 2008. Due to early difficulties in manufacturing \u2014 in particular, inconsistencies and low yield \u2014 SiC JFETs remained a niche product at first, with correspondingly high costs. By 2018, these manufacturing issues had been mostly resolved. By then, SiC JFETs were also commonly used in conjunction with conventional low-voltage Silicon MOSFETs. In this combination, SiC JFET + Si MOSFET devices have the advantages of wide band-gap devices as well as the easy gate drive of MOSFETs.\nStructure.\nThe JFET is a long channel of semiconductor material, doped to contain an abundance of positive charge carriers or holes (\"p-type\"), or of negative carriers or electrons (\"n-type\"). Ohmic contacts at each end form the source (S) and the drain (D). A pn-junction is formed on one or both sides of the channel, or surrounding it using a region with doping opposite to that of the channel, and biased using an ohmic gate contact (G).\nFunctions.\nJFET operation can be compared to that of a garden hose. The flow of water through a hose can be controlled by squeezing it to reduce the cross section and the flow of electric charge through a JFET is controlled by constricting the current-carrying channel. The current also depends on the electric field between source and drain (analogous to the difference in pressure on either end of the hose). This current dependency is not supported by the characteristics shown in the diagram above a certain applied voltage. This is the \"saturation region\", and the JFET is normally operated in this constant-current region where device current is virtually unaffected by drain-source voltage. The JFET shares this constant-current characteristic with junction transistors and with thermionic tube (valve) tetrodes and pentodes.\nConstriction of the conducting channel is accomplished using the field effect: a voltage between the gate and the source is applied to reverse bias the gate-source pn-junction, thereby widening the depletion layer of this junction (see top figure), encroaching upon the conducting channel and restricting its cross-sectional area. The depletion layer is so-called because it is depleted of mobile carriers and so is electrically non-conducting for practical purposes.\nWhen the depletion layer spans the width of the conduction channel, \"pinch-off\" is achieved and drain-to-source conduction stops. Pinch-off occurs at a particular reverse bias (\"V\"GS) of the gate\u2013source junction. The \"pinch-off voltage\" (Vp) (also known as \"threshold voltage\" or \"cut-off voltage\") varies considerably, even among devices of the same type. For example, \"V\"GS(off) for the Temic J202 device varies from \u22120.8 V to \u22124 V, while the \"V\"GS(off) for the J308 varies between \u22121 V and \u22126.5 V. (Confusingly, the term \"pinch-off voltage\" is also used to refer to the \"V\"DS value that separates the linear and saturation regions.)\nTo switch off an n-channel device requires a negative gate\u2013source voltage (\"V\"GS). Conversely, to switch off a p-channel device requires positive \"V\"GS.\nIn normal operation, the electric field developed by the gate blocks source\u2013drain conduction to some extent.\nSome JFET devices are symmetrical with respect to the source and drain.\nSchematic symbols.\nThe JFET gate is sometimes drawn in the middle of the channel (instead of at the drain or source electrode as in these examples). This symmetry suggests that \"drain\" and \"source\" are interchangeable, so the symbol should be used only for those JFETs where they are indeed interchangeable.\nThe symbol may be drawn inside a circle (representing the envelope of a discrete device) if the enclosure is important to circuit function, such as dual matched components in the same package.\nIn every case the arrow head shows the polarity of the P\u2013N junction formed between the channel and the gate. As with an ordinary diode, the arrow points from P to N, the direction of conventional current when forward-biased. An English mnemonic is that the arrow of an N-channel device \"points in\".\nComparison with other transistors.\nAt room temperature, JFET gate current (the reverse leakage of the gate-to-channel junction) is comparable to that of a MOSFET (which has insulating oxide between gate and channel), but much less than the base current of a bipolar junction transistor. The JFET has higher gain (transconductance) than the MOSFET, as well as lower flicker noise, and is therefore used in some low-noise, high input-impedance op-amps. Additionally the JFET is less susceptible to damage from static charge buildup.\nMathematical model.\nLinear ohmic region.\nThe current in N-JFET due to a small voltage \"V\"DS (that is, in the \"linear\" or \"ohmic\" or \"triode region\") is given by treating the channel as a rectangular bar of material of electrical conductivity formula_1: \n formula_2\nwhere\n \"I\"D = drain\u2013source current,\n \"b\" = channel thickness for a given gate voltage,\n \"W\" = channel width,\n \"L\" = channel length,\n \"q\" = electron charge = 1.6\u00d710\u221219\u00a0C,\n \"\u03bcn\" = electron mobility,\n \"Nd\" = n-type doping (donor) concentration,\n \"V\"P = pinch-off voltage.\nThen the drain current in the \"linear region\" can be approximated as\n formula_3\nIn terms of formula_4, the drain current can be expressed as\n formula_5\nConstant-current region.\nThe drain current in the \"saturation\" or \"active\" or \"pinch-off region\" is often approximated in terms of gate bias as\n formula_6\nwhere \"I\"DSS is the saturation current at zero gate\u2013source voltage, i.e. the maximum current that can flow through the FET from drain to source at any (permissible) drain-to-source voltage (see, e. g., the \"I\"\u2013\"V\" characteristics diagram above).\nIn the \"saturation region\", the JFET drain current is most significantly affected by the gate\u2013source voltage and barely affected by the drain\u2013source voltage.\nIf the channel doping is uniform, such that the depletion region thickness will grow in proportion to the square root of the absolute value of the gate\u2013source voltage, then the channel thickness \"b\" can be expressed in terms of the zero-bias channel thickness \"a\" as\n formula_7\nwhere \n \"V\"P is the pinch-off voltage\u00a0\u2013 the gate\u2013source voltage at which the channel thickness goes to zero,\n \"a\" is the channel thickness at zero gate\u2013source voltage.\nTransconductance.\nThe transconductance for the junction FET is given by\n formula_8\nwhere formula_9 is the pinchoff voltage, and \"I\"DSS is the maximum drain current. This is also called formula_10 or formula_11 (for transadmittance).\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "40347", "revid": "1307061178", "url": "https://en.wikipedia.org/wiki?curid=40347", "title": "House of Babenberg", "text": "Austrian noble dynasty from c. 962 to 1246\nThe House of Babenberg was a noble dynasty of Austrian Dukes and Margraves. Descending from the Popponids and originally from Bamberg in the Duchy of Franconia (present-day Bavaria), the Babenbergs ruled the imperial Margraviate of Austria from its creation in 976 AD until its elevation to a duchy in 1156, and from then until the extinction of the line in 1246, whereafter they were succeeded by the House of Habsburg.\nOrigin.\nElder and Younger Houses of Babenberg.\nThe Babenberg family can be broken down into two distinct groups;\nThe second group claimed to have originated from the first, but scholars have not been able to verify that claim. In addition, a descent from the Bavarian Luitpolding dukes is assumed (though perhaps in maternal line).\nPopponids.\nLike the French royal Capetian dynasty, the Elder Babenbergs likely descended from the Robertians. The earliest known Babenberg, Count Poppo I of Grapfeld, was first mentioned in 819 as a ruler in the \"gau\" of Grabfeld, a historic region in northeastern Franconia bordering on Thuringia. He may have been a descendant of the Robertian Count Cancor of Hesbaye.\nOne of Poppo's sons, Henry, served as \"princeps militiae\" under King Louis the Younger and was sometimes called margrave (\"marchio\") and duke (\"dux\") in Franconia under King Charles the Fat of East Francia. He was killed fighting against the Vikings during the Siege of Paris in 886. Another son, Poppo II, was margrave in Thuringia from 880 to 892, when he was deposed by King Charles' successor Arnulf of Carinthia. The Popponids had been favoured by Charles the Fat, but Arnulf reversed this policy in favour of rivalling Conrad the Elder, a member of the Conradine dynasty from the Lahngau in Rhenish Franconia and relative of Arnulf's consort Ota.\nBabenberg Feud.\nThe leaders of the Babenbergs were the sons of Duke Henry, who called themselves after their castle of Babenburg on the upper Main river, around which their possessions centered. The city of Bamberg was built around the ancestral castle of the family. The Conradines were led by Conrad the Elder and his brothers Rudolf and Gebhard, probably the sons of Count Udo of Neustria.\nThe rivalry between the Babenberg and Conradine families was intensified by their efforts to extend their authority in the region of the middle Main, and this quarrel, known as the \"Babenberg feud\", came to a first head in 892, when King Arnulf deposed Poppo II as Thuringian ruler, appointing Conrad the Elder instead, and installed Conrad's brother Rudolf as Bishop of W\u00fcrzburg. The struggle intensified at the beginning of the 10th century during the troubled reign of Arnulf's son King Louis the Child. Clashes of arms occurred in 902, when the Conradine laid siege to Babenburg Castle and arrested Adalhard of Babenberg. The next year, Adalhard was executed at the \"Reichstag\" of Forchheim; in return, the Babenbergs occupied the city of W\u00fcrzburg and expelled Bishop Rudolf.\nMeanwhile, Rudolf's brother Gebhard was appointed Duke of Lotharingia in 903, and had to cope both with revolting nobles and the continuing attacks by Babenberg forces. Both sides met in the battle of Fritzlar on 27 February 906, where the Conradines won a decisive victory, although Conrad the Elder fell in the battle. Two of the Babenberg brothers were also killed. The third, Adalbert, was summoned before the imperial court by the regent Archbishop Hatto I of Mainz, a partisan of the Conradines. He refused to appear, held his own for a time in his castle at Theres against the king's forces, but surrendered in 906, and in spite of a promise of safe-conduct by Hatto was beheaded.\nConrad the Younger became Duke of Franconia in 906 and King of East Francia (as Conrad I) in 911, while the Babenbergs lost their influence in Franconia.\nMargraves of Austria.\nIn 962, the Bavarian count Leopold I (\"Liupo\"), possibly a descendant of the Luitpolding duke Arnulf of Bavaria, was first mentioned as a faithful follower of Emperor Otto I. He remained a loyal supporter of Otto's son and successor Otto II and in 976 appears as count of the Bavarian Eastern March, then a district not more than 60 miles in breadth on the eastern frontier of the duchy, which grew into the Margraviate of Austria. Leopold, who received the territory as a reward for his fidelity to Emperor Otto II during the uprising of Duke Henry II of Bavaria, extended its area down the Danube river into what is today Lower Austria at the expense of the retreating Magyars.\nLeopold was succeeded in 994 by his son Henry I, who continued his father's policy, was followed in 1018 by his brother Adalbert, whose marked loyalty to Emperor Henry II and his Salian successor Henry III was rewarded by many tokens of favour. Adalbert expanded the Austrian territory up to the present borders on the Leitha, March and Thaya rivers. He was succeeded in 1055 by his son, Ernest.\nLeopold II, margrave from 1075, quarrelled with Emperor Henry IV during the Investiture Controversy, when he supported the papal side of Bishop Altmann of Passau. Though Leopold had to cope with the invading troops of Duke Vratislaus II of Bohemia and was defeated at the 1082 Battle of Mailberg, the emperor was unable to oust him from his march or to prevent the succession of his son Leopold III in 1096. Between 1075 - 1095 the dynasty had its seat at Babenberg Castle of Gars am Kamp.\nLeopold III supported Henry V, the son of Emperor Henry IV, in his rising against his father, but was soon drawn over to the emperor's side. In 1106 he married the daughter of Henry IV, Agnes, widow of Duke Frederick I of Swabia. In 1125 he declined the royal crown in favour of Lothair of Supplinburg. His zeal in founding monasteries, such as Klosterneuburg Monastery, earned for him his surname \"the Pious\", and canonization by Pope Innocent VIII in 1485. He is regarded as the patron saint of Lower and Upper Austria.\nDukes of Austria.\nOne of Leopold's younger sons was Bishop Otto of Freising. His eldest son Leopold IV became margrave in 1136, and in 1139 received the Duchy of Bavaria from the hands of King Conrad III, who had banned the Welf duke Henry the Proud.\nLeopold's brother Henry Jasomirgott (allegedly named after his favourite oath, \"Yes, so [help] me God!\") was made Count Palatine of the Rhine in 1140, and became Margrave of Austria on Leopold's death in 1141. Having married Gertrude, the widow of Henry the Proud, he was invested in 1143 with the Duchy of Bavaria, and resigned his office as count palatine. In 1147 he participated in the Second Crusade, and after his return, renounced Bavaria at the instance of the new king Frederick Barbarossa who gave the duchy of Bavaria to Henry the Proud's son, Duke Henry the Lion of Saxony. As compensation for this, Austria, the capital of which had been transferred to Vienna about 1155, was elevated into a duchy according to the \"Privilegium Minus\".\nRise to power.\nThe second duke was Henry's son Leopold V, who succeeded him in 1177 and took part in the crusades of 1182 and 1190 as well as the Third Crusade. In Palestine, he quarrelled with King Richard I of England, captured him on his homeward journey and handed him over to Emperor Henry VI. Leopold increased the territories of the Babenbergs by acquiring the Duchy of Styria under the will of his kinsman Duke Ottokar IV. He died in 1194, and Austria fell to one son, Frederick, and Styria to another, Leopold; but on Frederick's death in 1198 they were again united by Leopold as Duke Leopold VI, surnamed \"the Glorious\".\nThe new duke fought in the crusades in Spain, Egypt, and Palestine, but is more celebrated as a lawgiver, a patron of letters, and a founder of many towns. Under him Vienna became the centre of culture in Germany and the great school of Minnesingers. His later years were spent in strife with his son Frederick, and he died in 1230 at San Germano, now renamed Cassino, whither he had gone to arrange the peace between Emperor Frederick II and Pope Gregory IX.\nExtinction.\nFrederick II, Leopold VI's son by Theodora Angelina, succeeded his father as duke upon the elder man's death in 1230. Frederick II soon earned the epithet \"the Quarrelsome\" from his ongoing disputes with the kings of Hungary and Bohemia and with Holy Roman Emperor Frederick II. Duke Frederick deprived his mother and sisters of their possessions, was hated by his subjects on account of his oppressive rule, and, in 1236, was placed under the imperial ban and driven from Austria. However, he was later restored to his duchy when Emperor Frederick II was excommunicated. Subsequently, Duke Frederick II treated with Emperor Frederick II in vain to make Austria a kingdom.\nThe male line of the Babenbergs became extinct in 1246, when Frederick II was killed in battle (the Henneberg branch of the Franconian Babenbergs lived on until 1583 when its lands were divided among the two branches of the Wettin family).\nFrederick's heir general was Gertrude of Austria, the only child of his late elder brother, Henry of Austria by that man's wife, Agnes of Thuringia. However, neither her husbands nor her son succeeded in settling the Babenberg inheritance under their power. Gertrude's only surviving child, Agnes of Baden, tried to reclaim at least part of her inheritance through her third husband Ulrich II of Heunburg, but was unsuccessful.\nAfter some years of struggle known as the War of the Babenberg Succession (1246\u20131256/78/82), the Duchies of Austria and Styria fell to Ottokar II of Bohemia, and subsequently to Rudolph I of Habsburg, whose descendants were to rule Austria until 1918.\nGenetic legacy.\nByzantine blood.\nAll the Babenberg dukes from Leopold V onward were descended from Byzantine emperors \u2014 Leopold's mother, Theodora Komnene, being a granddaughter of the Emperor, John II Komnenos. Subsequently, Leopold V's younger son, Leopold VI, also married a Byzantine princess (Theodora Angelina), as did his youngest son (by Theodora), Frederick II, who married \"Sophia Laskarina\".\nThe Babenbergs and the Habsburgs.\nThe next dynasty in Austria\u2014the Habsburgs\u2014were originally not descendants of the Babenbergs. It was not until the children of Albert I of Germany that the Babenberg blood was brought into the Habsburg line, though this blood was from the pre-ducal Babenbergs. A side effect of this marriage was the use of the Babenberg name \"Leopold\" by the Habsburgs for one of their sons.\nThe Habsburgs did eventually gain descent from the Babenberg dukes, though at different times. The first Habsburg line to be descended from the Babenbergs was the \"Albertine\" line. This was achieved through the marriage of Albert III, Duke of Austria to Beatrix of Nuremberg. As such, their son, Albert IV, Duke of Austria, was the first Habsburg duke who was descended from the Babenberg dukes. However, the male line of that branch of the Habsburgs died out in 1457 with Ladislas V Posthumus of Bohemia.\nThe next Habsburg line to gain Babenberg blood was the \"Styrian\" line, which occurred with the children of Ferdinand I, Holy Roman Emperor and Anna of Bohemia and Hungary, the latter of whom descended from Babenberg dukes. It was actually from Elizabeth of Austria, the sister of Ladislas V Posthumus of Bohemia, that the Styrian line gained their Babenberg blood.\nThe \"Spanish\" line was the last Habsburg line to gain Babenberg blood. Again it was via the previous Habsburg line to gain Babenberg blood (i.e. the Styrian) that the Spanish Habsburg gained their descent from the Babenbergs \u2014 Anna of Austria, the wife of Philip II of Spain and mother of Philip (from whom all subsequent Spanish Habsburgs were descended), was a male-line granddaughter of Ferdinand and Anna. As a result, after 1598, all Habsburg scions descended from the Babenberg Dukes.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "40348", "revid": "27015025", "url": "https://en.wikipedia.org/wiki?curid=40348", "title": "Sacrosanctum Concilium", "text": "Catholic Constitution on the Liturgy\nSacrosanctum Concilium, the Constitution on the Sacred Liturgy, is one of the constitutions of the Second Vatican Council. It was approved by the assembled bishops by a vote of 2,147 to 4 and promulgated by Pope Paul VI on 4 December 1963. The main aim was to revise the traditional liturgical texts and rituals to reflect more fully fundamental principles, and be more pastorally effective in the changed conditions of the times, clarifying the role of ordained ministers and the forms of appropriate participation of lay faithful in the Catholic Church's liturgy, especially that of the Roman Rite. The title is taken from the opening lines of the document and means \"This Sacred Council\".\nTitle.\nThe document's official title is the \"Constitution on the Sacred Liturgy\", but as is customary with Catholic documents, the recognised name of this constitution, \"Sacrosanctum Concilium\" in Latin, is taken from the first line (\"incipit\") of the document, which sets the objective of liturgical reform within the wider context of the aims of the \"sacred Council\": \"to impart an ever increasing vigor to the Christian life of the faithful; to adapt more suitably to the needs of our own times those institutions which are subject to change; to foster whatever can promote union among all who believe in Christ; [and] to strengthen whatever can help to call the whole of mankind into the household of the Church.\"\nApplication.\nThe principles underlying the Council's liturgical reforms were applicable to the Roman Rite and to the Eastern rites, although the practical norms set out in the Constitution applied only to the Roman Rite. The Council returned to consider the Eastern rites in its November 1964 Decree on the Eastern Catholic Churches.\nInstructions were issued for the rites used for all seven of the sacraments of the Catholic Church to be revised.\n\"Aggiornamento\" and participation of the laity.\nOne of the first issues considered by the council, and the matter that had the most immediate effect on the lives of individual Catholics, was the renewal of the liturgy. The central idea was \"aggiornamento\" of the traditional liturgical texts and rituals to reflect more fully fundamental principles, and be more pastorally effective in the changed conditions of the times, clarifying not only the role of ordained ministers but also the forms of appropriate participation of lay faithful.&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt; Mother Church earnestly desires that all the faithful should be led to that fully conscious and active participation in liturgical celebrations which is demanded by the very nature of the liturgy. Such participation by the Christian people as a chosen race, a royal priesthood, a holy nation, a redeemed people (1 Peter 2:9; cf. 2:4\u20135), is their right and duty by reason of their baptism.\nPopes Pius X and Pius XII asked that the people be taught how to chant the responses at Mass and that they learn the prayers of the Mass in order to participate intelligently. Now the bishops decreed that: \"To promote active participation, the people should be encouraged to take part by means of acclamations, responses, psalmody, antiphons, and songs.\" Composers should \"produce compositions which\u00a0... [provide] for the active participation of the entire assembly of the faithful.\" In all, there are 12 references to \"active participation\" in the document.\nAfter centuries when, with the Mass in Latin, Catholic piety centred around popular devotions, the bishops decreed that \"Popular devotions ... should be so drawn up that they harmonize with the liturgical seasons, accord with the sacred liturgy, are in some fashion derived from it, and lead the people to it, since, in fact, the liturgy by its very nature far surpasses any of them.\"\nImplementation.\nThe council fathers established guidelines to govern the renewal of the liturgy, which included, allowed, and encouraged greater use of the vernacular (native language) in addition to Latin, particularly for the biblical readings, intercessions, and other prayers. Implementation of the council's directives on the liturgy was to be carried out under the authority of Pope Paul VI by a special papal commission known as the Council for the Implementation of the Constitution on the Sacred Liturgy (or the \"Consilium\" for short), later incorporated in the Congregation for Divine Worship and the Discipline of the Sacraments, and, in the areas entrusted to them, by national conferences of bishops, which, if they had a shared language, were expected to collaborate in producing a common translation. In his encyclical letter of August 1964, \"Ecclesiam Suam\", the pope called for the \"intelligent\" and \"zealous\" implementation of the constitution's provisions on the ministry of the Word.\nLegacy.\nPope John Paul II issued an Apostolic Letter commemorating the 40th anniversary of the constitution, entitled \"Spiritus et Sponsa\" (\"The Spirit and the Bride\") on 4 December 2003.\nOn 24 August 2017 Pope Francis emphasized that \"the reform of the liturgy is irreversible\" and called for continued efforts to implement the reforms, repeating what Pope Paul VI had said one year before he died: \"The time has come, now, to definitely leave aside the disruptive ferments, equally pernicious in one sense or the other, and to implement fully, according to its right inspiring criteria, the reform approved by us in application of the decisions of the council.\"\nReferences.\nCitations.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nBibliography.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;\nFurther reading.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "40349", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=40349", "title": "Spices", "text": ""}
{"id": "40350", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=40350", "title": "Tumeric", "text": ""}
{"id": "40351", "revid": "7903804", "url": "https://en.wikipedia.org/wiki?curid=40351", "title": "The Cathedral and the Bazaar", "text": "1990s software development essay and book by Eric S. Raymond\nThe Cathedral and the Bazaar: Musings on Linux and Open Source by an Accidental Revolutionary (abbreviated CatB) is an essay, and later a book, by Eric S. Raymond on software engineering methods, based on his observations of the Linux kernel development process and his experiences managing an open source project, fetchmail. It examines the struggle between top-down and bottom-up design. The essay was first presented by Raymond at the Linux Kongress on May 27, 1997, in W\u00fcrzburg, Germany, and was published as the second chapter of the same\u2011titled book in 1999.\nThe illustration on the cover of the book is a 1913 painting by titled \"Composition with Figures\" and belongs to the collection of the State Tretyakov Gallery. The book was released under the Open Publication License v2.0 in 1999.\nCentral thesis.\nThe software essay contrasts two different free software development models:\nThe essay's central thesis is Raymond's proposition that \"given enough eyeballs, all bugs are shallow\" (which he terms Linus's law): the more widely available the source code is for public testing, scrutiny, and experimentation, the more rapidly all forms of bugs will be discovered. In contrast, Raymond claims that an inordinate amount of time and energy must be spent hunting for bugs in the Cathedral model, since the working version of the code is available only to a few developers.\nLessons for creating good open source software.\nRaymond points to 19 \"lessons\" learned from various software development efforts, each describing attributes associated with good practice in open source software development:\nLegacy and reception.\nIn 1998, the essay helped the final push for Netscape Communications Corporation to release the source code for Netscape Communicator and start the Mozilla project; it was cited by Frank Hecker and other employees as an outside independent validation of his arguments. Netscape's public recognition of this influence brought Raymond renown in hacker culture.\nWhen O'Reilly Media published the book in 1999 it became one of the first complete, commercially distributed books published under the Open Publication License.\nMarshall Poe, in his essay \"The Hive\", likens Wikipedia to the bazaar model that Raymond defines. Jimmy Wales himself was inspired by the work (as well as arguments put forward in pre-Internet works, such as Friedrich Hayek's article \"The Use of Knowledge in Society\"), arguing that \"It opened my eyes to the possibility of mass collaboration\".\nIn 1999 Nikolai Bezroukov published two critical essays on Eric Raymond's views of open source software, the second one called \"A second look at \"The Cathedral and the Bazaar\"\". They produced a sharp response from Eric Raymond.\nCurtis Yarvin's essay \"The Cathedral or the Bizarre\", which argues for the end of American democracy, is named after the Raymond essay. \nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "40352", "revid": "274535", "url": "https://en.wikipedia.org/wiki?curid=40352", "title": "Gaudium et spes", "text": "Vatican II document on the church in the modern world\n (, \"Joys and Hopes\"), the Pastoral Constitution on the Church in the Modern World, is one of the four constitutions promulgated during the Second Vatican Council between 1963 and 1965. Issued on 7 December 1965, it was the last and longest published document from the council and is the first constitution published by a Catholic ecumenical council to address the entire world. \n clarified and reoriented the role of the church's mission to people outside of the Catholic faith. It was the first time that the church took explicit responsibility for its role in the larger world. The constitution's creation was necessitated by fear of the irrelevance in the modern era due to its ignorance on problems that plague the modern world. The document represents an inner examination of the church by the council and features a response to problems affecting the modern world.\nWithin are the themes of gift of self and the promotion of peace. While initial reception of the document was focused on the shift in theological considerations, reception of today marks the document as a turning point in the Church's focus on the world.\nWith the failure of the Church to respond promptly to major global events such as World War II and the Holocaust, Pope John XXIII began Vatican II with an emphasis on examining the role of the church in the world. This culminated with the creation of to address the role of the church in serving the world outside of Christianity. During the creation of the document itself, went through multiple versions of Schemas to reflect the idea Pope John XXIII wanted to achieve during the council. After long debate during the council over , the document came to cover a wide range of topics examining the inner workings of the Church and its interactions with the world as a whole. Such topics include marriage and family, the development of culture, economics, politics and peace and war.\nBecause of this role addressing how the Catholic Church relates towards the world at large, compared to the focus of on how the church understands itself, and have been called \"the two pillars of the Second Vatican Council.\"\nApproved by a vote of 2,307 to 75 among the bishops assembled at the council, it was promulgated by Pope Paul VI on 7 December 1965, the day the council ended. As is customary with Catholic documents, the title is taken from its opening words in Latin \"the joys and hopes\". The English translation begins:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;The joys and hopes, the grief and anguish of the people of our time, especially of those who are poor or afflicted, are the joys and hopes, the grief and anguish of the followers of Christ as well.\nBackground.\nContext within Vatican II.\nAt the beginning of the Second Vatican Council on October 11, 1962, Pope John XXIII celebrated the opening Mass of the council, during which he indirectly brought to light the economic and political issues for which the council was summoned. Such issues included the devastation of World War II, Nazi horrors, the current threat of a nuclear war between the United States and Russia, and the end of colonialism and racism. The church had failed to act substantially on these issues, contributing to a feeling of irrelevance within larger considerations of the state of the world. From an ecclesiastical standpoint, there were open issues concerning completing the work of the interrupted First Vatican Council and the need for reform within the church. As a result of these problems, in his opening speech, Gaudet Mater Ecclesia, Pope John XXIII distanced the council from focusing solely on the gloom of the problems of the world as the Church had done in previous councils. He wanted the council to focus on \"the marvelous progress of the discoveries of human genius\", while orienting the role of the church to one that should deal with right and wrong in the world. The council, as a whole, was to be an update to the essential inner workings and teachings of the church to better fit the modern world. was to be the culmination of this as Pope John XXIII envisioned the constitution to share in the \"joys and the hopes\" of the entire world.\nFollowing the death of John XXIII, his successor Pope Paul VI also referred to the relationship between the church and the changing world in his first encyclical letter, \"Ecclesiam Suam\".\nThe creation of the text of \"Gaudium et spes\".\n was not drafted before the council met, but arose from the floor of the council and was one of the last to be promulgated. In preparation for the council, Pope John XXIII asked for suggestions concerning the substance of Vatican II. In a large width of responses sorted through by a commission appointed by the Pope, there resulted in 67 thematic documents that would be placed for discussion during the council. Four of those documents, dealing with the church in the modern world, ultimately formed the logical backbone of what would become . In what is described as a turning point of the council, the harsh disagreement over the four documents drove the attendees to invalidate all 67 thematic documents as inadequate. This led to Pope John asking Cardinal Leo Jozef Suenens to create a new agenda for the council in November, 1962. The agenda was to include an examination on the Church and its role within the modern world, as necessitated by the debate over the four documents in question. By December 1962, Suenens revealed his work. The role of the church would be split between different viewpoints: \"Ad intra,\" internally, and \"ad extra,\" externally. These ultimately resulted in and , respectively. Schema 17 and, toward the end of the council, Schema 12 inspired the later creation of . Schema 12, while focusing on the church's role in world social issues, underwent many changes before ultimately being rejected by the attendees over a lack of cohesion within the document.\nCardinal Suenens was again tasked with producing a new schema; however, Pope John XXIII died before its completion on June 3, 1963. Upon the election of Pope Paul VI on June 21, 1963, Pope Paul continued the creation of the document. When the revised Schema 12 was published in September 1963, it was met with intense scrutiny by the bishops. Ultimately, the schema, through multiple revisions that lasted until 1964, was transformed into Schema 13, which would become . Schema 13 not only related the role of the church to the world but also dealt with questions dealing with modern problems. On November 16, 1964, Schema 13 was approved to be edited after all of the Bishops' suggestions were aggregated. Father Pierre Haubtmann led a commission tasked with editing the schema. Over the period of the next year, Father Haubtmann led discussions and continued to develop the schema in line with discussion offered during the council. Approved by a vote of 2,307 to 75 of the bishops assembled at the council, Schema 13 was promulgated as by Pope Paul VI on 7 December 1965, the day the council ended.\nOverview.\nThe Dogmatic Constitution, , was addressed \"not only to the sons of the Church and to all who invoke the name of Christ, but to the whole of humanity\" as part of the Second Council's effort to appeal to the larger considerations of the Catholic Church.\nWhereas the previous Vatican Council in 1869\u201370 had tried to defend the role of the church in an increasingly secular world, the Second Vatican Council focused on examining the role of the Church in the modern world.\n was adopted after \"Lumen Gentium\", the Constitution on the Church, and it reflects the ecclesiological approach of that text. \"Gaudium\" recognized and encouraged the laity to take on \"his or her own distinctive role\" in the modern world. In recent years, Church leaders, such as Cardinal Francis Arinze, have since clarified this to mean that laity should work to make themselves competent in their own profession, and observe laws proper to \"each discipline\". The decree was debated at length and approved by much the largest and most international council in the history of the Church.\nThe ecumenical constitution created by the Second Vatican Council focused on the role of the church within the modern world. It was the last document promulgated during the Second Vatican Council and the first church document to place the church within the significance of the world. illustrated the church is aware of problems within the world and its responsibilities toward them. While world problems are a focus of the text, it also brings to light the human person and their orientation toward God as well as the mission of the church itself. The mission of the Church needed to recognize the realities of secularization and pluralism. Bishop Christopher Butler points out that a key principle behind the \"audacious change\" in this and in several earlier outward-looking documents of the council was that the Church is \"Christ himself using us as his instruments to bring salvation to all\", and in charity we must \"presume that those who differ from us\u2026 are nevertheless [people] of good will\". As a whole, represented an inner looking of the Church on itself so that it may take responsibility and comment on issues affecting the world.\nSuch issues of responsibilities in the world are highlighted by the cardinals of the council such as Leo Joseph Suenens of Belgium, who urged the council to take on social responsibility for war, poverty, and Third World suffering. This stance was supported by other prominent clergy members including Cardinal Giovanni Battista Montini of Milan, who became Pope Paul VI, and Cardinal Lercaro of Bologna. Additionally, Thomas Rosica points out that the Council Fathers \"... were men who had experienced two world wars, the horror of the Holocaust, the onset of the nuclear weaponry, the hostility of communism, the awesome and only partially understood impact of science and technology.\" In the Introduction it states: \"the Church has always had the duty of scrutinizing the signs of the times and of interpreting them in the light of the Gospel.\"\nMarie-Dominique Chenu, professor of the Pontifical University of Saint Thomas Aquinas, \"Angelicum\" was influential in the composition of , as was Louis-Joseph Lebret. \"The problem of poverty and of overcoming it through a healthy economy, respectful of the primary value of the person, allows for a vast discussion on political ethics in .\" In the end, the \"council exhorts Christians, as citizens of two cities, to strive to discharge their earthly duties conscientiously and in response to the Gospel spirit\".\nContents.\nCentral themes.\nGift of self.\nThe \"gift of self\" from GS \u00a724 was a phrase used often by Pope John Paul II and particularly in his theology of the Body. This phrase has also been described as \"the Law of the Gift\". Citing the biblical concept that all men are created in the image of God, \"Gaudium\" states that man cannot truly find himself unless he understands the \"gift of self\", and that in understanding himself he will understand and love both God and his fellow man.\nAlthough no Vatican source has been found defining precisely what the \"gift of self\" is, some scholars and Church leaders appear to interpret it in recent decades as being associated with marriage. \nPromotion of peace.\nThe final chapter of the document is \"The Fostering of Peace and the Promotion of a Community of Nations\". This chapter references themes expressed near the start of Vatican II by Pope John XXIII in 1963 in his encyclical letter, \"Pacem in Terris.\" \"Pacem\" defines the \"common good\", arguing that while individualism leads to individualistic focus and behavior and collectivism leads to a loss of the individual, the \"common good\" strikes a middle ground and begins with the focus on the community before returning it to the individual. \"Pacem\" focuses its argument not on a theological basis, but rather employs \"natural law\" to appeal to both believers and non-believers who might not be as familiar with theological sources.\nReception.\nImmediately following Vatican II.\nInitial opposition came in the form of debate over the theological basis of Vatican II and . According to Henri de Lubac, the theological balance of nature and grace pre-Vatican II was overturned in favor of nature and the world which goes against the importance placed upon transcendence.\nReception today.\n has been evaluated as the shift of the church to its new globalized view of the world. It serves as the basis for multiculturalism in the modern church and has become the basis of the church's message to the world today. The document's frequent references to \"the human heart\" and \"the heart of man\" are reflected in several quotations adopted by Pope Francis in his encyclical letter, \"Dilexit nos\".\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "40353", "revid": "50829985", "url": "https://en.wikipedia.org/wiki?curid=40353", "title": "London, Ontario", "text": "London is a city in southwestern Ontario, Canada, along the Quebec City\u2013Windsor Corridor. The city had a population of 422,324 according to the 2021 Canadian census. London is at the confluence of the Thames River and North Thames River, approximately from both Toronto and Detroit; and about from Buffalo, New York. The city of London is politically separate from Middlesex County, though it remains the county seat.\nLondon and the Thames were named after the English city and river in 1793 by John Graves Simcoe, who proposed the site for the capital city of Upper Canada. The first European settlement was between 1801 and 1804 by Peter Hagerman. The village was founded in 1826 and incorporated in 1855. Since then, London has grown to be the largest southwestern Ontario municipality and Canada's 11th largest metropolitan area, having annexed many of the smaller communities that surround it.\nLondon is a regional centre of healthcare and education, being home to the University of Western Ontario (which brands itself \"Western University\"), Fanshawe College, and three major hospitals: Victoria Hospital, University Hospital and St. Joseph's Hospital. The city hosts a number of musical and artistic exhibits and festivals, which contribute to its tourism industry, but its economic activity is centered on education, medical research, manufacturing, financial services, and information technology. London's university and hospitals are among its top ten employers. London lies at the junction of Highways 401 and 402, connecting it to Toronto, Windsor, and Sarnia; these highways also make the Detroit-Windsor, Port Huron-Sarnia, and Niagara Falls border crossings with the United States easily accessible. The city also has railway stations and bus stations and is home to the London International Airport.\nHistory.\nA series of archaeological sites throughout southwestern Ontario, named for the Parkhill Complex excavated near Parkhill, indicate the presence of Paleo-Indians in the area dating back approximately 11,000 years. Just prior to European settlement, the London area was the site of several Attawandaron, Odawa, and Ojibwe villages. The Lawson Site in northwest London is an archaeological excavation and partial reconstruction of an approximately 500-year-old Neutral Iroquoian village, estimated to have been home to 2,000 people. These groups were driven out by the Iroquois by c.\u20091654 in the Beaver Wars. The Iroquois abandoned the region some 50 years later, driven out by the Ojibwa. An Anishinaabeg community site was described as located near the forks of Thames River (Anishinaabe language: Eshkani-ziibi, \"Antler River\") in c.\u20091690 and was referred to as Pahkatequayang (\"Baketigweyaang\":\"At the River Fork\" (lit: at where the by-stream is)).\nThe Oneida Nation of the Thames, Chippewas of the Thames First Nation, and Munsee-Delaware Nation reserves are located south-west of the city.\nSettlement.\nThe current location of London was selected as the site of the future capital of Upper Canada in 1793 by Lieutenant-Governor John Graves Simcoe, who also named the village which was founded in 1826. Originally, Simcoe had proposed to call it \"Georgiana\", in honour of George III, the reigning monarch at that time. It did not become the capital Simcoe envisioned. Rather, it was an administrative seat for the area west of the actual capital, York (now Toronto). The London Township Treaty of 1796 with the Chippewa ceded the original town site on the north bank of the Thames (then known as the \"Escunnisepe\") to Upper Canada.\nLondon was part of the Talbot Settlement, named for Colonel Thomas Talbot, the chief administrator of the area, who oversaw the land surveying and built the first government buildings for the administration of the western Ontario peninsular region. Together with the rest of southwestern Ontario, the village benefited from Talbot's provisions not only for building and maintaining roads but also for assignment of access priorities to main routes to productive land. Crown and clergy reserves then received preference in the rest of Ontario.\nIn 1814, the Battle of Longwoods took place during the War of 1812 in what is now Southwest Middlesex, near London. The retreating British Army were staying at Hungerford Hill when they were attacked by the Kentucky Mounted Riflemen. In 1827, a settlement was started Bryon when Cyrenius Hall built a gristmill.\nIn 1832, the new settlement suffered an outbreak of cholera. London proved a centre of strong Tory support during the Upper Canada Rebellion of 1837, notwithstanding a brief rebellion led by Charles Duncombe. Consequently, the British government located its Ontario peninsular garrison there in 1838, increasing its population with soldiers and their dependents, and the business support populations they required. London was incorporated as a town in 1840.\nOn 13 April 1845, a fire destroyed much of London, which was then largely constructed of wooden buildings. One of the first casualties was the town's only fire engine. The fire burned nearly of land, destroying 150 buildings, before it burned itself out later that day. One fifth of London was destroyed in the province's first million-dollar fire.\nDevelopment.\nJohn Carling, Tory MP for London, gave three events to explain the development of London in a 1901 speech: the location of the court and administration in London in 1826, the arrival of the military garrison in 1838, and the arrival of the railway in 1853.\nThe population in 1846 was 3,500. Brick buildings included a jail and court house, and large barracks. London had a fire company, a theatre, a large Gothic church, nine other churches or chapels, and two market buildings. The buildings that were destroyed by fire in 1845 were mostly rebuilt by 1846. Connection with other communities was by road, using mainly stagecoaches that ran daily. A weekly newspaper was published and mail was received daily by the post office. Two villages named Petersville and Kensington once stood where downtown London now is. Petersville was founded by Samuel Peters in 1853. Kensington was founded around about 1878. Petersville and Kensington were amalgamated on 4 March 1881 to form London West.\nOn 1 January 1855, London was incorporated as a city (10,000 or more residents). In the 1860s, a sulphur spring was discovered at the forks of the Thames River while industrialists were drilling for oil. The springs became a popular destination for wealthy Ontarians, until the turn of the 20th century when a textile factory was built at the site, replacing the spa.\nRecords from 1869 indicate a population of about 18,000 served by three newspapers, churches of all major denominations and offices of all the major banks. Industries included several tanneries, oil refineries and foundries, four flour mills, the Labatt Brewing Company and the Carling brewery in addition to other manufacturing companies such as EMCO Wheaton. Both the Great Western and Grand Trunk railways had stops here. Several insurance companies also had offices in the city.\nThe Crystal Palace Barracks, an octagonal brick building with eight doors and forty-eight windows built in 1861, was used for events such the Provincial Agricultural Fair of Canada West held in London that year. It was visited by Prince Arthur, Duke of Connaught and Strathearn, Governor-General John Young, 1st Baron Lisgar and Prime Minister John A. Macdonald.\nLong before the Royal Military College of Canada was established in 1876, there were proposals for military colleges in Canada. Staffed by British Regulars, adult male students underwent three-month-long military courses from 1865 at the School of Military Instruction in London. Established by Militia General Order in 1865, the school enabled Officers of Militia or Candidates for Commission or promotion in the Militia to learn Military duties, drill and discipline, to command a Company at Battalion Drill, to Drill a Company at Company Drill, the internal economy of a Company and the duties of a Company's Officer. The school was not retained at Confederation, in 1867.\nIn 1875, London's first iron bridge, the Blackfriars Street Bridge, was constructed. It replaced a succession of flood-failed wooden structures that had provided the city's only northern road crossing of the river. A rare example of a wrought iron bowstring arch through truss bridge, the Blackfriars remains open to pedestrian and bicycle traffic, though it was temporarily closed indefinitely to vehicular traffic due to various structural problems and was once again reopened to vehicular traffic 1 December 2018. The Blackfriars, amidst the river-distance between the Carling Brewery and the historic Tecumseh Park (including a major mill), linked London with its western suburb of Petersville, named for Squire Peters of Grosvenor Lodge. That community joined with the southern subdivision of Kensington in 1874, formally incorporating as the municipality of Petersville. Although it changed its name in 1880 to the more inclusive \"London West\", it remained a separate municipality until ratepayers voted for amalgamation with London in 1897, largely due to repeated flooding. The most serious flood was in July 1883, which resulted in serious loss of life and property devaluation. This area retains much original and attractively maintained 19th-century tradespeople's and workers' housing, including Georgian cottages as well as larger houses, and a distinct sense of place. In 1897, London West was annexed to London.\nLondon's eastern suburb, London East, was (and remains) an industrial centre, which also incorporated in 1874. It was founded as Lilley's Corners by Charles Lilley in 1854. Oil was discovered in the Petrolia area and Lilley's Corners was chosen as the refining site because it was close to the railroad. The Ontario Car Works, the Great Western Gasworks and the London Street Railroad all had their headquarters in Lilley's Corners. In 1872, Lilley's Corners became a village. It was annexed to London in 1885. Attaining the status of town in 1881, it continued as a separate municipality until concerns over expensive waterworks and other fiscal problems led to amalgamation in 1885. The southern suburb of London, including Wortley Village, was collectively known as \"London South\". Never incorporated, the South was annexed to the city in 1890, although Wortley Village still retains a distinct sense of place. The area started to be settled in the 1860s. In 1880, Polk's \"Directory\" called London South \"a charming suburb of the City of London.\". By contrast, the settlement at Broughdale on the city's north end had a clear identity, adjoined the university, and was not annexed until 1961. Broughdale was named after Reverend Charles C. Brough, the Anglican Archdeacon of London who settled there in 1854. Broughdale started to grow when it was connected to the London Street Railroad in 1901, leading to a real estate bubble. A post office was opened in Broughdale on 1 July 1904 with Charles Watlers as postmaster. Broughdale was initially named Brough, but was renamed Broughdale in 1906 because it sounded better. In 1924, the University of Western Ontario was founded in the former Broughdale. After the founding of the university, Broughdale became more like a city and less like a village. Broughdale was incorporated as a village in 1930. In 1961, Broughdale was annexed to London.\nIvor F. Goodson and Ian R. Dowbiggin have explored the battle over vocational education in London, Ontario, in the 1900\u20131930 era. The London Technical and Commercial High School came under heavy attack from the city's social and business elite, which saw the school as a threat to the budget of the city's only academic high school, London Collegiate Institute.\nThe Banting House, a National Historic Site of Canada, is where Frederick Banting developed the ideas that led to the discovery of insulin. Banting lived and practiced in London for ten months, from July 1920 to May 1921. London is also the site of the Flame of Hope, which is intended to burn until a cure for diabetes is discovered.\nLondon's role as a military centre continued into the 20th century during the two World Wars, serving as the administrative centre for the Western Ontario district. In 1905, the London Armoury was built and housed the First Hussars until 1975. A private investor purchased the historic site and built a new hotel (Delta London Armouries, 1996) in its place, preserving the shell of the historic building. In the 1950s, two reserve battalions amalgamated and became London and Oxford Rifles (3rd Battalion), The Royal Canadian Regiment. This unit continues to serve today as 4th Battalion, The Royal Canadian Regiment. The Regimental Headquarters of The Royal Canadian Regiment remains in London at Wolseley Barracks on Oxford Street. The barracks are home to the First Hussars militia regiment as well.\nAnnexation to present.\nLondon annexed many of the surrounding communities in 1961, including Byron and Masonville, adding 60,000 people and more than doubling its area. After this amalgamation, suburban growth accelerated as London grew outward in all directions, creating expansive new subdivisions such as Westmount, Oakridge, Whitehills, Pond Mills, White Oaks and Stoneybrook.\nOn 1 January 1993, London annexed nearly the entire township of Westminster, a large, primarily rural municipality directly south of the city, including the police village of Lambeth. With this massive annexation, which also included part of London township, London almost doubled in area again, adding several thousand more residents. In the present day, London stretches south to the boundary with Elgin County, north and east to Fanshawe Lake, north and west to the township of Middlesex Centre (the nearest developed areas of it being Arva to the north and Komoka to the west) and east to Nilestown and Dorchester.\nThe 1993 annexation, made London one of the largest urban municipalities in Ontario. Intense commercial and residential development is presently occurring in the southwest and northwest areas of the city. Opponents of this development cite urban sprawl, destruction of rare Carolinian zone forest and farm lands, replacement of distinctive regions by generic malls, and standard transportation and pollution concerns as major issues facing London. The City of London is currently the eleventh-largest urban area in Canada, eleventh-largest census metropolitan area in Canada, and the sixth-largest city in Ontario.\nDisasters.\nOn Victoria Day, 24 May 1881, the stern-wheeler ferry \"SS Victoria\" capsized in the Thames River close to Cove Bridge in West London. Approximately 200 passengers drowned in the shallow river, making it one of the worst disasters in London's history, and is now dubbed \"The Victoria Day Disaster\". At the time, London's population was relatively small; therefore it was hard to find a person in the city who did not have a family member affected by the tragedy.\nTwo years later, on 12 July 1883, the first of the two most devastating floods in London's history killed 17 people. The second major flood, on 26 April 1937, destroyed more than a thousand houses across London, and caused over $50 million in damages, particularly in West London.\nOn 3 January 1898, the floor of the assembly hall at London City Hall collapsed, killing 23 people and leaving more than 70 injured. Testimony at a coroner's inquest described the wooden beam under the floor as unsound, with knots and other defects reducing its strength by one fifth to one third.\nAfter repeated floods, the Upper Thames River Conservation Authority in 1953 built Fanshawe Dam on the North Thames to control the downstream rivers. Financing for this project came from the federal, provincial, and municipal governments. Other natural disasters include a 1984 tornado that led to damage on several streets in the White Oaks area of South London.\nOn 11 December 2020, a partially-constructed apartment building just off of Wonderland Road in southwest London collapsed, killing two people and injuring at least four others. In January 2024, both Oxford County companies involved in the building's construction were fined $400,000, with The Ministry of Labour, Immigration, Training and Skills Development declaring the companies had failed to \"provide proper information, instruction and supervision, specifically on the use of proper concrete measuring techniques on the project.\"\nGeography.\nThe area was formed during the retreat of the glaciers during the last ice age, which produced areas of marshland, notably the Sifton Bog, as well as some of the most agriculturally productive areas of farmland in Ontario.\nThe Thames River dominates London's geography. The North and South branches of the Thames River meet at the centre of the city, a location known as \"The Forks\" or \"The Fork of the Thames\". The North Thames runs through the man-made Fanshawe Lake in northeast London. Fanshawe Lake was created by Fanshawe Dam, constructed to protect the downriver areas from the catastrophic flooding which affected the city in 1883 and 1937.\nClimate.\nLondon has a humid continental climate (K\u00f6ppen \"Dfb\"), with modestly warm summers, and cold and cloudy winters with frequent snow.\nBecause of its location in the continent, London experiences large seasonal contrast, tempered to a point by the surrounding Great Lakes. The proximity of the lakes also ensure abundant cloud cover, particularly in late Fall and Winter. The summers are usually warm to hot and humid, with a July average of , and temperatures above occur on average 11 days per year. In 2016, however, temperatures rose above this temperature on more than 35 days, and in 2018, four heatwaves led to a peak humidex of . The city is affected by frequent thunderstorms due to hot, humid summer weather, as well as the convergence of breezes originating from Lake Huron and Lake Erie. The same convergence zone is responsible for spawning funnel clouds and the occasional tornado. Spring and autumn in between are not long, and winters are cold but with frequent thaws.\nAnnual precipitation averages . Its winter snowfall totals are heavy, averaging about per year, although the localized nature of snow squalls means the total can vary widely from year to year as do accumulations over different areas of the city. Some of the snow accumulation comes from lake effect snow and snow squalls originating from Lake Huron, some to the northwest, which occurs when strong, cold winds blow from that direction. From 5 December 2010, to 9 December 2010, London experienced record snowfall when up to of snow fell in parts of the city. Schools and businesses were closed for three days and bus service was cancelled after the second day of snow.\nThe highest temperature ever recorded in London was on 6 August 1918. The lowest temperature ever recorded was on 9 February 1934.\nParks.\nLondon has a number of parks. Victoria Park in downtown London is a major centre of community events, attracting an estimated 1 million visitors per year. Other major parks include Harris Park, Gibbons Park, Fanshawe Conservation Area (Fanshawe Pioneer Village), Springbank Park, White Oaks Park and Westminster Ponds. The city also maintains a number of gardens and conservatories. One of these, the Remembrance Gardens, commemorates those who died fighting in wars. In addition to an annual Remembrance Day gathering, it contains a poppy garden as a memorial to 1,200 who died in World War I, as well as a refurbished bell from the Netherlands.\nDemographics.\n&lt;templatestyles src=\"Module:Historical populations/styles.css\"/&gt;\nIn the 2021 Census of Population conducted by Statistics Canada, London had a population of living in of its total private dwellings, a change of from its 2016 population of . With a land area of , it had a population density of in 2021.\nAt the census metropolitan area (CMA) level in the 2021 census, the London CMA had a population of living in of its total private dwellings, a change of from its 2016 population of . With a land area of , it had a population density of in 2021.\nEthnicity.\nAs per the 2021 census, the most common ethnic or cultural origins in London are English (21.9%), Scottish (17.4%), Irish (16.8%), Canadian (12.1%), German (9.3%), French (6.6%), Dutch (5.0%), Italian (4.5%), British Isles (4.3%), Indian (3.7%), Polish (3.6%), and Chinese. Indigenous people made up 2.6% of the population, with most being First Nations (1.9%). Ethnocultural backgrounds in the city included European (68.7%), South Asian (6.5%), Arab (5.3%), Black (4.2%), Latin American (3.0%), Chinese (2.9%), Southeast Asian (1.4%), Filipino (1.4%), West Asian (1.3%), and Korean (1.0%).\nLanguage.\nThe 2021 census found English to be the mother tongue of 71.1% of the population. This was followed by Arabic (3.7%), Spanish (2.7%), Mandarin (1.6%), Portuguese (1.3%), French (1.1%), Polish (1.1%), Korean (0.8%), Punjabi (0.8%), Malayalam (0.8%), and Urdu (0.7%). Of the official languages, 98% of the population reported knowing English and 7.2% French.\nReligion.\nIn 2021, 48.8% of the population identified as Christian, with Catholics (21.5%) making up the largest denomination, followed by United Church (4.7%), Anglican (4.4%), Orthodox (2.0%), Presbyterian (1.5%), Baptist (1.4%), and other denominations. 37.2% of the population reported no religious affiliation. Others identified as Muslim (8.4%), Hindu (2.1%), Sikh (1.0%), Buddhist (0.9%), Jewish (0.5%), and with other religions.\nEconomy.\nMedicine.\nLondon's economy is dominated by medical research, financial services, manufacturing, and information technology. Much of the life sciences and biotechnology related research is conducted or supported by the University of Western Ontario (partly through the Robarts Research Institute), which adds about C$1.5 billion to the London economy annually. Private companies in the industry like Alimentiv, PolyAnalytik, KGK Sciences and Sernova are also based in London. The largest employer in London is the London Health Sciences Centre, which employs 10,555 people.\nTechnology.\nSince the economic crisis of 2009, the city has transitioned to become a technology hub with a focus on the Digital Creative sector. As of 2016, London is home to 300 technology companies that employ 3% of the city's labour force. Many of these companies have moved into former factories and industrial spaces in and around the downtown core, and have renovated them as modern offices. For example, Info-Tech Research Group's London office is in a hosiery factory, and Arcane Digital moved into a 1930s industrial building in 2015. The Historic London Roundhouse, a steam locomotive repair shop built in 1887, is now home to Royal LePage Triland Realty, rTraction and more. Its redesign, which opened in 2015, won the 2015 Paul Oberman Award for Adaptive Re-Use from the Architectural Conservancy of Ontario. London is also home to StarTech.com, Diply, video game companies like Digital Extremes, Big Blue Bubble and Big Viking Games, and Voices.com, which provides voiceover artists a platform to advertise and sell their services to those looking for voiceover work. Other tech companies located in London include AutoData, Carfax Canada, HRDownloads, Mobials, Northern Commerce and Paystone which recently raised $100M.\nPetroleum.\nOne of Canada's largest domestic integrated oil companies, Imperial Oil, was founded in London in 1880. The company was headquartered in London only until 1883, when it moved its offices to Petrolia. It is now headquartered in Calgary.\nIn 1925, Supertest Petroleum was founded in London. It was acquired in 1971 by BP Canada.\nFinance.\nSeveral financial houses have been founded in London. In May 1883, a groups of Londoners founded the Bank of London. It lasted only until August 1887, when it failed. The Libro Financial Group was founded in London 1951 and is the second largest credit union in Ontario and employs over 600 people. VersaBank is also headquartered in the city.\nIn the late 19th century, London became a major centre for the trust and loan industry. Companies founded in London included:\nIn 1899, Huron &amp; Erie purchased Canada Trust, which was founded in Calgary in 1894, and moved it to London. Canada Trust would go on to become one of the country's largest trust companies.\nOne of London's most iconic companies was London Life, which was founded in 1874. In 1997, the Power Corporation of Canada acquired control of London Life, and in 2020, London Life and Great-West Life merged into Canada Life. Other London insurance companies include Northern Life (1894\u20131986) and the London-Canada Insurance Company (1859\u20131987).\nBrewing.\nTwo of Canada's largest breweries were founded in London. The Carling Brewery was founded in 1840 by Thomas Carling, and the Labatt Brewing Company was founded in 1847 by John Kinder Labatt.\nIndustrials.\nThe headquarters of the Canadian division of 3M are in London. General Dynamics Land Systems (GDLS) builds armoured personnel carriers in the city. GDLS has a 14-year $15-billion deal to supply light armored vehicles and employs over 2,400 people. McCormick Canada, formerly Club House Foods, was founded in 1883 and currently employs more than 600 Londoners. A portion of the city's population work in factories outside of the city limits, including the General Motors automotive plant CAMI, and a Toyota plant in Woodstock. A Ford plant in Talbotville became one of the casualties of the economic crisis in 2011, the site will soon be home to a major Amazon distribution center employing 2,000 workers by 2023.\nLondon's city centre mall was first opened in 1960 as Wellington Square with of leasable area, with Eaton's and Woolworths as anchors. From 1986 to 1989, Campeau expanded Wellington Square into Galleria London with of leasable area and 200 stores including The Bay and Eaton's. However, the early 1990s recession, following by the bankruptcy of Eaton's in 1999 and then the departure of The Bay in 2000 resulted in only 20 stores left by 2001. Galleria London then began seeking non-retail tenants, becoming the home for London's central library branch, and satellite campuses for both Fanshawe College and Western University. The complex was purchased and renamed to Citi Plaza by Citigroup in 2009. Citi Plaza has been redeveloped as a mixed use complex that blends retail, office, businesses, and education providers. Alongside Citi Cards Canada's offices, in November 2016, CBC announced plans to move its expanded operations into the building.\nThe confectionary company O-Pee-Chee was founded in London in 1911.\nThere are many large Real Estate Development firms based in London which are active across Southwestern Ontario. These include Sifton Properties, Drewlo Holdings, Old Oak Properties, Tricar Developments, York Developments, Farhi Holdings and Westdell Developments. Combined, they own or operate over 300 million square feet of commercial and residential real estate.\nOn 11 December 2009, Minister of State Gary Goodyear announced a new $11-million cargo terminal at the London International Airport.\nCulture.\nFilm production.\nIn 2021, the city established \"FilmLondon\" through the London Economic Development Corporation in order to attract film and television productions to the city as an alternative to filming in the Greater Toronto Area. Notable productions that have resulted from this effort include \"The Amazing Race Canada 8\" and \"The Changeling\". Notable actors born in London include Ryan Gosling, Rachel McAdams, Victor Garber, Hume Cronyn, Michael McManus, and director Paul Haggis.\nFestivals.\nThe city is home to many festivals including \"SunFest\", the London Fringe Theatre Festival, the Forest City Film Festival, the London Ontario Live Arts Festival \"(LOLA)\", the Home County Folk Festival, Rock the Park London, Western Fair, Pride London, and others. The London Rib Fest is the second largest barbecue rib festival in North America. SunFest, a world music festival, is the second largest in Canada after Toronto Caribbean Carnival \"(Caribana)\" and is among the top 100 summer destinations in North America.\nMusic.\nLondon has a rich musical history. Guy Lombardo, the internationally acclaimed Big-Band leader, was born in London, as was jazz musician Rob McConnell, country music legend Tommy Hunter, singer-songwriter Meaghan Smith, the heavy metal band Kittie, film composer Trevor Morris, and DJ duo Loud Luxury; it is also the adopted hometown of hip-hop artist Shad Kabango, rock-music producer Jack Richardson, and 1960s folk-funk band Motherlode.\nAmerican country-music icon Johnny Cash proposed to his wife June Carter Cash onstage at the London Gardens\u2014site of the famous 26 April 1965, fifteen-minute Rolling Stones concert\u2014during his 22 February 1968 performance in the city (the hometown of his manager Saul Holiff).\nAvant-garde noise-pioneers The Nihilist Spasm Band formed in downtown London in 1965. Between 1966 and 1972, the group held a Monday night residency at the York Hotel in the city's core, which established it as a popular venue for emerging musicians and artists; known as Call the Office, the venue served as a hotbed for punk music in the late 1970s and 1980s and hosted college rock bands and weekly alternative-music nights until closing indefinitely in 2020.\nIn 2003, CHRW-FM developed The London Music Archives, an online music database that chronicled every album recorded in London between 1966 and 2006, and in 2019 the CBC released a documentary entitled \"London Calling\" which outlined \"The Secret Musical History of London Ontario\" (including its importance for the massively popular electronic-music duo Richie Hawtin and John Acquaviva). London also had (and still has, in an unofficial capacity) a professional symphony orchestra \u2013 Orchestra London \u2013 which was founded in 1937; although the organization filed for bankruptcy in 2015, members of the orchestra continue to play self-produced concerts under the moniker London Symphonia. In addition, the city is home to the London Community Orchestra, the London Youth Symphony, and the Amabile Choirs of London, Canada.\nThe Juno Awards of 2019 were hosted in London in March 2019, hosted by singer-songwriter Sarah McLachlan. In 2021, London was named Canada's first City of Music, by UNESCO. The labor union representing entertainment venue workers in London is IATSE Local 105.\nArt.\nLondon artists Jack Chambers and Greg Curnoe co-founded The Forest City Gallery in 1973 and the Canadian Artists' Representation society in 1968. Museum London, the city's central Art Gallery, was established in 1940 (initially operated from the London Public Library, until 1980, when Canadian architect Raymond Moriyama was commissioned to design its current home at the forks of the Thames River). London is also home to the Museum of Ontario Archaeology, owned and operated by Western University; it is Canada's only ongoing excavation and partial reconstruction of a prehistoric village\u2014in this case, a Neutral Nation village. The Royal Canadian Regiment Museum is a military museum at Wolseley Barracks (a Canadian former Forces Base in the city's Carling neighbourhood). The Secrets of Radar Museum was opened at Parkwood Hospital in 2003, and tells the story of the more than 6,000 Canadian World War II veterans who were recruited into a top-secret project during World War II involving radar. The London Regional Children's Museum in South London provides hands-on learning experiences for children and was one of the first children's museums established in Canada. The Canadian Medical Hall of Fame has its headquarters in downtown London and features a medical history museum.\nEldon House is the former residence of the prominent Harris Family and oldest surviving such building in London. The entire property was donated to the city of London in 1959 and is now a heritage site. An Ontario Historical Plaque was erected by the province to commemorate The Eldon House's role in Ontario's heritage.\nIn addition to Museum London and The Forest City Gallery, London is also home to a number of other galleries and art spaces, including the McIntosh Gallery at Western University, TAP Centre for Creativity, and various smaller galleries such as the Thielsen Gallery, the Westland Gallery, the Michael Gibson Gallery, the Jonathon Bancroft-Snell Gallery, The Art Exchange, Strand Fine Art and others. London also hosts an annual Nuit Blanche every June.\nTheatre.\nLondon is home to the Grand Theatre, a professional proscenium arch theatre in Central London. The building underwent renovations in 1975 to restore the stage proscenium arch and to add a secondary performance space. The architectural firm responsible for the redesign was awarded a Governor General's award in 1978 for their work on the venue. In addition to professional productions, the Grand Theatre also hosts the High School Project, a program unique to North America that provides high school students an opportunity to work with professional directors, choreographers, musical directors, and stage managers. The Palace Theatre, in Old East Village, originally opened as a silent movie theatre in 1929 and was converted to a live theatre venue in 1991. It is currently the home of the London Community Players, and as of 2016 is undergoing extensive historical restoration. The Original Kids Theatre Company, a nonprofit charitable youth organisation, currently puts on productions at the Spriet Family Theatre in the Covent Garden Market.\nLiterature.\nLondon serves as a core setting in Southern Ontario Gothic literature, most notably in the works of James Reaney. The psychologist Richard Maurice Bucke, author of \"Cosmic Consciousness: A Study in the Evolution of the Human Mind\" and Walt Whitman's literary executor, lived and worked in London, where he was often visited by Whitman (the Maurice Bucke Archive are part of the Special Collections in The Weldon Library of Western University). Modern writers from this city include fantasy-fiction authors R. Scott Bakker and Kelley Armstrong, Man Booker Prize winner Eleanor Catton, Scotiabank Giller Prize winner Bonnie Burnard and distinguished nominee Joan Barfoot. Emma Donoghue, whose 2010 novel, \"Room\", was adapted into a 2015 Academy Award-winning film of the same name, also lives in London. WordFest is an annual literary and creative arts festival that takes place each November.\nLivability.\nIn 2020 and 2021, house prices rose significantly across Canada. The average price of a home in Canada in March 2021 was $716,828, a 31.6% year-over-year increase. Meanwhile, the average cost to purchase a home in London was $607,000 in January 2021; since then increasing to $641,072 in June 2021 according to https://. As the COVID-19 pandemic has begun to decrease in severity, the housing market in London is showing signs of a cool-down according to some realtors. In April 2021, the Bank of Canada reported that the primary reason house prices had increased to such an unprecedented extent was due to housing inventory reaching record lows.\nNevertheless, the city's cost of living remains lower than many other southern Ontario cities. London is known for being a medium-sized city with big city amenities, having over 422,000 residents as of the 2021 census yet having all of the services one could find in a large city, including two large-scale shopping malls, Masonville Place and White Oaks Mall, regional health care centres, the London International Airport, Boler Mountain skiing center and post secondary education hubs such as the University of Western Ontario and Fanshawe College. In mid-2021, London had an 8.75% cheaper cost of living, and 27.5% cheaper cost of rent, compared to nearby Toronto.\nLondon has nine major parks and gardens throughout the city, many of which run along the Thames River and are interconnected by a series of pedestrian and bike paths, known as the Thames Valley Parkway. This path system is in length, and connects to an additional of bike and hiking trails throughout the city. The city's largest park, Springbank Park, is and contains of trails. It is also home to Storybook Gardens, a family attraction open year-round.\nThe city includes many pedestrian walkways throughout its neighbourhoods. Newer settled areas in the northwest end of the city include long pathways between housing developments and tall grass bordering Snake Creek, a thin waterway connected to the Thames River. These walkways connect the neighbourhoods of Fox Hollow, White Hills, Sherwood Forest and the western portion of Masonville, also running through parts of Medway Valley Heritage Forest.\nSports.\nLondon is the home of the London Knights of the Ontario Hockey League, who play at the Canada Life Place. The Knights are 2004\u20132005, 2015\u20132016 and 2024-2025 OHL and Memorial Cup Champions. During the summer months, the London Majors of the Intercounty Baseball League play at Labatt Park. FC London of League1 Ontario and founded in 2008 is the highest level of soccer in London. The squad plays at German Canadian Club of London Field. Other sports teams include the London Silver Dolphins Swim Team, the Forest City Volleyball Club, London Cricket Club, the London St. George's Rugby Club, the London Aquatics Club, the London Rhythmic Gymnastics Club, the London Rowing Club, London City Soccer Club and Forest City London.\nThe Eager Beaver Baseball Association (EBBA) is a baseball league for youths in London. It was first organized in 1955 by former Major League Baseball player Frank Colman, and London sportsman Gordon Berryhill.\nFootball teams include the London Beefeaters (Ontario Football Conference).\nLondon's basketball team, the London Lightning plays at Canada Life Place. Originally members of the National Basketball League of Canada until the league folded in 2023, they are now members of the Basketball Super League. As members of the NBLC, the London Lightning became six time NBL Canada champions. \nThere are also a number of former sports teams that have moved or folded. London's four former baseball teams are the London Monarchs (Canadian Baseball League), the London Werewolves (Frontier League), the London Tecumsehs (International Association) and the London Tigers (AA Eastern League). Other former sports teams include the London Lasers (Canadian Soccer League)\nIn March 2013, London hosted the 2013 World Figure Skating Championships. The University of Western Ontario's teams play under the name \"Mustangs\". The university's football team plays at TD Stadium. Western's Rowing Team rows out of a boathouse at Fanshawe Lake. Fanshawe College teams play under the name \"Falcons\". The Women's Cross Country team has won 3 consecutive Canadian Collegiate Athletic Association (CCAA) National Championships. In 2010, the program cemented itself as the first CCAA program to win both Men's and Women's National team titles, as well as CCAA Coach of the Year.\nThe Western Fair Raceway, about 85 acres harness racing track and simulcast centre, operates year-round. The grounds include a coin slot casino, a former IMAX theatre, and Sports and Agri-complex. Labatt Memorial Park the world's oldest continuously used baseball grounds was established as Tecumseh Park in 1877; it was renamed in 1937, because the London field has been flooded and rebuilt twice (1883 and 1937), including a re-orientation of the bases (after the 1883 flood). The Forest City Velodrome, at the former London Ice House, is the only indoor cycling track in Ontario and the third to be built in North America, opened in 2005. London is also home to World Seikido, the governing body of a martial art called Seikido which was developed in London in 1987.\nGovernment and law.\nLondon's municipal government is divided among fourteen councillors (one representing each of London's fourteen wards) and the mayor. Josh Morgan was elected mayor in the 2022 municipal election. Until the elections in 2010, there was a Board of Control, consisting of four controllers and the mayor, all elected citywide.\nAlthough London has many ties to Middlesex County, it has been a separate entity since 1855. The exception is the Middlesex County Courthouse and former jail, as the judiciary is administered directly by the province.\nLondon was the first city in Canada (in May 2017) to decide to move a ranked choice ballot for municipal elections starting in 2018. Voters mark their ballots in order of preference, ranking their top three favourite candidates. An individual must reach 50 per cent of the total to be declared elected; in each round of counting where a candidate has not yet reached that target, the person with the fewest votes is dropped from the ballot and their second or third choice preferences reallocated to the remaining candidates, with this process repeating until a candidate has reached 50 per cent.\nIn 2001, the City of London first published their Facilities Accessibility Design Standards (FADS) which was one of the first North American municipal accessibility requirements to include Universal Design. It has since been adopted by over 50 municipalities in Canada and the United States.\nCity councillors.\nIn addition to mayor Josh Morgan, the following councillors were elected in the 2022 municipal election for the 2022\u20132026 term:\nProvincial ridings.\nThe city includes four provincial ridings. In the provincial government, London is represented by New Democrats Terence Kernaghan (London North Centre), Teresa Armstrong (London\u2014Fanshawe) and Peggy Sattler (London West), and Progressive Conservative Rob Flack (Elgin\u2014Middlesex\u2014London).\nFederal ridings.\nThe London and surrounding area includes four federal ridings. In the federal government, London is represented by Conservative Karen Vecchio (Elgin\u2014Middlesex\u2014London), Liberals Peter Fragiskatos (London North Centre) and Arielle Kayabaga (London West), and New Democrat Lindsay Mathyssen (London\u2014Fanshawe).\nLaw enforcement and crime.\nLaw enforcement.\nAs of 2023[ [update]] the London Police Service (LPS) is headed by Chief of Police Thai Truong. He is supported by two deputy chiefs: Paul Bastien, in charge of operations, and Trish McIntyre, in charge of administration. The service is governed by a seven-member civilian police board, of which the current board chair is Ali Chabar, General Legal Counsel and Executive Officer with the Thames Valley District School Board c. As of December 2020, the LPS had the fewest police officers per capita in Southwestern Ontario. Its vehicles include light armoured vehicles donated by General Dynamics Land Systems, which the CBC observed in 2019 were rarely used.\nCrime.\nStatistics from police indicate that total overall crimes in London held steady between 2010 and 2016, at roughly 24,000 to 27,000 incidents per year. The majority of incidents are property crimes, with violent crimes dropping markedly (up to about 20%) between 2012 and 2014 but rising again in 2015\u20132016. In July 2018, Police Deputy Chief Steve Williams was quoted as saying many crimes go unreported to police. However, in 2021, the city surpassed its 2005 homicide record, with the city reporting 16 murders with a rate of 3.8 per 100,000 people.\nThe city has been home to several high-profile incidents over the years such as the Ontario Biker War and the London Conflict, it was also the location where most of the trial for the Shedden Massacre took place.\nResearch by Michael Andrew Arntfield, a police officer turned criminology professor, has determined that on a per-capita basis, London had more active serial killers than any locale in the world from 1959 to 1984. Arntfield determined there were at least six serial killers active in London during this era. Some went unidentified, but known killers in London included Russell Maurice Johnson, Gerald Thomas Archer, and Christian Magee.\nOn 6 June 2021, the London, Ontario truck attack took place in the North West of the city. Four members of a Canadian Muslim family, two women aged 74 and 44, a 46-year-old man and a 15-year-old girl were all killed by a pickup truck, which jumped the curb and ran them over. The sole survivor was a 9-year-old boy. According to the London Police Service, they were deliberately targeted in anti-Islamic hate crime. Later on the same day, 20-year-old Nathaniel Veltman was arrested in the parking lot of a nearby mall. He was charged with four counts of first-degree murder and one count of attempted murder.\nIn September 2023, the trial for the accused began in Windsor, ON. This was the first time Canadian jurors heard legal arguments for terrorism related to white supremacy. Shortly after the attack, the accused told police, \"I admit it was terrorism...I was a ticking bomb, ready to go off.\" He also admitted that his hate towards minority groups began with looking for information online about Donald Trump's election for U.S. president.\nIn February 2024 Veltman was sentenced to five life sentences with no possibility of parole for 25 years.\nCivic initiatives.\nThe City of London initiatives in Old East London are helping to create a renewed sense of vigour in the East London Business District. Specific initiatives include the creation of the Old East Heritage Conservation District under Part V of the \"Ontario Heritage Act\", special Building Code policies and Facade Restoration Programs.\nLondon is home to heritage properties representing a variety of architectural styles, including Queen Anne, Art Deco, Modern, and Brutalist.\nLondoners have become protective of the trees in the city, protesting \"unnecessary\" removal of trees. The City Council and tourist industry have created projects to replant trees throughout the city. As well, they have begun to erect metal trees of various colours in the downtown area, causing some controversy.\nTransportation.\nRoad transportation.\nLondon is at the junction of Highway 401 that connects the city to Toronto and Windsor, and Highway 402 to Sarnia. Also, Highway 403, which diverges from the 401 at nearby Woodstock, provides ready access to Brantford, Hamilton, and the Niagara Peninsula. Many smaller two-lane highways also pass through or near London, including Kings Highways 2, 3, 4, 7 and 22. Some of these are no longer highways, as provincial downloading in the 1980s and 1990s put responsibility for most provincial highways on municipal governments. Nevertheless, these roads continue to provide access from London to nearby communities and locations in much of Western Ontario, including Goderich, Port Stanley and Owen Sound. A 4.5\u00a0km long section of Highbury Ave., connecting the east end of London to Highway 401, consists of an controlled-access highway with 100\u00a0km/h speed limits.\nWellington Road between Commissioners Road East and Southdale Road E is London's busiest section of roadway, with more than 46,000 vehicles using the span on an average day London does not have any freeways passing directly through the city. City council rejected early plans for the construction of a freeway, and instead accepted the Veterans Memorial Parkway to serve the east end. Some Londoners have expressed concern the absence of a local freeway may hinder London's economic and population growth, while others have voiced concern such a freeway would destroy environmentally sensitive areas and contribute to London's suburban sprawl. Road capacity improvements have been made to Veterans Memorial Parkway (formerly named Airport Road and Highway 100) in the industrialized east end. However, the Parkway has received criticism for not being built as a proper highway; a study conducted in 2007 suggested upgrading it by replacing the intersections with interchanges.\nPublic transit.\nIn the late 19th century, and the early 20th century, an extensive network of streetcar routes served London.\nLondon's public transit system is run by the London Transit Commission, which has 44 bus routes throughout the city. Although the city has lost ridership over the last few years, the commission is making concerted efforts to enhance services by implementing a five-year improvement plan. In 2015, an additional 17,000 hours of bus service was added throughout the city. In 2016, 11 new operators, 5 new buses, and another 17,000 hours of bus service were added to the network. London has started construction of a bus rapid transit network. Construction of this network was initially anticipated to begin in 2019, but after delays, changes to the design, construction started on the first BRT project, the Downtown Loop, in spring 2021 and will continue in phases until 2030. The project received C$170 million in funding from the Ontario government on 15 January 2018.\nCycling network.\nLondon has of cycling paths throughout the city, of which have been added since 2005. In June 2016, London unveiled its first bike corrals, which replace parking for one vehicle with fourteen bicycle parking spaces, and fix-it stations, which provide cyclists with simple tools and a bicycle pump, throughout the city. In September 2016, city council approved a new 15 year cycling master plan that will see the construction of an additional of cycling paths added to the existing network.\nIntercity transport.\nLondon is on the Canadian National Railway main line between Toronto and Chicago (with a secondary main line to Windsor) and the Canadian Pacific Railway main line between Toronto and Detroit. Via Rail operates regional passenger service through London station as part of the Quebec City\u2013Windsor Corridor, with connections to the United States. Via Rail's London terminal is the fourth-busiest passenger terminal in Canada. In October 2021 GO Transit began a two-year pilot project providing commuter rail service between London and Toronto along the Kitchener line. The pilot project ended in October 2023.\nLondon is also a destination for inter-city bus travellers. In 2009, London was the seventh-busiest Greyhound Canada terminal in terms of passengers. Greyhound Canada no longer operates, but other operators have entered the market, including Megabus and FlixBus that provide service throughout southwestern Ontario.\nLondon International Airport (YXU) is the 12th busiest passenger airport in Canada and the 11th busiest airport in Canada by take-offs and landings. It is served by airlines including Air Canada Express, and WestJet, and provides direct flights to both domestic and international destinations, including Toronto, Orlando, Ottawa, Winnipeg, Calgary, Canc\u00fan, Vancouver, Varadero, Punta Cana, Montego Bay, Santa Clara, and Holguin.\nPlans.\nAdditional cycleways are planned for integration in road-widening projects, where there is need and sufficient space along routes. An expressway/freeway network is possible along the eastern and western ends of the city, from Highway 401 (and Highway 402 for the western route) past Oxford Street, potentially with another highway, joining the two in the city's north end.\nThe city of London has assessed the entire length of the Veterans Memorial Parkway, identifying areas where interchanges can be constructed, grade separations can occur, and where cul-de-sacs can be placed. Upon completion, the Veterans Memorial Parkway would no longer be an expressway, but a freeway, for the majority of its length.\nEducation.\nLondon public elementary and secondary schools are governed by four school boards \u2013 the Thames Valley District School Board, the London District Catholic School Board and the French first-language school boards (the \"Conseil scolaire Viamonde\" and the \"Conseil scolaire catholique Providence\" or CSC). The CSC has a satellite office in London.\nThere are also more than twenty private schools in the city.\nThe city is home to two post-secondary institutions: the University of Western Ontario (UWO) and Fanshawe College, a college of applied arts and technology. UWO, founded in 1878, has about 3,500 full-time faculty and staff members and almost 30,000 undergraduate and graduate students. The Richard Ivey School of Business, part of UWO, was formed in 1922. UWO has two affiliated colleges: Huron University College, founded in 1863 (also the founding college of UWO) and King's University College, founded in 1954. As well as one former affiliated college; Brescia University College, founded in 1919 (Canada's only university-level women's college). All three are liberal arts colleges with religious affiliations: Huron with the Anglican Church of Canada and King's and Brescia with the Roman Catholic Church.\nFanshawe College has an enrollment of approximately 15,000 students, including 3,500 apprentices and over 500 international students from more than 30 countries. It also has almost 40,000 students in part-time continuing education courses.\nThe Ontario Institute of Audio Recording Technology (OIART), founded in 1983, offers recording studio experience for audio engineering students.\nWestervelt College is also in London. This private career college was founded in 1885 and offers several diploma programs.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "40355", "revid": "50937102", "url": "https://en.wikipedia.org/wiki?curid=40355", "title": "Optimizing compiler", "text": "Compiler that optimizes generated code\nAn optimizing compiler is a compiler designed to generate code that is optimized in aspects such as minimizing program execution time, memory usage, storage size, and power consumption. Optimization is generally implemented as a sequence of optimizing transformations, a.k.a. compiler optimizations \u2013 algorithms that transform code to produce semantically equivalent code optimized for some aspect.\nOptimization is limited by a number of factors. Theoretical analysis indicates that some optimization problems are NP-complete, or even undecidable. Also, producing perfectly \"optimal\" code is not possible since optimizing for one aspect often degrades performance for another (see: superoptimization). Optimization is a collection of heuristic methods for improving resource usage in typical programs.585\nCategorization.\nLocal vs. global scope.\nScope describes how much of the input code is considered to apply optimizations.\nLocal scope optimizations use information local to a basic block. Since basic blocks contain no control flow statements, these optimizations require minimal analysis, reducing time and storage requirements. However, no information is retained across jumps.\nGlobal scope optimizations, also known as intra-procedural optimizations, operate on individual functions. This gives them more information to work with but often makes expensive computations necessary. Worst-case assumptions need to be made when function calls occur or global variables are accessed because little information about them is available.\nPeephole optimization.\nPeephole optimizations are usually performed late in the compilation process after machine code has been generated. This optimization examines a few adjacent instructions (similar to \"looking through a peephole\" at the code) to see whether they can be replaced by a single instruction or a shorter sequence of instructions.554 For instance, a multiplication of a value by two might be more efficiently executed by left-shifting the value or by adding the value to itself (this example is also an instance of strength reduction).\nInter-procedural optimization.\nInterprocedural optimizations analyze all of a program's source code. The more information available, the more effective the optimizations can be. The information can be used for various optimizations, including function inlining, where a call to a function is replaced by a copy of the function body.\nLink-time optimization.\nLink-time optimization (LTO), or whole-program optimization, is a more general class of interprocedural optimization. During LTO, the compiler has visibility across translation units which allows it to perform more aggressive optimizations like cross-module inlining and devirtualization.\nMachine and object code optimization.\nMachine code optimization involves using an object code optimizer to analyze the program after all machine code has been linked. Techniques such as macro compression, which conserves space by condensing common instruction sequences, become more effective when the entire executable task image is available for analysis.\nLanguage-independent vs. language-dependent.\nMost high-level programming languages share common programming constructs and abstractions, such as branching constructs (if, switch), looping constructs (for, while), and encapsulation constructs (structures, objects). Thus, similar optimization techniques can be used across languages. However, certain language features make some optimizations difficult. For instance, pointers in C and C++ make array optimization difficult; see alias analysis. However, languages such as PL/I that also support pointers implement optimizations for arrays. Conversely, some language features make certain optimizations easier. For example, in some languages, functions are not permitted to have side effects. Therefore, if a program makes several calls to the same function with the same arguments, the compiler can infer that the function's result only needs to be computed once. In languages where functions are allowed to have side effects, the compiler can restrict such optimization to functions that it can determine have no side effects.\nMachine-independent vs. machine-dependent.\nMany optimizations that operate on abstract programming concepts (loops, objects, structures) are independent of the machine targeted by the compiler, but many of the most effective optimizations are those that best exploit special features of the target platform. Examples are instructions that do several things at once, such as decrement register and branch if not zero.\nThe following is an instance of a local machine-dependent optimization. To set a register to 0, the obvious way is to use the constant '0' in an instruction that sets a register value to a constant. A less obvious way is to XOR a register with itself or subtract it from itself. It is up to the compiler to know which instruction variant to use. On many RISC machines, both instructions would be equally appropriate, since they would both be the same length and take the same time. On many other microprocessors such as the Intel x86 family, it turns out that the XOR variant is shorter and probably faster, as there will be no need to decode an immediate operand, nor use the internal \"immediate operand register\"; the same applies on IBM System/360 and successors for the subtract variant. A potential problem with this is that XOR or subtract may introduce a data dependency on the previous value of the register, causing a pipeline stall, which occurs when the processor must delay execution of an instruction because it depends on the result of a previous instruction. However, processors often treat the XOR of a register with itself or the subtract of a register from itself as a special case that does not cause stalls.\nNotable cases include code designed for parallel and vector processors, for which special parallelizing compilers are used.\nFirmware for an embedded system can be optimized for the target CPU and memory. System cost or reliability may be more important than the code speed. For example, compilers for embedded software usually offer options that reduce code size at the expense of speed. The code's timing may need to be predictable, rather than as fast as possible, so code caching might be disabled, along with compiler optimizations that require it.\nCommon themes.\nOptimization includes the following, sometimes conflicting themes.\nSpecific techniques.\nLoop optimizations.\n\"Loop optimization\" acts on the statements that make up a loop, such as a \"for\" loop, for example loop-invariant code motion. Loop optimizations can have a significant impact because many programs spend a large percentage of their time inside loops.596\nSome optimization techniques primarily designed to operate on loops include:\nPrescient store optimizations.\nPrescient store optimizations allow store operations to occur earlier than would otherwise be permitted in the context of threads and locks. The process needs some way of knowing ahead of time what value will be stored by the assignment that it should have followed. The purpose of this relaxation is to allow compiler optimization to perform certain kinds of code rearrangements that preserve the semantics of properly synchronized programs.\nData-flow optimizations.\nData-flow optimizations, based on data-flow analysis, primarily depend on how certain properties of data are propagated by control edges in the control-flow graph. Some of these include:\nSSA-based optimizations.\nThese optimizations are intended to be done after transforming the program into a special form called static single-assignment form, in which every variable is assigned in only one place. Although some function without SSA, they are most effective with SSA. Many optimizations listed in other sections also benefit with no special changes, such as register allocation.\nFunctional language optimizations.\nAlthough many of these also apply to non-functional languages, they either originate in or are particularly critical in functional languages such as Lisp and ML.\nFor example:\nOther optimizations.\nif (cond) {\n foo();\nif (cond) {\n bar();\n// becomes:\nif (cond) {\n foo();\n bar();\nand:\nif (cond) {\n foo();\nif (!cond) {\n bar();\n// becomes:\nif (cond) {\n foo();\n} else {\n bar();\nInterprocedural optimizations.\nInterprocedural optimization works on the entire program, across procedure and file boundaries. It works tightly with intraprocedural counterparts, carried out with the cooperation of a local part and a global part. Typical interprocedural optimizations are procedure inlining, interprocedural dead-code elimination, interprocedural constant propagation, and procedure reordering. As usual, the compiler needs to perform interprocedural analysis before its actual optimizations. Interprocedural analyses include alias analysis, array access analysis, and the construction of a call graph.\nInterprocedural optimization is common in modern commercial compilers from SGI, Intel, Microsoft, and Sun Microsystems. For a long time, the open source GCC was criticized for a lack of powerful interprocedural analysis and optimizations, though this is now improving. Another open-source compiler with full analysis and optimization infrastructure is Open64.\nDue to the extra time and space required by interprocedural analysis, most compilers do not perform it by default. Users must use compiler options explicitly to tell the compiler to enable interprocedural analysis and other expensive optimizations.\nPractical considerations.\nThere can be a wide range of optimizations that a compiler can perform, ranging from simple and straightforward optimizations that take little compilation time to elaborate and complex optimizations that involve considerable amounts of compilation time.15 Accordingly, compilers often provide options to their control command or procedure to allow the compiler user to choose how much optimization to request; for instance, the IBM FORTRAN H compiler allowed the user to specify no optimization, optimization at the registers level only, or full optimization.737 By the 2000s, it was common for compilers, such as Clang, to have several compiler command options that could affect a variety of optimization choices, starting with the familiar codice_8 switch.\nAn approach to isolating optimization is the use of so-called post-pass optimizers (some commercial versions of which date back to mainframe software of the late 1970s). These tools take the executable output by an optimizing compiler and optimize it even further. Post-pass optimizers usually work on the assembly language or machine code level (in contrast with compilers that optimize intermediate representations of programs). One such example is the Portable C Compiler (PCC) of the 1980s, which had an optional pass that would perform post-optimizations on the generated assembly code.736\nAnother consideration is that optimization algorithms are complicated and, especially when being used to compile large, complex programming languages, can contain bugs that introduce errors in the generated code or cause internal errors during compilation. Compiler errors of any kind can be disconcerting to the user, but especially so in this case, since it may not be clear that the optimization logic is at fault. In the case of internal errors, the problem can be partially ameliorated by a \"fail-safe\" programming technique in which the optimization logic in the compiler is coded such that a failure is trapped, a warning message issued, and the rest of the compilation proceeds to successful completion.\nHistory.\nEarly compilers of the 1960s were often primarily concerned with simply compiling code correctly or efficiently, such that compile times were a major concern. One notable early optimizing compiler was the IBM FORTRAN H compiler of the late 1960s.737 Another of the earliest and important optimizing compilers, that pioneered several advanced techniques, was that for BLISS (1970), which was described in \"The Design of an Optimizing Compiler\" (1975). By the late 1980s, optimizing compilers were sufficiently effective that programming in assembly language declined. This co-evolved with the development of RISC chips and advanced processor features such as superscalar processors, out-of-order execution, and speculative execution, which were designed to be targeted by optimizing compilers rather than by human-written assembly code.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "40359", "revid": "1300790409", "url": "https://en.wikipedia.org/wiki?curid=40359", "title": "Due process", "text": "Requirement that courts respect all legal rights owed to people\nDue process of law is application by the state of all legal rules and principles pertaining to a case so all legal rights that are owed to a person are respected. Due process balances the power of law of the land and protects the individual person from it. When a government harms a person without following the exact course of the law, this constitutes a due process violation, which offends the rule of law.\nDue process has also been frequently interpreted as limiting laws and legal proceedings (see substantive due process) so that judges, instead of legislators, may define and guarantee fundamental fairness, justice, and liberty. That interpretation has proven controversial. Analogous to the concepts of natural justice and procedural justice used in various other jurisdictions, the interpretation of due process is sometimes expressed as a command that the government must not be unfair to the people or abuse them physically or mentally. The term is not used in contemporary English law, but two similar concepts are natural justice, which generally applies only to decisions of administrative agencies and some types of private bodies like trade unions, and the British constitutional concept of the rule of law as articulated by A. V. Dicey and others. However, neither concept lines up perfectly with the American theory of due process, which, as explained below, presently contains many implied rights not found in either ancient or modern concepts of due process in England.\nDue process developed from clause 39 of Magna Carta in England. Reference to due process first appeared in a statutory rendition of clause 39 in 1354 thus: \"No man of what state or condition he be, shall be put out of his lands or tenements nor taken, nor disinherited, nor put to death, without he be brought to answer by due process of law.\" When English and American law gradually diverged, due process remained in force in England and became incorporated in the US Constitution.\nBy jurisdiction.\nMagna Carta.\nIn clause 39 of Magna Carta, issued in 1215, John of England promised: \"No free man shall be seized or imprisoned, or stripped of his rights or possessions, or outlawed or exiled, or deprived of his standing in any other way, nor will we proceed with force against him, or send others to do so, except by the lawful judgment of his equals or by the law of the land.\" Magna Carta itself immediately became part of the \"law of the land\", and Clause 61 of that charter authorized an elected body of 25 barons to determine by majority vote what redress the King must provide when the King offends \"in any respect against any man\". Thus, Magna Carta established the rule of law in England by not only requiring the monarchy to obey the law of the land but also limiting how the monarchy could change the law of the land. However, in the 13th century, the provisions may have been referring only to the rights of landowners, and not to ordinary peasantry or villagers.\nShorter versions of Magna Carta were subsequently issued by British monarchs, and Clause 39 of Magna Carta was renumbered \"29\". The phrase \"due process of law\" first appeared in a statutory rendition of Magna Carta in 1354 during the reign of Edward III of England, as follows: \"No man of what state or condition he be, shall be put out of his lands or tenements nor taken, nor disinherited, nor put to death, without he be brought to answer by due process of law.\"\nIn 1608, the English jurist Edward Coke wrote a treatise in which he discussed the meaning of Magna Carta. Coke explained that no man shall be deprived but by \"legem terrae\", the law of the land, \"that is, by the common law, statute law, or custom of England... (that is, to speak it once and for all) by the due course, and process of law..\"\nBoth the clause in Magna Carta and the later statute of 1354 were again explained in 1704 (during the reign of Queen Anne) by the Queen's Bench, in the case of \"Regina v. Paty\". In that case, the British House of Commons had deprived John Paty and certain other citizens of the right to vote in an election and committed them to Newgate Prison merely for the offense of pursuing a legal action in the courts. The Queen's Bench, in an opinion by Justice Littleton Powys, explained the meaning of \"due process of law\" as follows:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;\nChief Justice Holt dissented in this case because he believed that the commitment had not in fact been by a legal authority. The House of Commons had purported to legislate unilaterally, without approval of the British House of Lords, ostensibly to regulate the election of its members. Although the Queen's Bench held that the House of Commons had not infringed or overturned due process, John Paty was ultimately freed by Queen Anne when she prorogued Parliament.\nEnglish law and American law diverge.\nThroughout centuries of British history, many laws and treatises asserted various requirements as being part of \"due process\" or included in the \"law of the land\". That view usually held in regards to what was required by existing law, rather than what was intrinsically required by due process itself. As the United States Supreme Court has explained, a due process requirement in Britain was not \"essential to the idea of due process of law in the prosecution and punishment of crimes, but was only mentioned as an example and illustration of due process of law as it actually existed in cases in which it was customarily used\".\nUltimately, the scattered references to \"due process of law\" in English law did not limit the power of the government; in the words of American law professor John V. Orth, \"the great phrases failed to retain their vitality.\" Orth points out that this is generally attributed to the rise of the doctrine of parliamentary supremacy in the United Kingdom, which was accompanied by hostility towards judicial review as an undemocratic foreign invention.\nScholars have occasionally interpreted Lord Coke's ruling in \"Dr. Bonham's Case\" as implying the possibility of judicial review, but by the 1870s, Lord Campbell was dismissing judicial review as \"a foolish doctrine alleged to have been laid down extra-judicially in Dr. Bonham's Case..., a conundrum [that] ought to have been laughed at\". Lacking the power of judicial review of primary legislation, English courts possessed no means by which to declare an Act of Parliament invalid as a violation of due process. In contrast, American legislators and executive branch officers possessed virtually no means by which to overrule judicial invalidation of statutes or actions as due process violations, with the sole exception of proposing a constitutional amendment, which are rarely successful. As a consequence, English law and American law diverged. Unlike their English counterparts, American judges became increasingly assertive about enforcing due process of law. In turn, the legislative and executive branches learned how to avoid such confrontations in the first place, by tailoring statutes and executive actions to the constitutional requirements of due process as elaborated upon by the judiciary.\nIn 1977, an English political science professor explained the present situation in England for the benefit of American lawyers:\nAn American constitutional lawyer might well be surprised by the elusiveness of references to the term 'due process of law' in the general body of English legal writing... Today one finds no space devoted to due process in Halsbury's \"Laws of England\", in Stephen's \"Commentaries\", or Anson's \"Law and Custom of the Constitution.\" The phrase rates no entry in such works as Stroud's \"Judicial Dictionary\" or Wharton's \"Law Lexicon.\"\nTwo similar concepts in contemporary English law are natural justice, which generally applies only to decisions of administrative agencies and some types of private bodies like trade unions, and the British constitutional concept of the rule of law as articulated by A. V. Dicey and others. However, neither concept lines up perfectly with the American conception of due process, which presently contains many implied rights not found in the ancient or modern concepts of due process in England.\nUnited States.\nThe Fifth and Fourteenth Amendments to the United States Constitution each contain a Due Process Clause. Due process deals with the administration of justice and thus the Due Process Clause acts as a safeguard from arbitrary denial of life, liberty, or property by the government outside the sanction of law. The Supreme Court of the United States interprets the clauses as providing four protections: procedural due process (in civil and criminal proceedings), substantive due process, a prohibition against vague laws, and as the vehicle for the incorporation of the Bill of Rights.\nOthers.\nVarious countries recognize some form of due process under customary international law. Although the specifics are often unclear, most nations agree that they should guarantee foreign visitors a basic minimum level of justice and fairness. Some nations have argued that they are bound to grant no more rights to aliens than they do to their own citizens, the doctrine of national treatment, which also means that both would be vulnerable to the same deprivations by the government. With the growth of international human rights law and the frequent use of treaties to govern treatment of foreign nationals abroad, the distinction, in practice, between these two perspectives may be disappearing.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "40360", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=40360", "title": "Scintillation Counter", "text": ""}
{"id": "40363", "revid": "7611264", "url": "https://en.wikipedia.org/wiki?curid=40363", "title": "Dosimeter", "text": "Device measuring ionizing radiation exposure\nA radiation dosimeter is a device that measures the dose uptake of external ionizing radiation. It is worn by the person being monitored when used as a personal dosimeter, and is a record of the radiation dose received. Modern electronic personal dosimeters can give a continuous readout of cumulative dose and current dose rate, and can warn the wearer with an audible alarm when a specified dose rate or a cumulative dose is exceeded. Other dosimeters, such as thermoluminescent or film types, require processing after use to reveal the cumulative dose received, and cannot give a current indication of dose while being worn.\nPersonal dosimeters.\nThe personal ionising radiation dosimeter is of fundamental importance in the disciplines of radiation dosimetry and radiation health physics and is primarily used to estimate the radiation dose deposited in an individual wearing the device.\nIonising radiation damage to the human body is cumulative and related to the total dose received, for which the SI unit is the sievert. Radiographers, nuclear power plant workers, doctors using radiotherapy, HAZMAT workers, and other people in situations that involve handling radionuclides are often required to wear dosimeters so a record of occupational exposure can be made. Such devices are known as \"legal dosimeters\" if they have been approved for use in recording personnel doses for regulatory purposes.\nDosimeters are typically worn on the outside of clothing, a \"whole body\" dosimeter is worn on the chest or torso to represent dose to the whole body. This location monitors exposure of most vital organs and represents the bulk of body mass. Additional dosimeters can be worn to assess dose to extremities or in radiation fields that vary considerably depending on orientation of the body to the source.\nElectronic personal dosimeters.\nThe electronic personal dosimeter, the most commonly used type, is an electronic device that has a number of sophisticated functions, such as continual monitoring which allows alarm warnings at preset levels and live readout of dose accumulated. These are especially useful in high dose areas where residence time of the wearer is limited due to dose constraints. The dosimeter can be reset, usually after taking a reading for record purposes, and thereby re-used multiple times.\nMOSFET dosimeter.\nMetal\u2013oxide\u2013semiconductor field-effect transistor dosimeters are now used as clinical dosimeters for radiotherapy radiation beams. The main advantages of MOSFET devices are:\n1. The MOSFET dosimeter is direct reading with a very thin active area (less than 2\u03bcm ).\n2. The physical size of the MOSFET when packaged is less than 4\u00a0mm.\n3. The post radiation signal is permanently stored and is dose rate independent.\nGate oxide of MOSFET which is conventionally silicon dioxide is an active sensing material in MOSFET dosimeters. Radiation creates defects (acts like electron-hole pairs) in oxide, which in turn affects the threshold voltage of the MOSFET. This change in threshold voltage is proportional to radiation dose. Alternate high-k gate dielectrics like hafnium dioxide and aluminum oxides are also proposed as a radiation dosimeters.\nPIN dosimeters.\nPIN diodes are often used by military personnel for measuring radiation dosage.\nScintillation counter.\nA scintillation counter detects ionizing radiation by measuring the light emitted from a scintillator, giving a measurement of radiation levels almost instantly. Like thermoluminescent crystals, scintillation materials begin to glow when exposed to radiation. Unlike thermoluminescent crystals, which store absorbed radiation to measure accumulated doses over a period of time, scintillation materials release light immediately, and do not need to be heated.\nThermoluminescent dosimeter.\nA thermoluminescent dosimeter measures ionizing radiation exposure by measuring the intensity of light emitted from a Dy or B doped crystal in the detector when heated. The intensity of light emitted is dependent upon the radiation exposure. These were once sold surplus and one format once used by submariners and nuclear workers resembled a dark green wristwatch containing the active components and a highly sensitive IR wire ended diode mounted to the doped LiF2 glass chip that when the assembly is precisely heated (hence thermoluminescent) emits the stored radiation as narrow band infrared light until it is depleted The main advantage is that the chip records dosage passively until exposed to light or heat so even a used sample kept in darkness can provide valuable scientific data.\nLegacy types of dosimeters.\nFilm badge dosimeter.\nFilm badge dosimeters are for one-time use only. The level of radiation absorption is indicated by a change to the film emulsion, which is shown when the film is developed. They are now mostly superseded by electronic personal dosimeters and thermoluminescent dosimeters.\nQuartz fiber dosimeter.\nThese use the property of a quartz fiber to measure the static electricity held on the fiber. Before use by the wearer a dosimeter is charged to a high voltage, causing the fiber to deflect due to electrostatic repulsion. As the gas in the dosimeter chamber becomes ionized by radiation the charge leaks away, causing the fiber to straighten and thereby indicate the amount of dose received against a graduated scale, which is viewed by a small in-built microscope.\nThey are only used for short durations, such as a day or a shift, as they can suffer from charge leakage, which gives a false high reading.\nHowever they are immune to EMP so were used during the Cold War as a failsafe method of determining radiation exposure.\nThey are now largely superseded by electronic personal dosimeters for short term monitoring.\nGeiger tube dosimeter.\nThese use a conventional Geiger\u2013M\u00fcller tube, typically a ZP1301 or similar energy-compensated tube, requiring between 600 and 700V and pulse detection components. The display on most is a bubble or miniature LCD type with 4 digits and a discrete counter integrated chip such as 74C925/6. LED units usually have a button to turn the display on and off for longer battery life, and an infrared emitter for count verification and calibration.\nThe voltage is derived from a separate pinned or wire-ended module that often uses a unijunction transistor driving a small step-up coil and multiplier stage. While expensive, it is reliable over time and especially in high-radiation environments, sharing this trait with tunnel diodes, though the encapsulants, inductors and capacitors have been known to break down internally over time.\nThese have the disadvantage that the stored dose in becquerels or microsieverts is volatile and vanishes if the power supply is disconnected, though there can be a low-leakage capacitor to preserve the memory for short periods without a battery. Because of this, most units use long-life batteries and high-quality contacts. Recently-designed units log dose over time to non-volatile memory, such as a 24C256 chip so it may be read out via a serial port.\nDosimetry dose quantities.\nThe operational quantity for personal dosimetry is the personal dose equivalent, which is defined by the International Commission on Radiological Protection as the dose equivalent in soft tissue at an appropriate depth, below a specified point on the human body. The specified point is usually given by the position where the individual\u2019s dosimeter is worn.\nInstrument and dosimeter response.\nThis is an actual reading obtained from such as an ambient dose gamma monitor, or a personal dosimeter. The dosimeter is calibrated in a known radiation field to ensure display of accurate operational quantities and allow a relationship to known health effect. The personal dose equivalent is used to assess dose uptake, and allow regulatory limits to be met. It is the figure usually entered into the records of external dose for occupational radiation workers.\nThe dosimeter plays an important role within the international radiation protection system developed by the International Commission on Radiological Protection and the International Commission on Radiation Units and Measurements. This is shown in the accompanying diagram.\nDosimeter calibration.\nThe \"slab\" phantom is used to represent the human torso for calibration of whole body dosimeters. This replicates the radiation scattering and absorption effects of the human torso. The International Atomic Energy Agency states \"The slab phantom is 300 mm \u00d7 300 mm \u00d7 150 mm depth to represent the human torso\".\nProcess irradiation verification.\nManufacturing processes that treat products with ionizing radiation, such as food irradiation, use dosimeters to calibrate doses deposited in the matter being irradiated. These usually must have a greater dose range than personal dosimeters, and doses are normally measured in the unit of absorbed dose: the gray (Gy). The dosimeter is located on or adjacent to the items being irradiated during the process as a validation of dose levels received.\nDosimeters as a tracking device.\nA Soviet KGB spy \"Raduga\" (Rainbow) dosimeter, radiation tracking device was used by deliberately contaminating items such as money, artworks, documents, propaganda literature etc. with a nuclear radiation signature which can then be tracked using this device which would vibrate when in proximity to the radiation. The process was utilized as a way of catching thieves and corrupt officials and locating hidden stashes.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "40364", "revid": "7903804", "url": "https://en.wikipedia.org/wiki?curid=40364", "title": "Electrometer", "text": "Instrument for measuring electric charge\nAn electrometer is an electrical instrument for measuring electric charge or electrical potential difference. There are many different types, ranging from historical handmade mechanical instruments to high-precision electronic devices. Modern electrometers based on vacuum tube or solid-state technology can be used to make voltage and charge measurements with very low leakage currents, down to 1 femtoampere. A simpler but related instrument, the electroscope, works on similar principles but only indicates the relative magnitudes of voltages or charges.\nHistorical electrometers.\nGold-leaf electroscope.\nThe gold-leaf electroscope was one of the instruments used to indicate electric charge. It is still used for science demonstrations but has been superseded in most applications by electronic measuring instruments. The instrument consists of two thin leaves of gold foil suspended from an electrode. When the electrode is charged by induction or by contact, the leaves acquire similar electric charges and repel each other due to the Coulomb force. Their separation is a direct indication of the net charge stored on them. On the glass opposite the leaves, pieces of tin foil may be pasted, so that when the leaves diverge fully they may discharge into the ground. The leaves may be enclosed in a glass envelope to protect them from drafts, and the envelope may be evacuated to minimize charge leakage. This principle has been used to detect ionizing radiation, as seen in the quartz fibre electrometer and Kearny fallout meter.\nThis type of electroscope usually acts as an indicator and not a measuring device, although it can be calibrated. A calibrated electrometer with a more robust aluminium indicator was invented by Ferdinand Braun and first described in 1887. According to Braun, the standard gold-leaf electrometer is good up to about 800 V with a resolution of 0.1 V using an ocular micrometer. For larger voltages up to 4\u20136 kV Braun's instrument can achieve a resolution of 10 V.\nThe instrument was developed in the 18th century by several researchers, among them Abraham Bennet (1787) and Alessandro Volta.\nEarly quadrant electrometer.\nWhile the term \"quadrant electrometer\" eventually referred to Kelvin's version, this term was first used to describe a simpler device. Its body consists of an upright stem of wood affixed to a semicircle of ivory with angle markings. A light cork ball hangs by a string from a pivot at the center of the semicircle and makes contact with the stem. When the instrument is placed upon a charged body, the stem and ball become charged and repel each other. The amount of repulsion is quantified by reading the angle between the string and the stem off the semicircle, though the measured angle is not in direct proportion to the charge. Early inventors included William Henley (1770) and Horace-B\u00e9n\u00e9dict de Saussure.\nCoulomb's electrometer.\nTorsion is used to give a measurement more sensitive than repulsion of gold leaves or cork-balls. It consists of a glass cylinder with a glass tube on top. In the axes of the tube is a glass thread, the lower end of this holds a bar of gum lac, with a gilt pith ball at each extremity. Through another aperture on the cylinder, another gum lac rod with gilt balls may be introduced. This is called the carrier rod.\nIf the lower ball of the carrier rod is charged when it is entered into the aperture, this will repel one of the movable balls inside. An index and scale (not pictured) is attached to the top of the twistable glass rod. The number of degrees twisted to bring the balls back together is in exact proportion of the amount of charge of the ball of the carrier rod.\nFrancis Ronalds, the inaugural Director of the Kew Observatory, made important improvements to the Coulomb torsion balance around 1844 and the modified instrument was sold by London instrument-makers. Ronalds used a thin suspended needle rather than the gum lac bar and replaced the carrier rod with a fixed piece in the plane of the needle. Both were metal, as was the suspending line and its surrounding tube, so that the needle and the fixed piece could be charged directly through wire connections. Ronalds also employed a Faraday cage and trialled photography to record the readings continuously. It was the forerunner of Kelvin's quadrant electrometer (described below).\nPeltier electrometer.\nDeveloped by Peltier, this uses a form of magnetic compass to measure deflection by balancing the electrostatic force with a magnetic needle.\nBohnenberger electrometer.\nThe Bohnenberger electrometer, developed by J. G. F. von Bohnenberger from an invention by T. G. B. Behrens, consists of a single gold leaf suspended vertically between the anode and cathode of a dry pile. Any charge imparted to the gold leaf causes it to move toward one or the other pole; thus, the sign of the charge as well as its approximate magnitude may be gauged.\nAttraction electrometer.\nAlso known as \"attracted disk electrometers\", attraction electrometers are sensitive balances measuring the attraction between charged disks. William Snow Harris is credited with the invention of this instrument, which was further improved by Lord Kelvin.\nKelvin's quadrant electrometer.\nDeveloped by Lord Kelvin, this is the most sensitive and accurate of all the mechanical electrometers. The original design uses a light aluminum sector suspended inside a drum cut into four segments. The segments are insulated and connected diagonally in pairs. The charged aluminum sector is attracted to one pair of segments and repelled from the other. The deflection is observed by a beam of light reflected from a small mirror attached to the sector, just as in a galvanometer. The engraving on the right shows a slightly different form of this electrometer, using four flat plates rather than closed segments. The plates can be connected externally in the conventional diagonal way (as shown), or in a different order for specific applications.\nA more sensitive form of quadrant electrometer was developed by Frederick Lindemann. It employs a metal-coated quartz fiber instead of an aluminum sector. The deflection is measured by observing the movement of the fiber under a microscope. Initially used for measuring star light, it was employed for the infrared detection of airplanes in the early stages of World War II.\nSome mechanical electrometers were housed inside a cage often referred to as a \u201cbird cage\u201d. This is a form of Faraday Cage that protected the instrument from external electrostatic charges.\nElectrograph.\nElectricity readings may be recorded continuously with a device known as an electrograph. Francis Ronalds created an early electrograph around 1814 in which the changing electricity made a pattern in a rotating resin-coated plate. It was employed at Kew Observatory and the Royal Observatory, Greenwich in the 1840s to create records of variations in atmospheric electricity. In 1845, Ronalds invented photographic means of registering the atmospheric electricity. The photosensitive surface was pulled slowly past of the aperture diaphragm of the camera box, which also housed an electrometer, and captured ongoing movements of the electrometer indices as a trace. Kelvin used similar photographic means for his quadrant electrometer (see above) in the 1860s.\nModern electrometers.\nA modern electrometer is a highly sensitive electronic voltmeter whose input impedance is so high that the current flowing into it can be considered, for most practical purposes, to be zero. The actual value of input resistance for modern electronic electrometers is around 1014\u03a9, compared to around 1010\u03a9 for nanovoltmeters. Owing to the extremely high input impedance, special design considerations (such as driven shields and special insulation materials) must be applied to avoid leakage current.\nAmong other applications, electrometers are used in nuclear physics experiments as they are able to measure the tiny charges left in matter by the passage of ionizing radiation. The most common use for modern electrometers is the measurement of radiation with ionization chambers, in instruments such as geiger counters.\nVibrating reed electrometers.\nVibrating reed electrometers use a variable capacitor formed between a moving electrode (in the form of a vibrating reed) and a fixed input electrode. As the distance between the two electrodes varies, the capacitance also varies and electric charge is forced in and out of the capacitor. The alternating current signal produced by the flow of this charge is amplified and used as an analogue for the DC voltage applied to the capacitor. The DC input resistance of the electrometer is determined solely by the leakage resistance of the capacitor, and is typically extremely high, (although its AC input impedance is lower).\nFor convenience of use, the vibrating reed assembly is often attached by a cable to the rest of the electrometer. This allows for a relatively small unit to be located near the charge to be measured while the much larger reed-driver and amplifier unit can be located wherever it is convenient for the operator.\nValve electrometers.\nValve electrometers use a specialized vacuum tube (thermionic valve) with a very high gain (transconductance) and input resistance. The input current is allowed to flow into the high impedance grid, and the voltage so generated is vastly amplified in the anode (plate) circuit. Valves designed for electrometer use have leakage currents as low as a few femtoamperes (10\u221215 amperes). Such valves must be handled with gloved hands as the salts left on the glass envelope can provide leakage paths for these tiny currents.\nIn a specialized circuit called \"inverted triode\", the roles of anode and grid are reversed. This places the control electrode at a maximum distance from the space-charge region surrounding the filament, minimizing the number of electrons collected by the control electrode, and thus minimizing the input current.\nSolid-state electrometers.\nThe most modern electrometers consist of a solid state amplifier using one or more field-effect transistors, connections for external measurement devices, and usually a display and/or data-logging connections. The amplifier amplifies small currents so that they are more easily measured. The external connections are usually of a co-axial or tri-axial design, and allow attachment of diodes or ionization chambers for ionising radiation measurement. The display or data-logging connections allow the user to see the data or record it for later analysis. Electrometers designed for use with ionization chambers may include a high-voltage power supply, which is used to bias the ionization chamber.\nSolid-state electrometers are often multipurpose devices that can measure voltage, charge, resistance and current. They measure voltage by means of \"voltage balancing\", in which the input voltage is compared with an internal reference voltage source using an electronic circuit with a very high input impedance (of the order of 1014 \u03a9). A similar circuit modified to act as a current-to-voltage converter enables the instrument to measure currents as small as a few femtoamperes. Combined with an internal voltage source, the current measuring mode can be adapted to measure very high resistances, of the order of 1017 \u03a9. Finally, by calculation from the known capacitance of the electrometer's input terminal, the instrument can measure very small electric charges, down to a small fraction of a picocoulomb.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "40365", "revid": "3125232", "url": "https://en.wikipedia.org/wiki?curid=40365", "title": "Galvanometer", "text": "Instrument to measure electric current\nA galvanometer is an electromechanical measuring instrument for electric current. Early galvanometers were uncalibrated, but improved versions, called ammeters, were calibrated and could measure the flow of current more precisely. Galvanometers work by deflecting a pointer in response to an electric current flowing through a coil in a constant magnetic field. The mechanism is also used as an actuator in applications such as hard disks.\nGalvanometers came from the observation, first noted by Hans Christian \u00d8rsted in 1820, that a magnetic compass's needle deflects when near a wire having electric current. They were the first instruments used to detect and measure small amounts of current. Andr\u00e9-Marie Amp\u00e8re, who gave mathematical expression to \u00d8rsted's discovery, named the instrument after the Italian electricity researcher Luigi Galvani, who in 1791 discovered the principle of the frog galvanoscope \u2013 that electric current would make the legs of a dead frog jerk.\nGalvanometers have been essential for the development of science and technology in many fields. For example, in the 1800s they enabled long-range communication through submarine cables, such as the earliest transatlantic telegraph cables, and were essential to discovering the electrical activity of the heart and brain, by their fine measurements of current.\nGalvanometers have also been used as the display components of other kinds of analog meters (e.g., light meters and VU meters), capturing the outputs of these meters' sensors. Today, the main type of galvanometer still in use is the D'Arsonval/Weston type.\nOperation.\nModern galvanometers, of the D'Arsonval/Weston type, are constructed with a small pivoting coil of wire, called a spindle, in the field of a permanent magnet. The coil is attached to a thin pointer that traverses a calibrated scale. A tiny torsion spring pulls the coil and pointer to the zero position.\nWhen a direct current (DC) flows through the coil, the coil generates a magnetic field. This field acts against the permanent magnet. The coil twists, pushing against the spring, and moves the pointer. The hand points at a scale indicating the electric current. Careful design of the pole pieces ensures that the magnetic field is uniform so that the angular deflection of the pointer is proportional to the current. A useful meter generally contains a provision for damping the mechanical resonance of the moving coil and pointer, so that the pointer settles quickly to its position without oscillation.\nThe basic sensitivity of a meter might be, for instance, 100 microamperes full scale (with a voltage drop of, say, 50 millivolts at full current). Such meters are often calibrated to read some other quantity that can be converted to a current of that magnitude. The use of current dividers, often called shunts, allows a meter to be calibrated to measure larger currents. A meter can be calibrated as a DC voltmeter if the resistance of the coil is known by calculating the voltage required to generate a full-scale current. A meter can be configured to read other voltages by putting it in a voltage divider circuit. This is generally done by placing a resistor in series with the meter coil. A meter can be used to read resistance by placing it in series with a known voltage (a battery) and an adjustable resistor. In a preparatory step, the circuit is completed and the resistor adjusted to produce full-scale deflection. When an unknown resistor is placed in series in the circuit the current will be less than full scale and an appropriately calibrated scale can display the value of the previously unknown resistor.\nThese capabilities to translate different kinds of electric quantities into pointer movements make the galvanometer ideal for turning the output of other sensors that output electricity (in some form or another), into something that can be read by a human.\nBecause the pointer of the meter is usually a small distance above the scale of the meter, parallax error can occur when the operator attempts to read the scale line that \"lines up\" with the pointer. To counter this, some meters include a mirror along with the markings of the principal scale. The accuracy of the reading from a mirrored scale is improved by positioning one's head while reading the scale so that the pointer and the reflection of the pointer are aligned; at this point, the operator's eye must be directly above the pointer and any parallax error has been minimized.\nUses.\nProbably the largest use of galvanometers was of the D'Arsonval/Weston type used in analog meters in electronic equipment. Since the 1980s, galvanometer-type analog meter movements have been displaced by analog-to-digital converters (ADCs) for many uses. A digital panel meter (DPM) contains an ADC and numeric display. The advantages of a digital instrument are higher precision and accuracy, but factors such as power consumption or cost may still favor the application of analog meter movements.\nModern uses.\nMost modern uses for the galvanometer mechanism are in positioning and control systems. Galvanometer mechanisms are divided into moving magnet and moving coil galvanometers; in addition, they are divided into \"closed-loop\" and \"open-loop\" - or \"resonant\" - types.\n\"Mirror\" galvanometer systems are used as beam positioning or beam steering elements in laser scanning systems. For example, for material processing with high-power lasers, closed loop mirror galvanometer mechanisms are used with servo control systems. These are typically high power galvanometers and the newest galvanometers designed for beam steering applications can have frequency responses over 10\u00a0kHz with appropriate servo technology. Closed-loop mirror galvanometers are also used in similar ways in stereolithography, laser sintering, laser engraving, laser beam welding, laser TVs, laser displays and in imaging applications such as retinal scanning with Optical Coherence Tomography (OCT) and Scanning Laser Ophthalmoscopy (SLO). Almost all of these galvanometers are of the moving magnet type. The closed loop is obtained measuring the position of the rotating axis with an infrared emitter and 2 photodiodes. This feedback is an analog signal.\nOpen loop, or resonant mirror galvanometers, are mainly used in some types of laser-based bar-code scanners, printing machines, imaging applications, military applications and space systems. Their non-lubricated bearings are especially of interest in applications that require functioning in a high vacuum.\nMoving coil type galvanometer mechanisms (called 'voice coils' by hard disk manufacturers) are used for controlling the \"head positioning\" servos in hard disk drives and CD/DVD players, in order to keep mass (and thus access times), as low as possible.\nPast uses.\nA major early use for galvanometers was for finding faults in telecommunications cables. They were superseded in this application late in the 20th century by time-domain reflectometers.\nGalvanometer mechanisms were also used to get readings from photoresistors in the metering mechanisms of film cameras (as seen in the adjacent image).\nIn analog strip chart recorders such as used in electrocardiographs, electroencephalographs and polygraphs, galvanometer mechanisms were used to position the \"pen\". Strip chart recorders with galvanometer driven pens may have a full-scale frequency response of 100\u00a0Hz and several centimeters of deflection.\nHistory.\nHans Christian \u00d8rsted.\nThe deflection of a magnetic compass needle by the current in a wire was first described by Hans Christian \u00d8rsted in 1820. The phenomenon was studied both for its own sake and as a means of measuring electric current.\nSchweigger and Amp\u00e8re.\nThe earliest galvanometer was reported by Johann Schweigger at the University of Halle on 16 September 1820. Andr\u00e9-Marie Amp\u00e8re also contributed to its development. Early designs increased the effect of the magnetic field generated by the current by using multiple turns of wire. The instruments were at first called \"multipliers\" due to this common design feature. The term \"galvanometer,\" in common use by 1836, was derived from the surname of Italian electricity researcher Luigi Galvani, who in 1791 discovered that electric current would make a dead frog's leg jerk.\nPoggendorff and Thomson.\nOriginally, the instruments relied on the Earth's magnetic field to provide the restoring force for the compass needle. These were called \"tangent\" galvanometers and had to be oriented before use. Later instruments of the \"astatic\" type used opposing magnets to become independent of the Earth's field and would operate in any orientation.\nAn early mirror galvanometer was invented in 1826 by Johann Christian Poggendorff. An astatic galvanometer was invented by Hermann von Helmholtz in 1849; a more sensitive version of that device, the Thomson \"mirror galvanometer\", was patented in 1858 by William Thomson (Lord Kelvin). Thomson's design was able to detect very rapid current changes by using small magnets attached to a lightweight mirror, suspended by a thread, instead of a compass needle. The deflection of a light beam on the mirror greatly magnified the deflection induced by small currents. Alternatively, the deflection of the suspended magnets could be observed directly through a microscope.\nGeorg Ohm.\nThe ability to measure voltage and current quantitatively allowed Georg Ohm, in 1827, to formulate Ohm's law \u2013 that the voltage across a conductor is directly proportional to the current through it.\nD'Arsonval and Deprez.\nThe early moving-magnet form of galvanometer had the disadvantage that it was affected by any magnets or iron masses near it, and its deflection was not linearly proportional to the current. In 1882 Jacques-Ars\u00e8ne d'Arsonval and Marcel Deprez developed a form with a stationary permanent magnet and a moving coil of wire, suspended by fine wires which provided both an electrical connection to the coil and the restoring torque to return to the zero position. An iron tube between the magnet's pole pieces defined a circular gap through which the coil rotated. This gap produced a consistent, radial magnetic field across the coil, giving a linear response throughout the instrument's range. A mirror attached to the coil deflected a beam of light to indicate the coil position. The concentrated magnetic field and delicate suspension made these instruments sensitive; d'Arsonval's initial instrument could detect ten microamperes.\nEdward Weston.\nEdward Weston extensively improved the design of the galvanometer. He substituted the fine wire suspension with a pivot and provided restoring torque and electrical connections through spiral springs rather than through the traditional wristwatch balance wheel hairspring. He developed a method of stabilizing the magnetic field of the permanent magnet, so the instrument would have consistent accuracy over time. He replaced the light beam and mirror with a knife-edge pointer that could be read directly. A mirror under the pointer, in the same plane as the scale, eliminated parallax observation error. To maintain the field strength, Weston's design used a very narrow circumferential slot through which the coil moved, with a minimal air-gap. This improved linearity of pointer deflection with respect to coil current. Finally, the coil was wound on a light-weight form made of conductive metal, which acted as a damper. By 1888, Edward Weston had patented and brought out a commercial form of this instrument, which became a standard electrical equipment component. It was known as a \"portable\" instrument because it was affected very little by mounting position or by transporting it from place to place. This design is almost universally used in moving-coil meters today.\nInitially, laboratory instruments relying on the Earth's own magnetic field to provide restoring force for the pointer, galvanometers were developed into compact, rugged, sensitive portable instruments essential to the development of electro-technology.\nTaut-band movement.\nThe taut-band movement is a modern development of the D'Arsonval-Weston movement. The jewel pivots and hairsprings are replaced by tiny strips of metal under tension. Such a meter is more rugged for field use.\nTypes.\nThere are broadly two types of galvanometers. Some galvanometers use a solid pointer on a scale to show measurements; other very sensitive types use a miniature mirror and a beam of light to provide mechanical amplification of low-level signals.\nTangent galvanometer.\nA tangent galvanometer is an early measuring instrument used for the measurement of electric current. It works by using a compass needle to compare a magnetic field generated by the unknown current to the magnetic field of the Earth. It gets its name from its operating principle, the tangent law of magnetism, which states that the tangent of the angle a compass needle makes is proportional to the ratio of the strengths of the two perpendicular magnetic fields. It was first described by Johan Jakob Nervander in 1834.\nA tangent galvanometer consists of a coil of insulated copper wire wound on a circular non-magnetic frame. The frame is mounted vertically on a horizontal base provided with levelling screws. The coil can be rotated on a vertical axis passing through its centre. A compass box is mounted horizontally at the centre of a circular scale. It consists of a tiny, powerful magnetic needle pivoted at the centre of the coil. The magnetic needle is free to rotate in the horizontal plane. The circular scale is divided into four quadrants. Each quadrant is graduated from 0\u00b0 to 90\u00b0. A long thin aluminium pointer is attached to the needle at its centre and at right angle to it. To avoid errors due to parallax, a plane mirror is mounted below the compass needle.\nIn operation, the instrument is first rotated until the magnetic field of the Earth, indicated by the compass needle, is parallel with the plane of the coil. Then the unknown current is applied to the coil. This creates a second magnetic field on the axis of the coil, perpendicular to the Earth's magnetic field. The compass needle responds to the vector sum of the two fields and deflects to an angle equal to the tangent of the ratio of the two fields. From the angle read from the compass's scale, the current could be found from a table. The current supply wires have to be wound in a small helix, like a pig's tail, otherwise the field due to the wire will affect the compass needle and an incorrect reading will be obtained.\nTheory.\nThe galvanometer is oriented so that the plane of the coil is vertical and aligned along parallel to the horizontal component \"BH\" of the Earth's magnetic field (i.e. parallel to the local \"magnetic meridian\"). When an electric current flows through the galvanometer coil, a second magnetic field \"B\" is created. At the center of the coil, where the compass needle is located, the coil's field is perpendicular to the plane of the coil. The magnitude of the coil's field is:\nformula_1\nwhere \"I\" is the current in amperes, \"n\" is the number of turns of the coil and \"r\" is the radius of the coil. These two perpendicular magnetic fields add vectorially, and the compass needle points along the direction of their resultant \"BH+B\". The current in the coil causes the compass needle to rotate by an angle \"\u03b8\":\nformula_2\nFrom tangent law, \"B \n BH \"tan\" \u03b8\", i.e.\nformula_3\nor\nformula_4\nor \"I \n K \"tan\" \u03b8\", where \"K\" is called the Reduction Factor of the tangent galvanometer.\nOne problem with the tangent galvanometer is that its resolution degrades at both high currents and low currents. The maximum resolution is obtained when the value of \"\u03b8\" is 45\u00b0. When the value of \"\u03b8\" is close to 0\u00b0 or 90\u00b0, a large percentage change in the current will only move the needle a few degrees.\nGeomagnetic field measurement.\nA tangent galvanometer can also be used to measure the magnitude of the horizontal component of the geomagnetic field. When used in this way, a low-voltage power source, such as a battery, is connected in series with a rheostat, the galvanometer, and an ammeter. The galvanometer is first aligned so that the coil is parallel to the geomagnetic field, whose direction is indicated by the compass when there is no current through the coils. The battery is then connected and the rheostat is adjusted until the compass needle deflects 45 degrees from the geomagnetic field, indicating that the magnitude of the magnetic field at the center of the coil is the same as that of the horizontal component of the geomagnetic field. This field strength can be calculated from the current as measured by the ammeter, the number of turns of the coil, and the radius of the coils.\nAstatic galvanometer.\nUnlike the tangent galvanometer, the \"astatic galvanometer\" does not use the Earth's magnetic field for measurement, so it does not need to be oriented with respect to the Earth's field, making it easier to use. Developed by Leopoldo Nobili in 1825, it consists of two magnetized needles parallel to each other but with the magnetic poles reversed. These needles are suspended by a single silk thread. The lower needle is inside a vertical current sensing coil of wire and is deflected by the magnetic field created by the passing current, as in the tangent galvanometer above. The purpose of the second needle is to cancel the dipole moment of the first needle, so the suspended armature has no net magnetic dipole moment, and thus is not affected by the earth's magnetic field. The needle's rotation is opposed by the torsional elasticity of the suspension thread, which is proportional to the angle.\nMirror galvanometer.\nTo achieve higher sensitivity to detect extremely small currents, the mirror galvanometer substitutes a lightweight mirror for the pointer. It consists of horizontal magnets suspended from a fine fiber, inside a vertical coil of wire, with a mirror attached to the magnets. A beam of light reflected from the mirror falls on a graduated scale across the room, acting as a long mass-less pointer. The mirror galvanometer was used as the receiver in the first trans-Atlantic submarine telegraph cables in the 1850s, to detect the extremely faint pulses of current after their thousand-mile journey under the Atlantic. In a device called an oscillograph, the moving beam of light is used, to produce graphs of current versus time, by recording measurements on photographic film. The string galvanometer is a type of mirror galvanometer so sensitive that it was used to make the first electrocardiogram of the electrical activity of the human heart.\nBallistic galvanometer.\nA ballistic galvanometer is a type of sensitive galvanometer for measuring the quantity of charge discharged through it. It is an integrator, by virtue of the long time constant of its response, unlike a current-measuring galvanometer. The moving part has a large moment of inertia that gives it an oscillation period long enough to make the integrated measurement. It can be either of the moving coil or moving magnet type; commonly it is a mirror galvanometer.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "40366", "revid": "16749985", "url": "https://en.wikipedia.org/wiki?curid=40366", "title": "ADC", "text": "ADC may refer to:\n&lt;templatestyles src=\"Template:TOC_right/styles.css\" /&gt;\nOther uses.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "40367", "revid": "6010417", "url": "https://en.wikipedia.org/wiki?curid=40367", "title": "Analog-to-digital converter", "text": "System that converts an analog signal into a digital signal\nIn electronics, an analog-to-digital converter (ADC, A/D, or A-to-D) is a system that converts an analog signal, such as from fingers touching a touchscreen, sound entering a microphone, or light entering a digital camera, into a digital signal.\nAn ADC may also provide an isolated measurement such as an electronic device that converts an analog input voltage or current to a digital number representing the magnitude of the voltage or current. Typically the digital output is a two's complement binary number that is proportional to the input, but there are other possibilities.\nThere are several ADC architectures. Due to the complexity and the need for precisely matched components, all but the most specialized ADCs are implemented as integrated circuits (ICs). These typically take the form of metal\u2013oxide\u2013semiconductor (MOS) mixed-signal integrated circuit chips that integrate both analog and digital circuits.\nA digital-to-analog converter (DAC) performs the reverse function; it converts a digital signal into an analog signal.\nExplanation.\nAn ADC converts a continuous-time and continuous-amplitude analog signal to a discrete-time and discrete-amplitude digital signal. The conversion involves quantization of the input, so it necessarily introduces a small amount of quantization error. Furthermore, instead of continuously performing the conversion, an ADC does the conversion periodically, sampling the input, and limiting the allowable bandwidth of the input signal.\nThe performance of an ADC is primarily characterized by its bandwidth and signal-to-noise and distortion ratio (SNDR). The bandwidth of an ADC is characterized primarily by its sampling rate. The SNDR of an ADC is influenced by many factors, including the resolution, linearity and accuracy (how well the quantization levels match the true analog signal), aliasing and jitter. The SNDR of an ADC is often summarized in terms of its effective number of bits (ENOB), the number of bits of each measure it returns that are on average not noise. An ideal ADC has an ENOB equal to its resolution. ADCs are chosen to match the bandwidth and required SNDR of the signal to be digitized. If an ADC operates at a sampling rate greater than twice the bandwidth of the signal, then per the Nyquist\u2013Shannon sampling theorem, near-perfect reconstruction is possible. The presence of quantization error limits the SNDR of even an ideal ADC. However, if the SNDR of the ADC exceeds that of the input signal, then the effects of quantization error may be neglected, resulting in an essentially perfect digital representation of the bandlimited analog input signal.\nResolution.\nThe resolution of the converter indicates the number of different, i.e. discrete, values it can produce over the allowed range of analog input values. Thus a particular resolution determines the magnitude of the quantization error and therefore determines the maximum possible signal-to-noise ratio for an ideal ADC without the use of oversampling. The input samples are usually stored electronically in binary form within the ADC, so the resolution is usually expressed as the audio bit depth. In consequence, the number of discrete values available is usually a power of two. For example, an ADC with a resolution of 8 bits can encode an analog input to one in 256 different levels (28\u00a0=\u00a0256). The values can represent the ranges from 0 to 255 (i.e. as unsigned integers) or from \u2212128 to 127 (i.e. as signed integer), depending on the application.\nResolution can also be defined electrically, and expressed in volts. The change in voltage required to guarantee a change in the output code level is called the least significant bit (LSB) voltage. The resolution \"Q\" of the ADC is equal to the LSB voltage. The voltage resolution of an ADC is equal to its overall voltage measurement range divided by the number of intervals:\nformula_1\nwhere \"M\" is the ADC's resolution in bits and \"E\"FSR is the full-scale voltage range (also called 'span'). \"E\"FSR is given by\nformula_2\nwhere \"V\"RefHi and \"V\"RefLow are the upper and lower extremes, respectively, of the voltages that can be coded.\nNormally, the number of voltage intervals is given by\nformula_3\nwhere \"M\" is the ADC's resolution in bits.\nThat is, one voltage interval is assigned in between two consecutive code levels.\nExample:\nIn many cases, the useful resolution of a converter is limited by the signal-to-noise ratio (SNR) and other errors in the overall system expressed as an ENOB.\nQuantization error.\nQuantization error is introduced by the quantization inherent in an ideal ADC. It is a rounding error between the analog input voltage to the ADC and the output digitized value. The error is nonlinear and signal-dependent. In an ideal ADC, where the quantization error is uniformly distributed between \u2212&lt;templatestyles src=\"Fraction/styles.css\" /&gt;1\u20442 LSB and +&lt;templatestyles src=\"Fraction/styles.css\" /&gt;1\u20442 LSB, and the signal has a uniform distribution covering all quantization levels, the signal-to-quantization-noise ratio (SQNR) is given by\nformula_4\nwhere formula_5 is the number of quantization bits. For example, for a 16-bit ADC, the quantization error is 96.3\u00a0dB below the maximum level.\nQuantization error is distributed from DC to the Nyquist frequency. Consequently, if part of the ADC's bandwidth is not used, as is the case with oversampling, some of the quantization error will occur out-of-band, effectively improving the SQNR for the bandwidth in use. In an oversampled system, noise shaping can be used to further increase SQNR by forcing more quantization error out of band.\nDither.\nIn ADCs, performance can usually be improved using dither. This is a very small amount of random noise (e.g. white noise), which is added to the input before conversion. Its effect is to randomize the state of the LSB based on the signal. Rather than the signal simply getting cut off altogether at low levels, it extends the effective range of signals that the ADC can convert, at the expense of a slight increase in noise. Dither can only increase the resolution of a sampler. It cannot improve the linearity, and thus accuracy does not necessarily improve.\nQuantization distortion in an audio signal of very low level with respect to the bit depth of the ADC is correlated with the signal and sounds distorted and unpleasant. With dithering, the distortion is transformed into noise. The undistorted signal may be recovered accurately by averaging over time. Dithering is also used in integrating systems such as electricity meters. Since the values are added together, the dithering produces results that are more exact than the LSB of the analog-to-digital converter.\nDither is often applied when quantizing photographic images to a fewer number of bits per pixel\u2014the image becomes noisier but to the eye looks far more realistic than the quantized image, which otherwise becomes banded. This analogous process may help to visualize the effect of dither on an analog audio signal that is converted to digital.\nAccuracy.\nAn ADC has several sources of errors. Quantization error and (assuming the ADC is intended to be linear) non-linearity are intrinsic to any analog-to-digital conversion. These errors are measured in a unit called the least significant bit (LSB). In the above example of an 8-bit ADC, an error of one LSB is &lt;templatestyles src=\"Fraction/styles.css\" /&gt;1\u2044256 of the full signal range, or about 0.4%.\nNonlinearity.\nAll ADCs suffer from nonlinearity errors caused by their physical imperfections, causing their output to deviate from a linear function (or some other function, in the case of a deliberately nonlinear ADC) of their input. These errors can sometimes be mitigated by calibration, or prevented by testing. Important parameters for linearity are integral nonlinearity and differential nonlinearity. These nonlinearities introduce distortion that can reduce the signal-to-noise ratio performance of the ADC and thus reduce its effective resolution.\nJitter.\nWhen digitizing a sine wave formula_6, the use of a non-ideal sampling clock will result in some uncertainty in when samples are recorded. Provided that the actual sampling time uncertainty due to clock jitter is formula_7, the error caused by this phenomenon can be estimated as formula_8. This will result in additional recorded noise that will reduce the effective number of bits (ENOB) below that predicted by quantization error alone. The error is zero for DC, small at low frequencies, but significant with signals of high amplitude and high frequency. The effect of jitter on performance can be compared to quantization error: formula_9, where q is the number of ADC bits.\nClock jitter is caused by phase noise. The resolution of ADCs with a digitization bandwidth between 1\u00a0MHz and 1\u00a0GHz is limited by jitter. For lower bandwidth conversions such as when sampling audio signals at 44.1\u00a0kHz, clock jitter has a less significant impact on performance.\nSampling rate.\nAn analog signal is continuous in time and it is necessary to convert this to a flow of digital values. It is therefore required to define the rate at which new digital values are sampled from the analog signal. The rate of new values is called the \"sampling rate\" or \"sampling frequency\" of the converter. A continuously varying bandlimited signal can be sampled and then the original signal can be reproduced from the discrete-time values by a reconstruction filter. The Nyquist\u2013Shannon sampling theorem implies that a faithful reproduction of the original signal is only possible if the sampling rate is higher than twice the highest frequency of the signal.\nSince a practical ADC cannot make an instantaneous conversion, the input value must necessarily be held constant during the time that the converter performs a conversion (called the \"conversion time\"). An input circuit called a sample and hold performs this task\u2014in most cases by using a capacitor to store the analog voltage at the input, and using an electronic switch or gate to disconnect the capacitor from the input. Many ADC integrated circuits include the sample and hold subsystem internally.\nAliasing.\nAn ADC works by sampling the value of the input at discrete intervals in time. Provided that the input is sampled above the Nyquist rate, defined as twice the highest frequency of interest, then all frequencies in the signal can be reconstructed. If frequencies above half the Nyquist rate are sampled, they are incorrectly detected as lower frequencies, a process referred to as aliasing. Aliasing occurs because instantaneously sampling a function at two or fewer times per cycle results in missed cycles, and therefore the appearance of an incorrectly lower frequency. For example, a 2\u00a0kHz sine wave being sampled at 1.5\u00a0kHz would be reconstructed as a 500\u00a0Hz sine wave.\nTo avoid aliasing, the input to an ADC must be low-pass filtered to remove frequencies above half the sampling rate. This filter is called an \"anti-aliasing filter\", and is essential for a practical ADC system that is applied to analog signals with higher frequency content. In applications where protection against aliasing is essential, oversampling may be used to greatly reduce or even eliminate it.\nAlthough aliasing in most systems is unwanted, it can be exploited to provide simultaneous down-mixing of a band-limited high-frequency signal (see undersampling and frequency mixer). The alias is effectively the lower heterodyne of the signal frequency and sampling frequency.\nOversampling.\nFor economy, signals are often sampled at the minimum rate required with the result that the quantization error introduced is white noise spread over the whole passband of the converter. If a signal is sampled at a rate much higher than the Nyquist rate and then digitally filtered to limit it to the signal bandwidth produces the following advantages:\nOversampling is typically used in audio frequency ADCs where the required sampling rate (typically 44.1 or 48\u00a0kHz) is very low compared to the clock speed of typical transistor circuits (&gt;1\u00a0MHz). In this case, the performance of the ADC can be greatly increased at little or no cost. Furthermore, as any aliased signals are also typically out of band, aliasing can often be eliminated using very low cost filters.\nRelative speed and precision.\nThe speed of an ADC varies by type. The Wilkinson ADC is limited by the clock rate which is processable by current digital circuits. For a successive-approximation ADC, the conversion time scales with the logarithm of the resolution, i.e. the number of bits. Flash ADCs are certainly the fastest type of the three; The conversion is basically performed in a single parallel step.\nThere is a potential tradeoff between speed and precision. Flash ADCs have drifts and uncertainties associated with the comparator levels results in poor linearity. To a lesser extent, poor linearity can also be an issue for successive-approximation ADCs. Here, nonlinearity arises from accumulating errors from the subtraction processes. Wilkinson ADCs have the best linearity of the three.\nSliding scale principle.\nThe sliding scale or randomizing method can be employed to greatly improve the linearity of any type of ADC, but especially flash and successive approximation types. For any ADC the mapping from input voltage to digital output value is not exactly a floor or ceiling function as it should be. Under normal conditions, a pulse of a particular amplitude is always converted to the same digital value. The problem lies in that the ranges of analog values for the digitized values are not all of the same widths, and the differential linearity decreases proportionally with the divergence from the average width. The sliding scale principle uses an averaging effect to overcome this phenomenon. A random, but known analog voltage is added to the sampled input voltage. It is then converted to digital form, and the equivalent digital amount is subtracted, thus restoring it to its original value. The advantage is that the conversion has taken place at a random point. The statistical distribution of the final levels is decided by a weighted average over a region of the range of the ADC. This in turn desensitizes it to the width of any specific level.\nTypes.\nThese are several common ways of implementing an electronic ADC.\nRC charge time.\nResistor-capacitor (RC) circuits have a known voltage charging and discharging curve that can be used to solve for an unknown analog value.\nWilkinson.\nThe Wilkinson ADC was designed by Denys Wilkinson in 1950. The Wilkinson ADC is based on the comparison of an input voltage with that produced by a charging capacitor. The capacitor is allowed to charge until a comparator determines it matches the input voltage. Then, the capacitor is discharged linearly by using a constant current source. The time required to discharge the capacitor is proportional to the amplitude of the input voltage. While the capacitor is discharging, pulses from a high-frequency oscillator clock are counted by a register. The number of clock pulses recorded in the register is also proportional to the input voltage.\nMeasuring analog resistance or capacitance.\nIf the analog value to measure is represented by a resistance or capacitance, then by including that element in an RC circuit (with other resistances or capacitances fixed) and measuring the time to charge the capacitance from a known starting voltage to another known ending voltage through the resistance from a known voltage supply, the value of the unknown resistance or capacitance can be determined using the capacitor charging equation:\nformula_10\nand solving for the unknown resistance or capacitance using those starting and ending datapoints. This is similar but contrasts to the Wilkinson ADC which measures an unknown voltage with a known resistance and capacitance, by instead measuring an unknown resistance or capacitance with a known voltage.\nFor example, the positive (and/or negative) pulse width from a 555 Timer IC in monostable or astable mode represents the time it takes to charge (and/or discharge) its capacitor from &lt;templatestyles src=\"Fraction/styles.css\" /&gt;1\u20443\u00a0\"V\"supply to &lt;templatestyles src=\"Fraction/styles.css\" /&gt;2\u20443\u00a0\"V\"supply. By sending this pulse into a microcontroller with an accurate clock, the duration of the pulse can be measured and converted using the capacitor charging equation to produce the value of the unknown resistance or capacitance.\nLarger resistances and capacitances will take a longer time to measure than smaller one. And the accuracy is limited by the accuracy of the microcontroller clock and the amount of time available to measure the value, which potentially might even change during measurement or be affected by external parasitics.\nFlash ADC.\nA flash ADC, also known as a parallel search ADC, employs a bank of voltage comparators sampling the input signal in parallel, each with a different voltage threshold. The circuit consists of a resistive divider network, a set of voltage comparators and a priority encoder. Each node of the resistive divider provides a voltage threshold for one comparator. The comparator outputs are applied to a priority encoder, which generates a binary number proportional to the input voltage.\nFlash ADCs have a large die size and high power dissipation. They are used in a variety of applications, including video, wideband communications, and for digitizing other fast signals.\nThe circuit has the advantage of high speed as the conversion takes place simultaneously rather than sequentially. Typical conversion time is 100\u00a0ns or less. Conversion time is limited only by the speed of the comparator and of the priority encoder. This type of ADC has the disadvantage that for each additional output bit, the number of comparators required almost doubles and priority encoder becomes more complex.\nSuccessive approximation.\nA successive-approximation ADC uses a comparator and a binary search to successively narrow a range that contains the input voltage. At each successive step, the converter compares the input voltage to the output of an internal digital-to-analog converter (DAC) which initially represents the midpoint of the allowed input voltage range. At each step in this process, the approximation is stored in a successive approximation register (SAR) and the output of the digital-to-analog converter is updated for a comparison over a narrower range.\nRamp-compare.\nA ramp-compare ADC produces a saw-tooth signal that ramps up or down then quickly returns to zero.\nWhen the ramp starts, a timer starts counting. When the ramp voltage matches the input, a comparator fires, and the timer's value is recorded. Timed ramp converters can be implemented economically, however, the ramp time may be sensitive to temperature because the circuit generating the ramp is often a simple analog integrator. A more accurate converter uses a clocked counter driving a DAC. A special advantage of the ramp-compare system is that converting a second signal just requires another comparator and another register to store the timer value. To reduce sensitivity to input changes during conversion, a sample and hold can charge a capacitor with the instantaneous input voltage and the converter can time the time required to discharge with a constant current.\nIntegrating.\nAn integrating ADC (also dual-slope or multi-slope ADC) applies the unknown input voltage to the input of an integrator and allows the voltage to ramp for a fixed time period (the run-up period). Then a known reference voltage of opposite polarity is applied to the integrator and is allowed to ramp until the integrator output returns to zero (the run-down period). The input voltage is computed as a function of the reference voltage, the constant run-up time period, and the measured run-down time period. The run-down time measurement is usually made in units of the converter's clock, so longer integration times allow for higher resolutions. Likewise, the speed of the converter can be improved by sacrificing resolution. Converters of this type (or variations on the concept) are used in most digital voltmeters for their linearity and flexibility.\nDelta-encoded.\nA \"delta-encoded\" or \"counter-ramp\" ADC has an up-down counter that feeds a DAC. The input signal and the DAC both go to a comparator. The comparator controls the counter. The circuit uses negative feedback from the comparator to adjust the counter until the DAC's output matches the input signal and number is read from the counter. Delta converters have very wide ranges and high resolution, but the conversion time is dependent on the input signal behavior, though it will always have a guaranteed worst-case. Delta converters are often very good choices to read real-world signals as most signals from physical systems do not change abruptly. Some converters combine the delta and successive approximation approaches; this works especially well when high frequency components of the input signal are known to be small in magnitude.\nPipelined.\nA \"pipelined ADC\" (also called \"subranging quantizer\") uses two or more conversion steps. First, a coarse conversion is done. In a second step, the difference to the input signal is determined with a DAC. This difference is then converted more precisely, and the results are combined in the last step. This can be considered a refinement of the successive-approximation ADC wherein the feedback reference signal consists of the interim conversion of a whole range of bits (for example, four bits) rather than just the next-most-significant bit. By combining the merits of the successive approximation and flash ADCs this type is fast, has a high resolution, and can be implemented efficiently.\nDelta-sigma.\nA delta-sigma ADC (also known as a sigma-delta ADC) is based on a negative feedback loop with an analog filter and low resolution (often 1 bit) but high sampling rate ADC and DAC. The feedback loop continuously corrects accumulated quantization errors and performs noise shaping: quantization noise is reduced in the low frequencies of interest, but is increased in higher frequencies. Those higher frequencies may then be removed by a downsampling digital filter, which also converts the data stream from that high sampling rate with low bit depth to a lower rate with higher bit depth.\nTime-interleaved.\nA time-interleaved ADC uses M parallel ADCs where each ADC samples data every M:th cycle of the effective sample clock. The result is that the sample rate is increased M times compared to what each individual ADC can manage. In practice, the individual differences between the M ADCs degrade the overall performance reducing the spurious-free dynamic range (SFDR). However, techniques exist to correct for these time-interleaving mismatch errors.\nIntermediate FM stage.\nAn ADC with an intermediate FM stage first uses a voltage-to-frequency converter to produce an oscillating signal with a frequency proportional to the voltage of the input signal, and then uses a frequency counter to convert that frequency into a digital count proportional to the desired signal voltage. Longer integration times allow for higher resolutions. Likewise, the speed of the converter can be improved by sacrificing resolution. The two parts of the ADC may be widely separated, with the frequency signal passed through an opto-isolator or transmitted wirelessly. Some such ADCs use sine wave or square wave frequency modulation; others use pulse-frequency modulation. Such ADCs were once the most popular way to show a digital display of the status of a remote analog sensor.\nTime-stretch.\nA time-stretch analog-to-digital converter (TS-ADC) digitizes a very wide bandwidth analog signal, that cannot be digitized by a conventional electronic ADC, by time-stretching the signal prior to digitization. It commonly uses a photonic preprocessor to time-stretch the signal, which effectively slows the signal down in time and compresses its bandwidth. As a result, an electronic ADC, that would have been too slow to capture the original signal, can now capture this slowed-down signal. For continuous capture of the signal, the front end also divides the signal into multiple segments in addition to time-stretching. Each segment is individually digitized by a separate electronic ADC. Finally, a digital signal processor rearranges the samples and removes any distortions added by the preprocessor to yield the binary data that is the digital representation of the original analog signal.\nMeasuring physical values other than voltage.\nAlthough the term ADC is usually associated with measurement of an analog voltage, some partially-electronic devices that convert some measurable physical analog quantity into a digital number can also be considered ADCs, for instance:\nCommercial.\nIn many cases, the most expensive part of an integrated circuit is the pins, because they make the package larger, and each pin has to be connected to the integrated circuit's silicon. To save pins, it is common for ADCs to send their data one bit at a time over a serial interface to the computer, with each bit coming out when a clock signal changes state. This saves quite a few pins on the ADC package, and in many cases, does not make the overall design any more complex.\nCommercial ADCs often have several inputs that feed the same converter, usually through an analog multiplexer. Different models of ADC may include sample and hold circuits, instrumentation amplifiers or differential inputs, where the quantity measured is the difference between two inputs.\nApplications.\nMusic recording.\nAnalog-to-digital converters are integral to modern music reproduction technology and digital audio workstation-based sound recording. Music may be produced on computers using an analog recording and therefore analog-to-digital converters are needed to create the pulse-code modulation (PCM) data streams that go onto compact discs and digital music files. The current crop of analog-to-digital converters utilized in music can sample at rates up to 192 kilohertz. Many recording studios record in 24-bit 96\u00a0kHz pulse-code modulation (PCM) format and then downsample and dither the signal for Compact Disc Digital Audio production (44.1\u00a0kHz) or to 48\u00a0kHz for radio and television broadcast applications.\nDigital signal processing.\nADCs are required in digital signal processing systems that process, store, or transport virtually any analog signal in digital form. TV tuner cards, for example, use fast video analog-to-digital converters. Slow on-chip 8-, 10-, 12-, or 16-bit analog-to-digital converters are common in microcontrollers. Digital storage oscilloscopes need very fast analog-to-digital converters, also crucial for software-defined radio and their new applications.\nScientific instruments.\nDigital imaging systems commonly use analog-to-digital converters for digitizing pixels. Some radar systems use analog-to-digital converters to convert signal strength to digital values for subsequent signal processing. Many other in situ and remote sensing systems commonly use analogous technology.\nMany sensors in scientific instruments produce an analog signal; temperature, pressure, pH, light intensity etc. All these signals can be amplified and fed to an ADC to produce a digital representation.\nDisplays.\nFlat-panel displays are inherently digital and need an ADC to process an analog signal such as composite or VGA.\nTesting.\nTesting an analog-to-digital converter requires an analog input source and hardware to send control signals and capture digital data output. Some ADCs also require an accurate source of reference signal.\nThe key parameters to test an ADC are:\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt; "}
{"id": "40368", "revid": "1098647863", "url": "https://en.wikipedia.org/wiki?curid=40368", "title": "Heat engines", "text": ""}
{"id": "40372", "revid": "6574929", "url": "https://en.wikipedia.org/wiki?curid=40372", "title": "John Thaw", "text": "English actor (1942\u20132002)\nJohn Edward Thaw (3 January 1942 \u2013 21 February 2002) was an English actor. He became best known for his television roles starring as Detective Inspector Jack Regan in \"The Sweeney\" (1975\u201378) and as Detective Chief Inspector Morse in \"Inspector Morse\" (1987\u20132000). He also worked on stage and in films. \nFor four consecutive years, Thaw was nominated for the BAFTA Award for Best Actor for playing Morse, winning in 1990 and 1993. In 1988, he was nominated for the BAFTA Award for Best Actor in a Supporting Role for the film \"Cry Freedom\". In 2001, he was awarded the BAFTA Fellowship.\nEarly life.\nBorn in Gorton, Manchester, to John Edward (\"Jack\") Thaw, a tool-setter at the Fairey Aviation Company aircraft factory, later a long-distance lorry driver, and Dorothy (n\u00e9e Ablott). Dorothy left when he was seven years old. He and his younger brother, Raymond Stuart (Ray), had a difficult childhood due to their father's long absences. Thaw grew up in Gorton and Burnage, attending the Ducie Technical High School for Boys, gaining just one O Level. He entered the Royal Academy of Dramatic Art (RADA) at the age of 16 (two years underage), and won the Academy's Vanburgh Award. Ray emigrated to Australia in the mid-1960s.\nCareer.\nIn 1960, Thaw made his stage d\u00e9but in \"A Shred of Evidence\" at the Liverpool Playhouse and was awarded a contract with the theatre. His first film role was a bit part in \"The Loneliness of the Long Distance Runner\" (1962) starring Tom Courtenay and he also acted on stage opposite Laurence Olivier in \"Semi-Detached\" (1962). In 1963/64, he appeared in several episodes of the BBC series \"Z-Cars\" as a detective constable. Between 1964 and 1966, he starred in two series of the ABC Weekend Television/ITV production \"Redcap\", playing the hard-nosed military policeman Sergeant John Mann. He was also a guest star in an early episode of \"The Avengers\". In 1967 he appeared in \"Bat Out of Hell\" and in the Granada TV/ITV series \"Inheritance\", alongside James Bolam and Michael Goodliffe; TV plays including \"The Talking Head,\" and episodes of series such as \"Budgie\", where he played against type as an effeminate failed playwright with a full beard and a Welsh accent.\nThaw was cast in the police drama series \"The Sweeney\" (1975\u20131978) alongside Dennis Waterman and Garfield Morgan, playing the hard-bitten, tough-talking Flying Squad detective Jack Regan. It established him as a major star in the United Kingdom. He followed this with four series of the sitcom \"Home to Roost\" (1985\u20131990), which co-starred Reece Dinsdale, about a divorced father whose teenage son moves back in with him after choosing as a child to live with his mother. He had previously co-starred in another ITV sitcom, \"Thick as Thieves\" (1974), with Bob Hoskins.\nThaw's role as Detective Chief Inspector Endeavour Morse in \"Inspector Morse\" (1987\u201393, with later specials until 2000), cemented his fame. Alongside his put-upon Detective Sergeant Robert \"Robbie\" Lewis (Kevin Whately), Morse became a high-profile character\u2014\"a cognitive curmudgeon with his love of classical music, his drinking, his classic Jaguar and spates of melancholy\". According to \"The Guardian\", \"Thaw was the definitive Morse, grumpy, crossword-fixated, drunk, slightly anti-feminist, and pedantic about grammar.\" \"Inspector Morse\" became one of the UK's most popular TV series; at its peak in the mid-'90s, it was viewed by 18 million people, about one third of the British population. He won \"Most Popular Actor\" at the 1999 National Television Awards and won two BAFTA awards for his role as Morse. Thaw is mainly known in the United States for \"Inspector Morse\", as well as for the BBC series \"A Year in Provence\" (1993) with Lindsay Duncan.\nThaw subsequently played liberal working-class Lancastrian barrister James Kavanagh in \"Kavanagh QC\" (1995\u201399, and a special in 2001). \nThaw appeared in a number of films for director Richard Attenborough, including \"Cry Freedom\", in which he portrayed the conservative South African justice minister Jimmy Kruger (receiving a BAFTA nomination for Best Supporting Actor), and \"Chaplin\", playing the English music hall impresario Fred Karno alongside Robert Downey Jr. (Chaplin).\nThaw also appeared in the TV adaptation of the Michelle Magorian book \"Goodnight Mister Tom\" (Carlton Television/ITV). It won \"Most Popular Drama\" at the National Television Awards, 1999.\nDuring the 1970s and 1980s, Thaw appeared in productions with the Royal Shakespeare Company and the National Theatre.\nThaw was the subject of \"This Is Your Life\" in 1981 when he was surprised by Eamonn Andrews in the foyer of the National Theatre in London.\nPersonal life.\nIn 1964, Thaw married Sally Alexander, a feminist activist and stage manager, later professor of history at Goldsmiths, University of London. They divorced four years later. He met actress Sheila Hancock in 1969 on the set of \"So What About Love?\" She was married to fellow actor Alexander \"Alec\" Ross. They became friends, but she refused to have an affair as she did not want to disrupt her daughter's life. Following the death of her husband (from oesophageal cancer) in 1971, Thaw and Hancock married on 24 December 1973 in Cirencester. They remained together until his death in 2002 (also from oesophageal cancer).\nThaw had three daughters (all actresses): Abigail from his first marriage to Sally Alexander, Joanna from his second marriage to Sheila Hancock, and he also adopted Sheila Hancock's daughter Melanie Jane, from Hancock's first marriage to Alec Ross. His granddaughter Molly Whitmey made a cameo in the \"Endeavour\" episode \"Oracle\" (series 7, episode 1, broadcast 9 February 2020) as the younger version of her grandmother Sally Alexander.\nThaw was a committed socialist and a lifelong supporter of the Labour Party. He was appointed a Commander of the Most Excellent Order of the British Empire (CBE), the insignia for which he received in March 1993 from Queen Elizabeth II.\nIllness and death.\nA heavy drinker until going teetotal in 1995, and a heavy smoker from the age of 12, Thaw was diagnosed with cancer of the oesophagus in June 2001. He underwent chemotherapy in hope of overcoming the illness, and at first had appeared to respond well to the treatment, but just before Christmas 2001 he was informed that the cancer had spread and the prognosis was terminal.\nThaw died on 21 February 2002, seven weeks after his 60th birthday, the day after he signed a new contract with ITV, and the day before his wife's birthday. At the time of his death he was living at his country home, near the villages of Luckington and Sherston in Wiltshire, and was cremated in Westerleigh, near Yate in South Gloucestershire, in a private service. A memorial service was held on 4 September 2002 at St Martin-in-the-Fields church in Trafalgar Square, attended by 800 people including Charles, Prince of Wales, Richard Attenborough, Tom Courtenay and Cherie Blair.\nA memorial bench is dedicated to Thaw within the grounds of St Paul's, Covent Garden.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "40374", "revid": "4899266", "url": "https://en.wikipedia.org/wiki?curid=40374", "title": "Richter Scale", "text": ""}
{"id": "40375", "revid": "1293563619", "url": "https://en.wikipedia.org/wiki?curid=40375", "title": "Space technology", "text": "Technology developed for use in Space exploration\nSpace technology is technology for use in outer space. Space technology includes space vehicles such as spacecraft, satellites, space stations and orbital launch vehicles; ; ; and a wide variety of .\nMany common everyday services for terrestrial use such as weather forecasting, remote sensing, satellite navigation systems, satellite television, and some long-distance communications systems critically rely on space infrastructure. Of the sciences, astronomy and Earth science benefit from space technology. New technologies originating with or accelerated by space-related endeavors are often subsequently exploited in other economic activities.\nHistory of space technology.\nThe first country on Earth to put any technology into space was the Soviet Union. The Soviet Union sent the Sputnik 1 satellite on October 4, 1957. It weighed about , and is believed to have orbited around the globe. Analysis of the radio signals was used to gather information about the electron density of the ionosphere, while temperature and pressure data was encoded in the duration of radio beeps.\nThe first successful human spaceflight was \"Vostok 1\", carrying 27-year-old Soviet cosmonaut Yuri Gagarin in April 1961. The entire mission was controlled by either automatic systems or by ground control. This was because medical staff and spacecraft engineers were unsure how a human might react to weightlessness, and therefore it was decided to lock the pilot's manual controls.\nThe first probe to impact the surface of the Moon was the Soviet probe Luna 2, which made a hard landing on September 14, 1959. The far side of the Moon was first photographed on October 7, 1959, by the Soviet probe Luna 3.s\nOn December 24, 1968, the crew of Apollo 8, Frank Borman, James Lovell and William Anders, became the first human beings to enter lunar orbit and see the far side of the Moon in person. Humans first landed on the Moon on July 20, 1969. The first human to walk on the lunar surface was Neil Armstrong, commander of Apollo 11. The first space probe to land on moon South Pole of India.Chandrayaan-3 was launched aboard an LVM3-M4 rocket on 14 July 2023, at 09:05 UTC from Satish Dhawan Space Centre Second Launch Pad in Sriharikota, Andhra Pradesh, India, entering an Earth parking orbit with a perigee of 170 km (106 mi) and an apogee of 36,500 km (22,680 mi).\nApollo 11 was followed by Apollo 12, 14, 15, 16, and 17. Apollo 13 had a failure of the Apollo service module, but passed the far side of the Moon at an altitude of above the lunar surface, and 400,171\u00a0km (248,655\u00a0mi) from Earth, marking the record for the farthest humans traveled from Earth in 1970.\nThe first robotic lunar rover to land on the Moon was the Soviet vessel Lunokhod 1 on November 17, 1970, as part of the Lunokhod program. To date, the last human to stand on the Moon was Eugene Cernan, who, as part of the Apollo 17 mission, walked on the Moon in December 1972. Apollo 17 was followed by several uncrewed interplanetary missions operated by NASA. Also Technological innovations in space exploration have important effects on the economy, society, and the environment.\nEconomically, new safety features and technology have made space missions cheaper. Using reusable rockets helps companies save money because they do not need to fix or replace rockets as often. This makes space exploration more affordable and encourages more people to invest in the space industry.\nSocially, these new technologies have created many jobs in areas like engineering, research, and aerospace manufacturing. The growth of the space industry also helps other industries, such as telecommunications and materials engineering, by creating new job opportunities.\nEnvironmentally, using reusable rockets helps reduce space debris. By reusing rockets, space agencies can produce less waste and lessen the impact of space missions on the environment. This approach supports cleaner space exploration and a more sustainable future.\nIn summary, technological advances in space exploration positively affect the economy, create jobs, and promote environmental sustainability, helping the field continue to grow.\nOne of the notable interplanetary missions is \"Voyager 1\", the first artificial object to leave the Solar System into interstellar space on August 25, 2012. It is also the most distant artificial object from Earth. The probe passed the heliopause at 121 AU to enter interstellar space. \"Voyager 1\" is currently at a distance of (21.708 billion kilometers; 13.489 billion miles) from Earth as of January 1, 2019.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "40376", "revid": "212624", "url": "https://en.wikipedia.org/wiki?curid=40376", "title": "Cathedral and the Bazaar", "text": ""}
