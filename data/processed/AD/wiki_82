{"id": "41458", "revid": "329764", "url": "https://en.wikipedia.org/wiki?curid=41458", "title": "Optical disc", "text": "Flat, usually circular disc that encodes binary data\nAn optical disc is a flat, usually disc-shaped object that stores information in the form of physical variations on its surface that can be read with the aid of a beam of light. Optical discs can be reflective, where the light source and detector are on the same side of the disc, or transmissive, where light shines through the disc to be detected on the other side. They may contain analog or digital information, or a mixture of the two. Their main uses are the distribution of media and data, and long-term archival storage.\nDesign and technology.\nThe encoding material sits atop a thicker substrate (usually polycarbonate) that makes up the bulk of the disc and forms a dust defocusing layer. The encoding pattern follows a continuous, spiral path covering the entire disc surface and extending from the innermost track to the outermost track.\nThe data are stored on the disc with a laser or stamping machine, and can be accessed when the data path is illuminated with a laser diode in an optical disc drive that spins the disc at speeds of about 200 to 4,000 RPM or more, depending on the drive type, disc format, and the distance of the read head from the center of the disc (outer tracks are read at a higher data speed due to higher linear velocities at the same angular velocities).\nMost optical discs exhibit a characteristic iridescence as a result of the diffraction grating formed by their grooves. This side of the disc contains the actual data and is typically coated with a transparent material, usually lacquer.\nThe reverse side of an optical disc usually has a printed label, sometimes made of paper but often printed or stamped onto the disc itself. Unlike the 3&lt;templatestyles src=\"Fraction/styles.css\" /&gt;1\u20442-inch floppy disk, most optical discs do not have an integrated protective casing and are therefore susceptible to data transfer problems due to scratches, fingerprints, and other environmental problems. Blu-rays have a coating called durabis that mitigates these problems.\nOptical discs have been offered between in diameter, with becoming the dominant size beginning 1997. The so-called \"program area\" that contains the data commonly starts 25 millimeters away from the center point. A typical disc is about thick, while the track pitch (distance from the center of one track to the center of the next) ranges from 1.6 \u03bcm (for CDs) to 320 nm (for Blu-ray discs).\nRecording types.\nAn optical disc is designed to support one of three recording types: read-only (such as CD and CD-ROM), recordable (write-once, like CD-R), or re-recordable (rewritable, like CD-RW). Write-once optical discs commonly have an organic dye (may also be a (phthalocyanine) azo dye, mainly used by Verbatim, or an oxonol dye, used by Fujifilm) recording layer between the substrate and the reflective layer. Rewritable discs typically contain an alloy recording layer composed of a phase change material, most often AgInSbTe, an alloy of silver, indium, antimony, and tellurium. Azo dyes were introduced in 1996 and phthalocyanine only began to see wide use in 2002. The type of dye and the material used on the reflective layer on an optical disc may be determined by shining a light through the disc, as different dye and material combinations have different colors.\nBlu-ray Disc recordable discs do not usually use an organic dye recording layer, instead using an inorganic recording layer. Those that do are known as low-to-high (LTH) discs and can be made in existing CD and DVD production lines, but are of lower quality than traditional Blu-ray recordable discs.\nFile systems.\nFile systems specifically created for optical discs are ISO9660 and the Universal Disk Format (UDF).\nISO9660 can be extended using the \"Joliet\" extension to store longer file names than standalone ISO9660. The \"Rock Ridge\" extension can store even longer file names and Unix/Linux-style file permissions, but is not recognized by Windows and by DVD players and similar devices that can read data discs.\nFor cross-platform compatibility, multiple file systems can co-exist on one disc and reference the same files.\nUsage.\nOptical discs are most commonly used for digital preservation, storing music (particularly for use in a CD player), video (such as for use in a Blu-ray player), or data and programs for personal computers (PC), as well as offline hard copy data distribution due to lower per-unit prices than other types of media. The Optical Storage Technology Association (OSTA) promoted standardized optical storage formats.\nLibraries and archives enact optical media preservation procedures to ensure continued usability in the computer's optical disc drive or corresponding disc player.\nFile operations of traditional mass storage devices such as flash drives, memory cards and hard drives can be simulated using a UDF live file system.\nFor computer data backup and physical data transfer, optical discs such as CDs and DVDs are gradually being replaced with faster, smaller solid-state devices, especially the USB flash drive. This trend is expected to continue as USB flash drives continue to increase in capacity and drop in price.\nAdditionally, music, movies, games, software and TV shows purchased, shared or streamed over the Internet has significantly reduced the number of audio CDs, video DVDs and Blu-ray discs sold annually. However, audio CDs and Blu-rays are still preferred and bought by some, as a way of supporting their favorite works while getting something tangible in return and also since audio CDs (alongside vinyl records and cassette tapes) contain uncompressed audio without the artifacts introduced by lossy compression algorithms like MP3, and Blu-rays offer better image and sound quality than streaming media, without visible compression artifacts, due to higher bitrates and more available storage space. However, Blu-rays may sometimes be torrented over the internet, but torrenting may not be an option for some, due to restrictions put in place by ISPs on legal or copyright grounds, low download speeds or not having enough available storage space, since the content may weigh up to several dozen gigabytes. Blu-rays may be the only option for those looking to play large games without having to download them over an unreliable or slow internet connection, which is the reason why they are still (as of 2020) widely used by gaming consoles, like the PlayStation 4 and Xbox One X. As of 2020, it is unusual for PC games to be available in a physical format like Blu-ray.\nOptical discs are typically stored in special cases, sometimes called \"jewel cases\". Discs should not have any stickers and should not be stored together with paper; papers must be removed from the jewel case before storage. Discs should be handled by the edges to prevent scratching, with the thumb on the inner edge of the disc. The ISO Standard 18938:2014 is about best optical disc handling techniques. Optical disc cleaning should never be done in a circular pattern, to avoid concentric cirles from forming on the disc. Improper cleaning can scratch the disc. Recordable discs should not be exposed to light for extended periods of time. Optical discs should be stored in dry and cool conditions to increase longevity, with temperatures between -10 and 23\u00a0\u00b0C, never exceeding 32\u00a0\u00b0C, and with humidity never falling below 10%, with recommended storage at 20 to 50% of humidity without fluctuations of more than \u00b110%.\nDurability.\nAlthough optical discs are more durable than earlier audio-visual and data storage formats, they are susceptible to environmental and daily-use damage, if handled improperly.\nOptical discs are not prone to uncontrollable catastrophic failures such as head crashes, power surges, or exposure to water like hard disk drives and flash storage, since optical drives' storage controllers are not tied to optical discs themselves like with hard disk drives and flash memory controllers, and a disc is usually recoverable from a defective optical drive by pushing an unsharp needle into the emergency ejection pinhole, and has no point of immediate water ingress and no integrated circuitry.\nSecurity.\nAs the media itself only is accessed through a laser beam and has no internal control circuitry, it cannot contain malicious hardware in the same way as so-called \"rubber-duckies\" or \"USB killers\". Like any data storage media, optical discs can contain malicious \"data,\" they are able to contain and spread malware - as happened in the case of the Sony BMG copy protection rootkit scandal in 2005 where Sony misused discs by pre-loading them with malware.\nMany types of optical discs are factory-pressed or finalized write once read many storage devices and would therefore not be effective at spreading computer worms that are designed to spread by copying themselves onto optical media, because data on those discs can not be modified once pressed or written. However, re-writable disc technologies (such as CD-RW) \"are\" able to spread this type of malware.\nHistory.\nThe first recorded historical use of an optical disc was in 1884 when Alexander Graham Bell, Chichester Bell and Charles Sumner Tainter recorded sound on a glass disc using a beam of light.\nOptophonie is a very early (1931) example of a recording device using light for both recording and playing back sound signals on a transparent photograph.\nAn early analogue optical disc system existed in 1935, used on Welte's Lichttonorgel sampling organ.\nAn early analog optical disc used for video recording was invented by David Paul Gregg in 1958 and patented in the US in 1961 and 1969. This form of optical disc was a very early form of the DVD (https://). It is of special interest that https://, filed 1989, issued 1990, generated royalty income for Pioneer Corporation's DVA until 2007 \u2014then encompassing the CD, DVD, and Blu-ray systems. In the early 1960s, the Music Corporation of America bought Gregg's patents and his company, Gauss Electrophysics.\nAmerican inventor James T. Russell has been credited with inventing the first system to record a digital signal on an optical transparent foil that is lit from behind by a high-power halogen lamp. Russell's patent application was first filed in 1966 and he was granted a patent in 1970. Following litigation, Sony and Philips licensed Russell's patents (then held by a Canadian company, Optical Recording Corp.) in the 1980s.\nBoth Gregg's and Russell's disc are floppy media read in transparent mode, which imposes serious drawbacks, after this were developed four generations of optical drive that includes Laserdisc (1969), WORM (1979), Compact Discs (1984), DVD (1995), Blu-ray (2005), HD-DVD (2006), more formats are currently under development.\nFirst-generation.\nAt the start optical discs were read-only media used to store broadcast-quality analog video, and later digital media such as music or computer software. The LaserDisc format stored analog video signals for the distribution of home video, but commercially lost to the VHS videocassette format, due mainly to its high cost and non-re-recordability; other first-generation disc formats were designed only to store digital data and were not initially capable of use as a digital video medium.\nMost first-generation disc devices had an infrared laser reading head. The minimum size of the laser spot is proportional to the wavelength of the laser, so wavelength is a limiting factor upon the amount of information that can be stored in a given physical area on the disc. The infrared range is beyond the long-wavelength end of the visible light spectrum, so it supports less density than shorter-wavelength visible light. One example of high-density data storage capacity, achieved with an infrared laser, is 700\u00a0MB of net user data for a 12\u00a0cm compact disc.\nOther factors that affect data storage density include: the existence of multiple layers of data on the disc, the method of rotation (Constant linear velocity (CLV), Constant angular velocity (CAV), or zoned-CAV), the composition of lands and pits, and how much margin is unused is at the center and the edge of the disc.\nSony and Philips developed the first generation of the CDs in the mid-1980s with the complete specifications for these devices. With the help of this kind of technology the possibility of representing the analog signal into digital signal was exploited to a great level. For this purpose, the 16-bit samples of the analog signal were taken at the rate of 44,100 samples per second. This sample rate was based on the Nyquist rate of 40,000 samples per second required to capture the audible frequency range to 20\u00a0kHz without aliasing, with an additional tolerance to allow the use of less-than-perfect analog audio pre-filters to remove any higher frequencies. The first version of the standard allowed up to 74 minutes of music or 650\u00a0MB of data storage.\nTypes of Read-only Optical Discs:\nLaserdisc.\nIn the Netherlands in 1969, Philips Research physicist, Pieter Kramer invented an optical videodisc in reflective mode with a protective layer read by a focused laser beam https://, filed 1972, issued 1991. Kramer's physical format is used in all optical discs.\nIn 1975, Philips and MCA began to work together, and in 1978, commercially much too late, they presented their long-awaited Laserdisc in Atlanta. MCA delivered the discs and Philips the players. However, the presentation was a commercial failure, and the cooperation ended.\nIn Japan and the U.S., Pioneer succeeded with the Laserdisc until the advent of the DVD. In 1979, Philips and Sony, in consortium, successfully developed the audio compact disc.\nCD-ROM.\nThe CD-ROM format was developed by Sony and Philips, introduced in 1984, as an extension of Compact Disc Digital Audio and adapted to hold any form of digital data. The same year, Sony demonstrated a LaserDisc data storage format, with a larger data capacity of 3.28\u00a0GB.\nTypes of recordable Optical Discs\nMagneto-optical drive.\nMagneto-optical discs are erasable media, they can be written and read many times; the media and drives were first introduced in late 1987 and early 1988 by Sharp, MCI, Sony and others, all using SCSI interface. Capacity ranged from 512 MB on 130\u00a0mm media to 160 Mb on 90\u00a0mm media. By 1998 there were more than 50 models offered by 12 vendors with media of 86 and 130\u00a0mm in diameter and offering capacities up to 2,600 MB, almost all using SCSI interface.\nWORM drive.\nIn 1979, Exxon STAR Systems in Pasadena, CA built a computer controlled WORM drive that utilized thin film coatings of Tellurium and Selenium on a 12\" diameter glass disk. The recording system utilized blue light at 457\u00a0nm to record and red light at 632.8\u00a0nm to read. STAR Systems was bought by Storage Technology Corporation (STC) in 1981 and moved to Boulder, CO. Development of the WORM technology was continued using 14\" diameter aluminum substrates. Beta testing of the disk drives, originally labeled the Laser Storage Drive 2000 (LSD-2000), was only moderately successful. Many of the disks were shipped to RCA Laboratories (now David Sarnoff Research Center) to be used in the Library of Congress archiving efforts. The STC disks utilized a sealed cartridge with an optical window for protection https://.\nSecond-generation.\nSecond-generation optical discs were for storing great amounts of data, including broadcast-quality digital video. Such discs usually are read with a visible-light laser (usually red); the shorter wavelength and greater numerical aperture allow a narrower light beam, permitting smaller pits and lands in the disc. In the DVD format, this allows 4.7\u00a0GB storage on a standard 12\u00a0cm, single-sided, single-layer disc; alternatively, smaller media, such as the DataPlay format, can have capacity comparable to that of the larger, standard compact 12\u00a0cm disc.\nDVD-ROM.\nIn 1995, a consortium of manufacturers (Sony, Philips, Toshiba, Panasonic) developed the second generation of the optical disc, the DVD. The DVD disc appeared after the CD-ROM had become widespread in society.\nThird-generation.\nThird-generation optical discs are used for distributing high-definition video and videogames and support greater data storage capacities, accomplished with short-wavelength visible-light lasers and greater numerical apertures. Blu-ray Disc and HD DVD uses blue-violet lasers and focusing optics of greater aperture, for use with discs with smaller pits and lands, thereby greater data storage capacity per layer.\nIn practice, the effective multimedia presentation capacity is improved with enhanced video data compression codecs such as H.264/MPEG-4 AVC and VC-1.\nAnnounced but not released:\nBlu-ray and HD-DVD.\nThe third generation optical disc was developed in 2000\u20132006 and was introduced as Blu-ray Disc. First movies on Blu-ray Discs were released in June 2006. Blu-ray eventually prevailed in a high definition optical disc format war over a competing format, the HD DVD. A standard Blu-ray disc can hold about 25\u00a0GB of data, a DVD about 4.7\u00a0GB, and a CD about 700\u00a0MB.\nFourth-generation.\nThe following formats go beyond the current third-generation discs and have the potential to hold more than one terabyte (1 TB) of data and at least some are meant for cold data storage in data centers:\nAnnounced but abandoned:\nAnnounced but not released:\nIn 2004, development of the Holographic Versatile Disc (HVD) commenced, which promised the storage of several terabytes of data per disc. However, development stagnated towards the late 2000s due to lack of funding.\nIn 2006, it was reported that Japanese researchers developed ultraviolet ray lasers with a wavelength of 210 nanometers, which would enable a higher bit density than Blu-ray discs. As of 2022, no updates on that project have been reported.\nRecordable and writable optical discs.\nThere are numerous formats of optical direct to disk recording devices on the market, all of which are based on using a laser to change the reflectivity of the digital recording medium in order to duplicate the effects of the pits and lands created when a commercial optical disc is pressed. Formats such as CD-R and DVD-R are \"Write once read many\" or write-once, while CD-RW and DVD-RW are rewritable, more like a magnetic recording hard disk drive (HDD).\nMedia technologies vary, for example, M-DISC media uses a rock-like layer to retain data for longer than conventional recordable media. While being read-only compatible with existing DVD and Blu-ray drives, M-DISC media can only be written to using a stronger laser specifically made for this purpose, which is built into fewer optical drive models.\nSurface error scanning.\nOptical media can predictively be scanned for errors and media deterioration well before any data becomes unreadable. Optical formats include some redundancy for error correction, which works until the amount of error exceeds a threshold. A higher rate of errors may indicate deteriorating and/or low quality media, physical damage, an unclean surface and/or media written using a defective optical drive.\nPrecise error scanning requires access to the raw, uncorrected readout of a disc, which is not always provided by a drive. As a result, support of this functionality varies per optical drive manufacturer and model. On ordinary drives without this functionality, it is possible to still look for unexpected reduction in read speed as an indirect, much less reliable measure.\nOptical media, such as CDs and DVDs, can be scanned to detect errors and signs of deterioration well before data becomes unreadable. These formats include built-in error correction mechanisms, which function by adding redundant data. However, once the rate of errors surpasses the correction threshold, the media becomes vulnerable to failure. A high error rate can signal physical deterioration, low-quality manufacturing, surface contamination, or data recorded by a faulty optical drive.\nAccurate error scanning requires access to a disc's raw, uncorrected readout. However, not all optical drives provide this capability, and support for this feature can vary significantly between manufacturers and drive models. On drives lacking raw data access, users may rely on a less precise method: monitoring unexpected reductions in read speed, though this is a far less reliable indicator of disc health.\nSeveral specialized tools are available for performing error scans on optical media. Popular programs include Nero DiscSpeed, K-Probe, Opti Drive Control (previously known as \"CD Speed 2000\"), and DVD Info Pro for Windows. For cross-platform users, QPxTool is available to help monitor and maintain optical media integrity. Each of these tools allows for detailed analysis of the error rates and conditions affecting optical discs.\nError types.\nThere are different types of error measurements, including so-called \"C1\", \"C2\" and \"CU\" errors on CDs, and \"PI/PO (parity inner/outer) errors\" and the more critical \"PI/PO failures\" on DVDs. Finer-grain error measurements on CDs supported by very few optical drives are called \"E11\", \"E21\", \"E31\", \"E21\", \"E22\", \"E32\".\n\"CU\" and \"POF\" represent uncorrectable errors on data CDs and DVDs respectively, thus data loss, and can be a result of too many consecutive smaller errors.\nDue to the weaker error correction used on Audio CDs (Red Book standard) and Video CDs (White Book standard), C2 errors already lead to data loss. However, even with C2 errors, the damage is inaudible to some extent.\nBlu-ray discs use so-called \"LDC\" (\"Long Distance Code\"s) and \"BIS\" (\"Burst Indication Subcode\"s) error parameters. According to the developer of the \"Opti Drive Control\" software, a disc can be considered healthy at an \"LDC\" error rate below 13 and \"BIS\" error rate below 15.\nOptical disc manufacturing.\nOptical discs are made using replication. This process can be used with all disc types. Recordable discs have pre-recorded vital information, like manufacturer, disc type, maximum read and write speeds, etc. In replication, a cleanroom with yellow light is necessary to protect the light-sensitive photoresist and to prevent dust from corrupting the data on the disc.\nA glass master is used in replication. The master is placed in a machine that cleans it as much as possible using a rotating brush and deionized water, preparing it for the next step. In the next step, a surface analyzer inspects the cleanliness of the master before photoresist is applied on the master.\nThe photoresist is then baked in an oven to solidify it. Then, in the exposure process, the master is placed in a turntable where a laser selectively exposes the resist to light. At the same time, a developer and deionized water are applied to the disc to remove the exposed resist. This process forms the pits and lands that represent the data on the disc.\nA thin coating of metal is then applied to the master, making a negative of the master with the pits and lands in it. The negative is then peeled off the master and coated in a thin layer of plastic. The plastic protects the coating while a punching press punches a hole into the center of the disc, and punches excess material.\nThe negative is now a stamper - a part of the mold that will be used for replication. It is placed on one side of the mold with the data side containing the pits and lands facing out. This is done inside an injection molding machine. The machine then closes the mold and injects polycarbonate in the cavity formed by the walls of the mold, which forms or molds the disc with the data on it.\nThe molten polycarbonate fills the pits or spaces between the lands on the negative, acquiring their shape when it solidifies. This step is somewhat similar to record pressing.\nThe polycarbonate disc cools quickly and is promptly removed from the machine, before forming another disc. The disc is then metallized, covered with a thin reflective layer of aluminum. The aluminum fills the space once occupied by the negative.\nA layer of varnish is then applied to protect the aluminum coating and provide a surface suitable for printing. The varnish is applied near the center of the disc, and the disc is spun, evenly distributing the varnish on the surface of the disc. The varnish is hardened using UV light. The discs are then silkscreened or a label is otherwise applied.\nRecordable discs add a dye layer, and rewritable discs add a phase change alloy layer instead, which is protected by upper and lower dielectric (electrically insulating) layers. The layers may be sputtered. The additional layer is between the grooves and the reflective layer of the disc. Grooves are made in recordable discs in place of the traditional pits and lands found in replicated discs, and the two can be made in the same exposure process. In DVDs, the same processes as in CDs are carried out, but in a thinner disc. The thinner disc is then bonded to a second, equally thin but blank, disc using UV-curable Liquid optically clear adhesive, forming a DVD disc. This leaves the data in the middle of the disc, which is necessary for DVDs to achieve their storage capacity. In multi layer discs, semi reflective instead of reflective coatings are used for all layers except the last layer, which is the deepest one and uses a traditional reflective coating.\nDual layer DVDs are made slightly differently. After metallization (with a thinner metal layer to allow some light to pass through), base and pit transfer resins are applied and pre-cured in the center of the disc. Then the disc is pressed again using a different stamper, and the resins are completely cured using UV light before being separated from the stamper. Then the disc receives another, thicker metallization layer, and is then bonded to the blank disc using LOCA glue. DVD-R DL and DVD+R DL discs receive a dye layer after curing, but before metallization. CD-R, DVD-R, and DVD+R discs receive the dye layer after pressing but before metallization. CD-RW, DVD-RW and DVD+RW receive a metal alloy layer sandwiched between 2 dielectric layers. HD-DVD is made in the same way as DVD. In recordable and rewritable media, most of the stamper is composed of grooves, not pits and lands. The grooves contain a wobble frequency that is used to locate the position of the reading or writing laser on the disc. DVDs use pre-pits instead, with a constant frequency wobble.\nBlu-ray.\n\"HTL\" (high-to-low type) Blu-ray discs are made differently. First, a silicon wafer is used instead of a glass master. The wafer is processed in the same way a glass master would.\nThe wafer is then electroplated to form a 300-micron thick nickel stamper, which is peeled off from the wafer. The stamper is mounted onto a mold inside a press or embosser.\nThe polycarbonate discs are molded in a similar fashion to DVD and CD discs. If the discs being produced are BD-Rs or BD-REs, the mold is fitted with a stamper that stamps a groove pattern onto the discs, in lieu of the pits and lands found on BD-ROM discs.\nAfter cooling, a 35 nanometre-thick layer of silver alloy is applied to the disc using sputtering. Then the second layer is made by applying base and pit transfer resins to the disc, and are pre-cured in its center.\nAfter application and pre-curing, the disc is pressed or embossed using a stamper and the resins are immediately cured using intense UV light, before the disc is separated from the stamper. The stamper contains the data that will be transferred to the disc. This process is known as embossing and is the step that engraves the data onto the disc, replacing the pressing process used in the first layer, and it is also used for multi layer DVD discs.\nThen, a 30 nanometre-thick layer of silver alloy is then sputtered onto the disc and the process is repeated as many times as required. Each repetition creates a new data layer. (The resins are applied again, pre-cured, stamped (with data or grooves) and cured, silver alloy is sputtered and so on)\nBD-R and BD-RE discs receive (through sputtering) a metal (recording layer) alloy (that is sandwiched between two dielectric layers, also sputtered, in BD-RE), before receiving the 30 nanometre metallization (silver alloy, aluminum or gold) layer, which is sputtered. Alternatively, the silver alloy may be applied before the recording layer is applied. Silver alloys are usually used in Blu-rays, and aluminum is usually used on CDs and DVDs. Gold is used in some \"Archival\" CDs and DVDs, since it is more chemically inert and resistant to corrosion than aluminum, which corrodes into aluminum oxide, which can be seen in disc rot as transparent patches or dots in the disc, that prevent the disc from being read, since the laser light passes through the disc instead of being reflected back into the laser pickup assembly to be read. Normally, aluminum does not corrode since it has a thin oxide layer that forms on contact with oxygen. In this case, it can corrode due to its thinness.\nThen, the 98 micron-thick cover layer is applied using UV-curable liquid optically clear adhesive, and a 2 micron-thick hard coat (such as Durabis) is also applied and cured using UV light. In the last step, a 10 nanometre-thick silicon nitride barrier layer is applied to the label side of the disc to protect against humidity. Blu-rays have their data very close to the read surface of the disc, which is necessary for Blu-rays to achieve their capacity.\nDiscs in large quantities can either be replicated or duplicated. In replication, the process explained above is used to make the discs, while in duplication, CD-R, DVD-R or BD-R discs are recorded and finalized to prevent further recording and allow for wider compatibility. (See Optical disc authoring). The equipment is also different: replication is carried out by fully automated purpose-built machinery whose cost is in the hundreds of thousands of US dollars in the used market, while duplication can be automated (using what's known as an autoloader) or be done by hand, and only requires a small tabletop duplicator.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41459", "revid": "12270500", "url": "https://en.wikipedia.org/wiki?curid=41459", "title": "Normandie-Niemen", "text": ""}
{"id": "41460", "revid": "7903804", "url": "https://en.wikipedia.org/wiki?curid=41460", "title": "Optical isolator", "text": "Optical component allowing the transmission of light in only one direction\nAn optical isolator, or optical diode, is an optical component which allows the transmission of light in only one direction. It is typically used to prevent unwanted feedback into an optical oscillator, such as a laser cavity.\nThe operation of conventional optical isolators relies on the Faraday effect (which in turn is produced by magneto-optic effect), which is used in the main component, the Faraday rotator. However, integrated isolators which do not rely on magnetism have been made in recent years too. \nTheory.\nThe main component of the optical isolator is the Faraday rotator. The magnetic field, formula_1, applied to the Faraday rotator causes a rotation in the polarization of the light due to the Faraday effect. The angle of rotation, formula_2, is given by,\nformula_3,\nwhere, formula_4 is the Verdet constant of the material (amorphous or crystalline solid, or liquid, or crystalline liquid, or vaprous, or gaseous) of which the rotator is made, and formula_5 is the length of the rotator. This is shown in Figure 2. Specifically for an optical isolator, the values are chosen to give a rotation of 45\u00b0.\nIt has been shown that a crucial requirement for any kind of optical isolator (not only the Faraday isolator) is some kind of non-reciprocal optics \nPolarization dependent isolator.\nThe polarization dependent isolator, or Faraday isolator, is made of three parts, an input polarizer (polarized vertically), a Faraday rotator, and an output polarizer, called an analyzer (polarized at 45\u00b0).\nLight traveling in the forward direction becomes polarized vertically by the input polarizer. The Faraday rotator will rotate the polarization by 45\u00b0. The analyzer then enables the light to be transmitted through the isolator.\nLight traveling in the backward direction becomes polarized at 45\u00b0 by the analyzer. The Faraday rotator will again rotate the polarization by 45\u00b0. This means the light is polarized horizontally (the direction of rotation is not sensitive to the direction of propagation). Since the polarizer is vertically aligned, the light will be extinguished.\nFigure 2 shows a Faraday rotator with an input polarizer, and an output analyzer. For a polarization dependent isolator, the angle between the polarizer and the analyzer, formula_2, is set to 45\u00b0. The Faraday rotator is chosen to give a 45\u00b0 rotation.\nPolarization dependent isolators are typically used in free space optical systems. This is because the polarization of the source is typically maintained by the system. In optical fibre systems, the polarization direction is typically dispersed in non polarization maintaining systems. Hence the angle of polarization will lead to a loss.\nPolarization independent isolator.\nThe polarization independent isolator is made of three parts, an input birefringent wedge (with its ordinary polarization direction vertical and its extraordinary polarization direction horizontal), a Faraday rotator, and an output birefringent wedge (with its ordinary polarization direction at 45\u00b0, and its extraordinary polarization direction at \u221245\u00b0).\nLight traveling in the forward direction is split by the input birefringent wedge into its vertical (0\u00b0) and horizontal (90\u00b0) components, called the ordinary ray (o-ray) and the extraordinary ray (e-ray) respectively. The Faraday rotator rotates both the o-ray and e-ray by 45\u00b0. This means the o-ray is now at 45\u00b0, and the e-ray is at \u221245\u00b0. The output birefringent wedge then recombines the two components.\nLight traveling in the backward direction is separated into the o-ray at 45, and the e-ray at \u221245\u00b0 by the birefringent wedge. The Faraday Rotator again rotates both the rays by 45\u00b0. Now the o-ray is at 90\u00b0, and the e-ray is at 0\u00b0. Instead of being focused by the second birefringent wedge, the rays diverge.\nTypically collimators are used on either side of the isolator. In the transmitted direction the beam is split and then combined and focused into the output collimator. In the isolated direction the beam is split, and then diverged, so it does not focus at the collimator.\nFigure 3 shows the propagation of light through a polarization independent isolator. The forward travelling light is shown in blue, and the backward propagating light is shown in red. The rays were traced using an ordinary refractive index of 2, and an extraordinary refractive index of 3. The wedge angle is 7\u00b0.\nThe Faraday rotator.\nThe most important optical element in an isolator is the Faraday rotator. The characteristics that one looks for in a Faraday rotator optic include a high Verdet constant, low absorption coefficient, low non-linear refractive index and high damage threshold. Also, to prevent self-focusing and other thermal related effects, the optic should be as short as possible. The two most commonly used materials for the 700\u20131100\u00a0nm range are terbium doped borosilicate glass and terbium gallium garnet crystal (TGG). For long distance fibre communication, typically at 1310\u00a0nm or 1550\u00a0nm, yttrium iron garnet crystals are used (YIG). Commercial YIG based Faraday isolators reach isolations higher than 30 dB.\nOptical isolators are different from 1/4 wave plate based isolators because the Faraday rotator provides non-reciprocal rotation while maintaining linear polarization. That is, the polarization rotation due to the Faraday rotator is always in the same relative direction. So in the forward direction, the rotation is positive 45\u00b0. In the reverse direction, the rotation is \u221245\u00b0. This is due to the change in the relative magnetic field direction, positive one way, negative the other. This then adds to a total of 90\u00b0 when the light travels in the forward direction and then the negative direction. This allows the higher isolation to be achieved.\nOptical isolators and thermodynamics.\nIt might seem at first glance that a device that allows light to flow in only one direction would violate Kirchhoff's law and the second law of thermodynamics, by allowing light energy to flow from a cold object to a hot object and blocking it in the other direction, but the violation is avoided because the isolator must absorb (not reflect) the light from the hot object and will eventually reradiate it to the cold one. Attempts to re-route the photons back to their source unavoidably involve creating a route by which other photons can travel from the hot body to the cold one, avoiding the paradox."}
{"id": "41461", "revid": "26394783", "url": "https://en.wikipedia.org/wiki?curid=41461", "title": "Optical path length", "text": "Product of geometric length and refractive index\nIn optics, optical path length (OPL, denoted \u039b in equations), also known as optical length or optical distance, is the length that light needs to travel through a vacuum to create the same phase difference as it would have when traveling through a given medium. For a homogeneous medium through which the light ray propagates, it is calculated as taking the product of the geometric length of the optical path followed by light and the refractive index of the medium. For inhomogeneous optical media, the product above is generalized as a path integral as part of the ray tracing procedure. A difference in OPL between two paths is often called the optical path difference (OPD). OPL and OPD are important because they determine the phase of the light and govern interference and diffraction of light as it propagates.\nIn a medium of constant refractive index, \"n\", the OPL for a path of geometrical length \"s\" is just\nformula_1\nIf the refractive index varies along the path, the OPL is given by a line integral\nformula_2\nwhere \"n\" is the local refractive index as a function of distance along the path \"C\".\nAn electromagnetic wave propagating along a path \"C\" has the phase shift over \"C\" as if it was propagating a path in a vacuum, length of which, is equal to the optical path length of \"C\". Thus, if a wave is traveling through several different media, then the optical path length of each medium can be added to find the total optical path length. The optical path difference between the paths taken by two identical waves can then be used to find the phase change. Finally, using the phase change, the interference between the two waves can be calculated.\nFermat's principle states that the path light takes between two points is the path that has the minimum optical path length.\nOptical path difference.\nThe optical path difference (OPD) corresponds to the phase shift undergone by the light emitted from two previously coherent sources when passed through mediums of different refractive indices. For example, a wave passing through air appears to travel a shorter distance than an identical wave traveling the same distance in glass. This is because a larger number of wavelengths fit in the same distance due to the higher refractive index of the glass.\nThe OPD can be calculated from the following equation:\nformula_3\nwhere \"d\"1 and \"d\"2 are the distances of the ray passing through medium 1 or 2, \"n\"1 is the greater refractive index (e.g., glass) and \"n\"2 is the smaller refractive index (e.g., air)."}
{"id": "41462", "revid": "11677590", "url": "https://en.wikipedia.org/wiki?curid=41462", "title": "Optical power budget", "text": ""}
{"id": "41463", "revid": "42522270", "url": "https://en.wikipedia.org/wiki?curid=41463", "title": "Optical power margin", "text": "Difference in an optical communications link\nIn an optical communications link, the optical power margin is the difference between the optical power that is launched by a given transmitter into the fiber, less transmission losses from all causes, and the minimum optical power that is required by the receiver for a specified level of performance. An optical power margin is typically measured using a calibrated light source and an optical power meter.\nThe optical power margin is usually expressed in decibels (dB). At least several dB of optical power margin should be included in the optical power budget. The amount of optical power launched into a given fiber by a given transmitter depends on the nature of its active optical source (LED or laser diode) and the type of fiber, including such parameters as core diameter and numerical aperture. \nReferences.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41464", "revid": "1319443846", "url": "https://en.wikipedia.org/wiki?curid=41464", "title": "Visible spectrum", "text": "Portion of the electromagnetic spectrum that is visible to the human eye\nThe visible spectrum is the band of the electromagnetic spectrum that is visible to the human eye. Electromagnetic radiation in this range of wavelengths is called \"visible light\" (or simply light).\nThe optical spectrum is sometimes considered to be the same as the visible spectrum, but some authors define the term more broadly, to include the ultraviolet and infrared parts of the electromagnetic spectrum as well, known collectively as \"optical radiation\".\nA typical human eye will respond to wavelengths from about 380 to about 750 nanometers. In terms of frequency, this corresponds to a band in the vicinity of 400\u2013790\u00a0terahertz. These boundaries are not sharply defined and may vary per individual. Under optimal conditions, these limits of human perception can extend to 310\u00a0nm (ultraviolet) and 1100\u00a0nm (near infrared).\nThe spectrum does not contain all the colors that the human visual system can distinguish. \"Unsaturated colors\" such as pink, or purple variations like magenta, for example, are absent because they can only be made from a mix of multiple wavelengths. Colors containing only one wavelength are also called \"pure colors\" or spectral colors.\nVisible wavelengths pass largely unattenuated through the Earth's atmosphere via the \"optical window\" region of the electromagnetic spectrum. An example of this phenomenon is when clean air scatters blue light more than red light, and so the midday sky appears blue (apart from the area around the Sun which appears white because the light is not scattered as much). The optical window is also referred to as the \"visible window\" because it overlaps the human visible response spectrum. The near infrared (NIR) window lies just out of the human vision, as well as the medium wavelength infrared (MWIR) window, and the long-wavelength or far-infrared (LWIR or FIR) window, although other animals may perceive them.\nSpectral colors.\nColors that can be produced by visible light of a narrow band of wavelengths (monochromatic light) are called spectral colors. The various color ranges indicated in the illustration are an approximation: The spectrum is continuous, with no clear boundaries between one color and the next.\nHistory.\nIn the 13th century, Roger Bacon theorized that rainbows were produced by a similar process to the passage of light through glass or crystal.\nIn the 17th century, Isaac Newton discovered that prisms could disassemble and reassemble white light, and described the phenomenon in his book \"Opticks\". He was the first to use the word \"spectrum\" (Latin for \"appearance\" or \"apparition\") in this sense in print in 1671 in describing his experiments in optics. Newton observed that, when a narrow beam of sunlight strikes the face of a glass prism at an angle, some is reflected and some of the beam passes into and through the glass, emerging as different-colored bands. Newton hypothesized light to be made up of \"corpuscles\" (particles) of different colors, with the different colors of light moving at different speeds in transparent matter, red light moving more quickly than violet in glass. The result is that red light is bent (refracted) less sharply than violet as it passes through the prism, creating a spectrum of colors.\n Newton originally divided the spectrum into six named colors: red, orange, yellow, green, blue, and violet. He later added indigo as the seventh color since he believed that seven was a perfect number as derived from the ancient Greek sophists, of there being a connection between the colors, the musical notes, the known objects in the Solar System, and the days of the week. The human eye is relatively insensitive to indigo's frequencies, and some people who have otherwise-good vision cannot distinguish indigo from blue and violet. For this reason, some later commentators, including Isaac Asimov, have suggested that indigo should not be regarded as a color in its own right but merely as a shade of blue or violet. Evidence indicates that what Newton meant by \"indigo\" and \"blue\" does not correspond to the modern meanings of those color words. Comparing Newton's observation of prismatic colors with a color image of the visible light spectrum shows that \"indigo\" corresponds to what is today called blue, whereas his \"blue\" corresponds to cyan.\nIn the 18th century, Johann Wolfgang von Goethe wrote about optical spectra in his \"Theory of Colours\". Goethe used the word \"spectrum\" (\"Spektrum\") to designate a ghostly optical afterimage, as did Schopenhauer in \"On Vision and Colors\". Goethe argued that the continuous spectrum was a compound phenomenon. Where Newton narrowed the beam of light to isolate the phenomenon, Goethe observed that a wider aperture produces not a spectrum but rather reddish-yellow and blue-cyan edges with white between them. The spectrum appears only when these edges are close enough to overlap.\nIn the early 19th century, the concept of the visible spectrum became more definite, as light outside the visible range was discovered and characterized by William Herschel (infrared) and Johann Wilhelm Ritter (ultraviolet), Thomas Young, Thomas Johann Seebeck, and others.\nYoung was the first to measure the wavelengths of different colors of light, in 1802.\nThe connection between the visible spectrum and color vision was explored by Thomas Young and Hermann von Helmholtz in the early 19th century. Their theory of color vision correctly proposed that the eye uses three distinct receptors to perceive color.\nLimits to visible range.\nThe visible spectrum is limited to wavelengths that can both reach the retina and trigger visual phototransduction (excite a visual opsin). Insensitivity to UV light is generally limited by transmission through the lens. Insensitivity to IR light is limited by the spectral sensitivity functions of the visual opsins. The range is defined psychometrically by the luminous efficiency function, which accounts for all of these factors. In humans, there is a separate function for each of two visual systems, one for photopic vision, used in daylight, which is mediated by cone cells, and one for scotopic vision, used in dim light, which is mediated by rod cells. Each of these functions have different visible ranges. However, discussion on the visible range generally assumes photopic vision.\nAtmospheric transmission.\nThe visible range of most animals evolved to match the optical window, which is the range of light that can pass through the atmosphere. The ozone layer absorbs almost all UV light (below 315\u00a0nm). However, this only affects cosmic light (e.g. sunlight), not terrestrial light (e.g. Bioluminescence).\nOcular transmission.\nBefore reaching the retina, light must first transmit through the cornea and lens. UVB light (&lt;\u00a0315\u00a0nm) is filtered mostly by the cornea, and UVA light (315\u2013400\u00a0nm) is filtered mostly by the lens. The lens also yellows with age, attenuating transmission most strongly at the blue part of the spectrum. This can cause xanthopsia as well as a slight truncation of the short-wave (blue) limit of the visible spectrum. Subjects with aphakia are missing a lens, so UVA light can reach the retina and excite the visual opsins; this expands the visible range and may also lead to cyanopsia.\nOpsin absorption.\nEach opsin has a spectral sensitivity function, which defines how likely it is to absorb a photon of each wavelength. The luminous efficiency function is approximately the superposition of the contributing visual opsins. Variance in the position of the individual opsin spectral sensitivity functions therefore affects the luminous efficiency function and the visible range. For example, the long-wave (red) limit changes proportionally to the position of the L-opsin. The positions are defined by the peak wavelength (wavelength of highest sensitivity), so as the L-opsin peak wavelength blue shifts by 10\u00a0nm, the long-wave limit of the visible spectrum also shifts 10\u00a0nm. Large deviations of the L-opsin peak wavelength lead to a form of color blindness called protanomaly and a missing L-opsin (protanopia) shortens the visible spectrum by about 30\u00a0nm at the long-wave limit. Forms of color blindness affecting the M-opsin and S-opsin do not significantly affect the luminous efficiency function nor the limits of the visible spectrum.\nDifferent definitions.\nRegardless of actual physical and biological variance, the definition of the limits is not standard and will change depending on the industry. For example, some industries may be concerned with practical limits, so would conservatively report 420\u2013680\u00a0nm, while others may be concerned with psychometrics and achieving the broadest spectrum would liberally report 380\u2013750, or even 380\u2013800\u00a0nm. The luminous efficiency function in the NIR does not have a hard cutoff, but rather an exponential decay, such that the function's value (or vision sensitivity) at 1,050\u00a0nm is about 109 times weaker than at 700\u00a0nm; much higher intensity is therefore required to perceive 1,050\u00a0nm light than 700\u00a0nm light.\nVision outside the visible spectrum.\nUnder ideal laboratory conditions, subjects may perceive infrared light up to at least 1,064\u00a0nm. While 1,050\u00a0nm NIR light can evoke red, suggesting direct absorption by the L-opsin, there are also reports that pulsed NIR lasers can evoke green, which suggests two-photon absorption may be enabling extended NIR sensitivity.\nSimilarly, young subjects may perceive ultraviolet wavelengths down to about 310\u2013313\u00a0nm, but detection of light below 380\u00a0nm may be due to fluorescence of the ocular media, rather than direct absorption of UV light by the opsins. As UVA light is absorbed by the ocular media (lens and cornea), it may fluoresce and be released at a lower energy (longer wavelength) that can then be absorbed by the opsins. For example, when the lens absorbs 350\u00a0nm light, the fluorescence emission spectrum is centered on 440\u00a0nm.\nNon-visual light detection.\nIn addition to the photopic and scotopic systems, humans have other systems for detecting light that do not contribute to the primary visual system. For example, melanopsin has an absorption range of 420\u2013540\u00a0nm and regulates circadian rhythm and other reflexive processes. Since the melanopsin system does not form images, it is not strictly considered vision and does not contribute to the visible range.\nIn non-humans.\nThe visible spectrum is defined as that visible to humans, but the variance between species is large. Not only can cone opsins be spectrally shifted to alter the visible range, but vertebrates with 4 cones (tetrachromatic) or 2 cones (dichromatic) relative to humans' 3 (trichromatic) will also tend to have a wider or narrower visible spectrum than humans, respectively.\nVertebrates tend to have 1-4 different opsin classes:\nTesting the visual systems of animals behaviorally is difficult, so the visible range of animals is usually estimated by comparing the peak wavelengths of opsins with those of typical humans (S-opsin at 420\u00a0nm and L-opsin at 560\u00a0nm).\nMammals.\nMost mammals have retained only two opsin classes (LWS and VS), due likely to the nocturnal bottleneck. However, old world primates (including humans) have since evolved two versions in the LWS class to regain trichromacy. Unlike most mammals, rodents' UVS opsins have remained at shorter wavelengths. Along with their lack of UV filters in the lens, mice have a UVS opsin that can detect down to 340\u00a0nm. While allowing UV light to reach the retina can lead to retinal damage, the short lifespan of mice compared with other mammals may minimize this disadvantage relative to the advantage of UV vision. Dogs have two cone opsins at 429\u00a0nm and 555\u00a0nm, so see almost the entire visible spectrum of humans, despite being dichromatic. Horses have two cone opsins at 428\u00a0nm and 539\u00a0nm, yielding a slightly more truncated red vision.\nBirds.\nMost other vertebrates (birds, lizards, fish, etc.) have retained their tetrachromacy, including UVS opsins that extend further into the ultraviolet than humans' VS opsin. The sensitivity of avian UVS opsins vary greatly, from 355\u2013425\u00a0nm, and LWS opsins from 560\u2013570\u00a0nm. This translates to some birds with a visible spectrum on par with humans, and other birds with greatly expanded sensitivity to UV light. The LWS opsin of birds is sometimes reported to have a peak wavelength above 600\u00a0nm, but this is an effective peak wavelength that incorporates the filter of avian oil droplets. The peak wavelength of the LWS opsin alone is the better predictor of the long-wave limit. A possible benefit of avian UV vision involves sex-dependent markings on their plumage that are visible only in the ultraviolet range.\nFish.\nTeleosts (bony fish) are generally tetrachromatic. The sensitivity of fish UVS opsins vary from 347-383 nm, and LWS opsins from 500-570 nm. However, some fish that use alternative chromophores can extend their LWS opsin sensitivity to 625 nm. The popular belief that the common goldfish is the only animal that can see both infrared and ultraviolet light is incorrect, because goldfish cannot see infrared light.\nInvertebrates.\nThe visual systems of invertebrates deviate greatly from vertebrates, so direct comparisons are difficult. However, UV sensitivity has been reported in most insect species.\nBees and many other insects can detect ultraviolet light, which helps them find nectar in flowers. Plant species that depend on insect pollination may owe reproductive success to their appearance in ultraviolet light rather than how colorful they appear to humans. Bees' long-wave limit is at about 590\u00a0nm. Mantis shrimp exhibit up to 14 opsins, enabling a visible range of less than 300 nm to above 700 nm.\nThermal vision.\nSome snakes can \"see\" radiant heat at wavelengths between 5 and 30\u00a0\u03bcm to a degree of accuracy such that a blind rattlesnake can target vulnerable body parts of the prey at which it strikes, and other snakes with the organ may detect warm bodies from a meter away. It may also be used in thermoregulation and predator detection.\nSpectroscopy.\nSpectroscopy is the study of objects based on the spectrum of color they emit, absorb or reflect. Visible-light spectroscopy is an important tool in astronomy (as is spectroscopy at other wavelengths), where scientists use it to analyze the properties of distant objects. Chemical elements and small molecules can be detected in astronomical objects by observing emission lines and absorption lines. For example, helium was first detected by analysis of the spectrum of the Sun. The shift in frequency of spectral lines is used to measure the Doppler shift (redshift or blueshift) of distant objects to determine their velocities towards or away from the observer. Astronomical spectroscopy uses high-dispersion diffraction gratings to observe spectra at very high spectral resolutions.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41465", "revid": "15094012", "url": "https://en.wikipedia.org/wiki?curid=41465", "title": "Optical switch", "text": ""}
{"id": "41466", "revid": "27823944", "url": "https://en.wikipedia.org/wiki?curid=41466", "title": "Optical time-domain reflectometer", "text": "Optoelectronic instrument\nAn optical time-domain reflectometer (OTDR) is an optoelectronic instrument used to characterize an optical fiber. \nIt is the optical equivalent of an electronic time domain reflectometer which measures the impedance of the cable or transmission line under test. \nAn OTDR injects a series of optical pulses into the fiber under test and extracts, from the same end of the fiber, light that is scattered (Rayleigh backscatter) or reflected back from points along the fiber. The scattered or reflected light that is gathered back is used to characterize the optical fiber. The strength of the return pulses is measured and integrated as a function of time, and plotted as a function of length of the fiber.\nReliability and quality of OTDR equipment.\nThe reliability and quality of an OTDR is based on its accuracy, measurement range, ability to resolve and measure closely spaced events, measurement speed, and ability to perform satisfactorily under various environmental extremes and after various types of physical abuse. The instrument is also judged on the basis of its cost, features provided, size, weight, and ease of use.\nSome of the terms often used in specifying the quality of an OTDR are as follows:\nAccuracy: Defined as the correctness of the measurement i.e., the difference between the measured value and the true value of the event being measured.\nMeasurement range: Defined as the maximum attenuation that can be placed between the instrument and the event being measured, for which the instrument will still be able to measure the event within acceptable accuracy limits.\nInstrument resolution: Is a measure of how close two events can be spaced and still be recognized as two separate events. The duration of the measurement pulse and the data sampling interval create a resolution limitation for OTDRs. The shorter the pulse duration and the shorter the data sampling interval, the better the instrument resolution, but the shorter the measurement range. Resolution is also often limited when powerful reflections return to the OTDR and temporarily overload the detector. When this occurs, some time is required before the instrument can resolve a second fiber event. Some OTDR manufacturers use a \u201cmasking\u201d procedure to improve resolution. The procedure shields or \u201cmasks\u201d the detector from high-power fiber reflections, preventing detector overload and eliminating the need for detector recovery.\nIndustry requirements for the reliability and quality of OTDRs are specified in the Generic Requirements for Optical Time Domain Reflectometer (OTDR) Type Equipment.\nTypes of OTDR-like test equipment.\nThe common types of OTDR-like test equipment are:\nOTDR data format.\nIn the late 1990s, OTDR industry representatives and the OTDR user community developed a unique data format to store and analyze OTDR fiber data. This data was based on the specifications in GR-196, Generic Requirements for Optical Time Domain Reflectometer (OTDR) Type Equipment. The goal was for the data format to be truly universal, in that it was intended to be implemented by all OTDR manufacturers. OTDR suppliers developed the software to implement the data format. As they proceeded, they identified inconsistencies in the format, along with areas of misunderstanding among users.\nFrom 1997 to 2000, a group of OTDR supplier software specialists attempted to resolve problems and inconsistencies in what was then called the \u201cBellcore\u201d OTDR Data Format. This group, called the OTDR Data Format Users Group (ODFUG), made progress. Since then, many OTDR developers continued to work with other developers to solve individual interaction problems and enable cross use between manufacturers.\nIn 2011, Telcordia decided to compile industry comments on this data format into one document entitled Optical Time Domain Reflectometer (OTDR) Data Format. This Special Report (SR) summarizes the state of the Bellcore OTDR Data Format, renaming it as the Telcordia OTDR Data Format.\nThe data format is intended for all OTDR-related equipment designed to save trace data and analysis information. Initial implementations require standalone software to be provided by the OTDR supplier to convert existing OTDR trace files to the SR-4731 data format and to convert files from this universal format to a format that is usable by their older OTDRs. This file conversion software can be developed by the hardware supplier, the end user, or a third party. This software also provides backward compatibility of the OTDR data format with existing equipment.\nThe SR-4731 format describes binary data. While text information is contained in several fields, most numbers are represented as either 16-bit (2-byte) or 32-bit (4-byte) signed or unsigned integers stored as binary images. Byte ordering in this file format is explicitly low-byte ordering, as is common on Intel processor-based machines. String fields are terminated with a zero byte \u201c\\0\u201d. OTDR waveform data are represented as short, unsigned integer data uniformly spaced in time, in units of decibels (dB) times 1000, referenced to the maximum power level. The maximum power level is set to zero, and all waveform data points are assumed to be zero or negative (the sign bit is implied), so that the minimum power level in this format is -65.535 dB, and the minimum resolution between power level steps is 0.001 dB. In some cases, this will not provide sufficient power range to represent all waveform points. For this reason, the use of a scale factor has been introduced to expand the data point power range.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41467", "revid": "7611264", "url": "https://en.wikipedia.org/wiki?curid=41467", "title": "Optoelectronic", "text": ""}
{"id": "41469", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=41469", "title": "Out-of-band signaling", "text": ""}
{"id": "41472", "revid": "27823944", "url": "https://en.wikipedia.org/wiki?curid=41472", "title": "Outside plant", "text": "In telecommunications, the term outside plant has the following meanings:\nThe CATV industry divides its fixed assets between head end or inside plant, and outside plant. The electrical power industry also uses the term outside plant to refer to electric power distribution systems.\nContext.\nNetwork connections between devices such as computers, printers, and phones require a physical infrastructure to carry and process signals. Typically, this infrastructure will consist of:\nThe portion of this infrastructure contained within a building is the inside plant, and the portion of this infrastructure connecting buildings or facilities is the outside plant. Where these two plants meet in a given structure is the demarcation point.\nOutside plant cabling, whether copper or fiber, is generally installed as aerial cable between poles, in an underground conduit system, or by direct burial.\nHardware associated with the outside plant must be either protected from the elements (for example, distribution frames are generally protected by a street side cabinet) or constructed with materials suitable for exposure to the elements. Installation of the outside plant elements often require construction of significant physical infrastructure, such as underground vaults. In older large installations, cabling is sometimes protected by air pressure systems designed to prevent water infiltration. While this is not a modern approach, the cost of replacement of the older cabling with sealed cabling is often prohibitively expensive. The cabling used in the outside plant must also be protected from electrical disturbances caused by lightning or voltage surges due to electrical shorts or induction.\nExample: copper access network.\nIn civilian telecommunications, the copper access network (also known as the local loop) providing basic telephone or DSL services typically consists of the following elements:\nActive equipment (such as a POTS or DSL line circuit) can then be connected to the line in order to provide service, but this is not considered part of outside plant.\nProtecting equipment in the outside plant.\nThe environment can play a large role in the quality and lifespan of equipment used in the outside plant. It is critical that environmental testing criteria as well as design and performance requirements be defined for this type of equipment.\nThere are generally four operating environments or classes covering all outside plant (OSP) applications, including wireless facilities.\n*Class 1: Equipment in a Controlled Environment\n*Class 2: Protected Equipment in Outside Environments\n*Class 3: Protected Equipment in Severe Outside Environments\n*Class 4: Products in an Unprotected Environment\nElectronic equipment located in one or more of these environmental class locations is designed to withstand various environmental operating conditions resulting from climatic conditions that may include rain, snow, sleet, high winds, ice, salt spray, and sand storms. Since outside temperatures can possibly range from \u221240\u00a0\u00b0C (\u221240\u00a0\u00b0F) to 46\u00a0\u00b0C (115\u00a0\u00b0F), with varying degrees of solar loading, along with humidity levels ranging from below 10% up to 100%, significant environmental stresses within the enclosure or facility can be produced.\nTelcordia http://, contains the most recent industry data regarding each Class described above. It also discusses what is currently happening in ATIS and Underwriters Laboratories (UL).\nThe document also includes \n*Environmental criteria such as operating temperatures, humidity, particulate contamination, pollution exposure, and heat dissipation\n*Mechanical criteria such as structural requirements, packaging, susceptibility to vibration, earthquake, and handling\n*Electrical protection and safety including protection from lightning surges, AC power induction and faults, and Electromagnetic Interference (EMI), and DC power influences\nHandholes and other below-ground splice vaults.\nHandholes and other below-ground splice vaults house telecommunications components used in an Outside Plant (OSP) environment.\nHandholes are plastic or polymer concrete structures set below ground with their lids flush to the surrounding soil, turf, footpath, or road surface. They can be used to house and protect copper, coaxial, and optical fiber telephone cable splices and distribution elements. They safeguard and provide convenient access to cable termination and branch points, provide flexibility and access for installation operations (e.g., pulling or blowing cables), provide mechanical and environmental protection for splices, allow access for craftsperson work activities, and discourage access by unauthorized persons.\nHandholes and other below-ground splice vaults are deployed in a variety of environments. The major distinctions in these environments focus on the strength and frequency of vehicular and foot traffic loading. There are four basic application environments:\n*Light Duty: Pedestrian Only \n*Medium Duty: Pedestrian and Light Incidental Vehicular Traffic (Up to Class 5 Vehicles)\n*Heavy Duty: Non-Deliberate (Incidental) Vehicular Traffic (Up to Class 7 Vehicles)\n*Heavy Duty: Non-Deliberate (Incidental) Vehicular Traffic (Up to Class 8 Vehicles)\nHandhole-type products deployed in any environment are subjected to the following types of traffic loading: Vertical Cover Load, Vertical Sidewall Load, Lateral Sidewall Load, and Long-Term Lateral Sidewall Load.\nTelcordia http:// contains detailed industry requirements for handholes, and includes specific loading requirements for the defined application environments. It provides explicit correlations to other standards such as ANSI/SCTE-77, AASHTO specifications, and ASTM C857.\nCorrosion resistance.\nCorrosion in outside plant telecommunications network components is caused by exposure to the effects of temperature, humidity, electrical power, and contaminants. Corrosion resistance criteria for these network components are based on the environments to which they are exposed.\nOutside plant environments can be above-ground, underground, buried, or underwater. Industry requirements document Telcordia http:// defines these environments and provides corrosion resistance criteria for the telecommunications equipment in each. It also includes references to various associated ASTM Standards.\nAbove-ground plant.\nAbove-ground plant includes all the telecommunications equipment physically located on or above the ground. This includes enclosures such as huts, cabinets, and pedestals, and the equipment mounted therein. It also includes pole-mounted equipment and cases, and pole-line hardware.\nAbove-ground plant can be exposed to extreme temperatures, and to humidity that varies with the seasons and with daily temperature changes. When humidity condenses on the surfaces of outdoor apparatus or equipment, the corrosivity of the moisture layer can be increased by industrial pollutants that render the condensate moisture corrosive. In sea coastal areas, wind-borne, salt-laden water droplets can deposit on exposed components.\nNear large cultivated areas, where fertilizers are applied by airplanes, the wind may carry nitrates, phosphates, and ammonium compounds to settle on metallic components of the above-ground telephone plant. Similarly, in residential areas, lawn fertilizers and herbicides can cause corrosion. In regions with snow, the salts used to melt snow and ice on roadways can accelerate corrosion. Under extreme conditions, pedestals and cabinets may be flooded with water that contains mud and corrosive salts. Corrosion of these flooded components may be accelerated by the presence of dc voltages used to power the networks. Secretions from insects can also accelerate corrosion. Finally, chewing by rodents may expose metallic components, normally protected by a polymer or paint coating, to a corrosive environment.\nUnderground plant.\nUnderground plant includes all the telecommunications equipment installed in underground structures such as utility holes, Controlled Environment Vaults (CEVs), and ducts, along with associated hardware. Underground plant can be exposed to waters containing water-soluble salts of the native soil. Utility holes often show evidence of corrosion of support hardware and bonding ribbons that is caused by sulfate-reducing bacteria. The environment in utility holes and ducts can be made corrosive by man-made chemicals such as industrial effluent, fertilizers, and de-icing salts. Protective plastic coatings and cable jackets can rapidly deteriorate from leaking steam pipes present in many urban areas and from gasoline leaking from underground storage tanks.\nThe most aggressive contributor to corrosion of underground plant is dc stray current from electrified rail transportation systems, cathodic protection rectifiers, or welding and mining operations. Although such dc currents are mostly dealt with \u201cafter the fact\u201d using protective systems (e.g., low resistance bonds, reverse current switches, cathodic protection), some of the protection has to be included at the manufacturing stage. This protection may include insulating covers on cable shields, or nonmetallic components or coatings for apparatus.\nBuried plant.\nBuried plant consists of telecommunications equipment such as cables, splice closures, lower parts of pedestals, and grounding systems directly buried in the soil. Buried plant can be exposed to the same corrosive environment as underground plant. In addition, attack by gophers can expose underlying components to corrosion attack.\nUnderwater plant.\nUnderwater plant includes all telecommunications equipment located beneath the surface of a body of water. This includes cables and repeaters. The water can range from relatively pure, to brackish, to badly contaminated with industrial effluent.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41473", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41473", "title": "Ovality", "text": "In telecommunications and fiber optics, ovality or noncircularity is the degree of deviation from perfect circularity of the cross section of the core or cladding of the fiber.\nThe cross-sections of the core and cladding are assumed to a first approximation to be elliptical, and ovality is defined to be twice the third flattening of the ellipse, formula_1, where \"a\" is the length of the major axis and \"b\" is the length of the minor axis. This dimensionless quantity is between 0 and 1, and may be multiplied by 100 to express ovality as a percentage. Alternatively, ovality of the core or cladding may be specified by a tolerance field consisting of two concentric circles, within which the cross section boundaries must lie.\nIn measurements, ovality is the amount of out-of-roundness of a hole or cylindrical part in the typical form of an oval.\nIn chemistry.\nIn computational chemistry, especially in QSAR studies, ovality refers to, a measure of how the shape of a molecule approaches a sphere (at one extreme) or a cigar shape (at the other). \nOvality is described by a ratio of volume to area:\nformula_2\nwhere:\nO = Ovality\nA = Area\nV = Volume\nThe ovality of the He atom is 1.0 and that of HC24H (12 triple bonds) is ~1.7."}
{"id": "41474", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41474", "title": "Overfill", "text": "In telecommunications, overfill is the condition that prevails when the numerical aperture or the beam diameter of an optical source, such as a laser, light-emitting diode, or optical fiber, exceeds that of the driven element, e.g. an optical fiber core. In optical communications testing, overfill in both numerical aperture and mean diameter (core diameter or spot size) is usually required.\nIn polygonal mirror scanners, an overfilled type is one which uses each mirror facet at least in one dimension completely."}
{"id": "41475", "revid": "19404073", "url": "https://en.wikipedia.org/wiki?curid=41475", "title": "Overflow", "text": "Overflow may refer to:\nOther.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "41476", "revid": "38516105", "url": "https://en.wikipedia.org/wiki?curid=41476", "title": "Overhead information", "text": ""}
{"id": "41477", "revid": "43768634", "url": "https://en.wikipedia.org/wiki?curid=41477", "title": "Overmodulation", "text": "Excessive modulation resulting in distortion of signal\nOvermodulation is the condition that prevails in telecommunication when the instantaneous level of the modulating signal exceeds the value necessary to produce 100% modulation of the carrier. In the sense of this definition, it is almost always considered a fault condition. In layman's terms, the signal is going \"off the scale\". Overmodulation results in spurious emissions by the modulated carrier, and distortion of the recovered modulating signal. This means that the envelope of the output waveform is distorted. \nAlthough overmodulation is sometimes considered permissible, it should not occur in practice; a distorted waveform envelope will result in a distorted output signal of the receiving medium."}
{"id": "41478", "revid": "7098284", "url": "https://en.wikipedia.org/wiki?curid=41478", "title": "Override", "text": "Override may refer to:\nOther uses.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "41479", "revid": "20204978", "url": "https://en.wikipedia.org/wiki?curid=41479", "title": "Overshoot", "text": "Overshoot may refer to:\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "41480", "revid": "7611264", "url": "https://en.wikipedia.org/wiki?curid=41480", "title": "Overtone", "text": "Tone with a frequency higher than the frequency of the reference tone\nAn overtone is any resonant frequency above the fundamental frequency of a sound (or of any oscillation). An overtone may or may not be a harmonic. In other words, overtones are all pitches higher than the lowest pitch within an individual sound; the fundamental is the lowest pitch. While the fundamental is usually heard most prominently, overtones are actually present in any pitch except a true sine wave. The relative volume or amplitude of various overtone partials is one of the key identifying features of timbre, or the individual characteristic of a sound.\nUsing the model of Fourier analysis, the fundamental and the overtones together are called partials. Harmonics, or more precisely, harmonic partials, are partials whose frequencies are numerical integer multiples of the fundamental (including the fundamental, which is 1 times itself). These overlapping terms are variously used when discussing the acoustic behavior of musical instruments. (See etymology below.) The model of Fourier analysis provides for the inclusion of inharmonic partials, which are partials whose frequencies are not whole-number ratios of the fundamental (such as 1.1 or 2.14179).\nWhen a resonant system such as a blown pipe or plucked string is excited, a number of overtones may be produced along with the fundamental tone. In simple cases, such as for most musical instruments, the frequencies of these tones are the same as (or close to) the harmonics. Examples of exceptions include the circular drum \u2013 a timpano whose first overtone is about 1.6 times its fundamental resonance frequency, gongs and cymbals, and brass instruments. The human vocal tract is able to produce highly variable amplitudes of the overtones, called formants, which define different vowels.\nExplanation.\nMost oscillators, from a plucked guitar string to a flute that is blown, will naturally vibrate at a series of distinct frequencies known as normal modes. The lowest normal mode frequency is known as the fundamental frequency, while the higher frequencies are called overtones. Often, when an oscillator is excited \u2014 for example, by plucking a guitar string \u2014 it will oscillate at several of its modal frequencies at the same time. So when a note is played, this gives the sensation of hearing other frequencies (overtones) above the lowest frequency (the fundamental).\nTimbre is the quality that gives the listener the ability to distinguish between the sound of different instruments. The timbre of an instrument is determined by which overtones it emphasizes. That is to say, the relative volumes of these overtones to each other determines the specific \"flavor\", \"color\" or \"tone\" of sound of that family of instruments. The intensity of each of these overtones is rarely constant for the duration of a note. Over time, different overtones may decay at different rates, causing the relative intensity of each overtone to rise or fall independent of the overall volume of the sound. A carefully trained ear can hear these changes even in a single note. This is why the timbre of a note may be perceived differently when played staccato or legato.\nA driven non-linear oscillator, such as the vocal folds, a blown wind instrument, or a bowed violin string (but not a struck guitar string or bell) will oscillate in a periodic, non-sinusoidal manner. This generates the impression of sound at integer multiple frequencies of the fundamental known as harmonics, or more precisely, harmonic partials. For most string instruments and other long and thin instruments such as a bassoon, the first few overtones are quite close to integer multiples of the fundamental frequency, producing an approximation to a harmonic series. Thus, in music, overtones are often called harmonics. Depending upon how the string is plucked or bowed, different overtones can be emphasized.\nHowever, some overtones in some instruments may not be of a close integer multiplication of the fundamental frequency, thus causing a small dissonance. \"High quality\" instruments are usually built in such a manner that their individual notes do not create disharmonious overtones. In fact, the flared end of a brass instrument is not to make the instrument sound louder, but to correct for tube length \u201cend effects\u201d that would otherwise make the overtones significantly different from integer harmonics. This is illustrated by the following:\nConsider a guitar string. Its idealized 1st overtone would be exactly twice its fundamental if its length were shortened by \u00bd, perhaps by lightly pressing a guitar string at the 12th fret; however, if a vibrating string is examined, it will be seen that the string does not vibrate flush to the bridge and nut, but it instead has a small \u201cdead length\u201d of string at each end. This dead length actually varies from string to string, being more pronounced with thicker and/or stiffer strings. This means that halving the physical string length does not halve the actual string vibration length, and, hence, the overtones will not be exact multiples of a fundamental frequency. The effect is so pronounced that properly set up guitars will angle the bridge such that the thinner strings will progressively have a length up to few millimeters shorter than the thicker strings. Not doing so would result in inharmonious chords made up of two or more strings. Similar considerations apply to tube instruments.\nMusical usage term.\nAn overtone is a partial (a \"partial wave\" or \"constituent frequency\") that can be either a harmonic partial (a harmonic) other than the fundamental, or an inharmonic partial. A harmonic frequency is an integer multiple of the fundamental frequency. An inharmonic frequency is a non-integer multiple of a fundamental frequency.\nAn example of harmonic overtones: (absolute harmony)\nSome musical instruments, such as the piano, produce overtones that are slightly sharper or flatter than true harmonics. The sharpness or flatness of their overtones is one of the elements that contributes to their sound. Due to phase inconsistencies between the fundamental and the partial harmonic, this also has the effect of making their waveforms not perfectly periodic.\nMusical instruments that can create notes of any desired duration and definite pitch have harmonic partials.\nA tuning fork, provided it is sounded with a mallet (or equivalent) that is reasonably soft, has a tone that consists very nearly of the fundamental, alone; it has a sinusoidal waveform. Nevertheless, music consisting of pure sinusoids was found to be unsatisfactory in the early 20th century.\nEtymology.\nIn Hermann von Helmholtz's classic \"On The Sensations Of Tone\" he used the German \"Obert\u00f6ne\" which was a contraction of \"Oberpartialt\u00f6ne\", or in English: \"upper partial tones\". According to Alexander Ellis (in pages 24\u201325 of his English translation of Helmholtz), the similarity of German \"ober\" to English \"over\" caused a Prof. Tyndall to mistranslate Helmholtz' term, thus creating \"overtone\". Ellis disparages the term \"overtone\" for its awkward implications. Because \"overtone\" makes the upper partials seem like such a distinct phenomena, it leads to the mathematical problem where the first overtone is the second partial. Also, unlike discussion of \"partials\", the word \"overtone\" has connotations that have led people to wonder about the presence of \"undertones\" (a term sometimes confused with \"difference tones\" but also used in speculation about a hypothetical \"undertone series\").\n\"Overtones\" in choral music.\nIn barbershop music, a style of four-part singing, the word \"overtone\" is often used in a related but particular manner. It refers to a psychoacoustic effect in which a listener hears an audible pitch that is higher than, and different from, the fundamentals of the four pitches being sung by the quartet. The barbershop singer's \"overtone\" is created by the interactions of the upper partial tones in each singer's note (and by sum and difference frequencies created by nonlinear interactions within the ear). Similar effects can be found in other \"a cappella\" polyphonic music such as the music of the Republic of Georgia and the Sardinian \"cantu a tenore\". Overtones are naturally highlighted when singing in a particularly resonant space, such as a church; one theory of the development of polyphony in Europe holds that singers of Gregorian chant, originally monophonic, began to hear the overtones of their monophonic song and to imitate these pitches - with the fifth, octave, and major third being the loudest vocal overtones, it is one explanation of the development of the triad and the idea of consonance in music.\nThe first step in composing choral music with overtone singing is to discover what the singers can be expected to do successfully without extensive practice. The second step is to find a musical context in which those techniques could be effective, not mere special effects. It was initially hypothesized that beginners would be able to:\nSingers should not be asked to change the fundamental pitch while overtone singing and changing partials should always be to an adjacent partial. When a particular partial is to be specified, time should be allowed (a beat or so) for the singers to get the harmonics to \"speak\" and find the correct one.\nString instruments.\nString instruments can also produce multiphonic tones when strings are divided in two pieces or the sound is somehow distorted. The sitar has sympathetic strings which help to bring out the overtones while one is playing. The overtones are also highly important in the tanpura, the drone instrument in traditional North and South Indian music, in which loose strings tuned at octaves and fifths are plucked and designed to buzz to create sympathetic resonance and highlight the cascading sound of the overtones.\nWestern string instruments, such as the violin, may be played close to the bridge (a technique called \"sul ponticello\" or \"am Steg\") which causes the note to split into overtones while attaining a distinctive glassy, metallic sound. Various techniques of bow pressure may also be used to bring out the overtones, as well as using string nodes to produce natural harmonics. On violin family instruments, overtones can be played with the bow or by plucking. Scores and parts for Western violin family instruments indicate where the performer is to play harmonics. The most well-known technique on a guitar is playing flageolet tones or using distortion effects. The ancient Chinese instrument the guqin contains a scale based on the knotted positions of overtones. The Vietnamese \u0111\u00e0n b\u1ea7u functions on flageolet tones. Other multiphonic extended techniques used are prepared piano, prepared guitar and 3rd bridge.\nWind instruments.\nWind instruments manipulate the overtone series significantly in the normal production of sound, but various playing techniques may be used to produce multiphonics which bring out the overtones of the instrument. On many woodwind instruments, alternate fingerings are used. \"Overblowing\", or adding intensely exaggerated air pressure, can also cause notes to split into their overtones. In brass instruments, multiphonics may be produced by singing into the instrument while playing a note at the same time, causing the two pitches to interact - if the sung pitch is at specific harmonic intervals with the played pitch, the two sounds will blend and produce additional notes by the phenomenon of sum and difference tones.\nNon-western wind instruments also exploit overtones in playing, and some may highlight the overtone sound exceptionally. Instruments like the didgeridoo are highly dependent on the interaction and manipulation of overtones achieved by the performer changing their mouth shape while playing, or singing and playing simultaneously. Likewise, when playing a harmonica or pitch pipe, one may alter the shape of their mouth to amplify specific overtones. Though not a wind instrument, a similar technique is used for playing the jaw harp: the performer amplifies the instrument's overtones by changing the shape, and therefore the resonance, of their vocal tract.\nBrass Instruments.\nBrass instruments originally had no valves, and could only play the notes in the natural overtone, or harmonic series.\nBrass instruments still rely heavily on the overtone series to produce notes: the tuba typically has 3-4 valves, the tenor trombone has 7 slide positions, the trumpet has 3 valves, and the French horn typically has 4 valves. Each instrument can play (within their respective ranges) the notes of the overtone series in different keys with each fingering combination (open, 1, 2, 12, 123, etc). The role of each valve or rotor (excluding trombone) is as follows: 1st valve lowers major 2nd, 2nd valve lowers minor 2nd, 3rd valve-lowers minor 3rd, 4th valve-lowers perfect 4th (found on piccolo trumpet, certain euphoniums, and many tubas). The French horn has a trigger key that opens other tubing and is pitched a perfect fourth higher; this allows for greater ease between different registers of the instrument. Valves allow brass instruments to play chromatic notes, as well as notes within the overtone series (open valve = C overtone series, 2nd valve = B overtone series on the C Trumpet) by changing air speed and lip vibrations.\nThe tuba, trombone, and trumpet play notes within the first few octaves of the overtone series, where the partials are farther apart. The French horn sounds notes in a higher octave of the overtone series, so the partials are closer together and make it more difficult to play the correct pitches and partials.\nOvertone singing.\nOvertone singing is a traditional form of singing in many parts of the Himalayas and Altay; Tibetans, Mongols and Tuvans are known for their overtone singing. In these contexts it is often referred to as throat singing or khoomei, though it should not be confused with Inuit throat singing, which is produced by different means. There is also the possibility to create the overtone out of fundamental tones without any stress on the throat.\nAlso, the overtone is very important in singing to take care of vocal tract shaping, to improve color, resonance, and text declamation. During practice overtone singing, it helps the singer to remove unnecessary pressure on the muscle, especially around the throat. So if one can \"find\" a single overtone, then one will know where the sensation needs to be in order to bring out vocal resonance in general, helping to find the resonance in one's own voice on any vowel and in any register.\nOvertones in music composition.\nThe primacy of the triad in Western harmony comes from the first four partials of the overtone series. The eighth through fourteenth partials resemble the equal tempered acoustic scale:\n&lt;score sound=\"1\"&gt; {\n\\override Score.TimeSignature #'stencil = ##f\n\\relative c' {\n \\clef treble \\time 7/4\n c4^\\markup { Acoustic scale on C } d e fis g a bes c\n&lt;/score&gt;\nWhen this scale is rendered as a chord, it is called the lydian dominant thirteenth chord. This chord appears throughout Western music, but is notably used as the basis of jazz harmony, features prominently in the music of Franz Liszt, Claude Debussy, Maurice Ravel, and appears as the Mystic chord in the music of Alexander Scriabin.\n&lt;score sound=\"1\"&gt;\n\\new PianoStaff \u00ab\n&lt;/score&gt; Rimsky-Korsakov's voicing of a C major triad, consisting of the fundamental and partials 1, 2, 3, 4, 5, 6, 8, 10, 12, and 16.\nBecause the overtone series rises infinitely from the fundamental with no periodicity, in Western music the equal temperament scale was designed to create synchronicity between different octaves. This was achieved by de-tuning certain intervals, such as the perfect fifth. A true perfect fifth is 702 cents above the fundamental, but equal temperament flattens it by two cents. The difference is only barely perceptible, and allows both for the illusion of the scale being in-tune with itself across multiple octaves, and for tonalities based on all 12 chromatic notes to sound in-tune.\nWestern classical composers have also made use of the overtone series through orchestration. In his treatise \"Principles of Orchestration,\" Russian composer Nikolai Rimsky-Korsakov says the overtone series \"may serve as a guide to the orchestral arrangement of chords\". Rimsky-Korsakov then demonstrates how to voice a C major triad according to the overtone series, using partials 1, 2, 3, 4, 5, 6, 8, 10, 12, and 16.\nIn the 20th century, exposure to non-Western music and further scientific acoustical discoveries led some Western composers to explore alternate tuning systems. Harry Partch for example designed a tuning system that divides the octave into 43 tones, with each tone based on the overtone series. The music of Ben Johnston uses many different tuning systems, including his String Quartet No. 5 which divides the octave into more than 100 tones.\nSpectral music is a genre developed by G\u00e9rard Grisey and Tristan Murail in the 1970s and 80s, under the auspices of IRCAM. Broadly, spectral music deals with resonance and acoustics as compositional elements. For example, in Grisey's seminal work \"Partiels\", the composer used a sonogram to analyze the true sonic characteristics of the lowest note on a tenor trombone (E2). The analysis revealed which overtones were most prominent from that sound, and \"Partiels\" was then composed around the analysis. Another seminal spectral work is Tristan Murail's \"Gondwana\" for orchestra. This work begins with a spectral analysis of a bell, and gradually transforms it into the spectral analysis of a brass instrument. Other spectralists and post-spectralists include Jonathan Harvey, Kaija Saariaho, and Georg Friedrich Haas.\nJohn Luther Adams is known for his extensive use of the overtone series, as well as his tendency to allow musicians to make their own groupings and play at their own pace to alter the sonic experience. For example, his piece \"Sila: The Breath of the World\" can be played by 16 to 80 musicians and are separated into their own groups. The piece is set on sixteen \"harmonic clouds\" that are grounded on the first sixteen overtones of low B-flat. Another example is John Luther Adam's piece \"Everything That Rises\", which grew out of his piece \"Sila: The Breath of the World\". \"Everything That Rises\" is a piece for string quartet that has sixteen harmonic clouds that are built off of the fundamental tone (C0)\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41481", "revid": "28438779", "url": "https://en.wikipedia.org/wiki?curid=41481", "title": "Packet-switching node", "text": "A packet-switching node is a node in a packet-switching network that contains data switches and equipment for controlling, formatting, transmitting, routing, and receiving data packets. \n\"Note:\" In the Defense Data Network (DDN), a packet-switching node is usually configured to support up to thirty-two X.25 56\u00a0kbit/s host connections, as many as six 56\u00a0kbit/s interswitch trunk (IST) lines to other packet-switching nodes, and at least one Terminal Access Controller (TAC).\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41482", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41482", "title": "Paired disparity code", "text": "Line code in telecommunication\nIn telecommunications, a paired disparity code is a line code in which at least one of the data characters is represented by two codewords of opposite disparity that are used in sequence so as to minimize the total disparity of a longer sequence of digits.\nA particular codeword of any line code can either have no disparity (the average weight of the codeword is zero), negative disparity (the average weight of the codeword is negative), or positive disparity (the average weight of the codeword is positive).\nIn a paired disparity code, every codeword that averages to a negative level (negative disparity) is paired with some other codeword that averages to a positive level (positive disparity).\nIn a system that uses a paired disparity code, the transmitter must keep track of the running DC buildup\u00a0\u2013 the running disparity\u00a0\u2013 and always pick the codeword that pushes the DC level back towards zero. The receiver is designed so that either codeword of the pair decodes to the same data bits.\nMost line codes use either a paired disparity code or a constant-weight code.\nThe simplest paired disparity code is alternate mark inversion signal. Other paired disparity codes include 8b/10b, , the modified AMI codes, coded mark inversion, and 4B3T.\nThe digits may be represented by disparate physical quantities, such as two different frequencies, phases, voltage levels, magnetic polarities, or electrical polarities, each one of the pair representing a 0 or a 1."}
{"id": "41483", "revid": "45789152", "url": "https://en.wikipedia.org/wiki?curid=41483", "title": "Panning", "text": ""}
{"id": "41484", "revid": "139104", "url": "https://en.wikipedia.org/wiki?curid=41484", "title": "Parallel transmission", "text": ""}
{"id": "41485", "revid": "11521989", "url": "https://en.wikipedia.org/wiki?curid=41485", "title": "Par meter", "text": ""}
{"id": "41486", "revid": "36222423", "url": "https://en.wikipedia.org/wiki?curid=41486", "title": "Title 47 CFR Part 68", "text": "Title 47 CFR Part 68 is a section of the Code of Federal Regulations of the United States that regulates the direct electrical connection of telecommunications equipment and customer premises wiring with the public switched telephone network, certain private line services, and connection of private branch exchange (PBX) equipment to certain telecommunication interfaces.\nScope.\nPart 68 rules provide the technical and procedural standards under which direct electrical connection of customer-provided telephone equipment, systems, and protective apparatus may be made to the nationwide network without causing harm and without a requirement for protective circuit arrangements in the service-provider networks.\nThe equivalent European regulation is called TBR21.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41487", "revid": "1290070799", "url": "https://en.wikipedia.org/wiki?curid=41487", "title": "Party line", "text": "Party line or Party Line may refer to:\nOther uses.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "41488", "revid": "50411836", "url": "https://en.wikipedia.org/wiki?curid=41488", "title": "Passband", "text": "Frequencies or wavelengths that can pass through a filter\nA passband is the range of frequencies or wavelengths that can pass through a filter. For example, a radio receiver contains a bandpass filter to select the frequency of the desired radio signal out of all the radio waves picked up by its antenna. The passband of a receiver is the range of frequencies it can receive when it is tuned into the desired frequency as in a radio station or television channel.\nA bandpass-filtered signal (that is, a signal with energy only in a passband), is known as a bandpass signal, in contrast to a baseband signal. The bandpass filter usually has two band-stop filters.\nFilters.\nIn telecommunications, optics, and acoustics, a passband (a band-pass filtered signal) is the portion of the frequency spectrum that is transmitted (with minimum relative loss or maximum relative gain) by some filtering device. In other words, it is a \"band\" of frequencies which \"pass\"es through some filter or a set of filters.\nThe accompanying figure shows a schematic of a waveform being filtered by a bandpass filter consisting of a highpass and a lowpass filter.\nRadio receivers generally include a tunable band-pass filter with a passband that is wide enough to accommodate the bandwidth of the radio signal transmitted by a single station.\nDigital transmission.\nThere are two main categories of digital communication transmission methods: baseband and passband.\nDetails.\nIn general, there is an inverse relationship between the width of a filter's passband and the time required for the filter to respond to new inputs. Broad passbands yield faster response times. This is a consequence of the mathematics of Fourier analysis.\nThe limiting frequencies of a passband are defined as those at which the relative intensity or power decreases to a specified fraction of the maximum intensity or power. This decrease in power is often specified to be the half-power points, \"i.e.\", 3 dB below the maximum power.\nThe difference between the limiting frequencies is called the bandwidth, and is expressed in hertz (in the optical regime, in nanometers or micrometers of differential wavelength).\nThe related term \"bandpass\" is an adjective that describes a type of filter or filtering process; it is frequently confused with \"passband\", which refers to the actual portion of affected spectrum. These two words are both compound words that follow the English rules of formation: the primary meaning is the latter part of the compound, while the modifier is the first part. Hence, one may correctly say 'A dual bandpass filter has two passbands'.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41489", "revid": "11292982", "url": "https://en.wikipedia.org/wiki?curid=41489", "title": "Password length equation", "text": ""}
{"id": "41490", "revid": "47070250", "url": "https://en.wikipedia.org/wiki?curid=41490", "title": "Password length parameter", "text": ""}
{"id": "41491", "revid": "15707762", "url": "https://en.wikipedia.org/wiki?curid=41491", "title": "Patch bay", "text": ""}
{"id": "41492", "revid": "8087765", "url": "https://en.wikipedia.org/wiki?curid=41492", "title": "Path loss", "text": "Signal attenuation in telecommunications\nPath loss, or path attenuation, is the reduction in power density (attenuation) of an electromagnetic wave as it propagates through space. Path loss is a major component in the analysis and design of the link budget of a telecommunication system.\nThis term is commonly used in wireless communications and signal propagation. Path loss may be due to many effects, such as free-space loss, refraction, diffraction, reflection, aperture-medium coupling loss, and absorption. Path loss is also influenced by terrain contours, environment (urban or rural, vegetation and foliage), propagation medium (dry or moist air), the distance between the transmitter and the receiver, and the height and location of antennas.\nOverview.\nIn wireless communications, path loss is the reduction in signal strength as the signal travels from a transmitter to a receiver, and is an application for verifying the loss. There are several factors that affect this:\nIn understanding path loss and minimizing it, there are four key factors to consider in designing a wireless communication system:\n1) Determining the required transmitter power: The transmitter must have enough power to overcome the path loss in order for the signal to reach the receiver with sufficient strength.\n2) Determine the appropriate antenna design and gain: Antennas with higher gain can focus the waves in a specific direction, reducing the path loss.\n3) Optimize modulation scheme: The choice of modulation scheme can affect the robustness of the signal to path loss.\n4) Set the receiver sensitivity appropriately: The receiver must be sensitive enough to detect weak signals.\nCauses.\nPath loss normally includes \"propagation losses\" caused by the natural expansion of the radio wave front in free space (which usually takes the shape of an ever-increasing sphere), \"absorption losses\" (sometimes called penetration losses), when the signal passes through media not transparent to electromagnetic waves, \"diffraction losses\" when part of the radiowave front is obstructed by an opaque obstacle, and losses caused by other phenomena.\nThe signal radiated by a transmitter may also travel along many and different paths to a receiver simultaneously; this effect is called multipath. Multipath waves combine at the receiver antenna, resulting in a received signal that may vary widely, depending on the distribution of the intensity and relative propagation time of the waves and bandwidth of the transmitted signal. The total power of interfering waves in a Rayleigh fading scenario varies quickly as a function of space (which is known as \"small scale fading\"). Small-scale fading refers to the rapid changes in radio signal amplitude in a short period of time or distance of travel.\nLoss exponent.\nIn the study of wireless communications, path loss can be represented by the path loss exponent, whose value is normally in the range of 2 to 4 (where 2 is for propagation in free space, 4 is for relatively lossy environments and for the case of full specular reflection from the earth surface\u2014the so-called flat earth model). In some environments, such as buildings, stadiums and other indoor environments, the path loss exponent can reach values in the range of 4 to 6. On the other hand, a tunnel may act as a waveguide, resulting in a path loss exponent less than 2.\nPath loss is usually expressed in dB. In its simplest form, the path loss can be calculated using the formula\nformula_1\nwhere formula_2 is the path loss in decibels, formula_3 is the path loss exponent, formula_4 is the distance between the transmitter and the receiver, usually measured in meters, and formula_5 is a constant which accounts for system losses.\nRadio engineer formula.\nRadio and antenna engineers use the following simplified formula (derived from the Friis Transmission Formula) for the signal path loss between the feed points of two isotropic antennas in free space:\nPath loss in dB: \nformula_6\nwhere formula_2 is the path loss in decibels, formula_8 is the wavelength and formula_4 is the transmitter-receiver distance in the same units as the wavelength. Note the power density in space has no dependency on formula_8; The variable formula_8 exists in the formula to account for the effective capture area of the isotropic receiving antenna.\nPrediction.\nCalculation of the path loss is usually called \"prediction\". Exact prediction is possible only for simpler cases, such as the above-mentioned \"free space\" propagation or the \"flat-earth model\". For practical cases the path loss is calculated using a variety of approximations.\n\"Statistical\" methods (also called \"stochastic\" or \"empirical\") are based on measured and averaged losses along typical classes of radio links. Among the most commonly used such methods are Okumura\u2013Hata, the COST Hata model, W.C.Y.Lee, etc. These are also known as \"radio wave propagation models\" and are typically used in the design of cellular networks and public land mobile networks (PLMN). For wireless communications in the very high frequency (VHF) and ultra high frequency (UHF) frequency band (the bands used by walkie-talkies, police, taxis and cellular phones), one of the most commonly used methods is that of Okumura\u2013Hata as refined by the COST 231 project. Other well-known models are those of Walfisch\u2013Ikegami, W. C. Y. Lee, and Erceg. For FM radio and TV broadcasting the path loss is most commonly predicted using the ITU model as described in P.1546 (successor to P.370) recommendation.\nDeterministic methods based on the physical laws of wave propagation are also used; ray tracing is one such method. These methods are expected to produce more accurate and reliable predictions of the path loss than the empirical methods; however, they are significantly more expensive in computational effort and depend on the detailed and accurate description of all objects in the propagation space, such as buildings, roofs, windows, doors, and walls. For these reasons they are used predominantly for short propagation paths. Among the most commonly used methods in the design of radio equipment such as antennas and feeds is the finite-difference time-domain method.\nThe path loss in other frequency bands (medium wave (MW), shortwave (SW or HF), microwave (SHF)) is predicted with similar methods, though the concrete algorithms and formulas may be very different from those for VHF/UHF. Reliable prediction of the path loss in the SW/HF band is particularly difficult, and its accuracy is comparable to weather predictions.\nEasy approximations for calculating the path loss over distances significantly shorter than the distance to the radio horizon:\nExamples.\nIn cellular networks, such as UMTS and GSM, which operate in the UHF band, the value of the path loss in built-up areas can reach 110\u2013140\u00a0dB for the first kilometer of the link between the base transceiver station (BTS) and the mobile. The path loss for the first ten kilometers may be 150\u2013190\u00a0dB (\"Note\": These values are very approximate and are given here only as an illustration of the range in which the numbers used to express the path loss values \"can eventually be\", these are not definitive or binding figures\u2014the path loss may be very different for the same distance along two different paths and it can be different even along the same path if measured at different times.)\nIn the radio wave environment for mobile services the mobile antenna is close to the ground. Line-of-sight propagation (LOS) models are highly modified. The signal path from the BTS antenna normally elevated above the roof tops is refracted down into the local physical environment (hills, trees, houses) and the LOS signal seldom reaches the antenna. The environment will produce several deflections of the direct signal onto the antenna, where typically 2\u20135 deflected signal components will be vectorially added.\nThese refraction and deflection processes cause loss of signal strength, which changes when the mobile antenna moves (Rayleigh fading), causing instantaneous variations of up to 20\u00a0dB. The network is therefore designed to provide an excess of signal strength compared to LOS of 8\u201325\u00a0dB depending on the nature of the physical environment, and another 10\u00a0dB to overcome the fading due to movement.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41493", "revid": "24902", "url": "https://en.wikipedia.org/wiki?curid=41493", "title": "Path profile", "text": "Graphic representation of a radio propagation path\nIn telecommunications, a path profile is a graphic representation of the physical features of a propagation path in the vertical plane containing both endpoints of the path, showing the surface of the Earth and including trees, buildings, and other features that may obstruct the radio signal. \nProfiles are drawn either with an effective Earth radius simulated by a parabolic arc--in which case the ray paths are drawn as straight lines--or with a \"\"flat Earth\"--\" in which case the ray paths are drawn as parabolic arcs.\nReferences.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41494", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41494", "title": "Path quality analysis", "text": "Path quality analysis: In a communications path, an analysis that (a) includes the overall evaluation of the component quality measures, the individual link quality measures, and the aggregate path quality measures, and (b) is performed by evaluating communications parameters, such as bit error ratio, signal-plus-noise-plus-distortion to noise-plus-distortion ratio, and spectral distortion.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41495", "revid": "44120587", "url": "https://en.wikipedia.org/wiki?curid=41495", "title": "Payload", "text": "Carrying capacity of a vehicle\nPayload is the object or the entity that is being carried by an aircraft or launch vehicle. Sometimes payload also refers to the carrying capacity of an aircraft or launch vehicle, usually measured in terms of weight. Depending on the nature of the flight or mission, the payload of a vehicle may include cargo, passengers, flight crew, munitions, scientific instruments or experiments, or other equipment. Extra fuel, when optionally carried, is also considered part of the payload.\nIn a commercial context (i.e., an airline or air freight carrier), payload may refer only to revenue-generating cargo or paying passengers. A payload of ordnance carried by a combat aircraft is sometimes alternatively referred to as the aircraft's warload.\nFor a rocket, the payload can be a satellite, space probe, or spacecraft carrying humans, animals, or cargo. For a ballistic missile, the payload is one or more warheads and related systems; their total weight is referred to as the throw-weight.\nThe fraction of payload to the total liftoff weight of the air or spacecraft is known as the \"payload fraction\". When the weight of the payload and fuel are considered together, it is known as the \"useful load fraction\". In spacecraft, \"mass fraction\" is normally used, which is the ratio of payload to everything else, including the rocket structure.\nRelationship of range and payload.\nThere is a natural trade-off between the payload and the range of an aircraft. A payload range diagram (also known as the \"elbow chart\") illustrates the trade-off.\nThe top horizontal line represents the maximum payload. It is limited structurally by maximum zero-fuel weight (MZFW) of the aircraft. Maximum payload is the difference between maximum zero-fuel weight and operational empty weight (OEW). Moving left-to-right along the line shows the constant maximum payload as the range increases. More fuel needs to be added for more range.\nThe vertical line represents the range at which the combined weight of the aircraft, maximum payload and needed fuel reaches the maximum take-off weight (MTOW) of the aircraft. If the range is increased beyond that point, payload has to be sacrificed for fuel.\nThe maximum take-off weight is limited by a combination of the maximum net power of the engines and the lift/drag ratio of the wings. The diagonal line after the range-at-maximum-payload point shows how reducing the payload allows increasing the fuel (and range) when taking off with the maximum take-off weight.\nThe second kink in the curve represents the point at which the maximum fuel capacity is reached. Flying further than that point means that the payload has to be reduced further, for an even lesser increase in range. The absolute range is thus the range at which an aircraft can fly with maximum possible fuel without carrying any payload.\nExamples.\nExamples of payload capacity:\nStructural capacity.\nFor aircraft, the weight of fuel in wing tanks does not contribute as significantly to the bending moment of the wing as does weight in the fuselage. So even when the airplane has been loaded with its maximum payload that the wings can support, it can still carry a significant amount of fuel.\nPayload constraints.\nLaunch and transport system differ not only on the payload that can be carried but also in the stresses and other factors placed on the payload. The payload must not only be lifted to its target, it must also arrive safely, whether elsewhere on the surface of the Earth or a specific orbit. To ensure this the payload, such as a warhead or satellite, is designed to withstand certain amounts of various types of \"punishment\" on the way to its destination. Most rocket payloads are fitted within a payload fairing to protect them against dynamic pressure of high-velocity travel through the atmosphere, and to improve the overall aerodynamics of the launch vehicle. Most aircraft payloads are carried within the fuselage for similar reasons. Outsize cargo may require a fuselage with unusual proportions, such as the Super Guppy.\nThe various constraints placed on the launch system can be roughly categorized into those that cause physical damage to the payload and those that can damage its electronic or chemical makeup. Examples of physical damage include extreme accelerations over short time scales caused by atmospheric buffeting or oscillations, extreme accelerations over longer time scales caused by rocket thrust and gravity, and sudden changes in the magnitude or direction of the acceleration caused by how quick engines are throttled and shut down, etc. Electrical, chemical, or biological payloads can be damaged by extreme temperatures (hot or cold), rapid changes in temperature or pressure, contact with fast moving air streams causing ionization, and radiation exposure from cosmic rays, the van Allen belt, or solar wind.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41496", "revid": "2255048", "url": "https://en.wikipedia.org/wiki?curid=41496", "title": "Pseudo bit error ratio", "text": "Bit error ratio\nPseudo bit error ratio (PBER) in adaptive high-frequency (HF) radio, is a bit error ratio derived by majority logic decoding to processes redundant transmissions. \n\"Note:\" In adaptive HF radio automatic link establishment, PBER is determined by the extent of error correction, such as by using the fraction of non-unanimous votes in the 2-of-3 majority decoder. \n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41497", "revid": "1233313", "url": "https://en.wikipedia.org/wiki?curid=41497", "title": "PCS switching center", "text": ""}
{"id": "41498", "revid": "9571027", "url": "https://en.wikipedia.org/wiki?curid=41498", "title": "Greater Poland Voivodeship", "text": "Voivodeship in west-central Poland\nGreater Poland Voivodeship ( ) is a voivodeship, or province, in west-central Poland. The province is named after the region called Greater Poland (\"Wielkopolska\" ). The modern province includes most of this historic region, except for some western and northern parts.\nGreater Poland Voivodeship is second in area and third in population among Poland's sixteen voivodeships, with an area of and a population of close to 3.5\u00a0million. Its capital city is Pozna\u0144; other important cities include Kalisz, Konin, Pi\u0142a, Ostr\u00f3w Wielkopolski, Gniezno (an early capital of Poland) and Leszno. It is bordered by seven other voivodeships: West Pomeranian to the northwest, Pomeranian to the north, Kuyavian-Pomeranian to the north-east, \u0141\u00f3d\u017a to the south-east, Opole to the south, Lower Silesian to the southwest and Lubusz to the west.\nHistory.\nGreater Poland, sometimes called the \"cradle of Poland,\" formed the heart of the 10th-century early Polish state. Pozna\u0144 and Gniezno were early centers of royal power, but following the region's devastation by pagan rebellion in the 1030s, and an invasion by Bretislaus I of Bohemia in 1038, the capital was moved by Casimir the Restorer from Gniezno to Krak\u00f3w. The two cities are seats of Poland's oldest diocese (Pozna\u0144, est. in 968) and archdiocese (Gniezno, est. in 1000), playing a crucial role in the Christianization of Poland.\nIn the testament of Boles\u0142aw III Wrymouth, which initiated the period of fragmentation of Poland (1138\u20131320), the western part of Greater Poland (including Pozna\u0144) was granted to Mieszko III the Old. The eastern part, with Gniezno and Kalisz, was part of the Duchy of Krak\u00f3w, granted to W\u0142adys\u0142aw II the Exile. However, for most of the period the two parts were under a single ruler, and were known as the Duchy of Greater Poland (although at times there were separately ruled duchies of Pozna\u0144, Gniezno, Kalisz and Uj\u015bcie). It was one of the leading and fastest developing regions of Poland, with municipal rights modeled after Pozna\u0144 and Kalisz becoming the basis of municipal form of government for several towns in the region, as two of five local Polish variants of medieval town rights. The region came under the control of W\u0142adys\u0142aw I the Elbow-High in 1314, and thus became part of the reunited Poland of which W\u0142adys\u0142aw was crowned king in 1320.\nIn the reunited kingdom, and later in the Polish\u2013Lithuanian Commonwealth, the country came to be divided into administrative units called voivodeships. In the case of the Greater Poland region these were Pozna\u0144 Voivodeship and Kalisz Voivodeship. The Commonwealth also had larger subdivisions known as \"prowincja\", one of which was named Greater Poland. However, this \"prowincja\" covered a larger area than the Greater Poland region itself, also taking in Kuyavia, Masovia and Royal Prussia. (This division of Crown Poland into two entities called Greater and Lesser Poland had its roots in the Statutes of Casimir the Great of 1346\u20131362, where the laws of \"Greater Poland\" \u2013 the northern part of the country \u2013 were codified in the Piotrk\u00f3w statute, with those of \"Lesser Poland\" in the separate Wi\u015blica statute.)\nIn 1768, a new Gniezno Voivodeship was formed out of the northern part of Kalisz Voivodeship. However more far-reaching changes would come with the Partitions of Poland. In the first partition (1772), northern parts of Greater Poland along the Note\u0107 (German \"Netze\") were taken over by Prussia, becoming the Netze District. In the second partition (1793) the whole of Greater Poland was absorbed by Prussia, becoming part of the province of South Prussia. It remained so in spite of the first Greater Poland Uprising (1794), part of the unsuccessful Ko\u015bciuszko Uprising directed chiefly against the Russian Empire.\nMore successful was the Greater Poland Uprising of 1806, which led to the region's becoming part of the Napoleonic Duchy of Warsaw (forming the Pozna\u0144 Department and parts of the Kalisz and Bydgoszcz Departments). However, following the Congress of Vienna in 1815, Greater Poland was again partitioned, with the western part (including Pozna\u0144) going to Prussia. The eastern part joined the Russian-controlled Kingdom of Poland, where it formed the Kalisz Voivodeship until 1837, then the Kalisz Governorate (merged into the Warsaw Governorate between 1844 and 1867).\nWithin the Prussian empire, western Greater Poland became the Grand Duchy of Posen (Pozna\u0144), which theoretically held some autonomy. Following an unrealized uprising in 1846, and the more substantial but still unsuccessful uprising of 1848 (during the Spring of Nations), the Grand Duchy was replaced by the Province of Posen. The authorities made efforts to Germanize the region, particularly after the founding of Germany in 1871, and from 1886 onwards the Prussian Settlement Commission was active in increasing German land ownership in formerly Polish areas.\nFollowing the end of World War I, the Greater Poland uprising (1918\u20131919) ensured that most of the region became part of the newly independent Polish state, forming most of Pozna\u0144 Voivodeship (1919\u20131939). Northern and some western parts of Greater Poland remained in Germany, where they formed much of the province of Posen\u2013West Prussia (1922\u20131938), whose capital was Schneidem\u00fchl (Pi\u0142a).\nFollowing the German invasion of 1939, Greater Poland was incorporated into Nazi Germany, becoming the province called Reichsgau Posen, later Reichsgau Wartheland (\"Warthe\" being the German name for the Warta river). The Polish population was oppressed, with many former officials and others considered potential enemies by the Nazis being imprisoned or executed, including at the notorious Fort VII concentration camp in Pozna\u0144. The Polish population was also subjected to expulsions, kidnapping of children and forced labour. Germany also operated the Stalag XXI-A, Stalag XXI-C, Stalag XXI-D and other prisoner-of-war camps for Polish, French, British, Moroccan, Algerian, Dutch, Belgian, Serbian, Italian, American, Norwegian, and Soviet POWs. Pozna\u0144 was declared a stronghold city \"(Festung)\" in the closing stages of the war, being taken by the Red Army in the Battle of Pozna\u0144, which ended on 22 February 1945.\nAfter the war, Greater Poland was fully within the Polish People's Republic, as Pozna\u0144 Voivodeship. With the reforms of 1975 this was divided into smaller provinces (the voivodeships of Kalisz, Konin, Leszno and Pi\u0142a, and a smaller Pozna\u0144 Voivodeship). The present-day Greater Poland Voivodeship, again with Pozna\u0144 as its capital, was created on 1 January 1999 out of the former Pozna\u0144, Kalisz, Konin, Pi\u0142a and Leszno Voivodeships, pursuant to the Polish local government reforms adopted in 1998.\nCities and towns.\n&lt;templatestyles src=\"Module:Historical populations/styles.css\"/&gt;The voivodeship contains 7 cities and 106 towns. These are listed below in descending order of population (according to official figures for 2019):\nGeography.\nTopography.\nThe relief of Greater Poland, geological conditions and soil have been shaped by two glaciations:\nThe highest elevation is Greater Kobyla Mountain () in the Ostrzeszowski Hills, the lowest area is located in the valley of the Warta River at the mouth of its tributary the Note\u0107 () in the north-western part of the region. Agriculturally fertile soils account for around 60% of the province's area, while 20%, the rest of the non-forested or urban areas, is mostly wetland soil (muck-peat and alluvial soils).\nAn area of approximately is covered by forests, this represents around 25.8% of the total surface area of the region.\nIn the lake districts of the northern and central parts of the province there are about 800 lakes; 58% of which cover an area of at least and 8%, with an area exceeding . The largest reservoir is the natural Greater Powidzkie Lake () in the Gniezno Lake District.\nWielkopolska Region lies within the basin of the Oder River, 88% of the province's surface water drains into the Warta river basin, and the remaining 12% is drained by a multitude of other river systems, including the Barycz, Ladislaus Trench and Obrzycy waterways. The quality of river waters is generally poor, but their condition is gradually improving and should soon be classed as 'clean'.\nGeology.\nThe main mineral energy resources in Greater Poland are lignite, natural gas, oil and peat.\nBrown coal deposits are currently mined in the Konin area, and form the basis for the province's power industry (the P\u0105tn\u00f3w-Adams-Konin coal-fired power stations account for more than 10% of the national electricity production). The region also has significant quantities of peat deposits; it is calculated that there are ca. of land covered with an average thickness of of peat. An abundance of raw materials used in the production of numerous medicines was recently discovered in the muds of B\u0142a\u017cejewo, Oderbank and Mechnacz. In addition, very large deposits of brown coal have been discovered in the vicinity of Ko\u015bcian, these however are not currently being extracted and probably never will be extracted, due to the expense that would be incurred in adapting the site to build a coal mine and the need to resettle thousands of people.\nRock salt is mined intensively at a salt mine in K\u0142odawa (this mine alone accounts for about 20% of domestic production).\nThroughout the province there are significant deposits of aggregates, gypsum, ceramic materials, and lacustrine chalk. In Ko\u015bcian the largest and most modern, a natural gas production site is in operation. It supplies raw material for Ko\u015bcia\u0144ska Zieme, and Zielona Gora CHP. It is estimated that at the rate local gas reserves are being exploited, the reserves in Ko\u015bcian will be enough for about 20 years of operation, thus practically allowing for local independence against the effects of gas crises.\nClimate.\nWielkopolska is influenced by oceanic air masses that affect the mildness of the climate. The farther east one travels the more distinctly continental the climate becomes. The area is situated in the Silesian Greater Poland agro-climatic region where the average annual temperature is about 8.2\u00a0\u00b0C, and in the north drops to around 7.6\u00a0\u00b0C. It is slightly warmer in the south and west where the average temperature is usually about 8.5\u00a0\u00b0C. The number of days with snow can reach up to 57 days in and around the Kalisz district.\nThe growing season is one of the longest in Poland. On the province's southern plains this season constitutes around 228 days, while north of Gniezno and Szamotu\u0142y this gradually declines to 216 days.\nPrecipitation ranges from 500 to 550\u00a0mm. Despite this the region is still faced with a deficit in rainfall, particularly in the eastern part of the province (around S\u0142upcy, Kazimierz Biskupi, Kleczew) where sometimes experience only 450\u00a0mm of rainfall per year, this threatens steppization of the region. Throughout the province there is typically a prevailing westerly wind.\nTransportation.\nGreater Poland is a major transport hub within Poland; a great deal of traffic from Russia and other states of the former Soviet Union passes through Pozna\u0144 and Konin to reach Germany and other EU member states. To the south runs the international route from Gda\u0144sk via Pozna\u0144 and Leszno to Prague and then to the south of Europe. There is also a major highway in the province, the A2 motorway, which when completed will run from the western border of Poland with Germany, through Pozna\u0144 to Warsaw and then via Belarus to Moscow.\nThe main railway hubs located in Greater Poland are Pozna\u0144, Pi\u0142a and Ostr\u00f3w Wielkopolski. PKP Intercity operate a number of trains a day between Warsaw and Berlin which provide a fast connection for the two cities also to Pozna\u0144. This route was the first in Poland, adapted for use by the European high-speed transportation system. The Pozna\u0144 G\u0142\u00f3wny railway station is the second busiest railway station in Poland.\nIn the near future the government expects to construct a high-speed rail line in the shape of a Y connecting Kalisz and Pozna\u0144 from \u0141\u00f3d\u017a, Warsaw and Wroc\u0142aw.\nPozna\u0144 is the port of arrival for most international travellers as it plays host to \u0141awica International Airport, which has recently seen the second-highest passenger growth rate in the country.\nEconomy.\nThe Gross domestic product (GDP) of the province was 40.4 billion \u20ac in 2018, accounting for 8.1% of Polish economic output. GDP per capita adjusted for purchasing power was 19,700 \u20ac or 65% of the EU27 average in the same year. The GDP per employee was 72% of the EU average.\nPolitics.\nThe Greater Poland voivodeship's government is headed by the province's voivode \"(governor)\" who is appointed by the Polish Prime Minister. The voivode is then assisted in performing his duties by the voivodeship's marshal, who is the appointed speaker for the voivodeship's executive and is elected by the sejmik \"(provincial assembly)\". The current voivode of Greater Poland is \u0141ukasz Miko\u0142ajczyk, whilst the present marshal is Marek Wo\u017aniak.\nThe Sejmik of Greater Poland consists of 39 members.\nAdministrative division.\nGreater Poland Voivodeship is divided into 35 counties (powiats): 4 city counties and 31 land counties. These are further divided into 226 gminas.\nThe counties are listed in the following table (ordering within categories is by decreasing population).\nProtected areas.\nProtected areas in Greater Poland Voivodeship include two National Parks and 12 Landscape Parks. These are listed below.\nSights.\nGreater Poland Voivodeship boasts 11 Historic Monuments of Poland:\nThe province is rich in historic architecture ranging from Romanesque and Gothic to Renaissance, Baroque and Art Nouveau. Numerous towns possess preserved historic market squares and town halls. The voivodeship is abundant in palaces, including in Antonin (often visited by Fryderyk Chopin), Czempi\u0144, Kobylniki, Ko\u0142aczkowo (former home of Nobel Prize-winning novelist W\u0142adys\u0142aw Reymont), Objezierze (visited by writers Adam Mickiewicz and J\u00f3zef Ignacy Kraszewski) and \u015amie\u0142\u00f3w (former place of stay of Adam Mickiewicz).\nThere are numerous World War II memorials in the province, including memorials at the sites of Nazi massacres of Poles, and museums at the sites of the former Che\u0142mno extermination camp, Fort VII concentration camp in Pozna\u0144, and prison camp in Lubo\u0144. The W\u0142adys\u0142aw Golus Regional Museum in Ostrzesz\u00f3w, a town which was the location of the main German-operated prisoner of war camp for Norwegian POWs in occupied Poland during the war, hosts an exhibition devoted to the history of the Norwegian POWs.\nPoland's largest church, the Basilica of Our Lady of Liche\u0144, is located in the voivodeship.\nThere is an underground touristic route in the K\u0142odawa Salt Mine, considered the world's deepest underground tourist route.\nOne of the two principal and five total cemeteries of the Commonwealth War Graves Commission in Poland is located in Pozna\u0144, with more than 400 burials from both world wars.\nThe oldest preserved European signpost beyond the boundaries of the former Roman Empire is located in Konin.\nCuisine.\nIn addition to traditional nationwide Polish cuisine, Greater Poland Voivodeship is known for its variety of regional and local traditional foods and drinks, which include especially various meat products (incl. various types of kie\u0142basa), cheeses, honeys, beverages and various dishes and meals, officially protected by the Ministry of Agriculture and Rural Development of Poland. Among the most known local snacks are the St. Martin's croissant from Pozna\u0144 and Kalisz andruts.\nNotable centers of traditional meat production include Grodzisk Wielkopolski, Krotoszyn, Kruszewnia, Nowy Tomy\u015bl, Ostrzesz\u00f3w, Rawicz, Trzcianka and Z\u0142otniki, whereas centers of traditional cheese and quark production include W\u0105growiec, Gniezno, K\u0119pno, Oborniki, Witkowo, Witoldzin and Wrze\u015bnia.\nGrodzisk Wielkopolski is the place of origin of the Grodziskie beer style. Other traditional Polish beers, officially protected by the Ministry of Agriculture and Rural Development of Poland, are produced in Bojanowo, Czarnk\u00f3w and Mi\u0142os\u0142aw.\nSports.\nFootball and speedway enjoy the largest following in the province, with top football clubs being Lech Pozna\u0144 and Warta Pozna\u0144, and Poland's most accomplished speedway team being Unia Leszno.\nSince the establishment of the province, several international sports competitions were co-hosted by the province, including the EuroBasket 2009 and UEFA Euro 2012.\nSzczypiorno, Kalisz is considered the cradle of Polish handball.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41499", "revid": "8908655", "url": "https://en.wikipedia.org/wiki?curid=41499", "title": "PCS System", "text": ""}
{"id": "41500", "revid": "1278173489", "url": "https://en.wikipedia.org/wiki?curid=41500", "title": "Penetration", "text": "Penetration may refer to:\n&lt;templatestyles src=\"Template:TOC_right/styles.css\" /&gt;\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "41501", "revid": "57939", "url": "https://en.wikipedia.org/wiki?curid=41501", "title": "Performance management", "text": ""}
{"id": "41502", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41502", "title": "Performance measurement period", "text": "Teletraffic period\nIn telecommunications, performance measurement period is the period during which performance parameters are measured. \nA performance measurement period is determined by required confidence limits and may vary as a function of the observed parameter values. User time is divided into consecutive performance measurement periods to enable measurement of user information transfer reliability.\nReferences.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41503", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41503", "title": "Periscope antenna", "text": "In telecommunications, a periscope antenna is an antenna configuration in which the transmitting antenna is oriented to produce a vertical radiation pattern, and a flat or off-axis parabolic reflector, mounted above the transmitting antenna, is used to direct the beam in a horizontal path toward the receiving antenna. \nA periscope antenna facilitates increased terrain clearance without long transmission lines, while permitting the active equipment to be located at or near ground level for ease of maintenance."}
{"id": "41504", "revid": "1220790", "url": "https://en.wikipedia.org/wiki?curid=41504", "title": "Permanent virtual circuit", "text": ""}
{"id": "41506", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41506", "title": "Personal mobility", "text": "In Universal Personal Telecommunications (UPT), personal mobility is the ability of a user to access telecommunication services at any UPT terminal on the basis of a personal identifier, and the capability of the network to provide those services in accord with the user's service profile. \nPersonal mobility involves the network's capability to locate the terminal associated with the user for the purposes of addressing, routing, and charging the user for calls. \"Access\" is intended to convey the concepts of both originating and terminating services. Management of the service profile by the user is not part of personal mobility. The personal mobility aspects of personal communications are based on the UPT number.\nReferences.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41507", "revid": "2141280", "url": "https://en.wikipedia.org/wiki?curid=41507", "title": "Phantom circuit", "text": "Circuit within a circuit\nIn telecommunications and electrical engineering, a phantom circuit is an electrical circuit derived from suitably arranged wires with one or more conductive paths being a circuit in itself and at the same time acting as one conductor of another circuit.\nPhantom group.\nA phantom group is composed of three circuits that are derived from two single-channel circuits to form a \"phantom circuit\". Here the phantom circuit is a third circuit derived from two suitably arranged pairs of wires, called side circuits, with each pair of wires being a circuit in itself and at the same time acting as one conductor of the third circuit. The \"side circuits\" within phantom circuits can be coupled to their respective voltage drops by center-tapped transformers, usually called \"repeating coils\". The center taps are on the line side of the side circuits. Current from the phantom circuit is split evenly by the center taps. This cancels crosstalk from the phantom circuit to the side circuits. \nPhantom working increased the number of circuits on long-distance routes in the early 20th century without putting up more wires. Phantoming declined with the adoption of carrier systems.\nIt is theoretically possible to create a phantom circuit from two other phantom circuits and so on up in a pyramid with a maximum 2n-1 circuits being derived from n original circuits. However, more than one level of phantoming is usually impractical. Isolation between the phantom circuit and the side circuits relies on accurate balance of the line and transformers. Imperfect balance results in crosstalk between the phantom and side circuits and this effect accumulates as each level of phantoms is added. Even small levels of crosstalk are unacceptable on analogue telecommunications circuits since speech crosstalk is still intelligible down to quite low levels.\nPhantom microphone powering.\nCondenser microphones have impedance converter (current amplifier) circuitry that requires powering; in addition, the capsule of any non-electret, non-RF condenser microphone requires a polarizing voltage to be applied. Since the mid- to late 1960s most balanced, professional condenser microphones for recording and broadcast have used phantom powering. It can be provided by outboard AC or battery supplies, but nowadays is most often built into the mixing console, recorder or microphone preamplifier to which the microphones are connected.\nThe most common circuit uses +48 V DC fed through a matched pair of 6.8 k\u03a9 resistors for each input channel. This arrangement has been standardized by the IEC and ISO, along with a less-commonly-used arrangement with +12 V DC and 680 \u03a9 feed resistors.\nAs a practical matter, phantom powering allows the same two-conductor shielded cables to be used for both dynamic microphones and condenser microphones, while being harmless to balanced microphones that aren't designed to consume it, since the circuit balance prevents any substantial DC from flowing through the output circuit of those microphones.\nDC phantom.\nSimple DC signalling can be achieved on a telecommunications line in a similar way to phantom powering of microphones. A switch connected to the transformer centre-tap at one end of the line can operate a similarly connected relay at the other end. The return path is through the ground connection. This arrangement can be used for remotely controlling equipment.\nCarrier circuit phantoms.\nFrom the 1950s to around the 1980s, using phantoms on star-quad trunk carrier circuits was a popular method of deriving a high quality broadcast audio circuit. The multiplexed FDM telecommunications carrier system usually did not use the baseband of the cable because it was inconvenient to separate low frequencies with filters. On the other hand, a one-way audio phantom could be formed from the two pairs (go and return signals) making up the star-quad cable.\nUnloaded phantom.\nUnloaded phantom is a phantom configuration of loaded lines (a circuit fitted with loading coils). The idea here is not to create additional circuits. Rather, the purpose is to cancel or greatly reduce the effect of the loading coils fitted to a line. The reason for doing this is that loaded lines have a definite cut-off frequency and it may be desired to equalise the line to a frequency which is higher than this, for example to make a circuit suitable for use by a broadcaster. Ideally, the loading would be removed or reduced for a permanent connection, but this is not feasible for temporary arrangements such as a requirement for outside broadcast. Instead, two circuits in a phantom configuration can be used to greatly reduce the inductance being inserted by the loading coils, and hence the loading effect.\nIt works because the loading coils used on balanced lines have two windings, one for each leg of the circuit. They are both wound on a common core and the windings are so arranged that the magnetic flux induced by both of them is in the same direction. Both windings induce an emf in each other as well as their own self-induction. This effect greatly increases the inductance of the coil and hence its loading effectiveness. By contrast, when the circuit is in the phantom configuration the currents in the two wires of each pair are in the same direction and the magnetic flux is being cancelled. This has precisely the opposite effect and the inductance is greatly reduced.\nThis configuration is most commonly used on the two pairs of a star-quad cable. It is not so successful with other pairs of wires. The difference in the path of the two pairs can easily destroy the balance and results in crosstalk and interference.\nThis configuration can also be called \"bunched pairs\". However, \"bunched pairs\" can also refer to the straightforward connection of two lines in parallel which is not a phantom circuit and will not reduce the loading.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "41508", "revid": "11292982", "url": "https://en.wikipedia.org/wiki?curid=41508", "title": "Phase angle (vectors, phasors, and periodic phenomena)", "text": ""}
{"id": "41509", "revid": "22732625", "url": "https://en.wikipedia.org/wiki?curid=41509", "title": "Phased array", "text": "Array of antennas creating a steerable beam\nIn antenna theory, a phased array usually means an electronically scanned array, a computer-controlled array of antennas which creates a beam of radio waves that can be electronically steered to point in different directions without moving the antennas.\nIn a phased array, the power from the transmitter is fed to the radiating elements through devices called \"phase shifters\", controlled by a computer system, which can alter the phase or signal delay electronically, thus steering the beam of radio waves to a different direction. Since the size of an antenna array must extend many wavelengths to achieve the high gain needed for narrow beamwidth, phased arrays are mainly practical at the high frequency end of the radio spectrum, in the UHF and microwave bands, in which the operating wavelengths are conveniently small.\nPhased arrays were originally invented for use in military radar systems, to detect fast moving planes and missiles, but are now widely used and have spread to civilian applications such as 5G MIMO for cell phones. The phased array principle is also used in acoustics in such applications as phased array ultrasonics, and in optics.\nThe term \"phased array\" is also used to a lesser extent for unsteered array antennas in which the radiation pattern of the antenna array is fixed. For example, AM broadcast radio antennas consisting of multiple mast radiators are also called \"phased arrays\".\nDescription.\nA phased array is an electronically scanned array, a computer-controlled array of antennas which creates a beam of radio waves that can be electronically steered to point in different directions without moving the antennas.\nThe general theory of an electromagnetic phased array also finds applications in ultrasonic and medical imaging application (\"phased array ultrasonics\") and in optics (\"optical phased array\").\nIn a simple array antenna, the radio frequency current from the transmitter is fed to multiple individual antenna elements with the proper phase relationship so that the radio waves from the separate elements combine (superpose) to form beams. This can be configured to increase power radiated in desired directions and suppress radiation in undesired directions.\nIn a phased array, the power from the transmitter is fed to the radiating elements through devices called \"phase shifters\", controlled by a computer system. The computer can alter the phase or signal delay of each antenna element electronically, resulting in a beam of radio waves that can be dynamically \"steered\" to propagate in arbitrary directions.\nPhased arrays were originally conceived for use in military radar systems, to steer a beam of radio waves quickly across the sky to detect planes and missiles. These systems are now widely used and have spread to civilian applications such as 5G MIMO for cell phones. The phased array principle is also used in acoustics, and phased arrays of acoustic transducers are used in medical ultrasound imaging scanners (phased array ultrasonics), oil and gas prospecting (reflection seismology), and military sonar systems.\nThe term \"phased array\" is also used to a lesser extent for non steerable array antennas in which the phase of the feed power and thus the radiation pattern of the antenna array is fixed. For example, AM broadcast radio antennas consisting of multiple mast radiators fed so as to create a specific radiation pattern are also called \"phased arrays\".\nTypes.\nPhased arrays take multiple forms. However, the four most common are the passive electronically scanned array (PESA), active electronically scanned array (AESA), hybrid beam forming phased array, and digital beam forming (DBF) array.\nA \"passive phased array\" or \"passive electronically scanned array\" (PESA) is a phased array in which the antenna elements are connected to a single transmitter and/or receiver, as shown in the first animation at top. PESAs are the most common type of phased array. Generally speaking, a PESA uses one receiver/exciter for the entire array.\nAn \"active phased array\" or \"active electronically scanned array\" (AESA) is a phased array in which each antenna element has an analog transmitter/receiver (T/R) module which creates the phase shifting required to electronically steer the antenna beam. Active arrays are a more advanced, second-generation phased-array technology that are used in military applications; unlike PESAs they can radiate several beams of radio waves at multiple frequencies in different directions simultaneously. However, the number of simultaneous beams is limited by practical reasons of electronic packaging of the beam formers to approximately three simultaneous beams for an AESA. Each beam former has a receiver/exciter connected to it.\nA \"digital beam forming (DBF) phased array\" has a digital receiver/exciter at each element in the array. The signal at each element is digitized by the receiver/exciter. This means that antenna beams can be formed digitally in a field programmable gate array (FPGA) or the array computer. This approach allows for multiple simultaneous antenna beams to be formed.\nA \"hybrid beam forming phased array\" can be thought of as a combination of an AESA and a digital beam forming phased array. It uses subarrays that are active phased arrays (for instance, a subarray may be 64, 128 or 256 elements and the number of elements depends upon system requirements). The subarrays are combined to form the full array. Each subarray has its own digital receiver/exciter. This approach allows clusters of simultaneous beams to be created.\nA \"conformal antenna\" is a phased array in which the individual antennas, instead of being arranged in a flat plane, are mounted on a curved surface. The phase shifters compensate for the different path lengths of the waves due to the antenna elements' varying position on the surface, allowing the array to radiate a plane wave. Conformal antennas are used in aircraft and missiles, to integrate the antenna into the curving surface of the aircraft to reduce aerodynamic drag.\nTime and frequency domains.\nThere are two main types of beamformers. These are time domain beamformers and frequency domain beamformers. From a theoretical point of view, both are in principle the same operation, with just a Fourier transform allowing conversion from one to the other type.\nA graduated attenuation window is sometimes applied across the face of the array to improve side-lobe suppression performance, in addition to the phase shift.\nTime domain beamformer works by introducing time delays. The basic operation is called \"delay and sum\". It delays the incoming signal from each array element by a certain amount of time, and then adds them together. A Butler matrix allows several beams to be formed simultaneously, or one beam to be scanned through an arc. The most common kind of time domain beam former is serpentine waveguide. Active phased array designs use individual delay lines that are switched on and off. Yttrium iron garnet phase shifters vary the phase delay using the strength of a magnetic field.\nThere are two different types of frequency domain beamformers.\nThe first type separates the different frequency components that are present in the received signal into multiple frequency bins (using either a Discrete Fourier transform (DFT) or a filterbank). When different delay and sum beamformers are applied to each frequency bin, the result is that the main lobe simultaneously points in multiple different directions at each of the different frequencies. This can be an advantage for communication links, and is used with the SPS-48 radar.\nThe other type of frequency domain beamformer makes use of Spatial Frequency. Discrete samples are taken from each of the individual array elements. The samples are processed using a DFT. The DFT introduces multiple different discrete phase shifts during processing. The outputs of the DFT are individual channels that correspond with evenly spaced beams formed simultaneously. A 1-dimensional DFT produces a fan of different beams. A 2-dimensional DFT produces beams with a pineapple configuration.\nThese techniques are used to create two kinds of phased array.\n* Dynamic\u00a0\u2013 an array of variable phase shifters are used to move the beam\n* Fixed\u00a0\u2013 the beam position is stationary with respect to the array face and the whole antenna is moved\nThere are two further sub-categories that modify the kind of dynamic array or fixed array.\n* Active\u00a0\u2013 amplifiers or processors are in each phase shifter element\n* Passive\u00a0\u2013 large central amplifier with attenuating phase shifters\nDynamic phased array.\nEach array element incorporates an adjustable phase shifter. These are collectively used to move the beam with respect to the array face.\nDynamic phased arrays require no physical movement to aim the beam. The beam is moved electronically. This can produce antenna motion fast enough to use a small pencil beam to simultaneously track multiple targets while searching for new targets using just one radar set, a capability known as \"track while search\".\nAs an example, an antenna with a 2-degree beam with a pulse rate of 1\u00a0kHz will require approximately 8 seconds to cover an entire hemisphere consisting of 8,000 pointing positions. This configuration provides 12 opportunities to detect a vehicle over a range of , which is suitable for military applications.\nThe position of mechanically steered antennas can be predicted, which can be used to create electronic countermeasures that interfere with radar operation. The flexibility resulting from phased array operation allows beams to be aimed at random locations, which eliminates this vulnerability. This is also desirable for military applications.\nFixed phased array.\nFixed phased array antennas are typically used to create an antenna with a more desirable form factor than the conventional parabolic reflector or cassegrain reflector. Fixed phased arrays incorporate fixed phase shifters. For example, most commercial FM Radio and TV antenna towers use a collinear antenna array, which is a fixed phased array of dipole elements.\nIn radar applications, this kind of phased array is physically moved during the track and scan process. There are two configurations.\n* Multiple frequencies with a delay-line\n* Multiple adjacent beams\nThe SPS-48 radar uses multiple transmit frequencies with a serpentine delay line along the left side of the array to produce vertical fan of stacked beams. Each frequency experiences a different phase shift as it propagates down the serpentine delay line, which forms different beams. A filter bank is used to split apart the individual receive beams. The antenna is mechanically rotated.\nSemi-active radar homing uses monopulse radar that relies on a fixed phased array to produce multiple adjacent beams that measure angle errors. This form factor is suitable for gimbal mounting in missile seekers.\nActive phased array.\nActive electronically-scanned arrays (AESA) elements incorporate transmit amplification with phase shift in each antenna element (or group of elements). Each element also includes receive pre-amplification. The phase shifter setting is the same for transmit and receive.\nActive phased arrays do not require phase reset after the end of the transmit pulse, which is compatible with Doppler radar and pulse-Doppler radar.\nPassive phased array.\nPassive phased arrays typically use large amplifiers that produce all of the microwave transmit signal for the antenna. Phase shifters typically consist of waveguide elements controlled by magnetic field, voltage gradient, or equivalent technology.\nThe phase shift process used with passive phased arrays typically puts the receive beam and transmit beam into diagonally opposite quadrants. The sign of the phase shift must be inverted after the transmit pulse is finished and before the receive period begins to place the receive beam into the same location as the transmit beam. That requires a phase impulse that degrades sub-clutter visibility performance on Doppler radar and Pulse-Doppler radar. As an example, Yttrium iron garnet phase shifters must be changed after transmit pulse quench and before receiver processing starts to align transmit and receive beams. That impulse introduces FM noise that degrades clutter performance.\nPassive phased array design is used in the AEGIS Combat System for direction-of-arrival estimation.\nHistory.\nPhased array transmission was originally shown in 1905 by Nobel laureate Karl Ferdinand Braun who demonstrated enhanced transmission of radio waves in one direction. During World War II, Nobel laureate Luis Alvarez used phased array transmission in a rapidly steerable radar system for \"ground-controlled approach\", a system to aid in the landing of aircraft. At the same time, the German GEMA company (German for \"Gesellschaft f\u00fcr elektroakustische und mechanische Apparate\") built the Mammut 1. It was later adapted for radio astronomy leading to Nobel Prizes for Physics for Antony Hewish and Martin Ryle after several large phased arrays were developed at the University of Cambridge Interplanetary Scintillation Array. This design is also used for radar, and is generalized in interferometric radio antennas.\nIn 1966, most phased-array radars use ferrite phase shifters or traveling-wave tubes to dynamically adjust the phase.\nThe AN/SPS-33 -- installed on the nuclear-powered ships Long Beach and Enterprise around 1961 -- was claimed to be the only operational 3-D phased array in the world in 1966.\nThe AN/SPG-59 was designed to generate multiple tracking beams from the transmitting array and simultaneously program independent receiving arrays.\nThe first civilian 3D phased array was built in 1960 at the National Aviation Facilities Experimental Center; but was abandoned in 1961.\nIn 2004, Caltech researchers demonstrated the first integrated silicon-based phased array receiver at 24\u00a0GHz with 8 elements. This was followed by their demonstration of a CMOS 24\u00a0GHz phased array transmitter in 2005 and a fully integrated 77\u00a0GHz phased array transceiver with integrated antennas in 2006 by the Caltech team. In 2007, DARPA researchers announced a 16-element phased-array radar antenna which was also integrated with all the necessary circuits on a single silicon chip and operated at 30\u201350\u00a0GHz.\nThe relative amplitudes of\u2014and constructive and destructive interference effects among\u2014the signals radiated by the individual antennas determine the effective radiation pattern of the array. A phased array may be used to point a fixed radiation pattern, or to scan rapidly in azimuth or elevation. Simultaneous electrical scanning in both azimuth and elevation was first demonstrated in a phased array antenna at Hughes Aircraft Company, California in 1957.\nFormulation.\nArray factor.\nThe total directivity of a phased array will be a result of the gain of the individual array elements, and the directivity due their positioning in an array. This latter component is closely tied (but not equal to) to the array factor. In a (rectangular) planar phased array, of dimensions formula_1, with inter-element spacing formula_2 and formula_3, respectively, the array factor can be calculated accordingly:formula_4\nHere, formula_5 and formula_6 are the directions which we are taking the array factor in, in the coordinate frame depicted to the right. The factors formula_7 and formula_8 are the \"progressive phase shift\" that is used to steer the beam electronically. The factors formula_9 and formula_10 are the excitation coefficients of the individual elements.\nBeam steering is indicated in the same coordinate frame, however the direction of steering is indicated with formula_11 and formula_12, which is used in calculation of progressive phase:\nformula_13\nformula_14\nIn all above equations, the value formula_15 describes the wavenumber of the frequency used in transmission.\nThese equations can be solved to predict the nulls, main lobe, and grating lobes of the array. Referring to the exponents in the array factor equation, we can say that major and grating lobes will occur at integer formula_16 solutions to the following equations:\nformula_17\nformula_18\nWorked example.\nIt is common in engineering to provide phased array formula_19 values in decibels through formula_20. Recalling the complex exponential in the array factor equation above, often, what is \"really\" meant by array factor is the magnitude of the summed phasor produced at the end of array factor calculation. With this, we can produce the following equation:formula_21For the ease of visualization, we will analyze array factor given an input \"azimuth and elevation\", which we will map to the array frame formula_5 and formula_6 through the following conversion:\nformula_24\nformula_25\nThis represents a coordinate frame whose formula_26 axis is aligned with the array formula_27 axis, and whose formula_28 axis is aligned with the array formula_26 axis.\nIf we consider a formula_30 phased array, this process provides the following values for formula_31, when steering to bore-sight (formula_32,formula_33):\nThese values have been clipped to have a minimum formula_19 of -50 dB, however, in reality, null points in the array factor pattern will have values significantly smaller than this.\nApplications.\nRadar.\nPhased arrays were invented for radar tracking of ballistic missiles, and because of their fast tracking abilities phased array radars are widely used in military applications. For example, because of the rapidity with which the beam can be steered, phased array radars allow a warship to use one radar system for surface detection and tracking (finding ships), air detection and tracking (finding aircraft and missiles) and missile uplink capabilities. Before using these systems, each surface-to-air missile in flight required a dedicated fire-control radar, which meant that radar-guided weapons could only engage a small number of simultaneous targets. Phased array systems can be used to control missiles during the mid-course phase of the missile's flight. During the terminal portion of the flight, continuous-wave fire control directors provide the final guidance to the target. Because the antenna pattern is electronically steered, phased array systems can direct radar beams fast enough to maintain a fire control quality track on many targets simultaneously while also controlling several in-flight missiles.\nThe AN/SPY-1 phased array radar, part of the Aegis Combat System deployed on modern U.S. cruisers and destroyers, \"is able to perform search, track and missile guidance functions simultaneously with a capability of over 100 targets.\" Likewise, the Thales Herakles phased array multi-function radar used in service with France and Singapore has a track capacity of 200 targets and is able to achieve automatic target detection, confirmation and track initiation in a single scan, while simultaneously providing mid-course guidance updates to the MBDA Aster missiles launched from the ship. The German Navy and the Royal Dutch Navy have developed the Active Phased Array Radar System (APAR). The MIM-104 Patriot and other ground-based antiaircraft systems use phased array radar for similar benefits.\nSonar.\nPhased arrays are used in naval sonar, in active (transmit and receive) and passive (receive only) and hull-mounted and towed array sonar.\nOne of first acoustic phased arrays was the German Gruppenhorchger\u00e4t device.\nIn acoustics, microphone arrays and line arrays of loudspeakers are also used.\nSpace probe communication.\nThe \"MESSENGER\" spacecraft was a space probe mission to the planet Mercury (2011\u20132015). This was the first deep-space mission to use a phased-array antenna for communications. The radiating elements are circularly-polarized, slotted waveguides. The antenna, which uses the X band, used 26 radiative elements and can gracefully degrade.\nWeather research usage.\nThe National Severe Storms Laboratory has been using a SPY-1A phased array antenna, provided by the US Navy, for weather research at its Norman, Oklahoma facility since April 23, 2003. It is hoped that research will lead to a better understanding of thunderstorms and tornadoes, eventually leading to increased warning times and enhanced prediction of tornadoes. Current project participants include the National Severe Storms Laboratory and National Weather Service Radar Operations Center, Lockheed Martin, United States Navy, University of Oklahoma School of Meteorology, School of Electrical and Computer Engineering, and Atmospheric Radar Research Center, Oklahoma State Regents for Higher Education, the Federal Aviation Administration, and Basic Commerce and Industries. The project includes research and development, future technology transfer and potential deployment of the system throughout the United States. It is expected to take 10 to 15 years to complete and initial construction was approximately $25 million. A team from Japan's RIKEN Advanced Institute for Computational Science (AICS) has begun experimental work on using phased-array radar with a new algorithm for instant weather forecasts.\nOptics.\nWithin the visible and infrared spectrum of electromagnetic waves it is possible to construct optical phased arrays (OPAs) which allow for dynamic beam forming and beam steering without mechanically moving parts. They are used in wavelength multiplexers and filters for telecommunication purposes, as well as in Lidar, Free-space optical communication, and holography. OPAs were also shown to enable lensless projectors, lensless cameras, and chip-scale optical tweezers.\nDue to the short wavelengths OPAs are typically realised in nanofabricated photonic integrated circuit platforms utilising materials such as silicon on insulator, germanium on silicon, silicon nitride or polymers.\nSynthetic array heterodyne detection is an efficient method for multiplexing an entire phased array onto a single element photodetector.\nSatellite broadband internet transceivers.\nStarlink is a low Earth orbit satellite constellation that is available in over a hundred countries. It provides broadband internet connectivity to consumers; the user terminals of the system use phased array antennas.\nRadio-frequency identification (RFID).\nBy 2014, phased array antennas were integrated into RFID systems to increase the area of coverage of a single system by 100% to while still using traditional passive UHF tags.\nHuman-machine interfaces (HMI).\nA phased array of acoustic transducers, denominated airborne ultrasound tactile display (AUTD), was developed in 2008 at the University of Tokyo's Shinoda Lab to induce tactile feedback. This system was demonstrated to enable a user to interactively manipulate virtual holographic objects.\nRadio astronomy.\nPhased Array Feeds (PAF) have recently been used at the focus of radio telescopes to provide many beams, giving the radio telescope a very wide field of view. Three examples are the ASKAP telescope in Australia, the Apertif upgrade to the Westerbork Synthesis Radio Telescope in the Netherlands, and the Florida Space Institute in the United States .\nBroadcasting.\nIn broadcast engineering, the term 'phased array' has a meaning different from its normal meaning, it means an ordinary array antenna, an array of multiple mast radiators designed to radiate a directional radiation pattern, as opposed to a single mast which radiates an omnidirectional pattern. Broadcast phased arrays have fixed radiation patterns and are not 'steered' during operation as are other phased arrays.\nPhased arrays are used by many AM broadcast radio stations to enhance signal strength and therefore coverage in the city of license, while minimizing interference to other areas. Due to the differences between daytime and nighttime ionospheric propagation at mediumwave frequencies, it is common for AM broadcast stations to change between day (groundwave) and night (skywave) radiation patterns by switching the phase and power levels supplied to the individual antenna elements (mast radiators) daily at sunrise and sunset. For shortwave broadcasts many stations use arrays of horizontal dipoles. A common arrangement uses 16 dipoles in a 4\u00d74 array. Usually this is in front of a wire grid reflector. The phasing is often switchable to allow beam steering in azimuth and sometimes elevation.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41510", "revid": "1246887251", "url": "https://en.wikipedia.org/wiki?curid=41510", "title": "Phase distortion", "text": "Nonlinear phase response to filters\nIn signal processing, phase distortion or phase-frequency distortion is distortion, that is, change in the shape of the waveform, that occurs when (a) a filter's phase response is not linear over the frequency range of interest, that is, the phase shift introduced by a circuit or device is not directly proportional to frequency, or (b) the zero-frequency intercept of the phase-frequency characteristic is not 0 or an integral multiple of 2\u03c0 radians.\nAudibility of phase distortion.\nGrossly changed phase relationships, without changing amplitudes, can be audible but the degree of audibility of the type of phase shifts expected from typical sound systems remains debated.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt; \n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41512", "revid": "50437975", "url": "https://en.wikipedia.org/wiki?curid=41512", "title": "Voivodeships of Poland", "text": "Highest-level administrative division of Poland\nA voivodeship ( ; ; plural: ) is the highest-level administrative division of Poland, corresponding to a province in many other countries. The term has been in use since the 14th century and is commonly translated into English as \"province\".\nThe Polish local government reforms adopted in 1998, which went into effect on 1 January 1999, reduced the number of voivodeships to sixteen. These 16 replaced the 49 former voivodeships that had existed from 1 July 1975, and bear a greater resemblance (in territory, but not in name) to the voivodeships that existed between 1950 and 1975.\nToday's voivodeships are mostly named after historical and geographical regions, while those prior to 1998 generally took their names from the cities on which they were centered. The new units range in area from under (Opole Voivodeship) to over (Masovian Voivodeship), and in population from nearly one million (Opole Voivodeship) to over five million (Masovian Voivodeship).\nAdministrative authority at the voivodeship level is shared between a government-appointed governor called a voivode (), an elected assembly called a , and an executive board () chosen by that assembly, headed by a voivodeship marshal (). Voivodeships are further divided into ('counties') and ('communes' or 'municipalities'), the smallest administrative divisions of Poland.\nEtymology and use.\nSome English-language sources, in historical contexts, speak of \"palatinates\" rather than \"voivodeships\". The term \"palatinate\" traces back to the Latin , which traces back to \"palatium\" (\"palace\").\nMore commonly used now is \"province\" or \"voivodeship\". The latter is a loanword-calque hybrid formed on the Polish \"\".\nSome writers argue against rendering in English as \"province\", on historical grounds: before the third, last Partition of the Polish\u2013Lithuanian Commonwealth, in 1795, each of the main constituent \"regions\" of the Polish\u2013Lithuanian Commonwealth\u2014Greater Poland, Lesser Poland, Lithuania, and Royal Prussia\u2014was sometimes idiosyncratically referred to as a \"province\" (). According to the argument, such a \"prowincja\" (for example, Greater Poland) cannot consist of a number of subdivisions (\", the plural of \") that are likewise called \"provinces\". This, however, is an antiquarian consideration, as the word \"province\" has not been used in Poland in this sense of a \"region\" for over two centuries; and those former larger political units, all now obsolete, can now be referred to in English as what they actually were: \"regions\".\nThe Polish , designating a second-tier Polish or Polish\u2013Lithuanian administrative unit, derives from , (etymologically, a 'warlord', 'war leader' or 'leader of warriors', giving it the same etymology as the English word \"Duchy\", but now simply the governor of a ) and the suffix (a \"state or condition\").\nThe English \"voivodeship\", which is a hybrid of the loanword \"voivode\" and \"-ship\" (the latter a suffix that calques the Polish suffix ), has never been much used and is absent from many dictionaries. According to the \"Oxford English Dictionary\", it first appeared in 1792, spelt \"woiwodship\", in the sense of \"the district or province governed by a voivode.\" The word subsequently appeared in 1886 also in the sense of \"the office or dignity of a voivode.\"\nPoland's Commission on Standardization of Geographic Names outside the Republic of Poland prefers the form which omits the 'e', recommending the spelling \"voivodship\", for use in English.\nCurrent.\nAdministrative powers.\nCompetences and powers at voivodeship level are shared between the voivode (governor), the sejmik (regional assembly) and the marshal. In most cases these institutions are all based in one city, but in Kuyavian-Pomeranian and Lubusz Voivodeship the voivode's offices are in a different city from those of the executive and the sejmik. Voivodeship capitals are listed in the table below.\nThe \"voivode\" is appointed by the Prime Minister and is the regional representative of the central government. The voivode acts as the head of central government institutions at regional level (such as the police and fire services, passport offices, and various inspectorates), manages central government property in the region, oversees the functioning of local government, coordinates actions in the field of public safety and environment protection, and exercises special powers in emergencies. The voivode's offices collectively are known as the .\nThe is elected every five years. (The first of the five-year terms began in 2018; previous terms lasted four years.)) Elections for the fall at the same time as that of local authorities at and level. The passes by-laws, including the voivodeship's development strategies and budget. It also elects the and other members of the executive, and holds them to account.\nThe \"executive\" (), headed by the drafts the budget and development strategies, implements the resolutions of the , manages the voivodeship's property, and deals with many aspects of regional policy, including management of European Union funding. The marshal's offices are collectively known as the .\nEconomies.\nAccording to 2017 Eurostat data, the GDP per capita of Polish voivodeships varies notably and there is a large gap between the richest per capita voivodeship (being the Masovian Voivodeship at 33,500 EUR) and the poorest per capita (being the Lublin Voivodeship at 14,400 EUR).\nHistorical development.\nPolish\u2013Lithuanian Commonwealth.\nGreater Poland (\"Wielkopolska\").\nThe following is a list of the Voivodeships within Greater Poland at various points over the period from the mid-16th century until the late 18th century:\nLesser Poland (\"Ma\u0142opolska\").\nThe following is a list of the Voivodeships within Lesser Poland over the period of the mid-16th century until the late 18th century:\nGrand Duchy of Lithuania.\nVoivodeships of the Grand Duchy of Lithuania during the Polish\u2013Lithuanian Commonwealth were based on the administrative structure that existed in the Duchy prior to the Commonwealth's formation, from at least the early-15th century. They were:\nDuchy of Livonia.\nWhile the Duchy of Livonia was part of the Polish\u2013Lithuanian Commonwealth, approximately 1569\u20131772, in various periods it comprised the following voivodeships in varying combinations:\nCongress Poland.\nFrom 1816 to 1837 there were 8 voivodeships in Congress Poland.\nSecond Polish Republic.\nThe administrative division of Poland in the interwar period included 16 voivodeships and Warsaw (with voivodeship rights). The voivodeships that remained in Poland after World War II as a result of Polish\u2013Soviet border agreement of August 1945 were very similar to the current voivodeships.\nPolish People's Republic.\nAfter World War II, the new administrative division of the country within the new national borders was based on the prewar one and included 14 (+2) voivodeships, then 17 (+5). The voivodeships in the east that had not been annexed by the Soviet Union had their borders left almost unchanged. The newly acquired territories in the west and north were organized into the new voivodeships of Szczecin, Wroc\u0142aw and Olsztyn, and partly joined to Gda\u0144sk, Katowice and Pozna\u0144 voivodeships. Two cities were granted voivodeship status: Warsaw and \u0141\u00f3d\u017a.\nIn 1950, new voivodeships were created: Koszalin (previously part of Szczecin), Opole (previously part of Katowice), and Zielona G\u00f3ra (previously part of Pozna\u0144, Wroc\u0142aw and Szczecin voivodeships). In 1957, three more cities were granted voivodeship status: Wroc\u0142aw, Krak\u00f3w and Pozna\u0144.\nPoland's voivodeships 1975\u20131998\nAdministrative division of Poland between 1979 and 1998 included 49 voivodeships upheld after the establishment of the Third Polish Republic in 1989 for another decade. This reorganisation of the administrative division of Poland was mainly a result of the local government reform acts of 1973\u20131975. In place of the three-level administrative division (voivodeship, county, commune), a new two-level administrative division was introduced (49 small voivodeships, and communes). The three smallest voivodeships\u2014Warsaw, Krak\u00f3w and \u0141\u00f3d\u017a\u2014had the special status of municipal voivodeship; the city president (mayor) was also provincial governor.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41515", "revid": "43197772", "url": "https://en.wikipedia.org/wiki?curid=41515", "title": "Cod", "text": "Common name for several fish, but mainly the demersal genus Gadus\nCod (pl.: cod) is the common name for the demersal fish genus \"Gadus\", belonging to the family Gadidae. Cod is also used as part of the common name for a number of other fish species, and one species that belongs to genus \"Gadus\" is not commonly called cod (Alaska pollock, \"Gadus chalcogrammus\").\nThe two most common species of cod are the Atlantic cod (\"Gadus morhua\"), which lives in the colder waters and deeper sea regions throughout the North Atlantic, and the Pacific cod (\"Gadus macrocephalus\"), which is found in both eastern and western regions of the northern Pacific. \"Gadus morhua\" was named by Linnaeus in 1758. (However, \"G. morhua callarias\", a low-salinity, nonmigratory race restricted to parts of the Baltic, was originally described as \"Gadus callarias\" by Linnaeus.)\nCod as food is popular in several parts of the world. It has a mild flavour and a dense, flaky, white flesh. Cod livers are processed to make cod liver oil, a common source of vitamin A, vitamin D, vitamin E, and omega-3 fatty acids (EPA and DHA). Scrod is young Atlantic cod or haddock. In the United Kingdom, Atlantic cod is one of the most common ingredients in fish and chips, along with haddock and plaice.\n&lt;templatestyles src=\"Template:TOC limit/styles.css\" /&gt;\nSpecies.\nAt various times in the past, taxonomists included many species in the genus \"Gadus\". Most of these are now either classified in other genera, or have been recognized as forms of one of three species. All these species have a number of common names, most of them ending with the word \"cod\", whereas other species, as closely related, have other common names (such as pollock and haddock). However, many other, unrelated species also have common names ending with cod. The usage often changes with different localities and at different times.\nCod in the genus \"Gadus\"/True cod.\nThree species in the genus \"Gadus\" are currently called cod:\nThe fourth species of genus Gadus, \"Gadus chalcogrammus\", is commonly called \"Alaska pollock\" or \"walleye pollock\". But there are also less widespread alternative trade names highlighting the fish's belonging to the cod genus, like \"snow cod\" or \"bigeye cod\".\nRelated species.\n\"Cod\" forms part of the common name of many other fish no longer classified in the genus \"Gadus\". Many are members of the family Gadidae; others are members of three related families within the order Gadiformes whose names include the word \"cod\": the morid cods, Moridae (100 or so species); the eel cods, Muraenolepididae (four species); and the Eucla cod, Euclichthyidae (one species). The tadpole cod family (Ranicipitidae) has now been placed in Gadidae.\nSome fish have common names derived from \"cod\", such as codling, codlet, or tomcod. (\"Codling\" is also used as a name for a young cod.)\nOther species.\nSome fish commonly known as cod are unrelated to \"Gadus\". Part of this name confusion is market-driven. Severely shrunken Atlantic cod stocks have led to the marketing of cod replacements using culinary names of the form \"\"x\" cod\", according to culinary rather than phyletic similarity. The common names for the following species have become well established; note that all inhabit the Southern Hemisphere.\nPerciformes.\nFish of the order Perciformes that are commonly called \"cod\" include:\nRock cod, reef cod, and coral cod.\nAlmost all coral cod, reef cod or rock cod are also in order Perciformes. Most are better known as groupers, and belong to the family Serranidae. Others belong to the Nototheniidae. Two exceptions are the Australasian red rock cod, which belongs to a different order (see below), and the fish known simply as the rock cod and as soft cod in New Zealand, \"Lotella rhacina\", which as noted above actually is related to the true cod (it is a morid cod).\nScorpaeniformes.\nFrom the order Scorpaeniformes:\nOphidiiformes.\nThe tadpole cod family, Ranicipitidae, and the Eucla cod family, Euclichthyidae, were formerly classified in the order Ophidiiformes, but are now grouped with the Gadiformes.\nMarketed as cod.\nSome fish that do not have \"cod\" in their names are sometimes sold as cod. Haddock and whiting belong to the same family, the Gadidae, as cod.\nCharacteristics.\nCods of the genus \"Gadus\" have three rounded dorsal and two anal fins. The pelvic fins are small, with the first ray extended, and are set under the gill cover (i.e. the throat region), in front of the pectoral fins. The upper jaw extends over the lower jaw, which has a well-developed chin barbel. The eyes are medium-sized, approximately the same as the length of the chin barbel. Cod have a distinct white lateral line running from the gill slit above the pectoral fin, to the base of the caudal or tail fin. The back tends to be a greenish to sandy brown, and shows extensive mottling, especially towards the lighter sides and white belly. Dark brown colouration of the back and sides is not uncommon, especially for individuals that have resided in rocky inshore regions.\nThe Atlantic cod can change colour at certain water depths. It has two distinct colour phases: gray-green and reddish brown. Its average weight is , but specimens weighing up to have been recorded. Pacific cod are smaller than Atlantic cod and are darker in colour.\nDistribution.\nAtlantic cod (\"Gadus morhua\") live in the colder waters and deeper sea regions throughout the North Atlantic. Pacific cod (\"Gadus macrocephalus\") is found in both eastern and western regions of the Pacific.\nAtlantic cod could be further divided into several stocks, including the Arcto-Norwegian, North Sea, Baltic Sea, Faroe, Iceland, East Greenland, West Greenland, Newfoundland, and Labrador stocks. There seems to be little interchange between the stocks, although migrations to their individual breeding grounds may involve distances of or more. For instance, eastern Baltic cod shows specific reproductive adaptations to low salinity compared to Western Baltic and Atlantic cod.\nAtlantic cod occupy varied habitats, favouring rough ground, especially inshore, and are demersal in depths between , on average, although not uncommonly to depths of . Off the Norwegian and New England coasts and on the Grand Banks of Newfoundland, cod congregate at certain seasons in water of depth. Cod are gregarious and form schools, although shoaling tends to be a feature of the spawning season.\nLife cycle.\nSpawning of northeastern Atlantic cod occurs between January and April (March and April are the peak months), at a depth of in specific spawning grounds at water temperatures between . Around the UK, the major spawning grounds are in the middle to southern North Sea, the start of the Bristol Channel (north of Newquay), the Irish Channel (both east and west of the Isle of Man), around Stornoway, and east of Helmsdale.\nPrespawning courtship involves fin displays and male grunting, which leads to pairing. The male inverts himself beneath the female, and the pair swim in circles while spawning. The eggs are planktonic and hatch between eight and 23 days, with larva reaching in length. This planktonic phase lasts some ten weeks, enabling the young cod to increase its body weight by 40-fold, and growing to about . The young cod then move to the seabed and change their diet to small benthic crustaceans, such as isopods and small crabs. They increase in size to in the first six months, by the end of their first year, and to by the end of the second. Growth tends to be less at higher latitudes. Cod reach maturity at about at about 3 to 4 years of age. Changes in growth rate over decades of particular stocks have been reported, current eastern Baltic cod shows the lowest growth observed since 1955.\nEcology.\nAdult cod are active hunters, feeding on sand eels, whiting, haddock, small cod, squid, crabs, lobsters, mussels, worms, mackerel, and molluscs.\nIn the Baltic Sea the most important prey species are herring and sprat. Many studies that analyze the stomach contents of these fish indicate that cod is the top predator, preying on the herring and sprat. Sprat form particularly high concentrations in the Bornholm Basin in the southern Baltic Sea. Although cod feed primarily on adult sprat, sprat tend to prey on the cod eggs and larvae.\nCod and related species are plagued by parasites. For example, the cod worm, \"Lernaeocera branchialis\", starts life as a copepod-like larva, a small free-swimming crustacean. The first host used by the larva is a flatfish or lumpsucker, which it captures with grasping hooks at the front of its body. It penetrates the fish with a thin filament, which it uses to suck the fish's blood. The nourished larvae then mate on the fish. The female larva, with her now fertilized eggs, then finds a cod, or a cod-like fish such as a haddock or whiting. There the larva clings to the gills while it metamorphoses into a plump sinusoidal wormlike body with a coiled mass of egg strings at the rear. The front part of the worm's body penetrates the body of the cod until it enters the rear bulb of the host's heart. There, firmly rooted in the cod's circulatory system, the front part of the parasite develops like the branches of a tree, reaching into the main artery. In this way, the worm extracts nutrients from the cod's blood, remaining safely tucked beneath the cod's gill cover until it releases a new generation of offspring into the water.\nFisheries.\nThe 2006 northwest Atlantic cod quota is 23,000 tons, representing half the available stocks, while the northeast Atlantic quota is 473,000 tons. Pacific cod is currently enjoying strong global demand. The 2006 total allowable catch (TAC) for the Gulf of Alaska and Aleutian Islands was 260,000 tons.\nAquaculture.\nFarming of Atlantic cod has received a significant amount of interest due to the overall trend of increasing cod prices alongside reduced wild catches. However, progress in creating large scale farming of cod has been slow, mainly due to bottlenecks in the larval production stage, where survival and growth are often unpredictable. It has been suggested that this bottleneck may be overcome by ensuring cod larvae are fed diets with similar nutritional content as the copepods they feed on in the wild Recent examples have shown that increasing dietary levels of minerals such as selenium, iodine and zinc may improve survival and/or biomarkers for health in aquaculture reared cod larvae.\nAs food.\nCod is popular as a food with a mild flavour and a dense, flaky white flesh. Cod livers are processed to make cod liver oil, an important source of vitamin A, vitamin D, vitamin E and omega-3 fatty acids (EPA and DHA).\nYoung Atlantic cod or haddock prepared in strips for cooking is called scrod. In the United Kingdom, Atlantic cod is one of the most common ingredients in fish and chips, along with haddock and plaice. Cod's soft liver can be tinned (canned) and eaten. \nHistory.\nCod has been an important economic commodity in international markets since the Viking period (around 800 AD). Norwegians travelled with dried cod and soon a dried cod market developed in southern Europe. This market has lasted for more than 1,000 years, enduring the Black Death, wars and other crises, and is still an important Norwegian fish trade. The Portuguese began fishing cod in the 15th century. Clipfish is widely enjoyed in Portugal. The Basques played an important role in the cod trade, and allegedly found the Canadian fishing banks before Columbus' discovery of America. The North American east coast developed in part due to the vast cod stocks. Many cities in the New England area are located near cod fishing grounds. The fish was so important to the history and development of Massachusetts, the state's House of Representatives hung a wood carving of a codfish, known as the Sacred Cod of Massachusetts, in its chambers.\nApart from the long history, cod differ from most fish because the fishing grounds are far from population centres. The large cod fisheries along the coast of North Norway (and in particular close to the Lofoten islands) have been developed almost uniquely for export, depending on sea transport of stockfish over large distances. Since the introduction of salt, dried and salted cod (clipfish or 'klippfisk' in Norwegian) has also been exported. By the end of the 14th century, the Hanseatic League dominated trade operations and sea transport, with Bergen as the most important port.\nWilliam Pitt the Elder, criticizing the Treaty of Paris in Parliament, claimed cod was \"British gold\"; and that it was folly to restore Newfoundland fishing rights to the French.\nIn the 17th and 18th centuries in the New World, especially in Massachusetts and Newfoundland, cod became a major commodity, creating trade networks and cross-cultural exchanges. In 1733, Britain tried to gain control over trade between New England and the British Caribbean by imposing the Molasses Act, which they believed would eliminate the trade by making it unprofitable. The cod trade grew instead, because the \"French were eager to work with the New Englanders in a lucrative contraband arrangement\". In addition to increasing trade, the New England settlers organized into a \"codfish aristocracy\". The colonists rose up against Britain's \"tariff on an import\".\nIn the 20th century, Iceland re-emerged as a fishing power and entered the Cod Wars. In the late 20th and early 21st centuries, fishing off the European and American coasts severely depleted stocks and become a major political issue. The necessity of restricting catches to allow stocks to recover upset the fishing industry and politicians who are reluctant to hurt employment.\nCollapse of the Atlantic northwest cod fishery.\nOn July 2, 1992, John Crosbie, Canadian Federal Minister of Fisheries and Oceans, declared a two-year moratorium on the Northern Cod fishery, a designated fishing region off the coast of Newfoundland, after data showed that the total cod biomass had suffered a collapse to less than 1% of its normal value. The minister championed the measure as a temporary solution, allowing the cod population time to recover. The fisheries had long shaped the lives and communities on Canada's Atlantic eastern coast for the preceding five centuries. Societies which are dependent on fishing have a strong mutual relationship with them: the act of fishing changes the ecosystems' balance, which forces the fishery and, in turn, the fishing societies to adapt to new ecological conditions.\nThe near-complete destruction of the Atlantic northwest cod biomass off the shores devastated coastal communities, which had been overexploiting the same cod population for decades. The fishermen along the Atlantic northwest had employed modern fishing technologies, including the ecologically-devastating practice of trawling, especially in the years leading up to the 1990s, in the misguided belief that fishing stocks are perpetually plentiful and unable to be depleted. After this assumption was empirically and abruptly shown to be incorrect, to the dismay of government officials and rural workers, some 19,000 fishermen and cod processing plant workers in Newfoundland lost their employment. Nearly 40,000 workers and harvesters in the provinces of Newfoundland and Labrador applied for the federal relief program TAGS (the Atlantic Groundfish Strategy). Abandoned and rusting fishing boats still litter the coasts of Newfoundland and the Canadian northwest to this day.\nThe fishery minister, John Crosbie, after delivering a speech on the day before the declaration of the moratorium, or July 1, 1992, was publicly heckled and verbally harassed by disgruntled locals at a fishing village. The moratorium, initially lasting for only two years, was indefinitely extended after it became evident that cod populations had not recovered at all but, instead, had continued to spiral downward in both size and numbers, due to the damage caused by decades of horrible fishing practices, and the fact that the moratorium had permitted exceptions for food fisheries for \"personal consumption\" purposes to this very day. Some 12,000 tons of Northwest cod are still being caught every year along the Newfoundland coast by local fishermen.\nThe collapse of the four-million ton biomass, which had persevered through several previous marine extinctions over tens of millions of years, in a timespan of no more than 20 years, is oft-cited by researchers as one of the most visible examples of the phenomenon of the \"Tragedy of the Commons.\" Factors which had been implicated as contributing to the collapse include: overfishing; government mismanagement; the disregard of scientific uncertainty; warming habitat waters; declining reproduction; and plain human ignorance. The Northern Cod biomass has been recovering slowly since the imposition of the moratorium. However, as of 2021, the growth of the cod population has been stagnant since 2017, and some scientists argue that the population will not rebound unless the Fisheries Department of Canada lower its yearly quota to 5,000 tons.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41516", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=41516", "title": "Summer Solstice", "text": ""}
{"id": "41519", "revid": "28979433", "url": "https://en.wikipedia.org/wiki?curid=41519", "title": "Photic zone", "text": "Uppermost layer of a sea water column that is exposed to sunlight\nThe photic zone (or euphotic zone, epipelagic zone, or sunlight zone) is the uppermost layer of a body of water that receives sunlight, allowing phytoplankton to perform photosynthesis. It undergoes a series of physical, chemical, and biological processes that supply nutrients into the upper water column. The photic zone is home to the majority of aquatic life due to the activity (primary production) of the phytoplankton. The thicknesses of the photic and euphotic zones vary with the intensity of sunlight as a function of season and latitude and with the degree of water turbidity. The bottommost, or aphotic, zone is the region of perpetual darkness that lies beneath the photic zone and includes most of the ocean waters.\nPhotosynthesis in photic zone.\nIn the photic zone, the photosynthesis rate exceeds the respiration rate. This is due to the abundant solar energy which is used as an energy source for photosynthesis by primary producers such as phytoplankton. These phytoplankton grow extremely quickly because of sunlight's heavy influence, enabling it to be produced at a fast rate. In fact, ninety five percent of photosynthesis in the ocean occurs in the photic zone. Therefore, if we go deeper, beyond the photic zone, such as into the compensation point, there is little to no phytoplankton, because of insufficient sunlight. The zone which extends from the base of the euphotic zone to the aphotic zone is sometimes called the dysphotic zone.\nLife in the photic zone.\nNinety percent of marine life lives in the photic zone, which is approximately two hundred meters deep. This includes phytoplankton (plants), including dinoflagellates, diatoms, cyanobacteria, coccolithophores, and cryptomonads. It also includes zooplankton, the consumers in the photic zone. There are carnivorous meat eaters and herbivorous plant eaters. Next, copepods are the small crustaceans distributed everywhere in the photic zone. Finally, there are nekton (animals that can propel themselves, like fish, squids, and crabs), which are the largest and the most obvious animals in the photic zone, but their quantity is the smallest among all the groups. Phytoplankton are microscopic plants living suspended in the water column that have little or no means of motility. They are primary producers that use solar energy as a food source.\n\"Detritivores and scavengers are rare in the photic zone. Microbial decomposition of dead organisms begins here and continues once the bodies sink to the aphotic zone where they form the most important source of nutrients for deep sea organisms.\" The depth of the photic zone depends on the transparency of the water. If the water is very clear, the photic zone can become very deep. If it is very murky, it can be only fifty feet (fifteen meters) deep.\nAnimals within the photic zone use the cycle of light and dark as an important environmental signal, migration is directly linked to this fact, fishes use the concept of dusk and dawn when its time to migrate, the photic zone resembles this concept providing a sense of time. These animals can be herrings and sardines and other fishes that consistently live within the photic zone.\nNutrient uptake in the photic zone.\nDue to biological uptake, the photic zone has relatively low levels of nutrient concentrations. As a result, phytoplankton doesn't receive enough nutrients when there is high water-column stability. The spatial distribution of organisms can be controlled by a number of factors. Physical factors include: temperature, hydrostatic pressure, turbulent mixing such as the upward turbulent flux of inorganic nitrogen across the nutricline. Chemical factors include oxygen and trace elements. Biological factors include grazing and migrations. Upwelling carries nutrients from the deep waters into the photic zone, strengthening phytoplankton growth. The remixing and upwelling eventually bring nutrient-rich wastes back into the photic zone. The Ekman transport additionally brings more nutrients to the photic zone. Nutrient pulse frequency affects the phytoplankton competition. Photosynthesis produces more of it. Being the first link in the food chain, what happens to phytoplankton creates a rippling effect for other species. Besides phytoplankton, many other animals also live in this zone and utilize these nutrients. The majority of ocean life occurs in the photic zone, the smallest ocean zone by water volume. The photic zone, although small, has a large impact on those who reside in it.\nPhotic zone depth.\nThe depth is, by definition, where radiation is degraded down to 1% of its surface strength. Accordingly, its thickness depends on the extent of light attenuation in the water column. As incoming light at the surface can vary widely, this says little about the net growth of phytoplankton. Typical euphotic depths vary from only a few centimetres in highly turbid eutrophic lakes, to around 200 meters in the open ocean. It also varies with seasonal changes in turbidity, which can be strongly driven by phytoplankton concentrations, such that the depth of the photic zone often decreases as primary production increases. Moreover, the respiration rate is actually greater than the photosynthesis rate. The reason why phytoplankton production is so important is because it plays a prominent role when interwoven with other food webs.\nLight attenuation.\nMost of the solar energy reaching the Earth is in the range of visible light, with wavelengths between about 400\u2013700\u00a0nm. Each colour of visible light has a unique wavelength, and together they make up white light. The shortest wavelengths are on the violet and ultraviolet end of the spectrum, while the longest wavelengths are at the red and infrared end. In between, the colours of the visible spectrum comprise the familiar \u201cROYGBIV\u201d; red, orange, yellow, green, blue, indigo, and violet.\nWater is very effective at absorbing incoming light, so the amount of light penetrating the ocean declines rapidly (is attenuated) with depth. At one metre depth only 45% of the solar energy that falls on the ocean surface remains. At 10 metres depth only 16% of the light is still present, and only 1% of the original light is left at 100 metres. No light penetrates beyond 1000 metres.\nIn addition to overall attenuation, the oceans absorb the different wavelengths of light at different rates. The wavelengths at the extreme ends of the visible spectrum are attenuated faster than those wavelengths in the middle. Longer wavelengths are absorbed first; red is absorbed in the upper 10 metres, orange by about 40 metres, and yellow disappears before 100 metres. Shorter wavelengths penetrate further, with blue and green light reaching the deepest depths.\nThis is why things appear blue underwater. How colours are perceived by the eye depends on the wavelengths of light that are received by the eye. An object appears red to the eye because it reflects red light and absorbs other colours. So the only colour reaching the eye is red. Blue is the only colour of light available at depth underwater, so it is the only colour that can be reflected back to the eye, and everything has a blue tinge under water. A red object at depth will not appear red to us because there is no red light available to reflect off of the object. Objects in water will only appear as their real colours near the surface where all wavelengths of light are still available, or if the other wavelengths of light are provided artificially, such as by illuminating the object with a dive light.\nWater in the open ocean appears clear and blue because it contains much less particulate matter, such as phytoplankton or other suspended particles, and the clearer the water, the deeper the light penetration. Blue light penetrates deeply and is scattered by the water molecules, while all other colours are absorbed; thus the water appears blue. On the other hand, coastal water often appears greenish. Coastal water contains much more suspended silt and algae and microscopic organisms than the open ocean. Many of these organisms, such as phytoplankton, absorb light in the blue and red range through their photosynthetic pigments, leaving green as the dominant wavelength of reflected light. Therefore the higher the phytoplankton concentration in water, the greener it appears. Small silt particles may also absorb blue light, further shifting the colour of water away from blue when there are high concentrations of suspended particles.\nThe ocean can be divided into depth layers depending on the amount of light penetration, as discussed in pelagic zone. The upper 200 metres is referred to as the photic or euphotic zone. This represents the region where enough light can penetrate to support photosynthesis, and it corresponds to the epipelagic zone. From 200 to 1000 metres lies the dysphotic zone, or the twilight zone (corresponding with the mesopelagic zone). There is still some light at these depths, but not enough to support photosynthesis. Below 1000 metres is the aphotic (or midnight) zone, where no light penetrates. This region includes the majority of the ocean volume, which exists in complete darkness.\nPaleoclimatology.\nPhytoplankton are unicellular microorganisms which form the base of the ocean food chains. They are dominated by diatoms, which grow silicate shells called frustules. When diatoms die their shells can settle on the seafloor and become microfossils. Over time, these microfossils become buried as opal deposits in the marine sediment. Paleoclimatology is the study of past climates. Proxy data is used in order to relate elements collected in modern-day sedimentary samples to climatic and oceanic conditions in the past. Paleoclimate proxies refer to preserved or fossilized physical markers which serve as substitutes for direct meteorological or ocean measurements. An example of proxies is the use of diatom isotope records of \u03b413C, \u03b418O, \u03b430Si (\u03b413Cdiatom, \u03b418Odiatom, and \u03b430Sidiatom). In 2015, Swann and Snelling used these isotope records to document historic changes in the photic zone conditions of the north-west Pacific Ocean, including nutrient supply and the efficiency of the soft-tissue biological pump, from the modern day back to marine isotope stage 5e, which coincides with the last interglacial period. Peaks in opal productivity in the marine isotope stage are associated with the breakdown of the regional halocline stratification and increased nutrient supply to the photic zone.\nThe initial development of the halocline and stratified water column has been attributed to the onset of major Northern Hemisphere glaciation at 2.73 Ma, which increased the flux of freshwater to the region, via increased monsoonal rainfall and/or glacial meltwater, and sea surface temperatures. The decrease of abyssal water upwelling associated with this may have contributed to the establishment of globally cooler conditions and the expansion of glaciers across the Northern Hemisphere from 2.73 Ma. While the halocline appears to have prevailed through the late Pliocene and early Quaternary glacial\u2013interglacial cycles, other studies have shown that the stratification boundary may have broken down in the late Quaternary at glacial terminations and during the early part of interglacials.\nPhytoplankton.\nAn increase in the amount of phytoplankton also creates an increase in zooplankton, the zooplankton feeds on the phytoplankton as they are at the bottom of the food chain.\nPhytoplankton are restricted to the photic zone only, as their growth is completely dependent upon photosynthesis. This results in phytoplankton only occupying the uppermost 50\u2013100\u00a0m of the water column. Phytoplankton growth within the photic zone can also be influenced by terrestrial factors, like the weathering of crustal rocks or nutrients from the respiration of plants and animals on land that are carried to the ocean via runoff or riverine input.\nDimethylsulfide loss within the photic zone is controlled by microbial uptake and photochemical degradation.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41520", "revid": "50891833", "url": "https://en.wikipedia.org/wiki?curid=41520", "title": "Masovian Voivodeship", "text": "Voivodeship of Poland\nMasovian Voivodeship (, ) is a voivodeship (province) in east-central Poland, containing Poland's capital Warsaw. \nMasovian Voivodeship has an area of and had a 2019 population of 5,411,446, making it Poland's largest and most populous province. Its principal cities are Warsaw (1.783 million) in the center of the Warsaw metropolitan area, Radom (212,230) to the south, P\u0142ock (119,709) to the west, Siedlce (77,990) to the east, and Ostro\u0142\u0119ka (52,071) to the north. It borders six other provinces: Warmian-Masurian to the north, Podlaskie to the northeast, Lublin to the southeast, \u015awi\u0119tokrzyskie (Holy Cross) to the south, \u0141\u00f3d\u017a to the southwest, and Kuyavian\u2013Pomeranian to the northwest.\nThe name of the province recalls the region's traditional name, Mazovia (in Polish \"Mazowsze\", also spelled Masovia), with which it is roughly coterminous. However, the province's southern part, including Radom, historically belonged to Lesser Poland; while \u0141om\u017ca with environs, though historically part of Mazovia, is now part of Podlaskie Voivodeship.\nMasovian Voivodeship is Poland's prime center of science, research, education, industry, and infrastructure. It has Poland's lowest unemployment rate and is a very high-income province. It is also popular with tourists due to the many historical monuments and its over 20% forested area of pine and oak. The province's Kampinos National Park is a UNESCO biosphere reserve.\nHistory.\nIn the Early Middle Ages, the territory was inhabited by the Masovians, an old Polish tribe. It formed part of Poland since its establishment in the 10th century, with the then-regional capital P\u0142ock being the capital of Poland from 1079 to 1138. The \"Wzg\u00f3rze Tumskie\" (\"Cathedral Hill\") in P\u0142ock with the P\u0142ock Castle and the Catholic Cathedral, seat of one of the oldest Polish dioceses, est. in 1075, which contains the sarcophagi of a number of Polish monarchs, is listed as a Historic Monument of Poland. Later, P\u0142ock, Warsaw and Czersk were medieval ducal seats of the Piast dynasty. \nIn 1505, Radom hosted the session of the Sejm (Polish Parliament), which enacted the \"Nihil novi\" act, and in the 16th century, Warsaw hosted several sessions of the Sejm, before King Sigismund III Vasa moved the Polish capital from Krak\u00f3w to Warsaw in 1596. \nFollowing the late-18th-century Partitions of Poland, the region witnessed several uprisings against foreign rule: the Ko\u015bciuszko Uprising of 1794, the November Uprising of 1830\u20131831, and the January Uprising of 1863\u20131864. \nIn the interbellum, the region was part of reborn independent Poland. In 1920, the region was invaded by Soviet Russia, but Poland secured its freedom in the victorious Battle of Warsaw. The southern part of the current province was rapidly industrialized as part of the Central Industrial Region of Poland.\nDuring World War II, it was occupied by Germany, with the occupiers committing their genocidal policies against Poles and Jews in the region, with expulsions, massacres of civilians and prisoners of war, including at Ciepiel\u00f3w, \u015alad\u00f3w, Zakroczym, Ostr\u00f3w Mazowiecka, Palmiry, Firlej, Sk\u0142oby, Nur, Ochota, Wola, and Lipniak-Majorat. Germany operated numerous prisons, forced labour camps, the Treblinka extermination camp, in which some 700,000\u2013900,000 people were murdered, and several prisoner-of-war camps for Polish, Italian, French, Soviet, and Romanian prisoners of war.\nMasovian Province was created on 1 January 1999, under the Polish local-government reforms adopted in 1998, out of the former provinces of Warsaw, P\u0142ock, Ciechan\u00f3w, Ostro\u0142\u0119ka, Siedlce, and Radom.\nAdministrative division.\nMasovian Voivodeship is divided into 42 counties, including five city counties and 37 land counties. These are subdivided into 314 gminas (municipalities), which include 85 urban gminas.\nCities and towns.\n&lt;templatestyles src=\"Module:Historical populations/styles.css\"/&gt;The voivodeship contains 10 cities and 78 towns. These are listed below in descending order of population (according to official figures for 2019):\nPolitics.\nThe Masovian voivodeship's government is headed by the province's ' (governor) who is appointed by the Polish Prime Minister. The ' is then assisted in performing his duties by the voivodeship's marshal, who is the appointed speaker for the voivodeship's executive and is elected by the ' (provincial assembly). The current ' of Masovia is Mariusz Frankowski.\nThe Sejmik of Masovia consists of 51 members.\nProtected areas.\nProtected areass include one National Park and nine Landscape parks. These are shown below.\nHistorical.\nMasovian Voivodeship (1526\u20131795).\nMasovia Voivodeship, 1526\u20131795 () was an administrative region of the Kingdom of Poland, and of the Polish\u2013Lithuanian Commonwealth, from the 15th century until the partitions of the Polish-Lithuanian Commonwealth (1795). Together with P\u0142ock and Rawa Voivodeships, it formed the province (prowincja) of Masovia.\nMasovian Voivodeship (1816\u20131837).\nMasovian Voivodeship was one of the voivodeships of Congress Poland. It was formed from the Warsaw Department and transformed into the Masovia Governorate.\nTransport.\nThree major international road routes pass through the voivodeship: Cork\u2013Berlin\u2013Pozna\u0144\u2013Warszawa\u2013Minsk\u2013Moscow\u2013Omsk (European route E30), Prague\u2013Wroc\u0142aw\u2013Warsaw\u2013Bia\u0142ystok\u2013Helsinki (E67) and Pskov\u2013Gda\u0144sk\u2013Warsaw\u2013Krak\u00f3w\u2013Budapest (E77).\nCurrently, there are various stretches of highways in the area, with the A2 highway connecting the region, and therefore the capital city, with the rest of Europe. The highway passes directly through the voivodeship from west to east, connecting it with Belarus and Germany. However, the A2 is yet to be built east of Warsaw to connect Poland with Belarus. The S7 expressway runs through Poland from the north to the south passing through Warsaw, the S8 connects Warsaw with Bia\u0142ystok, in the neighboring north-eastern province, also forming part of the Via Baltica which heads on to Lithuania, and to Wroc\u0142aw in the south-west, and the S17 being built to connect Warsaw with Lublin in the south-east and on to Ukraine.\nThe two main railway carriers operating in the region are the regional Koleje Mazowieckie and nationwide PKP Intercity. Three of ten busiest railway stations of Poland are located in the voivodeship: Warszawa Centralna, Warszawa Wschodnia, Warszawa Zachodnia.\nThe main international airport in the region is Warsaw Frederic Chopin Airport.\nEconomy.\nMasovian Voivodeship is the wealthiest province in Poland. The gross domestic product (GDP) of the province was PLN 596 billion in 2021, accounting for 22.8% of the Polish economic output. GDP per capita adjusted for purchasing power was around PLN123,000in the same year.\nUnemployment.\nThe unemployment rate stood at 4.8% in 2017 and was higher than the national and the European average.\nSights and tourism.\nThe top tourist destination of the voivodeship is the capital city of Warsaw with its Old Town and Royal Castle, a UNESCO World Heritage Site and Historic Monument of Poland. Further Historic Monuments in Warsaw include the Royal Route with several palaces and parks, most notably the \u0141azienki Palace and Wilan\u00f3w Palace, and the Warsaw Water Filters.\nOther historic cities include Radom with its old center and parks, Pu\u0142tusk with the longest paved marketplace of Europe, and P\u0142ock, former medieval capital of Poland, with its Old Town and \"Wzg\u00f3rze Tumskie\" (\"Cathedral Hill\") with the P\u0142ock Castle and the P\u0142ock Cathedral, which contains the sarcophagi of a number of Polish monarchs.\nThere are several medieval castles, including at Ciechan\u00f3w, Czersk, Liw, P\u0142ock, and numerous palaces in the voivodeship, including at Otwock Wielki, Guz\u00f3w, Radziejowice, Krubki-G\u00f3rki, Sanniki, Korczew and multiple in Warsaw itself. Unique historic churches include the Temple of Mercy and Charity in P\u0142ock, the worldwide headquarters of the Mariavite Church, the Abbey Church in Czerwi\u0144sk nad Wis\u0142\u0105, one of the best preserved Romanesque fortified churches in Poland, and the Saints Roch and John the Baptist church in Broch\u00f3w, a Gothic-Renaissance fortified church, place of baptism of Fryderyk Chopin. Otwock, J\u00f3zef\u00f3w and Warsaw are home to the local \u015awidermajer architectural style. There are also the Modlin Fortress and Warsaw Citadel.\nThe sole spa town of the voivodeship is Konstancin-Jeziorna.\nThere are museums dedicated to composer Fryderyk Chopin and chemist Marie Curie at their birthplaces in \u017belazowa Wola and Warsaw, respectively. There is also a Fryderyk Chopin Museum in Warsaw. There is a museum dedicated to famous Renaissance poet Jan Kochanowski in Czarnolas. The Krasi\u0144ski Palace in Opinog\u00f3ra G\u00f3rna hosts the Museum of Romanticism.\nThere are numerous World War II memorials, including memorials at the sites of Nazi massacres of Poles, including Palmiry, and Holocaust memorials, and museums at the sites of the former Nazi German Treblinka extermination camp, Pawiak Prison in Warsaw and Dulag 121 camp in Pruszk\u00f3w. Two of the few Italian war cemeteries in Poland are located in Warsaw (from both world wars) and Nowe Opole (from WW2).\nThe highest point in the voivodeship, , G\u00f3ra Altana, is located south of Szyd\u0142owiec, near the southern boundary with the \u015awi\u0119tokrzyskie Voivodeship.\nSports.\nFootball, handball, volleyball and basketball enjoy the largest following in the voivodeship. Successful clubs include Legia Warsaw and Polonia Warsaw in football and basketball, and Wis\u0142a P\u0142ock in handball.\nSince the establishment of the province, several major international sports competitions were co-hosted by the province, including the 2002 World Weightlifting Championships, 2003 World Short Track Speed Skating Championships, 2009 UCI Track Cycling World Championships, EuroBasket 2009, UEFA Euro 2012, 2014 FIVB Volleyball Men's World Championship, 2017 Men's European Volleyball Championship, 2018 FIVB Volleyball Men's Club World Championship, 2019 UCI Track Cycling World Championships, 2023 World Men's Handball Championship.\nDeepspot, the world's second deepest swimming pool, is located in Mszczon\u00f3w.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41522", "revid": "25112844", "url": "https://en.wikipedia.org/wiki?curid=41522", "title": "Northanger Abbey", "text": "1818 novel by Jane Austen\nNorthanger Abbey ( ), written by the English author Jane Austen, is a coming-of-age novel and a satire of Gothic novels. Although the title page is dated 1818 and the novel was published posthumously in 1817 with \"Persuasion\", \"Northanger Abbey\" was completed in 1799. From a fondness of Gothic novels and an active imagination distorting her worldview, the story follows Catherine Morland, the na\u00efve young protagonist, as she develops to better understand herself and the world around her.\nBased on the different styles and different references to Gothic novels, it is apparent that Austen wrote \"Northanger Abbey\" over the span of many years. Not until after her death was her brother finally able to obtain publication for the book. Once published, Austen received a mix of reviews. The novel covers a wide array of topics such as high society, Gothic fiction, bildungsroman, the value of reading, and the importance of time. This novel is considered to be more juvenile than her others.\nThroughout \"Northanger Abbey\", Austen makes references to many different Gothic novels, most notably Ann Radcliffe's \"The Mysteries of Udolpho\". \"Northanger Abbey\" is credited for reviving interest in seven Gothic titles that had largely fallen into obscurity; the \"horrid novels\". There are also many references to \"Northanger Abbey\" in contemporary novels. Various different adaptations have been made throughout the years.\nPlot summary.\nSeventeen-year-old Catherine Morland is one of ten children of a country clergyman. Although a tomboy in her childhood, she is \"in training for a heroine\" and is fond of reading Gothic novels \"provided they [are] all story and no reflection.\"The Allens (her wealthier neighbours in Fullerton) invite Catherine to accompany them in their visit to the city of Bath and partake in the winter season of balls, theatre and other social activities. Shortly after their arrival, she is introduced to a young gentleman, Henry Tilney, with whom she dances. Mrs. Allen meets an old school friend, Mrs. Thorpe, whose daughter, Isabella, quickly becomes friends with Catherine. Isabella introduces Catherine to Ann Radcliffe's 1794 Gothic novel \"Mysteries of Udolpho\". Mrs. Thorpe's son, John, is a friend of Catherine's older brother, James, at Oxford University where they are both students. The two young men come to Bath, where John is then introduced to Catherine.\nThe Thorpes are not happy about Catherine's friendship with the Tilneys. They correctly perceive Henry as a rival for Catherine's affections even though Catherine is not at all interested in John Thorpe. Despite Thorpe continually attempting to sabotage her relationship with the Tilneys, Catherine tries to maintain her friendships with both the Thorpes and the Tilneys. This leads to several misunderstandings, which put Catherine in the awkward position of having to explain herself to the Tilneys.\nIsabella and James become engaged. James' father approves of the match and offers his son a country parson's living of a modest sum, \u00a3400 annually, but they must wait until he can obtain the benefice in two and a half years. Isabella is dissatisfied, but to Catherine, she misrepresents her distress as being caused solely by the delay, and not by the value of the sum. Isabella immediately begins to flirt with Captain Frederick Tilney, Henry's older brother. Innocent Catherine cannot understand her friend's behaviour, but Henry understands all too well as he knows his brother's character and habits.\nThe Tilneys invite Catherine to stay with them for a few weeks at their home, Northanger Abbey. Once at Northanger Abbey, Catherine and Eleanor Tilney, Henry and Frederick's younger sister, get to know each other better on a personal level. Catherine, in accordance with her novel reading, expects the house to be exotic and frightening. Henry teases her about this as it turns out that Northanger Abbey is pleasant and decidedly not Gothic. However, the house includes a mysterious suite of rooms that no one ever enters; Catherine learns that they were the apartments of Mrs. Tilney, who died nine years earlier due to a serious illness, leaving Mr. Tilney with three children to raise by himself. As General Tilney no longer appears to be affected by her death, Catherine decides that he may have imprisoned her in her chamber, or even murdered her.\nCatherine discovers that her over-active imagination has led her astray as nothing is strange or distressing in the apartments. Henry finds and questions her; he surmises and informs her that his father loved his wife in his own way and was truly upset by her death. She leaves the apartments, crying, fearing that she has lost Henry's regard entirely. Realising how foolish she has been, Catherine comes to believe that, though novels may be delightful, their content does not relate to everyday life. Henry does not mention this incident to her again.\nJames writes to inform her that he has broken off his engagement to Isabella and implies that she has become engaged instead to Captain Tilney. Henry and Eleanor Tilney are sceptical that their brother has actually become engaged to Isabella Thorpe. Catherine is terribly disappointed, realising what a dishonest person Isabella is. A subsequent letter from Isabella herself confirms the Tilney siblings' doubts and shows that Frederick Tilney was merely flirting with Isabella. The General goes off to London, and the atmosphere at Northanger Abbey immediately becomes lighter and more pleasant from his absence. Catherine passes several enjoyable days with Henry and Eleanor until the General returns abruptly in a temper in Henry's absence. He forces Catherine to go home early the next morning in a shocking and unsafe mode that forces Catherine to undertake the journey alone.\nAt home, Catherine is listless and unhappy. Henry pays a sudden unexpected visit and explains what happened. General Tilney (on the misinformation of John Thorpe) had believed her to be exceedingly rich as the Allens' prospective heiress, and therefore a proper match for Henry. In London, General Tilney ran into Thorpe again, who, angry at Catherine's refusal of his earlier half-made proposal of marriage, said instead that she was nearly destitute. Enraged, General Tilney, (again on the misinformation of John Thorpe), returned home to evict Catherine. When Henry returned to Northanger, his father informed him of what had occurred and forbade him to think of Catherine again. When Henry learns how she had been treated, he breaks with his father and tells Catherine he still wants to marry her despite his father's disapproval. Catherine is delighted, though when Henry seeks her parents' approval, they tell the young couple that final approval will only happen when General Tilney consents.\nEventually, General Tilney acquiesces because Eleanor has become engaged to a wealthy and titled man; he discovers that the Morlands, while not extremely rich, are far from destitute.\nComposition.\nAccording to notes written by Austen's sister Cassandra after Jane's death in 1817, the novel was finished by 1798 or 1799. The close resemblance in style to Austen's \"juvenilia\" of the early 1790s together with several in-jokes that only the Austen family could have appreciated strongly suggests that the book was begun during that period, probably about 1794. However, the references to several Gothic novels published after 1794 would indicate Austen did not finish the book until about 1798 or 1799 as Cassandra Austen remembered. The scholar Cecil Emden argued that differences between the Catherine portrayed in the Bath section of the novel vs. the Catherine at Northanger Abbey were due to Austen finishing the book at a different stage of her life than when she started.\nPublication.\nAusten initially sold the novel, then titled \"Susan\", for \u00a310 to a London bookseller, Crosby &amp; Co. in 1803. This publisher did not print the work but held on to the manuscript. Austen reportedly threatened to take her work back from them, but Crosby &amp; Co responded that she would face legal consequences for reclaiming her text. In the spring of 1816, the bookseller sold it back to the novelist's brother, Henry Austen, for the same sum as they had paid for it. There is evidence that Austen further revised the novel in 1816\u20131817 with the intention of having it published. She rewrote sections, renaming the main character Catherine and using that as her working title.\nAfter her death, Austen's brother Henry gave the novel its final name and arranged for publication of \"Northanger Abbey\" in late December 1817 (1818 given on the title page), as the first two volumes of a four-volume set, with a preface for the first time publicly identifying Jane Austen as the author of all her novels. Neither \"Northanger Abbey\" nor \"Persuasion\" was published under the working title Jane Austen used. Aside from first being published together, the two novels are not connected; later editions were published separately.\nReputation.\n\"Northanger Abbey\" and \"Persuasion\", published together posthumously in December 1817, were reviewed in the \"British Critic\" in March 1818 and in the \"Edinburgh Review and Literary Miscellany\" in May 1818. The reviewer for the \"British Critic\" felt that Austen's exclusive dependence on realism was evidence of a deficient imagination. The reviewer for the \"Edinburgh Review\" disagreed, praising Austen for her \"exhaustless invention\" and the combination of the familiar and the surprising in her plots.\nAusten scholars have pointed out that these early reviewers did not know what to make of her novels\u00a0\u2013 for example, they misunderstood her use of irony. Reviewers, for example, reduced \"Sense and Sensibility\" and \"Pride and Prejudice\" to didactic tales of virtue prevailing over vice.\nMajor themes.\nAs in all of Austen's novels, the subjects of society, status, behavior, and morality are addressed. \"Northanger Abbey\", however, being chronologically the first novel completed by Austen (though revised later in her life), is notably considered a \"point of departure\" from her other work as a result of the \"boldness with which it flaunts its ... deceptive air of simplicity with broad, bold humour\".\nLove, marriage and high society.\nThroughout \"Northanger Abbey\", Austen demonstrates the ways in which women are socially and economically disadvantaged. Beth Lau demonstrates how Austen depicts Isabella wanting to be of higher status by choosing Captain Tilney over James Morland. Isabella tries to shop around in marriage market even though she does not have any choices to make. In doing so, she is turning herself into a commodity with nothing to offer. The washing bill that Catherine finds in the abbey works to highlight the disadvantaged position women hold to men economically. It is because of women that men's economic position advances. To contrast the lack of choice women have in the economy, Austen uses the novel to give women a choice. Catherine is able to consume/buy novels rather than be a participant/commodity. Eleanor, however, is trapped within patriarchy through her selection to read masculine history instead of novels.\nBoth General Tilney and Captain Tilney work as examples of superficiality within the high society. With General Tilney, it is evident throughout the novel, but a specific clue is his obsession with fine china. This obsession showcases his greed and superficiality. Frederick, known as \"the Captain,\" represents society's dual standards of behavior for men and women. Captain Tilney refuses to dance with any of the women because of his disregard for them. Due to his higher status, he believes he is better than the women present. He also adds to the mystique of the Tilney family: like father, like son. Frederick's actions make Henry and Eleanor more sympathetic characters, and his ruining of Isabella does the same for her character. Henry makes it clear that Captain Tilney is just using Isabella since he would not marry someone of lower status. Regina Jeffers notes that many readers perceive Frederick as nothing but selfish, greedy, and conniving.\nWhen Henry tries to dissuade Catherine from her Gothic-inspired notion that General Tilney is a murderer, he cites male authors who were influential in establishing rules of proper conduct. This is an attempt to dismiss one genre that was popular with women with another genre that was popular with men. Austen uses this discourse of the essays as an example of imposing power over women by using a type of language that limits what one may think. Henry's speech is that expected in polite society in Britain at the time. The ingenue Catherine is unfamiliar with the ways of polite society. Henry establishes himself as worthy of being Catherine's husband in his role as a \"lover mentor\" who teaches Catherine the ways of polite society to allow her to eventually fit in.\nLife lived as in a Gothic novel.\nBy creating a heroine who is an ordinary girl, Austen is upending the traditional role of Gothic heroines. The way for Catherine to find happiness is by having an ordinary life, not one full of Gothic fantasy. When Catherine fears that General Tilney murdered his wife, it stems from her knowledge of Gothic novels. Her fears of fantastical evil prove to be false, but the book ends with her discovery of a realistic evil based on economic propositions. Once Catherine faces reality, she is able to find happiness. When General Tilney kicks her out of the abbey, she leaves easily, acting inwardly rather than outwardly. Waldo S. Glock argues that this is a display of her genuineness instead of sentimentality. Catherine's internal sadness showcases how she is not a typical Gothic heroine. To contrast, Isabella Thorpe acts more accurately as a Gothic heroine. Because of her insincerity, Isabella is more at danger of Gothic disillusionment and sentimental notions.\nAusten uses elements of Gothic fiction as a tool to help showcase portions of the marriage plot. This is evident in the use of the cabinet at the abbey. When Henry comes up with a Gothic story to tease Catherine, he makes a joke about the narrator overlooking a cabinet that is crucial to the made-up story as a way to create tension. Overlooking a key detail is similar to the manner in which marriage plots conceal information to build suspense. Gothic fiction also helps reveal negative aspects of marriage that are not as obvious in a traditional courtship plot.\n\"Northanger Abbey\" is a parody of Gothic fiction. One way that Austen achieves this is through the washing bill that Catherine finds in the abbey. Catherine thinks that there is an elaborate story behind the washing bills, but it leads to no big discovery. Austen reverses the expectation in Gothic fiction for there to be some sort of depth to a story with the washing bills. It also showcases Catherine as a victim of the economy for believing that the washing bill contained a larger story than it actually did. Susan Zlotnick highlights that it is common for Gothic novels to portray women as victims to the economy. Another way that Austen satirizes Gothic fiction is through the cabinet that Catherine finds the washing bills in. The cabinet is from Japan which plays on the Gothic idea of exoticism. It removes the exaggerated exotic feature to the scope of the room instead. In contrast, Robert Irvine, a British critic, argues that the interpretation of the novel as a complete satire of the Gothic genre is problematic even though parts of the book do satirize the Gothic novels popular in the 18th century. \"Northanger Abbey\" makes fun of the silliness of Gothic fiction but also praises it and depends on it to tell the story. Claudia L. Johnson notes in \"Jane Austen: Women, Politics, and the Novel\" that \"\"Northanger Abbey\" does not refute, but rather clarifies and reclaims, gothic conventions in distinctly political ways,\" and that Austen's ridicule is directed more towards readers of Gothic fiction, rather than the novels themselves.\nBildungsroman.\nThe story begins with the narrator remarking that the heroine is not really a heroine. The narrator describes Catherine as not especially clever, nor a great beauty, and good without being virtuous. When the narrator has anything positive to say about Catherine, it is attached with the adjective \"extraordinary.\" Austen uses this term ironically since Catherine's traits are actually rather ordinary. Another aspect of Catherine that makes her seem not really like a heroine is that she does not have any ambitions outside of being with Henry. Because she actually has ambitions, Isabella appears more like a heroine, but it is those ambitions that turn her into a comedic villain. By creating a protagonist who does not fit the traditional role of a heroine, Austen is satirizing how women were portrayed in contemporary literature.\nAt the beginning of the novel, Catherine has a hard time interpreting the actions of the people around her, especially Isabella. She does not understand Isabella's contradictory actions because she can not understand that there is a double meaning to what Isabella says. This creates confusion for Catherine which forces her to realize that she should not rely solely on others who are negative influences, such as Isabella. Her inability to understand Isabella's contradictory actions has to do with Catherine's inability to grasp both the fictional and the real world. However, Catherine develops to realize that she should be an independent thinker.\nThough Austen encourages her audience to read novels, Catherine must learn to separate life from fiction and to rein in her very active imagination. By focusing only on Gothic novels, Catherine is not able to interact with others properly. On the other hand, it is her novel reading that transforms her into a heroine and causes her to be an active character. Henry also plays a role in Catherine's development from his teachings. By the end of the novel, Catherine understands that people are not completely good nor completely bad. For example, she does not see Henry as without any faults. She recognizes that he has a superior attitude towards those he thinks are inferior to him.\nThe value of reading.\n\"Northanger Abbey\" is a story about reading novels. Laura Jeanne Baudot highlights this point through the discussion of the washing bill Catherine finds in a cabinet at the abbey. Through the washing bill, Austen draws the audience's attention to the clothes that the fantasy man who marries Eleanor wears. Austen is forcing the audience to conjure up a clich\u00e9 image of what the man looks like. In doing so, Austen is reminding the audience of their current act of reading. The body of the man reminds the audience of the physical act of reading a book. It is clear that Austen is defending novel reading. Specifically, Henry Tilney, the hero of \"Northanger Abbey\", is an ideal reader. Jodi L. Wyett classifies Henry as an ideal reader because of his knowledge about different texts from different genres. This flips the gender hierarchy by showing men as novel readers instead of women. An early sign that Henry Tilney is the hero instead of John Thorpe is that the former likes to read books while the latter does not. John Thorpe's lack of interest in reading novels, specifically in reading Radcliffe's novels, makes him boorish. It is hard for Catherine to connect with him because Catherine uses novels as a conversation starter.\nThe importance of time.\nVarious scholars such as the French historian Michel Foucault and the British Marxist E.P. Thompson have argued that the 18th century became the \"era of the clock\" as availability of mass-produced clocks and watches allowed time to be measured more accurately. From these devices creating a new increased emphasis on time management, Thompson called this era the beginning of \"time discipline.\" As a result of living in the new era of \"time discipline,\" Austen frequently uses clocks as symbols of General Tilney's authority over Northanger Abbey. General Tilney is always checking his watch and is most insistent that the servants as well as his own family observe the clocks to make sure they are on time. Because of the importance of staying on schedule, even when General Tilney is not around, clocks serve as a symbol of his power as Catherine finds herself always checking the time. After arriving at Northanger Abbey, Catherine discovers that everything at the abbey happens on a strict schedule because of General Tilney. This is a marked difference from Catherine's lax attitude that she displays in Bath. Catherine compares General Tilney to a clock, as something inhuman and mechanical that operates with no regard to the human body. When Catherine visits the kitchen at Northanger Abbey, she notes that it is equipped with all manner of \"modern\" cooking equipment and that the cooks work in an efficient manner like soldiers performing a drill. This is a direct reflection of the General's wish to have everything ordered.\nFamily entertainment.\nAccording to Austen biographer Claire Tomalin, \"there is very little trace of personal allusion in the book, although it is written more in the style of a family entertainment than any of the others\". Joan Aiken writes: \"We can guess that \"Susan\" [the original title of \"Northanger Abbey\"], in its first outline, was written very much for family entertainment, addressed to a family audience, like all Jane Austen's juvenile works, with their asides to the reader, and absurd dedications; some of the juvenilia, we know, were specifically addressed to her brothers Charles and Frank; all were designed to be circulated and read by a large network of relations.\"\nAllusions to other works.\n&lt;templatestyles src=\"Template:Quote_box/styles.css\" /&gt;\nIsabella: Dear creature! how much I am obliged to you; and when you have finished \"The Mysteries of Udolpho\", we will read \"The Italian\" together; and I have made out a list of ten or twelve more of the same kind for you.\nCatherine: ...but are they all horrid, are you sure they are all horrid?\nIsabella: Yes, quite sure, for a particular friend of mine, a Miss Andrews, a sweet girl, one of the sweetest creatures in the world, has read every one of them.\nJane Austen, \"Northanger Abbey\", chapter VI\nSeveral Gothic novels and authors are mentioned in the book, including Fanny Burney and \"The Monk\". Isabella Thorpe gives Catherine a list of seven books that are commonly referred to as the \"Northanger 'horrid' novels\". These works were thought to be of Austen's own invention until the 1920s, when British writers Montague Summers and Michael Sadleir found that the novels did exist. The list is as follows:\nAll seven of these were republished by the Folio Society in London in 1968 with introductions for each novel written by Devendra Varma. Varma, in his book, \"The Gothic Flame\" suggests that Austen carefully selected the seven titles, which reflect different types of Gothic fiction, as well as reflecting the development of the genre. Since 2005, Valancourt Books has released new editions of the \"horrids\", the seventh and final being released in 2015.\n\"The Mysteries of Udolpho\".\nThe most significant allusion, however, is to Ann Radcliffe's \"The Mysteries of Udolpho\", as it is the Gothic novel most frequently mentioned within this text. Notably, Jane Austen sold the manuscript of \"Northanger Abbey\" to the same firm that published Radcliffe's novel in 1794.\nThis outside text is first mentioned in Chapter Six, when Isabella and Catherine discuss the mystery \"behind the black veil\", and further establish their friendship based on their similar interests in novel genre and their plans to continue reading other Gothic novels together. Austen further satirizes the novel through Catherine's stay at Northanger Abbey, believing that General Tilney has taken the role of Gothic novel villain.\nAusten's discussion of \"Udolpho\" is also used to clearly separate Catherine and the Tilney siblings from John Thorpe, as when Catherine talks about the novel with him, he crudely responds that he \"never reads novels\" but qualifies his statement by arguing he would only read a novel by Ann Radcliffe, who is the author of \"Udolpho\". Here, Austen humorously categorizes \"Northanger Abbey's\" characters into two spheres: those who read novels, and those who do not. When Catherine and Henry Tilney later discuss reading novels, and Henry earnestly responds that he enjoys reading novels, and was especially titillated by \"Udolpho\", the match between Catherine and Henry is implied as both smart and fitting.\nAllusions to \"Northanger Abbey\".\nA passage from the novel appears as the preface of Ian McEwan's \"Atonement\", thus likening the naive mistakes of Austen's Catherine Morland to those of his own character Briony Tallis, who is in a similar position: both characters have very over-active imaginations, which lead to misconceptions that cause distress in the lives of people around them. Both treat their own lives like those of heroines in fantastical works of fiction, with Miss Morland likening herself to a character in a Gothic novel and young Briony Tallis writing her own melodramatic stories and plays with central characters such as \"spontaneous Arabella\" based on herself.\nRichard Adams quotes a portion of the novel's last sentence for the epigraph to Chapter 50 in his \"Watership Down\"; the reference to the General is felicitous as the villain in \"Watership Down\" is also a general.\nJasper Fforde, in his alternate history comic fantasy novel \"First Among Sequels\", refers to \"Northanger Abbey\" as being under maintenance and \"should be ready on time as long as Catherine stops attempting to have the book 'Gothicized'.\" It appears again as the prize in a reality program, based on the lives of the Bennets from \"Pride and Prejudice\".\nAdaptations.\nLiterature.\nHarperCollins hired Scottish crime writer Val McDermid in 2012 to adapt \"Northanger Abbey\" for a modern audience, as a suspenseful teen thriller, the second rewrite in The Austen Project. McDermid said of the project, \"At its heart it's a teen novel, and a satire \u2013 that's something which fits really well with contemporary fiction. And you can really feel a shiver of fear moving through it. I will be keeping the suspense \u2013 I know how to keep the reader on the edge of their seat. I think Jane Austen builds suspense well in a couple of places, but she squanders it, and she gets to the endgame too quickly. So I will be working on those things.\" The novel was published in 2014.\nIn 2011, Marvel published a graphic novel version of \"Northanger Abbey\", adapted by Nancy Butler (writer), Janet K. Lee (artist) and Nick Filardi (colour artist).\nThe same year, author Jenni James published a teen version, \"Northanger Alibi\", published by Inkberry Press, in which the main character's obsession for Stephenie Meyer's \"Twilight saga\" replaces Catherine's love for Regency gothic novels.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41523", "revid": "12845131", "url": "https://en.wikipedia.org/wiki?curid=41523", "title": "Bath, Somerset", "text": "City in Somerset, England\nBath ( , ) is a city in Somerset, England, known for and named after its Roman-built baths. At the 2021 census, the population was 94,092. Bath is in the valley of the River Avon, west of London and southeast of Bristol. The city became a UNESCO World Heritage Site in 1987, and was later added to the transnational World Heritage Site known as the \"Great Spa Towns of Europe\" in 2021. Bath is also the largest city and settlement in Somerset.\nThe city became a spa with the Latin name \"\" (\"the waters of Sulis\") c. 60\u00a0AD when the Romans built baths and a temple in the valley of the River Avon, although hot springs were known even before then. Bath Abbey was founded in the 7th century and became a religious centre; the building was rebuilt in the 12th and 16th centuries. In the 17th century, claims were made for the curative properties of water from the springs, and Bath became popular as a spa town in the Georgian era. Georgian architecture, crafted from Bath Stone, includes the Royal Crescent, Circus, Pump Room, and the Assembly Rooms, where Beau Nash presided over the city's social life from 1705 until his death in 1761.\nMany of the streets and squares were laid out by John Wood, the Elder, and in the 18th century the city became fashionable and the population grew. Jane Austen lived in Bath in the early 19th century. Further building was undertaken in the 19th century and following the Bath Blitz in World War II. Bath became part of the county of Avon in 1974, and, following Avon's abolition in 1996, has been the principal centre of Bath and North East Somerset.\nBath has over 6 million yearly visitors, making it one of the ten English cities visited most by overseas tourists. Attractions include the spas, canal boat tours, Royal Crescent, Bath Skyline, Parade Gardens, Sydney Gardens and Royal Victoria Park which hosts carnivals and seasonal events. Shopping areas include SouthGate shopping centre, the Corridor arcade and artisan shops at Walcot, Milsom, Stall and York Streets. There are theatres, including the Theatre Royal, as well as several museums including the Museum of Bath Architecture, the Victoria Art Gallery, the Museum of East Asian Art, the Herschel Museum of Astronomy, Fashion Museum, and the Holburne Museum. The city has two universities \u2013 the University of Bath and Bath Spa University \u2013 with Bath College providing further education. Sporting clubs from the city include Bath Rugby and Bath City.\nHistory.\nStone, Bronze, and Iron Ages.\nThe hills in the locality such as Bathampton Down saw human activity from the Mesolithic period. Several Bronze Age round barrows were opened by John Skinner in the 18th century. A long barrow site believed to be from the Early Bronze Age Beaker people was flattened to make way for RAF Charmy Down. Solsbury Hill overlooking the current city was an Iron Age hill fort and the adjacent Bathampton Camp may also have been one.\nRoman baths and town.\nArchaeological evidence shows that the site of the Roman baths' main spring may have been treated as a shrine by the Britons, and was dedicated to the goddess Sulis, whom the Romans identified with Minerva; the name Sulis continued to be used after the Roman invasion, appearing in the town's Roman name, \"\" (literally, \"the waters of Sulis\"). Messages to her scratched onto metal, known as curse tablets, have been recovered from the sacred spring by archaeologists. The tablets were written in Latin, and laid curses on personal enemies. For example, if a citizen had his clothes stolen at the baths, he might write a curse against the suspects on a tablet to be read by the goddess.\nA temple was constructed in AD\u00a060\u201370, and a bathing complex was built up over the next 300\u00a0years. Engineers drove oak piles into the mud to provide a stable foundation, and surrounded the spring with an irregular stone chamber lined with lead. In the 2nd century, the spring was enclosed within a wooden barrel-vaulted structure that housed the caldarium (hot bath), tepidarium (warm bath), and frigidarium (cold bath).\nThe town was later given defensive walls, probably in the 3rd century. After the failure of Roman authority in the first decade of the 5th century, the baths fell into disrepair and were eventually lost as a result of rising water levels and silting.\nIn March 2012, a hoard of 30,000 silver Roman coins, one of the largest discovered in Britain, was unearthed in an archaeological dig. The coins, believed to date from the 3rd century, were found about from the Roman baths.\nPost-Roman and medieval.\nBath may have been the site of the Battle of Badon (c. 500\u00a0AD), in which Arthur, the hero of later legends, is said to have defeated the Anglo-Saxons. The town was captured by the West Saxons in 577 after the Battle of Deorham; the Anglo-Saxon poem \"The Ruin\" may describe the appearance of the Roman site about this time. A monastery was founded at an early date\u00a0\u2013 reputedly by Saint David although more probably in 675 by Osric, King of the Hwicce, perhaps using the walled area as its precinct. Nennius, a 9th-century historian, mentions a \"Hot Lake\" in the land of the Hwicce along the River Severn, and adds \"It is surrounded by a wall, made of brick and stone, and men may go there to bathe at any time, and every man can have the kind of bath he likes. If he wants, it will be a cold bath; and if he wants a hot bath, it will be hot\". Bede described hot baths in the geographical introduction to the \"Ecclesiastical History\" in terms very similar to those of Nennius. King Offa of Mercia gained control of the monastery in 781 and rebuilt the church, which was dedicated to St. Peter.\nAccording to the Victorian churchman Edward Churton, during the Anglo-Saxon era Bath was known as \"Acemannesceastre\" ('Akemanchester'), or 'aching men's city', on account of the reputation these springs had for healing the sick.\nBy the 9th century, the old Roman street pattern was lost and Bath was a royal possession. King Alfred laid out the town afresh, leaving its south-eastern quadrant as the abbey precinct. In the Burghal Hidage, Bath is recorded as a burh (borough) and is described as having walls of and was allocated 1000 men for defence. During the reign of Edward the Elder coins were minted in Bath based on a design from the Winchester mint but with 'BAD' on the obverse relating to the Anglo-Saxon name for the town, Ba\u00f0um, Ba\u00f0an or Ba\u00f0on, meaning \"at the baths\", and this was the source of the present name. Edgar of England was crowned king of England in Bath Abbey in 973, in a ceremony that formed the basis of all future English coronations.\nWilliam Rufus granted the town, abbey and mint to a royal physician, John of Tours, who became Bishop of Wells and Abbot of Bath, following the sacking of the town during the Rebellion of 1088. It was papal policy for bishops to move to more urban seats, and John of Tours translated his own from Wells to Bath. The bishop planned and began a much larger church as his cathedral, to which was attached a priory, with the bishop's palace beside it. New baths were built around the three springs. Later bishops returned the episcopal seat to Wells while retaining the name Bath in the title, Bishop of Bath and Wells. St John's Hospital was founded around 1180 by Bishop Reginald Fitz Jocelin and is among the oldest almshouses in England. The 'hospital of the baths' was built beside the hot springs of the Cross Bath, for their health-giving properties and to provide shelter for the poor infirm.\nAdministrative systems fell within the hundreds. The Bath Hundred had various names including the Hundred of Le\u00a0Buri. The Bath Foreign Hundred or Forinsecum covered the area outside the city and was later combined into the Bath Forum Hundred. Wealthy merchants had no status within the hundred courts and formed guilds to gain influence. They built the first guildhall probably in the 13th century. Around 1200, the first mayor was appointed.\nEarly modern.\nBy the 15th century, Bath's abbey church was dilapidated and Oliver King, Bishop of Bath and Wells, decided to rebuild it on a smaller scale in 1500. The new church was completed just a few years before Bath Priory was dissolved in 1539 by Henry VIII. The abbey church became derelict before being restored as the city's parish church in the Elizabethan era, when the city experienced a revival as a spa. The baths were improved and the city began to attract the aristocracy. A royal charter granted by Queen Elizabeth I in 1590 confirmed city status. James Montagu, Bishop of Bath and Wells from 1608, spent considerable sums in restoring Bath Abbey and actively supported the Baths themselves, aware that the 'towne liveth wholly by them'. In 1613, perhaps at his behest, Queen Anne visited the town to take the waters: the Queen's Bath was named after her. The cue for the visit may have been the completion of the restoration work to Bath Abbey, the last instalment of which had been paid for two years previously. Anne of Denmark came to Bath in 1613 and 1615. By the beginning of the English Civil War, the city was a first-class resort. However, it lost much of this trade in 1642; with the outbreak of war, fiddlers, \"ladies who are there\", and ale-house guides, lost their customers.\nThe city was initially garrisoned for Charles I. Seven thousand pounds was spent on fortifications, but on the appearance of parliamentary forces the gates were thrown open and the city surrendered. It became a significant post for the Western Association army under William Waller. Bath was retaken by the royalists in July 1643 following the Battle of Lansdowne and occupied for two years until 1645. The city was spared widespread destruction of property, overcrowding, bubonic plague, or starvation of its inhabitants, etc, unlike nearby Bristol and Gloucester, and it had good water piped in from its surrounding hills. Still, soldiers who were billeted in private houses contributed to disorder and vandalism, though this never caused the general destruction and plundering seen in Marlborough and other towns. Bath remained a health resort, often for wounded soldiers, its markets continued open and well-regulated, and its shopkeepers and craftsmen continued busy. Nevertheless, council spending, rents and grants all decreased and the finances of the Bath City Council were seriously affected. \nNormality to the city quickly recovered after the war when the city council achieved a healthy budget surplus. Thomas Guidott, a student of chemistry and medicine at Wadham College, Oxford, set up a practice in the city in 1668. He was interested in the curative properties of the waters, and he wrote \"A discourse of Bathe, and the hot waters there. Also, Some Enquiries into the Nature of the water\" in 1676. It brought the health-giving properties of the hot mineral waters to the attention of the country, and the aristocracy arrived to partake in them.\nSeveral areas of the city were developed in the Stuart period, and more building took place during Georgian times in response to the increasing number of visitors who required accommodation. Architects John Wood the Elder and his son laid out the new quarters in streets and squares, the identical fa\u00e7ades of which gave an impression of palatial scale and classical decorum. Much of the creamy gold Bath stone, a type of limestone used for construction in the city, was obtained from the Combe Down and Bathampton Down Mines owned by Ralph Allen (1694\u20131764). Allen, to advertise the quality of his quarried limestone, commissioned the elder John Wood to build a country house on his Prior Park estate between the city and the mines. Allen was responsible for improving and expanding the postal service in western England, for which he held the contract for more than forty years. Although not fond of politics, Allen was a civic-minded man and a member of Bath Corporation for many years. He was elected mayor for a single term in 1742.\nIn the early 18th century, Bath acquired its first purpose-built theatre, the Old Orchard Street Theatre. It was rebuilt as the Theatre Royal, along with the Grand Pump Room attached to the Roman Baths and assembly rooms. Master of ceremonies Beau Nash, who presided over the city's social life from 1704 until his death in 1761, drew up a code of behaviour for public entertainments. Bath had become perhaps the most fashionable of the rapidly developing British spa towns, attracting many notable visitors such as the wealthy London bookseller Andrew Millar and his wife, who both made long visits. In 1816, it was described as \"a seat of amusement and dissipation\", where \"scenes of extravagance in this receptacle of the wealthy and the idle, the weak and designing\" were habitual.\nLate modern.\nThe population of the city was 40,020 at the 1801 census, making it one of the largest cities in Britain. William Thomas Beckford bought a house in Lansdown Crescent in 1822, and subsequently two adjacent houses to form his residence. Having acquired all the land between his home and the top of Lansdown Hill, he created a garden more than in length and built Beckford's Tower at the top.\nEmperor Haile Selassie of Ethiopia spent four years in exile, from 1936 to 1940, at Fairfield House in Bath. During World War II, between the evening of 25 April and the early morning of 27 April 1942, Bath suffered three air raids in reprisal for RAF raids on the German cities of L\u00fcbeck and Rostock, part of the Luftwaffe campaign popularly known as the Baedeker Blitz. During the Bath Blitz, more than 400 people were killed, and more than 19,000 buildings damaged or destroyed.\nHouses in Royal Crescent, Circus and Paragon were burnt out along with the Assembly Rooms. A high explosive bomb landed on the east side of Queen Square, resulting in houses on the south side being damaged and the Francis Hotel losing of its frontage. The buildings have all been restored although there are still signs of the bombing.\nA postwar review of inadequate housing led to the clearance and redevelopment of areas of the city in a postwar style, often at variance with the local Georgian style. In the 1950s, the nearby villages of Combe Down, Twerton and Weston were incorporated into the city to enable the development of housing, much of it council housing. In 1965, town planner Colin Buchanan published \"Bath: A Planning and Transport Study\", which to a large degree sought to better accommodate the motor car, including the idea of a traffic tunnel underneath the centre of Bath. Though criticised by conservationists, some parts of the plan were implemented.\nIn the 1970s and 1980s, it was recognised that conservation of historic buildings was inadequate, leading to more care and reuse of buildings and open spaces. In 1987, the city was selected by UNESCO as a World Heritage Site, recognising its international cultural significance.\nBetween 1991 and 2000, Bath was the scene of a series of rapes committed by an unidentified man dubbed the \"Batman rapist\". The attacker remains at large and is the subject of Britain's longest-running serial rape investigation. He is said to have a tights fetish, have a scar below his bottom lip and resides in the Bath area or knows it very well. He has also been linked to the unsolved murder of Melanie Hall, which occurred in the city in 1996. Although the offender's DNA is known and several thousand men in Bath were DNA tested, the attacker continues to evade police.\nSince 2000, major developments have included the Thermae Bath Spa, the SouthGate shopping centre, the residential Western Riverside project on the Stothert &amp; Pitt factory site, and the riverside Bath Quays office and business development. In 2021, Bath become part of a second UNESCO World Heritage Site, a group of spa towns across Europe known as the \"Great Spas of Europe\". This makes it one of the only places to be formally recognised twice as a World Heritage site.\nGovernment.\nSince 1996, the city has had a single tier of local government \u2014 Bath and North East Somerset Council.\nHistorical development.\nBath had long been an ancient borough, having that status since 878 when it became a royal borough (burh) of Alfred the Great, and was reformed into a municipal borough in 1835. It has formed part of the county of Somerset since 878, when ceded to Wessex, having previously been in Mercia (the River Avon had acted as the border between the two kingdoms since 628). However, Bath was made a county borough in 1889, independent of the newly created administrative county and Somerset County Council. Bath became part of Avon when the non-metropolitan county was created in 1974, resulting in its abolition as a county borough, and instead became a non-metropolitan district with borough status.\nWith the abolition of Avon in 1996, the non-metropolitan district and borough were abolished too, and Bath has since been part of the unitary authority district of Bath and North East Somerset (B&amp;NES). The unitary district included also the Wansdyke district and therefore includes a wider area than the city (the 'North East Somerset' element) including Keynsham which is home to many of the council's offices, though the council meets at the Guildhall in Bath.\nBath was returned to the ceremonial county of Somerset in 1996, though as B&amp;NES is a unitary authority, it is not part of the area covered by Somerset County Council.\nCharter trustees.\nBath City Council was abolished in 1996, along with the district of Bath, and there is no longer a parish council for the city. The City of Bath's ceremonial functions, including its formal status as a city, its twinning arrangements, the mayoralty of Bath \u2013 which can be traced back to 1230 \u2013 and control of the city's coat of arms, are maintained by the charter trustees of the City of Bath.\nThe councillors elected by the electoral wards that cover Bath (see below) are the trustees, and they elect one of their number as their chair and mayor. The mayor holds office for one municipal year and in modern times the mayor begins their term in office on the first Saturday in June, at a ceremony at Bath Abbey with a civic procession from and to the Guildhall. The 798th mayor, who began his office on 7 June 2025, is Bharat Pankhania. A deputy mayor is also elected.\nCoat of arms.\nThe coat of arms includes a depiction of the city wall, and two silver stripes representing the River Avon and the hot springs. The sword of St. Paul is a link to Bath Abbey. The supporters, a lion and a bear, stand on a bed of acorns, a link to Bladud, the subject of the Legend of Bath. The knight's helmet indicates a municipality and the crown is that of King Edgar (referencing his coronation at the Abbey). A mural crown, indicating a city, is alternatively used instead of the helmet and Edgar's crown.\nThe Arms bear the motto \"Aqvae Svlis\", the Roman name for Bath in Latin script; although not on the Arms, the motto \"Floreat Bathon\" is sometimes used (\"may Bath flourish\" in Latin).\nBath Area Forum.\nBath and North East Somerset Council has established the Bath City Forum, comprising B&amp;NES councillors representing wards in Bath and up to 13 co-opted members drawn from the communities of the city. The first meeting of the Forum was held on 13 October 2015, at the Guildhall, where the first chair and vice-chair were elected. In 2021, this was re-launched as the Bath Area Forum.\nParliamentary elections.\nBath is one of the oldest extant parliamentary constituencies in the United Kingdom, being in continuous existence since the Model Parliament of 1295. Before the Reform Act 1832, Bath elected two members to the unreformed House of Commons, as an ancient parliamentary borough. From 1832 until 1918 it elected two MPs and then was reduced to one.\nHistorically the constituency covered only the city of Bath; however, it was enlarged into some outlying areas between 1997 and 2010. The constituency since 2010 once again covers exactly the city of Bath and is currently represented by Liberal Democrat Wera Hobhouse who beat Conservative Ben Howlett at the 2017 general election and retained her seat at the 2019 general election. Howlett had replaced the retiring Liberal Democrat Don Foster at the 2015 general election. Foster's election was a notable result of the 1992 general election, as Chris Patten, the previous Member (and Cabinet Minister) played a major part, as Chairman of the Conservative Party, in re-electing the government of John Major, but failed to defend his marginal seat.\nElectoral wards.\nThe fifteen electoral wards of Bath are: Bathwick, Combe Down, Kingsmead, Lambridge, Lansdown, Moorlands, Newbridge, Odd Down, Oldfield Park, Southdown, Twerton, Walcot, Westmoreland, Weston and Widcombe &amp; Lyncombe. These wards are co-extensive with the city, except that Newbridge includes also two parishes beyond the city boundary.\nThese wards return a total of 28 councillors to Bath and North East Somerset Council; all except two wards return two councillors (Moorlands and Oldfield Park return one each). The most recent elections were held on 4 May 2023 and all wards returned Liberal Democrats except for Lambridge and Westmoreland which returned Green Party and independent councillors respectively.\nBoundary changes enacted from 2 May 2019 included the abolition of Abbey ward, the merger of Lyncombe and Widcombe wards, the creation of Moorlands ward, and the replacement of Oldfield with Oldfield Park, as well as considerable changes to boundaries affecting all wards.\nGeography and environment.\nPhysical geography.\nBath is in the Avon Valley and is surrounded by limestone hills as it is near the southern edge of the Cotswolds, a designated Area of Outstanding Natural Beauty, and the Mendip Hills rise around south of the city. The hills that surround and make up the city have a maximum altitude of on the Lansdown plateau. Bath has an area of .\nThe floodplain of the Avon has an altitude of about above sea level, although the city centre is at an elevation of around above sea level. The river, once an unnavigable series of braided streams broken up by swamps and ponds, has been controlled by weirs into a single channel. Periodic flooding, which shortened the life of many buildings in the lowest part of the city, was normal until major flood control works were completed in the 1970s. Kensington Meadows is an area of mixed woodland and open meadow next to the river which has been designated as a local nature reserve.\nWater bubbling up from the ground as geothermal springs originates as rain on the Mendip Hills. The rain percolates through limestone aquifers to a depth of between where geothermal energy raises the water's temperature to between 64 and 96\u00a0\u00b0C (approximately 147\u2013205\u00a0\u00b0F). Under pressure, the heated water rises to the surface along fissures and faults in the limestone. Hot water at a temperature of rises here at the rate of daily, from the Pennyquick geological fault.\nIn 1983, a new spa-water bore-hole was sunk, providing a clean and safe supply for drinking in the Pump Room. There is no universal definition to distinguish a hot spring from a geothermal spring, although, by several definitions, the Bath springs can be considered the only hot springs in the UK. Three of the springs feed the thermal baths.\nClimate.\nAlong with the rest of South West England, Bath has a temperate climate which is generally wetter and milder than the rest of the country. The annual mean temperature is approximately . Seasonal temperature variation is less extreme than most of the United Kingdom because of the adjacent sea temperatures. The summer months of July and August are the warmest, with mean daily maxima of approximately . In winter, mean minimum temperatures of are common. In the summer, the Azores high pressure affects the south-west of England bringing fair weather; however, convective cloud sometimes forms inland, reducing the number of hours of sunshine. Annual sunshine rates are slightly less than the regional average of 1,600\u00a0hours.\nMost of the rainfall in the south-west is caused by Atlantic depressions or by convection. In summer, a large proportion of the rainfall is caused by sun heating the ground, leading to convection and to showers and thunderstorms. Average rainfall is around . About 8\u201315 days of snowfall is typical. November to March have the highest mean wind speeds, and June to August have the lightest winds. The predominant wind direction is from the southwest.\nGreen belt.\nBath is fully enclosed by green belt as a part of a wider environmental and planning policy first designated in the late 1950s, and this extends into much of the surrounding district and beyond, helping to maintain local green space, prevent further urban sprawl and unplanned expansion towards Bristol and Bradford-on-Avon, as well as protecting smaller villages in between. Suburbs of the city bordering the green belt include Batheaston, Bathford, Bathampton, the University of Bath campus, Ensleigh, Twerton, Upper Weston, Odd Down, and Combe Down.\nParts of the Cotswolds AONB southern extent overlap the green belt north of the city, with other nearby landscape features and facilities within the green belt including the River Avon, Kennet and Avon Canal, Bath Racecourse, Bath Golf Club, Bathampton Down, Bathampton Meadow Nature Reserve, Bristol and Bath Railway Path, the Cotswold Way, Limestone Link route, Pennyquick Park, Little Solsbury Hill, and Primrose Hill.\nDemography.\nDistrict.\nAccording to the 2021 census, Bath, together with North East Somerset, which includes areas around Bath as far as the Chew Valley, had a population of 193,400 (up 9.9% from 2011).\nThe district is largely non-religious and Christian at 47.9% and 42.2%, respectively, with no other religion reaching more than 1%. These figures generally compare with the national averages, though the non-religious, at 47.9%, are significantly more prevalent than the national 36.7%. 84.5% of residents rated their health as good or very good, higher than the national level (81.7%). Nationally, 17.7% of people identified as being disabled; in Bath it is 16.2%.\nThe table below compares the unitary authority district as a whole (including the city) and South West England and contrasts changes since the 2011 census. More detailed updated information, including figures specifically for the city of Bath, appear to be unavailable.\nCity.\nThe 2011 census recorded a population of 94,782 for the Bath built-up area and 88,859 for the city, with the latter exactly corresponding to the boundaries of the parliament constituency. The Bath built-up area extends slightly beyond the boundaries of the city itself, taking in areas to the northeast such as Bathampton and Bathford. The 2001 census figure for the city was 83,992. By 2019, the population was estimated at 90,000.\nAn inhabitant of Bath is known as a Bathonian.\nThe table below compares the city of Bath with the unitary authority district as a whole (including the city) and South West England.\nEconomy.\nIndustry.\nBath once had an important manufacturing sector, particularly in crane manufacture, furniture manufacture, printing, brass foundries, quarries, dye works and Plasticine manufacture, as well as many mills. Significant Bath companies included Stothert &amp; Pitt, Bath Cabinet Makers and Bath &amp; Portland Stone.\nDuring and after World War II Bath was a major location of Ministry of Defence offices, with three major sites on the outskirts of Bath (Ensleigh, Foxhill and Warminster Road) and a number of smaller central offices including the Empire Hotel. After the Cold War staff numbers declined, and from 2010 to 2013 about 2,600 remaining staff were moved to MoD Abbey Wood in Bristol. In 2013 the three major sites were sold for the development of over 1,000 new houses.\nNowadays, manufacturing is in decline, but the city boasts strong software, publishing and service-oriented industries, and the international manufacturing company Rotork has its headquarters in the city. The city's attraction to tourists has also led to a significant number of jobs in tourism-related industries. Important economic sectors in Bath include education and health (30,000 jobs), retail, tourism and leisure (14,000 jobs) and business and professional services (10,000 jobs).\nMajor employers are the National Health Service, Bath Spa University, the University of Bath, and Bath and North East Somerset Council. Growing employment sectors include information and communication technologies and creative and cultural industries where Bath is one of the recognised national centres for publishing, with the magazine and digital publisher Future plc employing around 650 people. Others include Buro Happold (400) and IPL Information Processing Limited (250). The city boasts over 400 retail shops, half of which are run by independent specialist retailers, and around 100 restaurants and cafes primarily supported by tourism.\nTourism.\nOne of Bath's principal industries is tourism, with annually more than one million staying visitors and 3.8\u00a0million day visitors. The visits mainly fall into the categories of heritage tourism and cultural tourism, aided by the city's selection in 1987 as a World Heritage Site in recognition of its international cultural importance. All significant stages of the history of England are represented within the city, from the Roman Baths (including their significant Celtic presence), to Bath Abbey and the Royal Crescent, to the more recent Thermae Bath Spa.\nThe size of the tourist industry is reflected in the almost 300 places of accommodation\u00a0\u2013 including more than 80 hotels, two of which have 'five-star' ratings, over 180 bed and breakfasts\u00a0\u2013 many of which are located in Georgian buildings, and two campsites located on the western edge of the city. The city also has about 100 restaurants and a similar number of pubs and bars.\nSeveral companies offer open top bus tours around the city, as well as tours on foot and on the river. Since the opening of Thermae Bath Spa in 2006, the city has attempted to recapture its historical position as the only town or city in the United Kingdom offering visitors the opportunity to bathe in naturally heated spring waters.\nIn the 2010 Google Street View Best Streets Awards, the Royal Crescent took second place in the \"Britain's Most Picturesque Street\" award, first place being given to The Shambles in York. Milsom Street was also awarded \"Britain's Best Fashion Street\" in the 11,000-strong vote.\nArchitecture.\nThere are many Roman archaeological sites throughout the central area of the city. The baths themselves are about below the present city street level. Around the hot springs, Roman foundations, pillar bases, and baths can still be seen; however, all the stonework above the level of the baths is from more recent periods.\nBath Abbey was a Norman church built on earlier foundations. The present building dates from the early 16th century and shows a late Perpendicular style with flying buttresses and crocketed pinnacles decorating a crenellated and pierced parapet. The choir and transepts have a fan vault by Robert and William Vertue. A matching vault was added to the nave in the 19th century. The building is lit by 52 windows.\nMost buildings in Bath are made from the local, golden-coloured Bath stone, and many date from the 18th and 19th century. The dominant style of architecture in Central Bath is Georgian; this style evolved from the Palladian revival style that became popular in the early 18th century. Many of the prominent architects of the day were employed in the development of the city. The original purpose of much of Bath's architecture is concealed by the honey-coloured classical fa\u00e7ades; in an era before the advent of the luxury hotel, these apparently elegant residences were frequently purpose-built lodging houses, where visitors could hire a room, a floor, or (according to their means) an entire house for the duration of their visit, and be waited on by the house's communal servants. The masons Reeves of Bath were prominent in the city from the 1770s to 1860s.\nThe Circus consists of three long, curved terraces designed by the elder John Wood to form a circular space or theatre intended for civic functions and games. The games give a clue to the design, the inspiration behind which was the Colosseum in Rome. Like the Colosseum, the three fa\u00e7ades have a different order of architecture on each floor: Doric on the ground level, then Ionic on the piano nobile, and finishing with Corinthian on the upper floor, the style of the building thus becoming progressively more ornate as it rises. Wood never lived to see his unique example of town planning completed as he died five days after personally laying the foundation stone on 18 May 1754.\nThe most spectacular of Bath's terraces is the Royal Crescent, built between 1767 and 1774 and designed by the younger John Wood. Wood designed the great curved fa\u00e7ade of what appears to be about 30 houses with Ionic columns on a rusticated ground floor, but that was the extent of his input: each purchaser bought a certain length of the fa\u00e7ade, and then employed their own architect to build a house to their own specifications behind it; hence what appears to be two houses is in some cases just one. This system of town planning is betrayed at the rear of the crescent: while the front is completely uniform and symmetrical, the rear is a mixture of differing roof heights, juxtapositions and fenestration. The \"Queen Anne fronts and Mary-Anne backs\" architecture occurs repeatedly in Bath and was designed to keep hired women at the back of the house. Other fine terraces elsewhere in the city include Lansdown Crescent and Somerset Place on the northern hill.\nAround 1770 the neoclassical architect Robert Adam designed Pulteney Bridge, using as the prototype for the three-arched bridge spanning the Avon an original, but unused, design by Andrea Palladio for the Rialto Bridge in Venice. Thus, Pulteney Bridge became not just a means of crossing the river, but also a shopping arcade. Along with the Rialto Bridge and the Ponte Vecchio in Florence, which it resembles, it is one of the very few surviving bridges in Europe to serve this dual purpose. It has been substantially altered since it was built. The bridge was named after Frances and William Pulteney, the owners of the Bathwick estate for which the bridge provided a link to the rest of Bath. \nThe Georgian streets in the vicinity of the river tended to be built high above the original ground level to avoid flooding, with the carriageways supported on vaults extending in front of the houses. This can be seen in the multi-storey cellars around Laura Place south of Pulteney Bridge, in the colonnades below Grand Parade, and in the grated coal holes in the pavement of North Parade. In some parts of the city, such as George Street, and London Road near Cleveland Bridge, the developers of the opposite side of the road did not match this pattern, leaving raised pavements with the ends of the vaults exposed to a lower street below.\nThe heart of the Georgian city was the Pump Room, which, together with its associated Lower Assembly Rooms, was designed by Thomas Baldwin, a local builder responsible for many other buildings in the city, including the terraces in Argyle Street and the Guildhall. Baldwin rose rapidly, becoming a leader in Bath's architectural history.\nIn 1776, he was made the chief City Surveyor, and Bath City Architect. Great Pulteney Street, where he eventually lived, is another of his works: this wide boulevard, constructed around 1789 and over long and wide, is lined on both sides by Georgian terraces.\nIn the 1960s and early 1970s some parts of Bath were unsympathetically redeveloped, resulting in the loss of some 18th- and 19th-century buildings. This process was largely halted by a popular campaign which drew strength from the publication of Adam Fergusson's \"The Sack of Bath\". Controversy has revived periodically, most recently with the demolition of the 1930s Churchill House, a neo-Georgian municipal building originally housing the Electricity Board, to make way for a new bus station. This is part of the Southgate redevelopment in which an ill-favoured 1960s shopping precinct, bus station and multi-storey car park were demolished and replaced by a new area of neo-Georgian shopping streets.\nAs a result of this and other changes, notably plans for abandoned industrial land along the Avon, the city's status as a World Heritage Site was reviewed by UNESCO in 2009. The decision was made to let Bath keep its status, but UNESCO asked to be consulted on future phases of the Riverside development, saying that the density and volume of buildings in the second and third phases of the development need to be reconsidered. It also demanded Bath do more to attract world-class architecture in new developments.\nIn 2021, Bath received its second UNESCO World Heritage inscription, becoming part of a group of 11 spa towns across seven countries that were listed by UNESCO as the \"Great Spas of Europe\".\nCulture.\nBath became the centre of fashionable life in England during the 18th century when its Old Orchard Street Theatre and architectural developments such as Lansdown Crescent, the Royal Crescent, The Circus, and Pulteney Bridge were built.\nBath's five theatres\u00a0\u2013 Theatre Royal, Ustinov Studio, the Egg, the Rondo Theatre, and the Mission Theatre\u00a0\u2013 attract internationally renowned companies and directors and an annual season by Sir Peter Hall. The city has a long-standing musical tradition; Bath Abbey, home to the Klais Organ and the largest concert venue in the city, stages about 20 concerts and 26 organ recitals each year. Another concert venue, the 1,600-seat art deco The Forum, originated as a cinema. The city holds the annual Bath International Music Festival and Mozartfest, the annual Bath Literature Festival (and its counterpart for children), the Bath Film Festival, the Bath Digital Festival. the Bath Fringe Festival, the Bath Beer Festival and the Bath Chilli Festival. The Bach Festivals occur at two and a half-year intervals. An annual Bard of Bath competition aims to find the best poet, singer or storyteller.\nThe city is home to the Victoria Art Gallery, the Museum of East Asian Art, and Holburne Museum, numerous commercial art galleries and antique shops, as well as a number of other museums, among them Bath Postal Museum, the Fashion Museum, the Jane Austen Centre, the Herschel Museum of Astronomy and the Roman Baths. The Bath Royal Literary and Scientific Institution (BRLSI) in Queen Square was founded in 1824 from the Society for the encouragement of Agriculture, Planting, Manufactures, Commerce and the Fine Arts founded in 1777. In September 1864, BRLSI hosted the 34th annual meeting of the British Science Association, which was attended by explorers David Livingstone, Sir Richard Francis Burton, and John Hanning Speke. The history of the city is displayed at the Museum of Bath Architecture, which is housed in a building built in 1765 as the Trinity Presbyterian Church. It was also known as the Countess of Huntingdon's Chapel, as she lived in the attached house from 1707 to 1791.\nThe arts.\nDuring the 18th century Thomas Gainsborough and Sir Thomas Lawrence lived and worked in Bath. John Maggs, a painter best known for coaching scenes, was born and lived in Bath with his artistic family.\nJane Austen lived there from 1801 with her father, mother and sister Cassandra, and the family resided at four different addresses until 1806. Jane Austen never liked the city, and wrote to Cassandra, \"It will be two years tomorrow since we left Bath for Clifton, with what happy feelings of escape.\" Bath has honoured her name with the Jane Austen Centre and a city walk. Austen's \"Northanger Abbey\" and \"Persuasion\" are set in the city and describe taking the waters, social life, and music recitals.\nWilliam Friese-Greene experimented with celluloid and motion pictures in his studio in the 1870s, developing some of the earliest movie camera technology. He is credited as being one of the inventors of cinematography.\nSatirist and political journalist William Hone was born in Bath in 1780.\nTaking the waters is described in Charles Dickens' novel \"The Pickwick Papers\" in which Pickwick's servant, Sam Weller, comments that the water has \"a very strong flavour o' warm flat irons\". The Royal Crescent is the venue for a chase between two characters, Dowler and Winkle. Moyra Caldecott's novel \"The Waters of Sul\" is set in Roman Bath in AD\u00a072, and \"The Regency Detective\", by David Lassman and Terence James, revolves around the exploits of Jack Swann investigating deaths in the city during the early 19th century. Richard Brinsley Sheridan's play \"The Rivals\" takes place in the city, as does Roald Dahl's chilling short story, \"The Landlady\".\nMany films and television programmes have been filmed using its architecture as the backdrop, including the 2004 film of Thackeray's \"Vanity Fair\", \"The Duchess\" (2008), \"The Elusive Pimpernel\" (1950) and \"The Titfield Thunderbolt\" (1953). In 2012, Pulteney Weir was used as a replacement location during post production of the film adaptation of \"Les Mis\u00e9rables\". Stunt shots were filmed in October 2012 after footage acquired during the main filming period was found to have errors. The ITV police drama McDonald &amp; Dodds is set and mostly filmed in Bath using many of the city's famous sites.\nIn August 2003 The Three Tenors sang at a concert to mark the opening of the Thermae Bath Spa, a new hot water spa in the city centre, but delays to the project meant the spa actually opened three years later on 7 August 2006. In 2008, 104 decorated pigs were displayed around the city in a public art event called \"King Bladud's Pigs in Bath\". It celebrated the city, its origins and artists. Decorated pig sculptures were displayed throughout the summer and were auctioned to raise funds for Two Tunnels Greenway.\nParks.\nRoyal Victoria Park, a short walk from the city centre, was opened in 1830 by the 11-year-old Princess Victoria, and was the first park to carry her name. The public park is overlooked by the Royal Crescent and covers . It has a skatepark, tennis courts, a bowling green, a putting green and a 12- and 18-hole golf course, a pond, open-air concerts, an annual travelling funfair at Easter, and a children's play area. Much of its area is lawn; a notable feature is a ha-ha that segregates it from the Royal Crescent while giving the impression from the Crescent of uninterrupted grassland across the park to Royal Avenue. It has a \"Green Flag Award\", the national standard for parks and green spaces in England and Wales, and is registered by English Heritage as of National Historic Importance. The botanical gardens were formed in 1887 and contain one of the finest collections of plants on limestone in the West Country.\nA replica Roman Temple was built at the British Empire Exhibition at Wembley in 1924, and, following the exhibition, was dismantled and rebuilt in Victoria Park in Bath. In 1987, the gardens were extended to include the Great Dell, a disused quarry with a collection of conifers.\nOther parks include Alexandra Park on a hill overlooking the city; Parade Gardens, along the river near the abbey in the city centre; Sydney Gardens, an 18th-century pleasure garden; Henrietta Park; Hedgemead Park; and Alice Park. Jane Austen wrote \"It would be pleasant to be near the Sydney Gardens. We could go into the Labyrinth every day.\" Alexandra, Alice and Henrietta parks were built into the growing city among the housing developments. Linear Park is built on the old Somerset and Dorset Joint Railway line, and connects with the Two Tunnels Greenway which contains the longest cycling and walking tunnel in the UK. Cleveland Pools were built around 1815 close to the River Avon, now the oldest surviving public outdoor lido in England. Restoration was completed in 2023, after a 20-year fund-raising campaign, with the lido opening for the first time in 40 years on 10 September.\nQueen Victoria.\nVictoria Art Gallery and Royal Victoria Park are named after Queen Victoria, who wrote in her journal in 1837, \"The people are really too kind to me.\" This feeling seemed to have been reciprocated by the people of Bath: \"Lord James O'Brien brought a drawing of the intended pillar which the people of Bath are so kind as to erect in commemoration of my 18th birthday.\"\nFood.\nSeveral foods have an association with the city. \"Sally Lunn buns\" (a type of teacake) have long been baked in Bath. They were first mentioned by name in verses printed in the Bath Chronicle, in 1772. At that time they were eaten hot at public breakfasts in Spring Gardens. They can be eaten with sweet or savoury toppings and are sometimes confused with \"Bath buns\", which are smaller, round, very sweet and very rich. They were associated with the city following The Great Exhibition. Bath buns were originally topped with crushed comfits created by dipping caraway seeds repeatedly in boiling sugar; but today seeds are added to a 'London Bath Bun' (a reference to the bun's promotion and sale at the Great Exhibition). The seeds may be replaced by crushed sugar granules or 'nibs'.\nBath has lent its name to one other distinctive recipe\u00a0\u2013 \"Bath Olivers\"\u00a0\u2013 a dry baked biscuit invented by Dr William Oliver, physician to the Mineral Water Hospital in 1740. Oliver was an anti-obesity campaigner and author of a \"Practical Essay on the Use and Abuse of warm Bathing in Gluty Cases\". In more recent years, Oliver's efforts have been traduced by the introduction of a version of the biscuit with a plain chocolate coating. Bath chaps, the salted and smoked cheek and jawbones of the pig, takes its name from the city and is available from a stall in the daily covered market. Bath Ales brewery is located in Warmley and Abbey Ales are brewed in the city.\nTwinning.\nCity twinning is the responsibility of the Charter Trustees and each twinning arrangement is managed by a Twinning Association. Bath is twinned with four other cities in Europe:\nThere is also a historic connection with Manly, New South Wales, Australia, which is referred to as a sister city; a partnership arrangement with Beppu, \u014cita Prefecture, Japan; and a friendship agreement with Oleksandriia, Kirovohrad Oblast, Ukraine.\nEducation.\nBath has two universities, the University of Bath and Bath Spa University. Established in 1966, the University of Bath was named University of the Year by \"The Sunday Times\" in 2011. It offers programs in politics, languages, the physical sciences, engineering, mathematics, architecture, management and technology.\nBath Spa University was first granted degree-awarding powers in 1992 as a university college before being granted university status in August 2005. It offers courses leading to a Postgraduate Certificate in Education. It has schools in the following subject areas: Art and Design, Education, English and Creative Studies, Historical and Cultural Studies, Music and the Performing Arts, Science and the Environment and Social Sciences.\nBath College offers further education, and Norland College provides education and training in childcare.\nSport.\nRugby.\nBath Rugby is a rugby union team who play in the Premiership, England's top division of rugby. It plays in blue, white and black kit at the Recreation Ground in the city, where it has been since the late 19th century, following its establishment in 1865. Bath Rugby is the joint-most successful club in England, having won 21 major trophies. It was particularly successful between 1984 and 1998, when it won 10 Domestic Cups, 6 of its 7 League titles, and became the first English side to win the European Cup in 1998. In 2008 and 2025, Bath also won the European Challenge Cup, the continent's second-tier competition.\nThe team's squad includes several members who also play, or have played in the English national team, including Tom Dunn, Beno Obano, Will Stuart, Charlie Ewels, Ted Hill, Guy Pepper, Sam Underhill, Ben Spencer, Ollie Lawrence, Max Ojomoh, Henry Arundell, Joe Cokanasiga and Will Muir. The former England Rugby Team Manager and former Scotland national coach Andy Robinson used to play for Bath Rugby team and was captain and later coach. Both of Robinson's predecessors, Clive Woodward and Jack Rowell, as well as his successor Brian Ashton, were also former Bath coaches and managers.\nBath was described by former head coach Jack Rowell as \u201ca Georgian city, a Roman city, but more so than that, it\u2019s a rugby city\u201d. Bath Rugby routinely sell out matches at the 14,509-capacity Recreation Ground, and in 2024 the club submitted updated plans to redevelop it into a modern, 18,000-capacity stadium. In June 2025, Bath won their first Premiership Rugby title in 29 years, completing the third leg of a historic treble, having already secured the Premiership Rugby Cup and the European Rugby Challenge Cup earlier that season. The following day, thousands of supporters lined the streets for a victory parade. Players travelled on two open-top buses across Pulteney Bridge and up Milsom Street, before heading towards Bath Abbey and ending at The Recreation Ground, where a ticketed party was held.\nFootball.\nBath City F.C. is the semi-professional football team. Founded in 1889, the club has played their home matches at Twerton Park since 1932. Bath City's history is entirely in non-league football, predominantly in the 5th tier. Bath narrowly missed out on election to the Football League by a few votes in 1978 and again in 1985. The club have a good history in the FA Cup, reaching the third round six times. The record attendance, 18,020, at the ground was in 1960 against Brighton. The club's colours are black and white and their official nickname is \"The Romans\", stemming from Bath's Ancient Roman history. The club is sometimes called \"The Stripes\", referring to their striped kit.\nUntil 2009 Team Bath F.C. operated as an affiliate to the University Athletics programme. In 2002, Team Bath became the first university team to enter the FA Cup in 120\u00a0years, and advanced through four qualifying rounds to the first round proper. The university's team was established in 1999 while the city team has existed since before 1908 (when it entered the Western League). However, in 2009, the Football Conference ruled that Team Bath would not be eligible to gain promotion to a National division, nor were they allowed to participate in Football Association cup competitions. This ruling led to the decision by the club to fold at the end of the 2008\u201309 Conference South competition. In their final season, Team Bath F.C. finished 11th in the league.\nBath also has Non-League football clubs Odd Down F.C. who play at the Lew Hill Memorial Ground and Larkhall Athletic F.C. who play at Plain Ham.\nOther sports.\nMany cricket clubs are based in the city, including Bath Cricket Club, who are based at the North Parade Ground and play in the West of England Premier League. Cricket is also played on the Recreation Ground, just across from the rugby club. The Recreation Ground is also home to Bath Croquet Club, which was re-formed in 1976 and is affiliated with the South West Federation of Croquet Clubs.\nThe Bath Half Marathon is run annually through the city streets, with over 10,000 runners.\nTeamBath is the umbrella name for all of the University of Bath sports teams, including the aforementioned football club. Other sports for which TeamBath is noted are athletics, badminton, basketball, bob skeleton, bobsleigh, hockey, judo, modern pentathlon, netball, rugby union, swimming, tennis, triathlon and volleyball. The City of Bath Triathlon takes place annually at the university.\nBath Roller Derby Girls (BRDG) is a flat track roller derby club, founded in 2012, they compete in the British Roller Derby Championships Tier 3. As of 2015, they are full members of the United Kingdom Roller Derby Association (UKRDA.)\nBath is home to a table tennis League, made up of 3 divisions and a number of clubs based in Bath and the surrounding area.\nTransport.\nRailways.\nThe city is served by Bath Spa railway station, designed by Isambard Kingdom Brunel, which is on the Great Western Main Line. Services are provided by Great Western Railway on the following routes:\nThere is a suburban station on the main line, Oldfield Park, which has a limited commuter service to Bristol.\nBath Green Park station was once the terminus of the Midland Railway, and junction for the Somerset and Dorset Joint Railway, whose line, always steam hauled, went through the Devonshire tunnel (under the Wellsway, St Luke's Church and the Devonshire Arms), through the Combe Down Tunnel and climbed over the Mendips to serve many towns and villages on its run to Bournemouth. This example of an English rural line was closed as part of the Beeching cuts in March 1966. Its Bath station building, now restored, houses shops, small businesses, a Saturday farmers' market and parking for a supermarket, while the route of the Somerset and Dorset through the suburbs to Midford has been reused for the Two Tunnels Greenway, a shared use path that extends National Cycle Route 24 into the city.\nBuses.\nBath has a network of bus routes, operated by First West of England, with services to surrounding towns and cities, such as Bristol, Trowbridge, Frome and Wells.\nFaresaver Buses also operate services to surrounding towns. The Bath Bus Company runs open-top double-decker bus tours around the city, as well as frequent services to Bristol Airport. Stagecoach West also provides services to Tetbury and the South Cotswolds. The suburbs of Bath are also served by the WESTlink on demand service, available Monday to Saturday.\nNational Express operates inter-city coach services from Bath bus station.\nRoads.\nBath is approximately south-east of the larger city and port of Bristol, to which it is linked by the A4 road and is a similar distance south of the M4 motorway at junction 18. The potential new junction 18a linking the M4 with the A4174 Avon Ring Road could provide an additional direct route from Bath to the motorway.\nIn an attempt to reduce the level of car use, park and ride schemes have been introduced, with sites at Odd Down, Lansdown and Newbridge. A large increase in city centre parking was provided under the 2010 SouthGate shopping centre development, which introduced more car traffic. A bus gate scheme in Northgate aims to reduce private car use in the city centre.\nA transport study (the Bristol/Bath to South Coast Study) was published in 2004, after being initiated by the Government Office for the South West and Bath and North East Somerset Council and undertaken by WSP Global as a result of the de-trunking in 1999 of the A36/A46 trunk road network from Bath to Southampton.\nThe Bath Clean Air Zone was introduced for central Bath on 15 March 2021. A Class C zone, it charges the most polluting commercial vehicles \u00a39 per day (and up to \u00a3100 per day for coaches and HGVs). It is the first UK road pollution charging zone outside London, and reduced nitrogen dioxide levels in the city by 26% over the following two years, meeting legal standards.\nCycling.\nBath is on National Cycle Route 4, with one of Britain's first cycleways, the Bristol and Bath Railway Path, to the west, and an eastern route toward London on the canal towpath. Bath is about from Bristol Airport. Bath also benefits from several bridleways and byways.\nRivers and canals.\nThe city is connected to Bristol and the sea by the River Avon, navigable via locks by small boats. The river was connected to the Thames and London by the Kennet and Avon Canal in 1810 via Bath Locks; this waterway\u00a0\u2013 closed for many years but restored in the last years of the 20th century\u00a0\u2013 is now popular with narrowboat users.\nTrams.\nThe Bath Tramways Company began operations on 24 December 1880. The gauge cars were horse-drawn along a route from London Road to the railway station. The system closed in 1902 and was replaced by electric tramcars on a greatly expanded gauge system that opened in 1904. This eventually extended to with routes to Combe Down, Oldfield Park, Twerton, Newton St Loe, Weston and Bathford. There was a fleet of 40 cars, all but six being double deck. The first line to close was replaced by a bus service in 1938, and the last went on 6 May 1939.\nIn 2005, a detailed plan was presented to the council to reintroduce trams to Bath, but the plan did not proceed, reportedly due to the focus by the council on the government-supported busway planned to run from the Newbridge park and ride into the city centre. Part of the justification for the plan was pollution from vehicles in the city, which was twice the legal levels, and heavy traffic congestion due to high car usage. In 2015 another group, Bath Trams, building on the earlier tram group proposals, created interest in the idea of reintroducing trams with several public meetings and meetings with the council. In 2017, Bath and North East Somerset Council announced a feasibility study into implementing a light rail or tram system in the city. In November 2016, the West of England Local Enterprise Partnership began a consultation process on their Transport Vision Summary Document, outlining potential light rail or tram routes in the region, one of them a route from Bristol city centre along the A4 road to Bath to relieve pressure on bus and rail services between the two cities.\nMedia.\nBath's local newspaper is the \"Bath Chronicle\", owned by Local World. Published since 1760, the \"Chronicle\" was a daily newspaper until mid-September 2007, when it became a weekly. Since 2018 its website has been operated by Trinity Mirror's \"SomersetLive\" platform.\nThe BBC Bristol website has featured coverage of news and events within Bath since 2003.\nFor television, Bath is served by the BBC West studios based in Bristol, and by ITV West Country, formerly HTV, also from studios in Bristol.\nRadio stations broadcasting to the city include BBC Radio Bristol which has a studio in Kingsmead Square in the city centre, BBC Radio Somerset in Taunton, Greatest Hits Radio South West on 107.9FM and Heart West, formerly GWR FM, as well as The University of Bath's University Radio Bath, a student-focused radio station available on campus and also online. Bath Sound (formerly Bath Hospital Radio and BA1 Radio) is an online community station run by a charity.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41524", "revid": "980331", "url": "https://en.wikipedia.org/wiki?curid=41524", "title": "Renaissance architecture", "text": "15th\u201316th-century European architectural style\nRenaissance architecture is the European architecture of the period between the early 15th and early 16th centuries in different regions, demonstrating a conscious revival and development of certain elements of ancient Greek and Roman thought and material culture. Stylistically, Renaissance architecture followed Gothic architecture and was succeeded by Baroque architecture and neoclassical architecture. \nDeveloped first in Florence, with Filippo Brunelleschi as one of its innovators, the Renaissance style quickly spread to other Italian cities. The style was carried to other parts of Europe at different dates and with varying degrees of impact. It began in Florence in the early 15th century and reflected a revival of classical Greek and Roman principles such as symmetry, proportion, and geometry. This movement was supported by wealthy patrons, including the Medici family and the Catholic Church, who commissioned works to display both religious devotion and political power. Architects such as Filippo Brunelleschi, Leon Battista Alberti, and later Andrea Palladio revolutionized urban landscapes with domes, columns, and harmonious facades. While Renaissance architecture flourished most in Italy, its influence spread across Europe reaching France, Spain, and the Low Countries adapting to local traditions. Public buildings, churches, and palaces became symbols of civic pride and imperial strength, linking humanism with empire-building.\nRenaissance style places emphasis on symmetry, proportion, geometry and the regularity of parts, as demonstrated in the architecture of classical antiquity and in particular ancient Roman architecture, of which many examples remained. Orderly arrangements of columns, pilasters and lintels, as well as the use of semicircular arches, hemispherical domes, niches and aediculae replaced the more complex proportional systems and irregular profiles of medieval buildings.\n&lt;templatestyles src=\"Template:TOC limit/styles.css\" /&gt;\nHistoriography.\nThe word \"Renaissance\" derives from the term \"rinascita\", which means rebirth, first appeared in Giorgio Vasari's \"Lives of the Most Excellent Painters, Sculptors, and Architects\", 1550.\nAlthough the term Renaissance was used first by the French historian Jules Michelet, it was given its more lasting definition from the Swiss historian Jacob Burckhardt, whose book \"The Civilization of the Renaissance in Italy\", 1860, was influential in the development of the modern interpretation of the Italian Renaissance. The folio of measured drawings \"\u00c9difices de Rome moderne; ou, Recueil des palais, maisons, \u00e9glises, couvents et autres monuments\" (The Buildings of Modern Rome), first published in 1840 by Paul Letarouilly, also played an important part in the revival of interest in this period. Erwin Panofsky, \"Renaissance and Renascences in Western Art\", (New York: Harper and Row, 1960) The Renaissance style was recognized by contemporaries in the term \"all'antica\", or \"in the ancient manner\" (of the Romans).\nPrincipal phases.\nHistorians often divide the Renaissance in Italy into three phases. Whereas art historians might talk of an \"Early Renaissance\" period, in which they include developments in 14th-century painting and sculpture, this is usually not the case in architectural history. The bleak economic conditions of the late 14th century did not produce buildings that are considered to be part of the Renaissance. As a result, the word \"Renaissance\" among architectural historians usually applies to the period 1400 to c.\u20091525, or later in the case of non-Italian Renaissances.\nHistorians often use the following designations:\nDuring the \"Quattrocento,\" sometimes known as the Early Renaissance, concepts of architectural order were explored and rules were formulated. The study of classical antiquity led in particular to the adoption of Classical detail and ornamentation. Space, as an element of architecture, was used differently than it was in the Middle Ages. Space was organised by proportional logic, its form and rhythm subject to geometry, rather than being created by intuition as in Medieval buildings. The prime example of this is the Basilica of San Lorenzo, Florence by Filippo Brunelleschi (1377\u20131446).\nDuring the High Renaissance, concepts derived from classical antiquity were developed and used with greater confidence. The most representative architect is Donato Bramante (1444\u20131514), who expanded the applicability of classical architecture to contemporary buildings. His Tempietto di San Pietro in Montorio (1503) was directly inspired by circular Roman temples. He was, however, hardly a slave to the classical forms and it was his style that was to dominate Italian architecture in the 16th century.\nDuring the Mannerist period, architects experimented with using architectural forms to emphasize solid and spatial relationships. The Renaissance ideal of harmony gave way to freer and more imaginative rhythms. The best known architect associated with the Mannerist style was Michelangelo (1475\u20131564), who frequently used the giant order in his architecture, a large pilaster that stretches from the bottom to the top of a fa\u00e7ade. He used this in his design for the Piazza del Campidoglio in Rome. Prior to the 20th century, the term \"Mannerism\" had negative connotations, but it is now used to describe the historical period in more general non-judgemental terms.\nAs the new style of architecture spread out from Italy, most other European countries developed a sort of Proto-Renaissance style, before the construction of fully formulated Renaissance buildings. Each country in turn then grafted its own architectural traditions to the new style, so that Renaissance buildings across Europe are diversified by region. Within Italy the evolution of Renaissance architecture into Mannerism, with widely diverging tendencies in the work of Michelangelo, Giulio Romano and Andrea Palladio, led to the Baroque style in which the same architectural vocabulary was used for very different rhetoric. Outside Italy, Baroque architecture was more widespread and fully developed than the Renaissance style, with significant buildings as far afield as Mexico and the Philippines.\nHistory.\nDevelopment in Italy.\nItaly of the 15th century, and the city of Florence in particular, was home to the Renaissance. It is in Florence that the new architectural style had its beginning, not slowly evolving in the way that Gothic grew out of Romanesque, but consciously brought to being by particular architects who sought to revive the order of a past \"Golden Age\". The scholarly approach to the architecture of the ancient coincided with the general revival of learning. A number of factors were influential in bringing this about.\nArchitectural.\nItalian architects had always preferred forms that were clearly defined and structural members that expressed their purpose. Many Tuscan Romanesque buildings demonstrate these characteristics, as seen in the Florence Baptistery and Pisa Cathedral.\nItaly had never fully adopted the Gothic style of architecture. Apart from Milan Cathedral, (influenced by French Rayonnant Gothic), few Italian churches show the emphasis on vertical, the clustered shafts, ornate tracery and complex ribbed vaulting that characterise Gothic in other parts of Europe.\nThe presence, particularly in Rome, of ancient architectural remains showing the ordered Classical style provided an inspiration to artists at a time when philosophy was also turning towards the Classical.\nPolitical.\nIn the 15th century, Florence and Venice extended their power through much of the area that surrounded them, making the movement of artists possible. This enabled Florence to have significant artistic influence in Milan, and through Milan, France.\nIn 1377, the return of the Pope from the Avignon Papacy and the re-establishment of the Papal court in Rome, brought wealth and importance to that city, as well as a renewal in the importance of the Pope in Italy, which was further strengthened by the Council of Constance in 1417. Successive Popes, especially Julius II, 1503\u201313, sought to extend the Papacy's temporal power throughout Italy.\nCommercial.\nIn the early Renaissance, Venice controlled sea trade over goods from the East. The large towns of Northern Italy were prosperous through trade with the rest of Europe, Genoa providing a seaport for the goods of France and Spain; Milan and Turin being centres of overland trade, and maintaining substantial metalworking industries. Trade brought wool from England to Florence, ideally located on the river for the production of fine cloth, the industry on which its wealth was founded. By dominating Pisa, Florence gained a seaport, and became the most powerful state in Tuscany. In this commercial climate, one family in particular turned their attention from trade to the lucrative business of money-lending. The Medici became the chief bankers to the princes of Europe, becoming virtually princes themselves as they did so, by reason of both wealth and influence. Along the trade routes, and thus offered some protection by commercial interest, moved not only goods but also artists, scientists and philosophers. \nReligious.\nThe return of the Pope Gregory XI from Avignon in September 1377 and the resultant new emphasis on Rome as the center of Christian spirituality, brought about a surge in the building of churches in Rome such as had not taken place for nearly a thousand years. This commenced in the mid 15th century and gained momentum in the 16th century, reaching its peak in the Baroque period. The construction of the Sistine Chapel with its uniquely important decorations and the entire rebuilding of St. Peter's Basilica, one of Christendom's most significant churches, were part of this process.\nIn the wealthy Republic of Florence, the impetus for church-building was more civic than spiritual. The unfinished state of the enormous Florence Cathedral dedicated to the Blessed Virgin Mary did no honour to the city under her patronage. However, as the technology and finance were found to complete it, the rising dome did credit not only to the Virgin Mary, its architect and the Church but also to the Signoria, the Guilds and the sectors of the city from which the manpower to construct it was drawn. The dome inspired further religious works in Florence.\nPhilosophic.\nThe development of printed books, the rediscovery of ancient writings, the expanding of political and trade contacts and the exploration of the world all increased knowledge and the desire for education.\nThe reading of philosophies that were not based on Christian theology led to the development of humanism through which it was clear that while God had established and maintained order in the Universe, it was the role of Man to establish and maintain order in Society.\nCivil.\nThrough humanism, civic pride and the promotion of civil peace and order were seen as the marks of citizenship. This led to the building of structures such as Brunelleschi's Hospital of the Innocents with its elegant colonnade forming a link between the charitable building and the public square, and the Laurentian Library where the collection of books established by the Medici family could be consulted by scholars.\nSome major ecclesiastical building works were also commissioned, not by the church, but by guilds representing the wealth and power of the city. Brunelleschi's dome at Florence Cathedral, more than any other building, belonged to the populace because the construction of each of the eight segments was achieved by a different quarter of the city.\nPatronage.\nAs in the Platonic Academy of Athens, it was seen by those of Humanist understanding that those people who had the benefit of wealth and education ought to promote the pursuit of learning and the creation of that which was beautiful. To this end, wealthy families\u2014the Medici of Florence, the Gonzaga of Mantua, the Farnese in Rome, the Sforzas in Milan\u2014gathered around them people of learning and ability, promoting the skills and creating employment for the most talented artists and architects of their day.\nRise of architectural theory.\nDuring the Renaissance, architecture became not only a question of practice, but also a matter for theoretical discussion. Printing played a large role in the dissemination of ideas.\nSpread of the Renaissance in Italy.\nIn the 15th century the courts of certain other Italian states became centres for spreading of Renaissance philosophy, art and architecture.\nIn Mantua at the court of the Gonzaga, Alberti designed two churches, the Basilica of Sant'Andrea and San Sebastiano.\nUrbino was an important centre with the Ducal Palace being constructed for Federico da Montefeltro in the mid 15th century. The Duke employed Luciano Laurana from Dalmatia, renowned for his expertise at fortification. The design incorporates much of the earlier medieval building and includes an unusual turreted three-storeyed fa\u00e7ade. Laurana was assisted by Francesco di Giorgio Martini. Later parts of the building are clearly Florentine in style, particularly the inner courtyard, but it is not known who the designer was.\nFerrara, under the Este, was expanded in the late 15th century, with several new palaces being built such as the Palazzo dei Diamanti and Palazzo Schifanoia for Borso d'Este.\nIn Milan, under the Visconti, the Certosa di Pavia was completed, and then later under the Sforza, the Castello Sforzesco was built.\nVenetian Renaissance architecture developed a particularly distinctive character because of local conditions. San Zaccaria received its Renaissance fa\u00e7ade at the hands of Antonio Gambello and Mauro Codussi, begun in the 1480s. Giovanni Maria Falconetto, the Veronese architect-sculptor, introduced Renaissance architecture to Padua with the Loggia and Odeo Cornaro in the garden of Alvise Cornaro.\nIn southern Italy, Renaissance masters were called to Naples by Alfonso V of Aragon after his conquest of the Kingdom of Naples. The most notable examples of Renaissance architecture in that city are the Cappella Caracciolo, attributed to Bramante, and the Palazzo Orsini di Gravina, built by Gabriele d'Angelo between 1513 and 1549.\nCharacteristics.\nThe Classical orders were analysed and reconstructed to serve new purposes. While the obvious distinguishing features of Classical Roman architecture were adopted by Renaissance architects, the forms and purposes of buildings had changed over time, as had the structure of cities. Among the earliest buildings of the reborn Classicism were the type of churches that the Romans had never constructed. Neither were there models for the type of large city dwellings required by wealthy merchants of the 15th century. Conversely, there was no call for enormous sporting fixtures and public bath houses such as the Romans had built.\nPlan.\nThe plans of Renaissance buildings have a square, symmetrical appearance in which proportions are usually based on a module. Within a church, the module is often the width of an aisle. The need to integrate the design of the plan with the fa\u00e7ade was introduced as an issue in the work of Filippo Brunelleschi, but he was never able to carry this aspect of his work into fruition. The first building to demonstrate this was Basilica of Sant'Andrea, Mantua by Leone Battista Alberti. The development of the plan in secular architecture was to take place in the 16th century and culminated with the work of Palladio.\nFa\u00e7ade.\nFa\u00e7ades are symmetrical around their vertical axis. Church fa\u00e7ades are generally surmounted by a pediment and organised by a system of pilasters, arches and entablatures. The columns and windows show a progression towards the centre. One of the first true Renaissance fa\u00e7ades was Pienza Cathedral (1459\u20131462), which has been attributed to the Florentine architect Bernardo Gambarelli (known as Rossellino) with Leone Battista Alberti perhaps having some responsibility in its design as well.\nDomestic buildings are often surmounted by a cornice. There is a regular repetition of openings on each floor, and the centrally placed door is marked by a feature such as a balcony, or rusticated surround. An early and much copied prototype was the fa\u00e7ade for the Palazzo Rucellai (1446 and 1451) in Florence with its three registers of pilasters.\nColumns and pilasters.\nRoman and Greek orders of columns are used: Tuscan, Doric, Ionic, Corinthian and Composite. The orders can either be structural, supporting an arcade or architrave, or purely decorative, set against a wall in the form of pilasters. During the Renaissance, architects aimed to use columns, pilasters, and entablatures as an integrated system. One of the first buildings to use pilasters as an integrated system was in the Old Sacristy (1421\u20131440) by Brunelleschi.\nArches.\nArches are semi-circular or (in the Mannerist style) segmental. Arches are often used in arcades, supported on piers or columns with capitals. There may be a section of entablature between the capital and the springing of the arch. Alberti was one of the first to use the arch on a monumental scale at the Basilica of Sant'Andrea, Mantua.\nVaults.\nVaults do not have ribs. They are semi-circular or segmental and on a square plan, unlike the Gothic vault which is frequently rectangular. The barrel vault is returned to architectural vocabulary as at St. Andrea in Mantua.\nDomes.\nThe dome is used frequently, both as a very large structural feature that is visible from the exterior, and also as a means of roofing smaller spaces where they are only visible internally. After the success of the dome in Brunelleschi's design for Florence Cathedral and its use in Bramante's plan for St. Peter's Basilica (1506) in Rome, the dome became an indispensable element in church architecture and later even for secular architecture, such as Palladio's Villa Rotonda.\nCeilings.\nRoofs are fitted with flat or coffered ceilings. They are not left open as in Medieval architecture. They are frequently painted or decorated.\nDoors.\nDoors usually have square lintels. They may be set with in an arch or surmounted by a triangular or segmental pediment. Openings that do not have doors are usually arched and frequently have a large or decorative keystone.\nWindows.\nWindows may be paired and set within a semi-circular arch. They may have square lintels and triangular or segmental pediments, which are often used alternately. Emblematic in this respect is the Palazzo Farnese in Rome, begun in 1517.\nIn the Mannerist period the Palladian arch was employed, using a motif of a high semi-circular topped opening flanked with two lower square-topped openings. Windows are used to bring light into the building and in domestic architecture, to give views. Stained glass, although sometimes present, is not a feature.\nWalls.\nExternal walls are generally constructed of brick, rendered, or faced with stone in highly finished ashlar masonry, laid in straight courses. The corners of buildings are often emphasized by rusticated quoins. Basements and ground floors were often rusticated, as at the Palazzo Medici Riccardi (1444\u20131460) in Florence. Internal walls are smoothly plastered and surfaced with lime wash. For more formal spaces, internal surfaces are decorated with frescoes.\nDetails.\nCourses, mouldings and all decorative details are carved with great precision. Studying and mastering the details of the ancient Romans was one of the important aspects of Renaissance theory. The different orders each required different sets of details. Some architects were stricter in their use of classical details than others, but there was also a good deal of innovation in solving problems, especially at corners. Mouldings stand out around doors and windows rather than being recessed, as in Gothic architecture. Sculptured figures may be set in niches or placed on plinths. They are not integral to the building as in Medieval architecture.\nEarly Renaissance.\nThe leading architects of the Early Renaissance or Quattrocento were Filippo Brunelleschi, Michelozzo and Leon Battista Alberti.\nBrunelleschi.\nThe person generally credited with bringing about the Renaissance view of architecture is Filippo Brunelleschi, (1377\u20131446). The underlying feature of the work of Brunelleschi was \"order\".\nIn the early 15th century, Brunelleschi began to look at the world to see what the rules were that governed one's way of seeing. He observed that the way one sees regular structures such as the Florence Baptistery and the tiled pavement surrounding it follows a mathematical order \u2013 linear perspective.\nThe buildings remaining among the ruins of ancient Rome appeared to respect a simple mathematical order in the way that Gothic buildings did not. One incontrovertible rule governed all Ancient Roman architecture \u2013 a semi-circular arch is exactly twice as wide as it is high. A fixed proportion with implications of such magnitude occurred nowhere in Gothic architecture. A Gothic pointed arch could be extended upwards or flattened to any proportion that suited the location. Arches of differing angles frequently occurred within the same structure. No set rules of proportion applied.\nFrom the observation of the architecture of Rome came a desire for symmetry and careful proportion in which the form and composition of the building as a whole and all its subsidiary details have fixed relationships, each section in proportion to the next, and the architectural features serving to define exactly what those rules of proportion are. Brunelleschi gained the support of a number of wealthy Florentine patrons, including the Silk Guild and Cosimo de' Medici.\nFlorence Cathedral.\nBrunelleschi's first major architectural commission was for the enormous brick dome which covers the central space of Florence's cathedral, designed by Arnolfo di Cambio in the 14th century but left unroofed. While often described as the first building of the Renaissance, Brunelleschi's daring design utilises the pointed Gothic arch and Gothic ribs that were apparently planned by Arnolfo. It seems certain, however, that while stylistically Gothic, in keeping with the building it surmounts, the dome is in fact structurally influenced by the great dome of Ancient Rome, which Brunelleschi could hardly have ignored in seeking a solution. This is the dome of the Pantheon, a circular temple, now a church.\nInside the Pantheon's single-shell concrete dome is coffering which greatly decreases the weight. The vertical partitions of the coffering effectively serve as ribs, although this feature does not dominate visually. At the apex of the Pantheon's dome is an opening, 8 meters across. Brunelleschi was aware that a dome of enormous proportion could in fact be engineered without a keystone. The dome in Florence is supported by the eight large ribs and sixteen more internal ones holding a brick shell, with the bricks arranged in a herringbone manner. Although the techniques employed are different, in practice, both domes comprise a thick network of ribs supporting very much lighter and thinner infilling. And both have a large opening at the top.\nSan Lorenzo.\nThe new architectural philosophy of the Renaissance is best demonstrated in the churches of San Lorenzo, and Santo Spirito, Florence. Designed by Brunelleschi in about 1425 and 1428 respectively, both have the shape of the Latin cross. Each has a modular plan, each portion being a multiple of the square bay of the aisle. This same formula controlled also the vertical dimensions. In the case of Santo Spirito, which is entirely regular in plan, transepts and chancel are identical, while the nave is an extended version of these. In 1434 Brunelleschi designed the first Renaissance centrally planned building, Santa Maria degli Angeli, Florence. It is composed of a central octagon surrounded by a circuit of eight smaller chapels. From this date onwards numerous churches were built in variations of these designs.\nMichelozzo.\nMichelozzo Michelozzi (1396\u20131472), was another architect under patronage of the Medici family, his most famous work being the Palazzo Medici Riccardi, which he was commissioned to design for Cosimo de' Medici in 1444. A decade later he built the Villa Medici, Fiesole. Among his other works for Cosimo are the library at the Convent of San Marco, Florence. He went into exile in Venice for a time with his patron. He was one of the first architects to work in the Renaissance style outside Italy, building a palace at Dubrovnik.\nThe Palazzo Medici Riccardi is Classical in the details of its pedimented windows and recessed doors, but, unlike the works of Brunelleschi and Alberti, there are no classical orders of columns in evidence. Instead, Michelozzo has respected the Florentine liking for rusticated stone. He has seemingly created three orders out of the three defined rusticated levels, the whole being surmounted by an enormous Roman-style cornice which juts out over the street by 2.5 meters.\nAlberti.\nLeon Battista Alberti, born in Genoa (1402\u20131472), was an important Humanist theoretician and designer whose book on architecture \"De re Aedificatoria\" was to have lasting effect. An aspect of Renaissance humanism was an emphasis of the anatomy of nature, in particular the human form, a science first studied by the Ancient Greeks. Humanism made man the measure of things. Alberti perceived the architect as a person with great social responsibilities.\nHe designed a number of buildings, but unlike Brunelleschi, he did not see himself as a builder in a practical sense and so left the supervision of the work to others. Miraculously, one of his greatest designs, that of the Basilica of Sant'Andrea, Mantua, was brought to completion with its character essentially intact. Not so the Church of San Francesco in Rimini, a rebuilding of a Gothic structure, which, like Sant'Andrea, was to have a fa\u00e7ade reminiscent of a Roman triumphal arch. This was left sadly incomplete.\nSant'Andrea is an extremely dynamic building both without and within. Its triumphal fa\u00e7ade is marked by extreme contrasts. The projection of the order of pilasters that define the architectural elements, but are essentially non-functional, is very shallow. This contrasts with the gaping deeply recessed arch which makes a huge portico before the main door. The size of this arch is in direct contrast to the two low square-topped openings that frame it. The light and shade play dramatically over the surface of the building because of the shallowness of its mouldings and the depth of its porch. In the interior Alberti has dispensed with the traditional nave and aisles. Instead there is a slow and majestic progression of alternating tall arches and low square doorways, repeating the \"triumphal arch\" motif of the fa\u00e7ade.\nTwo of Alberti's best known buildings are in Florence, the Palazzo Rucellai and at Santa Maria Novella. For the palace, Alberti applied the classical orders of columns to the fa\u00e7ade on the three levels, 1446\u20131451. At Santa Maria Novella he was commissioned to finish the decoration of the fa\u00e7ade. He completed the design in 1456 but the work was not finished until 1470.\nThe lower section of the building had Gothic niches and typical polychrome marble decoration. There was a large ocular window in the end of the nave which had to be taken into account. Alberti simply respected what was already in place, and the Florentine tradition for polychrome that was well established at the Baptistery of San Giovanni, the most revered building in the city. The decoration, being mainly polychrome marble, is mostly very flat in nature, but a sort of order is established by the regular compartments and the circular motifs which repeat the shape of the round window. For the first time, Alberti linked the lower roofs of the aisles to nave using two large scrolls. These were to become a standard Renaissance device for solving the problem of different roof heights and bridge the space between horizontal and vertical surfaces.\nHigh Renaissance.\nIn the late 15th century and early 16th century, architects such as Bramante, Antonio da Sangallo the Younger and others showed a mastery of the revived style and ability to apply it to buildings such as churches and city palazzo which were quite different from the structures of ancient times. The style became more decorated and ornamental, statuary, domes and cupolas becoming very evident. The architectural period is known as the \"High Renaissance\" and coincides with the age of Leonardo, Michelangelo and Raphael.\nBramante.\nDonato Bramante, (1444\u20131514), was born in Urbino and turned from painting to architecture, finding his first important patronage under Ludovico Sforza, Duke of Milan, for whom he produced a number of buildings over 20 years. After the fall of Milan to the French in 1499, Bramante travelled to Rome where he achieved great success under papal patronage.\nBramante's finest architectural achievement in Milan is his addition of crossing and choir to the abbey church of Santa Maria delle Grazie (Milan). This is a brick structure, the form of which owes much to the Northern Italian tradition of square domed baptisteries. The new building is almost centrally planned, except that, because of the site, the chancel extends further than the transept arms. The hemispherical dome, of approximately 20 metres across, rises up hidden inside an octagonal drum pierced at the upper level with arched classical openings. The whole exterior has delineated details decorated with the local terracotta ornamentation. From 1488 to 1492 he worked for Ascanio Sforza on Pavia Cathedral, on which he imposed a central plan scheme and built some apses and the crypt, inspired by the thermal baths of the Roman age.\nIn Rome Bramante created what has been described as \"a perfect architectural gem\", the Tempietto in the Cloister of San Pietro in Montorio. This small circular temple marks the spot where St Peter was martyred and is thus the most sacred site in Rome. The building adapts the style apparent in the remains of the Temple of Vesta, the most sacred site of Ancient Rome. It is enclosed by and in spatial contrast with the cloister which surrounds it. As approached from the cloister, as in the , it is seen framed by an arch and columns, the shape of which are echoed in its free-standing form.\nBramante went on to work on the Apostolic Palace, where he designed the Cortile del Belvedere. In 1506 his design for Pope Julius II's rebuilding of St. Peter's Basilica was selected, and the foundation stone laid. After Bramante's death and many changes of plan, Michelangelo, as chief architect, reverted to something closer to Bramante's original proposal.\nSangallo.\nAntonio da Sangallo the Younger (1485\u20131546) was one of a family of military engineers. His uncle, Giuliano da Sangallo was one of those who submitted a plan for the rebuilding of St Peter's and was briefly a co-director of the project, with Raphael.\nAntonio da Sangallo also submitted a plan for St Peter's and became the chief architect after the death of Raphael, to be succeeded himself by Michelangelo.\nHis fame does not rest upon his association with St Peter's but in his building of the Farnese Palace, \"the grandest palace of this period\", started in 1530. The impression of grandness lies in part in its sheer size, (56\u00a0m long by 29.5 meters high) and in its lofty location overlooking a broad piazza. Unusually for such a large and luxurious house of the time, it was built principally of stuccoed brick, rather than of stone. Against the smooth pink-washed walls the stone quoins of the corners, the massive rusticated portal and the repetition of finely detailed windows produce an elegant effect. The upper of the three equally sized floors was added by Michelangelo. The travertine for its architectural details came not from a quarry, but from the Colosseum.\nRaphael.\nRaphael (1483\u20131520), born in Urbino, trained under Perugino in Perugia before moving to Florence, was for a time the chief architect for St. Peter's, working in conjunction with Antonio Sangallo. He also designed a number of buildings, most of which were finished by others. His single most influential work is the Palazzo Pandolfini in Florence with its two stories of strongly articulated windows of a \"tabernacle\" type, each set around with ordered pilasters, cornice and alternate arched and triangular pediments.\nMannerism.\nMannerism in architecture was marked by widely diverging tendencies in the work of Michelangelo, Giulio Romano, Baldassare Peruzzi and Andrea Palladio, that led to the Baroque style in which the same architectural vocabulary was used for very different rhetoric.\nPeruzzi.\nBaldassare Peruzzi, (1481\u20131536), was an architect born in Siena, but working in Rome, whose work bridges the High Renaissance and the Mannerist period. His Villa Farnesina of 1509 is a very regular monumental cube of two equal stories, the bays being strongly articulated by orders of pilasters. The building is unusual for its frescoed walls.\nPeruzzi's most famous work is the Palazzo Massimo alle Colonne in Rome. The unusual features of this building are that its fa\u00e7ade curves gently around a curving street. It has in its ground floor a dark central portico running parallel to the street, but as a semi enclosed space, rather than an open loggia. Above this rise three undifferentiated floors, the upper two with identical small horizontal windows in thin flat frames which contrast strangely with the deep porch, which has served, from the time of its construction, as a refuge to the city's poor.\nGiulio Romano.\nGiulio Romano (1499\u20131546), was a pupil of Raphael, assisting him on various works for the Vatican. Romano was also a highly inventive designer, working for Federico II Gonzaga at Mantua on the Palazzo Te (1524\u20131534), a project which combined his skills as architect, sculptor and painter. In this work, incorporating garden grottoes and extensive frescoes, he uses illusionistic effects, surprising combinations of architectural form and texture, and the frequent use of features that seem somewhat disproportionate or out of alignment. The total effect is eerie and disturbing. Ilan Rachum cites Romano as \"one of the first promoters of Mannerism\".\nMichelangelo.\nMichelangelo Buonarroti (1475\u20131564) was one of the creative giants whose achievements mark the High Renaissance. He excelled in each of the fields of painting, sculpture and architecture, and his achievements brought about significant changes in each area. His architectural fame lies chiefly in two buildings: the interiors of the Laurentian Library and its lobby at the monastery of San Lorenzo in Florence, and St Peter's Basilica in Rome.\nSt. Peter's was \"the greatest creation of the Renaissance\", and a great number of architects contributed their skills to it. But at its completion, there was more of Michelangelo's design than of any other architect, before or after him.\nSt. Peter's.\nThe plan that was accepted at the laying of the foundation stone in 1506 was that by Bramante. Various changes in plan occurred in the series of architects that succeeded him, but Michelangelo, when he took over the project in 1546, reverted to Bramante's Greek-cross plan and redesigned the piers, the walls and the dome, giving the lower weight-bearing members massive proportions and eliminating the encircling aisles from the chancel and identical transept arms. Helen Gardner says: \"Michelangelo, with a few strokes of the pen, converted its snowflake complexity into a massive, cohesive unity.\"\nMichelangelo's dome was a masterpiece of design using two masonry shells, one within the other and crowned by a massive roof lantern supported, as at Florence, on ribs. For the exterior of the building he designed a giant order which defines every external bay, the whole lot being held together by a wide cornice which runs unbroken like a rippling ribbon around the entire building.\nThere is a wooden model of the dome, showing its outer shell as hemispherical. When Michelangelo died in 1564, the building had reached the height of the drum. The architect who succeeded Michelangelo was Giacomo della Porta. The dome, as built, has a much steeper projection than the dome of the model. It is generally presumed that it was della Porta who made this change to the design, to lessen the outward thrust. But, in fact it is unknown who it was that made this change, and it is equally possible and a stylistic likelihood that the person who decided upon the more dynamic outline was Michelangelo himself at some time during the years that he supervised the project.\nLaurentian Library.\nMichelangelo was at his most Mannerist in the design of the vestibule of the Laurentian Library, also built by him to house the Medici collection of books at the convent of San Lorenzo, Florence, the same San Lorenzo's at which Brunelleschi had recast church architecture into a Classical mold and established clear formula for the use of Classical orders and their various components.\nMichelangelo takes all Brunelleschi's components and bends them to his will. The Library is upstairs. It is a long low building with an ornate wooden ceiling, a matching floor and crowded with corrals finished by his successors to Michelangelo's design. But it is a light room, the natural lighting streaming through a long row of windows that appear positively crammed between the order of pilasters that march along the wall. The vestibule, on the other hand, is tall, taller than it is wide and is crowded by a large staircase that pours out of the library in what Nikolaus Pevsner refers to as a \"flow of lava\", and bursts in three directions when it meets the balustrade of the landing. It is an intimidating staircase, made all the more so because the rise of the stairs at the center is steeper than at the two sides, fitting only eight steps into the space of nine.\nThe space is crowded and it is to be expected that the wall spaces would be divided by pilasters of low projection. But Michelangelo has chosen to use paired columns, which, instead of standing out boldly from the wall, he has sunk deep into recesses within the wall itself. In the Basilica di San Lorenzo nearby, Brunelleschi used little scrolling console brackets to break the strongly horizontal line of the course above the arcade. Michelangelo has borrowed Brunelleschi's motifs and stood each pair of sunken columns on a pair of twin console brackets. Pevsner says the \"Laurenziana [...] reveals Mannerism in its most sublime architectural form\".\nGiacomo della Porta.\nGiacomo della Porta, (c.\u20091533\u20131602), was famous as the architect who made the dome of St. Peter's Basilica a reality. The change in outline between the dome as it appears in the model and the dome as it was built, has brought about speculation as to whether the changes originated with della Porta or with Michelangelo himself.\nDella Porta spent nearly all his working life in Rome, designing villas, palazzi and churches in the Mannerist style. One of his most famous works is the fa\u00e7ade of the Church of the Ges\u00f9, a project that he inherited from his teacher Jacopo Barozzi da Vignola. Most characteristics of the original design are maintained, subtly transformed to give more weight to the central section, where della Porta uses, among other motifs, a low triangular pediment overlaid on a segmental one above the main door. The upper storey and its pediment give the impression of compressing the lower one. The center section, like that of Sant'Andrea at Mantua, is based on the triumphal arch, but has two clear horizontal divisions like Santa Maria Novella. See Alberti above. The problem of linking the aisles to the nave is solved using Alberti's scrolls, in contrast to Vignola's solution which provided much smaller brackets and four statues to stand above the paired pilasters, visually weighing down the corners of the building. The influence of the design may be seen in Baroque churches throughout Europe.\nAndrea Palladio.\nAndrea Palladio, (1508\u201380), \"the most influential architect of the whole Renaissance\", was, as a stonemason, introduced to Humanism by the poet Giangiorgio Trissino. His first major architectural commission was the rebuilding of the Basilica Palladiana at Vicenza, in the Veneto where he was to work most of his life.\nPalladio was to transform the architectural style of both palaces and churches by taking a different perspective on the notion of Classicism. While the architects of Florence and Rome looked to structures like the Colosseum and the Arch of Constantine to provide formulae, Palladio looked to classical temples with their simple peristyle form. When he used the triumphal arch motif of a large arched opening with lower square-topped opening on either side, he invariably applied it on a small scale, such as windows, rather than on a large scale as Alberti used it at Sant'Andrea's. This Ancient Roman motif is often referred to as the Palladian Arch.\nThe best known of Palladio's domestic buildings is Villa Capra, otherwise known as \"La Rotonda\", a centrally planned house with a domed central hall and four identical fa\u00e7ades, each with a temple-like portico like that of the Pantheon, Rome. At the Villa Cornaro, the projecting portico of the north fa\u00e7ade and recessed loggia of the garden fa\u00e7ade are of two ordered stories, the upper forming a balcony.\nLike Alberti, della Porta and others, in the designing of a church fa\u00e7ade, Palladio was confronted by the problem of visually linking the aisles to the nave while maintaining and defining the structure of the building. Palladio's solution was entirely different from that employed by della Porta. At the church of San Giorgio Maggiore in Venice he overlays a tall temple, its columns raised on high plinths, over another low wide temple fa\u00e7ade, its columns rising from the basements and its narrow lintel and pilasters appearing behind the giant order of the central nave.\nProgression from Early Renaissance through to Baroque.\nIn Italy, there appears to be a seamless progression from Early Renaissance architecture through the High Renaissance and Mannerism to the Baroque style. Pevsner comments about the vestibule of the Laurentian Library that it \"has often been said that the motifs of the walls show Michelangelo as the father of the Baroque\".\nWhile continuity may be the case in Italy, it was not necessarily the case elsewhere. The adoption of the Renaissance style of architecture was slower in some areas than in others, as may be seen in England, for example. Indeed, as Pope Julius II was having the Old St. Peter's Basilica demolished to make way for the new, Henry VII of England was adding a glorious new chapel in the Perpendicular Gothic style to Westminster Abbey.\nLikewise, the style that was to become known as Baroque evolved in Italy in the early 17th century, at about the time that the first fully Renaissance buildings were constructed at Greenwich and Whitehall in England, after a prolonged period of experimentation with Classical motifs applied to local architectural forms, or conversely, the adoption of Renaissance structural forms in the broadest sense with an absence of the formulae that governed their use. While the English were just discovering what the rules of Classicism were, the Italians were experimenting with methods of breaking them. In England, following the Restoration of the Monarchy in 1660, the architectural climate changed, and taste moved in the direction of the Baroque. Rather than evolving, as it did in Italy, it arrived fully fledged.\nIn a similar way, in many parts of Europe that had few purely classical and ordered buildings like Brunelleschi's Santo Spirito and Michelozzo's Medici Riccardi Palace, Baroque architecture appeared almost unheralded, on the heels of a sort of Proto-Renaissance local style. The spread of the Baroque and its replacement of traditional and more conservative Renaissance architecture was particularly apparent in the building of churches as part of the Counter Reformation.\nSpread in Europe.\nThe 16th century saw the economic and political ascendancy of France, Spain and Portugal, then later the rise of England, Poland and Russia and the Dutch Republic. The result was that these places began to import the Renaissance style as indicators of their new cultural position. This also meant that it was not until about 1500 and later that signs of Renaissance architectural style began to appear outside Italy.\nThough Italian architects were highly sought after, such as Sebastiano Serlio in France, Aristotile Fioravanti in Russia, and Francesco Fiorentino in Poland, soon, non-Italians were studying Italian architecture and translating it into their own idiom. These included Philibert de l'Orme (1510\u20131570) in France, Juan Bautista de Toledo (died: 1567) in Spain, Inigo Jones (1573\u20131652) in England and Elias Holl (1573\u20131646) in Germany.\nBooks or ornament prints with engraved illustrations demonstrating plans and ornament were very important in spreading Renaissance styles in Northern Europe, with among the most important authors being Androuet du Cerceau in France, and Hans Vredeman de Vries in the Netherlands, and Wendel Dietterlin, author of \"Architectura\" (1593\u201394) in Germany.\nBaltic region.\nThe Renaissance arrived late in what is today Estonia, Latvia and Lithuania, the so-called Baltic States, and did not make a great imprint architecturally. It was a politically tumultuous time, marked by the decline of the State of the Teutonic Order and the Livonian War.\nIn Estonia, artistic influences came from Dutch, Swedish and Polish sources. The building of the Brotherhood of the Blackheads in Tallinn with a fa\u00e7ade designed by Arent Passer, is the only truly Renaissance building in the country that has survived more or less intact. Significantly for these troubled times, the only other examples are purely military buildings, such as the \"Fat Margaret\" cannon tower, also in Tallinn.\nLatvian Renaissance architecture was influenced by Polish-Lithuanian and Dutch style, with Mannerism following from Gothic without intermediaries. St. John's Church in the Latvian capital of Riga is an example of an earlier Gothic church which was reconstructed in 1587\u201389 by the Dutch architect Gert Freze (Joris Phraeze). The prime example of Renaissance architecture in Latvia is the heavily decorated House of the Blackheads, rebuilt from an earlier Medieval structure into its present Mannerist forms as late as 1619\u201325 by the architects A. and L. Jansen. It was destroyed during World War II and rebuilt during the 1990s.\nLithuania meanwhile formed a large dual state with Poland, known as the Polish\u2013Lithuanian Commonwealth. Renaissance influences grew stronger during the reign of Sigismund I the Old and Sigismund II Augustus. The Palace of the Grand Dukes of Lithuania (destroyed in 1801, a copy built in 2002\u20132009) show Italian influences. Several architects of Italian origin were active in the country, including Bernardino Zanobi de Gianotis, Giovanni Cini and Giovanni Maria Mosca.\nBohemia.\nThe Renaissance style first appeared in the Crown of Bohemia in the 1490s. Bohemia together with its incorporated lands, especially Moravia, thus ranked among the areas of the Holy Roman Empire with the earliest known examples of the Renaissance architecture.\nThe lands of the Bohemian Crown were never part of the ancient Roman Empire, thus they missed their own ancient classical heritage and had to be dependent on the primarily Italian models. As well as in other Central European countries the Gothic style kept its position especially in the church architecture. The traditional Gothic architecture was considered timeless and therefore able to express the sacredness. The Renaissance architecture coexisted with the Gothic style in Bohemia and Moravia until the late 16th century (e. g. the residential part of a palace was built in the modern Renaissance style but its chapel was designed with Gothic elements). The fa\u00e7ades of Czech Renaissance buildings were often decorated with sgraffito (figural or ornamental).\nDuring the reign of Rudolph II, Holy Roman Emperor and Bohemian king, the city of Prague became one of the most important European centers of the late Renaissance art (so-called Mannerism). Nevertheless, not many architecturally significant buildings have been preserved from that time.\nCroatia.\nIn the 15th century, Croatia was divided into three states: the northern and central part of Croatia and Slavonia were in union with the Kingdom of Hungary, while Dalmatia, with the exception of the independent Republic of Ragusa, was under the rule of the Venetian Republic. The Cathedral of St James in \u0160ibenik, was begun in 1441 in the Gothic style by Giorgio da Sebenico \"(Juraj Dalmatinac)\". Its unusual construction does not use mortar, the stone blocks, pilasters and ribs being bonded with joints and slots in the way that was usual in wooden constructions. In 1477 the work was unfinished, and continued under Niccol\u00f2 di Giovanni Fiorentino, who respected the mode of construction and the plan of the former architect, but continued the work which includes the upper windows, the vaults and the dome, in the Renaissance style. The combination of a high barrel vault with lower half-barrel vaults over the aisles the gives the fa\u00e7ade its distinctive trefoil shape, the first of this type in the region. The cathedral was listed as a UNESCO World Heritage List in 2001.\nEngland.\nAfter some first efforts by kings and courtiers, most now vanished, like Henry VII's Richmond Palace (c.\u20091501), Henry VIII's Nonsuch Palace, and the first Somerset House in London, a local style of Renaissance architecture emerged in England during the reign of Elizabeth I, much influenced by the Low countries where among other features it acquired versions of the Dutch gable, and Flemish strapwork in geometric designs adorning the walls. The new style tended to manifest itself in large square tall prodigy houses such as Longleat House.\nThe first great exponent of classicizing Italian Renaissance architecture in England was Inigo Jones (1573\u20131652), who had studied architecture in Italy where the influence of Palladio was very strong. Jones returned to England full of enthusiasm for the new movement and immediately began to design such buildings as the Queen's House at Greenwich in 1616 and the Banqueting House, Whitehall three years later. These works, with their clean lines, and symmetry were revolutionary in a country still enamoured with mullion windows, crenellations and turrets.\nFrance.\nDuring the early years of the 16th century the French were involved in wars in northern Italy, bringing back to France not just the Renaissance art treasures as their war booty, but also stylistic ideas. In the Loire Valley a wave of building was carried and many Renaissance ch\u00e2teaux appeared at this time, the earliest example being the Ch\u00e2teau d'Amboise (c.\u20091495) in which Leonardo da Vinci spent his last years. The style became dominant under Francis I (See Ch\u00e2teaux of the Loire Valley).\nGermany.\nThe Renaissance in Germany was inspired first by German philosophers and artists such as Albrecht D\u00fcrer and Johannes Reuchlin who visited Italy. Important early examples of this period are especially the Landshut Residence, Heidelberg Castle, Johannisburg Palace in Aschaffenburg, Schloss Weilburg, the City Hall and Fugger Houses in Augsburg and St. Michael's Church, Munich. A particular form of Renaissance architecture in Germany is the Weser Renaissance, with prominent examples such as Bremen City Hall and the Juleum in Helmstedt.\nIn July 1567 the city council of Cologne approved a design in the Renaissance style by Wilhelm Vernukken for a two storied loggia for Cologne City Hall. St Michael in Munich is the largest Renaissance church north of the Alps. It was built by William V, Duke of Bavaria between 1583 and 1597 as a spiritual center for the Counter Reformation and was inspired by the Church of the Ges\u00f9 in Rome. The architect is unknown. Many examples of Brick Renaissance buildings can be found in Hanseatic old towns, such as Stralsund, Wismar, L\u00fcbeck, L\u00fcneburg, Friedrichstadt and Stade. Notable German Renaissance architects include Friedrich Sustris, Benedikt Rejt, Abraham van den Blocke, Elias Holl and Hans Krumpper.\nHungary.\nOne of the earliest places to be influenced by the Renaissance style of architecture was the Kingdom of Hungary. The style appeared following the marriage of King Matthias Corvinus and Beatrice of Naples in 1476. Many Italian artists, craftsmen and masons arrived at Buda with the new queen. Important remains of the Early Renaissance summer palace of King Matthias can be found in Visegr\u00e1d. The Ottoman conquest of Hungary after 1526 cut short the development of Renaissance architecture in the country and destroyed its most famous examples. Today, the only completely preserved work of Hungarian Renaissance architecture is the Bak\u00f3cz Chapel (commissioned by the Hungarian cardinal Tam\u00e1s Bak\u00f3cz), now part of the Esztergom Basilica.\nHabsburg Netherlands.\nAs in painting, Renaissance architecture took some time to reach the Habsburg Netherlands and did not entirely supplant the Gothic elements. An architect directly influenced by the Italian masters was Cornelis Floris de Vriendt, who designed Antwerp City Hall, finished in 1564. The style is sometimes called the Flemish-Italian Renaissance style and is also known as the Floris style. In this style the overall structure was similar to that of late-Gothic buildings, but with larger windows and much florid decoration and detailing in the Renaissance styles. This style became widely influential across Northern Europe, for example in Elizabethan architecture, and is part of the wider movement of Northern Mannerism.\nDutch Republic.\nIn the early 17th century Dutch Republic, Hendrick de Keyser played an important role in developing the \"Amsterdam Renaissance\" style, which has local characteristics including the prevalence of tall narrow town-houses, the \"trapgevel\" or Dutch gable and the employment of decorative triangular pediments over doors and windows in which the apex rises much more steeply than in most other Renaissance architecture, but in keeping with the profile of the gable. Carved stone details are often of low profile, in strapwork resembling leatherwork, a stylistic feature originating in the School of Fontainebleau. This feature was exported to England.\nPoland.\nPolish Renaissance architecture is divided into three periods:\nThe first period (1500\u201350) is the so-called \"Italian\" as most of Renaissance buildings of this time were designed by Italian architects, mainly from Florence, including Francesco Fiorentino and Bartolomeo Berrecci. Renowned architects from Southern Europe became sought-after during the reign of Sigismund I the Old and his Italian-born wife, Queen Bona Sforza. Notable examples from this period include Wawel Castle Courtyard and Sigismund's Chapel.\nIn the second period (1550\u20131600), Renaissance architecture became more common, with the beginnings of Mannerist and under the influence of the Netherlands, particularly in northern Poland and Pomerania, but also in parts of Lesser Poland. Buildings of this kind include the Cloth Hall in Krak\u00f3w and city halls of Tarn\u00f3w and Sandomierz. The most famous example is the 16th-century Pozna\u0144 Town Hall, designed by Giovanni Battista di Quadro.\nIn the third period (1600\u201350), the rising power of sponsored Jesuits and Counter Reformation gave impetus to the development of Mannerist architecture and Baroque. Most notable example of this period is Kalwaria Zebrzydowska park, mannerist architectural and park landscape complex and pilgrimage park, which consists Basilica of St. Mary and 42 chapels modelled and named after the places in Jerusalem and Holy Land. This is a UNESCO World Heritage Site. Another great example from this period is Krasiczyn Castle, which is an palazzo in fortezza with a unique sgraffito wall decorations, whose total area is about 7000 square meters.\nPortugal.\nThe adoption of the Renaissance style in Portugal was gradual. The so-called Manueline style (c.\u20091490\u20131535) married Renaissance elements to Gothic structures with the superficial application of exuberant ornament similar to the Isabelline Gothic of Spain. Examples of Manueline include the Bel\u00e9m Tower, a defensive building of Gothic form decorated with Renaissance-style loggias, and the Jer\u00f3nimos Monastery, with Renaissance ornaments decorating portals, columns and cloisters.\nThe first \"pure\" Renaissance structures appear under King John III, like the Chapel of Nossa Senhora da Concei\u00e7\u00e3o in Tomar (1532\u201340), the \"Porta Especiosa\" of Coimbra Cathedral and the Church of Nossa Senhora da Gra\u00e7a (\u00c9vora) (c.\u20091530\u20131540), as well as the cloisters of Viseu Cathedral (c.\u20091528\u20131534) and Convent of Christ in Tomar (John III Cloisters, 1557\u20131591). The Lisbon buildings of S\u00e3o Roque Church (1565\u201387) and the Mannerist Monastery of S\u00e3o Vicente de Fora (1582\u20131629), strongly influenced religious architecture in both Portugal and its colonies in the next centuries.\nRussia.\nPrince Ivan III introduced Renaissance architecture to Russia by inviting a number of architects from Italy, who brought new construction techniques and some Renaissance style elements with them, while in general following the traditional designs of the Russian architecture. In 1475 the Bolognese architect Aristotele Fioravanti came to rebuild the Cathedral of the Dormition in the Moscow Kremlin, damaged in an earthquake. Fioravanti was given the 12th-century Assumption Cathedral in Vladimir as a model, and produced a design combining traditional Russian style with a Renaissance sense of spaciousness, proportion and symmetry.\nIn 1485, Ivan III commissioned the building of a royal Terem Palace within the Kremlin, with Aloisio da Milano being the architect of the first three floors. Aloisio da Milano, as well as the other Italian architects, also greatly contributed to the construction of the Kremlin walls and towers. The small banqueting hall of the Russian Tsars, called the Palace of Facets because of its facetted upper story, is the work of two Italians, Marco Ruffo and Pietro Solario, and shows a more Italian style.\nIn 1505, an Italian known in Russia as Aleviz Novyi built twelve churches for Ivan III, including the Cathedral of the Archangel, a building remarkable for the successful blending of Russian tradition, Orthodox requirements and Renaissance style.\nScandinavia.\nThe Renaissance architecture that found its way to Scandinavia was influenced by the Flemish architecture, and included high gables and a castle air as demonstrated in the architecture of Frederiksborg Palace. Consequently, much of the Neo-Renaissance to be found in the Scandinavian countries is derived from this source.\nIn Denmark, Renaissance architecture thrived during the reigns of Frederick II and especially Christian IV. Inspired by the French castles of the times, Flemish architects designed masterpieces such as Kronborg Castle in Helsing\u00f8r and Frederiksborg Castle in Hiller\u00f8d. The Frederiksborg Castle (1602\u20131620) is the largest Renaissance palace in Scandinavia.\nElsewhere in Sweden, with Gustav Vasa's seizure of power and the onset of the Protestant reformation, church construction and aristocratic building projects came to a near standstill. During this time period, several magnificent so-called \"Vasa castles\" appeared. They were erected at strategic locations to control the country as well as to accommodate the travelling royal court. Gripsholm Castle, Kalmar Castle and Vadstena Castle are known for their fusion of medieval elements with Renaissance architecture.\nThe architecture of Norway was influenced partly by the occurrence of the plague during the Renaissance era. After the Black Death, monumental construction in Norway came to a standstill. There are few examples of Renaissance architecture in Norway, the most prominent being renovations to the medieval Rosenkrantz Tower in Bergen, Barony Rosendal in Hardanger, and the contemporary Austrat manor near Trondheim, and parts of Akershus Fortress.\nThere is little evidence of Renaissance influence in Finnish architecture.\nSpain.\nIn Spain, Renaissance began to be grafted to Gothic forms in the last decades of the 15th century. The new style is called Plateresque, because of the extremely decorated fa\u00e7ade, that brought to the mind the decorative motifs of the intricately detailed work of silversmiths, the \"Plateros\". Classical orders and candelabra motifs (\"a candelieri\") combined freely. As decades passed, the Gothic influence disappeared and the research of an orthodox classicism reached high levels. Although Plateresco is a commonly used term to define most of the architectural production of the late 15th and first half of 16th century, some architects acquired a more sober personal style, like Diego Siloe, and Andr\u00e9s de Vandelvira in Andalusia, and Alonso de Covarrubias and Rodrigo Gil de Honta\u00f1\u00f3n in Castile. This phase of Spanish Renaissance is called Purism. From the mid-sixteenth century, under such architects as Pedro Machuca, Juan Bautista de Toledo and Juan de Herrera there was a closer adherence to the art of ancient Rome, sometimes anticipating Mannerism, examples of which include the palace of Charles V in Granada and El Escorial. This \"Herrerian style\" or \"arquitectura herreriana\" of architecture was developed during the last third of the 16th century under the reign of Philip II (1556\u20131598), and continued in force in the 17th century, but transformed by the Baroque style of the time. \nSpread in the Colonial Americas.\nBolivia.\nRenaissance architecture spread to Colonial Bolivia, with examples being the Church of Curahuara de Carangas built between 1587 and 1608 known as the \"Sistine Chapel of the Andes\" by the Bolivians for its rich Mannerist decoration in its interior; and the Basilica of Our Lady of Copacabana built between 1601 and 1619 designed by the Spanish architect Francisco Jim\u00e9nez de Siguenza.\nBrazil.\nThe best-known examples of the Renaissance architecture in Colonial Brazil are the Mannerist Cathedral Basilica of Salvador built between 1657 and 1746 and the Franciscan Convent of Santo Ant\u00f4nio in Jo\u00e3o Pessoa built between 1634 and 1779.\nDominican Republic.\nThe House of the Five Medallions is a historic house built in 1540, located in Santo Domingo, this preserves a Plateresque Renaissance fa\u00e7ade.\nEcuador.\nThe large Basilica and Convent of San Francisco, Quito, built between 1535 and 1650, is of Mannerist Renaissance style.\nMexico.\nA notable example of Renaissance architecture in New Spain is the Cathedral of M\u00e9rida, Yucat\u00e1n, one of the oldest cathedrals in the Americas, built between 1562 and 1598 and designed by Pedro de Aulestia and Juan Miguel de Ag\u00fcero.\nPeru.\nSeveral of the churches of the city of Cusco were begun during the Renaissance period, including Cusco Cathedral, (1539). Many others are Baroque in style.\nLegacy.\nMany styles of Late Renaissance and Mannerist architecture transitioned fairly easily in local styles of Baroque architecture; in other areas the change was more abrupt. Baroque and Neoclassical architecture dominated the later 17th and the 18th century in most areas, and persisted well into the 19th century in many places and individual buildings.\nDuring the 19th century there was a conscious revival of the style in Renaissance Revival architecture, that paralleled the Gothic Revival. Whereas the Gothic style was perceived by architectural theorists as being the most appropriate style for Church building, the Renaissance palazzo was a good model for urban secular buildings requiring an appearance of dignity and reliability such as banks, gentlemen's clubs and apartment blocks. Buildings that sought to impress, such as the Palais Garnier, were often of a more Mannerist or Baroque style. Architects of factories, office blocks and department stores continued to use the Renaissance palazzo form into the 20th century, in Mediterranean Revival Style architecture with an Italian Renaissance emphasis.\nMany of the concepts and forms of Renaissance architecture can be traced through subsequent architectural movements\u2014from Renaissance to High-Renaissance, to Mannerism, to Baroque (or Rococo), to Neo-Classicism, and to Eclecticism. While Renaissance style and motifs were largely purged from Modernism, they have been reasserted in some Postmodern architecture. The influence of Renaissance architecture can still be seen in many of the modern styles and rules of architecture today.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nBibliography.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "41525", "revid": "125972", "url": "https://en.wikipedia.org/wiki?curid=41525", "title": "Amadeus (play)", "text": "1979 stage play by Peter Shaffer\nAmadeus is a play by Peter Shaffer which gives a fictional account of the lives of composers Wolfgang Amadeus Mozart and Antonio Salieri, imagining a rivalry between the two at the court of Joseph II, Holy Roman Emperor. First performed in 1979, it was inspired by Alexander Pushkin's short 1830 play \"Mozart and Salieri\", which Nikolai Rimsky-Korsakov used in 1897 as the libretto for an opera of the same name.\nThe play makes significant use of the music of Mozart, Salieri and other composers of the period. The premieres of Mozart's operas \"The Abduction from the Seraglio\", \"The Marriage of Figaro,\" \"Don Giovanni\", and \"The Magic Flute\" are the settings for key scenes. It was presented at the Royal National Theatre, London in 1979, then moved to Her Majesty's Theatre in the West End followed by a Broadway production. It won the 1981 Tony Award for Best Play and Shaffer adapted it for the much acclaimed 1984 film of the same name.\nPlot.\n\"Since the play's original run, Shaffer extensively revised his play, including changes to plot details; the following is common to all revisions.\"\nThe composer Salieri is an old man, having long outlived his fame. Speaking directly to the audience, he claims to have used poison to assassinate Mozart and promises to explain himself. The action then flashes back to the eighteenth century, at a time when Salieri, then the court composer of the Austrian emperor, has not met Mozart but has heard of him and his music. He adores Mozart's compositions and is thrilled at the chance to meet him, during a salon honouring his patron. But when he finally catches sight of Mozart, he is deeply disappointed to find the composer lacking the grace and charm of his compositions. Mozart is crawling around on his hands and knees, engaging in profane talk with his future bride Constanze Weber.\nSalieri cannot reconcile Mozart's boorish behaviour with the genius that God has inexplicably bestowed upon him. A devout Catholic all his life, Salieri cannot believe that God would choose Mozart over him for such a gift. Salieri renounces God and vows to do everything in his power to destroy Mozart as a way of retaliating against his Creator. Salieri pretends to be Mozart's ally to his face while doing his utmost to destroy his reputation and any success his compositions may have. Mozart's own fortunes are not helped due to his vulgar nature. On more than one occasion, only the intervention of Emperor Joseph II allows Mozart to continue (interventions which Salieri opposes and then is all too happy to take credit for when Mozart assumes it was \"he\" who intervened). Salieri humiliates Constanze by forcing her to strip naked in front of him when she comes to him for help. He smears Mozart's character with the Imperial Court, ruining many opportunities for the composer.\nA major theme in \"Amadeus\" is Mozart's repeated attempts to win the acceptance of Vienna's aristocracy with increasingly brilliant compositions, which are always frustrated either by Salieri's machinations or because the aristocrats cannot appreciate Mozart's innovations and willingness to challenge the accepted mores of classical music. \nTowards the end of the play, with his life completely ruined, Mozart is visited one last time by Salieri, who reveals he has been Mozart's enemy all along. After hearing this, Mozart breaks down and is left totally defeated, singing a nursery tune. He later dies in Constanze's arms.\nSalieri recounts on how he was praised throughout Europe for more than 30 years, after which audiences began to turn away from his music and toward that of Mozart, as the world finally comes to recognize his true genius. Salieri then reveals that he left a false confession of having murdered Mozart with arsenic in a last attempt to be remembered, and attempts to commit suicide with a razor. He survives and his confession is met with disbelief and eventually rejected. Defeated, Salieri, doomed to live the rest of his life in obscurity and failure, absolves the audience of their mediocrity.\nBackground and production.\nThe play used as incidental music mainly works by Mozart, for which in the first stage production arrangements were made by Harrison Birtwistle; the only piece included of Salieri being a \u201cbanal greeting march\u201d on which Mozart extemporises mockingly to produce \"Non pi\u00f9 andrai\" (the aria which closes Act 1 of \"Le nozze di Figaro)\". Nicholas Kenyon argues that the play (and film) helped to rekindle interest in Salieri's music and increase performances of his operas.\nHistorical accuracy.\nShaffer used artistic licence in his portrayals of Mozart and Salieri. Documentary evidence suggests that there may have been some occasional antipathy between the two men but the idea that Salieri was the instigator of Mozart's demise is not taken seriously by scholars of the men's lives and careers. In fact, there is evidence that they enjoyed a relationship marked by mutual respect. As an example, Salieri later tutored Mozart's son Franz in music. He probably conducted some of Mozart's works.\nWriter David Cairns called \"Amadeus\" \"myth-mongering\" and argued against Shaffer's portrait of Mozart as \"two contradictory beings, sublime artist and fool\", positing instead that Mozart was \"fundamentally well-integrated\". Cairns also rejects the \"romantic legend\" that Mozart always wrote out perfect manuscripts of works already completely composed in his head, citing major and prolonged revisions to several manuscripts (see: Mozart's compositional method). Mozart scholar H. C. Robbins Landon commented that \"it may prove difficult to dissuade the public from the current Schafferian view of the composer as a divinely gifted drunken lout, pursued by a vengeful Salieri. By the same token, Constanze Mozart, she (in the film) of the extraordinary d\u00e9collet\u00e9 and fatuous giggle, needs to be rescued from Schaffer's view of her\".\nNotable productions.\n\"Amadeus\" was first presented at the National Theatre, London in 1979, directed by Peter Hall and starring Paul Scofield as Salieri, Simon Callow as Mozart and Felicity Kendal as Constanze. (Callow appeared in the film version in a different role.) It was later transferred in modified form to Her Majesty's Theatre in the West End, starring Frank Finlay as Salieri. The cast also included Andrew Cruickshank (Rosenberg), Basil Henson (von Strack), Philip Locke (Greybig), John Normington (Joseph II) and Nicholas Selby (van Swieten).\nThe play premiered on Broadway on 11 December 1980 at the Broadhurst Theatre, with Ian McKellen as Salieri, Tim Curry as Mozart and Jane Seymour as Constanze. It ran for 1,181 performances, closing on 16 October 1983 and was nominated for seven Tony Awards (Best Actor for both McKellen and Curry, Best Director for Peter Hall, Best Play, Best Costume Design, Lighting and Set Design for John Bury), of which it won five (including Best Play and Best Actor for McKellen). In 2015, Curry stated in an interview that the original Broadway production was the favourite stage production that he had ever been in. During the run of the play McKellen was replaced by John Wood, Frank Langella, David Dukes, David Birney, John Horton and Daniel Davis. Curry was replaced by Peter Firth, Peter Crook, Dennis Boutsikaris, John Pankow, Mark Hamill and John Thomas Waite. Also playing Constanze were Amy Irving, Suzanne Lederer, Michele Farr, Caris Corfman and Maureen Moore.\nIn June 1981, Roman Polanski directed and co-starred (as Mozart) in a stage production of the play, first in Warsaw (with Tadeusz \u0141omnicki as Salieri), then at the Th\u00e9\u00e2tre Marigny in Paris with Fran\u00e7ois P\u00e9rier as Salieri. The play was again directed by Polanski, in Milan, in 1999.\nIn 1982, Richard Wherrett directed a Sydney Theatre Company production at the Theatre Royal Sydney. It starred John Gaden as Salieri, Drew Forsythe as Mozart and Linda Cropper as Constanze, with Lyn Collingwood as Mrs Salieri and Robert Hughes as Venticello II. It ran from 6 April to 29 May 1982. Adam Redfield (as Mozart) and Terry Finn (as Constanze) appeared in the 1984 Virginia Stage Company production, at the Wells Theatre in Norfolk, Virginia, directed by Charles Towers.\nThe play was revived in 1998 at the Old Vic Theatre in London, directed again by Peter Hall and produced by Kim Poster. Starring in the play were Michael Sheen as Mozart, David Suchet as Salieri, Cindy Katz as Constanze and David McCallum as Joseph II. The play subsequently transferred to the Music Box Theatre, New York City, where it ran for 173 performances (15 December 1999 until 14 May 2000), and received Tony Award nominations for Best Revival and Best Actor in a Play (for Suchet's Salieri).\nIn July 2006, the Los Angeles Philharmonic presented a production of portions from the latest revision of the play at the Hollywood Bowl. Neil Patrick Harris starred as Mozart, Kimberly Williams-Paisley as Constanze Mozart, and Michael York as Salieri. Leonard Slatkin conducted the Philharmonic Orchestra. Rupert Everett played Salieri in a production at the refurbished Chichester Festival Theatre from 12 July through 2 August 2014. The cast included Joshua McGuire as Mozart, Jessie Buckley as Constanze and John Standing as Count Orsini-Rosenberg. Simon Jones played Joseph II. Peter Shaffer attended the play at the closing performance.\nThe play was revived at the National Theatre in London in a new production directed by Michael Longhurst, from October 2016 to March 2017. It starred Lucian Msamati as Salieri alongside Adam Gillen as Mozart, Karla Crome as Constanze, Hugh Sachs as Count Orsini-Rosenberg and Tom Edden as Joseph II, accompanied with a live orchestra by the Southbank Sinfonia. The production sold out with rave reviews and returned to the Olivier Theatre at the NT with Msamati and Gillen reprising the roles of Salieri and Mozart from February to 24 April 2018, again with rave reviews.\nThe play was performed at the Estates Theatre, where \"Don Giovanni\" made its premier in 1787, and where part of the 1984 film was shot, in 2017 for the first time in English in the Czech Republic, directed by Guy Roberts. \"Amadeus\" was directed by Javad Molania in Tehran in March 2018 at Hafez Hall. The play was directed by I\u015f\u0131l Kasapo\u011flu in Turkey in January/February 2020 at Uniq Hall Theatre, Istanbul. A new production, scheduled for December 2022 at the Sydney Opera House, was announced in July 2022 with Michael Sheen as Salieri and Rahel Romahn as Mozart. Sheen received Best Performer in a Play at the 2023 BroadwayWorld Australia - Sydney Awards for his Salieri.\nIn other media.\nRadio.\nIn 1983, BBC Radio 3 aired an audio version directed by Peter Hall which starred the original cast of his National Theatre production. The cast included:\nThis radio production was re-broadcast on 2 January 2011 as part of Radio 3's \"Genius of Mozart\" season. To celebrate Mozart's 250th birthday in 2006, BBC Radio 2 broadcast an adaptation by Neville Teller of Shaffer's play in eight fifteen-minute episodes directed by Peter Leslie Wilde and narrated by F. Murray Abraham as Salieri. This version was re-broadcast 24 May \u2013 2 June 2010 on BBC Radio 7.\nFilm.\nThe 1984 film adaptation won the Academy Award for Best Picture. In total, the film won eight Academy Awards. It starred F. Murray Abraham as Salieri (winning the Oscar for Best Actor for his performance), Tom Hulce as Mozart (also nominated for Best Actor) and Elizabeth Berridge as Constanze. The play was thoroughly reworked by Shaffer and the film's director, Milo\u0161 Forman, with scenes and characters not found in the play. While the focus of the play is primarily on Salieri, the film goes further into developing the characters of both composers.\nTelevision series.\nIn November 2022, it was announced that Joe Barton would be adapting \"Amadeus\" into a television series for Sky. It will be directed by Julian Farino and Alice Seabright. On February 20, 2024, it was announced Will Sharpe would play Mozart, with Paul Bettany announced as Salieri, and Gabrielle Creevy as Constanze.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41526", "revid": "444206", "url": "https://en.wikipedia.org/wiki?curid=41526", "title": "Luciferians", "text": ""}
{"id": "41527", "revid": "11104410", "url": "https://en.wikipedia.org/wiki?curid=41527", "title": "Contrapposto", "text": "Sculptural disposition of human figure\n ( 'counterpoise'), in the visual arts, is a human figure standing with most of its weight on one foot, so that its shoulders and arms twist off-axis from the hips and legs in the axial plane.\nFirst appearing in Ancient Greece in the early 5th century BCE, \"contrapposto\" is considered a crucial development in the history of Ancient Greek art (and, by extension, Western art), as it marks the first time in Western art that the human body is used to express a psychological disposition. The style was further developed and popularized by sculptors in the Hellenistic and Imperial Roman periods, fell out of use in the Middle Ages, and was later revived during the Renaissance. Michelangelo's statue of \"David\", one of the most iconic sculptures in the world, is a famous example of \"contrapposto\".\nDefinition.\n\"Contrapposto\" was historically an important sculptural development, for its appearance marks the first time in Western art that the human body is used to express a more relaxed psychological disposition. This gives the figure a more dynamic, or alternatively relaxed appearance. In the frontal plane this also results in opposite levels of shoulders and hips, for example: if the right hip is higher than the left; correspondingly the right shoulder will be lower than the left, and vice versa. It can further encompass the tension as a figure changes from resting on a given leg to walking or running upon it (so-called \"ponderation\"). The leg that carries the weight of the body is known as the \"engaged\" leg, the relaxed leg is known as the \"free\" leg. Usually, the \"engaged\" leg is straight, or very slightly bent, and the \"free\" leg is slightly bent. \"Contrapposto\" is less emphasized than the more sinuous S-curve, and creates the illusion of past and future movement. A 2019 eye tracking study, by showing that \"contrapposto\" acts as supernormal stimulus and increases perceived attractiveness, has provided evidence and insight as to why, in artistic presentation, goddesses of beauty and love are often depicted in \"contrapposto\" pose. This was later supported in a neuroimaging study. The term \"contrapposto\" can also be used to refer to multiple figures which are in counter-pose (or opposite pose) to one another.\nHistory.\nClassical.\nPrior to the introduction of \"contrapposto\", the statues that dominated ancient Greece were the archaic kouros (male) and the kore (female). The first known statue to use \"contrapposto\" is \"Kritios Boy\", c. 480 BCE, so called because it was once attributed to the sculptor Kritios. It is possible, even likely, that earlier bronze statues had used the technique, but if they did, they have not survived and Kenneth Clark called the statue \"the first beautiful nude in art\". The statue is a Greek marble original and not a Roman copy.\nAccording to the \"canon\" of the Classical Greek sculptor Polykleitos in the 4th century BCE, \"contrapposto\" is one of the most important characteristics of his figurative works and those of his successors, Lysippos, Skopas, etc. The Polykletian statues (\"Discophoros\" (\"discus-bearer\") and \"Doryphoros\" (\"spear-bearer\"), for example) are idealized athletic young men with the divine sense, and captured in \"contrapposto\". In these works, the pelvis is no longer axial with the vertical kourous archaic style of earlier Greek sculpture before \"Kritios Boy\".\n\"Contrapposto\" can be clearly seen in the Roman copies of the statues of Hermes and Heracles. A famous example is the marble statue of \"Hermes and the Infant Dionysus\" in Olympia by Praxiteles. It can also be seen in the Roman copies of Polyclitus's \"Amazon\".\nGreek art emphasized humanism along with the human mind and the human body's beauty. Greek youths trained and competed in athletic contests in the nude. A great contribution to the \"contrapposto\" pose was the concept of a canon of proportions, in which mathematical properties are used to create proportions.\nRenaissance.\nClassical \"contrapposto\" was revived in Renaissance art by the Italian artists Donatello and Leonardo da Vinci, followed by Michelangelo, Raphael and other artists of the High Renaissance. One of the achievements of the Italian Renaissance was the re-discovery of \"contrapposto\".\nModern times.\nThe technique continues to be widely employed in sculpture. Modern psychological research confirms the attractiveness of the pose.\nReferences and sources.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41528", "revid": "26248", "url": "https://en.wikipedia.org/wiki?curid=41528", "title": "Forrest Gump", "text": "1994 American film by Robert Zemeckis\nForrest Gump is a 1994 American comedy-drama film directed by Robert Zemeckis. An adaptation of the 1986 novel by Winston Groom, the film's screenplay was written by Eric Roth. It stars Tom Hanks in the title role, alongside Robin Wright, Gary Sinise, Mykelti Williamson, and Sally Field in lead roles. The film follows the life of an Alabama man named Forrest Gump (Hanks) and his experiences in the 20th-century United States.\nPrincipal photography took place between August and December 1993, mainly in Georgia, North Carolina, and South Carolina. Extensive visual effects were used to incorporate Hanks into archived footage and to develop other scenes. The soundtrack features songs reflecting the different periods seen in the film. Various interpretations have been made of the protagonist and the film's political symbolism.\n\"Forrest Gump\" was released in the United States on July 6, 1994, and received widespread critical acclaim for Zemeckis's direction, the performances (particularly those of Hanks and Sinise), story, writing, emotional weight, visual effects, music, character development and screenplay. The film was a major success at the box office: it became the top-grossing film in the United States released that year and earned over US$ worldwide during its theatrical run, making it the second-highest-grossing film of 1994, behind \"The Lion King\". The soundtrack sold over 12 million copies. \"Forrest Gump\" is often regarded as one of the greatest and most influential films ever made, and won six Academy Awards: Best Picture, Best Director, Best Actor for Hanks, Best Adapted Screenplay, Best Visual Effects, and Best Film Editing. It received many award nominations, including Golden Globes, British Academy Film Awards, and Screen Actors Guild Awards.\nIn 2011, the Library of Congress selected the film for preservation in the United States National Film Registry as being \"culturally, historically, or aesthetically significant\".\nPlot.\nIn 1981 a feather lands at a bus stop in Savannah, Georgia, Forrest Gump collects it, then recounts his life story to strangers on a bus bench.\nIn 1950s Alabama, Forrest is fitted with leg braces to correct a curved spine. His mother runs a boarding house out of their home. Among their tenants is Elvis Presley, who incorporates Forrest's jerky dance movements into his performances. On his first day of school, Forrest befriends a girl named Jenny Curran.\nForrest is often bullied because of his physical disability and low intelligence. While fleeing from several bullies, his leg braces break off, revealing Forrest to be a very fast runner. This talent allows him to receive a football scholarship at the University of Alabama in 1963, where he is coached by Bear Bryant, and witnesses Governor George Wallace's Stand in the Schoolhouse Door, during which he returns a dropped book to Vivian Malone Jones. Forrest becomes a top kick returner, is named to the All-American team, and meets President John F. Kennedy at the White House.\nAfter graduating college in 1966, Forrest is drafted into the U.S. Army where he befriends Benjamin Buford \"Bubba\" Blue, who convinces Forrest to go into the shrimping business with him after their service. They arrive in Vietnam in 1967 and serve with the 9th Infantry Division in the Mekong Delta. Their platoon is ambushed during a patrol, and Forrest saves several wounded platoon mates\u00a0\u2013 including his lieutenant, Dan Taylor, but Bubba is killed. Forrest is awarded the Medal of Honor for his heroism by President Lyndon B. Johnson.\nAt the anti-war March on the Pentagon rally, Forrest meets Abbie Hoffman and briefly reunites with Jenny, who has been living a hippie lifestyle. He also develops a talent for ping-pong, and becomes a sports celebrity as he competes against Chinese teams in ping-pong diplomacy, earning him an interview alongside John Lennon on \"The Dick Cavett Show\", influencing the song \"Imagine\". He spends the 1971 New Year's Eve in New York City with Dan, who lost his legs in Vietnam and has become deeply embittered. Forrest meets President Richard Nixon, who grants him a room in the Watergate Hotel, where he unwittingly exposes the Watergate scandal.\nDischarged from the Army, Forrest returns to Alabama and endorses a ping-pong paddle manufacturer, using the earnings to buy a shrimping boat in Bayou La Batre, fulfilling his promise to Bubba. Dan joins Forrest in 1974, and their lack of success changes when their boat becomes the sole survivor of Hurricane Carmen. They create the \"Bubba Gump Shrimp Company\", Dan reconciles himself to his disabilities, and finally thanks Forrest for saving his life. Forrest returns home to his mother as she dies of cancer. Dan invests their money in Apple Computer and the two become millionaires. Forrest shares his earnings with the community and Bubba's family.\nJenny returns to stay with Forrest in 1976, recovering from years of child abuse, drugs, and prostitution. When Forrest proposes to her, she tells him she loves him and the two have sex, but she leaves the next morning. Heartbroken, Forrest spends the next three years in a relentless cross-country run.\nIn 1981, Forrest reveals that he is waiting at the bus stop because he received a letter from Jenny inviting him to visit. She introduces him to their son, Forrest Gump Jr. Jenny tells Forrest she is sick with an unknown incurable virus, and the three move back to Alabama. Jenny and Forrest marry, but she dies a year later. Forrest sends his son off on his first day of school as the feather from the movie's opening floats on the wind.\nProduction.\nPre-production and script.\n&lt;templatestyles src=\"Template:Quote_box/styles.css\" /&gt;\n\"The writer, Eric Roth, departed substantially from the book. We flipped the two elements of the book, making the love story primary and the fantastic adventures secondary. Also, the book was cynical and colder than the movie. In the movie, Gump is a completely decent character, always true to his word. He has no agenda and no opinion about anything except Jenny, his mother and God.\"\n\u2014director Robert Zemeckis\nThe film is based on the 1986 novel by Winston Groom. Both center on the character of Forrest Gump. However, the film primarily focuses on the first eleven chapters of the novel before skipping ahead to the end of the novel, with the founding of Bubba Gump Shrimp Co. and the meeting with Forrest Jr. In addition to skipping some parts of the novel, the film adds several aspects to Gump's life that do not occur in the novel, such as his needing leg braces as a child and his run across the United States.\nGump's core character and personality are also changed from the novel; among other things, his film character is less of a savant \u2013 in the novel, while playing football at the university, he fails craft and gym but receives a perfect score in an advanced physics class he is enrolled in by his coach to satisfy his college requirements. The novel also features Gump as an astronaut, a professional wrestler, and a chess player.\nThe book had a bidding war regarding an adaptation even before publication, with Wendy Finerman and Steve Tisch acquiring them by joining forces with Warner Bros., where Finerman's husband Mark Canton was president of production. Groom was paid $500,000 and also wrote the first three first drafts of the screenplay, which leaned closer to the events of the novel. After \"Rain Man\" told the story of a savant, Warner Bros. lost interest in the picture, and by 1990 the project was in turnaround. Finerman contacted Columbia Pictures, who went on to reject it, while hiring Eric Roth to rewrite the script. Roth and Finerman kept in contact with Groom to ensure the script was historically accurate. Roth delivered a screenplay in 1992, which Paramount Pictures chairwoman Sherry Lansing liked enough to bring the project to her studio, who acquired the rights from Warner Bros. in exchange for the script for \"Executive Decision\".\nIvan Reitman, Penny Marshall and Terry Gilliam passed on the project before Robert Zemeckis was hired. Barry Sonnenfeld was attached to the film, but left to direct \"Addams Family Values\".\nCasting.\nJohn Travolta was the original choice to play the title role and later said that passing on the role was a mistake. Bill Murray, Chevy Chase, and Matthew Broderick were also considered for the role. Sean Penn had stated in an interview that he had been the second choice for the role; he would later portray a character with a disability in the 2001 film \"I Am Sam\". Tom Hanks revealed that he signed on to the film after an hour and a half of reading the script. He initially wanted to ease Forrest's pronounced Southern accent, but was eventually persuaded by director Robert Zemeckis to portray the heavy accent stressed in the novel. Hanks also said it took him three days producing unusable footage in order to learn how to play the role. Winston Groom, who wrote the original novel, describes the film as having taken the \"rough edges\" off the character whom he had envisioned being played by John Goodman. Additionally, Tom's younger brother Jim Hanks is his acting double in the movie for the scenes whenever Forrest was running. Tom's daughter Elizabeth Hanks appears in the movie as the girl on the school bus who refuses to let young Forrest sit next to her. Sally Field agreed to take on the role of Mrs. Gump after reading the script. Joe Pesci and Kevin Bacon were considered for the role of Lieutenant Dan Taylor, which was eventually given to Gary Sinise. Sinise drew inspiration from the struggles that Vietnam War veterans, some on his wife's side of his family, were going through when returning from serving in Vietnam. David Alan Grier, Ice Cube and Dave Chappelle were offered the role of Benjamin Buford Blue, but all three turned it down. Chappelle, who said he believed the film would be unsuccessful, has been reported as saying that he regrets not taking the role. Hanks was aware of Chappelle's disappointment in missing out on the part and agreed to work with him in a future movie, which ended up being \"You've Got Mail\". Rapper Tupac Shakur also auditioned.\nFilming.\nFilming began in August 1993 and ended in December of that year. Although most of the film is set in Alabama, filming took place mainly in and around Beaufort, South Carolina, as well as parts of coastal Virginia and North Carolina, including a running shot on the Blue Ridge Parkway near Grandfather Mountain where a part of the road subsequently became known as \"Forrest Gump Curve\". Downtown portions of the fictional town of Greenbow were filmed in Varnville, South Carolina. The studio was about to pull the plug on the film, until Zemeckis and Hanks cut the running sequence in the middle. Zemeckis and Hanks used their own money for the sequence. The scene of Forrest running through Vietnam while under fire was filmed on Hunting Island State Park and Fripp Island, South Carolina. Additional filming took place on the Biltmore Estate in Asheville, North Carolina. \nThe Gump family home set was built along the Combahee River near Yemassee, South Carolina, and the nearby land was used to film Curran's home as well as some of the Vietnam scenes. Over 20\u00a0palmetto trees were planted to improve the Vietnam scenes. Forrest Gump narrated his life's story at the northern edge of Chippewa Square in Savannah, Georgia, as he sat at a bus stop bench. There were other scenes filmed in and around the Savannah area as well, including a running shot on the Richard V. Woods Memorial Bridge in Beaufort while he was being interviewed by the press, and on West Bay Street in Savannah. Most of the college campus scenes were filmed in Los Angeles at the University of Southern California. The lighthouse that Forrest runs across to reach the Atlantic Ocean the first time is the Marshall Point Lighthouse in Port Clyde, Maine. Additional scenes were filmed in Arizona, Utah's Monument Valley, and Montana's Glacier National Park.\nVisual effects.\nKen Ralston and his team at Industrial Light &amp; Magic were responsible for the film's visual effects. Using CGI techniques, it was possible to depict Gump meeting deceased personages and shaking their hands. Hanks was first shot against a blue screen along with reference markers so that he could line up with the archive footage. To record the voices of the historical figures, voice actors were filmed and special effects were used to alter lip-syncing for the new dialogue. Archival footage was used and with the help of such techniques as chroma key, image warping, morphing, and rotoscoping, Hanks was integrated into it.\nIn one Vietnam War scene, Gump carries Bubba away from an incoming napalm attack. To create the effect, stunt actors were initially used for compositing purposes. Then, Hanks and Williamson were filmed, with Williamson supported by a cable wire as Hanks ran with him. The explosion was then filmed, and the actors were digitally added to appear just in front of the explosions. The jet fighters and napalm canisters were also added by CGI.\nThe CGI removal of actor Gary Sinise's legs, after his character had them amputated, was achieved by wrapping his legs with a blue fabric, which later facilitated the work of the \"roto-paint\" team to paint out his legs from every single frame. At one point, while hoisting himself into his wheelchair, his legs are used for support.\nThe scene where Forrest spots Jenny at a peace rally at the Lincoln Memorial and Reflecting Pool in Washington, D.C., required visual effects to create the large crowd of people. Over two days of filming, approximately 1,500 extras were used. At each successive take, the extras were rearranged and moved into a different quadrant away from the camera. With the help of computers, the extras were multiplied to create a crowd of several hundred thousand people.\nReception.\nBox office.\nProduced on a budget of $55\u00a0million, \"Forrest Gump\" opened in 1,332\u00a0theaters in the United States and Canada on Wednesday, July 6, 1994, and grossed more than $8 million in its first two days before expanding on Friday to 1,595\u00a0theaters and grossing $24,450,602 in its opening weekend, Paramount's biggest opening non-holiday weekend gross, surpassing the record set by \"The Addams Family\". Motion picture business consultant and screenwriter Jeffrey Hilton suggested to producer Wendy Finerman to double the P&amp;A (film marketing budget) based on his viewing of an early print of the film. The budget was immediately increased, in line with his advice. In its opening weekend, the film placed first at the US box office, narrowly beating \"The Lion King\", which was in its fourth week of release. For the first twelve weeks of release, the film was in the top 3 at the US box office, topping the list 5 times, including in its tenth week of release, when it surpassed \"Raiders of the Lost Ark\" as Paramount's highest-grossing film in the United States and Canada. Paramount removed the film from release in the United States when its gross hit $300\u00a0million in January 1995, and it was the second-highest-grossing film of the year, behind \"The Lion King\" with $305\u00a0million. The film was reissued on February 17, 1995, after the Academy Awards nominations were announced. After the reissue in 1,100 theaters, the film grossed an additional $29\u00a0million in the United States and Canada, bringing its total to $329.7\u00a0million, making it the third-highest-grossing film at that time behind only \"E.T. the Extra-Terrestrial\" and \"Jurassic Park\". Box Office Mojo estimates that the film sold over 78.5\u00a0million tickets in the US and Canada in its initial theatrical run.\nThe film was the fastest grossing Paramount film to pass $100\u00a0million (18 days), $200\u00a0million (46 days; fourth fastest in history), and $300\u00a0million (193 days) in box office receipts (at the time of its release). After reissues, the film has gross receipts of $330,252,182 in the U.S. and Canada and $347,693,217 in international markets for a total of $677,945,399 worldwide. Ultimately, it finished as the fourth highest grossing film of the 1990s in the United States and Canada.\nEven with such revenue, the film was known as a \"successful failure\"; due to distributors' and exhibitors' high fees, Paramount's \"losses\" clocked in at $62\u00a0million, leaving executives realizing the necessity of better deals. This has also been associated with Hollywood accounting, where expenses are inflated to minimize profit sharing.\n\"Forrest Gump\" held the record for being the highest-grossing Paramount film until it was overtaken by \"Titanic\" three years later in 1997. However, it remained the highest-grossing film solely distributed by Paramount until it was surpassed by \"Shrek the Third\" 13 years later in 2007.\nFor 12 years, it remained as the highest-grossing film starring Tom Hanks; it was surpassed in 2006 by \"The Da Vinci Code\".\nCritical reception.\nOn the review aggregator website Rotten Tomatoes, of 158 critics' reviews are positive, with an average rating of 7.80/10. The website's consensus reads: \"Tom Hanks' rigorously earnest performance keeps \"Forrest Gump\" sincere even when it gets glib with American history, making for a whimsical odyssey of debatable wisdom but undeniable heart.\" At the website Metacritic, the film earned a rating of 82 out of 100 based on 21 reviews by mainstream critics, indicating \"universal acclaim\". Audiences polled by CinemaScore gave the film a rare \"A+\" grade.\nThe story was commended by several critics. Roger Ebert of the \"Chicago Sun-Times\" wrote, \"I've never met anyone like Forrest Gump in a movie before, and for that matter I've never seen a movie quite like 'Forrest Gump.' Any attempt to describe him will risk making the movie seem more conventional than it is, but let me try. It's a comedy, I guess. Or maybe a drama. Or a dream. The screenplay by Eric Roth has the complexity of modern fiction...The performance is a breathtaking balancing act between comedy and sadness, in a story rich in big laughs and quiet truths...What a magical movie.\" Todd McCarthy of \"Variety\" wrote that the film \"has been very well worked out on all levels, and manages the difficult feat of being an intimate, even delicate tale played with an appealingly light touch against an epic backdrop.\" In contrast, Anthony Lane of \"The New Yorker\" called the film \"Warm, wise, and wearisome as hell.\" Owen Gleiberman of \"Entertainment Weekly\" said that the film was \"glib, shallow, and monotonous\" and \"reduces the tumult of the last few decades to a virtual-reality theme park: a baby-boomer version of Disney's America.\"\nGump garnered comparisons to fictional character Huckleberry Finn, as well as U.S. politicians Ronald Reagan, Pat Buchanan and Bill Clinton. Peter Chomo writes that Gump acts as a \"social mediator and as an agent of redemption in divided times\". Peter Travers of \"Rolling Stone\" called Gump \"everything we admire in the American character \u2013 honest, brave, and loyal with a heart of gold.\" \"The New York Times\" reviewer Janet Maslin called Gump a \"hollow man\" who is \"self-congratulatory in his blissful ignorance, warmly embraced as the embodiment of absolutely nothing.\" Marc Vincenti of \"Palo Alto Weekly\" called the character \"a pitiful stooge taking the pie of life in the face, thoughtfully licking his fingers.\" Bruce Kawin and Gerald Mast's textbook on film history notes that Forrest Gump's dimness was a metaphor for glamorized nostalgia in that he represented a blank slate onto which the Baby Boomer generation projected their memories of those events.\nRe-evaluation.\nWriting in 2004, \"Entertainment Weekly\" said, \"Nearly a decade after it earned gazillions and swept the Oscars, Robert Zemeckis' ode to 20th-century America still represents one of cinema's most clearly drawn lines in the sand. One half of folks see it as an artificial piece of pop melodrama, while everyone else raves that it's sweet as a box of chocolates.\"\nIn 2015, \"The Hollywood Reporter\" polled hundreds of academy members, asking them to re-vote on past controversial decisions. Academy members said that, given a second chance, they would award the 1994 Oscar for Best Picture to \"The Shawshank Redemption\" instead.\nAuthor payment controversy.\nWinston Groom was paid $350,000 for the screenplay rights to his novel \"Forrest Gump\" and was contracted for a 3\u00a0percent share of the film's \"net\" profits. However, Paramount and the film's producers did not pay him the percentage, using Hollywood accounting to posit that the blockbuster film lost money. Tom Hanks, by contrast, contracted for a percent share of the film's \"gross\" receipts instead of a salary, and he and director Zemeckis each received $40\u00a0million. In addition, Groom was not mentioned once in any of the film's six Oscar-winner speeches.\nGroom's dispute with Paramount was later effectively resolved after Groom declared he was satisfied with Paramount's explanation of their accounting, this coinciding with Groom receiving a seven-figure contract with Paramount for film rights to another of his books, \"Gump &amp; Co.\" This film was never made, remaining in development hell for at least a dozen years.\nHome video.\n\"Forrest Gump\" was first released on VHS on April 27, 1995, and on Laserdisc the following day. The laserdisc was THX certified and released without chapters, requiring the film be watched start to finish. Film magazines of the period stated this was at the request of Zemeckis who wanted viewers to enjoy the film in its entirety. It became the best-selling adult sell-through video, with sales of over 12 million. \nA widescreen VHS release debuted a year later on September 10, 1996. It was released in a two-disc DVD set on August 28, 2001. Special features included director and producer commentaries, production featurettes, and screen tests. The film was released on Blu-ray in November 2009. Paramount released the film on Ultra HD Blu-ray in June 2018. On May 7, 2019, Paramount Pictures released a newly remastered two-disc Blu-ray that contains bonus content.\nAccolades.\n\"Forrest Gump\" won Best Picture, Best Actor in a Leading Role (Hanks had won the previous year for \"Philadelphia\"), Best Director, Best Visual Effects, Best Adapted Screenplay, and Best Film Editing at the 67th Academy Awards. The film was nominated for seven Golden Globe Awards, winning three of them: Best Actor \u2013 Motion Picture Drama, Best Director \u2013 Motion Picture, and Best Motion Picture \u2013 Drama. The film was also nominated for six Saturn Awards and won two for Best Fantasy Film and Best Supporting Actor (Film).\nIn addition to the film's multiple awards and nominations, it has also been recognized by the American Film Institute on several of its lists. The film ranks 37th on \"100 Years...100 Cheers\", 71st on \"100 Years...100 Movies\", and 76th on \"100 Years...100 Movies (10th Anniversary Edition)\". In addition, the quote \"Mama always said life was like a box of chocolates. You never know what you're gonna get,\" was ranked 40th on \"100 Years...100 Movie Quotes\". The film also ranked at number 61 on \"Empire\"'s list of the 100 Greatest Movies of All Time.\nIn December 2011, \"Forrest Gump\" was selected for preservation in the Library of Congress' National Film Registry. The Registry said that the film was \"honored for its technological innovations (the digital insertion of Gump seamlessly into vintage archival footage), its resonance within the culture that has elevated Gump (and what he represents in terms of American innocence) to the status of folk hero, and its attempt to engage both playfully and seriously with contentious aspects of the era's traumatic history.\"\nAmerican Film Institute lists\nSymbolism.\nFeather.\n&lt;templatestyles src=\"Template:Quote_box/styles.css\" /&gt;\n\"I don't want to sound like a bad version of 'the child within'. But the childlike innocence of Forrest Gump is what we all once had. It's an emotional journey. You laugh and cry. It does what movies are supposed to do: make you feel alive.\"\n\u2014producer Wendy Finerman\nVarious interpretations have been suggested for the feather present at the opening and conclusion of the film. Sarah Lyall of \"The New York Times\" noted several suggestions made about the feather: \"Does the white feather symbolize \"The Unbearable Lightness of Being\"? Forrest Gump's impaired intellect? The randomness of experience?\" Hanks interpreted the feather as: \"Our destiny is only defined by how we deal with the chance elements to our life and that's kind of the embodiment of the feather as it comes in. Here is this thing that can land anywhere and that it lands at your feet. It has theological implications that are really huge.\" Sally Field compared the feather to fate, saying: \"It blows in the wind and just touches down here or there. Was it planned or was it just perchance?\" Visual effects supervisor Ken Ralston compared the feather to an abstract painting: \"It can mean so many things to so many different people.\"\nPolitical interpretations.\nHanks states that \"the film is non-political and thus non-judgmental\". Nevertheless, CNN's \"Crossfire\" debated in 1994 whether the film promoted conservative values or was an indictment of the counterculture of the 1960s. Thomas Byers called it \"an aggressively conservative film\" in a \"Modern Fiction Studies\" article.\n&lt;templatestyles src=\"Template:Quote_box/styles.css\" /&gt;\nAll over the political map, people have been calling Forrest their own. But, \"Forrest Gump\" isn't about politics or conservative values. It's about humanity, it's about respect, tolerance and unconditional love.\n\u2014producer Steve Tisch\nIt has been noted that while Gump follows a very conservative lifestyle, Jenny's life is full of countercultural embrace, complete with drug use, promiscuity, and antiwar rallies, and that their eventual marriage might be a kind of reconciliation. Jennifer Hyland Wang argues in a \"Cinema Journal\" article that Jenny's death to an unnamed virus \"symbolizes the death of liberal America and the death of the protests that defined a decade\" in the 1960s. She also notes that the film's screenwriter, Eric Roth, developed the screenplay from the novel and transferred to Jenny \"all of Gump's flaws and most of the excesses committed by Americans in the 1960s and 1970s\".\nOther commentators believe the film forecast the 1994 Republican Revolution and used the image of Forrest Gump to promote movement leader Newt Gingrich's traditional, conservative values. Jennifer Hyland Wang observes that the film idealizes the 1950s, as made evident by the lack of \"Whites Only\"-signs in Gump's Southern childhood, and envisions the 1960s as a period of social conflict and confusion. She argues that this sharp contrast between the decades criticizes the counterculture values and reaffirms conservatism. Wang argues that the film was used by Republican politicians to illustrate a \"traditional version of recent history\" to gear voters toward their ideology for the congressional elections. Presidential candidate Bob Dole stated that the film's message was \"no matter how great the adversity, the American Dream is within everybody's reach\".\nIn 1995, \"National Review\" included \"Forrest Gump\" in its list of the \"Best 100 Conservative Movies\" of all time, and ranked it number four on its \"25 Best Conservative Movies of the Last 25 Years\" list. \"National Review\"'s John Miller wrote that \"Tom Hanks plays the title-character, an amiable dunce who is far too smart to embrace the lethal values of the 1960s. The love of his life, wonderfully played by Robin Wright Penn, chooses a different path; she becomes a drug-addled hippie, with disastrous results.\"\nProfessor James Burton at Salisbury University argues that conservatives claimed \"Forrest Gump\" as their own due less to the content of the film and more to the historical and cultural context of 1994. Burton claims that the film's content and advertising campaign were affected by the cultural climate of the 1990s, which emphasized family values and American values, epitomized in the book \"Hollywood vs. America\". He claims that this climate influenced the apolitical nature of the film, which allowed many different political interpretations.\nSome commentators see the conservative readings of \"Forrest Gump\" as indicating the death of irony in American culture. Vivian Sobchack notes that the film's humor and irony rely on the assumption of the audience's historical knowledge.\nSoundtrack.\nThe soundtrack, featuring 32 songs from the film, was released on July 6, 1994. With the exception of a lengthy suite of themes from Alan Silvestri's original score, all the songs are previously released. Among the artists featured in the film are Elvis Presley, Bob Dylan, Hank Williams, Creedence Clearwater Revival, Aretha Franklin, Lynyrd Skynyrd, Three Dog Night, The Byrds, The Beach Boys, The Jimi Hendrix Experience, The Doors, Canned Heat, Harry Nilsson, The Mamas &amp; the Papas, The Doobie Brothers, Simon &amp; Garfunkel, Bob Seger, Randy Newman, Willie Nelson, Fleetwood Mac, KC &amp; The Sunshine Band, and Buffalo Springfield. Reflecting on compiling the soundtrack, music producer Joel Sill stated \"We wanted to have very recognizable material that would pinpoint time periods, yet we didn't want to interfere with what was happening cinematically.\" The film and the two-disc album have a variety of music from the 1950s to the 1980s performed by American artists. According to Sill, Zemeckis requested this because he thought that American music was the only kind of music Forrest would buy, further stating \"All the material in there is American. Bob (Zemeckis) felt strongly about it. He felt that Forrest wouldn't buy anything but American.\"\nThe soundtrack reached a peak of number 2 on the \"Billboard\" album chart. The soundtrack went on to sell twelve\u00a0million copies, and is one of the top selling albums in the US. The Oscar-nominated score for the film was composed and conducted by Alan Silvestri and released on August 2, 1994.\nAdaptations.\nNovel-sequel.\nThe screenplay for the sequel was written by Eric Roth in 2001. It is based on the original novel's sequel, \"Gump and Co.\", written by Winston Groom in 1995. Roth's script begins with Forrest sitting on a bench waiting for his son to return from school. After the September 11 attacks, Roth, Zemeckis, and Hanks decided the story was no longer \"relevant.\" In March 2007, however, it was reported Paramount producers took another look at the screenplay.\nOn the first page of the sequel novel, Forrest Gump tells readers \"Don't never let nobody make a movie of your life's story,\" and \"Whether they get it right or wrong, it doesn't matter.\" The first chapter of the book suggests the real-life events surrounding the film have been incorporated into Forrest's storyline, and that Forrest got a lot of media attention as a result of the film. During the course of the sequel novel, Gump runs into Tom Hanks and at the end of the novel in the film's release, includes Gump going on \"The David Letterman Show\" and attending the Academy Awards.\nIndian remake.\nThe Indian film \"Laal Singh Chaddha\", released in August 2022 and starring Aamir Khan and Kareena Kapoor in the title role, is an authorized remake of \"Forrest Gump\", set in India between the late 1970s and the 2010s. The film was directed by Advait Chandan and produced by Aamir Khan Productions, Viacom18 Studios and Paramount Pictures.\nFake sequel trailers.\nFollowing the advent of AI video editing, several fake trailers were created and gained attention, some featuring an older Tom Hanks costarring with actors like Tom Holland or Timoth\u00e9e Chalamet. Hanks himself has said that he is glad that a sequel was never made, given the completeness of the original film.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41531", "revid": "48643156", "url": "https://en.wikipedia.org/wiki?curid=41531", "title": "Stanis\u0142aw Ulam", "text": "Polish mathematician and physicist (1909\u20131984)\nStanis\u0142aw Marcin Ulam (Polish: ; 13 April 1909\u00a0\u2013 13 May 1984) was a Polish and American mathematician, nuclear physicist and computer scientist. He participated in the Manhattan Project, originated the Teller\u2013Ulam design of thermonuclear weapons, discovered the concept of the cellular automaton, invented the Monte Carlo method of computation, and suggested nuclear pulse propulsion. In pure and applied mathematics, he proved a number of theorems and proposed several conjectures.\nBorn into a wealthy Polish Jewish family in Lemberg, Austria-Hungary, Ulam studied mathematics at the Lw\u00f3w Polytechnic Institute, where he earned his PhD in 1933 under the supervision of Kazimierz Kuratowski and W\u0142odzimierz Sto\u017cek. In 1935, John von Neumann, whom Ulam had met in Warsaw, invited him to come to the Institute for Advanced Study in Princeton, New Jersey, for a few months. From 1936 to 1939, he spent summers in Poland and academic years at Harvard University in Cambridge, Massachusetts, where he worked to establish important results regarding ergodic theory. On 20 August 1939, he sailed for the United States for the last time with his 17-year-old brother Adam Ulam. He became an assistant professor at the University of Wisconsin\u2013Madison in 1940, and a United States citizen in 1941.\nIn October 1943, he received an invitation from Hans Bethe to join the Manhattan Project at the secret Los Alamos Laboratory in New Mexico. There, he worked on the hydrodynamic calculations to predict the behavior of the explosive lenses that were needed by an implosion-type weapon. He was assigned to Edward Teller's group, where he worked on Teller's \"Super\" bomb for Teller and Enrico Fermi. After the war he left to become an associate professor at the University of Southern California, but returned to Los Alamos in 1946 to work on thermonuclear weapons. With the aid of a cadre of female \"computers\" he found that Teller's \"Super\" design was unworkable. In January 1951, Ulam and Teller came up with the Teller\u2013Ulam design, which became the basis for all thermonuclear weapons.\nUlam considered the problem of nuclear propulsion of rockets, which was pursued by Project Rover, and proposed, as an alternative to Rover's nuclear thermal rocket, to harness small nuclear explosions for propulsion, which became Project Orion. With Fermi, John Pasta, and Mary Tsingou, Ulam studied the Fermi\u2013Pasta\u2013Ulam\u2013Tsingou problem, which became the inspiration for the field of nonlinear science. He is probably best known for realizing that electronic computers made it practical to apply statistical methods to functions without known solutions, and as computers have developed, the Monte Carlo method has become a common and standard approach to many problems.\nPoland.\nStanis\u0142aw Marcin Ulam was born in Lemberg, Galicia, on 13 April 1909. At this time, Galicia was in the Kingdom of Galicia and Lodomeria of the Austro-Hungarian Empire, which was known to Poles as the Austrian partition. In 1918, it became part of the newly restored Poland, the Second Polish Republic, and the city took its Polish name again, Lw\u00f3w.\nThe Ulams were a wealthy Polish Jewish family of bankers, industrialists, and other professionals. Ulam's immediate family was \"well-to-do but hardly rich\". His father, J\u00f3zef Ulam, was born in Lw\u00f3w and was a lawyer, and his mother, Anna (n\u00e9e Auerbach), was born in Stryj. His uncle, Micha\u0142 Ulam, was an architect, building contractor, and lumber industrialist. From 1916 until 1918, J\u00f3zef's family lived temporarily in Vienna. After they returned, Lw\u00f3w became the epicenter of the Polish\u2013Ukrainian War, during which the city experienced a Ukrainian siege.\nIn 1919, Ulam entered Lw\u00f3w Gymnasium Nr. VII, from which he graduated in 1927. He then studied mathematics at the Lw\u00f3w Polytechnic Institute. Under the supervision of Kazimierz Kuratowski, he received his Master of Arts degree in 1932, and became a Doctor of Science in 1933. At the age of 20, in 1929, he published his first paper \"Concerning Functions of Sets\" in the journal \"Fundamenta Mathematicae\". From 1931 until 1935, he traveled to and studied in Wilno (Vilnius), Vienna, Z\u00fcrich, Paris, and Cambridge, England, where he met G. H. Hardy and Subrahmanyan Chandrasekhar.\nAlong with Stanis\u0142aw Mazur, Mark Kac, W\u0142odzimierz Sto\u017cek, Kuratowski, and others, Ulam was a member of the Lw\u00f3w School of Mathematics. Its founders were Hugo Steinhaus and Stefan Banach, who were professors at the Jan Kazimierz University. Mathematicians of this \"school\" met for long hours at the Scottish Caf\u00e9, where the problems they discussed were collected in the Scottish Book, a thick notebook provided by Banach's wife. Ulam was a major contributor to the book. Of the 193 problems recorded between 1935 and 1941, he contributed 40 problems as a single author, another 11 with Banach and Mazur, and an additional 15 with others. In 1957, he received from Steinhaus a copy of the book, which had survived the war, and translated it into English. In 1981, Ulam's friend R. Daniel Mauldin published an expanded and annotated version.\nMove to the United States.\nIn 1935, John von Neumann, whom Ulam had met in Warsaw, invited him to come to the Institute for Advanced Study in Princeton, New Jersey, for a few months. In December of that year, Ulam sailed to the US. At Princeton, he went to lectures and seminars, where he heard Oswald Veblen, James Alexander, and Albert Einstein. During a tea party at von Neumann's house, he encountered G. D. Birkhoff, who suggested that he apply for a position with the Harvard Society of Fellows. Following up on Birkhoff's suggestion, Ulam spent summers in Poland and academic years at Harvard University in Cambridge, Massachusetts from 1936 to 1939, where he worked with John C. Oxtoby to establish results regarding ergodic theory. These appeared in \"Annals of Mathematics\" in 1941.\nOn 20 August 1939, in Gdynia, J\u00f3zef Ulam, along with his brother Szymon, put his two sons, Stanislaw and 17-year-old Adam, on a ship headed for the US. Eleven days later, the Germans invaded Poland. Within two months, the Germans completed their occupation of western Poland, and the Soviets invaded and occupied eastern Poland. Within two years, J\u00f3zef Ulam and the rest of his family, including Stanislaw's sister Stefania Ulam, were victims of the Holocaust, Hugo Steinhaus was in hiding, Kazimierz Kuratowski was lecturing at the underground university in Warsaw, W\u0142odzimierz Sto\u017cek and his two sons had been killed in the massacre of Lw\u00f3w professors, and the last problem had been recorded in the Scottish Book. Stefan Banach survived the Nazi occupation by feeding lice at Rudolf Weigl's typhus research institute. In 1963, Adam Ulam, who had become an eminent kremlinologist at Harvard, received a letter from George Volsky, who hid in J\u00f3zef Ulam's house after deserting from the Polish army. This reminiscence gave a chilling account of Lw\u00f3w's chaotic scenes in late 1939. In later life Ulam described himself as \"an agnostic. Sometimes I muse deeply on the forces that are for me invisible. When I am almost close to the idea of God, I feel immediately estranged by the horrors of this world, which he seems to tolerate\".\nIn 1940, after being recommended by Birkhoff, Ulam became an assistant professor at the University of Wisconsin\u2013Madison. Here, he became a United States citizen in 1941. That year, he married Fran\u00e7oise Aron. She had been a French exchange student at Mount Holyoke College, whom he met in Cambridge. They had one daughter, Claire. In Madison, Ulam met his friend and colleague C. J. Everett, with whom he collaborated on a number of papers.\nManhattan Project.\nIn early 1943, Ulam asked von Neumann to find him a war job. In October, he received an invitation to join an unidentified project near Santa Fe, New Mexico. The letter was signed by Hans Bethe, who had been appointed as leader of the theoretical division of Los Alamos National Laboratory by Robert Oppenheimer, its scientific director. Knowing nothing of the area, he borrowed a New Mexico guide book. On the checkout card, he found the names of his Wisconsin colleagues, Joan Hinton, David Frisch, and Joseph McKibben, all of whom had mysteriously disappeared. This was Ulam's introduction to the Manhattan Project, which was the US's wartime effort to create the atomic bomb.\nHydrodynamical calculations of implosion.\nA few weeks after Ulam reached Los Alamos in February 1944, the project experienced a crisis. In April, Emilio Segr\u00e8 discovered that plutonium made in reactors would not work in a gun-type plutonium weapon like the \"Thin Man\", which was being developed in parallel with a uranium weapon, the \"Little Boy\" that was dropped on Hiroshima. This problem threatened to waste an enormous investment in new reactors at the Hanford site and to make slow uranium isotope separation the only way to prepare fissile material suitable for use in bombs. To respond, Oppenheimer implemented, in August, a sweeping reorganization of the laboratory to focus on development of an implosion-type weapon and appointed George Kistiakowsky head of the implosion department. He was a professor at Harvard and an expert on precise use of explosives.\nThe basic concept of implosion is to use chemical explosives to crush a chunk of fissile material into a critical mass, where neutron multiplication leads to a nuclear chain reaction, releasing a large amount of energy. Cylindrical implosive configurations had been studied by Seth Neddermeyer, but von Neumann, who had experience with shaped charges used in armor-piercing ammunition, was a vocal advocate of spherical implosion driven by explosive lenses. He realized that the symmetry and speed with which implosion compressed the plutonium were critical issues, and enlisted Ulam to help design lens configurations that would provide nearly spherical implosion. Within an implosion, because of enormous pressures and high temperatures, solid materials behave much like fluids. This meant that hydrodynamical calculations were needed to predict and minimize asymmetries that would spoil a nuclear detonation. Of these calculations, Ulam said:&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;\nNevertheless, with the primitive facilities available at the time, Ulam and von Neumann did carry out numerical computations that led to a satisfactory design. This motivated their advocacy of a powerful computational capability at Los Alamos, which began during the war years, continued through the cold war, and still exists. Otto Frisch remembered Ulam as \"a brilliant Polish topologist with a charming French wife. At once he told me that he was a pure mathematician who had sunk so low that his latest paper actually contained numbers with decimal points!\"\nStatistics of branching and multiplicative processes.\nEven the inherent statistical fluctuations of neutron multiplication within a chain reaction have implications with regard to implosion speed and symmetry. In November 1944, David Hawkins and Ulam addressed this problem in a report entitled \"Theory of Multiplicative Processes\". This report, which invokes probability-generating functions, is also an early entry in the extensive literature on statistics of branching and multiplicative processes. In 1948, its scope was extended by Ulam and Everett.\nEarly in the Manhattan project, Enrico Fermi's attention was focused on the use of reactors to produce plutonium. In September 1944, he arrived at Los Alamos, shortly after breathing life into the first Hanford reactor, which had been poisoned by a xenon isotope. Soon after Fermi's arrival, Teller's \"Super\" bomb group, of which Ulam was a part, was transferred to a new division headed by Fermi. Fermi and Ulam formed a relationship that became very fruitful after the war.\nPost war Los Alamos.\nIn September 1945, Ulam left Los Alamos to become an associate professor at the University of Southern California in Los Angeles. In January 1946, he suffered an acute attack of encephalitis, which put his life in danger, but which was alleviated by emergency brain surgery. During his recuperation, many friends visited, including Nicholas Metropolis from Los Alamos and the famous mathematician Paul Erd\u0151s, who remarked: \"Stan, you are just like before.\" This was encouraging, because Ulam was concerned about the state of his mental faculties, for he had lost the ability to speak during the crisis. Another friend, Gian-Carlo Rota, asserted in a 1987 article that the attack changed Ulam's personality: afterwards, he turned from rigorous pure mathematics to more speculative conjectures concerning the application of mathematics to physics and biology; Rota also cites Ulam's former collaborator Paul Stein as noting that Ulam was sloppier in his clothing afterwards, and John Oxtoby as noting that Ulam before the encephalitis could work for hours on end doing calculations, while when Rota worked with him, was reluctant to solve even a quadratic equation. This assertion was not accepted by Fran\u00e7oise Aron Ulam.\nBy late April 1946, Ulam had recovered enough to attend a secret conference at Los Alamos to discuss thermonuclear weapons. Those in attendance included Ulam, von Neumann, Metropolis, Teller, Stan Frankel, and others. Throughout his participation in the Manhattan Project, Teller's efforts had been directed toward the development of a \"super\" weapon based on nuclear fusion, rather than toward development of a practical fission bomb. After extensive discussion, the participants reached a consensus that his ideas were worthy of further exploration. A few weeks later, Ulam received an offer of a position at Los Alamos from Metropolis and Robert D. Richtmyer, the new head of its theoretical division, at a higher salary, and the Ulams returned to Los Alamos.\nMonte Carlo method.\nLate in the war, under the sponsorship of von Neumann, Frankel and Metropolis began to carry out calculations on the first general-purpose electronic computer, the ENIAC at the Aberdeen Proving Ground in Maryland. Shortly after returning to Los Alamos, Ulam participated in a review of results from these calculations. Earlier, while playing solitaire during his recovery from surgery, Ulam had thought about playing hundreds of games to estimate statistically the probability of a successful outcome. With ENIAC in mind, he realized that the availability of computers made such statistical methods very practical. John von Neumann immediately saw the significance of this insight. In March 1947 he proposed a statistical approach to the problem of neutron diffusion in fissionable material. Because Ulam had often mentioned his uncle, Micha\u0142 Ulam, \"who just had to go to Monte Carlo\" to gamble, Metropolis dubbed the statistical approach \"The Monte Carlo method\". Metropolis and Ulam published the first unclassified paper on the Monte Carlo method in 1949.\nFermi, learning of Ulam's breakthrough, devised an analog computer known as the Monte Carlo trolley, later dubbed the FERMIAC. The device performed a mechanical simulation of random diffusion of neutrons. As computers improved in speed and programmability, these methods became more useful. In particular, many Monte Carlo calculations carried out on modern massively parallel supercomputers are embarrassingly parallel applications, whose results can be very accurate.\nTeller\u2013Ulam design.\nOn 29 August 1949, the Soviet Union tested its first fission bomb, the RDS-1. Created under the supervision of Lavrentiy Beria, who sought to duplicate the US effort, this weapon was nearly identical to Fat Man, for its design was based on information provided by spies Klaus Fuchs, Theodore Hall, and David Greenglass. In response, on 31 January 1950, President Harry S. Truman announced a crash program to develop a fusion bomb.\nTo advocate an aggressive development program, Ernest Lawrence and Luis Alvarez came to Los Alamos, where they conferred with Norris Bradbury, the laboratory director, and with George Gamow, Edward Teller, and Ulam. Soon, these three became members of a short-lived committee appointed by Bradbury to study the problem, with Teller as chairman. At this time, research on the use of a fission weapon to create a fusion reaction had been ongoing since 1942, but the design was still essentially the one originally proposed by Teller. His concept was to put tritium and/or deuterium in close proximity to a fission bomb, with the hope that the heat and intense flux of neutrons released when the bomb exploded, would ignite a self-sustaining fusion reaction. Reactions of these isotopes of hydrogen are of interest because the energy per unit mass of fuel released by their fusion is much larger than that from fission of heavy nuclei.\nBecause the results of calculations based on Teller's concept were discouraging, many scientists believed it could not lead to a successful weapon, while others had moral and economic grounds for not proceeding. Consequently, several senior people of the Manhattan Project opposed development, including Bethe and Oppenheimer. To clarify the situation, Ulam and von Neumann resolved to do new calculations to determine whether Teller's approach was feasible. To carry out these studies, von Neumann decided to use electronic computers: ENIAC at Aberdeen, a new computer, MANIAC, at Princeton, and its twin, which was under construction at Los Alamos. Ulam enlisted Everett to follow a completely different approach, one guided by physical intuition. Fran\u00e7oise Ulam was one of a cadre of women \"computers\" who carried out laborious and extensive computations of thermonuclear scenarios on mechanical calculators, supplemented and confirmed by Everett's slide rule. Ulam and Fermi collaborated on further analysis of these scenarios. The results showed that, in workable configurations, a thermonuclear reaction would not ignite, and if ignited, it would not be self-sustaining. Ulam had used his expertise in combinatorics to analyze the chain reaction in deuterium, which was much more complicated than the ones in uranium and plutonium, and he concluded that no self-sustaining chain reaction would take place at the (low) densities that Teller was considering. In late 1950, these conclusions were confirmed by von Neumann's results.\nIn January 1951, Ulam had another idea: to channel the mechanical shock of a nuclear explosion so as to compress the fusion fuel. On the recommendation of his wife, Ulam discussed this idea with Bradbury and Mark before he told Teller about it. Almost immediately, Teller saw its merit, but noted that soft X-rays from the fission bomb would compress the thermonuclear fuel more strongly than mechanical shock and suggested ways to enhance this effect. On 9 March 1951, Teller and Ulam submitted a joint report describing these innovations. A few weeks later, Teller suggested placing a fissile rod or cylinder at the center of the fusion fuel. The detonation of this \"spark plug\" would help to initiate and enhance the fusion reaction. The design based on these ideas, called staged radiation implosion, has become the standard way to build thermonuclear weapons. It is often described as the \"Teller\u2013Ulam design\".\nIn September 1951, after a series of differences with Bradbury and other scientists, Teller resigned from Los Alamos, and returned to the University of Chicago. At about the same time, Ulam went on leave as a visiting professor at Harvard for a semester. Although Teller and Ulam submitted a joint report on their design and jointly applied for a patent on it, they soon became involved in a dispute over who deserved credit. After the war, Bethe returned to Cornell University, but he was deeply involved in the development of thermonuclear weapons as a consultant. In 1954, he wrote an article on the history of the H-bomb, which presents his opinion that both men contributed very significantly to the breakthrough. This balanced view is shared by others who were involved, including Mark and Fermi, but Teller persistently attempted to downplay Ulam's role. \"After the H-bomb was made,\" Bethe recalled, \"reporters started to call Teller the father of the H-bomb. For the sake of history, I think it is more precise to say that Ulam is the father, because he provided the seed, and Teller is the mother, because he remained with the child. As for me, I guess I am the midwife.\"\nWith the basic fusion reactions confirmed, and with a feasible design in hand, there was nothing to prevent Los Alamos from testing a thermonuclear device. On 1 November 1952, the first thermonuclear explosion occurred when Ivy Mike was detonated on Enewetak Atoll, within the US Pacific Proving Grounds. This device, which used liquid deuterium as its fusion fuel, was immense and utterly unusable as a weapon. Nevertheless, its success validated the Teller\u2013Ulam design, and stimulated intensive development of practical weapons.\nFermi\u2013Pasta\u2013Ulam\u2013Tsingou problem.\nWhen Ulam returned to Los Alamos, his attention turned away from weapon design and toward the use of computers to investigate problems in physics and mathematics. With John Pasta, who helped Metropolis to bring MANIAC on line in March 1952, he explored these ideas in a report \"Heuristic Studies in Problems of Mathematical Physics on High Speed Computing Machines\", which was submitted on 9 June 1953. It treated several problems that cannot be addressed within the framework of traditional analytic methods: billowing of fluids, rotational motion in gravitating systems, magnetic lines of force, and hydrodynamic instabilities.\nSoon, Pasta and Ulam became experienced with electronic computation on MANIAC, and by this time, Enrico Fermi had settled into a routine of spending academic years at the University of Chicago and summers at Los Alamos. During these summer visits, Pasta, Ulam, and Mary Tsingou, a programmer in the MANIAC group, joined him to study a variation of the classic problem of a string of masses held together by springs that exert forces linearly proportional to their displacement from equilibrium. Fermi proposed to add to this force a nonlinear component, which could be chosen to be proportional to either the square or cube of the displacement, or to a more complicated \"broken linear\" function. This addition is the key element of the Fermi\u2013Pasta\u2013Ulam\u2013Tsingou problem, which is often designated by the abbreviation FPUT.\nA classical spring system can be described in terms of vibrational modes, which are analogous to the harmonics that occur on a stretched violin string. If the system starts in a particular mode, vibrations in other modes do not develop. With the nonlinear component, Fermi expected energy in one mode to transfer gradually to other modes, and eventually, to be distributed equally among all modes. This is roughly what began to happen shortly after the system was initialized with all its energy in the lowest mode, but much later, essentially all the energy periodically reappeared in the lowest mode. This behavior is very different from the expected equipartition of energy. It remained mysterious until 1965, when Kruskal and Zabusky showed that, after appropriate mathematical transformations, the system can be described by the Korteweg\u2013de Vries equation, which is the prototype of nonlinear partial differential equations that have soliton solutions. This means that FPUT behavior can be understood in terms of solitons.\nNuclear propulsion.\nStarting in 1955, Ulam and Frederick Reines considered nuclear propulsion of aircraft and rockets. This is an attractive possibility, because the nuclear energy per unit mass of fuel is a million times greater than that available from chemicals. From 1955 to 1972, their ideas were pursued during Project Rover, which explored the use of nuclear reactors to power rockets. In response to a question by Senator John O. Pastore at a congressional committee hearing on \"Outer Space Propulsion by Nuclear Energy\", on January 22, 1958, Ulam replied that \"the future as a whole of mankind is to some extent involved inexorably now with going outside the globe.\"\nUlam and C. J. Everett also proposed, in contrast to Rover's continuous heating of rocket exhaust, to harness small nuclear explosions for propulsion. Project Orion was a study of this idea. It began in 1958 and ended in 1965, after the Partial Nuclear Test Ban Treaty of 1963 banned nuclear weapons tests in the atmosphere and in space. Work on this project was spearheaded by physicist Freeman Dyson, who commented on the decision to end Orion in his article, \"Death of a Project\".\nBradbury appointed Ulam and John H. Manley as research advisors to the laboratory director in 1957. These newly created positions were on the same administrative level as division leaders, and Ulam held his until he retired from Los Alamos. In this capacity, he was able to influence and guide programs in many divisions: theoretical, physics, chemistry, metallurgy, weapons, health, Rover, and others.\nIn addition to these activities, Ulam continued to publish technical reports and research papers. One of these introduced the Fermi\u2013Ulam model, an extension of Fermi's theory of the acceleration of cosmic rays. Another, with Paul Stein and Mary Tsingou, titled \"Quadratic Transformations\", was an early investigation of chaos theory and is considered the first published use of the phrase \"chaotic behavior\".\nReturn to academia.\nDuring his years at Los Alamos, Ulam was a visiting professor at Harvard from 1951 to 1952, MIT from 1956 to 1957, the University of California, San Diego, in 1963, and the University of Colorado at Boulder from 1961 to 1962 and 1965 to 1967. In 1967, the last of these positions became permanent, when Ulam was appointed Professor and Chairman of the Department of Mathematics at the University of Colorado. He kept a residence in Santa Fe, which made it convenient to spend summers at Los Alamos as a consultant. He was an elected member of the American Academy of Arts and Sciences, the United States National Academy of Sciences, and the American Philosophical Society.\nIn Colorado, where he rejoined his friends Gamow, Richtmyer, and Hawkins, Ulam's research interests turned toward biology. In 1968, recognizing this emphasis, the University of Colorado School of Medicine appointed Ulam as Professor of Biomathematics, and he held this position until his death. With his Los Alamos colleague Robert Schrandt he published a report, \"Some Elementary Attempts at Numerical Modeling of Problems Concerning Rates of Evolutionary Processes\", which applied his earlier ideas on branching processes to evolution. Another, report, with William Beyer, Temple F. Smith, and M. L. Stein, titled \"Metrics in Biology\", introduced new ideas about numerical taxonomy and evolutionary distances.\nWhen he retired from Colorado in 1975, Ulam began to spend winter semesters at the University of Florida, where he was a graduate research professor. In 1976, he was awarded the Commander's Cross with the Star of the Order of Polonia Restituta by the Polish government-in-exile in London. Except for sabbaticals at the University of California, Davis from 1982 to 1983, and at Rockefeller University from 1980 to 1984, this pattern of spending summers in Colorado and Los Alamos and winters in Florida continued until Ulam died of an apparent heart attack in Santa Fe on 13 May 1984.\nPaul Erd\u0151s noted that \"he died suddenly of heart failure, without fear or pain, while he could still prove and conjecture.\" In 1987, Fran\u00e7oise Ulam deposited his papers with the American Philosophical Society Library in Philadelphia. She continued to live in Santa Fe until she died in 2011, at the age of 93. Both Fran\u00e7oise and her husband were buried with her family in Montparnasse Cemetery in Paris.\nCoining of the term \"singularity\".\nUlam reported in 1958 an earlier discussion with John von Neumann \"centered on the accelerating progress of technology and changes in human life, which gives the appearance of approaching some essential singularity in the history of the race beyond which human affairs, as we know them, could not continue\".\nImpact and legacy.\nUlam participated in the creation of a hydrogen bomb as part of the Los Alamos Laboratory nuclear project. From the publication of his first paper as a student in 1929 until his death, Ulam was constantly writing on mathematics. The list of Ulam's publications includes more than 150 papers. Topics represented by a significant number of papers are: set theory (including measurable cardinals and abstract measures), topology, functional analysis, transformation theory, ergodic theory, group theory, projective algebra, number theory, combinatorics, and graph theory.\nNotable results of this work are:\n&lt;templatestyles src=\"Col-begin/styles.css\"/&gt;\nUlam played pivotal role in the development of thermonuclear weapons. According to Fran\u00e7oise Ulam: \"Stan would reassure me that, barring accidents, the H-bomb rendered nuclear war impossible.\" In 1980, Ulam and his wife appeared in the television documentary \"The Day After Trinity\".\nThe Monte Carlo method has become a ubiquitous and standard approach to computation, and the method has been applied to a vast number of scientific problems. In addition to problems in physics and mathematics, the method has been applied to finance, social science, environmental risk assessment, linguistics, radiation therapy, and sports.\nThe Fermi\u2013Pasta\u2013Ulam\u2013Tsingou problem is credited not only as \"the birth of experimental mathematics\", but also as inspiration for the vast field of Nonlinear Science. In his Lilienfeld Prize lecture, David K. Campbell noted this relationship and described how FPUT gave rise to ideas in chaos, solitons, and dynamical systems. In 1980, Donald Kerr, laboratory director at Los Alamos, with the strong support of Ulam and Mark Kac, founded the Center for Nonlinear Studies (CNLS). In 1985, CNLS initiated the \"Stanislaw M. Ulam Distinguished Scholar\" program, which provides an annual award that enables a noted scientist to spend a year carrying out research at Los Alamos.\nThe fiftieth anniversary of the original FPUT paper was the subject of the March 2005 issue of the journal Chaos, and the topic of the 25th Annual International Conference of CNLS. The University of Southern Mississippi and the University of Florida supported the \"Ulam Quarterly\", which was active from 1992 to 1996, and which was one of the first online mathematical journals. Florida's Department of Mathematics has sponsored, since 1998, the annual \"Ulam Colloquium Lecture\", and in March 2009, the \"Ulam Centennial Conference\".\nUlam's work on non-Euclidean distance metrics in the context of molecular biology made a significant contribution to sequence analysis and his contributions in theoretical biology are considered watersheds in the development of cellular automata theory, population biology, pattern recognition, and biometrics generally (David Sankoff, however, challenged conclusions of Walter by writing that Ulam had only modest influence on early development of sequence alignment methods.). Colleagues noted that some of his greatest contributions were in clearly identifying problems to be solved and general techniques for solving them.\nIn 1987, Los Alamos issued a special issue of its \"Science\" publication, which summarized his accomplishments, and which appeared, in 1989, as the book \"From Cardinals to Chaos\". Similarly, in 1990, the University of California Press issued a compilation of mathematical reports by Ulam and his Los Alamos collaborators: \"Analogies Between Analogies\". During his career, Ulam was awarded honorary degrees by the Universities of New Mexico, Wisconsin, and Pittsburgh.\nIn 2021, German film director Thorsten Klein made a film adaptation of Ulam's autobiography, \"Adventures of a Mathematician\".\nIn 2019, Polish entomologist Marcin Kami\u0144ski introduced the generic name \"Ulamus\" to honor Ulam's work. \"Ulamus\" belongs to the molluscan family \"Pteriidae\".\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41533", "revid": "3055476", "url": "https://en.wikipedia.org/wiki?curid=41533", "title": "Gy\u00f6rgy Dalos", "text": "Hungarian writer and translator (born 1943)\nGy\u00f6rgy Dalos (born 23 September 1943) is a Hungarian Jewish writer and historian. He is best known for his novel \"1985\", and \"The Guest from the Future: Anna Akhmatova and Isaiah Berlin\".\nLife.\nDalos was born in Budapest and spent his childhood with his grandparents, as his father had died in 1945 in a labor camp, where he had been sent to as a Jew during World War II. From 1962 to 1967, he studied history at the Lomonossov University in Moscow. He then returned to his native town Budapest to work as a museologist. In 1968, Dalos was accused of \"Maoist activities\" and was handed seven months prison on probation and a Berufsverbot (professional disqualification) and a publication ban; due to that, he worked as a translator. In 1977, he was among the founders of the opposition movement against the Communist regime of Hungary. In 1988/89 he was co-editor of the East German underground opposition paper \"Ostkreuz\". From 1995 to 1999, Dalos was head of the Institute for Hungarian Culture in Berlin. Since 2009 he is member of the International Council of Austrian Service Abroad.\nDalos lived in Vienna from 1987 to 1995. Since 1995, he has lived in Berlin as a freelance publisher and editor.\nWork.\nArticles\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41534", "revid": "44217690", "url": "https://en.wikipedia.org/wiki?curid=41534", "title": "Eldred v. Ashcroft", "text": "2003 United States Supreme Court case\nEldred v. Ashcroft, 537 U.S. 186 (2003), was a decision by the Supreme Court of the United States upholding the constitutionality of the 1998 Sonny Bono Copyright Term Extension Act (CTEA). The practical result of this was to prevent a number of works from entering the public domain in 1998 and following years, as would have occurred under the Copyright Act of 1976. Materials which the plaintiffs had worked with and were ready to republish were now unavailable due to copyright restrictions.\nInternet publisher Eric Eldred was the lead petitioner, and was joined by a group of commercial and non-commercial interests who relied on the public domain for their work (including Dover Publications) and many \"amici\" including the Free Software Foundation, the American Association of Law Libraries, the Bureau of National Affairs, and the College Art Association. Eldred was represented by Lawrence Lessig and a team at the Berkman Center for Internet and Society.\nSupporting the law were United States Attorneys General Janet Reno and John Ashcroft, along with a set of \"amici\" including the Motion Picture Association of America, the Recording Industry Association of America, ASCAP and Broadcast Music Incorporated.\nBackground.\nThe Sonny Bono Copyright Term Extension Act (or CTEA) extended existing copyright terms by an additional 20 years from the terms set by the Copyright Act of 1976. The law affected both new and existing works (making it both a \"prospective\" extension as well as a \"retroactive\" one). Specifically, for works published before January 1, 1978, and still in copyright on October 27, 1998, the term was extended to 95 years. For works authored by \"individuals\" on or after January 1, 1978 (including new works), the copyright term was extended to equal the life of the author plus 70 years. For works authored by joint authors, the copyright term was extended to the life of the last surviving author plus 70 years. In the case of works-for-hire, anonymous or pseudonymous works, the term was set at 95 years from the date of first publication, or 120 years from creation.\nThe practical result of this was to prevent a number of works from entering the public domain in 1998 and following years, as would have occurred under the Copyright Law of 1976. Materials which the plaintiffs had worked with and were ready to republish were now unavailable due to copyright restrictions.\nThe lead petitioner, Eric Eldred, is an Internet publisher. Eldred was joined by a group of commercial and non-commercial interests who relied on the public domain for their work. These included Dover Publications, a commercial publisher of paperback books; Luck's Music Library, Inc. and Edwin F. Kalmus &amp; Co., Inc., publishers of orchestral sheet music; and many \"amici\" including the Free Software Foundation; Tri-Horn International of Boston, Massachusetts, a golf publishing and technology company; the American Association of Law Libraries; the Bureau of National Affairs; and the College Art Association.\nSupporting the law were the U.S. government, represented by the Attorney General in an \"ex officio\" capacity (originally Janet Reno, later replaced by John Ashcroft), along with a set of \"amici\" including the Motion Picture Association of America, the Recording Industry Association of America, ASCAP, and Broadcast Music Incorporated.\nDistrict court.\nThe original complaint was filed in the United States District Court for the District of Columbia on January 11, 1999. The plaintiffs' argument was threefold:\nIn response, the government argued that Congress does indeed have the latitude to retroactively extend terms, so long as the individual extensions are also for \"limited Times\", as required by the Constitution. As an argument for this position, they referred to the Copyright Act of 1790, the first Federal copyright legislation, which applied Federal protection to existing works. Furthermore, they argued, neither the First Amendment nor the doctrine of public trust is applicable to copyright cases.\nOn October 28, 1999, Judge June Green issued a brief opinion rejecting all three of the petitioners' arguments. On the first count, she wrote that Congress had the power to extend terms as it wished, as long as the terms themselves were of limited duration. On the second count, she rejected the notion of First Amendment scrutiny in copyright cases, based on her interpretation of \"Harper and Row Publishers, Inc., v. Nation Enterprises\", an earlier Supreme Court decision. On the third count, she rejected the notion that public trust doctrine was applicable to copyright law.\nCourt of Appeals.\nThe plaintiffs appealed the decision of the district court to the United States Court of Appeals for the District of Columbia Circuit, filing their initial brief on May 22, 2000, and arguing the case on October 5 of the same year in front of a three-judge panel. Arguments were similar to those made in the district court, except for those regarding the public trust doctrine, which were not included in the appeal.\nInstead, the plaintiffs extended their argument on the copyright clause to note that the clause requires Congress to \"promote the Progress of Science and useful Arts\", and argued that retroactive extensions do not directly serve this purpose in the standard \"quid pro quo\" previously required by the courts.\nThe case was decided on February 16, 2001. The appeals court upheld the decision of the district court in a 2\u20131 opinion. In his dissent, Judge David Sentelle agreed with the plaintiffs that CTEA was indeed unconstitutional based on the \"limited Times\" requirement. Supreme Court precedent, he argued, held that one must be able to discern an \"outer limit\" to a limited power; in the case of retrospective copyright extensions, Congress could continue to extend copyright terms indefinitely through a set of limited extensions, thus rendering the \"limited times\" requirement meaningless.\nFollowing this ruling, plaintiffs petitioned for a rehearing \"en banc\" (in front of the full panel of nine judges). This petition was rejected, 7\u20132, with Judges Sentelle and David Tatel dissenting.\nSupreme Court.\nOn October 11, 2001, the plaintiffs filed a petition for certiorari to the Supreme Court of the United States. On February 19, 2002, the Court granted certiorari, agreeing to hear the case.\nOral arguments were presented on October 9, 2002. Lead counsel for the plaintiff was Lawrence Lessig; the government's case was argued by Solicitor General Theodore Olson.\nLessig focused the Plaintiffs' brief to emphasize the Copyright Clause restriction, as well as the First Amendment argument from the Court of Appeals case. The decision to emphasize the Copyright Clause argument was based on both the minority opinion of Judge Sentelle in the appeals court, and on several recent Supreme Court decisions authored by Chief Justice William Rehnquist: \"United States v. Lopez\" (1996) and \"United States v. Morrison\" (2000).\nIn both of those decisions, Rehnquist, along with four of the Court's more conservative justices, held Congressional legislation unconstitutional, because that legislation exceeded the limits of the Constitution's Commerce Clause. This profound reversal of precedent, Lessig argued, could not be limited to only one of the enumerated powers. If the court felt that it had the power to review legislation under the Commerce Clause, Lessig argued, then the Copyright clause deserved similar treatment, or at very least a \"principled reason\" must be stated for according such treatment to only one of the enumerated powers.\nOn January 15, 2003, the Court held the CTEA constitutional by a 7\u20132 decision. The majority opinion, written by Justice Ginsburg, relied heavily on the Copyright Acts of 1790, 1831, 1909, and 1976 as precedent for retroactive extensions. One of the arguments supporting the act was that life expectancy has significantly increased among the human population since the 18th century, and therefore copyright law needed extending as well. However, the major argument for the act that carried over into the case was that the Constitution specified that Congress only needed to set time limits for copyright, the length of which was left to their discretion. Thus, as long as the limit is not \"forever\", any limit set by Congress can be deemed constitutional.\nA key factor in the CTEA's passage was a 1993 European Union (EU) directive instructing EU members to establish a baseline copyright term of life plus 70 years and to deny this longer term to the works of any non-EU country whose laws did not secure the same extended term. By extending the baseline United States copyright term, Congress sought to ensure that American authors would receive the same copyright protection in Europe as their European counterparts.\nThe Supreme Court declined to address Lessig's contention that \"Lopez\" and \"Morrison\" offered precedent for enforcing the Copyright clause, and instead reiterated the lower court's reasoning that a retroactive term extension \"can\" satisfy the \"limited Times\" provision in the Copyright Clause, as long as the extension itself is limited instead of perpetual. Furthermore, the Court refused to apply the proportionality standards of the Fourteenth Amendment or the free-speech standards in the First Amendment to limit Congress's ability to confer copyrights for limited terms.\nJustice Breyer dissented, arguing that the CTEA amounted to a grant of perpetual copyright that undermined public interests. While the constitution grants Congress power to extend copyright terms in order to \"promote the progress of science and useful arts\", CTEA granted precedent to continually renew copyright terms making them virtually perpetual. Justice Breyer argued that it is highly unlikely any artist will be more inclined to produce work knowing their great-grandchildren will receive royalties. With regard to retroactive copyright extension, he considered it foolish to apply the government's argument that income received from royalties allows artists to produce more work saying, \"How will extension help today's Noah Webster create new works 50 years after his death?\" He also attacked the idea that the fair use defense would efficiently solve the First Amendment issue, as the defense could not help \"those who wish to obtain from electronic databases material that is not there\", e.g. teachers searching online for material to be used in the class (and finding that the ideal material has been deleted from the database).\nIn a separate dissenting opinion, Justice Stevens also challenged the virtue of an individual reward, analyzing it from the perspective of patent law. He argued that the focus on compensation results only in \"frustrating the legitimate members of the public who want to make use of it (a completed invention) in a free market\". Further, the compelling need to encourage creation is proportionally diminished once a work is already created. Yet while a formula pairing commercial viability to duration of protection may be said to produce more economically efficient results in respect of high technology inventions with shorter shelf-lives, the same perhaps cannot be said for certain forms of copyrighted works, for which the present value of expenditures relating to creation depend less on scientific equipment and research and development programs and more on unquantifiable creativity.\nLessig expressed surprise that no decision was authored by Chief Justice Rehnquist or by any of the other four justices who supported the \"Lopez\" or \"Morrison\" decisions. Lessig later expressed regret that he based his argument on precedent rather than attempting to demonstrate that the weakening of the public domain would cause harm to the economic health of the country.\nLater developments.\nWithin a year of \"Eldred\", it was serving as decisive precedent. Two cases, \"Luck\u2019s Music Library, Inc. v. Ashcroft and Peters\" and \"Golan v. Ashcroft and Peters\", challenged the constitutionality of the Uruguay Round Agreements Act on the grounds that its \"restoration amendment,\" which provided copyright restriction to foreign works that were in the public domain because foreign works were formerly not copyrightable, violated the First Amendment rights of those who would no longer be able to perform the works without observing copyright. The court cited \"Eldred\" and dismissed \"Luck's Music\" on the grounds that the First Amendment did not protect the ability to use others' words as much as it does protect one's ability to use their own. \"Golan v. Ashcroft and Peters\"'s Uruguay Round portion survived a motion to dismiss even though its own challenge to the Sonny Bono Act did not. That case culminated in \"Golan v. Holder\", which held that there was nothing in the Constitution preventing the government from taking works out of the public domain.\nA 2007 case, \"Kahle v. Gonzales\", worked from the \"Eldred v. Ashcroft\" opinion to argue that a change in copyright law as drastic as the change from opt-in to opt-out required a review in regard to freedom of speech. The plaintiffs, represented by Lawrence Lessig, argued that the limitations placed on speech and expression by copyright were drastically expanded and possibly too limiting. The Ninth Circuit determined that the argument was too similar to the one adjudicated in \"Eldred\" and dismissed the case.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nExternal links.\n is available from: https:// https:// https:// https:// http:// "}
{"id": "41535", "revid": "50635548", "url": "https://en.wikipedia.org/wiki?curid=41535", "title": "Bix Beiderbecke", "text": "American jazz cornetist, pianist and composer (1903\u20131931)\nLeon Bismark \"Bix\" Beiderbecke ( ; March 10, 1903 \u2013 August 6, 1931) was an American jazz cornetist, pianist and composer. Beiderbecke was one of the most influential jazz soloists of the 1920s, a cornet player noted for an inventive lyrical approach and purity of tone, with such clarity of sound that one contemporary famously described it like \"shooting bullets at a bell\u201d.\nHis solos on seminal recordings such as \"Singin' the Blues\" and \"I'm Coming, Virginia\" (both 1927) demonstrate a gift for extended improvisation that heralded the jazz ballad style, in which jazz solos are an integral part of the composition. Moreover, his use of extended chords and an ability to improvise freely along harmonic as well as melodic lines are echoed in post-WWII developments in jazz. \"In a Mist\" (1927) is the best known of Beiderbecke's published piano compositions and the only one that he recorded. His piano style reflects both jazz and classical (mainly impressionist) influences. All five of his piano compositions were published by Robbins Music during his lifetime.\nA native of Davenport, Iowa, Beiderbecke taught himself to play the cornet largely by ear, leading him to adopt a non-standard fingering technique that informed his unique style. He first recorded with Midwestern jazz ensemble The Wolverines in 1924, after which he played briefly for the Detroit-based Jean Goldkette Orchestra before joining Frankie \"Tram\" Trumbauer for an extended engagement at the Arcadia Ballroom in St. Louis, also under the auspices of Goldkette's organisation. Beiderbecke and Trumbauer joined Goldkette's main band at the Graystone Ballroom in Detroit in 1926. The band toured widely and famously played a set opposite Fletcher Henderson at the Roseland Ballroom in New York City in October 1926. He made his greatest recordings in 1927. The Goldkette band folded in September 1927 and, after briefly joining bass saxophone player Adrian Rollini's band in New York, Trumbauer and Beiderbecke joined America's most popular dance band: Paul Whiteman and his Orchestra.\nBeiderbecke's most influential recordings date from his time with Goldkette and Whiteman, although he also recorded under his own name and that of Trumbauer's. The Whiteman period marked a precipitous decline in his health due to his increasing use of alcohol. Treatment for alcoholism in rehabilitation centers, with the support of Whiteman and the Beiderbecke family, failed to stop his decline. He left the Whiteman band in 1929 and in the summer of 1931 died aged 28 in his Sunnyside, Queens, New York apartment.\nHis death, in turn, gave rise to one of the original legends of jazz. In magazine articles, musicians' memoirs, novels, and Hollywood films, Beiderbecke has been envisaged as a Romantic hero, the \"Young Man with a Horn\" (a novel, later made into a movie starring Kirk Douglas, Lauren Bacall, Doris Day, and his friend Hoagy Carmichael). His life has often been portrayed as that of a jazz musician who had to compromise his art for the sake of commercialism. Beiderbecke remains the subject of scholarly controversy regarding his full name, the cause of his death and the importance of his contributions to jazz.\nHe composed or played on recordings that are jazz classics and standards such as \"Davenport Blues\", \"In a Mist\", \"Copenhagen\", \"Riverboat Shuffle\", \"Singin' the Blues\", and \"Georgia on My Mind\".\nEarly life.\nThe son of Bismark Herman Beiderbecke and Agatha Jane Hilton, Bix Beiderbecke was born on March 10, 1903, in Davenport, Iowa. There is disagreement over whether Beiderbecke was christened Leon Bix or Leon Bismark and nicknamed \"Bix\". His father was nicknamed \"Bix\", as was his older brother, Charles Burnette \"Burnie\" Beiderbecke. Burnie Beiderbecke claimed that the boy was named Leon Bix and biographers have reproduced birth certificates that agree. More recent research \u2014 which takes into account church and school records in addition to the will of a relative \u2014 suggests he was named Leon Bismark. Regardless, his parents called him Bix, which seems to have been his preference. In a letter to his mother when he was nine years old, Beiderbecke signed off, \"frome your Leon Bix Beiderbecke not Bismark Remeber [\"sic\"]\".\nThe son of German immigrants, Beiderbecke's father was a well-to-do coal and lumber merchant named after Otto von Bismarck of his native Germany. Beiderbecke's mother was the daughter of a Mississippi riverboat captain. She played the organ at Davenport's First Presbyterian Church and encouraged young Beiderbecke's interest in the piano.\nBeiderbecke was the youngest of three children. His brother, Burnie, was born in 1895, and his sister, Mary Louise, in 1898. He began playing piano at age two or three. His sister recalls that he stood on the floor and played it with his hands over his head. Five years later, he was the subject of an admiring article in the \"Davenport Daily Democrat\" that proclaimed, \"Seven-year-old boy musical wonder! Little Bickie Beiderbecke plays any selection he hears.\"\nBurnie recalled that he stopped coming home for supper to hurry to the riverfront, slip aboard an excursion boat, and play the calliope. A friend remembered that Beiderbecke showed little interest in the Saturday matinees they attended, but as soon as the lights came on he rushed home to duplicate the melodies the accompanist had played.\nWhen Burnie returned to Davenport at the end of 1918 after serving stateside during World War I, he brought with him a Victrola phonograph and several records, including \"Tiger Rag\" and \"Skeleton Jangle\" by the Original Dixieland Jazz Band. From these records, Beiderbecke learned to love hot jazz; he taught himself to play cornet by listening to Nick LaRocca's horn lines. He also listened to jazz from the riverboats that docked in downtown Davenport. Louis Armstrong and the drummer Baby Dodds claimed to have met Beiderbecke when their excursion boat stopped in Davenport. Historians disagree over whether such an event occurred.\nBeiderbecke attended Davenport High School from 1918 to 1921. During this time, he sat in and played professionally with various bands, including those of Wilbur Hatch, Floyd Bean, and Carlisle Evans. In the spring of 1920 he performed for the school's Vaudeville Night, singing in a vocal quintet called the Black Jazz Babies and playing his cornet. At the invitation of his friend Fritz Putzier, he subsequently joined Neal Buckley's Novelty Orchestra. The group was hired for a gig in December 1920, but a complaint was lodged with the American Federation of Musicians, Local 67, that the boys did not have union cards. In an audition before a union executive, Beiderbecke was forced to sight read and failed. He did not earn his card.\nOn April 22, 1921, a month after he turned 18, Beiderbecke was arrested by two Davenport police officers on an accusation that he had taken a five-year-old girl named Sarah Ivens into a neighbor's garage and committed a lewd and lascivious act with her\u2014a statutory felony in Iowa. According to the police ledger, the girl accused Beiderbecke of \"putting his hands on her person outside of her dress.\" The ledger went on to state that Beiderbecke and the girl \"were in an auto in the garage and he closed the door on the girl and she hollered,\" attracting the attention of two young men who were across the street. The young men \"went over [to the garage] and the girl went home.\" Beiderbecke was released after a $1,500 bail bond was posted. Sarah's father, Preston Ivens, requested that the Scott County grand jury drop the charge to avoid \"harm that would result to her in going over this case,\" and in September 1921, the grand jury returned no indictment, whereupon the County Attorney filed a dismissal of the case. It is not clear from the official documents if Sarah herself had identified Beiderbecke, but the two young men had told her father, when he questioned them a day after the alleged incident, that they had seen Beiderbecke take the girl into the garage. The surviving official documents concerning the arrest and its aftermath \u2013 including two police entries and Preston Ivens' grand jury testimony \u2013 were first made available in 2001 by Professor Albert Haim on the Bixography website. Jean Pierre Lion in his 2005 biography discussed the incident briefly and printed the texts of the documents. Earlier biographies had not reported the alleged incident.\nIn September 1921, Beiderbecke enrolled at the Lake Forest Academy, a boarding school north of Chicago in Lake Forest, Illinois. While historians have traditionally suggested that his parents sent him to Lake Forest to discourage his interest in jazz, others believe that he may have been sent away in response to his arrest. Regardless, Mr. and Mrs. Beiderbecke apparently felt that a boarding school would provide their son with both the faculty attention and discipline required to improve his academic performance, necessitated by the fact that Bix had failed most courses in high school, remaining a junior in 1921 despite turning 18 in March of that year. His interests, however, remained limited to music and sports. In pursuit of the former, Beiderbecke often visited Chicago to listen to jazz bands at night clubs and speakeasies, including the infamous Friar's Inn, where he sometimes sat in with the New Orleans Rhythm Kings. He also traveled to the predominantly African-American South Side to listen to classic black jazz bands such as King Oliver's Creole Jazz Band, which featured Louis Armstrong on second cornet. \"Don't think I'm getting hard, Burnie,\" he wrote to his brother, \"but I'd go to hell to hear a good band.\" On campus, he helped organize the Cy-Bix Orchestra with drummer Walter \"Cy\" Welge and almost immediately got into trouble with the Lake Forest headmaster for performing indecorously at a school dance.\nBeiderbecke often failed to return to his dormitory before curfew, and sometimes stayed off-campus the next day. In the early morning hours of May 20, 1922, he was caught on the fire escape to his dormitory, attempting to climb back into his room. The faculty voted to expel him the next day, due both to his academic failings and his extracurricular activities, which included drinking. The headmaster informed Beiderbecke's parents by letter that following his expulsion school officials confirmed that Beiderbecke \"was drinking himself and was responsible, in part at least, in having liquor brought into the School.\" Soon after, Beiderbecke began pursuing a career in music.\nHe returned to Davenport briefly in the summer of 1922, then moved to Chicago to join the Cascades Band, working that summer on Lake Michigan excursion boats. He gigged around Chicago until the fall of 1923, at times returning to Davenport to work for his father.\nCareer.\nWolverines.\nBeiderbecke joined the Wolverine Orchestra late in 1923, and the seven-man group first played a speakeasy called the Stockton Club near Hamilton, Ohio. Specializing in hot jazz and recoiling from so-called sweet music, the band took its name from one of its most frequent numbers, Jelly Roll Morton's \"Wolverine Blues.\" During this time, Beiderbecke also took piano lessons from a young woman who introduced him to the works of Eastwood Lane. Lane's piano suites and orchestral arrangements were self-consciously American whilst also having French Impressionist allusions, and influenced Beiderbecke's style, especially on \"In a Mist.\" A subsequent gig at Doyle's Dance Academy in Cincinnati became the occasion for a series of band and individual photographs that resulted in the image of Beiderbecke\u2014sitting fresh-faced, his hair perfectly combed and his cornet resting on his right knee.\nOn February 18, 1924, the Wolverines made their first recordings. Two sides were waxed that day at the Gennett Records studios in Richmond, Indiana: \"Fidgety Feet\", written by Nick LaRocca and Larry Shields from the Original Dixieland Jazz Band, and \"Jazz Me Blues\", written by Tom Delaney. Beiderbecke's solo on the latter heralded something new and significant in jazz, according to biographers Richard M. Sudhalter and Philip R. Evans:\nBoth qualities\u2014complementary or \"correlated\" phrasing and cultivation of the vocal, \"singing\" middle-range of the cornet\u2014are on display in Bix's \"Jazz Me Blues\" solo, along with an already discernible inclination for unusual accidentals and inner chordal voices. It is a pioneer record, introducing a musician of great originality with a pace-setting band. And it astonished even the Wolverines themselves.\nThe Wolverines recorded 15 sides for Gennett Records between February and October 1924. The titles revealed a strong and well-formed cornet talent. His lip had strengthened from earlier, more tentative years; on nine of the Wolverines' recorded titles he proceeds commandingly from lead to opening solo without any need for a respite from playing.\nIn some respects, Beiderbecke's playing was \"sui generis\", but he nevertheless listened to, and learned from, the music around him: from the Dixieland jazz as exemplified by the Original Dixieland Jazz Band; to the hotter Chicago style of the New Orleans Rhythm Kings and the south-side bands of King Oliver and other black artists; to the classical compositions of Claude Debussy and Maurice Ravel.\nLouis Armstrong also provided a source of inspiration, though Beiderbecke's style was very different from that of Armstrong, according to \"The Oxford Companion to Jazz\":Where Armstrong's playing was bravura, regularly optimistic, and openly emotional, Beiderbecke's conveyed a range of intellectual alternatives. Where Armstrong, at the head of an ensemble, played it hard, straight, and true, Beiderbecke, like a shadowboxer, invented his own way of phrasing \"around the lead.\" Where Armstrong's superior strength delighted in the sheer power of what a cornet could produce, Beiderbecke's cool approach invited rather than commanded you to listen.Armstrong tended to accentuate showmanship and virtuosity, whereas Beiderbecke emphasized melody, even when improvising, and rarely strayed into the upper reaches of the register. Mezz Mezzrow recounted in his autobiography driving 53 miles to Hudson Lake, Indiana, with Frank Teschemacher in order to play Armstrong's \"Heebie Jeebies\" for Beiderbecke when it was released. In addition to listening to Armstrong's records, Beiderbecke and other white musicians patronized the Sunset Caf\u00e9 on Fridays to listen to Armstrong and his band. Paul Mares of the New Orleans Rhythm Kings insisted that Beiderbecke's chief influence was the New Orleans cornetist Emmett Hardy, who died in 1925 at the age of 23. Indeed, Beiderbecke had met Hardy and the clarinetist Leon Roppolo in Davenport in 1921 when the two joined a local band and played in town for three months. Beiderbecke apparently spent time with them, but it is difficult to discern the degree to which Hardy's style influenced Beiderbecke's\u2014especially since there is no publicly known recording of a Hardy performance.\nBeiderbecke certainly found a kindred musical spirit in Hoagy Carmichael, whose amusingly unconventional personality he also appreciated. The two became firm friends. A law student and aspiring pianist and songwriter, Carmichael invited the Wolverines to play at the Bloomington campus of Indiana University in the spring of 1924. On May 6, 1924, the Wolverines recorded a tune Carmichael had written especially for Beiderbecke and his colleagues: \"Riverboat Shuffle\".\nGoldkette.\nDuring an engagement at the Cinderella Ballroom in New York during September\u2013October 1924, Bix tendered his resignation with the Wolverines, leaving to join Jean Goldkette and his Orchestra in Detroit, but Beiderbecke's tenure with the band proved to be short-lived. Goldkette recorded for the Victor Talking Machine Company, whose musical director, Eddie King, objected to Beiderbecke's modernistic style of jazz playing. Moreover, despite the fact that Beiderbecke's position within the Goldkette band was \"third trumpet\", a less taxing role than 1st or 2nd trumpet, he struggled with the complex ensemble passages due to his limited reading abilities. After a few weeks, Beiderbecke and Goldkette agreed to part company, but to keep in touch, with Goldkette advising Beiderbecke to brush up on his reading and learn more about music. Some six weeks after leaving the band, Bix arranged a Gennett recording session back in Richmond with some of the Goldkette band members, under the name Bix and His Rhythm Jugglers. On January 26, 1925, they set two tunes to wax: \"Toddlin' Blues\", another number by LaRocca and Shields, and Beiderbecke's own composition, \"Davenport Blues\", which subsequently became a classic jazz number, recorded by musicians ranging from Bunny Berigan to Ry Cooder and Geoff Muldaur. An arrangement of \"Davenport Blues\" as a piano solo was published by Robbins Music in 1927.\nIn February 1925, Beiderbecke enrolled at the University of Iowa in Iowa City. His stint in academia was even briefer than his time in Detroit, however. When he attempted to pack his course schedule with music, his guidance counselor forced him instead to take religion, ethics, physical education, and military training. It was an institutional blunder that Benny Green described as being, in retrospect, \"comical,\" \"fatuous,\" and \"a parody.\" Beiderbecke promptly began to skip classes, and after he participated in a drunken incident in a local bar, he was expelled. According to Lion, he was not expelled, but quit. That summer he played with his friends Don Murray and Howdy Quicksell at a lake resort in Michigan. The band was run by Goldkette, and it put Beiderbecke in touch with another musician he had met before: the C-melody saxophone player Frankie Trumbauer. The two hit it off, both personally and musically, despite Trumbauer having been warned by other musicians: \"Look out, he's trouble. He drinks and you'll have a hard time handling him.\" They were inseparable for much of the rest of Beiderbecke's career, with Trumbauer acting as something of a guardian to Beiderbecke. When Trumbauer organized a band for an extended run at the Arcadia Ballroom in St. Louis, Beiderbecke joined him. There he also played alongside the clarinetist Pee Wee Russell, who praised Beiderbecke's ability to drive the band. \"He more or less made you play whether you wanted to or not,\" Russell said. \"If you had any talent at all he made you play better.\"\nIn the spring of 1926, Bix and Trumbauer joined Goldkette's main dance band, splitting the year between playing a Summer season at a Goldkette-owned resort on Lake Hudson, Indiana, and headlining at Detroit's Graystone Ballroom, which was also owned by Goldkette. In October 1926, Goldkette's \"Famous Fourteen\", as they came to be called, opened at the Roseland Ballroom in New York City opposite the Fletcher Henderson Orchestra, one of the East Coast's outstanding African American big bands. The Roseland promoted a \"Battle of the Bands\" in the local press and, on October 12, after a night of furious playing, Goldkette's men were declared the winners. \"We [\u2026] were amazed, angry, morose, and bewildered,\" Rex Stewart, Fletcher's lead trumpeter, said of listening to Beiderbecke and his colleagues play. He called the experience \"most humiliating\". On October 15, 1931, a few months after Beiderbecke's death, the Fletcher Henderson Orchestra recorded a version of \"Singin' the Blues\" that included Rex Stewart performing a nearly note-for-note homage to Beiderbecke's most famous solo.\nAlthough the Goldkette Orchestra recorded numerous sides for Victor during this period, none of them showcases Beiderbecke's most famous solos. The band found itself subjected to the commercial considerations of the popular music sector that Victor deliberately targeted the band's recordings at. The few exceptions to the policy include \"My Pretty Girl\" and \"Clementine\", the latter being one of the band's final recordings and its effective swan song. In addition to these commercial sessions with Goldkette, Beiderbecke and Trumbauer also recorded under their own names for the OKeh label; Bix waxed some of his best solos as a member of Trumbauer's recording band, starting with \"Clarinet Marmalade\" and \"Singin' the Blues\", recorded on February 4, 1927. Again with Trumbauer, Beiderbecke re-recorded Carmichael's \"Riverboat Shuffle\" in May and delivered two further seminal solos a few days later on \"I'm Coming, Virginia\" and \"Way Down Yonder in New Orleans\". Beiderbecke earned co-writing credit with Trumbauer on \"For No Reason at All in C\", recorded under the name Tram, Bix and Eddie (in their Three Piece Band). Beiderbecke switched between cornet and piano on that number, and then in September played only piano for his recording of \"In A Mist\". This was perhaps the most fruitful year of his short career.\nUnder financial pressure, Goldkette folded his premier band in September 1927 in New York. Paul Whiteman hoped to snatch up Goldkette's best musicians for his traveling orchestra, but Beiderbecke, Trumbauer, Murray, Bill Rank, Chauncey Morehouse, and Frank Signorelli instead joined the bass saxophone player Adrian Rollini at the Club New Yorker. The band also included guitarist Eddie Lang and violinist Joe Venuti, who had often recorded on a freelance basis with the Goldkette Orchestra. Another newcomer was Sylvester Ahola, a schooled trumpeter who could play improvised jazz solos and read complex scores. When Ahola introduced himself, Beiderbecke famously stated \"Hell, I'm only a musical degenerate\". When that job ended sooner than expected, in October 1927, Beiderbecke and Trumbauer signed on with Whiteman. They joined his orchestra in Indianapolis on October 27.\nWhiteman.\nThe Paul Whiteman Orchestra was the most popular and highest paid dance band of the day. In spite of Whiteman's appellation \"The King of Jazz\", his band was not a jazz ensemble as such, but a popular music outfit that drew from both jazz and classical music repertoires, according to the demands of its record-buying and concert-going audience. Whiteman was perhaps best known for having premiered George Gershwin's \"Rhapsody in Blue\" in New York in 1924, and the orchestrator of that piece, Ferde Grof\u00e9, continued to be an important part of the band throughout the 1920s. Whiteman was large physically and important culturally \u2014\"a man flabby, virile, quick, coarse, untidy and sleek, with a hard core of shrewdness in an envelope of sentimentalism\", according to a 1926 \"New Yorker\" profile. A number of Beiderbecke partisans have criticised Whiteman for not giving Bix the opportunities he deserved as a jazz musician. James complains that, after Beiderbecke joined the band, \"Whiteman moved farther and farther away from the easy-going, rhythmically inclined style of his earlier days\", becoming \"more subservient to his business sense\". He goes on to suggest that this artistically compromised Beiderbecke, in part causing his death.\nBenny Green, in particular, derided Whiteman for being a mere \"mediocre vaudeville act\", and suggesting that \"today we only tolerate the horrors of Whiteman's recordings at all in the hope that here and there a Bixian fragment will redeem the mess.\" Richard Sudhalter has responded by suggesting that Beiderbecke saw the Whiteman band as an opportunity to pursue musical ambitions that did not stop at jazz:\nColleagues have testified that, far from feeling bound or stifled by the Whiteman orchestra, as Green and others have suggested, Bix often felt a sense of exhilaration. It was like attending a music school, learning and broadening: formal music, especially the synthesis of the American vernacular idiom with a more classical orientation, so much sought-after in the 1920s, were calling out to him.\nBeiderbecke is featured on a number of Whiteman recordings, including \"From Monday On\", \"Back In Your Own Back Yard\", \"You Took Advantage Of Me\", \"Sugar\", \"Changes\" and \"When\". These feature specially written arrangements that emphasize Beiderbecke's improvisational skills. Bill Challis, an arranger who had also worked in this capacity for Jean Goldkette, was particularly sympathetic in writing scores with Beiderbecke in mind, sometimes arranging entire ensemble passages based on solos that Bix played. Beiderbecke also played on several notable hit records recorded by Whiteman, such as \"Together\", \"Ramona\" and \"Ol' Man River\", the latter featuring Bing Crosby on vocals.\nThe heavy touring and recording schedule with Whiteman's orchestra may have exacerbated Beiderbecke's long-term alcoholism, though this is a contentious point. Whiteman's violinist Matty Malneck said \"The work was so hard, you almost had to drink\" adding \"He didn't get to play the things he loved with the Whiteman band because we were a symphonic band and we played the same thing every night, and it got to be tiresome.\"\nOn November 30, 1928, whilst on tour in Cleveland, Beiderbecke suffered what Lion terms \"a severe nervous crisis\" and Sudhalter and Evans suggest \"was in all probability an acute attack of delirium tremens\", presumably triggered by Beiderbecke's attempt to curb his alcohol intake. \"He cracked up, that's all\", trombonist Bill Rank said. \"Just went to pieces; broke up a roomful of furniture in the hotel.\"\nIn February 1929, Beiderbecke returned home to Davenport to convalesce and was hailed by the local press as \"the world's hottest cornetist\". He then spent the summer with Whiteman's band in Hollywood in preparation for the shooting of a new talking picture, \"The King of Jazz\". Production delays prevented any real work from being done on the film, leaving Beiderbecke and his pals plenty of time to drink heavily. By September, he was back in Davenport, where his parents helped him to seek treatment. He spent a month, from October 14 until November 18, at the Keeley Institute in Dwight, Illinois. According to Lion, an examination by Keeley physicians confirmed the damaging effects of Bix's long-term reliance on alcohol: \"Bix admitted to having used liquor 'in excess' for the past nine years, his daily dose over the last three years amounting to three pints of 'whiskey' and twenty cigarettes...A Hepatic dullness was obvious, 'knee jerk could not be obtained' \u2013 which confirmed the spread of the polyneuritis, and Bix was 'swaying in Romberg position' \u2013 standing up with his eyes closed\".\nWhile he was away, Whiteman famously kept his chair open in Beiderbecke's honor, in the hope that he would occupy it again. However, when he returned to New York at the end of January 1930, Beiderbecke did not rejoin Whiteman and performed only sparingly. On his last recording session, in New York, on September 15, 1930, Beiderbecke played on the original recording of Hoagy Carmichael's new song, \"Georgia on My Mind\", with Carmichael doing the vocals, Eddie Lang on guitar, Joe Venuti on violin, Jimmy Dorsey on clarinet and alto saxophone, Jack Teagarden on trombone, and Bud Freeman on tenor saxophone. The song would go on to become a jazz and popular music standard. In 2014, the 1930 recording of \"Georgia on My Mind\" was inducted into the Grammy Hall of Fame.\nBeiderbecke's playing had an influence on Carmichael as a composer. One of his compositions, \"Stardust\", was inspired by Beiderbecke's improvisations, with a cornet phrase reworked by Carmichael into the song's central theme. Bing Crosby, who sang with Whiteman, also cited Beiderbecke as an important influence. \"Bix and all the rest would play and exchange ideas on the piano\", he said.\nWith all the noise [of a New York pub] going on, I don't know how they heard themselves, but they did. I didn't contribute anything, but I listened and learned [\u2026] I was now being influenced by these musicians, particularly horn men. I could hum and sing all of the jazz choruses from the recordings made by Bix, Phil Napoleon, and the rest.\nFollowing the Wall Street Crash of 1929, the once-booming music industry contracted and work became more difficult to find. For a while, Beiderbecke's only regular income came from his work as a member of Nat Shilkret's orchestra on \"The Camel Pleasure Hour\" NBC radio show. However, during a live broadcast on October 8, 1930, Beiderbecke's seemingly limitless gift for improvisation finally failed him: \"He stood up to take his solo, but his mind went blank and nothing happened\", recalled a fellow musician, Frankie Cush. The cornetist spent the rest of the year at home in Davenport and then, in February 1931, he returned to New York one last time.\nDeath.\nBeiderbecke died in his apartment, No.\u00a01G, 43\u201330 46th Street, in Sunnyside, Queens, New York, on August 6, 1931. The week had been stiflingly hot, making sleep difficult. Suffering from insomnia, Beiderbecke played the piano late into the evenings, to both the annoyance and the delight of his neighbors. On the evening of August 6, at about 9:30 pm, his rental agent, George Kraslow, heard noises coming from across the hallway. \"His hysterical shouts brought me to his apartment on the run,\" Kraslow told Philip Evans in 1959, continuing:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;He pulled me in and pointed to the bed. His whole body was trembling violently. He was screaming there were two Mexicans hiding under his bed with long daggers. To humor him, I looked under the bed and when I rose to assure him there was no one hiding there, he staggered and fell, a dead weight, in my arms. I ran across the hall and called in a woman doctor, Dr. Haberski, to examine him. She pronounced him dead.\nHistorians have disagreed over the identity of the doctor who pronounced Beiderbecke dead, with several sources stating that it was Dr. John Haberski (the husband of the woman Kraslow identified) who pronounced Beiderbecke dead in his apartment. The official cause of death, as indicated on the death certificate, was lobar pneumonia. Unofficially, edema of the brain, coupled with the effects of long-term alcoholism, have been cited as contributory factors. Beiderbecke's mother and brother took the train to New York and arranged for his body to be taken home to Davenport. He was buried there on August 11, 1931, in the family plot at Oakdale Cemetery.\nLegend and legacy.\nCritical analysis of Beiderbecke's work during his lifetime was sparse. His innovative playing initially received greater attention and appreciation among European critics than those in the country of his birth. The British music trade magazine \"Melody Maker\" published a number of reviews of his recordings and assessments of his cornet playing. In the April 1927 issue, bandleader Fred Elizalde stated: \"Bix Bidlebeck (sic) is considered by Red Nichols himself and every other trumpet player in the States, for that matter, as the greatest trumpet player of all time\". The magazine's editor, Edgar Jackson, was equally fulsome in his praise: \"Bix has a heart as big as your head, which shines through his playing with the warmth of the sun's rays\" (September 1927 issue); \"The next sixteen bars are a trumpet solo by Bix, and if this doesn't get you right in the heart, you'd better see a vet\u2026.\"\nAt the time of his death, Beiderbecke was still little known by the public at large, though his appreciation among fellow musicians and the collegiate set is indicated by contemporary news reports:To a large circle of those boys and girls of high school and college age whom a staid world likes to label \"the jazz-mad generation,\" the news that Leon Bix Beiderbecke is dead will mean something, however lacking in significance it might be to their critical elders. \"Bixie\" was a symbol of that jazz generation, expressing its wistful, restless temperament through the medium of the unconventional dance music which constitutes its theme song. In his mind were conceived the wild, strange contortions of rhythm and harmony which established the basic motif of the popular music of a year ago...To most youngsters in college, however, the weird flourishes that \"Bixie's\" fingers executed on trumpet and piano were expressive. They could hear the lilting melody of youth that formed a smooth background for his fantastic caricatures in sound. Hundreds of young collegians who couldn't recall a strain of Beethoven or Wagner could whistle Bix Beiderbecke choruses. In the world of professional popular music, \"Bixie\" was an artist comparable to Kreisler in the field of conventional music. Paul Whiteman called him \"the finest trumpet player in the country\".Perhaps \"Bixie's\" death at the age of twenty-eight also is symbolical of the futility of the \"jazz-mad generation's\" quest for self-expression. However that may be, if it is true, as some critics contend, that \"jazz\" music is establishing foundations on which a distinctive and thoroughly legitimate American music eventually will be built, Bix Beiderbecke has left his mark on the future culture of the nation.One of the first serious, analytical obituaries to have been published in the months after his death was by the French jazz writer Hugues Panassi\u00e9. The notice appeared in October 1931.\nThe \"New Republic\" critic Otis Ferguson wrote two short articles for the magazine, \"Young Man with a Horn\" and \"Young Man with a Horn Again\", that worked to revive interest not only in Beiderbecke's music but also in his biography. Beiderbecke \"lived very briefly [\u2026] in what might be called the servants' entrance to art\", Ferguson wrote. \"His story is a good story, quite humble and right.\" The romantic notion of the short-lived, doomed jazz genius can be traced back at least as far as Beiderbecke, and lived on in Glenn Miller, Charlie Parker, Billie Holiday, Jaco Pastorius and many more.\nFerguson's sense of what was \"right\" became the basis for the Beiderbecke Romantic legend, which has traditionally emphasized the musician's Iowa roots, his often careless dress, his difficulty sight reading, the purity of his tone, his drinking, and his early death. These themes were repeated by Beiderbecke's friends in various memoirs, including \"The Stardust Road\" (1946) and \"Sometimes I Wonder\" (1965) by Hoagy Carmichael, \"Really the Blues\" (1946) by Mezz Mezzrow, and \"We Called It Music\" (1947) by Eddie Condon. Beiderbecke was portrayed as a tragic genius along the lines of Ludwig van Beethoven. \"For his talent there were no conservatories to get stuffy in, no high-trumpet didoes to be learned doggedly, note-perfect as written,\" Ferguson wrote, \"because in his chosen form the only writing of any account was traced in the close shouting air of Royal Gardens, Grand Pavilions, honkeytonks, etc.\" He was \"this big overgrown kid, who looked like he'd been snatched out of a cradle in the cornfields\", Mezzrow wrote. \"The guy didn't have an enemy in the world,\" recalled fellow musician Russ Morgan, \"[b]ut he was \"out of this world\" most of the time.\" [Italics in original.] According to Ralph Berton, he was \"as usual gazing off into his private astronomy\", but his cornet, Condon famously quipped, sounded \"like a girl saying yes\".\nIn 1938, Dorothy Baker borrowed the title of her friend Otis Ferguson's first article and published the novel \"Young Man with a Horn\". Her story of the doomed trumpet player Rick Martin was inspired, she wrote, by \"the music, but not the life\" of Beiderbecke, but the image of Martin quickly became the image of Beiderbecke: his story is about \"the gap between the man's musical ability and his ability to fit it to his own life.\" In 1950, Michael Curtiz directed the film \"Young Man with a Horn\", starring Kirk Douglas, Lauren Bacall, and Doris Day. In this version, in which Hoagy Carmichael also plays a role, the Rick Martin character lives.\nIn \"Blackboard Jungle\", a 1955 film starring Glenn Ford and Sidney Poitier, Beiderbecke's music is briefly featured, but as a symbol of cultural conservatism in a nation on the cusp of the rock and roll revolution.\nBrendan Wolfe, the author of \"Finding Bix\", spoke of Beiderbecke's lasting influence on Davenport, Iowa: \"His name and face are still a huge part of the city's identity. There's an annual Bix Beiderbecke Memorial Jazz Festival, and a Bix 7 road race with tens of thousands of runners, Bix T-shirts, bumper stickers, bobble-head dolls, the whole works.\" In 1971, on the 40th anniversary of Beiderbecke's death, the Bix Beiderbecke Memorial Jazz Festival was founded in Davenport, Iowa, to honor the musician. In 1974, Sudhalter and Evans published their biography, \"Bix: Man and Legend\", which was nominated for a National Book Award. In 1977, the Beiderbecke childhood home at 1934 Grand Avenue in Davenport was added to the National Register of Historic Places.\nA dance piece by Twyla Tharp was created in 1971 to music by Bix Biederbecke with The Paul Whiteman Orchestra. Originally titled \"True Confessions\", it was later named \"The Bix Pieces.\"\n\"Bix: 'Ain't None of Them Play Like Him Yet\", a 1981 film documentary on Beiderbecke's life directed and produced by Brigitte Berman, featured interviews with Hoagy Carmichael, Bill Challis and others, who knew and worked with Bix.\nBeiderbecke's music was featured in three British comedy drama television series, all written by Alan Plater: \"The Beiderbecke Affair\" (1984), \"The Beiderbecke Tapes\" (1987), and \"The Beiderbecke Connection\" (1988). In 1991, the Italian director Pupi Avati released \"Bix: An Interpretation of a Legend\". Filmed partially in the Beiderbecke home, which Avati had purchased and renovated, \"Bix\" was screened at the Cannes Film Festival.\nAt the beginning of the 21st century, Beiderbecke's music continued to reside mostly out of the mainstream and some of the facts of his life are still debated, but scholars largely agree \u2014 due in part to the influence of Sudhalter and Evans \u2014 that he was an important innovator in early jazz; jazz cornetists, including Sudhalter (who died in 2008), and Tom Pletcher, closely emulate his style. In 2003, to mark the hundredth anniversary of his birth, the Greater Astoria Historical Society and other community organizations, spearheaded by Paul Maringelli and The Bix Beiderbecke Sunnyside Memorial Committee, erected a plaque in Beiderbecke's honor at the apartment building in which he died in Queens. That same year, Frederick Turner published his novel \"1929\", which followed the facts of Beiderbecke's life fairly closely, focusing on his summer in Hollywood and featuring appearances by Al Capone and Clara Bow. The critic and musician Digby Fairweather sums up Beiderbecke's musical legacy, arguing that \"with Louis Armstrong, Bix Beiderbecke was the most striking of jazz's cornet (and of course, trumpet) fathers; a player who first captivated his 1920s generation and after his premature death, founded a dynasty of distinguished followers beginning with Jimmy McPartland and moving on down from there.\"\nMusic.\nStyle and influence.\nIn New Orleans, jazz had traditionally been expressed through polyphonic ensemble playing, with the various instruments weaving their parts into a single and coherent aural tapestry. By the early 1920s, developments in jazz saw the rise of the jazz soloist, with solos becoming longer and more complex. Both Beiderbecke and Armstrong were key figures in this evolution, as can be heard on their earliest recordings. According to the critic Terry Teachout, they are \"the two most influential figures in the early history of jazz\" and \"the twin lines of descent from which most of today's jazz can be traced.\"\nBeiderbecke's cornet style is often described by contrasting it with Armstrong's markedly different approach. Armstrong was a virtuoso on his instrument, and his solos often took advantage of that fact. Beiderbecke was largely, although not completely, self-taught, and the constraints imposed by that fact were evident in his music. While Armstrong often soared into the upper register, Beiderbecke stayed in the middle range, more interested in exploring the melody and harmonies than in dazzling the audience. Armstrong often emphasized the performance aspect of his playing, while Beiderbecke tended to stare at his feet while playing, uninterested in personally engaging his listeners. Armstrong was deeply influenced by the blues, while Beiderbecke was influenced as much by modernist composers such as Debussy and Ravel as by his fellow jazzmen.\nBeiderbecke's most famous solo was on \"Singin' the Blues\", recorded February 4, 1927. It has been hailed as an important example of the \"jazz ballad style\"\u2014\"a slow or medium-tempo piece played gently and sweetly, but not cloyingly, with no loss of muscle.\" The tune's laid-back emotions hinted at what would become, in the 1950s, the cool jazz style, personified by Chet Baker and Bill Evans. More than that, though, \"Singin' the Blues\" has been noted for the way its improvisations feel less improvised than composed, with each phrase building on the last in a logical fashion. Benny Green describes the solo's effect on practiced ears:\nWhen a musician hears Bix's solo on 'Singing the Blues', he becomes aware after two bars that the soloist knows exactly what he is doing and that he has an exquisite sense of discord and resolution. He knows also that this player is endowed with the rarest jazz gift of all, a sense of form which lends to an improvised performance a coherence which no amount of teaching can produce. The listening musician, whatever his generation or his style, recognizes Bix as a modern, modernism being not a style but an attitude.\nLike Green, who made particular mention of Beiderbecke's \"amount of teaching,\" the jazz historian Ted Gioia also has emphasized Beiderbecke's lack of formal instruction, suggesting that it caused him to adopt \"an unusual, dry embouchure\" and \"unconventional fingerings,\" which he retained for the rest of his life. Gioia points to \"a characteristic streak of obstinacy\" in Beiderbecke that provokes \"this chronic disregard of the tried-and-true.\" He argues that this stubbornness was behind Beiderbecke's decision not to switch from cornet to trumpet when many other musicians, including Armstrong, did so. In addition, Gioia highlights Beiderbecke's precise timing, relaxed delivery, and pure tone, which contrasted with \"the dirty, rough-edged sound\" of King Oliver and his prot\u00e9g\u00e9 Armstrong, whose playing was often more energetic and whose style held more sway early in the 1920s than Beiderbecke's.\nBeiderbecke's playing \u2013 both as a cornetist and a pianist \u2013 had a profound effect on a number of his contemporaries. Eddie Condon, for instance, described Beiderbecke's cornet playing as \"like a girl saying yes\" and also wrote of being amazed by Beiderbecke's piano playing: \"All my life I had been listening to music [\u2026] But I had never heard anything remotely like what Beiderbecke played. For the first time I realized music isn't all the same, it had become an entirely new set of sounds\" \"I tried to explain Bix to the gang,\" Hoagy Carmichael wrote, but \"[i]t was no good, like the telling of a vivid, personal dream [\u2026] the emotion couldn't be transmitted.\"\nMezz Mezzrow described Beiderbecke's tone as being \"pickled in alcohol [\u2026] I have never heard a tone like he got before or since. He played mostly open horn, every note full, big, rich and round, standing out like a pearl, loud but never irritating or jangling, with a powerful drive that few white musicians had in those days.\"\nSome critics have highlighted \"Jazz Me Blues\", recorded with the Wolverines on February 18, 1924, as being particularly important to understanding Beiderbecke's style. Although it was one of his earliest recordings, the hallmarks of his playing are evident. \"The overall impression we get from this solo, as in all of Bix at his best,\" writes the trumpeter Randy Sandke, \"is that every note is spontaneous yet inevitable.\" Richard Hadlock describes Beiderbecke's contribution to \"Jazz Me Blues\" as \"an ordered solo that seems more inspired by clarinetists Larry Shields of the ODJB and Leon Roppolo of the NORK than by other trumpet players.\" He goes on to suggest that clarinetists, by virtue of their not being tied to the melody as much as cornetists and trumpet players, could explore harmonies.\n\"Jazz Me Blues\" was also important because it introduced what has been called the \"correlated chorus\", a method of improvising that Beiderbecke's Davenport friend Esten Spurrier attributed to both Beiderbecke and Armstrong. \"Louis departed greatly from all cornet players in his ability to compose a close-knit individual 32 measures with all phrases compatible with each other\", Spurrier told the biographers Sudhalter and Evans, \"so Bix and I always credited Louis as being the father of the correlated chorus: play two measures, then two related, making four measures, on which you played another four measures related to the first four, and so on ad infinitum to the end of the chorus. So the secret was simple\u2014a series of related phrases.\"\nBeiderbecke plays piano on his recordings \"Big Boy\" (October 8, 1924), \"For No Reason at All in C\" (May 13, 1927), \"Wringin' and Twistin'\" (September 17, 1927)\u2014all with ensembles\u2014and his only solo recorded work, \"In a Mist\" (September 8, 1927). Critic Frank Murphy argues that many of the same characteristics that mark Beiderbecke on the cornet are also reflected in his piano playing: the uncharacteristic fingering, the emphasis on inventive harmonies, and the correlated choruses. Those inventive harmonies, on both cornet and piano, pointed the way to future developments in jazz, particularly bebop.\nCompositions.\nBix Beiderbecke wrote or co-wrote six instrumental compositions during his career:\n\"Candlelights\", \"Flashes\", and \"In the Dark\" are piano compositions transcribed with the help of Bill Challis but never recorded by Beiderbecke. Two additional compositions were attributed to him by two other jazz composers: \"Betcha I Getcha\", attributed to Beiderbecke as a co-composer by Joe Venuti, the composer of the song, and \"Cloudy\", attributed to Beiderbecke by composer Charlie Davis as a composition from circa 1924.\nGrammy Hall of Fame.\nBix Beiderbecke was posthumously inducted into the Grammy Hall of Fame, which is a special Grammy award established in 1973 to honor recordings that are at least 25 years old and that have \"qualitative or historical significance.\"\nReferences.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nCitations.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41536", "revid": "2842084", "url": "https://en.wikipedia.org/wiki?curid=41536", "title": "Duke Ellington", "text": "American jazz pianist and composer (1899\u20131974)\nEdward Kennedy \"Duke\" Ellington (April 29, 1899\u00a0\u2013 May 24, 1974) was an American jazz pianist, composer, and leader of his eponymous jazz orchestra from 1924 through the rest of his life.\nBorn and raised in Washington, D.C., Ellington was based in New York City from the mid-1920s and gained a national profile through his orchestra's appearances at the Cotton Club in Harlem. A master at writing miniatures for the three-minute 78\u00a0rpm recording format, Ellington wrote or collaborated on more than one thousand compositions; his extensive body of work is the largest recorded personal jazz legacy, and many of his pieces have become standards. He also recorded songs written by his bandsmen, such as Juan Tizol's \"Caravan\", which brought a Spanish tinge to big band jazz.\nAt the end of the 1930s, Ellington began a nearly thirty five-year collaboration with composer-arranger-pianist Billy Strayhorn, whom he called his writing and arranging companion. With Strayhorn, he composed multiple extended compositions, or suites, as well as many short pieces. For a few years at the beginning of Strayhorn's involvement, Ellington's orchestra featured bassist Jimmy Blanton and tenor saxophonist Ben Webster, and reached what many claim to be a creative peak for the group. Some years later following a low-profile period, an appearance by Ellington and his orchestra at the Newport Jazz Festival in July 1956 led to a major revival and regular world tours. Ellington recorded for most American record companies of his era, performed in and scored several films, and composed a handful of stage musicals.\nAlthough a pivotal figure in the history of jazz, in the opinion of Gunther Schuller and Barry Kernfeld, \"the most significant composer of the genre\", Ellington himself embraced the phrase \"beyond category\", considering it a liberating principle, and referring to his music as part of the more general category of American Music. Ellington was known for his inventive use of the orchestra, or big band, as well as for his eloquence and charisma. He was awarded a posthumous Pulitzer Prize Special Award for music in 1999.\nEarly life and education.\nEllington was born on April 29, 1899, to James Edward Ellington and Daisy (n\u00e9e Kennedy) Ellington in Washington, D.C. Both his parents were pianists. Daisy primarily played parlor songs, and James preferred operatic arias. They lived with Daisy's parents at 2129 Ida Place (now Ward Place) NW, in D.C.'s West End neighborhood. Duke's father was born in Lincolnton, North Carolina, on April 15, 1879, and in 1886, moved to D.C. with his parents. Daisy Kennedy was born in Washington, D.C., on January 4, 1879, the daughter of two former American slaves. James Ellington made blueprints for the United States Navy.\nWhen Ellington was a child, his family showed racial pride and support in their home, as did many other families. African Americans in D.C. worked to protect their children from the era's Jim Crow laws.\nAt the age of seven, Ellington began taking piano lessons from Marietta Clinkscales. Daisy surrounded her son with dignified women to reinforce his manners and teach him elegance. His childhood friends noticed that his casual, offhand manner and dapper dress gave him the bearing of a young nobleman, so they began calling him \"Duke\". Ellington credited his friend Edgar McEntee for the nickname: \"I think he felt that in order for me to be eligible for his constant companionship, I should have a title. So he called me Duke.\"\nThough Ellington took piano lessons, he was more interested in baseball. \"President [Theodore] Roosevelt would come on his horse sometimes, and \"stop and watch us play\", he recalled. Ellington went to Armstrong Technical High School in Washington, D.C. His first job was selling peanuts at Washington Senators baseball games.\nEllington started sneaking into Frank Holiday's Poolroom at age fourteen. Hearing the music of the poolroom pianists ignited Ellington's love for the instrument, and he began to take his piano studies seriously. Among the many piano players he listened to were Doc Perry, Lester Dishman, Louis Brown, Turner Layton, Gertie Wells, Clarence Bowser, Sticky Mack, Blind Johnny, Cliff Jackson, Claude Hopkins, Phil Wurd, Caroline Thornton, Luckey Roberts, Eubie Blake, Joe Rochester, and Harvey Brooks.\nIn the summer of 1914, while working as a soda jerk at the Poodle Dog Caf\u00e9, Ellington wrote his first composition, \"Soda Fountain Rag\" (also known as the \"Poodle Dog Rag\"). He created the piece by ear, as he had not yet learned to read and write music. \"I would play the 'Soda Fountain Rag' as a one-step, two-step, waltz, tango, and fox trot\", Ellington recalled. \"Listeners never knew it was the same piece. I was established as having my own repertoire.\" In his autobiography, \"Music is my Mistress\" (1973), Ellington wrote that he missed more lessons than he attended, feeling at the time that piano was not his talent.\nEllington continued listening to, watching, and imitating ragtime pianists, not only in Washington, D.C. but also in Philadelphia and Atlantic City, where he vacationed with his mother during the summer. He would sometimes hear strange music played by those who could not afford much sheet music, so for variations, they played the sheets upside down. Henry Lee Grant, a Dunbar High School music teacher, gave him private lessons in harmony. With the additional guidance of Washington pianist and band leader Oliver \"Doc\" Perry, Ellington learned to read sheet music, project a professional style, and improve his technique. Ellington was also inspired by his first encounters with stride pianists James P. Johnson and Luckey Roberts. Later in New York, he took advice from Will Marion Cook, Fats Waller, and Sidney Bechet. He started to play gigs in caf\u00e9s and clubs in and around Washington, D.C. His attachment to music was so strong that in 1916 he turned down an art scholarship to the Pratt Institute in Brooklyn. Three months before graduating, he dropped out of Armstrong Manual Training School, where he was studying commercial art.\nCareer.\nEarly career.\nWorking as a freelance sign painter from 1917, Ellington began assembling groups to play for dances. In 1919, he met drummer Sonny Greer from New Jersey, who encouraged Ellington's ambition to become a professional musician. Ellington built his music business through his day job. When a customer asked him to make a sign for a dance or party, he would ask if they had musical entertainment; if not, Ellington would offer to play for the occasion. He also had a messenger job with the U.S. Navy and State departments, where he made a wide range of contacts.\nEllington moved out of his parents' home and bought his own as he became a successful pianist. At first, he played in other ensembles, and in late 1917 formed his first group, \"The Duke's Serenaders\" (\"Colored Syncopators\", his telephone directory advertising proclaimed). He was also the group's booking agent. His first play date was at the True Reformer's Hall, where he took home 75 cents.\nEllington played throughout the D.C. area and into Virginia for private society balls and embassy parties. The band included childhood friend Otto Hardwick, who began playing the string bass, then moved to C-melody sax and finally settled on alto saxophone; Arthur Whetsel on trumpet; Elmer Snowden on banjo; and Sonny Greer on drums. The band thrived, performing for both African-American and white audiences, rare in the segregated society of the day.\nWhen his drummer Sonny Greer was invited to join the Wilbur Sweatman Orchestra in New York City, Ellington left his successful career in D.C. and moved to Harlem, ultimately becoming part of the Harlem Renaissance. New dance crazes such as the Charleston emerged in Harlem, as well as African-American musical theater, including Eubie Blake's and Noble Sissle's (the latter of whom was his neighbor) \"Shuffle Along\". After the young musicians left the Sweatman Orchestra to strike out on their own, they found an emerging jazz scene that was highly competitive with difficult inroad. They hustled pool by day and played whatever gigs they could find. The young band met stride pianist Willie \"The Lion\" Smith, who introduced them to the scene and gave them some money. They played at rent-house parties for income. After a few months, the young musicians returned to Washington, D.C., feeling discouraged.\nIn June 1923, they played a gig in Atlantic City, New Jersey and another at the prestigious Exclusive Club in Harlem. This was followed in September 1923 by a move to the Hollywood Club (at 49th and Broadway) and a four-year engagement, which gave Ellington a solid artistic base. He was known to play the bugle at the end of each performance. The group was initially called Elmer Snowden and his Black Sox Orchestra and had seven members, including trumpeter James \"Bubber\" Miley. They renamed themselves The Washingtonians. Snowden left the group in early 1924, and Ellington took over as bandleader. After a fire, the club was re-opened as the Club Kentucky (often referred to as the Kentucky Club).\nEllington then made eight records in 1924, receiving composing credit on three including \"Choo Choo\". In 1925, Ellington contributed four songs to \"Chocolate Kiddies\" starring Lottie Gee and Adelaide Hall, an all\u2013African-American revue which introduced European audiences to African-American styles and performers. Duke Ellington and his Kentucky Club Orchestra grew to a group of ten players; they developed their own sound via the non-traditional expression of Ellington's arrangements, the street rhythms of Harlem, and the exotic-sounding trombone growls and wah-wahs, high-squealing trumpets, and saxophone blues licks of the band members. For a short time, soprano saxophonist and clarinetist Sidney Bechet played with them, reportedly becoming the dominant personality in the group, with Sonny Greer saying Bechet \"fitted out the band like a glove\". His presence resulted in friction with Miley and trombonist Charlie Irvis, whose styles differed from Bechet's New Orleans-influenced playing. It was mainly Bechet's unreliability\u2014he was absent for three days in succession\u2014which made his association with Ellington short-lived.\nCotton Club engagement.\nIn October 1926, Ellington made an agreement with agent-publisher Irving Mills, giving Mills a 45% interest in Ellington's future. Mills had an eye for new talent and published compositions by Hoagy Carmichael, Dorothy Fields, and Harold Arlen early in their careers. After recording a handful of acoustic sides during 1924\u201326, Ellington's signing with Mills allowed him to record prolifically. However, sometimes he recorded different versions of the same tune. Mills regularly took a co-composer credit. From the beginning of their relationship, Mills arranged recording sessions on nearly every label, including Brunswick, Victor, Columbia, OKeh, Path\u00e9 (and its subsidiary, Perfect), the ARC/Plaza group of labels (Oriole, Domino, Jewel, Banner) and their dime-store labels (Cameo, Lincoln, Romeo), Hit of the Week, and Columbia's cheaper labels (Harmony, Diva, Velvet Tone, Clarion), labels that gave Ellington popular recognition. On OKeh, his records were usually issued as The Harlem Footwarmers. In contrast, the Brunswicks were usually issued as The Jungle Band. Whoopee Makers and the Ten BlackBerries were other pseudonyms.\nIn September 1927, King Oliver turned down a regular booking for his group as the house band at Harlem's Cotton Club; the offer passed to Ellington after Jimmy McHugh suggested him and Mills arranged an audition. Ellington had to increase from a six to 11-piece group to meet the requirements of the Cotton Club's management for the audition, and the engagement finally began on December 4. With a weekly radio broadcast, the Cotton Club's exclusively white and wealthy clientele poured in nightly to see them. At the Cotton Club, Ellington's group performed all the music for the revues, which mixed comedy, dance numbers, vaudeville, burlesque, music, and illicit alcohol. The musical numbers were composed by Jimmy McHugh and the lyrics were written by Dorothy Fields (later Harold Arlen and Ted Koehler), with some Ellington originals mixed in. (Here, he moved in with a dancer, his second wife Mildred Dixon). Weekly radio broadcasts from the club gave Ellington national exposure. At the same time, Ellington also recorded Fields-JMcHugh and Fats Waller\u2013Andy Razaf songs.\nAlthough trumpeter Bubber Miley was a member of the orchestra for only a short period, he had a major influence on Ellington's sound. As an early exponent of growl trumpet, Miley changed the sweet dance band sound of the group to one that was hotter, which contemporaries termed Jungle Style, which can be seen in his feature chorus in \"East St. Louis Toodle-Oo\" (1926). In October 1927, Ellington and his Orchestra recorded several compositions with Adelaide Hall. One side in particular, \"Creole Love Call\", became a worldwide sensation and gave both Ellington and Hall their first hit record. Miley had composed most of \"Creole Love Call\" and \"Black and Tan Fantasy\". An alcoholic, Miley had to leave the band before they gained wider fame. He died in 1932 at the age of 29, but he was an important influence on Cootie Williams, who replaced him.\nIn 1929, the Cotton Club Orchestra appeared on stage for several months in Florenz Ziegfeld's Show Girl, along with vaudeville stars Jimmy Durante, Eddie Foy, Jr., Ruby Keeler, and with music and lyrics by George Gershwin and Gus Kahn. Will Vodery, Ziegfeld's musical supervisor, recommended Ellington for the show. According to John Edward Hasse's \"Beyond Category: The Life and Genius of Duke Ellington\", \"Perhaps during the run of \"Show Girl\", Ellington received what he later termed 'valuable lessons in orchestration from Will Vody.\" In his 1946 biography, \"Duke Ellington\", Barry Ulanov wrote:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;From Vodery, as he (Ellington) says himself, he drew his chromatic convictions, his uses of the tones ordinarily extraneous to the diatonic scale, with the consequent alteration of the harmonic character of his music, it's broadening, The deepening of his resources. It has become customary to ascribe the classical influences upon Duke\u2014Delius, Debussy, and Ravel\u2014to direct contact with their music. Actually, his serious appreciation of those and other modern composers, came after he met with Vody.\nEllington's film work began with \"Black and Tan\" (1929), a 19-minute all-African-American RKO short in which he played the hero \"Duke\". He also appeared in the Amos 'n' Andy film \"Check and Double Check\" released in 1930, which features the orchestra playing \"Old Man Blues\" in an extended ballroom scene. That year, Ellington and his Orchestra connected with a whole different audience in a concert with Maurice Chevalier and they also performed at the Roseland Ballroom, \"America's foremost ballroom\". Australian-born composer Percy Grainger was an early admirer and supporter. He wrote, \"The three greatest composers who ever lived are Bach, Delius and Duke Ellington. Unfortunately, Bach is dead, Delius is very ill but we are happy to have with us today The Duke\". Ellington's first period at the Cotton Club concluded in 1931.\nEarly 1930s.\nEllington led the orchestra by conducting from the keyboard using piano cues and visual gestures; very rarely did he conduct using a baton. By 1932 his orchestra consisted of six brass instruments, four reeds, and a rhythm section of four players. As the leader, Ellington was not a strict disciplinarian; he maintained control of his orchestra with a combination of charm, humor, flattery, and astute psychology. A complex, private person, he revealed his feelings to only his closest intimates. He effectively used his public persona to deflect attention away from himself.\nEllington signed exclusively to Brunswick in 1932 and stayed with them through to late 1936 (albeit with a short-lived 1933\u201334 switch to Victor when Irving Mills temporarily moved his acts from Brunswick).\nAs the Depression worsened, the recording industry was in crisis, dropping over 90% of its artists by 1933. Ivie Anderson was hired as the Ellington Orchestra's featured vocalist in 1931. She is the vocalist on \"It Don't Mean a Thing (If It Ain't Got That Swing)\" (1932) among other recordings. Sonny Greer had been providing occasional vocals and continued to do in a cross-talk feature with Anderson. Radio exposure helped maintain Ellington's public profile as his orchestra began to tour. The other 78s of this era include: \"Mood Indigo\" (1930), \"Sophisticated Lady\" (1933), \"Solitude\" (1934), and \"In a Sentimental Mood\" (1935).\nWhile Ellington's United States audience remained mainly African-American in this period, the orchestra had a significant following overseas. They traveled to England and Scotland in 1933, as well as France (three concerts at the Salle Pleyel in Paris) and the Netherlands before returning to New York. On June 12, 1933, the Duke Ellington Orchestra gave its British debut at the London Palladium; Ellington received an ovation when he walked on stage. They were one of 13 acts on the bill and were restricted to eight short numbers; the booking lasted until June 24. The British visit saw Ellington win praise from members of the serious music community, including composer Constant Lambert, which gave a boost to Ellington's interest in composing longer works.\nHis longer pieces had already begun to appear. Ellington had composed and recorded \"Creole Rhapsody\" as early as 1931 (issued as both sides of a 12\" record for Victor and both sides of a 10\" record for Brunswick). A tribute to his mother, \"Reminiscing in Tempo\", took four 10\" 78rpm record sides to record in 1935 after her death in that year. \"Symphony in Black\" (also 1935), a short film, featured his extended piece 'A Rhapsody of Negro Life'. It introduced Billie Holiday, and won the Academy Award for Best Musical Short Subject. Ellington and his Orchestra also appeared in the features \"Murder at the Vanities\" and \"Belle of the Nineties\" (both 1934).\nFor agent Mills, the attention was a publicity triumph, as Ellington was now internationally known. On the band's tour through the segregated South in 1934, they avoided some of the traveling difficulties of African Americans by touring in private railcars. These provided accessible accommodations, dining, and storage for equipment while avoiding the indignities of segregated facilities.\nHowever, the competition intensified as swing bands like Benny Goodman's began to receive widespread attention. Swing dancing became a youth phenomenon, particularly with white college audiences, and danceability drove record sales and bookings. Jukeboxes proliferated nationwide, spreading the gospel of swing. Ellington's band could certainly swing, but their strengths were mood, nuance, and richness of composition, hence his statement \"jazz is music, the swing is business\".\nLater 1930s.\nFrom 1936, Ellington began to make recordings with smaller groups (sextets, octets, and nonets) drawn from his then-15-man orchestra. He composed pieces intended to feature a specific instrumentalist, such as \"Jeep's Blues\" for Johnny Hodges, \"Yearning for Love\" for Lawrence Brown, \"Trumpet in Spades\" for Rex Stewart, \"Echoes of Harlem\" for Cootie Williams and \"Clarinet Lament\" for Barney Bigard. In 1937, Ellington returned to the Cotton Club, which had relocated to the mid-town Theater District. In the summer of that year, his father died, and due to many expenses, Ellington's finances were tight. However, his situation improved in the following years.\nAfter leaving agent Irving Mills, he signed on with the William Morris Agency. Mills, though, continued to record Ellington. After only a year, his Master and Variety labels (the small groups had recorded for the latter) collapsed in late 1937. Mills placed Ellington back on Brunswick and those small group units on Vocalion through to 1940. Well-known sides continued to be recorded, \"Caravan\" in 1937, and \"I Let a Song Go Out of My Heart\" the following year.\nBilly Strayhorn, originally hired as a lyricist, began his association with Ellington in 1939. Nicknamed \"Sweet Pea\" for his mild manner, Strayhorn soon became a vital member of the Ellington organization. Ellington showed great fondness for Strayhorn and never failed to speak glowingly of the man and their collaborative working relationship, \"my right arm, my left arm, all the eyes in the back of my head, my brain waves in his head, and his in mine\". Strayhorn, with his training in classical music, not only contributed his original lyrics and music but also arranged and polished many of Ellington's works, becoming a second Ellington or \"Duke's doppelg\u00e4nger\". It was not uncommon for Strayhorn to fill in for Duke, whether in conducting or rehearsing the band, playing the piano, on stage, and in the recording studio. The decade ended with a very successful European tour in 1939 just as World War II loomed in Europe.\nEarly to mid-1940s.\nTwo musicians who joined Ellington at this time created a sensation in their own right, Jimmy Blanton and Ben Webster. Blanton was effectively hired on the spot in late October 1939, before Ellington was aware of his name, when he dropped in on a gig of Fate Marable in St Louis. The short-lived Blanton transformed the use of double bass in jazz, allowing it to function as a solo/melodic instrument rather than a rhythm instrument alone.Terminal illness forced him to leave by late 1941 after around two years. Ben Webster's principal tenure with Ellington spanned 1939 to 1943. An ambition of his, he told his previous employer, Teddy Wilson, then leading a big band, that Ellington was the only rival he would leave Wilson for. He was the orchestra's first regular tenor saxophonist and increased the size of the sax section to five for the first time. Much influenced by Johnny Hodges, he often credited Hodges with showing him \"how to play my horn\". The two men sat next to each other in the orchestra.\nTrumpeter Ray Nance joined, replacing Cootie Williams who had defected to Benny Goodman. Additionally, Nance added violin to the instrumental colors Ellington had at his disposal. Recordings exist of Nance's first concert date on November 7, 1940, at Fargo, North Dakota. Privately made by Jack Towers and Dick Burris, these recordings were first legitimately issued in 1978 as \"Duke Ellington at Fargo, 1940 Live\"; they are among the earliest of innumerable live performances which survive. Nance was an occasional vocalist as well, although Herb Jeffries was the main male vocalist in this era (until 1943) while Al Hibbler (who replaced Jeffries in 1943) continued until 1951. Ivie Anderson left in 1942 for health reasons after 11 years, the longest term of any of Ellington's vocalists.\nOnce more recording for Victor (from 1940), with the small groups being issued on their Bluebird label, three-minute masterpieces on 78 rpm record sides continued to flow from Ellington, Billy Strayhorn, Ellington's son Mercer Ellington, and members of the orchestra. \"Cotton Tail\", \"Main Stem\", \"Harlem Air Shaft\", \"Jack the Bear\", and dozens of others date from this period. Strayhorn's \"Take the \"A\" Train\", a hit in 1941, became the band's theme, replacing \"East St. Louis Toodle-Oo\". Ellington and his associates wrote for an orchestra of distinctive voices displaying tremendous creativity. The commercial recordings from this era were re-issued in the three-CD collection, \"\", in 2003.\nEllington's long-term aim, though, was to extend the jazz form from that three-minute limit, of which he was an acknowledged master. While he had composed and recorded some extended pieces before, such works now became a regular feature of Ellington's output. In this, he was helped by Strayhorn, who had enjoyed a more thorough training in the forms associated with classical music than Ellington. The first of these, \"Black, Brown, and Beige\" (1943), was dedicated to telling the story of African Americans and the place of slavery and the church in their history. \"Black, Brown and Beige\" debuted at Carnegie Hall on January 23, 1943, beginning an annual series of Ellington concerts at the venue over the next four years. While some jazz musicians had played at Carnegie Hall before, none had performed anything as elaborate as Ellington's work. Unfortunately, starting a regular pattern, Ellington's longer works were generally not well received.\nA partial exception was \"Jump for Joy\", a full-length musical based on themes of African-American identity, which debuted on July 10, 1941, at the Mayan Theater in Los Angeles. Hollywood actors John Garfield and Mickey Rooney invested in the production, and Charlie Chaplin and Orson Welles offered to direct. At one performance, Garfield insisted that Herb Jeffries, who was light-skinned, should wear makeup. Ellington objected in the interval and compared Jeffries to Al Jolson. The change was reverted. The singer later commented that the audience must have thought he was an entirely different character in the second half of the show.\nAlthough it had sold-out performances and received positive reviews, it ran for only 122 performances until September 29, 1941, with a brief revival in November of that year. Its subject matter did not make it appealing to Broadway; Ellington had unfulfilled plans to take it there. Despite this disappointment, a Broadway production of Ellington's \"Beggar's Holiday\", his sole book musical, premiered on December 23, 1946, under the direction of Nicholas Ray.\nThe settlement of the first recording ban of 1942\u201344, leading to an increase in royalties paid to musicians, had a severe effect on the financial viability of the big bands, including Ellington's Orchestra. His income as a songwriter ultimately subsidized it. Although he always spent lavishly and drew a respectable income from the orchestra's operations, the band's income often just covered expenses. In 1943 Ellington asked Webster to leave; the saxophonist's personality made his colleagues anxious and the saxophonist was regularly in conflict with the leader.\nEarly postwar years.\nMusicians enlisting in the military and travel restrictions made touring difficult for big bands, and dancing became subject to a new tax, which continued for many years, affecting the choices of club owners. By the time World War II ended, the big band era was effectively over as the focus of popular music was shifting towards solo singers such as Frank Sinatra and Jo Stafford. As the cost of hiring big bands had increased, club owners now found smaller jazz groups more cost-effective. Some of Ellington's new works, such as the wordless vocal feature \"Transblucency\" (1946) with Kay Davis, were not going to have a similar reach as the newly emerging stars.\nEllington continued on his own course through these tectonic shifts. While Count Basie, like many other big bands at the time, was forced to disband his whole ensemble and work as an octet for a time, Ellington was able to tour most of Western Europe between April 6 and June 30, 1950, with the orchestra playing 74 dates over 77 days. During the tour, according to Sonny Greer, Ellington did not perform the newer works. However, Ellington's extended composition, \"Harlem\" (1950), was in the process of being completed at this time. Ellington later presented its score to music-loving President Harry Truman. Also during his time in Europe, Ellington would compose the music for a stage production by Orson Welles. Titled \"Time Runs\" in Paris and \"An Evening With Orson Welles\" in Frankfurt, the variety show also featured a newly discovered Eartha Kitt, who performed Ellington's original song \"Hungry Little Trouble\" as Helen of Troy. Ellington recorded his first LP in December 1950, \"Masterpieces by Ellington\", consisting of extended, complex new \"concert arrangements\" of 1930s classics \"Mood Indigo\", \"Sophisticated Lady\", and \"(In My) Solitude\", along with the more recent tone poem \"The Tattooed Bride\", fashioned by Ellington and Strayhorn.\nIn 1951, Ellington suffered a significant loss of personnel: Sonny Greer, Lawrence Brown, and, most importantly, Johnny Hodges left to pursue other ventures. However, only Greer was a permanent departee. Drummer Louie Bellson replaced Greer, and his \"Skin Deep\" was a hit for Ellington. Tenor player Paul Gonsalves had joined in December 1950 after periods with Count Basie and Dizzy Gillespie and stayed for the rest of his life, while Clark Terry joined in November 1951.\nAndr\u00e9 Previn said in 1952: \"You know, Stan Kenton can stand in front of a thousand fiddles and a thousand brass and make a dramatic gesture and every studio arranger can nod his head and say, \"Oh, yes, that's done like this.\" But Duke merely lifts his finger, three horns make a sound, and I don't know what it is!\" However, by 1955, after three years of recording for Capitol, Ellington lacked a regular recording affiliation.\nCareer revival.\nEllington's appearance at the Newport Jazz Festival on July 7, 1956, returned him to wider prominence. The feature \"Diminuendo and Crescendo in Blue\" comprised two tunes that had been in the band's book since 1937. Ellington, who had abruptly ended the band's scheduled set because of the late arrival of four key players, called the two tunes as the time was approaching midnight. Announcing that the two pieces would be separated by an interlude played by tenor saxophonist Paul Gonsalves, Ellington proceeded to lead the band through the two pieces, with Gonsalves' 27-chorus marathon solo whipping the crowd into a frenzy, leading the Maestro to play way beyond the curfew time despite urgent pleas from festival organizer George Wein to bring the program to an end.\nThe concert made international headlines, and led to one of only five \"Time\" magazine cover stories dedicated to a jazz musician, and resulted in an album produced by George Avakian that would become the best-selling LP of Ellington's career. Much of the music on the LP was, in effect, simulated, with only about 40% actually from the concert itself. According to Avakian, Ellington was dissatisfied with aspects of the performance and felt the musicians had been under-rehearsed. The band assembled the next day to re-record several numbers with the addition of the faked sound of a crowd, none of which was disclosed to purchasers of the album. Not until 1999 was the concert recording properly released for the first time. The revived attention brought about by the Newport appearance should not have surprised anyone, Johnny Hodges had returned the previous year, and Ellington's collaboration with Strayhorn was renewed around the same time, under terms more amenable to the younger man.\nThe original \"Ellington at Newport\" album was the first release in a new recording contract with Columbia Records which yielded several years of recording stability, mainly under producer Irving Townsend, who coaxed both commercial and artistic productions from Ellington.\nIn 1957, CBS (Columbia Records' parent corporation) aired a live television production of \"A Drum Is a Woman\", an allegorical suite which received mixed reviews. Festival appearances at the new Monterey Jazz Festival and elsewhere provided venues for live exposure, and a European tour in 1958 was well received. \"Such Sweet Thunder\" (1957), based on Shakespeare's plays and characters, and \"The Queen's Suite\" (1958), dedicated to Britain's Queen Elizabeth II, were products of the renewed impetus which the Newport appearance helped to create. However, the latter work was not commercially issued at the time. The late 1950s also saw Ella Fitzgerald record her \"Duke Ellington Songbook\" (Verve) with Ellington and his orchestra\u2014a recognition that Ellington's songs had now become part of the cultural canon known as the 'Great American Songbook'.\nAround this time Ellington and Strayhorn began to work on film scoring. The first of these was \"Anatomy of a Murder\" (1959), a courtroom drama directed by Otto Preminger and featuring James Stewart, in which Ellington appeared fronting a roadhouse combo. Film historians have recognized the score \"as a landmark\u2014the first significant Hollywood film music by African Americans comprising non-diegetic music, that is, music whose source is not visible or implied by action in the film, like an on-screen band.\" The score avoided the cultural stereotypes which previously characterized jazz scores and rejected a strict adherence to visuals in ways that presaged the New Wave cinema of the '60s\". Ellington and Strayhorn, always looking for new musical territory, produced suites for John Steinbeck's novel \"Sweet Thursday\", Tchaikovsky's \"Nutcracker Suite\" and Edvard Grieg's \"Peer Gynt\".\n\"Anatomy of a Murder\" was followed by \"Paris Blues\" (1961), which featured Paul Newman and Sidney Poitier as jazz musicians. For this work, Ellington was nominated for the Academy Award for Best Score.\nIn the early 1960s, Ellington embraced recording with artists who had been friendly rivals in the past or were younger musicians who focused on later styles. The Ellington and Count Basie orchestras recorded together with the album \"First Time! The Count Meets the Duke\" (1961). During a period when Ellington was between recording contracts, he made records with Louis Armstrong (Roulette), Coleman Hawkins, John Coltrane (both for Impulse) and participated in a session with Charles Mingus and Max Roach which produced the \"Money Jungle\" (United Artists) album. He signed to Frank Sinatra's new Reprise label, but the association with the label was short-lived.\nMusicians who had previously worked with Ellington returned to the Orchestra as members: Lawrence Brown in 1960 and Cootie Williams in 1962.\nThe writing and playing of music is a matter of intent... You can't just throw a paintbrush against the wall and call whatever happens art. My music fits the tonal personality of the player. I think too strongly in terms of altering my music to fit the performer to be impressed by accidental music. You can't take doodling seriously.\nHe was now performing worldwide and spent a significant part of each year on overseas tours. As a consequence, he formed new working relationships with artists from around the world, including the Swedish vocalist Alice Babs, and the South African musicians Dollar Brand and Sathima Bea Benjamin (\"A Morning in Paris\", 1963/1997).\nEllington wrote an original score for director Michael Langham's production of Shakespeare's \"Timon of Athens\" at the Stratford Festival in Ontario, Canada, which opened on July 29, 1963. Langham has used it for several subsequent productions, including a much later adaptation by Stanley Silverman which expands the score with some of Ellington's best-known works.\nLast years.\nEllington was shortlisted for the Pulitzer Prize for Music in 1965. However, no prize was ultimately awarded that year. Then 66 years old, he joked: \"Fate is being kind to me. Fate doesn't want me to be famous too young.\" In 1999, he was posthumously awarded a special Pulitzer Prize \"commemorating the centennial year of his birth, in recognition of his musical genius, which evoked aesthetically the principles of democracy through the medium of jazz and thus made an indelible contribution to art and culture.\"\nIn September 1965, he premiered the first of his Sacred Concerts. He created a jazz Christian liturgy. Although the work received mixed reviews, Ellington was proud of the composition and performed it dozens of times. This concert was followed by two others of the same type in 1968 and 1973, known as the Second and Third Sacred Concerts. Many saw the Sacred Music suites as an attempt to reinforce commercial support for organized religion. However, Ellington simply said it was \"the most important thing I've done\". The Steinway piano upon which the Sacred Concerts were composed is part of the collection of the Smithsonian's National Museum of American History. Like Haydn and Mozart, Ellington conducted his orchestra from the piano\u2014he always played the keyboard parts when the Sacred Concerts were performed.\nEllington turned 65 in the spring of 1964 but showed no sign of slowing down as he continued to make recordings of significant works such as \"The Far East Suite\" (1966), \"New Orleans Suite\" (1970), \"The Afro-Eurasian Eclipse\" (1971) and the \"Latin American Suite\" (1972), much of it inspired by his world tours. It was during this time that he recorded his only album with Frank Sinatra, titled \"Francis A. &amp; Edward K.\" (1967). In August 1972, he recorded several solo piano tracks at Mediasound Studios in New York, with the then brand-new assistant engineer Bob Clearmountain. The session remained unreleased until 2017, when Storyville Records released it as \"An Intimate Piano Session\".\nIn 1972\u20131974 Ellington worked on his only opera, \"Queenie Pie\", together with Maurice Peress. Ellington got an idea to write an opera about a black beautician in the 1930s, but did not finish it.\nThe final new recorded material Ellington released was a collaboration with Teresa Brewer titled \"It Don't Mean a Thing If It Don't Got That Swing\", released on her husband Bob Thiele's label Flying Dutchman Records in the UK in 1973 and in the US the following year. Among the last shows Ellington and his orchestra performed were one on March 21, 1973, at Purdue University's Hall of Music, two on March 22, 1973, at the Sturges-Young Auditorium in Sturgis, Michigan and the \"Eastbourne Performance\" on December 1, 1973, later issued on LP. Ellington performed what is considered his final full concert in a ballroom at Northern Illinois University on March 20, 1974. Since 1980, that ballroom has been dedicated as the \"Duke Ellington Ballroom\".\nPersonal life.\nEllington married his high school sweetheart, Edna Thompson (d. 1967), on July 2, 1918, when he was 19. The next spring, on March 11, 1919, Edna gave birth to their only child, Mercer Kennedy Ellington.\nEllington was joined in New York City by his wife and son in the late 1920s, but the couple soon permanently separated. According to her obituary in \"Jet\" magazine, she was \"homesick for Washington\" and returned. In 1929, Ellington became the companion of Mildred Dixon, who traveled with him, managed Tempo Music, inspired songs, such as \"Sophisticated Lady\", at the peak of his career, and raised his son.\nIn 1938, he left his family (his son was 19) and moved in with Beatrice \"Evie\" Ellis, a Cotton Club employee. Their relationship, though stormy, continued after Ellington met and formed a relationship with Fernanda de Castro Monte in the early 1960s. Ellington supported both women for the rest of his life.\nEllington's sister Ruth (1915\u20132004) later ran Tempo Music, his music publishing company. Ruth's second husband was the bass-baritone McHenry Boatwright, whom she met when he sang at her brother's funeral. As an adult, son Mercer Ellington (d. 1996) played trumpet and piano, led his own band, worked as his father's business manager, and kept the Ellington band alive for 20 years after Duke's death.\nEllington was a member of Alpha Phi Alpha and was a Freemason associated with Prince Hall Freemasonry.\nDeath.\nEllington died on May 24, 1974, of complications from lung cancer and pneumonia, aged 75. At his funeral, attended by over 12,000 people at the Cathedral of St. John the Divine, Ella Fitzgerald summed up the occasion: \"It's a very sad day. A genius has passed.\"\nHe was interred in the Woodlawn Cemetery, the Bronx, New York City.\nLegacy.\nMemorialized.\nNumerous memorials have been dedicated to Duke Ellington in cities from New York and Washington, D.C. to Los Angeles.\nIn Ellington's birthplace, Washington, D.C., the Duke Ellington School of the Arts educates talented students who are considering careers in the arts by providing art instruction and academic programs to prepare students for post-secondary education and professional careers. In 1974, the District renamed the Calvert Street Bridge, originally built in 1935, as the Duke Ellington Bridge. Another school is P.S. 004 Duke Ellington in New York.\nIn 1989, a bronze plaque was attached to the newly named Duke Ellington Building at 2121 Ward Place NW. In 2012, the new owner of the building commissioned a mural by Aniekan Udofia that appears above the lettering \"Duke Ellington\". In 2010 the triangular park, across the street from Duke Ellington's birth site, at the intersection of New Hampshire and M Streets NW, was named the Duke Ellington Park.\nEllington's residence at 2728 Sherman Avenue NW, during the years 1919\u20131922, is marked by a bronze plaque.\nOn February 24, 2009, the United States Mint issued a coin with Duke Ellington on it, making him the first African American to appear by himself on a circulating U.S. coin. Ellington appears on the reverse (tails) side of the District of Columbia quarter. The coin is part of the U.S. Mint's program honoring the District and the U.S. territories and celebrates Ellington's birthplace in the District of Columbia. Ellington is depicted on the quarter seated at a piano, sheet music in hand, along with the inscription \"Justice for All\", which is the District's motto.\nIn 1986, a United States commemorative stamp was issued featuring Ellington's likeness.\nEllington lived out his final years in Manhattan, in a townhouse at 333 Riverside Drive near West 106th Street. His sister Ruth, who managed his publishing company, also lived there, and his son Mercer lived next door. After his death, West 106th Street was officially renamed Duke Ellington Boulevard.\nA large memorial to Ellington, created by sculptor Robert Graham, was dedicated in 1997 in New York's Central Park, near Fifth Avenue and 110th Street, an intersection named Duke Ellington Circle.\nA statue of Ellington at a piano is featured at the entrance to UCLA's Schoenberg Hall. According to \"UCLA\" magazine:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;When UCLA students were entranced by Duke Ellington's provocative tunes at a Culver City club in 1937, they asked the budding musical great to play a free concert in Royce Hall. 'I've been waiting for someone to ask us!' Ellington exclaimed.\nOn the day of the concert, Ellington accidentally mixed up the venues and drove to USC instead. He eventually arrived at the UCLA campus and, to apologize for his tardiness, played to the packed crowd for more than four hours. And so, \"Sir Duke\" and his group played the first-ever jazz performance in a concert venue.\nThe Essentially Ellington High School Jazz Band Competition and Festival is a nationally renowned annual competition for prestigious high school bands. Started in 1996 at Jazz at Lincoln Center, the festival is named after Ellington because of the significant focus that the festival places on his works.\nTributes.\nAfter Duke died, his son Mercer took over leadership of the orchestra, continuing until he died in 1996. Like the Count Basie Orchestra, this \"ghost band\" continued to release albums for many years. \"Digital Duke\", credited to The Duke Ellington Orchestra, won the 1988 Grammy Award for Best Large Jazz Ensemble Album. Mercer Ellington had been handling all administrative aspects of his father's business for several decades. Mercer's children continue a connection with their grandfather's work.\nGunther Schuller wrote in 1989:\nEllington composed incessantly to the very last days of his life. Music was indeed his mistress; it was his total life and his commitment to it was incomparable and unalterable. In jazz he was a giant among giants. And in twentieth century music, he may yet one day be recognized as one of the half-dozen greatest masters of our time.\nMartin Williams said: \"Duke Ellington lived long enough to hear himself named among our best composers. And since his death in 1974, it has become not at all uncommon to see him named, along with Charles Ives, as the greatest composer we have produced, regardless of category.\"\nIn the opinion of Bob Blumenthal of \"The Boston Globe\" in 1999: \"[i]n the century since his birth, there has been no greater composer, American or otherwise, than Edward Kennedy Ellington.\"\nIn 2002, scholar Molefi Kete Asante listed Duke Ellington on his list of 100 Greatest African Americans.\nHis compositions have been revisited by artists and musicians worldwide as sources of inspiration and a bedrock of their performing careers:\nThere are hundreds of albums dedicated to the music of Duke Ellington and Billy Strayhorn by artists famous and obscure. \"Sophisticated Ladies\", an award-winning 1981 musical revue, incorporated many tunes from Ellington's repertoire. A second Broadway musical interpolating Ellington's music, \"Play On!\", debuted in 1997.\nAwards and honors.\nGrammy Awards.\nEllington earned 14 Grammy awards from 1959 to 2000 (three of which were posthumous) and a total of 25 nominations\nGrammy Hall of Fame.\nRecordings of Duke Ellington were inducted into the Grammy Hall of Fame, a special Grammy award established in 1973 to honor recordings at least 25 years old that have qualitative or historical significance.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41538", "revid": "50564086", "url": "https://en.wikipedia.org/wiki?curid=41538", "title": "Rahsaan Roland Kirk", "text": "American jazz multi-instrumentalist (1935\u20131977)\nRahsaan Roland Kirk (born Ronald Theodore Kirk; August 7, 1935 \u2013 December 5, 1977), known earlier in his career simply as Roland Kirk, was an American jazz multi-instrumentalist who played tenor saxophone, flute, and many other instruments. He was renowned for his onstage vitality; in his shows, virtuoso improvisation was accompanied by comic banter, political ranting, and the simultaneous playing of several instruments.\nLife.\nRonald Theodore Kirk was born in Columbus, Ohio, where he lived in a neighborhood known as Flytown. He became blind at two years old, which he said was a result of improper medical treatment. As a teenager, Kirk studied at the Ohio State School for the Blind. By 15, he was on the road playing rhythm and blues on weekends with Boyd Moore's band. According to saxophonist Hank Crawford, \"He would be like this 14-year-old blind kid playing two horns at once. They would bring him out and he would tear the joint up.\" Crawford heard him during this period and said he was unbelievable. He remarked, \"Now they had him doing all kinds of goofy stuff but he was playing the two horns and he was playing the shit out of them. He was an original from the beginning.\" Kirk felt compelled by a dream to transpose two letters in his first name to make \"Roland\". In 1970, Kirk added \"Rahsaan\" to his name after hearing it in a dream.\nKirk was politically outspoken. During his concerts, between songs he often talked about topical issues, including African-American history and the Civil Rights Movement. His monologues were often laced with satire and absurdist humor. According to comedian Jay Leno, when Leno toured with Kirk as Kirk's opening act, Kirk would introduce him by saying: \"I want to introduce a young brother who knows the black experience and knows all about the white devils... Please welcome Jay Leno!\"\nIn 1975, Kirk had a major stroke which led to partial paralysis of one side of his body. He continued to perform and record, modifying his instruments to enable him to play with one arm. At a live performance at Ronnie Scott's Jazz Club in London he even managed to play two instruments, and carried on to tour internationally and to appear on television.\nHe died from a second stroke in 1977, aged 42, the morning after performing in the Frangipani Room of the Indiana University Student Union in Bloomington, Indiana.\nColumbus Mayor Jack Sensenbrenner declared Saturday, December 10, 1970, \"Rahsaan day\", according to the Columbus Dispatch obituary that appeared on Thursday, December 8, 1977.\nKirk's hometown of Columbus was not appreciative of his work for most of his career. He was thrown out of a local nightclub because his music was too difficult to understand, and he left for Los Angeles and further touring. In the 21st century, jazz fans in Columbus have been embracing his legacy.\nHis widow Dorthaan Kirk became notable in her own right for her work as a curator and producer of jazz events primarily connected with Newark, dating back to her joining WBGO, New Jersey's first public jazz station, in 1978, leading to her being a recipient of the 2020 A.B. Spellman NEA Jazz Masters Fellowship for Jazz Advocacy.\nInstruments and techniques.\nKirk's musical career spans from 1955 until his death in 1977. He preferred to lead his own bands and rarely performed as a sideman, although he did record with arranger Quincy Jones, drummer Roy Haynes and worked with bassist Charles Mingus. One of his best-known recorded performances is the lead flute and solo on Jones' \"Soul Bossa Nova\", a 1964 hit song repopularized in the \"Austin Powers\" films.\nKirk's multi-instrumentality was credited as having a substantial musical conception. This inclusivity included blues music, a love of stride piano and early jazz, and an appreciation for pop tunes. But his vision was much wider than that of most of his contemporaries. According to producer Joel Dorn, he was also hugely knowledgeable about classical music. Pieces by Saint-Sa\u00ebns, Hindemith, Tchaikovsky, Dvorak and Villa-Lobos would all feature on his albums over the years, alongside standards, pop songs and original compositions. Rahsaan's influences went beyond jazz and consequentially, he preferred the term \"Black Classical Music\".\nHis playing was generally rooted in soul jazz or hard bop, but Kirk's knowledge of jazz history allowed him to draw from many elements of the music's past, from ragtime to swing and free jazz. Kirk also absorbed classical influences, and his artistry reflected elements of pop music by composers such as Smokey Robinson and Burt Bacharach, as well as Duke Ellington, John Coltrane and other jazz musicians.\nKirk played and collected many musical instruments, mainly multiple saxophones, clarinets and flutes. His primary saxophones were a standard tenor saxophone, stritch (a straight alto sax lacking the instrument's conventional upturned bell), and a manzello (a modified saxello soprano sax, with a larger, upturned bell). A number of his instruments were exotic or homemade. Kirk modified instruments himself to accommodate his simultaneous playing technique. Critic Gary Giddins wrote that Kirk's tenor playing alone would have been enough to secure his reputation.\nUsually, he appeared on stage with all three horns hanging around his neck, and at times he would play a number of these horns at once, harmonizing with himself, or sustain a note for lengthy durations by using circular breathing. He used the multiple horns to play true chords, essentially functioning as a one-man saxophone section. Kirk insisted that he was only trying to emulate the sounds he heard in his head. Even while playing two or three saxophones at once, the music was intricate, powerful jazz with a strong feel for the blues. The live album \"Bright Moments\" (1973) is an example of one of his shows.\nKirk was also an influential flute player, including recorders. According to Giddins, Kirk was the first major jazz innovator on flute after Eric Dolphy (who died in 1964). Kirk employed several techniques, including singing or humming into the flute at the same time as playing. Another was to play the standard transverse flute at the same time as a nose flute.\nHe played a variety of other instruments, including whistles, clarinet, harmonica, and English horn. He often kept a gong within reach, and was a competent trumpeter. He utilized unique approaches, such as playing a trumpet with a saxophone mouthpiece.\nHe also made use of non-musical devices, such as alarm clocks, sirens, or a section of common garden hose (dubbed \"the black mystery pipes\"). From the early 1970s, his studio recordings used tape-manipulated \"musique concr\u00e8te\" and primitive electronic sounds before such things became commonplace.\n\"The Case of the 3 Sided Dream in Audio Color\" was a unique album in the annals of recorded jazz and popular music. It was a two-LP set, with Side 4 apparently \"blank\", the label not indicating any content. However, once word of \"the secret message\" got around among Rahsaan's fans, one would find that about 12 minutes into Side 4 appeared the first of two telephone answering machine messages recorded by Kirk, the second following soon thereafter (but separated by more blank grooves). The surprise impact of these segments appearing on \"blank\" Side 4 was lost on the initial CD reissue of this album (though restored as track 20 on the CD re-release).\nHe gleaned information on what was happening in the world via radio and TV. His later recordings often incorporated his spoken commentaries on current events, including Richard Nixon's involvement in the Watergate scandal. The \"3-Sided Dream\" album was a \"concept album\" that incorporated \"found\" or environmental sounds and tape loops, tapes being played backwards, and more. Snippets of Billie Holiday singing are also heard briefly. The album even confronts the rise of influence of computers in society, as Kirk threatens to pull the plug on the machine trying to tell him what to do.\nIn the 1976 album \"Other Folks' Music\" the spoken words of Paul Robeson, another outspoken black artist, can be briefly heard.\nDiscography.\nAs leader.\nCompilations and box sets\nAs sideman.\nWith Quincy Jones\nWith Charles Mingus\nWith others\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41544", "revid": "196446", "url": "https://en.wikipedia.org/wiki?curid=41544", "title": "John McLoughlin", "text": "Hudson's Bay Company figure in Oregon (1784\u20131857)\nJohn McLoughlin, baptized Jean-Baptiste McLoughlin, (October 19, 1784 \u2013 September 3, 1857) was a French-Canadian, later American, Chief Factor and Superintendent of the Columbia District of the Hudson's Bay Company at Fort Vancouver from 1824 to 1845. He was later known as the \"Father of Oregon\" for his role in assisting the American cause in the Oregon Country. In the late 1840s, his general store in Oregon City was famous as the last stop on the Oregon Trail.\nEarly days.\nMcLoughlin was born in October 1784 in Rivi\u00e8re-du-Loup, Quebec, and was of Scottish and French Canadian descent. He lived with his great uncle, Colonel William Fraser, for a while as a child. Though baptized Roman Catholic, he was raised Anglican. In his later life, he returned to the Roman Catholic faith.\nIn 1798, he began to study medicine under Sir James Fisher of Quebec. McLoughlin was granted a licence to practice medicine in Lower Canada (now Quebec) in 1803. He evidently completed his course, as he is widely referred to as \"Dr. John McLoughlin\".\nNorth West Company.\nMcLoughlin was hired as a physician at Fort William, the inland headquarters and a fur trade post of the North West Company on Lake Superior. There he became a trader and mastered several Indian languages. In 1814, he became a partner in the company.\nIn 1816, McLoughlin was charged with complicity in the massacre at the Red River Colony after the Battle of Seven Oaks. He and all the other parties from the North West Company were exonerated. The Hudson's Bay Company was found culpable by the appointed Royal Commissioner at its trial on October\u00a030, 1818, and in the later prosecutions by Lord Selkirk and the successful counter-suits.\nMcLoughlin was instrumental in the negotiations leading to the North West Company's 1821 merger with the Hudson's Bay Company. He was promoted to head the Lac la Pluie district temporarily shortly after the merger.\nHudson's Bay Company.\nIn 1824, the Hudson's Bay Company appointed McLoughlin, already a Chief Factor, as Superintendent of the Columbia Department (roughly parallel to what Americans know as the Oregon Country), and Peter Skene Ogden was appointed to assist him. At the time, the region was under joint occupation of both the United States and Britain pursuant to the Treaty of 1818. Upon his arrival, McLoughlin determined that the headquarters of the company at Fort Astoria (now Astoria, Oregon), at the mouth of the Columbia River, was unfit. The York Factory Express trade route had evolved from an earlier express brigade used by the North West Company between Fort George, founded in 1811 by John Jacob Astor's American Fur Company) at the mouth of the Columbia River, to Fort William on Lake Superior.\nIn the 1821 merger with the North West Company, the Hudson's Bay Company gained control of North West Company trading posts west of the Rocky Mountains. They established headquarters at Fort George (formerly Astoria). George Simpson, Governor of Hudson's Bay Company, visited the Columbia District in 1824\u201325, journeying from York Factory. He investigated a quicker route than previously used, following the Saskatchewan River and crossing the mountains at Athabasca Pass. This route was thereafter followed by the York Factory Express brigades.\nFort Vancouver.\nMcLoughlin built Fort Vancouver as a replacement for Fort George, on the north side of the Columbia River, a few miles upstream from the confluence of the Columbia and Willamette Rivers. The site was chosen by Sir George Simpson. The post was opened for business on March 19, 1825. From his Columbia Department headquarters in Fort Vancouver, McLoughlin supervised trade and kept peace with the Indians, inaugurated salmon and timber trade with Mexican-controlled California and Hawaii, and supplied Russian America with produce.\nFort Vancouver became the center of activity in the Pacific Northwest. Every year ships would come from London to drop off supplies and trade goods in exchange for the furs. It was the nexus for the fur trade on the Pacific Coast; its influence reached from the Rocky Mountains to the Hawaiian Islands, and from Russian Alaska into Mexican-controlled California. From Fort Vancouver, at its pinnacle, McLoughlin watched over 34 outposts, 24 ports, six ships, and 600 employees. Under McLoughlin's management, the Columbia Department remained highly profitable, in part due to the ongoing high demand for beaver hats in Europe. John McLoughlin was worried Fort Vancouver would be attacked and plundered of its heavy stock of supplies, due to its proximity to the Willamette Valley, in which there was already an American settlement of some size.\nYork Factory Express.\nBy 1825, there were usually two brigades from opposite ends of the route, (Fort Vancouver in the Columbia District on the lower Columbia River and the other from York Factory on Hudson Bay), that set out in spring and passed each other in the middle of the continent. Each brigade consisted of about forty to seventy-five men and two to five specially made boats that traveled at breakneck speed (for the time). These brigades often needed help from Indians, who would help the men portage around falls and unnavigable rapids; in return, the Indians were paid with trade goods. An 1839 report cites the travel time as three months and ten days\u2014almost 26 miles (40\u00a0km) per day on average. The brigades used boat, horseback, and backpacks to bring the supplies in and furs out to the forts and trading posts along the route.\nPuget Sound Agricultural Company.\nThe Hudson Bay Company officially discouraged settlement because it interfered with the lucrative fur trade. Two developments in the late 1830s made a reappraisal of Hudson's Bay Company operations in the Columbia Department necessary. Apprehensions about American antagonism rose due to US Senator Lewis F. Linn, who in 1838 called for a naval force to be dispatched to the Columbia River, although the measure never passed. Favorable relations with the Russian-American Company (RAC) were established with the signing of the RAC-HBC Agreement in 1839.\nTo meet the new commercial obligations and to support British claims in the Oregon Question, the Hudson's Bay Company formally incorporated the Puget Sound Agricultural Company (PSAC) subsidiary in 1840. The new venture, while nominally independent, was administratively included within the Columbia Department. McLoughlin criticized the idea of a fur trading monopoly maintaining agricultural operations, as he felt independent farmers would be efficient. Nonetheless, he was appointed as the PSAC supervisor.\nThe fertile plains near the Cowlitz River were selected as a suitable location for Cowlitz Farm, the principal PSAC farm. Fort Nisqually was also assigned to the PSAC, where numerous livestock herds were maintained. Several locations were considered for potential farmers, including among the French Canadian and M\u00e9tis farmers of the Willamette Valley, Scotland, and the Red River colony.\nIn November 1839, Sir George Simpson instructed Duncan Finlayson to begin promoting the PSAC among the Red River colonists. James Sinclair was appointed by Finlayson to guide the mostly M\u00e9tis settler families to Fort Vancouver. In June 1841, the party left Fort Garry with 23 families consisting of 121 people. When they arrived at Fort Vancouver, they numbered 21 families of 116 people. Fourteen families were relocated to Fort Nisqually, while the remaining seven families were sent to Fort Cowlitz.\nJapanese shipwreck.\nWhen three Japanese sailors, among them Otokichi, were shipwrecked on the Olympic Peninsula in 1834, McLoughlin thought they might present an opportunity to open trade with Japan. He sent the three men to London on the \"Eagle\" to try to convince the Crown of his plan. They reached London in 1835, probably the first Japanese to do so since the 16th-century Christopher and Cosmas. When the British government did not show interest, the castaways were sent to Macau so that they could be returned to Japan. Even that was not possible, as Japan did not allow any outside ships to enter its waters.\nRelations with American settlers.\nIn 1821, with the merger of HBC and the North West Company, the British Parliament imposed the laws of Upper Canada on British subjects in Rupert\u2019s Land and the Columbia District, and gave the authority to enforce those laws to the newly reconfigured Hudson's Bay Company. McLoughlin, as Superintendent of Fort Vancouver, applied the law to British subjects, kept peace with the natives and sought to maintain law and order over American settlers as well.\nIn August 1828, McLoughlin was in charge at Fort Vancouver when American explorer Jedediah Smith, John Turner, Arthur Black, and Richard Leland arrived, the only survivors of the massacre of fifteen members of his exploring party by Umpqua people, who lived to the south in Oregon. McLoughlin sent a party headed by Alexander Roderick McLeod to recover Smith's property.\nIn the early 1840s, with the arrival of the first wagon trains via the Oregon Trail, McLoughlin disobeyed company orders and extended substantial aid to the American settlers. Relations between Britain and the United States had become very strained, and many expected war to break out any time. McLoughlin's aid probably prevented an armed attack on his outpost by the numerous American settlers. The settlers understood that his motives were not purely altruistic, and some resented the assistance, working against him for the rest of his life.\nPressure over the Pacific Northwest.\nAs tensions mounted in the Oregon boundary dispute; Simpson, realizing that border might ultimately be as far north as the 49th parallel, ordered McLoughlin to relocate their regional headquarters to Vancouver Island. McLoughlin, in turn, directed James Douglas to construct Fort Camosun (now Victoria, British Columbia, Canada) in 1843. But McLoughlin, whose life was increasingly connected to the Willamette River Valley, refused to move there.\nMcLoughlin was involved with the debate over the future of the Oregon Country. He advocated an independent nation that would be free of the United States during debates at the Oregon Lyceum in 1842 through his lawyer. This view won support at first and a resolution adopted but was later moved away from in favor of a resolution by George Abernethy of the Methodist Mission to wait on forming an independent country.\nIn 1843, American settlers established their own government, called the Provisional Government of Oregon. A legislative committee drafted a code of laws known as the Organic Law. It included the creation of an executive committee of three, a judiciary, militia, land laws, and four counties. There was vagueness and confusion over the nature of the 1843 Organic Law, in particular, whether it was constitutional or statutory. In 1844, a new legislative committee decided to consider it statutory. The 1845 Organic Law made additional changes, including allowing the participation of British subjects in the government. Although the Oregon Treaty of 1846 settled the boundaries of US jurisdiction upon all lands south of the 49th parallel, the Provisional Government continued to function until 1849, when the first governor of Oregon Territory arrived.\nPersonal life.\nMcLoughlin's first child, Joseph, was born in 1809. The name of Joseph's mother is unknown, but it is likely that she was Ojibwe. Around 1810, McLoughlin entered into a relationship with Marguerite Waddens McKay. McKay was the daughter of Jean-\u00c9tienne Waddens, who was one of the original partners of the North West Company, and an indigenous woman whose name is unknown. She was the widow of Alexander McKay, a trader killed in the \"Tonquin\" incident. Her son Thomas became McLoughlin's stepson. McLoughlin and McKay had four children: John Jr., Elisabeth, Eloisa, and David. They were legally married in 1842 at Fort Vancouver.\nMcLoughlin's appearance, 6 foot 4\u00a0inches (193\u00a0cm) tall with long, prematurely white hair, brought him respect; but he was also generally known for his fair treatment of the people with whom he dealt, whether they were British subjects, U.S. citizens, or of indigenous origin (notwithstanding for example, his asymmetric use of force against the S'Klallam tribe after an earlier raid\u2014an HBC ship under his command fired its cannons into an unrelated village near Port Townsend in the early morning, killing twenty-seven people and leveling the village.) . At the time, the wives of many Hudson's Bay field employees were indigenous, including McLoughlin's own wife.\nJohn McLoughlin lost one son to a violent death. John McLoughlin, Jr. had been appointed the second Clerk in Charge at Fort Stikine, only to die in April 1842 at the hands of one of the fort employees, Urbain Heroux, who was charged with his murder but acquitted for lack of evidence, which added to the grievances John Sr. held against the company.\nLater life in the Oregon Territory.\nAfter resigning from the Hudson's Bay Company in 1846, McLoughlin moved his family south to Oregon City in the Willamette Valley. The Oregon Treaty had been ratified by that time, and the region, now known as the Oregon Territory, was part of the United States. The valley was the destination of choice for settlers streaming in over the Oregon Trail. At his Oregon City store, he sold food and farming tools to settlers.\nIn 1847, McLoughlin was given the Knighthood of St. Gregory, bestowed on him by Pope Gregory XVI, and was made a Knight Commander. He became a U.S. citizen in 1849. McLoughlin's opponents succeeded in inserting a clause forfeiting his land claim in the Donation Land Claim Act of 1850 by Samuel R. Thurston. Although it was never enforced, it embittered the elderly McLoughlin. He served as mayor of Oregon City in 1851, winning 44 of 66 votes. He died of natural causes in 1857. His grave is now located beside his home overlooking downtown Oregon City.\nLegacy.\nMcLoughlin is featured on the 1925 Fort Vancouver Centennial half dollar designed by Laura Gardin Fraser.\nIn 1953, the state of Oregon donated to the National Statuary Hall Collection a bronze statue of McLoughlin, which is currently displayed at the Capitol Visitor Center. The title \"Father of Oregon\" was officially bestowed on him by the Oregon Legislative Assembly in 1957, on the centennial of his death. Many landmarks in Oregon are named after him, including:\nMcLoughlin's former residence in Oregon City, now known as the McLoughlin House, is today a museum; it is part of the Fort Vancouver National Historic Site.\nCitations.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nBibliography.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "41545", "revid": "12331483", "url": "https://en.wikipedia.org/wiki?curid=41545", "title": "Avogadro constant", "text": "Conversion constant for amount of substance\nIn chemistry, the Avogadro constant, commonly denoted \"N\"A, is a conversion constant or ratio between an amount of substance and the number of particles that it contains. The particles in question are any designated elementary entity, such as molecules, atoms, ions, or ion pairs. It is an SI defining constant with the exact value (reciprocal mole). The numerical value of this constant when expressed in terms of the mole is known as the Avogadro number, commonly denoted \"N\"0. The Avogadro \"number\" is an exact number equal to the number of constituent particles in one mole of any substance (by definition of the mole), historically derived from the experimental determination of the number of atoms in 12\u00a0grams of carbon-12 (12C) before the 2019 revision of the SI, i.e. the gram-to-dalton ratio, g/Da. Both the constant and the number are named after the Italian physicist and chemist Amedeo Avogadro.\nThe Avogadro constant is used as a proportionality factor to define the \"amount of substance\" \"n\"(X), in a sample of a substance X, in terms of the number of elementary entities \"N\"(X) in that sample:\n formula_1.\nThe Avogadro constant \"N\"A is also the factor that converts the average mass \"m\"(X) of one particle of a substance to its molar mass \"M\"(X). That is, \"M\"(X) = \"m\"(X) \u22c5 \"N\"A. Applying this equation to 12C with an atomic mass of exactly 12\u00a0Da and a molar mass of 12\u00a0g/mol yields (after rearrangement) the following relation for the Avogadro constant: \"N\"A\u00a0=\u00a0(g/Da)\u00a0mol\u22121, making the Avogadro number \"N\"0\u00a0=\u00a0g/Da. Historically, this was precisely true, but since the 2019 revision of the SI, the relation is now merely approximate, although equality may still be assumed with high accuracy.\nThe constant \"N\"A also relates the molar volume (the volume per mole) of a substance to the average volume nominally occupied by one of its particles, when both are expressed in the same units of volume. For example, since the molar volume of water in ordinary conditions is about 18 mL/mol, the volume occupied by one molecule of water is about 18/() mL, or about (cubic nanometres). For a crystalline substance, it provides as similarly relationship between the volume of a crystal to that of its unit cell.\nDefinition.\nThe Avogadro constant was historically derived from the old definition of the mole as the amount of substance in 12\u00a0grams of carbon-12 (12C). By this old definition, the numerical value of the Avogadro constant in mol\u22121 (the Avogadro number) was a physical constant that had to be determined experimentally.\nThe historical relationship of the Avogadro constant to the molar mass of carbon-12, \"M\"(12C), and its atomic mass, \"m\"(12C), can be expressed in the following equation: formula_2Thus, \"N\"0, the numerical value of \"N\"A when expressed in mol\u22121, was equal to the number of daltons in a gram (g/Da), where the dalton is defined as of the mass of a 12C atom.\nThe redefinition of the mole in 2019, as being the amount of substance consisting of exactly elementary entities, means that the mass of 1 mole of a substance is now exactly the product of the Avogadro number and the average mass of one of the entities involved. The dalton, however, is still defined as of the mass of a 12C atom, which must be determined experimentally and is known only with finite accuracy. Thus, prior experiments that aimed to determine the numerical value of the Avogadro constant when expressed in reciprocal moles\u2014i.e. the Avogadro number (now numerically fixed)\u2014are re-interpreted as measurements of the numerical value in grams of the dalton.\nBy the old definition of mole, the numerical value of the mass of one mole of a substance expressed in grams (i.e., its molar mass in g/mol or kg/kmol), was precisely equal to the average mass of one particle expressed in daltons. With the new definition, this numerical equivalence is no longer exact, as it is affected by the uncertainty in the value of the gram-to-dalton (g/Da) mass-unit ratio. However, it may still be assumed for all practical purposes. For example, the average mass of one molecule of water is about 18.0153\u00a0daltons, and an amount of one mole of water has a corresponding macroscopic mass of about 18.0153\u00a0grams. Also, the Avogadro number is the approximate number of nucleons (protons and neutrons) in one gram of ordinary matter. \nAn amount of substance consisting of just a single elementary entity might be thought of as an \"elementary amount\", analogous to the elementary charge, \"e\". Letting \"n\"a denote this elementary amount, then 1 mol = \"N\"0 \"n\"a. With the mole defined such that \"N\"A = \"N\"0/mol, this can be rearranged as 1 mol = \"N\"0/\"N\"A. Thus, \"n\"a = 1/\"N\"A, the reciprocal of the Avogadro constant. The fundamental definition of the Avogadro constant itself is therefore one per elementary amount (\"N\"A = 1/\"n\"a), independent of any macroscopic base unit chosen for the physical quantity. (Since there is an aggregate of an Avogadro number of elementary entities in one mole, the Avogadro constant can also be \"expressed\" (in terms of the mole) as an Avogadro number per mole\u2014but this is \"not\" its \"definition\".) The Avogadro constant, a well-defined quantity value with dimension N\u22121, independent of the mole, is therefore a \"bona fide\" defining constant for the 2019 redefinition of the mole. \nIntroducing \"n\"a in place of 1/\"N\"A, means that \"n\"(X) = \"N\"(X) \"n\"a\u2014amount of substance is an aggregate of \"N\"(X) elementary entities\u2014which is easier to comprehend than \"N\"(X) \"reciprocal Avogadro constants\". Also the molar mass is then \"M\"(X) = \"m\"(X)/\"n\"a\u2014the entity mass per entity, which is self-evident. \nIn older literature, the Avogadro number was also denoted N, although that conflicts with the symbol for number of particles in statistical mechanics.\nHistory.\nOrigin of the concept.\nThe Avogadro constant is named after the Italian scientist Amedeo Avogadro (1776\u20131856), who, in 1811, first proposed that the volume of a gas (at a given pressure and temperature) is proportional to the number of atoms or molecules regardless of the nature of the gas.\nAvogadro's hypothesis was popularized four years after his death by Stanislao Cannizzaro, who advocated Avogadro's work at the Karlsruhe Congress in 1860.\nThe name \"Avogadro's number\" was coined in 1909 by the physicist Jean Perrin, who defined it as the number of molecules in exactly 32 grams of oxygen gas. This definition numerically equates the mass of a mole of a substance, in grams, to the mass of one molecule relative to the mass of a hydrogen atom. Because of the law of definite proportions, a hydrogen atom was the natural unit of atomic mass and was assumed to be of the atomic mass of oxygen.\nFirst measurements.\nThe value of Avogadro's number (not yet known by that name) was first obtained indirectly by Josef Loschmidt in 1865, by estimating the number of particles in a given volume of gas. This value, the number density \"n\"0 of particles in an ideal gas, is now called the Loschmidt constant in his honor, and is related to the Avogadro constant, \"N\"A, by\n formula_3\nwhere \"p\"0 is the pressure, \"R\" is the gas constant, and \"T\"0 is the absolute temperature. Because of this work, the symbol \"L\" is sometimes used for the Avogadro constant, and, in German literature, that name may be used for both constants, distinguished only by the units of measurement. (However, \"N\"A should not be confused with the entirely different Loschmidt constant in English-language literature.)\nPerrin himself determined the Avogadro number, which he called \"Avogadro's constant\" (constante d'Avogadro), by several different experimental methods. He was awarded the 1926 Nobel Prize in Physics, largely for this work.\nThe electric charge per mole of electrons is a constant called the Faraday constant and has been known since 1834, when Michael Faraday published his works on electrolysis. In 1910, Robert Millikan with the help of Harvey Fletcher obtained the first measurement of the charge on an electron. Dividing the charge on a mole of electrons by the charge on a single electron provided a more accurate estimate of the Avogadro number.\nX-ray crystallography.\nX-ray crystallography uses the diffraction of X-rays by a crystal to accurately measure the distances between layers in its lattice, from which the volume occupied by each atom can be determined. The Avogadro Project used this technique to measure the unit cell dimensions of extremely pure single-crystal spheres of silicon-28, with the goal of a more accurate silicon-based definition of the Avogadro constant.\nSI definition of 1971.\nIn 1971, in its 14th conference, the International Bureau of Weights and Measures (BIPM) decided to regard the amount of substance as an independent dimension of measurement, with the mole as its base unit in the International System of Units (SI). Specifically, the mole was defined as the amount of a substance that contains as many elementary entities as there are atoms in 12 grams (0.012 kilograms) of carbon-12 (12C). Thus, in particular, an amount of one mole of carbon 12 had a corresponding mass that was \"exactly\" 12 grams of that element. \nBy this definition, one mole of any substance contained exactly as many elementary entities as one mole of any other substance. However, this number \"N\"0 was a physical constant that had to be experimentally determined since it depended on the mass (in grams) of one atom of 12C, and therefore, it was known only to a limited number of decimal digits. The common rule of thumb that \"one gram of matter contains \"N\"0 nucleons\" was exact for carbon-12, but slightly inexact for other elements and isotopes.\nIn the same conference, the BIPM also named \"N\"A (the factor that related the amount of a substance to the corresponding number of particles) the \"Avogadro \"constant\"\". However, the term \"Avogadro number\" continued to be used, especially in introductory works. As a consequence of this definition, \"N\"A was not a pure number, but had the quantity dimension of reciprocal of amount of substance (N\u22121).\nSI redefinition of 2019.\nBefore 2019, the mole was defined by the amount of substance in exactly 12 grams of carbon-12. Effective 20 May 2019, the BIPM defined the Avogadro constant \"N\"A as the exact value , thus redefining the mole as the amount of a substance having exactly constituent particles of that substance. One consequence of this change is that the mass of a mole of 12C atoms is no longer exactly 0.012\u00a0kg. On the other hand, the dalton, Da ( unified atomic mass unit, u), remains unchanged as of the mass of 12C. Thus, the molar mass constant remains very close to but no longer exactly equal to 1\u00a0g/mol, although the difference ( in relative terms for the CODATA 2022 recommended value) is insignificant for all practical purposes.\nConnection to other constants.\nThe Avogadro constant \"N\"A is related to other physical constants and properties.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41547", "revid": "629503", "url": "https://en.wikipedia.org/wiki?curid=41547", "title": "Phase jitter", "text": ""}
{"id": "41548", "revid": "46866511", "url": "https://en.wikipedia.org/wiki?curid=41548", "title": "Phase-locked loop", "text": "Electronic control system\nA phase-locked loop or phase lock loop (PLL) is a control system that generates an output signal whose phase is fixed relative to the phase of an input signal. Keeping the input and output phase in lockstep also implies keeping the input and output frequencies the same, thus a phase-locked loop can also track an input frequency. Furthermore, by incorporating a frequency divider, a PLL can generate a stable frequency that is a multiple of the input frequency.\nThese properties are used for clock synchronization, demodulation, frequency synthesis, clock multipliers, and signal recovery from a noisy communication channel. Since 1969, a single integrated circuit can provide a complete PLL building block, and nowadays have output frequencies from a fraction of a hertz up to many gigahertz. Thus, PLLs are widely employed in radio, telecommunications, computers (e.g. to distribute precisely timed clock signals in microprocessors), grid-tie inverters (electronic power converters used to integrate DC renewable resources and storage elements such as photovoltaics and batteries with the power grid), and other electronic applications.\nSimple example.\nA simple analog PLL is an electronic circuit consisting of a variable frequency oscillator and a phase detector in a feedback loop (Figure 1). The oscillator generates a periodic signal Vo with frequency proportional to an applied voltage, hence the term voltage-controlled oscillator (VCO). The phase detector compares the phase of the VCO's output signal with the phase of periodic input reference signal Vi and outputs a voltage (stabilized by the filter) to adjust the oscillator's frequency to match the phase of Vo to the phase of Vi.\nClock analogy.\nPhase can be proportional to time, so a phase difference can correspond to a time difference.\nLeft alone, different clocks will mark time at slightly different rates. A mechanical clock, for example, might be fast or slow by a few seconds per hour compared to a reference atomic clock (such as the NIST-F2). That time difference becomes substantial over time. Instead, the owner can synchronize their mechanical clock (with varying degrees of accuracy) by phase-locking it to a reference clock.\nAn inefficient synchronization method involves the owner resetting their clock to that more accurate clock's time every week. But, left alone, their clock will still continue to diverge from the reference clock at the same few seconds per hour rate.\nA more efficient synchronization method (analogous to the simple PLL in Figure 1) utilizes the fast-slow timing adjust control (analogous to how the VCO's frequency can be adjusted) available on some clocks. Analogously to the phase comparator, the owner could notice their clock's misalignment and turn its timing adjust a small proportional amount to make their clock's frequency a little slower (if their clock was fast) or faster (if their clock was slow). If they don't overcompensate, then their clock will be more accurate than before. Over a series of such weekly adjustments, their clock's notion of a second would agree close enough with the reference clock, so they could be said to be locked both in frequency and phase.\nAn early electromechanical version of a phase-locked loop was used in 1921 in the Shortt-Synchronome clock.\nHistory.\nSpontaneous synchronization of weakly coupled pendulum clocks was noted by the Dutch physicist Christiaan Huygens as early as 1673. Around the turn of the 19th century, Lord Rayleigh observed synchronization of weakly coupled organ pipes and tuning forks. In 1919, W. H. Eccles and J. H. Vincent found that two electronic oscillators that had been tuned to oscillate at slightly different frequencies but that were coupled to a resonant circuit would soon oscillate at the same frequency. Automatic synchronization of electronic oscillators was described in 1923 by Edward Victor Appleton.\nIn 1925, David Robertson, first professor of electrical engineering at the University\u00a0of Bristol, introduced phase locking in his clock design to control the striking of the bell Great George in the new Wills Memorial Building. Robertson's clock incorporated an electromechanical device that could vary the rate of oscillation of the pendulum, and derived correction signals from a circuit that compared the pendulum phase with that of an incoming telegraph pulse from Greenwich Observatory every morning at 10:00 GMT. Including equivalents of every element of a modern electronic PLL, Robertson's system was notably ahead of its time in that its phase detector was a relay logic implementation of the transistor circuits for phase/frequency detectors not seen until the 1970s.\u00a0\nRobertson's work predated research towards what was later named the phase-lock loop in 1932, when British researchers developed an alternative to Edwin Armstrong's superheterodyne receiver, the Homodyne or direct-conversion receiver. In the homodyne or synchrodyne system, a local oscillator was tuned to the desired input frequency and multiplied with the input signal. The resulting output signal included the original modulation information. The intent was to develop an alternative receiver circuit that required fewer tuned circuits than the superheterodyne receiver. Since the local oscillator would rapidly drift in frequency, an automatic correction signal was applied to the oscillator, maintaining it in the same phase and frequency of the desired signal. The technique was described in 1932, in a paper by Henri de Bellescize, in the French journal \"L'Onde \u00c9lectrique\".\nIn analog television receivers since at least the late 1930s, phase-locked-loop horizontal and vertical sweep circuits are locked to synchronization pulses in the broadcast signal.\nIn 1969, Signetics introduced a line of low-cost monolithic integrated circuits like the NE565 using bipolar transistors, that were complete phase-locked loop systems on a chip, and applications for the technique multiplied. A few years later, RCA introduced the CD4046 Micropower Phase-Locked Loop using CMOS, which also became a popular integrated circuit building block.\nStructure and function.\nPhase-locked loop mechanisms may be implemented as either analog or digital circuits. Both implementations use the same basic structure.\nAnalog PLL circuits include four basic elements:\nVariations.\nThere are several variations of PLLs. Some terms that are used are \"analog phase-locked loop\" (APLL), also referred to as a linear phase-locked loop\" (LPLL), \"digital phase-locked loop\" (DPLL), \"all digital phase-locked loop\" (ADPLL), and \"software phase-locked loop\" (SPLL).\nApplications.\nPhase-locked loops are widely used for synchronization purposes; in space communications for coherent demodulation and threshold extension, bit synchronization, and symbol synchronization. Phase-locked loops can also be used to demodulate frequency-modulated signals. In radio transmitters, a PLL is used to synthesize new frequencies which are a multiple of a reference frequency, with the same stability as the reference frequency.\nOther applications include:\nClock recovery.\nSome data streams, especially high-speed serial data streams (such as the raw stream of data from the magnetic head of a disk drive), are sent without an accompanying clock. The receiver generates a clock from an approximate frequency reference, and then uses a PLL to phase-align it to the data stream's signal edges. This process is referred to as clock recovery. For this scheme to work, the data stream must have edges frequently-enough to correct any drift in the PLL's oscillator. Thus a line code with a hard upper bound on the maximum time between edges (e.g. 8b/10b encoding) is typically used to encode the data.\nDeskewing.\nIf a clock is sent in parallel with data, that clock can be used to sample the data. Because the clock must be received and amplified before it can drive the flip-flops which sample the data, there will be a finite, and process-, temperature-, and voltage-dependent delay between the detected clock edge and the received data window. This delay limits the frequency at which data can be sent. One way of eliminating this delay is to include a deskew PLL on the receive side, so that the clock at each data flip-flop is phase-matched to the received clock. In that type of application, a special form of a PLL called a delay-locked loop (DLL) is frequently used.\nClock generation.\nMany electronic systems include processors of various sorts that operate at hundreds of megahertz to gigahertz, well above the practical frequencies of crystal oscillators. Typically, the clocks supplied to these processors come from clock generator PLLs, which multiply a lower-frequency reference clock (usually 50 or 100\u00a0MHz) up to the operating frequency of the processor. The multiplication factor can be quite large in cases where the operating frequency is multiple gigahertz and the reference crystal is just tens or hundreds of megahertz.\nSpread spectrum.\nAll electronic systems emit some unwanted radio frequency energy. Various regulatory agencies (such as the FCC in the United States) put limits on the emitted energy and any interference caused by it. The emitted noise generally appears at sharp spectral peaks (usually at the operating frequency of the device, and a few harmonics). A system designer can use a spread-spectrum PLL to reduce interference with high-Q receivers by spreading the energy over a larger portion of the spectrum. For example, by changing the operating frequency up and down by a small amount (about 1%), a device running at hundreds of megahertz can spread its interference evenly over a few megahertz of spectrum, which drastically reduces the amount of noise seen on broadcast FM radio channels, which have a bandwidth of several tens of kilohertz.\nClock distribution.\nTypically, the reference clock enters the chip and drives a phase locked loop (PLL), which then drives the system's clock distribution. The clock distribution is usually balanced so that the clock arrives at every endpoint simultaneously. One of those endpoints is the PLL's feedback input. The function of the PLL is to compare the distributed clock to the incoming reference clock, and vary the phase and frequency of its output until the reference and feedback clocks are phase and frequency matched.\nPLLs are ubiquitous\u2014they tune clocks in systems several feet across, as well as clocks in small portions of individual chips. Sometimes the reference clock may not actually be a pure clock at all, but rather a data stream with enough transitions that the PLL is able to recover a regular clock from that stream. Sometimes the reference clock is the same frequency as the clock driven through the clock distribution, other times the distributed clock may be some rational multiple of the reference.\nAM detection.\nA PLL may be used to synchronously demodulate amplitude modulated (AM) signals. The PLL recovers the phase and frequency of the incoming AM signal's carrier. The recovered phase at the VCO differs from the carrier's by 90\u00b0, so it is shifted in phase to match, and then fed to a multiplier. The output of the multiplier contains both the sum and the difference frequency signals, and the demodulated output is obtained by low-pass filtering. Since the PLL responds only to the carrier frequencies which are very close to the VCO output, a PLL AM detector exhibits a high degree of selectivity and noise immunity which is not possible with conventional peak type AM demodulators. However, the loop may lose lock where AM signals have 100% modulation depth.\nJitter and noise reduction.\nOne desirable property of all PLLs is that the reference and feedback clock edges be brought into very close alignment. The average difference in time between the phases of the two signals when the PLL has achieved lock is called the static phase offset (also called the steady-state phase error). The variance between these phases is called tracking jitter. Ideally, the static phase offset should be zero, and the tracking jitter should be as low as possible.\nPhase noise is another type of jitter observed in PLLs, and is caused by the oscillator itself and by elements used in the oscillator's frequency control circuit. Some technologies are known to perform better than others in this regard. The best digital PLLs are constructed with emitter-coupled logic (ECL) elements, at the expense of high power consumption. To keep phase noise low in PLL circuits, it is best to avoid saturating logic families such as transistor-transistor logic (TTL) or CMOS.\nAnother desirable property of all PLLs is that the phase and frequency of the generated clock be unaffected by rapid changes in the voltages of the power and ground supply lines, as well as the voltage of the substrate on which the PLL circuits are fabricated. This is called substrate and supply noise rejection. The higher the noise rejection, the better.\nTo further improve the phase noise of the output, an injection locked oscillator can be employed following the VCO in the PLL.\nFrequency synthesis.\nIn digital wireless communication systems (GSM, CDMA etc.), PLLs are used to provide the local oscillator up-conversion during transmission and down-conversion during reception. In most cellular handsets this function has been largely integrated into a single integrated circuit to reduce the cost and size of the handset. However, due to the high performance required of base station terminals, the transmission and reception circuits are built with discrete components to achieve the levels of performance required. GSM local oscillator modules are typically built with a frequency synthesizer integrated circuit and discrete resonator VCOs.\nPhase angle reference.\nGrid-tie inverters based on voltage source inverters source or sink real power into the AC electric grid as a function of the phase angle of the voltage they generate relative to the grid's voltage phase angle, which is measured using a PLL. In photovoltaic applications, the more the sine wave produced leads the grid voltage wave, the more power is injected into the grid. For battery applications, the more the sine wave produced lags the grid voltage wave, the more the battery charges from the grid, and the more the sine wave produced leads the grid voltage wave, the more the battery discharges into the grid.\nBlock diagram.\nThe block diagram shown in the figure shows an input signal, \"F\"\"I\", which is used to generate an output, \"F\"\"O\". The input signal is often called the \"reference signal\" (also abbreviated \"F\"\"REF\").\nAt the input, a phase detector (shown as the Phase frequency detector and Charge pump blocks in the figure) compares two input signals, producing an error signal which is proportional to their phase difference. The error signal is then low-pass filtered and used to drive a VCO which creates an output phase. The output is fed through an optional divider back to the input of the system, producing a negative feedback loop. If the output phase drifts, the error signal will increase, driving the VCO phase in the opposite direction so as to reduce the error. Thus the output phase is locked to the phase of the input.\nAnalog phase locked loops are generally built with an analog phase detector, low-pass filter and VCO placed in a negative feedback configuration. A digital phase locked loop uses a digital phase detector; it may also have a divider in the feedback path or in the reference path, or both, in order to make the PLL's output signal frequency a rational multiple of the reference frequency. A non-integer multiple of the reference frequency can also be created by replacing the simple divide-by-\"N\" counter in the feedback path with a programmable pulse swallowing counter. This technique is usually referred to as a fractional-N synthesizer or fractional-N PLL.\nThe oscillator generates a periodic output signal. Assume that initially the oscillator is at nearly the same frequency as the reference signal. If the phase from the oscillator falls behind that of the reference, the phase detector changes the control voltage of the oscillator so that it speeds up. Likewise, if the phase creeps ahead of the reference, the phase detector changes the control voltage to slow down the oscillator. Since initially the oscillator may be far from the reference frequency, practical phase detectors may also respond to frequency differences, so as to increase the lock-in range of allowable inputs. Depending on the application, either the output of the controlled oscillator, or the control signal to the oscillator, provides the useful output of the PLL system.\nElements.\nPhase detector.\nA phase detector (PD) generates a voltage, which represents the phase difference between two signals. In a PLL, the two inputs of the phase detector are the reference input and the feedback from the VCO. The PD output voltage is used to control the VCO such that the phase difference between the two inputs is held constant, making it a negative feedback system.\nDifferent types of phase detectors have different performance characteristics.\nFor instance, the frequency mixer produces harmonics that adds complexity in applications where spectral purity of the VCO signal is important. The resulting unwanted (spurious) sidebands, also called \"reference spurs\" can dominate the filter requirements and reduce the capture range well below or increase the lock time beyond the requirements. In these applications the more complex digital phase detectors are used which do not have as severe a reference spur component on their output. Also, when in lock, the steady-state phase difference at the inputs using this type of phase detector is near 90 degrees.\nIn PLL applications it is frequently required to know when the loop is out of lock. The more complex digital phase-frequency detectors usually have an output that allows a reliable indication of an out of lock condition.\nAn XOR gate is often used for digital PLLs as an effective yet simple phase detector. It can also be used in an analog sense with only slight modification to the circuitry.\nFilter.\nThe block commonly called the PLL loop filter (usually a low-pass filter) generally has two distinct functions.\nThe primary function is to determine loop dynamics, also called stability. This is how the loop responds to disturbances, such as changes in the reference frequency, changes of the feedback divider, or at startup. Common considerations are the range over which the loop can achieve lock (pull-in range, lock range or capture range), how fast the loop achieves lock (lock time, lock-up time or settling time) and damping behavior. Depending on the application, this may require one or more of the following: a simple proportion (gain or attenuation), an integral (low-pass filter) and/or derivative (high-pass filter). Loop parameters commonly examined for this are the loop's gain margin and phase margin. Common concepts in control theory including the PID controller are used to design this function.\nThe second common consideration is limiting the amount of reference frequency energy (ripple) appearing at the phase detector output that is then applied to the VCO control input. This frequency modulates the VCO and produces FM sidebands commonly called \"reference spurs\".\nThe design of this block can be dominated by either of these considerations, or can be a complex process juggling the interactions of the two. The typical trade-off of increasing the bandwidth is degraded stability. Conversely, the tradeoff of extra damping for better stability is reduced speed and increased settling time. Often the phase-noise is also affected.\nOscillator.\nAll phase-locked loops employ an oscillator element with variable frequency capability. This can be an analog VCO either driven by analog circuitry in the case of an APLL or driven digitally through the use of a digital-to-analog converter as is the case for some DPLL designs. Ring oscillators are commonly used in CMOS technology for low-area applications , whereas cross-coupled LC oscillators are preferred in low-phase noise applications. Pure digital oscillators such as a numerically controlled oscillator are used in ADPLLs.\nFeedback path and optional divider.\nPLLs may include a divider between the oscillator and the feedback input to the phase detector to produce a frequency synthesizer. A programmable divider is particularly useful in radio transmitter applications and for computer clocking, since a large number of frequencies can be produced from a single stable, accurate, quartz crystal\u2013controlled reference oscillator (which were expensive before commercial-scale hydrothermal synthesis provided cheap synthetic quartz).\nSome PLLs also include a divider between the reference clock and the reference input to the phase detector. If the divider in the feedback path divides by formula_1 and the reference input divider divides by formula_2, it allows the PLL to multiply the reference frequency by formula_3. It might seem simpler to just feed the PLL a lower frequency, but in some cases the reference frequency may be constrained by other issues, and then the reference divider is useful.\nFrequency multiplication can also be attained by locking the VCO output to the \"N\"th harmonic of the reference signal. Instead of a simple phase detector, the design uses a harmonic mixer (sampling mixer). The harmonic mixer turns the reference signal into an impulse train that is rich in harmonics. The VCO output is coarse tuned to be close to one of those harmonics. Consequently, the desired harmonic mixer output (representing the difference between the \"N\" harmonic and the VCO output) falls within the loop filter passband.\nIt should also be noted that the feedback is not limited to a frequency divider. This element can be other elements such as a frequency multiplier, or a mixer. The multiplier will make the VCO output a sub-multiple (rather than a multiple) of the reference frequency. A mixer can translate the VCO frequency by a fixed offset. It may also be a combination of these. For example, a divider following a mixer allows the divider to operate at a much lower frequency than the VCO without a loss in loop gain.\nModeling.\nTime domain model of APLL.\nThe equations governing a phase-locked loop with an analog multiplier as the phase detector and linear filter may be derived as follows. Let the input to the phase detector be formula_4 and the output of the VCO is formula_5 with phases formula_6 and formula_7. The functions formula_8 and formula_9 describe waveforms of signals. Then the output of the phase detector formula_10 is given by\nformula_11\nThe VCO frequency is usually taken as a function of the VCO input\nformula_12 as\nformula_13\nwhere formula_14 is the \"sensitivity\" of the VCO and is expressed in Hz / V;\nformula_15 is a free-running frequency of VCO.\nThe loop filter can be described by a system of linear differential equations\nformula_16\nwhere formula_10 is an input of the filter,\nformula_12 is an output of the filter, formula_19 is\nformula_20-by-formula_20 matrix,\nformula_22. formula_23 represents an initial state of the filter. The star symbol is a conjugate transpose.\nHence the following system describes PLL\nformula_24\nwhere formula_25 is an initial phase shift.\nPhase domain model of APLL.\nConsider the input of PLL formula_4 and VCO output\nformula_5 are high frequency signals. Then for any piecewise differentiable formula_28-periodic functions\nformula_29 and formula_9 there is a function formula_31 such that the output formula_32 of Filter\nformula_33\nin phase domain is asymptotically equal (the difference formula_34 is small with respect to the frequencies) to the output of the Filter in time domain model. \nHere function formula_31 is a phase detector characteristic.\nDenote by formula_36 the phase difference\nformula_37\nThen the following dynamical system describes PLL behavior\nformula_38\nHere formula_39; formula_40 is the frequency of a reference oscillator (we assume that formula_15 is constant).\nExample.\nConsider sinusoidal signals\nformula_42\nand a simple one-pole RC circuit as a filter. The time-domain model takes the form\nformula_43\nPD characteristics for this signals is equal to\nformula_44\nHence the phase domain model takes the form\nformula_45\nThis system of equations is equivalent to the equation of mathematical pendulum\nformula_46\nLinearized phase domain model.\nPhase locked loops can also be analyzed as control systems by applying the Laplace transform. The loop response can be written as\nformula_47\nWhere\nThe loop characteristics can be controlled by inserting different types of loop filters. The simplest filter is a one-pole RC circuit. The loop transfer function in this case is\nformula_53\nThe loop response becomes:\nformula_54\nThis is the form of a classic harmonic oscillator. The denominator can be related to that of a second order system:\nformula_55\nwhere formula_56 is the damping factor and formula_57 is the natural frequency of the loop.\nFor the one-pole RC filter,\nformula_58\nformula_59\nThe loop natural frequency is a measure of the response time of the loop, and the damping factor is a measure of the overshoot and ringing. Ideally, the natural frequency should be high and the damping factor should be near 0.707 (critical damping). With a single pole filter, it is not possible to control the loop frequency and damping factor independently. For the case of critical damping,\nformula_60\nformula_61\nA slightly more effective filter, the lag-lead filter includes one pole and one zero. This can be realized with two resistors and one capacitor. The transfer function for this filter is\nformula_62\nThis filter has two time constants\nformula_63\nformula_64\nSubstituting above yields the following natural frequency and damping factor\nformula_65\nformula_66\nThe loop filter components can be calculated independently for a given natural frequency and damping factor\nformula_67\nformula_68\nReal world loop filter design can be much more complex e.g. using higher order filters to reduce various types or source of phase noise. (See the D Banerjee ref below)\nImplementing a digital phase-locked loop in software.\nDigital phase locked loops can be implemented in hardware, using integrated circuits such as a CMOS 4046. However, with microcontrollers becoming faster, it may make sense to implement a phase locked loop in software for applications that do not require locking onto signals in the MHz range or faster, such as precisely controlling motor speeds. Software implementation has several advantages including easy customization of the feedback loop including changing the multiplication or division ratio between the signal being tracked and the output oscillator. Furthermore, a software implementation is useful to understand and experiment with. As an example of a phase-locked loop implemented using a phase frequency detector is presented in MATLAB, as this type of phase detector is robust and easy to implement.\n% This example is written in MATLAB\n% Initialize variables\nvcofreq = zeros(1, numiterations);\nervec = zeros(1, numiterations);\n% Keep track of last states of reference, signal, and error signal\nqsig = 0; qref = 0; lref = 0; lsig = 0; lersig = 0;\nphs = 0;\nfreq = 0;\n% Loop filter constants (proportional and derivative)\n% Currently powers of two to facilitate multiplication by shifts\nprop = 1 / 128;\nderiv = 64;\nfor it = 1:numiterations\n % Simulate a local oscillator using a 16-bit counter\n phs = mod(phs + floor(freq / 2 ^ 16), 2 ^ 16);\n ref = phs &lt; 32768;\n % Get the next digital value (0 or 1) of the signal to track\n sig = tracksig(it);\n % Implement the phase-frequency detector\n rst = ~ (qsig &amp; qref); % Reset the \"flip-flop\" of the phase-frequency\n % detector when both signal and reference are high\n qsig = (qsig | (sig &amp; ~ lsig)) &amp; rst; % Trigger signal flip-flop and leading edge of signal\n qref = (qref | (ref &amp; ~ lref)) &amp; rst; % Trigger reference flip-flop on leading edge of reference\n lref = ref; lsig = sig; % Store these values for next iteration (for edge detection)\n ersig = qref - qsig; % Compute the error signal (whether frequency should increase or decrease)\n % Error signal is given by one or the other flip flop signal\n % Implement a pole-zero filter by proportional and derivative input to frequency\n filtered_ersig = ersig + (ersig - lersig) * deriv;\n % Keep error signal for proportional output\n lersig = ersig;\n % Integrate VCO frequency using the error signal\n freq = freq - 2 ^ 16 * filtered_ersig * prop;\n % Frequency is tracked as a fixed-point binary fraction\n % Store the current VCO frequency\n vcofreq(1, it) = freq / 2 ^ 16;\n % Store the error signal to show whether signal or reference is higher frequency\n ervec(1, it) = ersig;\nend\nIn this example, an array codice_1 is assumed to contain a reference signal to be tracked. The oscillator is implemented by a counter, with the most significant bit of the counter indicating the on/off status of the oscillator. This code simulates the two D-type flip-flops that comprise a phase-frequency comparator. When either the reference or signal has a positive edge, the corresponding flip-flop switches high. Once both reference and signal is high, both flip-flops are reset. Which flip-flop is high determines at that instant whether the reference or signal leads the other. The error signal is the difference between these two flip-flop values. The pole-zero filter is implemented by adding the error signal and its derivative to the filtered error signal. This in turn is integrated to find the oscillator frequency.\nIn practice, one would likely insert other operations into the feedback of this phase-locked loop. For example, if the phase locked loop were to implement a frequency multiplier, the oscillator signal could be divided in frequency before it is compared to the reference signal.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41549", "revid": "48643156", "url": "https://en.wikipedia.org/wiki?curid=41549", "title": "Phase noise", "text": "Frequency domain representation of random fluctuations in the phase of a waveform\nIn signal processing, phase noise is the frequency-domain representation of random fluctuations in the phase of a waveform, corresponding to time-domain deviations from perfect periodicity (jitter). Generally speaking, radio-frequency engineers speak of the phase noise of an oscillator, whereas digital-system engineers work with the jitter of a clock.\nDefinitions.\nAn ideal oscillator would generate a pure sine wave. In the frequency domain, this would be represented as a single pair of Dirac delta functions (positive and negative conjugates) at the oscillator's frequency; i.e., all the signal's power is at a single frequency. All real oscillators have phase modulated noise components. The phase noise components spread the power of a signal to adjacent frequencies, resulting in noise sidebands.\nConsider the following noise-free signal:\nformula_1\nPhase noise is added to this signal by adding a stochastic process represented by formula_2 to the signal as follows:\nformula_3\nDifferent phase noise processes, formula_2, possess different power Spectral density (PSD). For example, a white noise PSD follows a formula_5 trend, a pink noise PSD follows a formula_6 trend, and a brown noise PSD follows a formula_7 trend.\nformula_8 is the single-sided (f&gt;0) phase noise PSD formula_9, given by the Fourier transform of the Autocorrelation of the phase noise.\nformula_10\nThe noise can also be represented at the single-sided (f&gt;0) frequency noise PSD, formula_11, or the fractional frequency stability PSD, formula_12, which defines the frequency fluctuations in terms of the deviation from the carrier frequency, formula_13.\nformula_14\nformula_15\nThe phase noise can also be given as the spectral purity, formula_16, the single-sideband power in a 1\u00a0Hz bandwidth at a frequency offset, f, from the carrier frequency, formula_13, referenced to the carrier power.\nformula_18\nThe stochastic dynamics of formula_2 due to white noise can be understood as a random walk or diffusion process, with a characteristic diffusion coefficient formula_20. The variance of formula_2 grows with time formula_22 according to formula_23, and the PSD of the signal formula_24 yields what is described above in case of white noise perturbation. If phase diffusion is faster (larger formula_20) for a given signal amplitude formula_26, the PSD becomes wider and shorter, yielding larger phase noise. The problem of determining the PSD (phase noise) is commensurate with determining formula_20.\nJitter conversions.\nPhase noise is sometimes also measured and expressed as a power obtained by integrating \u2112(\"f\") over a certain range of offset frequencies. For example, the phase noise may be \u221240\u00a0dBc integrated over the range of 1\u00a0kHz to 100\u00a0kHz. This integrated phase noise (expressed in degrees) can be converted to jitter (expressed in seconds) using the following formula:\nformula_28\nIn the absence of 1/f noise in a region where the phase noise displays a \u201320dBc/decade slope (Leeson's equation), the RMS cycle jitter can be related to the phase noise by:\n formula_29\nLikewise:\n formula_30\nMeasurement.\nPhase noise can be measured using a spectrum analyzer if the phase noise of the device under test (DUT) is large with respect to the spectrum analyzer's local oscillator. Care should be taken that observed values are due to the measured signal and not the shape factor of the spectrum analyzer's filters. Spectrum analyzer based measurement can show the phase-noise power over many decades of frequency; e.g., 1\u00a0Hz to 10\u00a0MHz. The slope with offset frequency in various offset frequency regions can provide clues as to the source of the noise; e.g., low frequency flicker noise decreasing at 30\u00a0dB per decade (= 9\u00a0dB per octave).\nPhase noise measurement systems are alternatives to spectrum analyzers. These systems may use internal and external references and allow measurement of both residual (additive) and absolute noise. Additionally, these systems can make low-noise, close-to-the-carrier, measurements.\nLinewidths.\nThe sinusoidal output of an ideal oscillator is a Dirac delta function in the power spectral density centered at the frequency of the sinusoid. Such perfect spectral purity is not achievable in a practical oscillator. Spreading of the spectrum line caused by phase noise is characterized by the fundamental linewidth and the integral linewidth.\nThe fundamental linewidth, also known as the White noise-limited linewidth or the intrinsic linewidth, is the linewidth of an oscillator's PSD in the presence of only white noise sources (noise with a PSD that follows a formula_5 trend, ie. equivalent across all frequencies). The fundamental linewidth takes Lorentzian spectral line shape, and is given by twice the phase diffusion constant, formula_32 (full width, half maximum). White noise provides a formula_33 Allan Deviation plot at small averaging times.\nThe integral linewidth, also known as the effective linewidth or the total linewidth, is the linewidth of an oscillator's PSD in the presence of both white noise sources (noise with a PSD that follows a formula_5 trend) and pink noise sources (noise with a PSD that follows a formula_6 trend). Pink noise is sometimes called Flicker noise, or simply 1/f noise. The integral linewidth takes Voigt lineshape, a convolution of the white noise-induced Lorentzian lineshape and the pink noise-induced Gaussian lineshape. Pink noise provides a formula_36 Allan Deviation plot at moderate averaging times. This flat line on the Allan Deviation plot is also known as the flicker floor.\nAdditionally, the oscillator might experience Frequency drift over long periods of time, slowly moving the center frequency of the Voigt lineshape. This drift is a brown noise source (noise with a PSD that follows a formula_7 trend), and provides a formula_38 Allan Deviation plot at large averaging times.\nLimiting System Performance.\nA laser is a common oscillator that is characterized by its noise, and thus its Laser linewidth. The laser noise provides fundamental limitations of the systems that the laser is used in, such as loss of sensitivity in radar and communications systems, lack of definition in imaging systems, and a higher bit error rate in digital systems.\nLasers with a near-Infrared center wavelength are used in many atomic, molecular, and optical physics experiments to provide photons that interact with atoms. The requirements for the spectral purity at specific frequency offsets of the lasers used in qubit operation (such as clock transition lasers and state preparation lasers) are highly stringent because the coherence time of the qubit is directly related to the linewidth of the lasers.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41550", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41550", "title": "Phase perturbation", "text": "Phase perturbation is the shifting, from whatever cause, in the phase of an electronic signal. The shifting is often quite rapid, and may appear to be random or cyclic. The phase departure in phase perturbation usually is larger, but less rapid, than in phase jitter.\nPhase perturbation may be expressed in degrees, with any cyclic component expressed in hertz.\nReferences.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41551", "revid": "18731412", "url": "https://en.wikipedia.org/wiki?curid=41551", "title": "Phase-shift keying", "text": "Type of data encoding\nPhase-shift keying (PSK) is a digital modulation process which conveys data by changing (modulating) the phase of a constant frequency carrier wave. The modulation is accomplished by varying the sine and cosine inputs at a precise time. It is widely used for wireless LANs, RFID and Bluetooth communication.\nAny digital modulation scheme uses a finite number of distinct signals to represent digital data. PSK uses a finite number of phases, each assigned a unique pattern of binary digits. Usually, each phase encodes an equal number of bits. Each pattern of bits forms the symbol that is represented by the particular phase. The demodulator, which is designed specifically for the symbol-set used by the modulator, determines the phase of the received signal and maps it back to the symbol it represents, thus recovering the original data. This requires the receiver to be able to compare the phase of the received signal to a reference signal\u00a0\u2013 such a system is termed coherent (and referred to as CPSK).\nCPSK requires a complicated demodulator, because it must extract the reference wave from the received signal and keep track of it, to compare each sample to. Alternatively, the phase shift of each symbol sent can be measured with respect to the phase of the previous symbol sent. Because the symbols are encoded in the difference in phase between successive samples, this is called differential phase-shift keying (DPSK). DPSK can be significantly simpler to implement than ordinary PSK, as it is a 'non-coherent' scheme, i.e. there is no need for the demodulator to keep track of a reference wave. A trade-off is that it has more demodulation errors.\nIntroduction.\nThere are three major classes of digital modulation techniques used for transmission of digitally represented data:\nAll convey data by changing some aspect of a base signal, the carrier wave (usually a sinusoid), in response to a data signal. In the case of PSK, the phase is changed to represent the data signal. There are two fundamental ways of utilizing the phase of a signal in this way:\nA convenient method to represent M-ary transmission PSK schemes is on a constellation diagram. This shows the points in the complex plane where, in this context, the real and imaginary axes are termed the in-phase and quadrature axes respectively due to their 90\u00b0 separation. Such a representation on perpendicular axes lends itself to straightforward implementation. The amplitude of each point along the in-phase axis is used to modulate a cosine (or sine) wave and the amplitude along the quadrature axis to modulate a sine (or cosine) wave. By convention, in-phase modulates cosine and quadrature modulates sine.\nIn PSK, the constellation points chosen are usually positioned with uniform angular spacing around a circle. This gives maximum phase-separation between adjacent points and thus the best immunity to corruption. They are positioned on a circle so that they can all be transmitted with the same energy. In this way, the moduli of the complex numbers they represent will be the same and thus so will the amplitudes needed for the cosine and sine waves. Two common examples are \"binary phase-shift keying\" (BPSK) which uses two phases, and \"quadrature phase-shift keying\" (QPSK) which uses four phases, although any number of phases may be used. Since the data to be conveyed are usually binary, the PSK scheme is usually designed with the number of constellation points being a power of two.\nDefinitions.\nFor determining error-rates mathematically, some definitions will be needed:\nformula_8 will give the probability that a single sample taken from a random process with zero-mean and unit-variance Gaussian probability density function will be greater or equal to formula_9. It is a scaled form of the complementary Gaussian error function:\n formula_10.\nThe error rates quoted here are those in additive white Gaussian noise (AWGN). These error rates are lower than those computed in fading channels, hence, are a good theoretical benchmark to compare with.\nBinary phase-shift keying (BPSK).\nBPSK (also sometimes called PRK, phase reversal keying, or 2PSK) is the simplest form of phase shift keying (PSK). It uses two phases which are separated by 180\u00b0 and so can also be termed 2-PSK. It does not particularly matter exactly where the constellation points are positioned, and in this figure they are shown on the real axis, at 0\u00b0 and 180\u00b0. Therefore, it handles the highest noise level or distortion before the demodulator reaches an incorrect decision. That makes it the most robust of all the PSKs. It is, however, only able to modulate at 1bit/symbol (as seen in the figure) and so is unsuitable for high data-rate applications.\nIn the presence of an arbitrary phase-shift introduced by the communications channel, the demodulator (see, e.g. Costas loop) is unable to tell which constellation point is which. As a result, the data is often differentially encoded prior to modulation.\nBPSK is functionally equivalent to 2-QAM modulation.\nImplementation.\nThe general form for BPSK follows the equation:\nformula_11\nThis yields two phases, 0 and \u03c0.\nIn the specific form, binary data is often conveyed with the following signals:\nformula_12 for binary \"0\"\nformula_13 for binary \"1\"\nwhere \"f\" is the frequency of the base band.\nHence, the signal space can be represented by the single basis function\nformula_14\nwhere 1 is represented by formula_15 and 0 is represented by formula_16. This assignment is arbitrary.\nThis use of this basis function is shown at the end of the next section in a signal timing diagram. The topmost signal is a BPSK-modulated cosine wave that the BPSK modulator would produce. The bit-stream that causes this output is shown above the signal (the other parts of this figure are relevant only to QPSK). After modulation, the base band signal will be moved to the high frequency band by multiplying formula_17.\nBit error rate.\nThe bit error rate (BER) of BPSK under additive white Gaussian noise (AWGN) can be calculated as:\nformula_18 or formula_19\nSince there is only one bit per symbol, this is also the symbol error rate.\nQuadrature phase-shift keying (QPSK).\nSometimes this is known as \"quadriphase PSK\", 4-PSK, or 4-QAM. (Although the root concepts of QPSK and 4-QAM are different, the resulting modulated radio waves are exactly the same.) QPSK uses four points on the constellation diagram, equispaced around a circle. With four phases, QPSK can encode two bits per symbol, shown in the diagram with Gray coding to minimize the bit error rate (BER)\u00a0\u2013 sometimes misperceived as twice the BER of BPSK.\nThe mathematical analysis shows that QPSK can be used either to double the data rate compared with a BPSK system while maintaining the \"same\" bandwidth of the signal, or to \"maintain the data-rate of BPSK\" but halving the bandwidth needed. In this latter case, the BER of QPSK is \"exactly the same\" as the BER of BPSK\u00a0\u2013 and believing differently is a common confusion when considering or describing QPSK. The transmitted carrier can undergo numbers of phase changes.\nGiven that radio communication channels are allocated by agencies such as the Federal Communications Commission giving a prescribed (maximum) bandwidth, the advantage of QPSK over BPSK becomes evident: QPSK transmits twice the data rate in a given bandwidth compared to BPSK - at the same BER. The engineering penalty that is paid is that QPSK transmitters and receivers are more complicated than the ones for BPSK. However, with modern electronics technology, the penalty in cost is very moderate.\nAs with BPSK, there are phase ambiguity problems at the receiving end, and differentially encoded QPSK is often used in practice.\nImplementation.\nThe implementation of QPSK is more general than that of BPSK and also indicates the implementation of higher-order PSK. Writing the symbols in the constellation diagram in terms of the sine and cosine waves used to transmit them:\nformula_20\nThis yields the four phases \u03c0/4, 3\u03c0/4, 5\u03c0/4 and 7\u03c0/4 as needed.\nThis results in a two-dimensional signal space with unit basis functions\nformula_21\nThe first basis function is used as the in-phase component of the signal and the second as the quadrature component of the signal.\nHence, the signal constellation consists of the signal-space 4 points\nformula_22\nThe factors of 1/2 indicate that the total power is split equally between the two carriers.\nComparing these basis functions with that for BPSK shows clearly how QPSK can be viewed as two independent BPSK signals. Note that the signal-space points for BPSK do not need to split the symbol (bit) energy over the two carriers in the scheme shown in the BPSK constellation diagram.\nQPSK systems can be implemented in a number of ways. An illustration of the major components of the transmitter and receiver structure are shown below.\nProbability of error.\nAlthough QPSK can be viewed as a quaternary modulation, it is easier to see it as two independently modulated quadrature carriers. With this interpretation, the even (or odd) bits are used to modulate the in-phase component of the carrier, while the odd (or even) bits are used to modulate the quadrature-phase component of the carrier. BPSK is used on both carriers and they can be independently demodulated.\nAs a result, the probability of bit-error for QPSK is the same as for BPSK:\nformula_23\nHowever, in order to achieve the same bit-error probability as BPSK, QPSK uses twice the power (since two bits are transmitted simultaneously).\nThe symbol error rate is given by:\nformula_24\nIf the signal-to-noise ratio is high (as is necessary for practical QPSK systems) the probability of symbol error may be approximated:\nformula_25\nThe modulated signal is shown below for a short segment of a random binary data-stream. The two carrier waves are a cosine wave and a sine wave, as indicated by the signal-space analysis above. Here, the odd-numbered bits have been assigned to the in-phase component and the even-numbered bits to the quadrature component (taking the first bit as number 1). The total signal\u00a0\u2013 the sum of the two components\u00a0\u2013 is shown at the bottom. Jumps in phase can be seen as the PSK changes the phase on each component at the start of each bit-period. The topmost waveform alone matches the description given for BPSK above.\nThe binary data that is conveyed by this waveform is: 11000110.\nVariants.\nOffset QPSK (OQPSK).\n\"Offset quadrature phase-shift keying\" (\"OQPSK\") is a variant of phase-shift keying modulation using four different values of the phase to transmit. It is sometimes called \"staggered quadrature phase-shift keying\" (\"SQPSK\").\nTaking four values of the phase (two bits) at a time to construct a QPSK symbol can allow the phase of the signal to jump by as much as 180\u00b0 at a time. When the signal is low-pass filtered (as is typical in a transmitter), these phase-shifts result in large amplitude fluctuations, an undesirable quality in communication systems. By offsetting the timing of the odd and even bits by one bit-period, or half a symbol-period, the in-phase and quadrature components will never change at the same time. In the constellation diagram shown on the right, it can be seen that this will limit the phase-shift to no more than 90\u00b0 at a time. This yields much lower amplitude fluctuations than non-offset QPSK and is sometimes preferred in practice.\nThe picture on the right shows the difference in the behavior of the phase between ordinary QPSK and OQPSK. It can be seen that in the first plot the phase can change by 180\u00b0 at once, while in OQPSK the changes are never greater than 90\u00b0.\nThe modulated signal is shown below for a short segment of a random binary data-stream. Note the half symbol-period offset between the two component waves. The sudden phase-shifts occur about twice as often as for OQPSK (since the signals no longer change together), but they are less severe. In other words, the magnitude of jumps is smaller in OQPSK when compared to QPSK.\nSOQPSK.\nThe license-free shaped-offset QPSK (SOQPSK) is interoperable with Feher-patented QPSK (FQPSK), in the sense that an integrate-and-dump offset QPSK detector produces the same output no matter which kind of transmitter is used.\nThese modulations carefully shape the I and Q waveforms such that they change very smoothly, and the signal stays constant-amplitude even during signal transitions. (Rather than traveling instantly from one symbol to another, or even linearly, it travels smoothly around the constant-amplitude circle from one symbol to the next.) SOQPSK modulation can be represented as the hybrid of QPSK and MSK: SOQPSK has the same signal constellation as QPSK, however the phase of SOQPSK is always stationary.\nThe standard description of SOQPSK-TG involves ternary symbols. SOQPSK is one of the most spread modulation schemes in application to LEO satellite communications.\n\"\u03c0\"/4-QPSK.\nThis variant of QPSK uses two identical constellations which are rotated by 45\u00b0 (formula_26 radians, hence the name) with respect to one another. Usually, either the even or odd symbols are used to select points from one of the constellations and the other symbols select points from the other constellation. This also reduces the phase-shifts from a maximum of 180\u00b0, but only to a maximum of 135\u00b0 and so the amplitude fluctuations of formula_26-QPSK are between OQPSK and non-offset QPSK.\nOne property this modulation scheme possesses is that if the modulated signal is represented in the complex domain, transitions between symbols never pass through 0. In other words, the signal does not pass through the origin. This lowers the dynamical range of fluctuations in the signal which is desirable when engineering communications signals.\nOn the other hand, formula_26-QPSK lends itself to easy demodulation and has been adopted for use in, for example, TDMA cellular telephone systems.\nThe modulated signal is shown below for a short segment of a random binary data-stream. The construction is the same as above for ordinary QPSK. Successive symbols are taken from the two constellations shown in the diagram. Thus, the first symbol (1 1) is taken from the \"blue\" constellation and the second symbol (0 0) is taken from the \"green\" constellation. Note that magnitudes of the two component waves change as they switch between constellations, but the total signal's magnitude remains constant (constant envelope). The phase-shifts are between those of the two previous timing-diagrams.\nDPQPSK.\nDual-polarization quadrature phase shift keying (DPQPSK) or dual-polarization QPSK - involves the polarization multiplexing of two different QPSK signals, thus improving the spectral efficiency by a factor of 2. This is a cost-effective alternative to utilizing 16-PSK, instead of QPSK to double the spectral efficiency.\nHigher-order PSK.\nAny number of phases may be used to construct a PSK constellation but 8-PSK is usually the highest order PSK constellation deployed. With more than 8 phases, the error-rate becomes too high and there are better, though more complex, modulations available such as quadrature amplitude modulation (QAM). Although any number of phases may be used, the fact that the constellation must usually deal with binary data means that the number of symbols is usually a power of 2 to allow an integer number of bits per symbol.\nBit error rate.\nFor the general M-PSK there is no simple expression for the symbol-error probability if formula_29. Unfortunately, it can only be obtained from\nformula_30\nwhere\nformula_31\nand formula_32 and formula_33 are each Gaussian random variables.\nThis may be approximated for high formula_34 and high formula_35 by:\nformula_36\nThe bit-error probability for formula_34-PSK can only be determined exactly once the bit-mapping is known. However, when Gray coding is used, the most probable error from one symbol to the next produces only a single bit-error and\nformula_38\nThe graph on the right compares the bit-error rates of BPSK, QPSK (which are the same, as noted above), 8-PSK and 16-PSK. It is seen that higher-order modulations exhibit higher error-rates; in exchange however they deliver a higher raw data-rate.\nBounds on the error rates of various digital modulation schemes can be computed with application of the union bound to the signal constellation.\nSpectral efficiency.\nBandwidth (or spectral) efficiency of M-PSK modulation schemes increases with increasing of modulation order \"M\" (unlike, for example, M-FSK):\nformula_39\nThe same relationship holds true for M-QAM.\nDifferential phase-shift keying (DPSK).\nDifferential encoding.\nDifferential phase shift keying (DPSK) is a common form of phase modulation that conveys data by changing the phase of the carrier wave. by some effect in the communications channel through which the signal passes. This problem can be overcome by using the data to \"change\" rather than \"set\" the phase.\nFor example, in differentially-encoded BPSK a binary \"1\" may be transmitted by adding 180\u00b0 to the current phase and a binary \"0\" by adding 0\u00b0 to the current phase. \nAnother variant of DPSK is symmetric differential phase shift keying, SDPSK, where encoding would be +90\u00b0 for a \"1\" and \u221290\u00b0 for a \"0\".\nIn differentially-encoded QPSK (DQPSK), the phase-shifts are 0\u00b0, 90\u00b0, 180\u00b0, \u221290\u00b0 corresponding to data \"00\", \"01\", \"11\", \"10\". This kind of encoding may be demodulated in the same way as for non-differential PSK but the phase ambiguities can be ignored. Thus, each received symbol is demodulated to one of the formula_34 points in the constellation and a comparator then computes the difference in phase between this received signal and the preceding one. The difference encodes the data as described above. \"Symmetric differential quadrature phase shift keying\" (SDQPSK) is like DQPSK, but encoding is symmetric, using phase shift values of \u2212135\u00b0, \u221245\u00b0, +45\u00b0 and +135\u00b0.\nThe modulated signal is shown below for both DBPSK and DQPSK as described above. In the figure, it is assumed that the \"signal starts with zero phase\", and so there is a phase shift in both signals at formula_41.\nAnalysis shows that differential encoding approximately doubles the error rate compared to ordinary formula_34-PSK but this may be overcome by only a small increase in formula_35. Furthermore, this analysis (and the graphical results below) are based on a system in which the only corruption is additive white Gaussian noise (AWGN). However, there will also be a physical channel between the transmitter and receiver in the communication system. This channel will, in general, introduce an unknown phase-shift to the PSK signal; in these cases the differential schemes can yield a \"better\" error-rate than the ordinary schemes which rely on precise phase information.\nOne of the most popular applications of DPSK is the Bluetooth standard where formula_26-DQPSK and 8-DPSK were implemented.\nDemodulation.\nFor a signal that has been differentially encoded, there is an obvious alternative method of demodulation. Instead of demodulating as usual and ignoring carrier-phase ambiguity, the phase between two successive received symbols is compared and used to determine what the data must have been. When differential encoding is used in this manner, the scheme is known as differential phase-shift keying (DPSK). Note that this is subtly different from just differentially encoded PSK since, upon reception, the received symbols are \"not\" decoded one-by-one to constellation points but are instead compared directly to one another.\nCall the received symbol in the formula_45th timeslot formula_46 and let it have phase formula_47. Assume without loss of generality that the phase of the carrier wave is zero. Denote the additive white Gaussian noise (AWGN) term as formula_48. Then\nformula_49\nThe decision variable for the formula_50th symbol and the formula_45th symbol is the phase difference between formula_46 and formula_53. That is, if formula_46 is projected onto formula_53, the decision is taken on the phase of the resultant complex number:\nformula_56\nwhere superscript * denotes complex conjugation. In the absence of noise, the phase of this is formula_57, the phase-shift between the two received signals which can be used to determine the data transmitted.\nThe probability of error for DPSK is difficult to calculate in general, but, in the case of DBPSK it is:\nformula_58\nwhich, when numerically evaluated, is only slightly worse than ordinary BPSK, particularly at higher formula_35 values.\nUsing DPSK avoids the need for possibly complex carrier-recovery schemes to provide an accurate phase estimate and can be an attractive alternative to ordinary PSK.\nIn optical communications, the data can be modulated onto the phase of a laser in a differential way. The modulation is a laser which emits a continuous wave, and a Mach\u2013Zehnder modulator which receives electrical binary data. For the case of BPSK, the laser transmits the field unchanged for binary '1', and with reverse polarity for '0'. The demodulator consists of a delay line interferometer which delays one bit, so two bits can be compared at one time. In further processing, a photodiode is used to transform the optical field into an electric current, so the information is changed back into its original state.\nThe bit-error rates of DBPSK and DQPSK are compared to their non-differential counterparts in the graph to the right. The loss for using DBPSK is small enough compared to the complexity reduction that it is often used in communications systems that would otherwise use BPSK. For DQPSK though, the loss in performance compared to ordinary QPSK is larger and the system designer must balance this against the reduction in complexity.\nExample: Differentially-encoded BPSK.\nAt the formula_60 time-slot call the bit to be modulated formula_61, the differentially encoded bit formula_62 and the resulting modulated signal formula_63. Assume that the constellation diagram positions the symbols at \u00b11 (which is BPSK). The differential encoder produces:\nformula_64\nwhere formula_65 indicates binary or modulo-2 addition.\nSo formula_62 only changes state (from binary \"0\" to binary \"1\" or from binary \"1\" to binary \"0\") if formula_61 is a binary \"1\". Otherwise it remains in its previous state. This is the description of differentially encoded BPSK given above.\nThe received signal is demodulated to yield formula_68 and then the differential decoder reverses the encoding procedure and produces\nformula_69\nsince binary subtraction is the same as binary addition.\nTherefore, formula_70 if formula_62 and formula_72 differ and formula_73 if they are the same. Hence, if both formula_62 and formula_72 are \"inverted\", formula_61 will still be decoded correctly. Thus, the 180\u00b0 phase ambiguity does not matter.\nDifferential schemes for other PSK modulations may be devised along similar lines. The waveforms for DPSK are the same as for differentially encoded PSK given above since the only change between the two schemes is at the receiver.\nThe BER curve for this example is compared to ordinary BPSK on the right. As mentioned above, whilst the error rate is approximately doubled, the increase needed in formula_35 to overcome this is small. The increase in formula_35 required to overcome differential modulation in coded systems, however, is larger\u00a0\u2013 typically about 3\u00a0dB. The performance degradation is a result of noncoherent transmission\u00a0\u2013 in this case it refers to the fact that tracking of the phase is completely ignored.\nApplications.\nOwing to PSK's simplicity, particularly when compared with its competitor quadrature amplitude modulation, it is widely used in existing technologies.\nThe wireless LAN standard, IEEE 802.11b-1999, uses a variety of different PSKs depending on the data rate required. At the basic rate of 1Mbit/s, it uses DBPSK (differential BPSK). To provide the extended rate of 2Mbit/s, DQPSK is used. In reaching 5.5Mbit/s and the full rate of 11Mbit/s, QPSK is employed, but has to be coupled with complementary code keying. The higher-speed wireless LAN standard, IEEE 802.11g-2003, has eight data rates: 6, 9, 12, 18, 24, 36, 48 and 54Mbit/s. The 6 and 9Mbit/s modes use OFDM modulation where each sub-carrier is BPSK modulated. The 12 and 18Mbit/s modes use OFDM with QPSK. The fastest four modes use OFDM with forms of quadrature amplitude modulation.\nBecause of its simplicity, BPSK is appropriate for low-cost passive transmitters, and is used in RFID standards such as ISO/IEC 14443 which has been adopted for biometric passports, credit cards such as American Express's ExpressPay, and many other applications.\nBluetooth 2 uses formula_26-DQPSK at its lower rate (2Mbit/s) and 8-DPSK at its higher rate (3Mbit/s) when the link between the two devices is sufficiently robust. Bluetooth 1 modulates with Gaussian minimum-shift keying, a binary scheme, so either modulation choice in version 2 will yield a higher data rate. A similar technology, IEEE 802.15.4 (the wireless standard used by Zigbee) also relies on PSK using two frequency bands: 868\u00a0MHz and 915MHz with BPSK and at 2.4GHz with OQPSK.\nBoth QPSK and 8PSK are widely used in satellite broadcasting. QPSK is still widely used in the streaming of SD satellite channels and some HD channels. High definition programming is delivered almost exclusively in 8PSK due to the higher bitrates of HD video and the high cost of satellite bandwidth. The DVB-S2 standard requires support for both QPSK and 8PSK. The chipsets used in new satellite set top boxes, such as Broadcom's 7000 series support 8PSK and are backward compatible with the older standard.\nHistorically, voice-band synchronous modems such as the Bell 201, 208, and 209 and the CCITT V.26, V.27, V.29, V.32, and V.34 used PSK.\nMutual information with additive white Gaussian noise.\nThe mutual information of PSK can be evaluated in additive Gaussian noise by numerical integration of its definition. The curves of mutual information saturate to the number of bits carried by each symbol in the limit of infinite signal to noise ratio formula_80. On the contrary, in the limit of small signal to noise ratios the mutual information approaches the AWGN channel capacity, which is the supremum among all possible choices of symbol statistical distributions.\nAt intermediate values of signal to noise ratios the mutual information (MI) is well approximated by:\nformula_81\nThe mutual information of PSK over the AWGN channel is generally lower than that of QAM modulation formats.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\nThe notation and theoretical results in this article are based on material presented in the following sources:"}
{"id": "41552", "revid": "7852030", "url": "https://en.wikipedia.org/wiki?curid=41552", "title": "Phonetic alphabet", "text": "Phonetic alphabet can mean:\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "41553", "revid": "20483999", "url": "https://en.wikipedia.org/wiki?curid=41553", "title": "Photocurrent", "text": "Electric current through a photosensitive device\nPhotocurrent is the electric current through a photosensitive device, such as a photodiode, as the result of exposure to radiant power. The photocurrent may occur as a result of the photoelectric, photoemissive, or photovoltaic effect. The photocurrent may be enhanced by internal gain caused by interaction among ions and photons under the influence of applied fields, such as occurs in an avalanche photodiode (APD).\nWhen a suitable radiation is used, the photoelectric current is directly proportional to intensity of radiation and increases with the increase in accelerating potential till the stage is reached when photo-current becomes maximum and does not increase with further increase in accelerating potential. The highest (maximum) value of the photo-current is called saturation current. The value of retarding potential at which photo-current becomes zero is called cut-off voltage or stopping potential for the given frequency of the incident ray.\nPhotovoltaics.\nThe generation of a photocurrent forms the basis of the photovoltaic cell.\nPhotocurrent spectroscopy.\nA characterization technique called photocurrent spectroscopy (PCS), also known as photoconductivity spectroscopy, is widely used for studying optoelectronic properties of semiconductors and other light absorbing materials. The setup of the technique involves having a semiconductor contacted with electrodes allowing for application of an electric bias, while at the same time a tunable light source incident with a given specific wavelength (energy) and power, usually pulsed by a mechanical chopper. \nThe quantity measured is the electrical response of the circuit, coupled with the spectrograph obtained by varying the incident light energy by a monochromator. The circuit and optics are coupled by use of a lock-in amplifier. The measurements give information related to the band gap of the semiconductor, allowing for identification of various charge transitions like exciton and trion energies. This is highly relevant for studying semiconductor nanostructures like quantum wells, and other nanomaterials like transition metal dichalcogenide monolayers.\nFurthermore, by using a piezo stage to vary the lateral position of the semiconductor with micron precision, one can generate a micrograph false color image of the spectra for different positions. This is called scanning photocurrent microscopy (SPCM).\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41554", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41554", "title": "Physical signaling sublayer", "text": ""}
{"id": "41555", "revid": "44062", "url": "https://en.wikipedia.org/wiki?curid=41555", "title": "Pilot (disambiguation)", "text": "A pilot is a person who flies or navigates an aircraft.\nPilot or The Pilot may also refer to:\n&lt;templatestyles src=\"Template:TOC_right/styles.css\" /&gt;\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "41556", "revid": "50968320", "url": "https://en.wikipedia.org/wiki?curid=41556", "title": "PIN diode", "text": "Optical diode invented by Jun-Ichi Nishizawa\nA PIN diode is a diode with a wide, undoped intrinsic semiconductor region between a p-type semiconductor and an n-type semiconductor region. The p-type and n-type regions are typically heavily doped because they are used for ohmic contacts.\nThe wide intrinsic region is in contrast to an ordinary p\u2013n diode. The wide intrinsic region makes the PIN diode an inferior rectifier (one typical function of a diode), but it makes it suitable for attenuators, fast switches, photodetectors, and high-voltage power electronics applications.\nThe PIN photodiode was invented by Jun-Ichi Nishizawa and his colleagues in 1950. It is a semiconductor device.\nOperation.\nA PIN diode operates under what is known as high-level injection. In other words, the intrinsic \"i\" region is flooded with charge carriers from the \"p\" and \"n\" regions. Its function can be likened to filling up a water bucket with a hole on the side. Once the water reaches the hole's level it will begin to pour out. Similarly, the diode will conduct current once the flooded electrons and holes reach an equilibrium point, where the number of electrons is equal to the number of holes in the intrinsic region.\nWhen the diode is forward biased, the injected carrier concentration is typically several orders of magnitude higher than the intrinsic carrier concentration. Due to this high level injection, which in turn is due to the depletion process, the electric field extends deeply (almost the entire length) into the region. This electric field helps in speeding up of the transport of charge carriers from the P to the N region, which results in faster operation of the diode, making it a suitable device for high-frequency operation.\nCharacteristics.\nThe PIN diode obeys the standard diode equation for low-frequency signals. At higher frequencies, the diode looks like an almost perfect (very linear, even for large signals) resistor. The P-I-N diode has a relatively large stored charge adrift in a thick intrinsic region. At a low-enough frequency, the stored charge can be fully swept and the diode turns off. At higher frequencies, there is not enough time to sweep the charge from the drift region, so the diode never turns off. The time required to sweep the stored charge from a diode junction is its reverse recovery time, and it is relatively long in a PIN diode. For a given semiconductor material, on-state impedance, and minimum usable RF frequency, the reverse recovery time is fixed. This property can be exploited; one variety of P-I-N diode, the step recovery diode, exploits the abrupt impedance change at the end of the reverse recovery to create a narrow impulse waveform useful for frequency multiplication with high multiples.\nThe high-frequency resistance is inversely proportional to the DC bias current through the diode. A PIN diode, suitably biased, therefore acts as a variable resistor. This high-frequency resistance may vary over a wide range (from 0.1 \u03a9 to 10 k\u03a9 in some cases; the useful range is smaller, though).\nThe wide intrinsic region also means the diode will have a low capacitance when reverse-biased.\nIn a PIN diode the depletion region exists almost completely within the intrinsic region. This depletion region is much larger than in a PN diode and almost constant-size, independent of the reverse bias applied to the diode. This increases the volume where electron-hole pairs can be generated by an incident photon. Some photodetector devices, such as PIN photodiodes and phototransistors (in which the base-collector junction is a PIN diode), use a PIN junction in their construction.\nThe diode design has some design trade-offs. Increasing the cross-section area of the intrinsic region increases its stored charge reducing its RF on-state resistance while also increasing reverse bias capacitance and increasing the drive current required to remove the charge during a fixed switching time, with no effect on the minimum time required to sweep the charge from the I region. Increasing the thickness of the intrinsic region increases the total stored charge, decreases the minimum RF frequency, and decreases the reverse-bias capacitance, but doesn't decrease the forward-bias RF resistance and increases the minimum time required to sweep the drift charge and transition from low to high RF resistance. Diodes are sold commercially in a variety of geometries for specific RF bands and uses.\nApplications.\nPIN diodes are useful as RF switches, attenuators, photodetectors, and phase shifters.\nRF and microwave switches.\nUnder zero- or reverse-bias (the \"off\" state), a PIN diode has a low capacitance. The low capacitance will not pass much of an RF signal. Under a forward bias of 1 mA (the \"on\" state), a typical PIN diode will have an RF resistance of about 1 ohm, making it a good conductor of RF. Consequently, the PIN diode makes a good RF switch.\nAlthough RF relays can be used as switches, they switch relatively slowly (on the order of tens of milliseconds). A PIN diode switch can switch much more quickly (e.g., 1 microsecond), although at lower RF frequencies it isn't reasonable to expect switching times in the same order of magnitude as the RF period.\nFor example, the capacitance of an \"off\"-state discrete PIN diode might be 1 pF. At 320 MHz, the capacitive reactance of 1 pF is 497 ohms:\nformula_1\nAs a series element in a 50 ohm system, the off-state attenuation is:\nformula_2\nThis attenuation may not be adequate. In applications where higher isolation is needed, both shunt and series elements may be used, with the shunt diodes biased in complementary fashion to the series elements. Adding shunt elements effectively reduces the source and load impedances, reducing the impedance ratio and increasing the off-state attenuation. However, in addition to the added complexity, the on-state attenuation is increased due to the series resistance of the on-state blocking element and the capacitance of the off-state shunt elements.\nPIN diode switches are used not only for signal selection, but also component selection. For example, some low-phase-noise oscillators use them to range-switch inductors.\nRF and microwave variable attenuators.\nBy changing the bias current through a PIN diode, it is possible to quickly change its RF resistance.\nAt high frequencies, the PIN diode appears as a resistor whose resistance is an inverse function of its forward current. Consequently, PIN diode can be used in some variable attenuator designs as amplitude modulators or output leveling circuits.\nPIN diodes might be used, for example, as the bridge and shunt resistors in a bridged-T attenuator. Another common approach is to use PIN diodes as terminations connected to the 0 degree and -90 degree ports of a quadrature hybrid. The signal to be attenuated is applied to the input port, and the attenuated result is taken from the isolation port. The advantages of this approach over the bridged-T and pi approaches are (1) complementary PIN diode bias drives are not needed\u2014the same bias is applied to both diodes\u2014and (2) the loss in the attenuator equals the return loss of the terminations, which can be varied over a very wide range.\nLimiters.\nPIN diodes are sometimes designed for use as input protection devices for high-frequency test probes and other circuits. If the input signal is small, the PIN diode has negligible impact, presenting only a small parasitic capacitance. Unlike a rectifier diode, it does not present a nonlinear resistance at RF frequencies, which would give rise to harmonics and intermodulation products. If the signal is large, then when the PIN diode starts to rectify the signal, the forward current charges the drift region and the device RF impedance is inversely proportional to the signal amplitude. That signal amplitude varying resistance can be used to terminate some predetermined portion of the signal in a resistive network dissipating the energy or to create an impedance mismatch that reflects the incident signal back toward the source. The latter may be combined with an isolator, a device containing a circulator which uses a permanent magnetic field to break reciprocity and a resistive load to separate and terminate the backward traveling wave. When used as a shunt limiter the PIN diode impedance is low over the entire RF cycle, unlike paired rectifier diodes that would swing from a high resistance to a low resistance during each RF cycle clamping the waveform and not reflecting it as completely. The ionization recovery time of gas molecules that permits the creation of the higher power spark gap input protection device ultimately relies on similar physics in a gas.\nPhotodetector and photovoltaic cell.\nThe PIN photodiode was invented by Jun-ichi Nishizawa and his colleagues in 1950.\nPIN photodiodes are used in fibre optic network cards and switches. As a photodetector, the PIN diode is reverse-biased. Under reverse bias, the diode ordinarily does not conduct (save a small dark current or Is leakage). When a photon of sufficient energy enters the depletion region of the diode, it creates an electron-hole pair. The reverse-bias field sweeps the carriers out of the region, creating current. Some detectors can use avalanche multiplication.\nThe same mechanism applies to the PIN structure, or p-i-n junction, of a solar cell. In this case, the advantage of using a PIN structure over conventional semiconductor p\u2013n junction is better long-wavelength response of the former. In case of long wavelength irradiation, photons penetrate deep into the cell. But only those electron-hole pairs generated in and near the depletion region contribute to current generation. The depletion region of a PIN structure extends across the intrinsic region, deep into the device. This wider depletion width enables electron-hole pair generation deep within the device, which increases the quantum efficiency of the cell.\nCommercially available PIN photodiodes have quantum efficiencies above 80-90% in the telecom wavelength range (~1500\u00a0nm), and are typically made of germanium or InGaAs. They feature fast response times (higher than their p-n counterparts), running into several tens of gigahertz, making them ideal for high speed optical telecommunication applications. Similarly, silicon p-i-n photodiodes have even higher quantum efficiencies, but can only detect wavelengths above the bandgap of silicon, i.e. ~1100\u00a0nm.\nTypically, amorphous silicon thin-film cells use PIN structures. On the other hand, CdTe cells use NIP structure, a variation of the PIN structure. In a NIP structure, an intrinsic CdTe layer is sandwiched by n-doped CdS and p-doped ZnTe; the photons are incident on the n-doped layer, unlike in a PIN diode.\nA PIN photodiode can also be used as a semiconductor detector to sense ionizing radiation.\nIn modern fiber-optical communications, the speed of optical transmitters and receivers is one of the most important parameters. Due to the small surface of the photodiode, its parasitic (unwanted) capacitance is reduced. The bandwidth of modern pin photodiodes is reaching the microwave and millimeter waves range.\nExample PIN photodiodes.\nSFH203 and BPW34 are cheap general purpose PIN diodes in 5\u00a0mm clear plastic cases with bandwidths over 100\u00a0MHz.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41557", "revid": "1030826", "url": "https://en.wikipedia.org/wiki?curid=41557", "title": "Planar array", "text": ""}
{"id": "41559", "revid": "37106542", "url": "https://en.wikipedia.org/wiki?curid=41559", "title": "Plane wave", "text": "Type of wave propagating in 3 dimensions\nIn physics, a plane wave is a special case of a wave or field: a physical quantity whose value, at any given moment, is constant through any plane that is perpendicular to a fixed direction in space.\nFor any position formula_1 in space and any time formula_2, the value of such a field can be written as\nformula_3\nwhere formula_4 is a unit-length vector, and formula_5 is a function that gives the field's value as dependent on only two real parameters: the time formula_2, and the scalar-valued displacement formula_7 of the point formula_1 along the direction formula_4. The displacement is constant over each plane perpendicular to formula_4.\nThe values of the field formula_11 may be scalars, vectors, or any other physical or mathematical quantity. They can be complex numbers, as in a complex exponential plane wave. \nWhen the values of formula_11 are vectors, the wave is said to be a longitudinal wave if the vectors are always collinear with the vector formula_4, and a transverse wave if they are always orthogonal (perpendicular) to it.\nSpecial types.\nTraveling plane wave.\nOften the term \"plane wave\" refers specifically to a \"traveling plane wave\", whose evolution in time can be described as simple translation of the field at a constant \"wave speed\" formula_14 along the direction perpendicular to the wavefronts. Such a field can be written as \nformula_15\nwhere formula_16 is now a function of a single real parameter formula_17, that describes the \"profile\" of the wave, namely the value of the field at time formula_18, for each displacement formula_7. In that case, formula_4 is called the \"direction of propagation\". For each displacement formula_21, the moving plane perpendicular to formula_4 at distance formula_23 from the origin is called a \"wavefront\". These planes travel along the direction of propagation formula_4 with velocity formula_14; and the value of the field is then the same, and constant in time, at every one of their points.\nSinusoidal plane wave.\nThe term is also used, even more specifically, to mean a \"monochromatic\" or sinusoidal plane wave: a travelling plane wave whose profile formula_16 is a sinusoidal function. That is,\nformula_27\nThe parameter formula_28, which may be a scalar or a vector, is called the amplitude of the wave; the scalar coefficient formula_29 is its \"spatial frequency\"; and the scalar formula_30 is its \"phase shift\".\nA true plane wave cannot physically exist, because it would have to fill all space. Nevertheless, the plane wave model is important and widely used in physics. The waves emitted by any source with finite extent into a large homogeneous region of space can be well approximated by plane waves when viewed over any part of that region that is sufficiently small compared to its distance from the source. That is the case, for example, of the light waves from a distant star that arrive at a telescope.\nPlane standing wave.\nA standing wave is a field whose value can be expressed as the product of two functions, one depending only on position, the other only on time. A plane standing wave, in particular, can be expressed as\nformula_31\nwhere formula_32 is a function of one scalar parameter (the displacement formula_7) with scalar or vector values, and formula_34 is a scalar function of time. \nThis representation is not unique, since the same field values are obtained if formula_34 and formula_32 are scaled by reciprocal factors. If formula_37 is bounded in the time interval of interest (which is usually the case in physical contexts), formula_34 and formula_32 can be scaled so that the maximum value of formula_37 is 1. Then formula_41 will be the maximum field magnitude seen at the point formula_1.\nProperties.\nA plane wave can be studied by ignoring the directions perpendicular to the direction vector formula_4; that is, by considering the function formula_44 as a wave in a one-dimensional medium. \nAny local operator, linear or not, applied to a plane wave yields a plane wave. Any linear combination of plane waves with the same normal vector formula_4 is also a plane wave.\nFor a scalar plane wave in two or three dimensions, the gradient of the field is always collinear with the direction formula_4; specifically, formula_47, where formula_48 is the partial derivative of formula_32 with respect to the first argument. \nThe divergence of a vector-valued plane wave depends only on the projection of the vector formula_5 in the direction formula_4. Specifically,\nformula_52\nIn particular, a transverse planar wave satisfies formula_53 for all formula_1 and formula_2.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "41560", "revid": "49339423", "url": "https://en.wikipedia.org/wiki?curid=41560", "title": "Plastic-clad silica fiber", "text": "In telecommunications and fiber optics, a plastic-clad silica fiber or polymer-clad silica fiber (PCS) is an optical fiber that has a silica-based core and a plastic cladding. The cladding of a PCS fiber should not be confused with the polymer overcoat of a conventional all-silica fiber. \nBenefits.\nThe main applications of plastic-clad silica fiber are industrial, medical or sensing applications where cores that are larger than those used in standard data communications fibers are advantageous.\nLimitation.\nPCS fibers in general have significantly lower performance characteristics, particularly higher transmission losses and lower bandwidths, than all-glass fibers.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41562", "revid": "35939985", "url": "https://en.wikipedia.org/wiki?curid=41562", "title": "Ingrid Bergman", "text": "Swedish actress (1915\u20131982)\nIngrid Bergman (29 August 1915\u00a0\u2013 29 August 1982) was a Swedish actress. With a career spanning five decades, Bergman is often regarded as one of the most influential actresses in the history of cinema. She won numerous accolades, including three Academy Awards, two Primetime Emmy Awards, a Tony Award; these accolades makes her the youngest performer to achieve Triple Crown of Acting and one of only four actresses to have received at least three acting Academy Awards (only Katharine Hepburn has four).\nBorn in Stockholm to a Swedish father and German mother, Bergman began her acting career in Swedish and German films. Her introduction to the U.S. audience came in the English-language remake of \"Intermezzo\" (1939). Known for her naturally luminous beauty, she starred in \"Casablanca\" (1942) as Ilsa Lund. Bergman's notable performances in the 1940s include the dramas \"For Whom the Bell Tolls\" (1943), \"Gaslight\" (1944), \"The Bells of St. Mary's\" (1945), and \"Joan of Arc\" (1948), all of which earned her nominations for the Academy Award for Best Actress; she won for \"Gaslight\". She made three films with Alfred Hitchcock: \"Spellbound\" (1945), \"Notorious\" (1946), and \"Under Capricorn\" (1949).\nIn 1950, she starred in Roberto Rossellini's \"Stromboli\", released after the revelation that she was having an affair with Rossellini; that and her pregnancy before their marriage created a scandal in the U.S. that prompted her to remain in Europe for several years. During this time, she starred in Rossellini's \"Europa '51\" and \"Journey to Italy\" (1954), the former of which won her the Volpi Cup for Best Actress. The Volpi Cup was not awarded to her in 1952 because she was dubbed (by Lydia Simoneschi) in the version presented at the Festival; she was awarded posthumously in 1992, and the prize was accepted by her son Roberto Rossellini. She returned to Hollywood, earning two more Academy Awards for her roles in \"Anastasia\" (1956) and \"Murder on the Orient Express\" (1974). During this period she also starred in \"Indiscreet\" (1958), \"Cactus Flower\" (1969), and \"Autumn Sonata\" (1978) receiving her sixth Best Actress nomination.\nBergman won the Tony Award for Best Actress in a Play for the Maxwell Anderson play \"Joan of Lorraine\" (1947). She also won two Primetime Emmy Awards for Outstanding Lead Actress in a Limited Series or Movie for \"The Turn of the Screw\" (1960), and \"A Woman Called Golda\" (1982). In 1974, Bergman discovered she was suffering from breast cancer but continued to work until shortly before her death on her sixty-seventh birthday in 1982. Bergman spoke five languages\u2014Swedish, English, German, Italian, and French\u2014and acted in each. In 1999, the American Film Institute recognized her as the fourth-greatest female screen legend of Classic Hollywood Cinema.\nEarly life.\nIngrid Bergman was born on 29 August 1915 in Stockholm, to a Swedish father, Justus Samuel Bergman, and a German mother, Frieda \"Friedel\" Henriette Auguste Louise Bergman (n\u00e9e Adler), who was born in Kiel. Her parents married in Hamburg on 13 June 1907. She was named after Princess Ingrid of Sweden. Although she was raised in Sweden, she spent her summers in Germany and spoke fluent German.\nBergman was raised as an only child, as two older siblings had died in infancy before she was born. When she was two and a half years old, her mother died. She learned to create imaginary friends as a child. Justus Bergman had wanted his daughter to become an opera star and had her take voice lessons for three years. He sent her to the , a prestigious girls' school in Stockholm where Bergman was reportedly neither a good student nor popular.\nJustus was a photographer and loved documenting his daughter's birthdays with his camera. He made his daughter one of his favorite photographic subjects. She enjoyed dancing, dressing up, and acting in front of her father's lenses. \"I was perhaps the most photographed child in Scandinavia,\" quipped Bergman in her later years. In 1929, when Bergman was around 14, her father died of stomach cancer. Losing her parents at such a young age was a trauma that Bergman later described as \"living with an ache\", an experience of which she was not even aware.\nAfter her father's death, Bergman was sent to live with her paternal aunt, Ellen, who died of heart disease six months later. Bergman then lived with her paternal uncle Otto and his wife Hulda, who had five children of their own. She also visited her maternal aunt, Elsa Adler, whom the young girl called (Mom) according to family lore. She later said, \"I have wanted to be an actress almost as long as I can remember\", sometimes wearing her deceased mother's clothing, and staging plays in her father's empty studio.\nBergman spoke Swedish and German as first languages, English and Italian (acquired later, while living in the U.S. and Italy), and French (learned in school). She acted in each of these languages at various times.\nBergman received a scholarship to the state-sponsored Royal Dramatic Training Academy, where Greta Garbo had some years earlier earned a similar scholarship. After several months, she was given a part in a new play, (\"A Crime\"), written by Sigfrid Siwertz. This was \"totally against procedure\" at the school, where girls were expected to complete three years of study before getting such acting roles. During her first summer break, Bergman was hired by Swedish film studio Svensk Filmindustri, which led her to leave the Royal Dramatic Theatre after just one year to work in films full-time.\nCareer.\n1935\u20131938: Swedish years.\nBergman's first film experience was as an extra in the 1932 film , an experience she described as \"walking on holy ground\". Her first speaking role was a small part in (1935). Bergman played Elsa, a maid in a seedy hotel, being pursued by the leading man, Edvin Adolphson. Critics called her \"hefty and sure of herself\", and \"somewhat overweight ... with an unusual way of speaking her lines\". The unflatteringly striped costume that she wore may have contributed to the unfavorable comments regarding her appearance. Soon after , Bergman was offered a studio contract and placed under director Gustaf Molander.\nBergman starred in \"Ocean Breakers\", in which she played a fisherman's daughter, and then in , where she had the opportunity to work alongside her idol G\u00f6sta Ekman. Next, she starred in \"Walpurgis Night\" (1935). She played Lena, a secretary in love with her boss, Johan, who is unhappily married. Throughout, Lena and the wife vie for Johan's affection, with the wife losing her husband to Lena at the end.\nIn 1936, in \"On the Sunny Side\" (P\u00e5 Solsidan), Bergman was cast as an orphan from a good family who marries a rich older gentleman. Also in 1936, she appeared in \"Intermezzo\", her first lead performance, where she was reunited with G\u00f6sta Ekman. This was a pivotal film for the young actress and allowed her to demonstrate her talent. Director Molander later said: \"I created \"Intermezzo\" for her, but I was not responsible for its success. Ingrid herself made it successful.\" In 1938, she starred in \"Only One Night\", playing an upper-class woman living on a country estate. She didn't like the part, calling it \"a piece of rubbish\". She only agreed to appear if only she could star in the studio's next film project, . She later acted in \"Dollar\" (1938), a Scandinavian screwball comedy. Bergman had just been voted Sweden's most admired movie star in the previous year and received top billing. wrote in its review: \"Ingrid Bergman's feline appearance as an industrial tycoon's wife overshadows them all.\"\nIn her next film, a role created especially for her,\u00a0 (\"A Woman's Face\"), she played against her usual casting, as a bitter, unsympathetic character, whose face had been hideously burned. Anna Holm is the leader of a blackmail gang that targets the wealthy folk of Stockholm for their money and jewellery. The film required Bergman to wear heavy make-up, as well as glue, to simulate a burned face. A brace was put in place to distort the shape of one cheek. In her diary, she called the film \"my own picture, my very own. I have fought for it.\". The critics loved her performance, citing her as an actor of great talent and confidence. The film was awarded a Special Recommendation at the 1938 Venice Film Festival, for its \"overall artistic contribution\". It was remade in 1941 by Metro-Goldwyn-Mayer with the same title, starring Joan Crawford.\nBergman signed a three-picture contract with UFA, the German major film company, although she only made one picture. At the time, she was pregnant, but, nonetheless, she arrived in Berlin to begin filming \"The Four Companions\" () (1938), directed by Carl Froelich. The film was intended as a star vehicle to launch Bergman's career in Germany. In the film, she played one of four ambitious young women, attempting to set up a graphic design agency. The film was a light-hearted combination of comedy and romance. At first, she did not comprehend the political and social situation in Germany. Later, she said: \"I saw very quickly that if you were anybody at all in films, you had to be a member of the Nazi party.\" By September, she was back in Sweden, and gave birth to her daughter, Pia. She was never to work in Germany again.\nBergman appeared in eleven films in her native Sweden before the age of twenty-five. Her characters were always plagued with uncertainty, fear, and anxiety. The early Swedish films were not masterpieces, but she worked with some of the biggest talents in the Swedish film industry, such as G\u00f6sta Ekman, Karin Swanstr\u00f6m, Victor Sj\u00f6str\u00f6m, and Lars Hanson. It showcased her immense acting talent, as a young woman with a bright future ahead of her.\n1939\u20131949: Hollywood and stage work breakthrough.\nBergman's first acting role in the United States was in \"\" by Gregory Ratoff which premiered on 22 September 1939. She accepted the invitation of Hollywood producer David O. Selznick, who wished her to star in the English-language remake of her earlier Swedish film \"Intermezzo\" (1936). Unable to speak English, and uncertain about her acceptance by the American audience, she expected to complete this one film and return home to Sweden. Her husband, Petter Aron Lindstr\u00f6m, remained in Sweden with their daughter Pia (born 1938). In \"Intermezzo\", she played the role of a young piano accompanist, opposite Leslie Howard, who played a famous violin virtuoso. Bergman arrived in Los Angeles on 6 May 1939 and stayed at the Selznick home until she could find another residence.\nAccording to Selznick's son Danny, who was a child at the time, his father had concerns about Bergman: \"She didn't speak English, she was too tall, her name sounded too German, and her eyebrows were too thick\". Bergman was soon accepted without having to modify her looks or name, despite some early suggestions by Selznick. \"He let her have her way\", notes a story in \"Life\" magazine. Selznick understood her fear of Hollywood make-up artists, who might turn her into someone she wouldn't recognize, and \"instructed them to lay off\". He was also aware that her natural good looks would compete successfully with Hollywood's \"synthetic razzle-dazzle\".\nDuring the following weeks, while \"Intermezzo\" was being filmed, Selznick was also filming \"Gone with the Wind\". In a letter to William Hebert, his publicity director, Selznick described a few of his early impressions of Bergman: &lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;Miss Bergman is the most completely conscientious actress with whom I have ever worked, in that she thinks of absolutely nothing but her work before and during the time she is doing a picture\u00a0... She practically never leaves the studio, and even suggested that her dressing room be equipped so that she could live here during the picture. She never for a minute suggests quitting at six o'clock or anything of the kind\u00a0... Because of having four stars acting in \"Gone with the Wind\", our star dressing-room suites were all occupied and we had to assign her a smaller suite. She went into ecstasies over it and said she had never had such a suite in her life\u00a0... All of this is completely unaffected and completely unique and I should think would make a grand angle of approach to her publicity\u00a0... so that her natural sweetness and consideration and conscientiousness become something of a legend\u00a0... and is completely in keeping with the fresh and pure personality and appearance which caused me to sign her.\n\"Intermezzo\" became an enormous success and as a result, Bergman became a star. Ratoff, said, \"She is sensational.\" This was the \"sentiment of the entire set\", wrote a retrospective, adding that workmen went out of their way to do things for her and that the cast and crew \"admired the quick, alert concentration she gave to direction and to her lines\". Film historian David Thomson notes that this became \"the start of an astonishing impact on Hollywood and America\", where her lack of make-up contributed to an \"air of nobility\". According to \"Life\", the impression that she left on Hollywood, after she returned to Sweden, was of a tall girl \"with light brown hair and blue eyes who was painfully shy, but friendly, with a warm, straight, quick smile\".\nSelznick appreciated her uniqueness. Bergman was hailed as a fine new talent, and received many positive reviews. \"The New York Times\" noted her \"freshness and simplicity and natural dignity\" and the maturity of her acting which was nonetheless, free of \"stylistic traits\u2014the mannerisms, postures, precise inflections\u2014that become the stock in trade of the matured actress\". Variety noted that she was warm and convincing, and provided an \"arresting performance\" and that her \"charm, sincerity\" ...and \"infectious vivaciousness\" would \"serve her well in both comedy and drama\". There was also recognition of her natural appearance, in contrast to other film actresses. \"The New York Tribune\" critic wrote: \"Using scarcely any make-up, but playing with mobile intensity, she creates the character so vividly and credibly that it becomes the core of [the] narrative.\" Bergman made her stage debut in 1940 with \"Liliom\" opposite Burgess Meredith, at a time when she was still learning English. Selznick was worried that his new starlet's value would diminish if she received bad reviews. Brooks Atkinson of \"The New York Times\" said that Bergman seemed at ease, and commanded the stage that evening. That same year she starred in \"June Night\" (), a Swedish language drama film directed by Per Lindberg. She plays Kerstin, a woman who has been shot by her lover. The news reaches the national papers. Kerstin moves to Stockholm under the new name of Sara, but lives under the scrutiny and watchful eye of her new community. wrote, \"Bergman establishes herself as an actress belonging to the world elite.\"\nBergman was loaned out of David O. Selznick's company, to appear in three films which were released in 1941. On 18 February, Robert Sherwood Productions' released her second collaboration with Gregory Ratoff, \"Adam Had Four Sons.\" On 7 March, Metro-Goldwyn-Mayer released W. S. Van Dyke's \"Rage in Heaven\". On 12 August, Victor Fleming's \"Dr. Jekyll and Mr. Hyde\", another Metro-Goldwyn-Mayer production, had its New York opening. Bergman was supposed to play the \"good girl\" role of Dr Jekyll's fianc\u00e9e but pleaded with the studio that she should play the \"bad girl\" Ivy, the saucy barmaid. Reviews noted that \"she gave a finely-shaded performance\". A New York Times review stated that \"the young Swedish actress proves again, that a shining talent can sometimes lift itself above an impossibly written role\". Another review said: \"she displays a canny combination of charm, understanding, restraint and sheer acting ability.\"\nOn 30 July 1941 at the Lobero Theatre in Santa Barbara, Bergman made her second stage appearance in \"Anna Christie.\" She was praised for her performance as a whore in the play based on Eugene O'Neill's work. A San Francisco paper said she was as unspoiled as a fresh Swedish snowball. Selznick called her \"The Palmolive Garbo\", a reference to a popular soap, and a well-known Swedish actress of the time. Thornton Delaharty said, \"Lunching with Ingrid is like sitting down to an hour or so of conversation with an intelligent orchid.\"\n\"Casablanca\", by Michael Curtiz, opened on 26 November 1942. Bergman co-starred with Humphrey Bogart in the film; this remains her best-known role. She played the role of Ilsa, the former love of Rick Blaine and wife of Victor Laszlo, fleeing with Laszlo to the United States. The film premiered on 26 November 1942 at New York's Hollywood Theater. \"The Hollywood Reporter\" wrote, \"The events are shot with sharp humor and delightful touches of political satire.\" It went into more general release, in January 1943.\"Casablanca\" was not one of Bergman's favorite performances. \"I made so many films which were more important, but the only one people ever want to talk about is that one with Bogart.\" In later years, she stated, \"I feel about \"Casablanca\" that it has a life of its own. There is something mystical about it. It seems to have filled a need, a need that was there before the film, a need that the film filled\". Despite her personal views regarding her performance, Bosley Crowther of \"The New York Times\" wrote that \"Bergman was surprisingly lovely, crisp and natural\u00a0... and lights the romantic passages with a warm and genuine glow\". Other reviewers said that she \"[plays] the heroine with\u00a0... appealing authority and beauty\" and \"illuminates every scene in which she appears\" and compared her to \"a youthful Garbo.\"\n\"For Whom the Bell Tolls\" had its New York premiere on 14 July 1943. With \"Selznick's steady boosting\", she played the part of Maria, it was also her first color film. For the role, she received her first Academy Award nomination for Best Actress. The film was adapted from Ernest Hemingway's novel of the same title and co-starred Gary Cooper. When the book was sold to Paramount Pictures, Hemingway stated that \"Miss Bergman, and no one else, should play the part\". His opinion came from seeing her in her first American role, \"Intermezzo\". They met a few weeks later, and after studying her, he declared, \"You \"are\" Maria!\". James Agee, writing in \"The Nation\", said Bergman \"bears a startling resemblance to an imaginable human being; she really knows how to act, in a blend of poetic grace with quiet realism, which almost never appears in American pictures.\" He speaks movingly of her character's confession of her rape, and her scene of farewell, \"which is shattering to watch\". Agee believed that Bergman has truly studied what Maria might feel and look like in real life, and not in a Hollywood film. Her performance is both \"devastating and wonderful to see\".\n\"Gaslight\" opened on 4 May 1944. Bergman won her first Academy Award for Best Actress for her performance. Under the direction of George Cukor, she portrayed a \"wife driven close to madness\" by her husband, played by Charles Boyer. The film, according to Thomson, \"was the peak of her Hollywood glory.\" Reviewers noted her sympathetic and emotional performance, and that she exercised restraint, by not allowing emotion to \"slip off into hysteria\". \"The New York Journal-American\" called her \"one of the finest actresses in filmdom\" and said that \"she flames in passion and flickers in depression until the audience\u2014becomes rigid in its seats\".\n\"The Bells of St. Mary's\" premiered on 6 December 1945. Bergman played a nun opposite Bing Crosby, for which she received her third consecutive nomination for Best Actress. Crosby plays a priest who is assigned to a Roman Catholic school where he conflicts with its headmistress, played by Bergman. Reviewer Nathan Robin said: \"Crosby's laconic ease brings out the impishness behind Bergman's fine-china delicacy, and Bergman proves a surprisingly spunky and spirited comic foil for Crosby\". The film was the biggest box office hit of 1945.\nAlfred Hitchcock's \"Spellbound\" premiered on 28 December 1945. In \"Spellbound\", Bergman played Dr. Constance Petersen, a psychiatrist whose analysis could determine whether or not Dr. Anthony Edwardes, played by Gregory Peck, is guilty of murder. Artist Salvador Dal\u00ed was hired to create a dream sequence but much of what had been shot was cut by Selznick. During the film, she had the opportunity to appear with Michael Chekhov, who was her acting coach during the 1940s. This would be the first of three collaborations she had with Hitchcock.\nNext, Bergman starred in \"Saratoga Trunk\", with Gary Cooper, a film originally shot in 1943, but released on 30 March 1946. It was first released to the armed forces overseas. In deference to more timely war-themed and patriotic films, Warner Bros held back the theatrical opening in the United States. On 6 September premiered Hitchcock's \"Notorious.\" In it, Bergman played a US spy, Alicia Huberman, who had been given an assignment to infiltrate the Nazi sympathizers in South America. Along the way, she fell in love with her fellow spy, played by Cary Grant. The film also starred Claude Rains in an Oscar-nominated performance by a supporting actor. According to Roger Ebert, \"Notorious\" is the most elegant expression of Hitchcock's visual style. \"\"Notorious\" is my favorite Hitchcock\", he asserted. Writing for the BFI, Samuel Wigley called it a \"perfect\" film. \"Notorious\" was selected by the National Film Registry in 2006 as culturally and significantly important.\nOn 5 October 1946, Bergman appeared in \"Joan of Lorraine,\" written by Maxwell Anderson as a play within a play\",\" at the Alvin Theatre in New York. Tickets were fully booked for a twelve-week run. It was the greatest hit in New York. After each performance, crowds were in line to see Bergman in person. \"Newsweek\" called her 'Queen of the Broadway Season.' She reportedly received roughly $129,000 plus 15 percent of the grosses. \"The Associated Press\" named her \"Woman of the Year\". \"Gallup\" certified her as the most popular actress in America.\nOn 17 February 1948, \"Arch of Triumph\", by Lewis Milestone was released with Bergman and Charles Boyer as the leading roles Based on Erich Maria Remarque's book, it follows a story of Joan Madou, an Italian-Romanian refugee who works as a cabaret singer in a Paris nightclub. Distressed by her lover's sudden death, she attempts suicide by plunging into the Seine, but is rescued by Dr Ravic, a German surgeon (Charles Boyer). On 11 November 1948, \"Joan of Arc\" had its world premiere. For her role, Bergman received another Best Actress nomination. The independent film was based on the Maxwell Anderson play \"Joan of Lorraine\", which had earned her a Tony Award earlier that year. Produced by Walter Wanger and initially released through RKO. Bergman had championed the role since her arrival in Hollywood, then chose to appear on the Broadway stage in Anderson's play. The film was not a big hit with the public, partly because of the Rossellini scandal, which broke while the film was still in theatres. Even worse, it received disastrous reviews, and, although nominated for several Academy Awards, did not receive a Best Picture nomination. It was subsequently cut by 45 minutes, but restored to full length in 1998, and released in 2004 on DVD. With the release of the DVD there was also a renewed assessment of the film's acting and technical attributes, positively noting the acting, script and presentation. \n\"Under Capricorn\" premiered on 9 September 1949, as another Bergman and Hitchcock collaboration. The film is set in the Australia of 1831. The story opens as Charles Adare, played by Michael Wilding, arrives in New South Wales with his uncle. Desperate to find his fortune, Adare meets Sam Flusky (Joseph Cotten), who is married to Adare's childhood friend Lady Henrietta (Bergman), an alcoholic kept locked in their mansion. Soon, Flusky becomes jealous of Adare's affection for his wife. The film met with negative reactions from critics. Some of the negativity may have been based on disapproval of Bergman's affair with the Italian director Roberto Rossellini. Their scandalous relationship became apparent, shortly after the film's release.\n1950\u20131955: Italian films with Rossellini.\n\"Stromboli\" was released by Italian director Roberto Rossellini on 18 February 1950. Bergman had greatly admired two films by Rossellini. She wrote to him in 1949, expressing her admiration and suggesting that she make a film with him. As a consequence, she was cast in \"Stromboli\". During the production, they began an affair, and Bergman became pregnant with their first child.\nThis affair caused a huge scandal in the United States, where it led to Bergman being denounced on the floor of the United States Senate. On 14 March 1950, Senator Edwin C. Johnson insisted that his once-favorite actress \"had perpetrated an assault upon the institution of marriage\", and went so far as to call her \"a powerful influence for evil\". \"The purity that made people joke about Saint Bergman when she played Joan of Arc,\" one writer commented, \"made both audiences and United States senators feel betrayed when they learned of her affair with Roberto Rossellini.\" Art Buchwald, permitted to read her mail during the scandal, reflected in an interview, \"Oh, that mail was bad, ten, twelve, fourteen huge mail bags. 'Dirty whore.' 'Bitch.' 'Son of a bitch.' And they were all Christians who wrote it.\"\nEd Sullivan chose not to have her on his show, despite a poll indicating that the public wanted her to appear. However, Steve Allen, whose show was equally popular, did have her as a guest, later explaining \"the danger of trying to judge artistic activity through the prism of one's personal life\". Spoto notes that Bergman had, by virtue of her roles and screen persona, placed herself \"above all that\". She had played a nun in \"The Bells of St. Mary's\" (1945), and a virgin saint in \"Joan of Arc\" (1948). Bergman later said, \"People saw me in \"Joan of Arc\", and declared me a saint. I'm not. I'm just a woman, another human being.\" As a result of the scandal, Bergman returned to Italy, left her first husband, and went through a publicized divorce and custody battle for their daughter. Bergman and Rossellini were married on 24 May 1950.\nIn the United States, the film \"Stromboli\" was a box office bomb but did better overseas, where Bergman and Rossellini's affair was considered less scandalous. In all, RKO lost $200,000 on the picture. In Italy, it was awarded the Rome Prize for Cinema as the best film of the year. The initial reception in America, however, was very negative. Bosley Crowther of \"The New York Times\" opened his review by writing: \"After all the unprecedented interest that the picture \"Stromboli\" has aroused\u2014it being, of course, the fateful drama which Ingrid Bergman and Roberto Rossellini have made\u2014it comes as a startling anticlimax to discover that this widely heralded film is incredibly feeble, inarticulate, uninspiring and painfully banal.\" Crowther added that Bergman's character \"is never drawn with clear and revealing definition, due partly to the vagueness of the script and partly to the dullness and monotony with which Rossellini has directed her.\"\nThe staff at \"Variety\" agreed, writing\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;Director Roberto Rossellini purportedly denied responsibility for the film, claiming the American version was cut by RKO beyond recognition. Cut or not cut, the film reflects no credit on him. Given elementary-school dialog to recite and impossible scenes to act, Ingrid Bergman's never able to make the lines real nor the emotion sufficiently motivated to seem more than an exercise ... The only visible touch of the famed Italian director is in the hard photography, which adds to the realistic, documentary effect of life on the rocky, lava-blanketed island. Rossellini's penchant for realism, however, does not extend to Bergman. She's always fresh, clean and well-groomed.\n\"Harrison's Reports\" wrote: \"As entertainment, it does have a few moments of distinction, but on the whole it is a dull slow-paced piece, badly edited and mediocre in writing, direction and acting.\" John McCarten of \"The New Yorker\" found that there was \"nothing whatsoever in the footage that rises above the humdrum\", and felt that Bergman \"doesn't really seem to have her heart in any of the scenes.\" Richard L. Coe of \"The Washington Post\" lamented, \"It's a pity that many people who never go to foreign-made pictures will be drawn into this by the Rossellini-Bergman names and will think that this flat, drab, inept picture is what they've been missing.\"\nRecent assessments have been more positive. Reviewing the film in 2013 in conjunction with its DVD release as part of The Criterion Collection, Dave Kehr called the film \"one of the pioneering works of modern European filmmaking.\" In an expansive analysis of the film, critic Fred Camper wrote of the drama, &lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;Like many of cinema's masterpieces, \"Stromboli\" is fully explained only in a final scene that brings into harmony the protagonist's state of mind and the imagery. This structure...suggests a belief in the transformative power of revelation. Forced to drop her suitcase (itself far more modest than the trunks she arrived with) as she ascends the volcano, Karin is stripped of her pride and reduced \u2014 or elevated \u2014 to the condition of a crying child, a kind of first human being who, divested of the trappings of self, must learn to see and speak again from a personal \"year zero\" (to borrow from another Rossellini film title).\nThe Venice Film Festival ranked \"Stromboli\" among the 100 most important Italian films (\"100 film italiani da salvare\") from 1942 to 1978. In 2012, the British Film Institute's \"Sight &amp; Sound\" critics' poll also listed it as one of the 250 greatest films of all time.\nIn 1952, Rossellini directed Bergman in \"Europa '51\", where she plays Irene Girard who is distraught by the sudden death of her son. Her husband played by Alexander Knox soon copes, but Irene seems to need a purpose in life to assuage her guilt of neglecting her son.\nRossellini directed her in a brief segment of his 1953 documentary film, \"Siamo donne\" (\"We, the Women\"), which was devoted to film actresses. His biographer, Peter Bondanella, notes that problems with communication during their marriage may have inspired his films' central themes of \"solitude, grace, and spirituality in a world without moral values\". In December 1953, Rossellini directed her in the play \"Joan of Arc at the Stake\" in Naples, Italy. They took the play to Barcelona, London, Paris and Stockholm. Her performance received generally good reviews.\nTheir following effort was \"Viaggio in Italia (Journey to Italy)\" in 1954. It follows a couple's journey to Naples, Italy to sell an inherited house. Trapped in a lifeless marriage, they are further unnerved by the locals' way of living. According to John Patterson of \"The Guardian\", the film started The French New Wave. Martin Scorsese picked this film to be among his favorites in his documentary short in 2001. On 17 February 1955, \"Joan at the Stake\" opened at the Stockholm Opera House. The play was attended by the prime minister and other theatrical figures in Sweden. \"Swedish Daily\" reported that Bergman seems vague, cool and lacking in charisma. Bergman was hurt by mostly negative reviews from the media of her native land. Stig Ahlgren was the most harsh when he labelled her a clever businesswoman, not an actress. \"Ingrid is a commodity, a desirable commodity which is offered in the free market.\" Another effort they released that year was \"Giovanna d'Arco al rogo (Joan of Arc at the Stake)\".\nTheir final effort in 1954 was \"La Paura\" (\"Fear\"), based on a play by Austro-Jewish writer Stefan Zweig's 1920 novella \"Angst\" about adultery and blackmail. In \"Fear\", Bergman plays a businesswoman who runs a pharmaceutical company founded by her husband (Mathias Wieman). She is having an affair with a man whose ex-lover turns up and blackmails her. The woman demands money, threatening to tell her husband about the affair if Bergman doesn't pay her off. Under constant threats, Bergman is pressed to the point of committing suicide.\nRossellini's use of a Hollywood star in his typically \"neorealist\" films, in which he normally used non-professional actors, provoked some negative reactions in certain circles. Rossellini, \"defying audience expectations[,]...employed Bergman \"as if\" she were a nonprofessional,\" depriving her of a script and the typical luxuries accorded to a star (indoor plumbing, for instance, or hairdressers) and forcing Bergman to act \"inspired by reality while she worked\", creating what one critic calls \"a new cinema of psychological introspection\". Bergman was aware of Rossellini's directing style before filming, as the director had earlier written to her explaining that he worked from \"a few basic ideas, developing them little by little\" as a film progressed. Rossellini then was accused of ruining her successful career by taking her away from Hollywood, while Bergman was seen as the impetus for Rossellini abandoning the aesthetic style and socio-political concerns of Neo-Realism.\nWhile the movies Bergman made with Rossellini were commercial failures, the films have garnered great appreciation and attention in recent times. According to Jordan Cronk in his article reviewing the movies, their work has inspired a beginning of a modern cinematic era. Rossellini's films during the Bergman era ponder issues of complex psychology as depicted by Bergman in films like \"Stromboli\", \"Europa '51\" and \"Journey to Italy\". The influence of Bergman and Rossellini's partnership can be felt in the movies by Godard, Fellini and Antonioni to, more recently, Abbas Kiarostami and Nuri Bilge Ceylan. David Kehr from \"The New York Times\" commented that their films now stand among the pioneering works whose influence can be felt in European modern filmmaking.\n1956\u20131972: Hollywood return.\nAfter separating from Rossellini, Bergman starred in Jean Renoir's \"Elena and Her Men\" (\"Elena et les Hommes\", 1956), a romantic comedy in which she played a Polish princess caught up in political intrigue. Bergman and Renoir had wanted to work together. In \"Elena and Her Men\", which Renoir had written for her, she plays down-on-her-luck Polish princess Elena Sorokowska. The film was a hit in Paris when it premiered in September 1956. Candice Russell, commented that Bergman is the best thing in the film. Roger Ebert wrote, \"The movie is about something else\u2014about Bergman's rare eroticism, and the way her face seems to have an inner light on film. Was there ever a more sensuous actress in the movies?\"\nIn 1956, Bergman also starred in a French adaptation of the stage production \"Tea and Sympathy\". It was presented at the Th\u00e9\u00e2tre de Paris, Paris. It tells a story of a \"boarding school boy\" who is thought to be homosexual. Bergman played the wife of the headmaster. She is supportive of the young man, grows closer to him and later has sex with him, as a way to \"prove\" and support his masculinity. It was a smash hit.\nTwentieth Century Fox had bought the rights to \"Anastasia\" with Anatole Litvak slated to direct. Executive producer Buddy Adler wanted Bergman, then still a controversial figure in the States, to return to the American screen after a seven-year absence. Litvak also felt she would be an excellent actress for the part and insisted on her starring in the film. Fox agreed to take a chance, making her a box-office risk to play the leading role. Filming would take place in England, Paris, and Copenhagen.\n\"Anastasia\" (1956) tells the story of a woman who may be the sole surviving member of the Romanov family. Yul Brynner is the scheming general, who tries to pass her off as the single surviving daughter of the late Tsar Nicholas II. He hopes to use her to collect a hefty inheritance. \"Anastasia\" was an immediate success. Bosley Crowther wrote in \"The New York Times\", \"It is a beautifully molded performance, worthy of an Academy Award and particularly gratifying in the light of Miss Bergman's long absence from commendable films.\"\nWith her role in \"Anastasia\", Bergman made a triumphant return to working for a Hollywood studio (albeit in a film produced in Europe) and won the Academy Award for Best Actress for a second time. Cary Grant accepted the award on her behalf. Its director, Anatole Litvak, described her as \"one of the greatest actresses in the world\":&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;Ingrid looks better now than she ever did. She's 42, but she looks divine. She is a simple, straightforward human being. Through all her troubles she held to the conviction that she had been true to herself and it made her quite a person. She is happy in her new marriage, her three children by Rossellini are beautiful, and she adores them.\nAfter Anastasia, Bergman starred in \"Indiscreet\" (1958), a romantic comedy directed by Stanley Donen. She plays a successful London stage actress, Anna Kalman, who falls in love with Philip Adams, a diplomat played by Cary Grant. The film is based on the play \"'Kind Sir\"' written by Norman Krasna. Unmarried and wanting to stay single, he tells her that he is married but cannot get a divorce. Cecil Parker and Phyllis Calvert also co-starred.\nBergman later starred in the 1958 picture \"The Inn of the Sixth Happiness\", based on a true story about Gladys Aylward, a Christian missionary in China who, despite many obstacles, was able to win the hearts of the Chinese through patience and sincerity. In the film's climactic scene, she leads a group of orphaned children to safety, to escape from the Japanese invasion. The \"New York Times\" wrote, \"the justification of her achievements is revealed by no other displays than those of Miss Bergman's mellow beauty, friendly manner and melting charm.\" The film also co-starred Robert Donat and Curd Jurgens.\nBergman made her first post-scandal public appearance in Hollywood at the 31st Academy Awards in 1959, as presenter of the award for Best Picture, and received a standing ovation when introduced. She presented the award for Best Motion Picture together with Cary Grant, with whom she had recently starred in \"Indiscreet\". Bergman made her television debut in an episode of \"Startime\", an anthology show, which presented dramas, musical comedies, and variety shows. The episode was \"The Turn of the Screw\", an adaptation of the horror novella by Henry James, directed by John Frankenheimer. She played a governess of two little children who are haunted by the ghost of their previous caretaker. For this performance, she was awarded the 1960 Emmy for best dramatic performance by an actress. Also in 1960, Bergman was inducted into the Hollywood Walk of Fame with a motion pictures star at 6759 Hollywood Boulevard.\nIn 1961, Bergman's second American television production, \"Twenty-four Hours in a Woman's Life\", was produced by her third husband, Lars Schmidt. Bergman played a bereaved wife in love with a younger man she has known for only 24 hours. She later starred in \"Goodbye Again\" as Paula Tessier, a middle-aged interior designer who falls in love with Anthony Perkins' character, fifteen years her junior. Paula is in a relationship with Roger Demarest, a womanizer, played by Yves Montand. Roger loves Paula but is reluctant to give up his womanizing ways. When Perkins starts pursuing her, the lonely Paula is suddenly forced to choose between the two men. In his review of the film, Bosley Crowther wrote that Bergman was neither convincing nor interesting in her part as Perkins's lover.\nIn 1962, Schmidt also co-produced his wife's third venture into American television, \"Hedda Gabler\", made for the BBC and CBS. She played the titular character opposite Michael Redgrave and Ralph Richardson. David Duprey wrote in his review, \"Bergman and Sir Ralph Richardson on screen at the same time is like peanut butter and chocolate spread on warm toast.\" Later in the year, she took the titular role of \"Hedda Gabler\" at the in Paris.\nOn 23 September 1964, \"The Visit\" premiered; based on Friedrich D\u00fcrrenmatt's 1956 play, \"Der Besuch der alten Dame; eine tragische Kom\u00f6die,\" it starred Bergman and Anthony Quinn. With a production budget of $1.5\u00a0million, principal photography took place in Capranica, outside of Rome. She plays Karla Zachanassian, the world's richest woman, who returns to her birthplace, seeking revenge.\nOn 13 May 1965, Anthony Asquith's \"The Yellow Rolls-Royce\" premiered. Bergman plays Gerda Millett, a wealthy American widow who meets up with a Yugoslavian partisan, Omar Sharif. For her role, she was reportedly paid $250,000. That same year, although known chiefly as a film star, Bergman appeared in London's West End, working with stage star Michael Redgrave in \"A Month in the Country.\" She took on the role of Natalia Petrovna, a lovely headstrong woman, bored with her marriage and her life. According to \"The Times\", \"The production would hardly have exerted this special appeal without the presence of Ingrid Bergman.\"\nIn 1966, Bergman acted in only one project, an hour-long television version of Jean Cocteau's one-character play, \"The Human Voice\". It tells a story of a lonely woman in her apartment talking on the phone to her lover who is about to leave her for another woman. \"The New York Times\" praised her performance, calling it a tour-de-force. \"The Times of London\" echoed the same sentiment, describing it as a great dramatic performance through this harrowing monologue.\nIn 1967, Bergman was cast in a short episode of Swedish anthology film, \"Stimulantia\". Her segment which is based on the Guy de Maupassant's \"The Jewellery\" reunited her with Gustaf Molander. Next, Eugene O'Neill's \"More Stately Mansions\" directed by Jos\u00e9 Quintero, opened on 26 October 1967. Bergman, Colleen Dewhurst, and Arthur Hill appeared in the leading roles. The show closed on 2 March 1968 after 142 performances. It was reported that thousands of spectators bought tickets, and travelled across the country, to see Bergman perform. Bergman returned as both a presenter and a performer during the 41st Annual Academy Awards in 1969.\nBergman wished to work in American films again, following a long hiatus. She starred in \"Cactus Flower\" released in 1969, with Walter Matthau and Goldie Hawn. Here, she played a prim spinster, a dental nurse-receptionist who is secretly in love with her boss, the dentist, played by Matthau. Howard Thompson wrote in \"The New York Times\":\nThe teaming of Matthau, whose dour, craggy virility now supplants the easy charm of Barry Nelson, and the ultra-feminine Miss Bergman, in a rare comedy venture, was inspirational on somebody's part. The lady is delightful as a (now) \"Swedish iceberg\", no longer young, who flowers radiantly while running interference for the boss's romantic bumbling. The two stars mesh perfectly.\nOn 9 April 1970, Guy Green's \"A Walk in the Spring Rain\" had its world premiere. Bergman played Libby, the middle-aged wife of a New York professor (Fritz Weaver). She accompanies him on his sabbatical in the Tennessee mountains, where he intends to write a book. She meets a local handyman, Will Cade (Anthony Quinn), and they form a mutual attraction. The screenplay, by writer-producer Stirling Silliphant, was based on the romantic novel written by Rachel Maddux. \"The New York Times\" in its review wrote, \"Striving mightily and looking lovely, Miss Bergman seems merely a petulant woman who falls into the arms of Quinn for novelty, from boredom with her equally bored husband, [Weaver], pecking away on a book in their temporary mountain retreat.\"\nOn 18 February 1971, \"Captain Brassbound's Conversion\", a play based on George Bernard Shaw's work, made a debut at London theatre. She took on the role of a woman whose husband has taken up with a woman half her age. Although the play was a commercial success, critics were not very receptive of Bergman's British accent.\nShe made an appearance in one episode of \"The Bob Hope Show\" in 1972. Also that year, U.S. Senator Charles H. Percy entered an apology into the \"Congressional Record\" for the verbal attack made on Bergman on 14 March 1950 by Edwin C. Johnson. Percy noted that she had been \"the victim of bitter attack in this chamber 22 years ago.\" He expressed regret that the persecution caused Bergman to \"leave this country at the height of her career\". Bergman said that the remarks had been difficult to forget, and had caused her to avoid the country for nine years. Although she had paid a high price, Bergman had made peace with America, according to her daughter, Isabella Rossellini.\n1973\u20131982: Later years and continued success.\nOn 27 September 1972, Fielder Cook's \"From the Mixed-Up Files of Mrs. Basil E. Frankweiler\" premiered. She plays the titular character, a wealthy recluse who befriends two children who are seeking \"treasure\" in the Metropolitan Museum of Art.\nAlso that year, Bergman was the president of the jury at the 1973 Cannes Film Festival. In an interview with \"The Daytona Beach Sunday News\" in 1978, she recalled this event because she met with the unrelated Ingmar Bergman. This gave her the opportunity to remind him about a letter she had written some ten years ago, asking him to cast her in one of his pictures. Knowing that Ingmar would be attending, she made a copy of his long-ago reply, and put it in his pocket. He didn't reply for two years.\nNext, Bergman returned to London's West End and appeared with John Gielgud in \"The Constant Wife,\" which was a critical success; the theatre was consistently packed. \"The Daily Telegraph\" found the play \"unusually entertaining\", while Harold Hobson of \"The Sunday Times\" was still peeved at Bergman for playing yet another English woman with a \"strange accent\".\nBergman became one of the few actresses ever to receive three Oscars when she won her third (and first in the category of Best Supporting Actress) for her performance in \"Murder on the Orient Express\" (1974). Director Sidney Lumet had offered Bergman the important part of Princess Dragomiroff, with which he felt she could win an Oscar. She insisted on playing the much smaller role of Greta Ohlsson, the old Swedish missionary. Lumet discussed Bergman's role:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;She had chosen a very small part, and I couldn't persuade her to change her mind. ... Since her part was so small, I decided to film her one big scene, where she talks for almost five minutes, straight, all in one long take. A lot of actresses would have hesitated over that. She loved the idea, and made the most of it. She ran the gamut of emotions. I've never seen anything like it.\nAt the 1975 Academy Awards, film director Jean Renoir was to receive a Lifetime Achievement Award for his contribution to the motion picture industry. As he was ill at the time, he asked that Ingrid Bergman accept this award on his behalf. Bergman made a speech of acceptance that praised his films and the \"compassion that marked all his works\" as well as his teaching of both young filmmakers and audiences. Although she had been nominated for the new Best Supporting Actress Award, she considered her role in \"Murder on the Orient Express\" to be quite minor and did not expect to win. When the award was announced, in her surprised and unrehearsed remarks, she remarked to the audience that Valentina Cortese should have won the award for her role in \"Day for Night\", by Truffaut. Bergman and Cortese spent the rest of the evening in each other's company, and were the subject of many photographs. Also in 1975, Bergman attended the AFI tribute to Orson Welles. The audience gave her a standing ovation when she appeared on stage. She joked that she hardly knew Welles and they only invited her because she was working across the street.\nIn 1976, Bergman was the first person to receive France's newly created Honorary C\u00e9sar, a national film award. She also appeared in \"A Matter of Time\", by Vincente Minnelli, which premiered on 7 October 1976. Roger Ebert in his review wrote,\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;\"A Matter of Time\" is a fairly large disappointment as a movie, but as an occasion for reverie, it does very nicely. Once we've finally given up on the plot \u2013 a meandering and jumbled business \u2013 we're left with the opportunity to contemplate Ingrid Bergman at 60. And to contemplate Ingrid Bergman at any age is, I submit, a passable way to spend one's time.\nFrom 1977 to 1978, Bergman returned to the London's West with Wendy Hiller in \"Waters of the Moon\". She played Helen Lancaster, a rich, self-centred woman whose car becomes stuck in a snowdrift. The play became the great new hit of the season.\nIn 1978, Bergman appeared in \"Autumn Sonata\" (), by accomplished Swedish filmmaker, Ingmar Bergman (no relation), for which she received her seventh\u2014and final\u2014Academy Award nomination. She did not attend the awards, due to her illness. This was her final cinema performance. The film gave her the opportunity to work with Liv Ullmann, another well-known and respected Scandinavian artist. In the film, Bergman plays a celebrity pianist, Charlotte, who travels to Norway intending to visit her neglected eldest daughter, Eva, played by Ullmann. Eva is married to a clergyman and they care for her sister, Helena, who is severely disabled, paralyzed, and unable to speak clearly. Charlotte has not visited either of her two daughters for seven years. Upon arrival at Eva's home, she is shocked and dismayed to learn that her younger daughter is also in residence, and not still in the institution \"home\". Very late that night, Eva and Charlotte have an impassioned and painful conversation about their past relationship. Charlotte leaves the next day. The film was shot in Norway.\nBergman was battling cancer at the time of the filming. The final two weeks of the shooting schedule required adjustment because she required additional surgery. Believing that her career was nearing its end, Bergman wanted her swan song to be honourable. She was pleased with the overwhelming critical acclaim for \"Autumn Sonata\". Stanley Kaufmann of \"The New Republic\" wrote, \"The astonishment is Bergman's performance. We've all adored her for decades but not many of us have thought her a superb actress. Here, she exalted in the hands of a master.\" \"Newsweek\" wrote, \"An expressive force we can't even remember seeing since Hollywood grabbed her.\" \"The Times\" (London) concurred that it was \"a tour-de-force, such as the cinema rarely sees\". Both Bergman and Ullmann won the \"New York Film Critic's Award\" and Italy's \"Donatello\" award, for their roles. Bergman later recalled that Ingmar had possibly given her the best role of her career, and that she would never make another movie again. \"I don't want to go down and play little parts. This should be the end.\"\nIn 1979, Bergman hosted the AFI's Life Achievement Award Ceremony for Alfred Hitchcock. At the program's finale, she presented him with the wine cellar key that was crucial to the plot of \"Notorious\". \"Cary Grant kept this for 10 years, then he gave it to me, and I kept it for 20 years for good luck and now I give it to you with my prayers,\" before adding \"God bless you, Hitch.\" Bergman was the guest of honour in the Variety's Club All Star Salute program in December 1979. The show was hosted by Jimmy Stewart and was attended by Cary Grant, Frank Sinatra, Goldie Hawn, Helen Hayes, Paul Henreid and many of her former co-stars. She was honored with the \"Illis Quorum\", the medal given to artists of significance by the King of Sweden.\nIn the late '70s, Bergman appeared on several talk shows and was interviewed by Merv Griffin, David Frost, Michael Parkinson, Mike Douglas, John Russell and Dick Cavett, discussing her life and career.\nIn 1980, Bergman's autobiography, \"Ingrid Bergman: My Story\", was written with the help of Alan Burgess. In it, she discusses her childhood, her early career, her life during her time in Hollywood, the Rossellini scandal, and subsequent events. The book was written after her son warned her that she would only be known through rumors and interviews if she did not tell her own story. In 1982, she was awarded the David di Donatello's Golden Medal of the Minister of Tourism, given by The Academy of Italian Cinema.\nFinally that year, Bergman played the starring role in a television miniseries, \"A Woman Called Golda\" (1982), about the late Israeli prime minister Golda Meir. It was to be her final acting role and she was honored posthumously with a second Emmy Award for Best Actress. Bergman was surprised to be offered the role, but the producer explained, \"People believe you and trust you, and this is what I want, because Golda Meir had the trust of the people.\" Her daughter Isabella added, \"Now, \"that\" was interesting to Mother.\" She was also persuaded that Golda was a \"grand-scale person\", one who people would assume was much taller than she actually was. Chandler notes that the role \"also had a special significance for her, as during World War II, Ingrid felt guilty because she had so misjudged the situation in Germany\".\nAccording to Chandler, \"Ingrid's rapidly deteriorating health was a more serious problem. Insurance for Bergman was impossible. Not only did she have cancer, but it was spreading, and if anyone had known how bad it was, no one would have gone on with the project.\" After viewing the series on TV, Isabella commented:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;She never showed herself like that in life. In life, Mum showed courage. She was always a little vulnerable, courageous, but vulnerable. Mother had a sort of presence, like Golda, I was surprised to see it\u00a0... When I saw her performance, I saw a mother that I'd never seen before\u2014this woman with balls.\nHer daughter said that Bergman identified with Golda Meir, because she, too had felt guilty. Bergman tried to strike a balance between home and work responsibilities and deal with \"the inability to be in two places at one time\". Bergman's arm was terribly swollen from her cancer surgery. She was often ill during the filming, recovering from a mastectomy and the removal of lymph nodes. It was important to her, as an actress, to make a certain gesture of Meir's, which required her to raise both arms, but she was unable to properly raise one arm. During the night, her arm was propped up, in an uncomfortable position, so that the fluid would drain, and enable her to perform her character's important gesture.\nDespite her health problems, she rarely complained or let others see the difficulties she endured. Four months after the filming was completed, Bergman died on her 67th birthday. After her death, her daughter Pia accepted her Emmy.\nPersonal life.\nMarriages and children.\nOn 10 July 1937, at the age of 21, in St\u00f6de, Bergman married a dentist, Petter Aron Lindstr\u00f6m, who later became a neurosurgeon. The couple had one child, a daughter, Friedel Pia Lindstr\u00f6m. After returning to the United States in 1940, she acted on Broadway before continuing to do films in Hollywood. The following year, her husband arrived from Sweden with Pia. Lindstr\u00f6m stayed in Rochester, New York, where he studied medicine and surgery at the University of Rochester.\nIn between films, Bergman travelled to New York and stayed at their small rented stucco house, her visits lasting from a few days to four months. According to an article in \"Life\", the \"doctor regards himself as the undisputed head of the family, an idea that Ingrid accepts cheerfully\". He insisted she draw the line between her professional life and her personal life, as he had a \"professional dislike for being associated with the tinseled glamor of Hollywood\". Lindstr\u00f6m later moved to San Francisco, California, where he completed his internship at a private hospital, and they continued to spend time together when she could travel between filming. Lindstr\u00f6m did not view Bergman as the rest of the world did. He thought she was too absorbed with her professional popularity and image, and was full of vanity. According to Bergman's biographer, Donald Spoto, Lindstr\u00f6m managed her career and financial matters. He was very frugal with money. On 27 August 1945, two days before her 30th birthday, she (as Ingrid Lindstrom) and her husband both filed U.S. citizenship applications.\nLindstr\u00f6m had been aware of his wife's affairs. When asked by the biographer why he did not ask for a divorce, he replied bluntly, \"I lived with that because of her income.\" Bergman returned to Europe after the scandalous publicity surrounding her affair with Italian director Roberto Rossellini during the filming of \"Stromboli\" in 1950. She begged Lindstr\u00f6m for a divorce and contact with their daughter Pia, but he refused.\nIn the same month \"Stromboli\" was released, she gave birth to a boy, Renato Roberto Ranaldo Giusto Giuseppe (\"Robin\") Rossellini (born 2 February 1950). A week after her son was born, she divorced Lindstr\u00f6m under Mexican jurisdiction, and married Rossellini by proxy on 24 May 1950. On 18 June 1952, she gave birth to twin daughters Isotta Ingrid Rossellini and Isabella Rossellini. Isabella became an actress and model, and Isotta a professor of Italian literature. It was not until 1957 that Bergman was reunited with Pia in Rome. Her ex-husband, however, remained bitter towards Bergman.\nDuring the scandal, Bergman received letters of support from Cary Grant, Helen Hayes, Ernest Hemingway, John Steinbeck and other celebrities. The adultery and remarriage is remembered as a major sex scandal of 20th century Hollywood. It received an extraordinary amount of media attention in the U.S. and abroad, including Bergman's native Sweden. Bergman was treated harshly by the conservative Swedish press, with some journalists claiming she had destroyed the international reputation of Sweden. The scandal also took ethnic overtones, with Rossellini described as the over-sexed Latin lover. On the other hand, Bergman was defended by Swedish feminists, especially in 1955 when Bergman first returned to Sweden after the scandal, causing friction between conservative journalists and the emerging feminist movement. In the U.S., the scandal also took xenophobic turns. Sen. Edwin C. Johnson stated that \"under the law, no alien guilty of turpitude can set foot on American soil again\" and that Bergman had \"deliberately exiled herself from this country that was so good to her\". Isabella Rossellini said that \"[..] she was chased out of America because they felt that foreigners and stars, we come to America, and then behave immorally and are bad examples to the younger generations.\" Although post-war social conservate morals stoked the public outrage, another factor was Bergman's on-screen image as a pure, saint-like character; she later commented: \"People saw me in \"Joan of Arc\", and declared me a saint. I'm not. I'm just a woman, another human being\" and \"It was because so many people, who knew me only on the screen, thought I was perfect and infallible and then were angry and disappointed that I wasn't ... A nun does not fall in love with an Italian.\"\nHer marriage with Rossellini declined and eventually ended in divorce in 1957. Rossellini's cousin, Renzo Avanzo, was worried that Bergman would discourage Rossellini from making pictures he should be making. Rossellini didn't like her friends, fearing they would lure her back to Hollywood. He was possessive and would not allow Bergman to work for anyone else. In 1957, Rossellini had an affair with Sonali Das Gupta while filming in India. Bergman met with Indian Prime Minister Jawaharlal Nehru in London, to get permission for Rossellini to leave India.\nOn 21 December 1958, Bergman married Lars Schmidt, a theatrical entrepreneur from a wealthy Swedish shipping family. She met Schmidt through her publicist, Kay Brown. The couple and their children spent summers together in Danholmen, Lars's private island off the coast of Sweden. They also stayed at Choisel near Paris. Bergman traveled frequently for filming, while Lars produced plays and television shows all over Europe, and their work schedules put a strain on their marriage. While vacationing with Schmidt in Monte Gordo beach in Algarve, Portugal in 1963, right after making the TV movie \"Hedda Gabler\", she received a ticket for wearing a too-revealing bikini by the modesty standards of conservative Portugal. After almost two decades of marriage, the couple divorced in 1975. Nonetheless, Lars was by her side when she died on 29 August 1982, her 67th birthday.\nOne of Bergman's grandchildren is Elettra Rossellini Wiedemann.\nIn October 1978, Bergman gave an interview on her last film role, \"Autumn Sonata\", which explored the relationship between a mother and daughter. She played a classical concert pianist, who values her career more than motherhood and caring for her two daughters. Bergman said: \"A lot of it is what I have lived through, leaving my children, having a career.\" She recalled instances in her own life, \"when she had to pry her children's arms from around her neck, 'and then go away' to advance her career.\" In her will, the bulk of her estate was divided among her four children, with some provision for Rossellini's niece Fiorella, Bergman's maid in Rome, and her agent's daughter Kate Brown.\nRelationships.\nBergman had affairs with her directors and co-stars in the 1940s. Spencer Tracy and Bergman briefly dated during the filming of \"Dr. Jekyll and Mr. Hyde\". She later had an affair with Gary Cooper while shooting \"For Whom the Bell Tolls\". Cooper said, \"No one loved me more than Ingrid Bergman, but the day after filming concluded, I couldn't even get her on the phone.\" Jeanine Basinger writes, [Victor] \"Fleming fell deeply in love with the irresistible Swede and never really got over it\". While directing his final film \"Joan of Arc\", he was completely enthralled with Bergman. She had a brief affair with musician Larry Adler when she was travelling across Europe entertaining the troops in 1945. In Anthony Quinn's autobiography, he mentions his sexual relationship with Bergman, among his many other affairs. Howard Hughes was also quite taken by Bergman. They met through Cary Grant and Irene Selznick. He phoned one day to inform her that he had just bought RKO as a present for her.\nDuring her marriage to Lindstr\u00f6m, Bergman also had affairs with the photographer Robert Capa and actor Gregory Peck. It was through Bergman's autobiography that her affair with Capa became known.p.\u00a0176 In June 1945, Bergman was passing through Paris, on her way to Berlin to entertain American soldiers. In response to a dinner invitation she met Capa and novelist Irwin Shaw. By her account, they had a wonderful evening. The next day, she departed for Berlin. Two months later, Capa was in Berlin, photographing ruins, and they met again. Distressed over her marriage to Lindstr\u00f6m, she fell in love with Capa, and wished to leave her husband. During their months together in Berlin, Capa made enough money to follow Bergman back to Hollywood. Although \"Life\" magazine assigned him to cover Bergman, he was unhappy with the \"frivolity\" of Hollywood.\nBergman's brief affair with \"Spellbound\" co-star Gregory Peck was kept private until Peck confessed it to Brad Darrach of \"People\" in an interview five years after Bergman's death. Peck said, \"All I can say is that I had a real love for her (Bergman), and I think that's where I ought to stop ... I was young. She was young. We were involved for weeks in close and intense work.\"\nBergman was a Lutheran, once saying of herself, \"I'm tall, Swedish, and Lutheran\".\nLater, her daughter Isabella Rossellini said: \"She showed that women are independent, that women want to tell their own story, want to take initiative, but sometimes, they can't because sometimes, our social culture doesn't allow women to break away from certain rules.\"\nAfter the making of \"\" (1939), producer David O. Selznick and his wife Irene became friends with Bergman and remained so throughout her career. Bergman also formed a lifelong friendship with her \"Notorious\" co-star, Cary Grant. They met briefly in 1938 at a party thrown by Selznick. Scott Eyman in his book, \"Cary Grant: A Brilliant Disguise\", wrote: \"Grant found that he liked Ingrid Bergman a great deal... She was beautiful, but lots of actresses are beautiful. What made Bergman special was her indifference to her looks, her clothes, to everything except her art.\" Bergman and Hitchcock also formed a sustained friendship out of mutual admiration.\nAt , she was taller than some of her male costars, causing occasional problems. She was two\u00a0inches (5\u00a0cm) taller than Humphrey Bogart, and Bogart was reported as standing on blocks or sitting on cushions in their scenes together in \"Casablanca\". Bergman was also taller than Claude Rains, and for the scenes in \"Notorious\" where Rains and Bergman were to walk hand-in-hand, Hitchcock devised a system of ramps that boosted Rains's height below camera level, and Rains also wore elevator shoes in some scenes.\nIllness and death.\nDuring the run of \"The Constant Wife\" in London, Bergman discovered a hard lump under her left breast. The lump was removed on 15 June 1974 in a London clinic. While working on \"Autumn Sonata\", Bergman discovered another lump, and flew back to London for another surgery. Afterward, she began rehearsals for \"Waters of the Moon\" (1978).\nDespite her illness, she agreed to play Golda Meir in 1981, then retired to her apartment in Cheyne Gardens, London, where she underwent chemotherapy. As photographers camped outside on the pavement, she refrained from approaching the front window. The cancer had spread to her spine, collapsing her twelfth vertebra; her right lung no longer functioned, and only a small part of her left lung had not collapsed.\nBergman died in London on 29 August 1982, at midnight on her 67th birthday. Her ex-husband Lars Schmidt and three other people were present, having drunk their last toast to her hours earlier. A copy of \"The Little Prince\" was at her bedside, opened to a page near the end. The memorial service was held in St Martin-in-the-Fields church in October with 1,200 mourners. In attendance were her children, the Rossellinis, and relatives from Sweden, as well as numerous fellow actors and costars, including Liv Ullmann, Sir John Gielgud, Dame Wendy Hiller, Birgit Nilsson and Joss Ackland. As part of the service, quotations from Shakespeare were read. Musical selections included \"This Old Man\" from \"The Inn of the Sixth Happiness\", and \"As Time Goes By\". Bergman's grandson, Justin Daly, recalled the event as hundreds of photographers were waiting and taking pictures, when a camera hit him on the head: \"In the middle of all this chaos, I could sense that she wasn't just my grandmother. She belonged to everyone else. She belonged to the world.\"\nIngrid Bergman was cremated at a private ceremony attended only by close relatives and five friends at Kensal Green Cemetery, London, and her ashes were taken to Sweden. Most were scattered in the sea around the islet of Dannholmen, near the fishing village of Fj\u00e4llbacka in Bohusl\u00e4n. The location is on the west coast of Sweden, a place where she had spent most of the summers from 1958 until her death. The remainder of her ashes were placed next to her parents' ashes in Norra Begravningsplatsen (Northern Cemetery or Burial Place), Stockholm, Sweden.\nActing style, public image and screen persona.\nAccording to the \"St. James Encyclopedia of Popular Culture\", upon her arrival in the U.S. Bergman quickly became \"the ideal of American womanhood\" and a contender for Hollywood's greatest leading actress. David O. Selznick once called her \"the most completely conscientious actress\" with whom he had ever worked.\nBergman was often associated with vulnerable yet strong characters who were in love but were also troubled by anxiety and fear. As preparation for \"Gaslight\" she went to a mental hospital and observed a particular patient. For \"A Woman Called Golda\" she reviewed tapes in order to master Meir's mannerisms. In \"Autumn Sonata\", she moves across the screen like a caged animal but always keeps a ladylike composure that makes her words even more \"silent but deathly\". Bergman could be rigid and stubborn in her acting approach. Ingmar Bergman stated that they argued frequently, on set. \"She went to a limit and objected to go beyond the limit.\" Jan G\u00f6ransson of the Swedish Film Institute described Bergman as stubborn and loved to question her directors, whose innovative ideas about acting eventually won her over.\nBergman's ability to instantly change emotions was one of her greatest talents. Funing Tang from the University of Miami asserted, \"even a moment of reticence, a little glance, or even an eye movement can alter the film's direction and provide her film and character with suspense, ambiguity and mysteriousness, which are rooted in her singular characteristics.\"\nRoger Ebert echoed the same observation when he cited that Bergman has her way of looking into a man's face. He added: \"She doesn't simply gaze at his eyes, as so many actresses do, their thoughts on the next line of dialogue. She peers into the eyes, searching for meaning and clues, and when she is in a close two-shot with an actor, watch the way her own eyes reflect the most minute changes in his expression.\"\nFor writer Susan Kerr, Bergman might have the greatest downcast eyes in history. \"She got her greatest effects in \"Casablanca\" and \"Gaslight\" and \"Spellbound\" and \"Notorious\" by swooping her eyes down to the floor and darting them back and forth, as if watching a mouse scurry across the room\", Kerr wrote.\n&lt;templatestyles src=\"Template:Quote_box/styles.css\" /&gt;\nOf course, some of Ingrid's pictures in those early American years were not masterpieces, but I remember very clearly that whatever she did I was always fascinated by her face. In her face-the skin-the eyes-the mouth-especially the mouth. There was this very strange radiance and an enormous erotic attraction. It had nothing to do with the body, but in the relationship between her mouth, her skin, and her eyes.\n\u2014Ingmar Bergman on Ingrid Bergman\nAccording to \"Stardom and the Aesthetics of Neorealism: Ingrid Bergman in Rossellini's Italy\", Alfred Hitchcock was responsible for transforming Bergman's screen persona towards \"less is more\". He coaxed her to be more understated and neutral, while his camera concentrated the expression in the micro-movement of her face. Much of his work with her involved efforts to quell her expressiveness, gestures and body movements. Susan White, one of the contributing authors in \"A Companion to Alfred Hitchcock\", argued that while Bergman was one of his favorite collaborators, she is not the quintessential Hitchcock blonde. She is more like \"a resistant and defiant blonde\", in contrast to the Grace Kelly type (who was) more malleable and conformative. For Bergman, the face became a central aspect to her persona. In many of her films, her body is covered up in what are often elaborate costumes; nun's habits, doctor's coats, soldier's armors, and Victorian dresses. The technique of \"chiaroscuro\" had been used in many of Bergman's films to capture the ambience and the emotional turmoils of her characters through her face. In the case of \"Casablanca\", shadows and lighting were used to make her face look thinner. Peter Byrnes of \"The Sydney Morning Herald\" wrote that \"Casablanca\" is perhaps the world's best close-up movie, in which he added, \"after the initial set-up, they just keep coming, a series of stunningly emotional close-ups to die for\". Byrnes asserted that these close-ups are the start of the seduction process between Bergman and the audience. He added, \"She is so beautiful, and so beautifully lit, that the audience feels they've had their money's worth already.\" Bergman's daughter, Pia Lindstrom felt that her mother gave some of her best acting in her later films once her mother had finally been freed of her youthful, radiant physical beauty.\nProminent film writer Dan Callahan commented that there is an element of suspense when watching how Bergman\u00a0\u2013 who was a polyglot\u00a0\u2013 emotes, enhanced by her voice and the way she read her lines. He wrote that Bergman was less effective while speaking in French and German, as if she were void of creative energy. Angelica Jade Basti\u00e9n of Vulture echoed the same sentiment, that Bergman's secret weapon is her voice and her accent.\nBergman portrayed women in extra-marital affairs in \"Intermezzo\" and \"Casablanca\", prostitutes in \"Arch of Triumph\" and \"Dr Jekyll and Mr Hyde\", and a villainess in \"Saratoga Trunk\". Nonetheless, the public seemed to believe that Bergman's off-screen persona was similar to the saintly characters she played in \"Joan of Arc\" and \"The Bells of St. Mary's\". Although the preponderance of \"fallen woman\" roles did not besmirch Bergman's saintly status, the publicized affair with Rossellini resulted in a public sense of betrayal. David O. Selznick testified later, \"I'm afraid I'm responsible for the public's image of her as Saint Ingrid. We deliberately built her up as the normal, healthy, unneurotic career woman, devoid of scandal and with an idyllic home life. I guess that backfired later.\"\nLegacy.\nThe news of Bergman's death was widely reported across the United States and Europe. Both the \"Los Angeles Times\" and the \"New York Post\" printed front page notices. The \"New York Post\" announcement was in bold red. \"The New York Times\" stated: \"Ingrid Bergman, Winner of Three Oscars Is Dead.\" \"The Washington Post\" paid its tribute in an article that called her \"an actress whose innocent yet provocative beauty made her one of the great stars of stage and screen\".\nBergman was mourned by many, especially her fellow co-stars. They praised her tenacity, spirit, and warmth. Joseph Cotten considered her a great friend and a great actress. Paul Henreid commented: \"She was so terribly beautiful in her youth. She was a very strong lady with great desires and emotions and she led a colorful life.\" Liv Ullmann said that she would mourn her because \"She made me very proud to be a woman,\" she added. Leonard Nimoy praised her tenacity and courage. \"I developed enormous respect for her as a person and talent. She was a marvelous lady and actress\".\nOn 30 August 1983, stars, friends and family came to Venice Film Festival to honour the late Bergman on the first anniversary of her death. Among the many guests were Gregory Peck, Walter Matthau, Audrey Hepburn, Roger Moore, Charlton Heston, Prince Albert of Monaco, Claudette Colbert and Olivia de Havilland. They dined and wined for five days while remembering Bergman and the legacy she left behind.\nDespite suffering from cancer for eight years, Bergman continued her career and won international honours for her final roles. \"Her spirit triumphed with remarkable grace and courage\", added biographer Donald Spoto. Director George Cukor once summed up her contributions to the film media when he said to her, \"Do you know what I especially love about you, Ingrid, my dear? I can sum it up as your naturalness. The camera loves your beauty, your acting, and your individuality. A star must have individuality. It makes you a great star.\"\nWriting about her first years in Hollywood, \"Life\" stated that \"All Bergman vehicles are blessed\", and \"they all go speedily and happily, with no temperament from the leading lady\". She was \"completely pleased\" with the management of her early career by David O. Selznick, who always found excellent dramatic roles for her to play, and equally satisfied with her salary, once saying, \"I am an actress, and I am interested in acting, not in making money.\" \"Life\" adds that \"she has greater versatility than any actress on the American screen\u00a0... Her roles have demanded an adaptability and sensitiveness of characterization to which few actresses could rise\".\nBiographer Donald Spoto said she was \"arguably the most international star in the history of entertainment\". After her American film debut in the film \"\" (1939), Hollywood saw her as a unique actress who was completely natural in style, and without need for much make-up. Film critic James Agee wrote that she \"not only bears a startling resemblance to an imaginable human being; she really knows how to act, in a blend of poetic grace with quiet realism\".\nFilm historian David Thomson, said she \"always strove to be a 'true' woman\", and many filmgoers identified with her:&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;\nAccording to her daughter Isabella Rossellini, her mother had a deep sense of freedom and independence. She then added: \"She was able to integrate so many cultures... she is not even American but she is totally part of American culture like she is totally part of the Swedish, Italian, French, European film making.\"\nWesleyan University hosts the \"Ingrid Bergman Collection\" of Bergman's personal papers, scripts, awards, portraits, photos, scrapbooks, costumes, legal papers, financial records, stills, clippings and memorabilia.\nActivism.\nDuring a 1946 press conference in Washington, D.C. for the promotion of the play \"Joan of Lorraine\", she protested to the newspapers regarding racial segregation after seeing it first hand at Lisner Auditorium, the theater where she was working. This led to significant publicity and some hate mail. A bust of Bergman has been placed outside the Lisner Auditorium, in recognition of her protest, and as a reminder of the venue's segregated past.\nBergman went to Alaska during World War II to entertain US Army troops. Soon after the war ended, she also went to Europe for the same purpose, where she was able to see the devastation caused by the war. She arrived in Paris on 6 June 1945 with Jack Benny, Larry Adler and Martha Tilton where they stayed at The Ritz Hotel. Bergman's performance was rather limited; she couldn't sing, she couldn't play an instrument and she didn't have Jack Benny's humour. In Kassel, she ran offstage in tears. When they went to see a concentration camp, she stayed behind. After the onset of World War II, Bergman felt guilt for her initial dismissal of the Nazi state in Germany. According to her biographer Charlotte Chandler, she had at first considered the Nazis only a \"temporary aberration, 'too foolish to be taken seriously'. She believed Germany would not start a war.\" Bergman felt that \"the good people there would not permit it\". Chandler adds that she \"felt guilty all the rest of her life because when she was in Germany at the end of the war, she had been afraid to go with others to witness the atrocities of the Nazi extermination camps\".\nCentennial celebration.\nIn 2015, to celebrate the Bergman centennial, exhibitions, film screenings, books, documentaries and seminars were presented by various institutions. The Museum of Modern Art (MOMA) held a screening of her films, chosen and introduced by her children. AFI Silver Theatre and Cultural Center presented an extensive retrospective of her Hollywood and Italian films. University of California, Berkeley hosted a lecture, where journalist and film critic, Ulrika Knutson called Bergman \"a pioneering feminist\". Toronto International Film Festival continued with \"Notorious: Celebrating the Ingrid Bergman Centenary\" which featured a series of her best-known films. \"Ingrid Bergman at BAM\" was screened at Brooklyn Academy of Music's Rose Cinemas. BAMcin\u00e9matek presented \"Ingrid Bergman Tribute\" on 12 September 2015, an event co-hosted by Isabella Rossellini and Jeremy Irons, which featured a live reading by Rossellini and Irons taken from Bergman's personal letters. The Plaza Cinema &amp; Media Arts Center in Patchogue, New York held a special screening of Bergman's films. Screenings and tributes occurred in other cities; London, Paris, Madrid, Rome, Tokyo and Melbourne. The Bohusl\u00e4n Museum in Uddevalla, north of Gothenburg opened an exhibition titled \"Ingrid Bergman in Fj\u00e4llbacka\". A pictorial book titled \"Ingrid Bergman: A Life in Pictures\" was published by the Bergman estate.\n\"\", is a 2015 Swedish documentary film, directed by Stig Bj\u00f6rkman, which was made to celebrate her centennial. It was screened in the Cannes Classics section at the 2015 Cannes Film Festival, where it received a special mention for L'\u0152il d'or. A photograph of Bergman, by David Seymour was featured as the main poster at Cannes. The festival described Bergman as a \"modern icon, an emancipated woman, an intrepid actress, and a figurehead for the new realism\". The New York Film Festival and The Tokyo International Film Festival also presented the documentary.\nAt the 2015 Vancouver International Film Festival, the film was chosen as \"Most Popular International Documentary\", based on audience balloting. The film \"loses no chance to illuminate the independence and courage she showed in her private life\". Although the viewer may pronounce judgement on \" Bergman's free-wheeling, non-conformist maternal lifestyle, there can be no doubt about her determination and professional commitment.\" Ending with her last screen appearance in \"Autumn Sonata\" in 1978, \"Bjorkman leaves behind the image of a uniquely strong, independent woman whose relaxed modernity was way ahead of its time.\"\nTo celebrate the same occasion, the U.S. Postal Service and Posten AB of Sweden, jointly issued commemorative stamps in Bergman's honor. The stamp art features a circa 1940 image of Bergman taken by L\u00e1szl\u00f3 Willinger, with a colorized still of Bergman from \"Casablanca\" as the selvage photograph. Her daughter, Pia Lindstrom commented, \"I think she would be very surprised that she is on a U.S. stamp and I know she would think it is great fun.\"\nBiographical stage plays.\nBergman was portrayed by her daughter, Isabella Rossellini in\u00a0\"My Dad is 100 years Old\"\u00a0(2005). In 2015,\u00a0\"'Notorious\"',\u00a0a play based on Hitchcock's\u00a0\"Notorious\"\u00a0was staged at\u00a0The Gothenburg Opera. Bergman's Italian period has been dramatised on stage in the musical play which is titled,\u00a0\"Camera; The Musical About Ingrid Bergman\".\u00a0It was written by Jan-Erik S\u00e4\u00e4f and Staffan Aspegren and performed in Stockholm, Sweden.\nIn popular culture.\nWoody Guthrie composed \"Ingrid Bergman\", a song about Bergman in 1950. The lyrics have been described as \"erotic\", and make reference to Bergman's relationship with Roberto Rossellini, which began during work on the film \"Stromboli\". Billy Bragg and Wilco covered the song on their album \"Mermaid Avenue\" (1998).\nAlfred Hitchcock based his film \"Rear Window\" (1954) (starring James Stewart as a \"Life\" wartime photographer) on Bergman and Capa's romance.\nIn 1984, a hybrid tea rose breed was named \"Ingrid Bergman\", in honor of the star.\nHer portrayal of Ilsa Lund from \"Casablanca\" was parodied by Kate McKinnon in one episode of \"Saturday Night Live\". In the opening montage of the 72nd Academy Awards, Billy Crystal, as Victor Laszlo, made a parody out of \"Casablanca's\" final scene. In the '80s, Warner Bros made \"'Carrotblanca\"' as a homage to Bogart and Bergman's character in \"Casablanca\". In \"When Harry Met Sally\" (1989), \"Casablanca\" is a recurring theme, with the lead characters arguing over the meaning of its ending throughout the film. Bogart and Bergman also appeared in Tesco's Clubcard advertisement (2019).\nTo help educate and inform about the importance of mask-wearing during the COVID-19 pandemic, WarnerMedia, the Ad Council, and the Centers for Disease Control and Prevention (CDC), released a video featuring Bogart and Bergman in a scene from \"Casablanca\" wearing masks.\nIn one scene from \"Dead Men Don't Wear Plaid\" (1981)\",\" with some creative editing, Steve Martin's character is having a conversation with Alicia Huberman from \"Notorious\". In one scene from the movie \"Lake House\" (2006), Sandra Bullock's character is seen to be watching the kiss scene from \"Notorious\". The kiss scene between Bergman and Spencer Tracy from \"Dr Jekyll and Mr Hyde\" is featured in the \"Cinema Paradiso\" (1989) closing montage. Bergman's Sister Benedict is referenced in \"The Godfather\" (1972). There is one episode in the second season of \"The Days and Nights of Molly Dodd\", which is titled \"Here's a Little Known Ingrid Bergman Incident\".\nBergman's Ilsa also inspired the character Ilsa Faust, played by Swedish actress Rebecca Ferguson, in the film series. Ferguson was told by costar Tom Cruise and director Christopher McQuarrie to review \"Notorious\" and other of Bergman's films as preparation for her role.\nIn the movie \"La La Land\" (2016), the lead female character has a poster of Bergman on her bedroom wall. Near the end of the movie, another poster of Bergman can be seen by the side of a road. One of the original soundtracks for the film is named 'Bogart and Bergman.'\nBergman's publicity photo from \"Notorious\" was used as the front cover of the book by Dan Callahan, \"The Camera Lies: Acting for Hitchcock\" (2020). Bergman's love affair with Robert Capa has been dramatised in a 2012 novel by Chris Greenhalgh, \"Seducing Ingrid Bergman\". Bergman is also referenced in Donald Trump's 2004 book \"\", and in \"Small Fry\", a memoir by Lisa Brennan-Jobs, the daughter of Steve Jobs.\nAs part of its dedication to the female icons of Italian cinema, Bergman was immortalised in a mural on a public staircase off Via Fiamignano near Rome. A mural of her image from \"Casablanca\" was painted on the outdoor cinema wall in Fremont, Seattle. The Dutch National Airline named one of their planes \"Ingrid Bergman\" in the 2010s. There is a wax figure of her displayed at Madame Tussaud's, Hollywood, California. In Fj\u00e4llbacka, off the coast of Sweden, a square was named as Ingrid Bergman's Square to honor her memory. A wooden mould of Bergman's feet is on display at Salvatore Ferragamo Museum in Florence, Italy.\nAwards and nominations.\nIngrid Bergman became the second actress to win three Academy Awards for acting: two for Best Actress, and one for Best Supporting Actress. She is tied for second place in Oscars won with Walter Brennan (all three for Best Supporting Actor), Jack Nicholson (two for Best Actor, and one for Best Supporting Actor), Meryl Streep (two for Best Actress, and one for Best Supporting Actress), Frances McDormand (all three for Best Actress), and Daniel Day-Lewis (all three for Best Actor). Katharine Hepburn holds the record, with four (all for Best Actress).\nIn 1960, Bergman became the second actress to complete the American Triple Crown of Acting status, after winning an Emmy Award.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41563", "revid": "3492060", "url": "https://en.wikipedia.org/wiki?curid=41563", "title": "Polarential telegraph system", "text": "A polarential telegraph system is a direct-current telegraph system employing polar transmission in one direction and a form of differential duplex transmission in the other. \nTwo types of polarential systems, known as types A and B, are in use. In half-duplex operation of a type A polarential system, the direct-current balance is independent of line resistance. In half-duplex operation of a type B polarential system, the direct current is substantially independent of the line leakage. Type A is better for cable loops where leakage is negligible but resistance varies with temperature. Type B is considered better for open wire where variable line leakage is frequent.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41564", "revid": "7611264", "url": "https://en.wikipedia.org/wiki?curid=41564", "title": "Polarization (waves)", "text": "Property of waves that can oscillate with more than one orientation\n', or ', is a property of transverse waves which specifies the geometrical orientation of the oscillations. In a transverse wave, the direction of the oscillation is perpendicular to the direction of motion of the wave. One example of a polarized transverse wave is vibrations traveling along a taut string, for example, in a musical instrument like a guitar string. Depending on how the string is plucked, the vibrations can be in a vertical direction, horizontal direction, or at any angle perpendicular to the string. In contrast, in longitudinal waves, such as sound waves in a liquid or gas, the displacement of the particles in the oscillation is always in the direction of propagation, so these waves do not exhibit polarization. Transverse waves that exhibit polarization include electromagnetic waves such as light and radio waves, gravitational waves, and transverse sound waves (shear waves) in solids.\nAn electromagnetic wave such as light consists of a coupled oscillating electric field and magnetic field that are always perpendicular to each other. Different states of polarization correspond to different relationships between the directions of the fields and the direction of propagation. In linear polarization, the electric and magnetic fields each oscillate in a single direction, perpendicular to one another. In circular or elliptical polarization, the fields rotate around the beam's direction of travel at a constant rate. The rotation can be either in the right-hand or in the left-hand direction.\nLight or other electromagnetic radiation from many sources, such as the sun, flames, and incandescent lamps, consists of short wave trains with an equal mixture of polarizations; this is called \"unpolarized light\". Polarized light can be produced by passing unpolarized light through a polarizer, which allows waves of only one polarization to pass through. The most common optical materials do not affect the polarization of light, but some materials\u2014those that exhibit birefringence, dichroism, or optical activity\u2014affect light differently depending on its polarization. Some of these are used to make polarizing filters. Light also becomes partially polarized when it reflects at an angle from a surface.\nAccording to quantum mechanics, electromagnetic waves can also be viewed as streams of particles called photons. When viewed in this way, the polarization of an electromagnetic wave is determined by a quantum mechanical property of photons called their spin. A photon has one of two possible spins: it can either spin in a right hand sense or a left hand sense about its direction of travel. Circularly polarized electromagnetic waves are composed of photons with only one type of spin, either right- or left-hand. Linearly polarized waves consist of photons that are in a superposition of right and left circularly polarized states, with equal amplitude and phases synchronized to give oscillation in a plane.\nPolarization is an important parameter in areas of science dealing with transverse waves, such as optics, seismology, radio, and microwaves. Especially impacted are technologies such as lasers, wireless and optical fiber telecommunications, and radar.\nIntroduction.\nWave propagation and polarization.\nMost sources of light are classified as incoherent and unpolarized (or only \"partially polarized\") because they consist of a random mixture of waves having different spatial characteristics, frequencies (wavelengths), phases, and polarization states. However, for understanding electromagnetic waves and polarization in particular, it is easier to just consider coherent plane waves; these are sinusoidal waves of one particular direction (or wavevector), frequency, phase, and polarization state. Characterizing an optical system in relation to a plane wave with those given parameters can then be used to predict its response to a more general case, since a wave with any specified spatial structure can be decomposed into a combination of plane waves (its so-called angular spectrum). Incoherent states can be modeled stochastically as a weighted combination of such uncorrelated waves with some distribution of frequencies (its \"spectrum\"), phases, and polarizations.\nTransverse electromagnetic waves.\nElectromagnetic waves (such as light), traveling in free space or another homogeneous isotropic non-attenuating medium, are properly described as transverse waves, meaning that a plane wave's electric field vector E and magnetic field H are each in some direction perpendicular to (or \"transverse\" to) the direction of wave propagation; E and H are also perpendicular to each other. By convention, the \"polarization\" direction of an electromagnetic wave is given by its electric field vector. Considering a monochromatic plane wave of optical frequency f (light of vacuum wavelength \u03bb has a frequency of \"f\" = \"c/\u03bb\" where c is the speed of light), let us take the direction of propagation as the z axis. Being a transverse wave the E and H fields must then contain components only in the x and y directions whereas \"Ez\" = \"Hz\" = 0. Using complex (or phasor) notation, the instantaneous physical electric and magnetic fields are given by the real parts of the complex quantities occurring in the following equations. As a function of time t and spatial position z (since for a plane wave in the +\"z\" direction the fields have no dependence on x or y) these complex fields can be written as:\nformula_1\nand\nformula_2\nwhere \u03bb = \u03bb0/\"n\" is the wavelength in the medium (whose refractive index is n) and \"T\" = 1/\"f\" is the period of the wave. Here ex, ey, hx, and hy are complex numbers. In the second more compact form, as these equations are customarily expressed, these factors are described using the wavenumber \"k\" = 2\u03c0\"n\"/\"\u03bb\"0 and angular frequency (or \"radian frequency\") \"\u03c9\" = 2\u03c0\"f\". In a more general formulation with propagation not restricted to the \"+z\" direction, then the spatial dependence \"kz\" is replaced by \"k\" \u2219 \"r\" where is called the wave vector, the magnitude of which is the wavenumber.\nThus the leading vectors e and h each contain up to two nonzero (complex) components describing the amplitude and phase of the wave's x and y polarization components (again, there can be no z polarization component for a transverse wave in the +\"z\" direction). For a given medium with a characteristic impedance \u03b7, h is related to e by:\nformula_3\nIn a dielectric, \u03b7 is real and has the value \"\u03b7\"0/\"n\", where n is the refractive index and \"\u03b7\"0 is the impedance of free space. The impedance will be complex in a conducting medium. Note that given that relationship, the dot product of E and H must be zero:\nformula_4\nindicating that these vectors are orthogonal (at right angles to each other), as expected.\nKnowing the propagation direction (+\"z\" in this case) and \u03b7, one can just as well specify the wave in terms of just ex and ey describing the electric field. The vector containing ex and ey (but without the z component which is necessarily zero for a transverse wave) is known as a Jones vector. In addition to specifying the polarization state of the wave, a general Jones vector also specifies the overall magnitude and phase of that wave. Specifically, the intensity of the light wave is proportional to the sum of the squared magnitudes of the two electric field components:\nformula_5\nHowever, the wave's \"state of polarization\" is only dependent on the (complex) ratio of ey to ex. So let us just consider waves whose |\"ex\"|2 + |\"ey\"|2 = 1; this happens to correspond to an intensity of about in free space (where \"\u03b7\" = \"\u03b7\"0). And because the absolute phase of a wave is unimportant in discussing its polarization state, let us stipulate that the phase of ex is zero; in other words ex is a real number while ey may be complex. Under these restrictions, ex and ey can be represented as follows:\nformula_6\nwhere the polarization state is now fully parameterized by the value of Q (such that \u22121 &lt; \"Q\" &lt; 1) and the relative phase \u03d5.\nNon-transverse waves.\nIn addition to transverse waves, there are many wave motions where the oscillation is not limited to directions perpendicular to the direction of propagation. These cases are far beyond the scope of the current article, which concentrates on transverse waves (such as most electromagnetic waves in bulk media), but one should be aware of cases where the polarization of a coherent wave cannot be described simply using a Jones vector, as we have just done.\nJust considering electromagnetic waves, we note that the preceding discussion strictly applies to plane waves in a homogeneous isotropic non-attenuating medium, whereas in an anisotropic medium (such as birefringent crystals as discussed below) the electric or magnetic field may have longitudinal as well as transverse components. In those cases the electric displacement D and magnetic flux density B still obey the above geometry but due to anisotropy in the electric susceptibility (or in the magnetic permeability), now given by a tensor, the direction of E (or H) may differ from that of D (or B). Even in isotropic media, so-called inhomogeneous waves can be launched into a medium whose refractive index has a significant imaginary part (or \"extinction coefficient\") such as metals; these fields are also not strictly transverse. Surface waves or waves propagating in a waveguide (such as an optical fiber) are generally not transverse waves, but might be described as an electric or magnetic transverse mode, or a hybrid mode.\nEven in free space, longitudinal field components can be generated in focal regions, where the plane wave approximation breaks down. An extreme example is radially or tangentially polarized light, at the focus of which the electric or magnetic field respectively is entirely longitudinal (along the direction of propagation).\nFor longitudinal waves such as sound waves in fluids, the direction of oscillation is by definition along the direction of travel, so the issue of polarization is normally not even mentioned. On the other hand, sound waves in a bulk solid can be transverse as well as longitudinal, for a total of three polarization components. In this case, the transverse polarization is associated with the direction of the shear stress and displacement in directions perpendicular to the propagation direction, while the longitudinal polarization describes compression of the solid and vibration along the direction of propagation. The differential propagation of transverse and longitudinal polarizations is important in seismology.\nPolarization state.\nPolarization can be defined in terms of pure polarization states with only a coherent sinusoidal wave at one optical frequency. The vector in the adjacent diagram might describe the oscillation of the electric field emitted by a single-mode laser (whose oscillation frequency would be typically times faster). The field oscillates in the xy-plane, along the page, with the wave propagating in the z direction, perpendicular to the page.\nThe first two diagrams below trace the electric field vector over a complete cycle for linear polarization at two different orientations; these are each considered a distinct \"state of polarization\" (SOP). The linear polarization at 45\u00b0 can also be viewed as the addition of a horizontally linearly polarized wave (as in the leftmost figure) and a vertically polarized wave of the same amplitude in the same phase.\nNow if one were to introduce a phase shift in between those horizontal and vertical polarization components, one would generally obtain elliptical polarization as is shown in the third figure. When the phase shift is exactly \u00b190\u00b0, and the amplitudes are the same, then circular polarization is produced (fourth and fifth figures). Circular polarization can be created by sending linearly polarized light through a quarter-wave plate oriented at 45\u00b0 to the linear polarization to create two components of the same amplitude with the required phase shift. The superposition of the original and phase-shifted components causes a rotating electric field vector, which is depicted in the animation on the right. Note that circular or elliptical polarization can involve either a clockwise or counterclockwise rotation of the field, depending on the relative phases of the components. These correspond to distinct polarization states, such as the two circular polarizations shown above.\nThe orientation of the x and y axes used in this description is arbitrary. The choice of such a coordinate system and viewing the polarization ellipse in terms of the x and y polarization components, corresponds to the definition of the Jones vector (below) in terms of those basis polarizations. Axes are selected to suit a particular problem, such as x being in the plane of incidence. Since there are separate reflection coefficients for the linear polarizations in and orthogonal to the plane of incidence (\"p\" and \"s\" polarizations, see below), that choice greatly simplifies the calculation of a wave's reflection from a surface.\nAny pair of orthogonal polarization states may be used as basis functions, not just linear polarizations. For instance, choosing right and left circular polarizations as basis functions simplifies the solution of problems involving circular birefringence (optical activity) or circular dichroism.\nPolarization ellipse.\nFor a purely polarized monochromatic wave the electric field vector over one cycle of oscillation traces out an ellipse.\nA polarization state can then be described in relation to the geometrical parameters of the ellipse, and its \"handedness\", that is, whether the rotation around the ellipse is clockwise or counter clockwise. One parameterization of the elliptical figure specifies the orientation angle \u03c8, defined as the angle between the major axis of the ellipse and the x-axis along with the ellipticity \"\u03b5\" = \"a/b\", the ratio of the ellipse's major to minor axis. (also known as the axial ratio). The ellipticity parameter is an alternative parameterization of an ellipse's eccentricity formula_7 or the ellipticity angle, formula_8 formula_9 as is shown in the figure. The angle \u03c7 is also significant in that the latitude (angle from the equator) of the polarization state as represented on the Poincar\u00e9 sphere (see below) is equal to \u00b12\"\u03c7\". The special cases of linear and circular polarization correspond to an ellipticity \u03b5 of infinity and unity (or \u03c7 of zero and 45\u00b0) respectively.\nJones vector.\nFull information on a completely polarized state is also provided by the amplitude and phase of oscillations in two components of the electric field vector in the plane of polarization. This representation was used above to show how different states of polarization are possible. The amplitude and phase information can be conveniently represented as a two-dimensional complex vector (the Jones vector):\nformula_10\nHere \"a\"1 and \"a\"2 denote the amplitude of the wave in the two components of the electric field vector, while \"\u03b8\"1 and \"\u03b8\"2 represent the phases. The product of a Jones vector with a complex number of unit modulus gives a different Jones vector representing the same ellipse, and thus the same state of polarization. The physical electric field, as the real part of the Jones vector, would be altered but the polarization state itself is independent of absolute phase. The basis vectors used to represent the Jones vector need not represent linear polarization states (i.e. be real). In general any two orthogonal states can be used, where an orthogonal vector pair is formally defined as one having a zero inner product. A common choice is left and right circular polarizations, for example to model the different propagation of waves in two such components in circularly birefringent media (see below) or signal paths of coherent detectors sensitive to circular polarization.\nCoordinate frame.\nRegardless of whether polarization state is represented using geometric parameters or Jones vectors, implicit in the parameterization is the orientation of the coordinate frame. This permits a degree of freedom, namely rotation about the propagation direction. When considering light that is propagating parallel to the surface of the Earth, the terms \"horizontal\" and \"vertical\" polarization are often used, with the former being associated with the first component of the Jones vector, or zero azimuth angle. On the other hand, in astronomy the equatorial coordinate system is generally used instead, with the zero azimuth (or position angle, as it is more commonly called in astronomy to avoid confusion with the horizontal coordinate system) corresponding to due north.\n\"s\" and \"p\" designations.\nAnother coordinate system frequently used relates to the \"plane of incidence\". This is the plane made by the incoming propagation direction and the vector perpendicular to the plane of an interface, in other words, the plane in which the ray travels before and after reflection or refraction. The component of the electric field parallel to this plane is termed \"p-like\" (parallel) and the component perpendicular to this plane is termed \"s-like\" (from , German for 'perpendicular'). Polarized light with its electric field along the plane of incidence is thus denoted \"&lt;dfn &gt;p-polarized&lt;/dfn&gt;\", while light whose electric field is normal to the plane of incidence is called \"&lt;dfn &gt;s-polarized&lt;/dfn&gt;\". \"P\"-polarization is commonly referred to as \"transverse-magnetic\" (TM), and has also been termed \"pi-polarized\" or \"\u03c0-polarized\", or \"tangential plane polarized\". \"S\"-polarization is also called \"transverse-electric\" (TE), as well as \"sigma-polarized\" or \"\u03c3-polarized\", or \"sagittal plane polarized\".\nDegree of polarization.\nDegree of polarization (DOP) is a quantity used to describe the portion of an electromagnetic wave which is polarized. DOP can be calculated from the Stokes parameters. A perfectly polarized wave has a DOP of 100%, whereas an unpolarized wave has a DOP of 0%. A wave which is partially polarized, and therefore can be represented by a superposition of a polarized and unpolarized component, will have a DOP somewhere in between 0 and 100%. DOP is calculated as the fraction of the total power that is carried by the polarized component of the wave.\nDOP can be used to map the strain field in materials when considering the DOP of the photoluminescence. The polarization of the photoluminescence is related to the strain in a material by way of the given material's photoelasticity tensor.\nDOP is also visualized using the Poincar\u00e9 sphere representation of a polarized beam. In this representation, DOP is equal to the length of the vector measured from the center of the sphere.\nImplications for reflection and propagation.\nPolarization in wave propagation.\nIn a vacuum, the components of the electric field propagate at the speed of light, so that the phase of the wave varies in space and time while the polarization state does not. That is, the electric field vector e of a plane wave in the +\"z\" direction follows:\nformula_11\nwhere k is the wavenumber. As noted above, the instantaneous electric field is the real part of the product of the Jones vector times the phase factor formula_12. When an electromagnetic wave interacts with matter, its propagation is altered according to the material's (complex) index of refraction. When the real or imaginary part of that refractive index is dependent on the polarization state of a wave, properties known as birefringence and polarization dichroism (or diattenuation) respectively, then the polarization state of a wave will generally be altered.\nIn such media, an electromagnetic wave with any given state of polarization may be decomposed into two orthogonally polarized components that encounter different propagation constants. The effect of propagation over a given path on those two components is most easily characterized in the form of a complex transformation matrix J known as a Jones matrix:\nformula_13\nThe Jones matrix due to passage through a transparent material is dependent on the propagation distance as well as the birefringence. The birefringence (as well as the average refractive index) will generally be dispersive, that is, it will vary as a function of optical frequency (wavelength). In the case of non-birefringent materials, however, the Jones matrix is the identity matrix (multiplied by a scalar phase factor and attenuation factor), implying no change in polarization during propagation.\nFor propagation effects in two orthogonal modes, the Jones matrix can be written as\nformula_14\nwhere \"g\"1 and \"g\"2 are complex numbers describing the phase delay and possibly the amplitude attenuation due to propagation in each of the two polarization eigenmodes. T is a unitary matrix representing a change of basis from these propagation modes to the linear system used for the Jones vectors; in the case of linear birefringence or diattenuation the modes are themselves linear polarization states so T and T\u22121 can be omitted if the coordinate axes have been chosen appropriately.\nBirefringence.\nIn a birefringent substance, electromagnetic waves of different polarizations travel at different speeds (phase velocities). As a result, when unpolarized waves travel through a plate of birefringent material, one polarization component has a shorter wavelength than the other, resulting in a phase difference between the components which increases the further the waves travel through the material. The Jones matrix is a unitary matrix: |\"g\"1| = |\"g\"2| = 1. Media termed diattenuating (or \"dichroic\" in the sense of polarization), in which only the amplitudes of the two polarizations are affected differentially, may be described using a Hermitian matrix (generally multiplied by a common phase factor). In fact, since any matrix may be written as the product of unitary and positive Hermitian matrices, light propagation through any sequence of polarization-dependent optical components can be written as the product of these two basic types of transformations.\nIn birefringent media there is no attenuation, but two modes accrue a differential phase delay. Well known manifestations of linear birefringence (that is, in which the basis polarizations are orthogonal linear polarizations) appear in optical wave plates/retarders and many crystals. If linearly polarized light passes through a birefringent material, its state of polarization will generally change, unless its polarization direction is identical to one of those basis polarizations. Since the phase shift, and thus the change in polarization state, is usually wavelength-dependent, such objects viewed under white light in between two polarizers may give rise to colorful effects, as seen in the accompanying photograph.\nCircular birefringence is also termed optical activity, especially in chiral fluids, or Faraday rotation, when due to the presence of a magnetic field along the direction of propagation. When linearly polarized light is passed through such an object, it will exit still linearly polarized, but with the axis of polarization rotated. A combination of linear and circular birefringence will have as basis polarizations two orthogonal elliptical polarizations; however, the term \"elliptical birefringence\" is rarely used.\nOne can visualize the case of linear birefringence (with two orthogonal linear propagation modes) with an incoming wave linearly polarized at a 45\u00b0 angle to those modes. As a differential phase starts to accrue, the polarization becomes elliptical, eventually changing to purely circular polarization (90\u00b0 phase difference), then to elliptical and eventually linear polarization (180\u00b0 phase) perpendicular to the original polarization, then through circular again (270\u00b0 phase), then elliptical with the original azimuth angle, and finally back to the original linearly polarized state (360\u00b0 phase) where the cycle begins anew. In general the situation is more complicated and can be characterized as a rotation in the Poincar\u00e9 sphere about the axis defined by the propagation modes. Examples for linear (blue), circular (red), and elliptical (yellow) birefringence are shown in the figure on the left. The total intensity and degree of polarization are unaffected. If the path length in the birefringent medium is sufficient, the two polarization components of a collimated beam (or ray) can exit the material with a positional offset, even though their final propagation directions will be the same (assuming the entrance face and exit face are parallel). This is commonly viewed using calcite crystals, which present the viewer with two slightly offset images, in opposite polarizations, of an object behind the crystal. It was this effect that provided the first discovery of polarization, by Erasmus Bartholinus in 1669.\nDichroism.\nMedia in which transmission of one polarization mode is preferentially reduced are called \"dichroic\" or \"diattenuating\". Like birefringence, diattenuation can be with respect to linear polarization modes (in a crystal) or circular polarization modes (usually in a liquid).\nDevices that block nearly all of the radiation in one mode are known as \"&lt;dfn &gt;polarizing filters&lt;/dfn&gt;\" or simply \"polarizers\". This corresponds to \"g\"2 = 0 in the above representation of the Jones matrix. The output of an ideal polarizer is a specific polarization state (usually linear polarization) with an amplitude equal to the input wave's original amplitude in that polarization mode. Power in the other polarization mode is eliminated. Thus if unpolarized light is passed through an ideal polarizer (where \"g\"1 = 1 and \"g\"2 = 0) exactly half of its initial power is retained. Practical polarizers, especially inexpensive sheet polarizers, have additional loss so that \"g\"1 &lt; 1. However, in many instances the more relevant figure of merit is the polarizer's degree of polarization or extinction ratio, which involve a comparison of \"g\"1 to \"g\"2. Since Jones vectors refer to waves' amplitudes (rather than intensity), when illuminated by unpolarized light the remaining power in the unwanted polarization will be (\"g\"2/\"g\"1)2 of the power in the intended polarization.\nSpecular reflection.\nIn addition to birefringence and dichroism in extended media, polarization effects describable using Jones matrices can also occur at (reflective) interface between two materials of different refractive index. These effects are treated by the Fresnel equations. Part of the wave is transmitted and part is reflected; for a given material those proportions (and also the phase of reflection) are dependent on the angle of incidence and are different for the \"s\"- and \"p\"-polarizations. Therefore, the polarization state of reflected light (even if initially unpolarized) is generally changed.\nAny light striking a surface at a special angle of incidence known as Brewster's angle, where the reflection coefficient for \"p\"-polarization is zero, will be reflected with only the \"s\"-polarization remaining. This principle is employed in the so-called \"pile of plates polarizer\" (see figure) in which part of the \"s\"-polarization is removed by reflection at each Brewster angle surface, leaving only the \"p\"-polarization after transmission through many such surfaces. The generally smaller reflection coefficient of the \"p\"-polarization is also the basis of polarized sunglasses; by blocking the \"s\"- (horizontal) polarization, most of the glare due to reflection from a wet street, for instance, is removed.\nIn the important special case of reflection at normal incidence (not involving anisotropic materials) there is no particular \"s\"- or \"p\"-polarization. Both the x and y polarization components are reflected identically, and therefore the polarization of the reflected wave is identical to that of the incident wave. However, in the case of circular (or elliptical) polarization, the handedness of the polarization state is thereby reversed, since by convention this is specified relative to the direction of propagation. The circular rotation of the electric field around the \"x-y\" axes called \"right-handed\" for a wave in the +\"z\" direction is \"left-handed\" for a wave in the \u2212\"z\" direction. But in the general case of reflection at a nonzero angle of incidence, no such generalization can be made. For instance, right-circularly polarized light reflected from a dielectric surface at a grazing angle, will still be right-handed (but elliptically) polarized. Linear polarized light reflected from a metal at non-normal incidence will generally become elliptically polarized. These cases are handled using Jones vectors acted upon by the different Fresnel coefficients for the \"s\"- and \"p\"-polarization components.\nMeasurement techniques involving polarization.\nSome optical measurement techniques are based on polarization. In many other optical techniques polarization is crucial or at least must be taken into account and controlled; such examples are too numerous to mention.\nMeasurement of stress.\nIn engineering, the phenomenon of stress induced birefringence allows for stresses in transparent materials to be readily observed. As noted above and seen in the accompanying photograph, the chromaticity of birefringence typically creates colored patterns when viewed in between two polarizers. As external forces are applied, internal stress induced in the material is thereby observed. Additionally, birefringence is frequently observed due to stresses \"frozen in\" at the time of manufacture. This is famously observed in cellophane tape whose birefringence is due to the stretching of the material during the manufacturing process.\nEllipsometry.\nEllipsometry is a powerful technique for the measurement of the optical properties of a uniform surface. It involves measuring the polarization state of light following specular reflection from such a surface. This is typically done as a function of incidence angle or wavelength (or both). Since ellipsometry relies on reflection, it is not required for the sample to be transparent to light or for its back side to be accessible.\nEllipsometry can be used to model the (complex) refractive index of a surface of a bulk material. It is also very useful in determining parameters of one or more thin film layers deposited on a substrate. Due to their reflection properties, not only are the predicted magnitude of the \"p\" and \"s\" polarization components, but their relative phase shifts upon reflection, compared to measurements using an ellipsometer. A normal ellipsometer does not measure the actual reflection coefficient (which requires careful photometric calibration of the illuminating beam) but the ratio of the \"p\" and \"s\" reflections, as well as change of polarization ellipticity (hence the name) induced upon reflection by the surface being studied. In addition to use in science and research, ellipsometers are used in situ to control production processes for instance.\nGeology.\nThe property of (linear) birefringence is widespread in crystalline minerals, and indeed was pivotal in the initial discovery of polarization. In mineralogy, this property is frequently exploited using polarization microscopes, for the purpose of identifying minerals. See optical mineralogy for more details.\nSound waves in solid materials exhibit polarization. Differential propagation of the three polarizations through the earth is a crucial in the field of seismology. Horizontally and vertically polarized seismic waves (shear waves) are termed SH and SV, while waves with longitudinal polarization (compressional waves) are termed P-waves.\nAutopsy.\nSimilarly, polarization microscopes can be used to aid in the detection of foreign matter in biological tissue slices if it is birefringent; autopsies often mention\u00a0(a lack of or presence of) \"polarizable foreign debris.\"\nChemistry.\nWe have seen (above) that the birefringence of a type of crystal is useful in identifying it, and thus detection of linear birefringence is especially useful in geology and mineralogy. Linearly polarized light generally has its polarization state altered upon transmission through such a crystal, making it stand out when viewed in between two crossed polarizers, as seen in the photograph, above. Likewise, in chemistry, rotation of polarization axes in a liquid solution can be a useful measurement. In a liquid, linear birefringence is impossible, but there may be circular birefringence when a chiral molecule is in solution. When the right and left handed enantiomers of such a molecule are present in equal numbers (a so-called racemic mixture) then their effects cancel out. However, when there is only one (or a preponderance of one), as is more often the case for organic molecules, a net circular birefringence (or \"optical activity\") is observed, revealing the magnitude of that imbalance (or the concentration of the molecule itself, when it can be assumed that only one enantiomer is present). This is measured using a polarimeter in which polarized light is passed through a tube of the liquid, at the end of which is another polarizer which is rotated in order to null the transmission of light through it.\nAstronomy.\nIn many areas of astronomy, the study of polarized electromagnetic radiation from outer space is of great importance. Although not usually a factor in the thermal radiation of stars, polarization is also present in radiation from coherent astronomical sources (e.g. hydroxyl or methanol masers), and incoherent sources such as the large radio lobes in active galaxies, and pulsar radio radiation (which may, it is speculated, sometimes be coherent), and is also imposed upon starlight by scattering from interstellar dust. Apart from providing information on sources of radiation and scattering, polarization also probes the interstellar magnetic field via Faraday rotation. The polarization of the cosmic microwave background is being used to study the physics of the very early universe. Synchrotron radiation is inherently polarized. It has been suggested that astronomical sources caused the chirality of biological molecules on Earth, but chirality selection on inorganic crystals has been proposed as an alternative theory.\nApplications and examples.\nPolarized sunglasses.\nUnpolarized light, after being reflected by a specular (shiny) surface, generally obtains a degree of polarization. This phenomenon was observed in the early 1800s by the mathematician \u00c9tienne-Louis Malus, after whom Malus's law is named. Polarizing sunglasses exploit this effect to reduce glare from reflections by horizontal surfaces, notably the road ahead viewed at a grazing angle.\nWearers of polarized sunglasses will occasionally observe inadvertent polarization effects such as color-dependent birefringent effects, for example in toughened glass (e.g., car windows) or items made from transparent plastics, in conjunction with natural polarization by reflection or scattering. The polarized light from LCD monitors (see below) is extremely conspicuous when these are worn.\nSky polarization and photography.\nPolarization is observed in the light of the sky, as this is due to sunlight scattered by aerosols as it passes through Earth's atmosphere. The scattered light produces the brightness and color in clear skies. This partial polarization of scattered light can be used to darken the sky in photographs, increasing the contrast. This effect is most strongly observed at points on the sky making a 90\u00b0 angle to the Sun. Polarizing filters use these effects to optimize the results of photographing scenes in which reflection or scattering by the sky is involved.\nSky polarization has been used for orientation in navigation. The Pfund sky compass was used in the 1950s when navigating near the poles of the Earth's magnetic field when neither the sun nor stars were visible (e.g., under daytime cloud or twilight). It has been suggested, controversially, that the Vikings exploited a similar device (the \"sunstone\") in their extensive expeditions across the North Atlantic in the 9th\u201311th centuries, before the arrival of the magnetic compass from Asia to Europe in the 12th century. Related to the sky compass is the \"polar clock\", invented by Charles Wheatstone in the late 19th century.\nDisplay technologies.\nThe principle of liquid-crystal display (LCD) technology relies on the rotation of the axis of linear polarization by the liquid crystal array. Light from the backlight (or the back reflective layer, in devices not including or requiring a backlight) first passes through a linear polarizing sheet. That polarized light passes through the actual liquid crystal layer which may be organized in pixels (for a TV or computer monitor) or in another format such as a seven-segment display or one with custom symbols for a particular product. The liquid crystal layer is produced with a consistent right (or left) handed chirality, essentially consisting of tiny helices. This causes circular birefringence, and is engineered so that there is a 90 degree rotation of the linear polarization state. However, when a voltage is applied across a cell, the molecules straighten out, lessening or totally losing the circular birefringence. On the viewing side of the display is another linear polarizing sheet, usually oriented at 90 degrees from the one behind the active layer. Therefore, when the circular birefringence is removed by the application of a sufficient voltage, the polarization of the transmitted light remains at right angles to the front polarizer, and the pixel appears dark. With no voltage, however, the 90 degree rotation of the polarization causes it to exactly match the axis of the front polarizer, allowing the light through. Intermediate voltages create intermediate rotation of the polarization axis and the pixel has an intermediate intensity. Displays based on this principle are widespread, and now are used in the vast majority of televisions, computer monitors and video projectors, rendering the previous CRT technology essentially obsolete. The use of polarization in the operation of LCD displays is immediately apparent to someone wearing polarized sunglasses, often making the display unreadable.\nIn a totally different sense, polarization encoding has become the leading (but not sole) method for delivering separate images to the left and right eye in stereoscopic displays used for 3D movies. This involves separate images intended for each eye either projected from two different projectors with orthogonally oriented polarizing filters or, more typically, from a single projector with time multiplexed polarization (a fast alternating polarization device for successive frames). Polarized 3D glasses with suitable polarizing filters ensure that each eye receives only the intended image. Historically such systems used linear polarization encoding because it was inexpensive and offered good separation. However, circular polarization makes separation of the two images insensitive to tilting of the head, and is widely used in 3-D movie exhibition today, such as the system from RealD. Projecting such images requires screens that maintain the polarization of the projected light when viewed in reflection (such as silver screens); a normal diffuse white projection screen causes depolarization of the projected images, making it unsuitable for this application.\nAlthough now obsolete, CRT computer displays suffered from reflection by the glass envelope, causing glare from room lights and consequently poor contrast. Several anti-reflection solutions were employed to ameliorate this problem. One solution utilized the principle of reflection of circularly polarized light. A circular polarizing filter in front of the screen allows for the transmission of (say) only right circularly polarized room light. Now, right circularly polarized light (depending on the convention used) has its electric (and magnetic) field direction rotating clockwise while propagating in the +z direction. Upon reflection, the field still has the same direction of rotation, but now propagation is in the \u2212z direction making the reflected wave \"left\" circularly polarized. With the right circular polarization filter placed in front of the reflecting glass, the unwanted light reflected from the glass will thus be in very polarization state that is \"blocked\" by that filter, eliminating the reflection problem. The reversal of circular polarization on reflection and elimination of reflections in this manner can be easily observed by looking in a mirror while wearing 3-D movie glasses which employ left- and right-handed circular polarization in the two lenses. Closing one eye, the other eye will see a reflection in which it cannot see itself; that lens appears black. However, the other lens (of the closed eye) will have the correct circular polarization allowing the closed eye to be easily seen by the open one.\nRadio transmission and reception.\nAll radio (and microwave) antennas used for transmitting or receiving are intrinsically polarized. They transmit in (or receive signals from) a particular polarization, being totally insensitive to the opposite polarization; in certain cases that polarization is a function of direction. Most antennas are nominally linearly polarized, but elliptical and circular polarization is a possibility. In the case of linear polarization, the same kind of filtering as described above, is possible. In the case of elliptical polarization (circular polarization is in reality just a kind of elliptical polarization where the length of both elasticity factors is the same), filtering out a single angle (e.g. 90\u00b0) will have virtually no impact as the wave at any time can be in any of the 360 degrees.\nThe vast majority of antennas are linearly polarized. In fact it can be shown from considerations of symmetry that an antenna that lies entirely in a plane which also includes the observer, can \"only\" have its polarization in the direction of that plane. This applies to many cases, allowing one to easily infer such an antenna's polarization at an intended direction of propagation. So a typical rooftop Yagi or log-periodic antenna with horizontal conductors, as viewed from a second station toward the horizon, is necessarily horizontally polarized. But a vertical \"whip antenna\" or AM broadcast tower used as an antenna element (again, for observers horizontally displaced from it) will transmit in the vertical polarization. A turnstile antenna with its four arms in the horizontal plane, likewise transmits horizontally polarized radiation toward the horizon. However, when that same turnstile antenna is used in the \"axial mode\" (upwards, for the same horizontally-oriented structure) its radiation is circularly polarized. At intermediate elevations it is elliptically polarized.\nPolarization is important in radio communications because, for instance, if one attempts to use a horizontally polarized antenna to receive a vertically polarized transmission, the signal strength will be substantially reduced (or under very controlled conditions, reduced to nothing). This principle is used in satellite television in order to double the channel capacity over a fixed frequency band. The same frequency channel can be used for two signals broadcast in opposite polarizations. By adjusting the receiving antenna for one or the other polarization, either signal can be selected without interference from the other.\nEspecially due to the presence of the ground, there are some differences in propagation (and also in reflections responsible for TV ghosting) between horizontal and vertical polarizations. AM and FM broadcast radio usually use vertical polarization, while television uses horizontal polarization. At low frequencies especially, horizontal polarization is avoided. That is because the phase of a horizontally polarized wave is reversed upon reflection by the ground. A distant station in the horizontal direction will receive both the direct and reflected wave, which thus tend to cancel each other. This problem is avoided with vertical polarization. Polarization is also important in the transmission of radar pulses and reception of radar reflections by the same or a different antenna. For instance, back scattering of radar pulses by rain drops can be avoided by using circular polarization. Just as specular reflection of circularly polarized light reverses the handedness of the polarization, as discussed above, the same principle applies to scattering by objects much smaller than a wavelength such as rain drops. On the other hand, reflection of that wave by an irregular metal object (such as an airplane) will typically introduce a change in polarization and (partial) reception of the return wave by the same antenna.\nThe effect of free electrons in the ionosphere, in conjunction with the earth's magnetic field, causes Faraday rotation, a sort of circular birefringence. This is the same mechanism which can rotate the axis of linear polarization by electrons in interstellar space as mentioned below. The magnitude of Faraday rotation caused by such a plasma is greatly exaggerated at lower frequencies, so at the higher microwave frequencies used by satellites the effect is minimal. However, medium or short wave transmissions received following refraction by the ionosphere are strongly affected. Since a wave's path through the ionosphere and the earth's magnetic field vector along such a path are rather unpredictable, a wave transmitted with vertical (or horizontal) polarization will generally have a resulting polarization in an arbitrary orientation at the receiver.\nPolarization and vision.\nMany animals are capable of perceiving some of the components of the polarization of light, e.g., linear horizontally polarized light. This is generally used for navigational purposes, since the linear polarization of sky light is always perpendicular to the direction of the sun. This ability is very common among the insects, including bees, which use this information to orient their communicative dances. Polarization sensitivity has also been observed in species of octopus, squid, cuttlefish, and mantis shrimp. In the latter case, one species measures all six orthogonal components of polarization, and is believed to have optimal polarization vision. The rapidly changing, vividly colored skin patterns of cuttlefish, used for communication, also incorporate polarization patterns, and mantis shrimp are known to have polarization selective reflective tissue. Sky polarization was thought to be perceived by pigeons, which was assumed to be one of their aids in homing, but research indicates this is a popular myth.\nThe naked human eye is weakly sensitive to polarization, without the need for intervening filters. Polarized light creates a very faint pattern near the center of the visual field, called Haidinger's brush. This pattern is very difficult to see, but with practice one can learn to detect polarized light with the naked eye.\nAngular momentum using circular polarization.\nIt is well known that electromagnetic radiation carries a certain linear momentum in the direction of propagation. In addition, however, light carries a certain angular momentum if it is circularly polarized (or partially so). In comparison with lower frequencies such as microwaves, the amount of angular momentum in light, even of pure circular polarization, compared to the same wave's linear momentum (or radiation pressure) is very small and difficult to even measure. However, it was utilized in an experiment to achieve speeds of up to 600 million revolutions per minute.\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nQuantum Physics\nOptics\nReferences.\nCited references.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nGeneral references.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "41565", "revid": "252195", "url": "https://en.wikipedia.org/wiki?curid=41565", "title": "Polarization-maintaining optical fiber", "text": "Single-mode optical fiber for linearly polarized light\nIn fiber optics, polarization-maintaining optical fiber (PMF or PM fiber) is a single-mode optical fiber in which linearly polarized light, if properly launched into the fiber, maintains a linear polarization during propagation, exiting the fiber in a specific linear polarization state; there is little or no cross-coupling of optical power between the two polarization modes. Such fiber is used in special applications where preserving polarization is essential.\nPolarization crosstalk.\nIn an ordinary (non-polarization-maintaining) fiber, different polarization modes have the same nominal phase velocity due to the fiber's circular symmetry. Stress induced birefringence in such a fiber, or bending of the fiber, will cause a tiny amount of crosstalk between different modes. Over the length of the fiber this tiny coupling between modes transfers significant amounts of power between them, completely changing the wave's net state of polarization. Polarization changes due to stress in a fiber vary randomly as the stresses change, and also vary with the temperature of the fiber and the wavelength of light.\nPrinciple of operation.\nPolarization-maintaining fibers work by \"intentionally\" introducing a systematic linear birefringence in the fiber, so that there are two well defined polarization modes which propagate along the fiber with very distinct phase velocities. The beat length Lb of such a fiber (for a particular wavelength) is the distance (typically a few millimeters) over which the wave in one mode will experience an additional delay of one wavelength compared to the other polarization mode. Thus a length Lb /2 of such fiber is equivalent to a half-wave plate. Now consider that there might be a random coupling between the two polarization states over a significant length of such fiber. At point 0 along the fiber, the wave in polarization mode 1 induces an amplitude into mode 2 at some phase. However at point 1/2 Lb along the fiber, the same coupling coefficient between the polarization modes induces an amplitude into mode 2 which is now 180 degrees \"out of phase\" with the wave coupled at point zero, leading to cancellation. At point Lb along the fiber the coupling is again in the original phase, but at 3/2 Lb it is again out of phase and so on. The possibility of coherent addition of wave amplitudes through crosstalk over distances much larger than Lb is thus eliminated. Most of the wave's power remains in the original polarization mode, and exits the fiber in that mode's polarization as it is oriented at the fiber end. Optical fiber connectors used for PM fibers are specially keyed so that the two polarization modes are aligned and exit in a specific orientation.\nNote that a polarization-maintaining fiber does not polarize light as a polarizer does. Rather, PM fiber maintains the linear polarization of linearly polarized light provided that it is launched into the fiber aligned with one of the fiber's polarization modes. Launching linearly polarized light into the fiber at a different angle will excite both polarization modes, conducting the same wave at slightly different phase velocities. At most points along the fiber the net polarization will be an elliptically polarized state, with a return to the original polarization state after an integer number of beat lengths. Consequently, if visible laser light is launched into the fiber exciting both polarization modes, scattering of propagating light viewed from the side, is observed with a light and dark pattern periodic over each beat length, since scattering is preferentially perpendicular to the polarization direction.\nDesigns.\nSeveral different designs are used to create birefringence in a fiber. The fiber may be geometrically asymmetric or have a refractive index profile which is asymmetric such as the design using an elliptical cladding as shown in the diagram. Alternatively, stress permanently induced in the fiber will produce stress birefringence; this may be accomplished using rods of another material included within the cladding. Several different shapes of rod are used, and the resulting fiber is sold under brand names such as \"PANDA\" and \"Bow-tie\". (\"PANDA\" refers to the resemblance of the fiber's cross-section to the face of a panda, and is also an acronym for \"Polarization-maintaining AND Absorption-reducing\".) \nIt is possible to create a circularly birefringent optical fiber just using an ordinary (circularly symmetric) single-mode fiber and twisting it, thus creating internal torsional stress. That causes the phase velocity of right and left hand circular polarizations to significantly differ. Thus the two circular polarizations propagate with little crosstalk in between them\nApplications.\nPolarization-maintaining optical fibers are used in special applications, such as in fiber optic sensing, interferometry and quantum key distribution. They are also commonly used in telecommunications for the connection between a source laser and a modulator, since the modulator requires polarized light as input. They are rarely used for long-distance transmission, because PM fiber is expensive and has higher attenuation than single-mode fiber. Another important application is fiber-optic gyroscopes, which are widely used in the aerospace industry. \nThe output of a PM fiber is typically characterized by its polarization extinction ratio (PER)\u2014the ratio of correctly to incorrectly polarized light, expressed in decibels. The quality of PM patchcords and pigtails can be characterized with a PER meter. Good PM fibers have extinction ratios in excess of 20 dB."}
{"id": "41566", "revid": "32860082", "url": "https://en.wikipedia.org/wiki?curid=41566", "title": "Polling, M\u00fchldorf", "text": "Polling () is a municipality in the district of M\u00fchldorf in Bavaria in Germany. It lies on the river Inn.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41567", "revid": "1030826", "url": "https://en.wikipedia.org/wiki?curid=41567", "title": "Power budget", "text": ""}
{"id": "41568", "revid": "545027", "url": "https://en.wikipedia.org/wiki?curid=41568", "title": "Power factor", "text": "Ratio of active power to apparent power\nIn electrical engineering, the power factor of an AC power system is defined as the ratio of the \"real power\" absorbed by the load to the \"apparent power\" flowing in the circuit. Real power is the average of the instantaneous product of voltage and current and represents the capacity of the electricity for performing work. Apparent power is the product of root mean square (RMS) current and voltage. Apparent power is often higher than real power because energy is cyclically accumulated in the load and returned to the source or because a non-linear load distorts the wave shape of the current. Where apparent power exceeds real power, more current is flowing in the circuit than would be required to transfer real power. Where the power factor magnitude is less than one, the voltage and current are not in phase, which reduces the average product of the two. A negative power factor occurs when the device (normally the load) generates real power, which then flows back towards the source.\nIn an electric power system, a load with a low power factor draws more current than a load with a high power factor for the same amount of useful power transferred. The larger currents increase the energy lost in the distribution system and require larger wires and other equipment. Because of the costs of larger equipment and wasted energy, electrical utilities will usually charge a higher cost to industrial or commercial customers with a low power factor.\nPower-factor correction (PFC) increases the power factor of a load, improving efficiency for the distribution system to which it is attached. Linear loads with a low power factor (such as induction motors) can be corrected with a passive network of capacitors or inductors. Non-linear loads, such as rectifiers, distort the current drawn from the system. In such cases, active or passive power factor correction may be used to counteract the distortion and raise the power factor. The devices for correction of the power factor may be at a central substation, spread out over a distribution system, or built into power-consuming equipment.\nGeneral case.\nThe general expression for power factor is given by\nformula_1\nformula_2\nwhere formula_3 is the real power measured by an ideal wattmeter, formula_4 is the rms current measured by an ideal ammeter, and formula_5 is the rms voltage measured by an ideal voltmeter. Apparent power, formula_6, is the product of the rms current and the rms voltage.\nIf the load is sourcing power back toward the generator, then formula_3 and formula_8 will be negative.\nIf the waveforms are periodic with the same fundamental period, then the power factor can be computed as follows:\nformula_9\nwhere formula_10 is the instantaneous current, formula_11 is the instantaneous voltage, formula_12 is an arbitrary starting time, and formula_13 is the period of the waveforms.\nIf the waveforms are not periodic and the physical meters have the same averaging time, then the equations for the periodic case can be used with the exception that formula_13 is the averaging time of the meters instead of the waveform period.\nLinear circuits.\nIn a linear circuit, consisting of combinations of resistors, inductors, and capacitors, current flow has a sinusoidal response to the sinusoidal line voltage. A linear load does not change the shape of the input waveform but may change the relative timing (phase) between voltage and current, due to its inductance or capacitance.\nIn a purely resistive AC circuit, voltage and current waveforms are in step (or in phase), changing polarity at the same instant in each cycle. All the power entering the load is consumed (or dissipated).\nWhere reactive loads are present, such as with capacitors or inductors, energy storage in the loads results in a phase difference between the current and voltage waveforms. During each cycle of the AC voltage, extra energy, in addition to any energy consumed in the load, is temporarily stored in the load in electric or magnetic fields then returned to the power grid a fraction of the period later.\nElectrical circuits containing predominantly resistive loads (incandescent lamps, devices using heating elements like electric toasters and ovens) have a power factor of almost 1, but circuits containing inductive or capacitive loads (electric motors, solenoid valves, transformers, fluorescent lamp ballasts, and others) can have a power factor well below 1.\nA circuit with a low power factor will use a greater amount of current to transfer a given quantity of real power than a circuit with a high power factor thus causing increased losses due to resistive heating in power lines, and requiring the use of higher-rated conductors and transformers.\nDefinition and calculation.\nAC power has two components:\nTogether, they form the complex power (formula_17) expressed as volt-amperes (VA). The magnitude of the complex power is the apparent power (formula_18), also expressed in volt-amperes (VA).\nThe VA and var are non-SI units dimensionally similar to the watt but are used in engineering practice instead of the watt to state what quantity is being expressed. The SI explicitly disallows using units for this purpose or as the only source of information about a physical quantity as used.\nThe power factor is defined as the ratio of real power to apparent power. As power is transferred along a transmission line, it does not consist purely of real power that can do work once transferred to the load, but rather consists of a combination of real and reactive power, called apparent power. The power factor describes the amount of real power transmitted along a transmission line relative to the total apparent power flowing in the line.\nThe power factor can also be computed as the cosine of the angle \u03b8 by which the current waveform lags or leads the voltage waveform.\nPower triangle.\nOne can relate the various components of AC power by using the power triangle in vector space. Real power extends horizontally in the real axis and reactive power extends in the direction of the imaginary axis. Complex power (and its magnitude, apparent power) represents a combination of both real and reactive power, and therefore can be calculated by using the vector sum of these two components. We can conclude that the mathematical relationship between these components is:\nformula_19\nAs the angle \u03b8 increases with fixed total apparent power, current and voltage are further out of phase with each other. Real power decreases, and reactive power increases.\nLagging, leading and unity power factors.\nPower factor is described as \"leading\" if the current waveform is advanced in phase concerning voltage, or \"lagging\" when the current waveform is behind the voltage waveform. A lagging power factor signifies that the load is inductive, as the load will \"consume\" reactive power. The reactive component formula_16 is positive as reactive power travels through the circuit and is \"consumed\" by the inductive load. A leading power factor signifies that the load is capacitive, as the load \"supplies\" reactive power, and therefore the reactive component formula_16 is negative as reactive power is being supplied to the circuit.\nIf \u03b8 is the phase angle between the current and voltage, then the power factor is equal to the cosine of the angle, formula_22:\nformula_23\nSince the units are consistent, the power factor is by definition a dimensionless number between -1 and 1. When the power factor is equal to 0, the energy flow is entirely reactive, and stored energy in the load returns to the source on each cycle. When the power factor is 1, referred to as the \"unity\" power factor, all the energy supplied by the source is consumed by the load. Power factors are usually stated as \"leading\" or \"lagging\" to show the sign of the phase angle. Capacitive loads are leading (current leads voltage), and inductive loads are lagging (current lags voltage).\nIf a purely resistive load is connected to a power supply, current and voltage will change polarity in step, the power factor will be 1, and the electrical energy flows in a single direction across the network in each cycle. Inductive loads such as induction motors (any type of wound coil) consume reactive power with the current waveform lagging the voltage. Capacitive loads such as capacitor banks or buried cables generate reactive power with the current phase leading the voltage. Both types of loads will absorb energy during part of the AC cycle, which is stored in the device's magnetic or electric field, only to return this energy back to the source during the rest of the cycle.\nFor example, to get 1\u00a0kW of real power, if the power factor is unity, 1\u00a0kVA of apparent power needs to be transferred (1\u00a0kW \u00f7 1 = 1\u00a0kVA). At low values of power factor, more apparent power needs to be transferred to get the same real power. To get 1\u00a0kW of real power at 0.2 power factor, 5\u00a0kVA of apparent power needs to be transferred (1\u00a0kW \u00f7 0.2 = 5\u00a0kVA). This apparent power must be produced and transmitted to the load and is subject to losses in the production and transmission processes.\nElectrical loads consuming alternating current power consume both real power and reactive power. The vector sum of real and reactive power is the complex power, and its magnitude is the apparent power. The presence of reactive power causes the real power to be less than the apparent power, and so, the electric load has a power factor of less than 1.\nA negative power factor (0 to \u22121) can result from returning active power to the source, such as in the case of a building fitted with solar panels when surplus power is fed back into the supply.\nPower factor correction of linear loads.\nA high power factor is generally desirable in a power delivery system to reduce losses and improve voltage regulation at the load. Compensating elements near an electrical load will reduce the apparent power demand on the supply system. Power factor correction may be applied by an electric power transmission utility to improve the stability and efficiency of the network. Individual electrical customers who are charged by their utility for low power factor may install correction equipment to increase their power factor to reduce costs.\nPower factor correction brings the power factor of an AC power circuit closer to 1 by supplying or absorbing reactive power, adding capacitors or inductors that act to cancel the inductive or capacitive effects of the load, respectively. In the case of offsetting the inductive effect of motor loads, capacitors can be locally connected. These capacitors help to generate reactive power to meet the demand of the inductive loads. This will keep that reactive power from having to flow from the utility generator to the load. In the electricity industry, inductors are said to consume reactive power, and capacitors are said to supply it, even though reactive power is just energy moving back and forth on each AC cycle.\nThe reactive elements in power factor correction devices can create voltage fluctuations and harmonic noise when switched on or off. They will supply or sink reactive power regardless of whether there is a corresponding load operating nearby, increasing the system's no-load losses. In the worst case, reactive elements can interact with the system and with each other to create resonant conditions, resulting in system instability and severe overvoltage fluctuations. As such, reactive elements cannot simply be applied without engineering analysis.\nAn automatic power factor correction unit consists of some capacitors that are switched by means of contactors. These contactors are controlled by a regulator that measures power factor in an electrical network. Depending on the load and power factor of the network, the power factor controller will switch the necessary blocks of capacitors in steps to make sure the power factor stays above a selected value.\nIn place of a set of switched capacitors, an unloaded synchronous motor can supply reactive power. The reactive power drawn by the synchronous motor is a function of its field excitation. It is referred to as a synchronous condenser. It is started and connected to the electrical network. It operates at a leading power factor and puts vars onto the network as required to support a system's voltage or to maintain the system power factor at a specified level.\nThe synchronous condenser's installation and operation are identical to those of large electric motors. Its principal advantage is the ease with which the amount of correction can be adjusted; it behaves like a variable capacitor. Unlike with capacitors, the amount of reactive power furnished is proportional to voltage, not the square of voltage; this improves voltage stability on large networks. Synchronous condensers are often used in connection with high-voltage direct-current transmission projects or in large industrial plants such as steel mills.\nFor power factor correction of high-voltage power systems or large, fluctuating industrial loads, power electronic devices such as the static VAR compensator or STATCOM are increasingly used. These systems are able to compensate sudden changes of power factor much more rapidly than contactor-switched capacitor banks and, being solid-state, require less maintenance than synchronous condensers.\nNon-linear loads.\nExamples of non-linear loads on a power system are rectifiers (such as used in a power supply), and arc discharge devices such as fluorescent lamps, electric welding machines, or arc furnaces. Because current in these systems is interrupted by a switching action, the current contains frequency components that are multiples of the power system frequency. \"Distortion power factor\" is a measure of how much the harmonic distortion of a load current decreases the average power transferred to the load.\nNon-sinusoidal components.\nIn linear circuits having only sinusoidal currents and voltages of one frequency, the power factor arises only from the difference in phase between the current and voltage. This is \"displacement power factor\".\nNon-linear loads change the shape of the current waveform from a sine wave to some other form. Non-linear loads create harmonic currents in addition to the original (fundamental frequency) AC current. This is of importance in practical power systems that contain non-linear loads such as rectifiers, some forms of electric lighting, electric arc furnaces, welding equipment, switched-mode power supplies, variable speed drives and other devices. Filters consisting of linear capacitors and inductors can prevent harmonic currents from entering the supplying system.\nTo measure the real power or reactive power, a wattmeter designed to work properly with non-sinusoidal currents must be used.\nDistortion power factor.\nThe distortion power factor is the distortion component associated with the harmonic voltages and currents present in the system.\nformula_24\nformula_25 is the total harmonic distortion of the load current. \nformula_26\nformula_27 is the fundamental component of the current, formula_4 is the total current, and formula_29 is the current on the hth harmonic; all are root mean square values (distortion power factor can also be used to describe individual order harmonics, using the corresponding current in place of total current). This definition with respect to total harmonic distortion assumes that the voltage stays undistorted (sinusoidal, without harmonics). This simplification is often a good approximation for stiff voltage sources (not being affected by changes in load downstream in the distribution network). Total harmonic distortion of typical generators from current distortion in the network is on the order of 1\u20132%, which can have larger scale implications but can be ignored in common practice.\nThe result when multiplied with the displacement power factor is the overall, true power factor or just power factor (PF):\nformula_30\nDistortion in three-phase networks.\nIn practice, the local effects of distortion current on devices in a three-phase distribution network rely on the magnitude of certain order harmonics rather than the total harmonic distortion.\nFor example, the triplen, or zero-sequence, harmonics (3rd, 9th, 15th, etc.) have the property of being in-phase when compared line-to-line. In a delta-wye transformer, these harmonics can result in circulating currents in the delta windings and result in greater resistive heating. In a wye-configuration of a transformer, triplen harmonics will not create these currents, but they will result in a non-zero current in the neutral wire. This could overload the neutral wire in some cases\nand create error in kilowatt-hour metering systems and billing revenue. The presence of current harmonics in a transformer also result in larger eddy currents in the magnetic core of the transformer. Eddy current losses generally increase as the square of the frequency, lowering the transformer's efficiency, dissipating additional heat, and reducing its service life.\nNegative-sequence harmonics (5th, 11th, 17th, etc.) combine 120 degrees out of phase, similarly to the fundamental harmonic but in a reversed sequence. In generators and motors, these currents produce magnetic fields which oppose the rotation of the shaft and sometimes result in damaging mechanical vibrations.\nCorrection of non-linear loads.\nThe simplest way to control the harmonic current is to use a filter that passes current only at line frequency (50 or 60\u00a0Hz). The filter consists of capacitors or inductors and makes a non-linear device look more like a linear load. An example of passive PFC is a valley-fill circuit. A disadvantage of passive PFC is that it requires larger inductors or capacitors than an equivalent power active PFC circuit. Also, in practice, passive PFC is often less effective at improving the power factor.\nActive PFC is the use of power electronics to change the waveform of current drawn by a load to improve the power factor. Some types of the active PFC are buck, boost, buck\u2013boost and synchronous condenser. Active power factor correction can be single-stage or multi-stage. In the case of a switched-mode power supply, a boost converter is inserted between the bridge rectifier and the main input capacitors. The boost converter attempts to maintain a constant voltage at its output while drawing a current that is always in phase with and at the same frequency as the line voltage. Another switched-mode converter inside the power supply produces the desired output voltage from the DC bus. This approach requires additional semiconductor switches and control electronics but permits cheaper and smaller passive components. It is frequently used in practice. For a three-phase SMPS, the Vienna rectifier configuration may be used to substantially improve the power factor. SMPSs with passive PFC can achieve power factor of about 0.7\u20130.75, SMPSs with active PFC, up to 0.99 power factor, while a SMPS without any power factor correction have a power factor of only about 0.55\u20130.65. Due to their very wide input voltage range, many power supplies with active PFC can automatically adjust to operate on AC power from about 100\u00a0V (Japan) to 240\u00a0V (Europe).\nDynamic power factor correction (DPFC), sometimes referred to as real-time power factor correction, is used for electrical stabilization in cases of rapid load changes (e.g. at large manufacturing sites). DPFC is useful when standard power factor correction would cause over or under correction. DPFC uses semiconductor switches, typically thyristors, to quickly connect and disconnect capacitors or inductors to improve power factor.\nImportance in distribution systems.\nPower factors below 1.0 require a utility to generate more than the minimum volt-amperes necessary to supply the real power (watts). This increases generation and transmission costs. For example, if the load power factor were as low as 0.7, the apparent power would be 1.4 times the real power used by the load. Line current in the circuit would also be 1.4 times the current required at 1.0 power factor, so the losses in the circuit would be doubled (since they are proportional to the square of the current). Alternatively, all components of the system such as generators, conductors, transformers, and switchgear would be increased in size (and cost) to carry the extra current. When the power factor is close to unity, for the same kVA rating of the transformer more load current can be supplied.\nUtilities typically charge additional costs to commercial customers who have a power factor below some limit, which is typically 0.9 to 0.95. Engineers are often interested in the power factor of a load as one of the factors that affect the efficiency of power transmission.\nWith the rising cost of energy and concerns over the efficient delivery of power, active PFC has become more common in consumer electronics. Current Energy Star guidelines for computers call for a power factor of \u2265 0.9 at 100% of rated output in the PC's power supply. According to a white paper authored by Intel and the U.S. Environmental Protection Agency, PCs with internal power supplies will require the use of active power factor correction to meet the ENERGY STAR 5.0 Program Requirements for Computers.\nIn Europe, EN 61000-3-2 requires power factor correction be incorporated into consumer products.\nSmall customers, such as households, are not usually charged for reactive power and so power factor metering equipment for such customers will not be installed.\nMeasurement techniques.\nThe power factor in a single-phase circuit (or balanced three-phase circuit) can be measured with the wattmeter-ammeter-voltmeter method, where the power in watts is divided by the product of measured voltage and current. The power factor of a balanced polyphase circuit is the same as that of any phase. The power factor of an unbalanced polyphase circuit is not uniquely defined.\nA direct reading power factor meter can be made with a moving coil meter of the electrodynamic type, carrying two perpendicular coils on the moving part of the instrument. The field of the instrument is energized by the circuit current flow. The two moving coils, A and B, are connected in parallel with the circuit load. One coil, A, will be connected through a resistor and the second coil, B, through an inductor, so that the current in coil B is delayed with respect to current in A. At unity power factor, the current in A is in phase with the circuit current, and coil A provides maximum torque, driving the instrument pointer toward the 1.0 mark on the scale. At zero power factor, the current in coil B is in phase with circuit current, and coil B provides torque to drive the pointer towards 0. At intermediate values of power factor, the torques provided by the two coils add and the pointer takes up intermediate positions.\nAnother electromechanical instrument is the polarized-vane type. In this instrument a stationary field coil produces a rotating magnetic field, just like a polyphase motor. The field coils are connected either directly to polyphase voltage sources or to a phase-shifting reactor if a single-phase application. A second stationary field coil, perpendicular to the voltage coils, carries a current proportional to current in one phase of the circuit. The moving system of the instrument consists of two vanes that are magnetized by the current coil. In operation, the moving vanes take up a physical angle equivalent to the electrical angle between the voltage source and the current source. This type of instrument can be made to register for currents in both directions, giving a four-quadrant display of power factor or phase angle.\nDigital instruments exist that directly measure the time lag between voltage and current waveforms. Low-cost instruments of this type measure the peak of the waveforms. More sophisticated versions measure the peak of the fundamental harmonic only, thus giving a more accurate reading for phase angle on distorted waveforms. Calculating power factor from voltage and current phases is only accurate if both waveforms are sinusoidal.\nPower Quality Analyzers, often referred to as Power Analyzers, make a digital recording of the voltage and current waveform (typically either one phase or three phase) and accurately calculate true power (watts), apparent power (VA) power factor, AC voltage, AC current, DC voltage, DC current, frequency, IEC61000-3-2/3-12 Harmonic measurement, IEC61000-3-3/3-11 flicker measurement, individual phase voltages in delta applications where there is no neutral line, total harmonic distortion, phase and amplitude of individual voltage or current harmonics, etc.\nMnemonics.\nAnglophone power engineering students are advised to remember:\n\"ELI the ICE man\" or \"ELI on ICE\" \u2013 the voltage E, leads the current I, in an inductor L. The current I leads the voltage E in a capacitor C.\nAnother common mnemonic is CIVIL \u2013 in a capacitor (C) the current (I) leads voltage (V), voltage (V) leads current (I) in an inductor (L).\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41570", "revid": "4904587", "url": "https://en.wikipedia.org/wiki?curid=41570", "title": "Power-law index profile", "text": "Index of refraction profile in fiber optics\nFor optical fibers, a power-law index profile is an index of refraction profile characterized by \nformula_1\nwhere\nformula_2\nand formula_3 is the nominal refractive index as a function of distance from the fiber axis, formula_4 is the nominal refractive index on axis, formula_5 is the refractive index of the cladding, which is taken to be homogeneous (formula_6), formula_7 is the core radius, and formula_8 is a parameter that defines the shape of the profile. formula_7 is often used in place of formula_8. Hence, this is sometimes called an alpha profile. \nFor this class of profiles, multimode distortion is smallest when formula_8 takes a particular value depending on the material used. For most materials, this optimum value is approximately 2. In the limit of infinite formula_8, the profile becomes a step-index profile."}
{"id": "41571", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41571", "title": "Power margin", "text": "Power difference\nIn telecommunications, the power margin is the difference between available signal power and the minimum signal power needed to overcome system losses and still satisfy the minimum input requirements of the receiver for a given performance level. \nSystem power margin reflects the excess signal level, present at the input of the receiver, that is available to compensate for (a) the effects of component aging in the transmitter, receiver, or physical transmission medium, and (b) a deterioration in propagation conditions. \"Synonym\" system power margin.\nReferences.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41572", "revid": "44062", "url": "https://en.wikipedia.org/wiki?curid=41572", "title": "Precision", "text": "Precision, precise or precisely may refer to:\n&lt;templatestyles src=\"Template:TOC_right/styles.css\" /&gt;\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "41573", "revid": "41195652", "url": "https://en.wikipedia.org/wiki?curid=41573", "title": "Pre-emphasis", "text": ""}
{"id": "41574", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41574", "title": "Preemphasis improvement", "text": "Signal enhancement\nIn FM broadcasting, preemphasis improvement is the improvement in the signal-to-noise ratio of the high-frequency portion of the baseband, \"i.e.,\" modulating signal, which improvement results from passing the modulating signal through a preemphasis network before transmission.\nThe reason that preemphasis is needed is that the process of detecting a frequency-modulated signal in a receiver produces a noise spectrum that rises in frequency (a so-called \"triangular\" spectrum). Without preemphasis, the received audio would sound unacceptably noisy at high frequencies, especially under conditions of low carrier-to-noise ratio, i.e., during fringe reception conditions. Preemphasis increases the magnitude of the higher signal frequencies, thereby improving the signal-to-noise ratio. At the output of the discriminator in the FM receiver, a deemphasis network restores the original signal power distribution.\n\"FM improvement factor\" is the quotient obtained by dividing the signal-to-noise ratio (SNR) at the output of an FM receiver by the carrier-to-noise ratio (CNR) at the input of the receiver. When the FM improvement factor is greater than unity, the improvement in the SNR is always obtained at the expense of an increased bandwidth in the receiver and the transmission path.\n\"FM improvement threshold\" is the point in an FM (frequency modulation) receiver at which the peaks in the RF signal equal the peaks of the thermal noise generated in the receiver. A baseband signal-to-noise ratio of about 30 dB is typical at the improvement threshold, and this ratio improves 1\u00a0dB for each decibel of increase in the signal above the threshold.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41575", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41575", "title": "Pre-emphasis network", "text": ""}
{"id": "41576", "revid": "8066546", "url": "https://en.wikipedia.org/wiki?curid=41576", "title": "Preventive maintenance", "text": ""}
{"id": "41577", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41577", "title": "Primary channel", "text": "In telecommunications, the term primary channel has the following meanings: \nA primary channel may support the transfer of information in one direction only, either direction alternately, or both directions simultaneously."}
{"id": "41578", "revid": "7903804", "url": "https://en.wikipedia.org/wiki?curid=41578", "title": "Primary Rate Interface", "text": "Telecommunication standard\nThe Primary Rate Interface (PRI) is a telecommunications interface standard used on an Integrated Services Digital Network (ISDN) for carrying multiple DS0 voice and data transmissions between the network and a user.\nPRI is the standard for providing telecommunication services to enterprises and offices. It is based on T-carrier (T1) transmission in the US, Canada, and Japan, while the E-carrier (E1) is common in Europe and Australia. The T1 line consists of 23 bearer (B) channels and one data (D) channel for control purposes, for a total bandwidth of 24x64-kbit/s or 1.544\u00a0Mbit/s. The E1 carrier provides 30 B- and one D-channel for a bandwidth of 2.048\u00a0Mbit/s. The first timeslot on the E1 is used for synchronization purposes and is not considered to be a B- or D-channel. The D-channel typically uses timeslot 16 on an E1, while it is timeslot 24 for a T1. Fewer active bearer channels, sometimes called user channels, may be used in fractional T1 or E1 services.\nISDN service types.\nThe Integrated Services Digital Network (ISDN) prescribes two levels of service:\nEach B-channel carries data, voice, and other services. The D-channel carries control and signaling information. Larger connections are possible using PRI pairing. A dual T1-PRI could have 24 + 23 = 47 B-channels and 1 D-channel (often called \"47B + D\"), but more commonly has 46 B-channels and 2 D-channels thus providing a backup signaling channel. The concept applies to E1s as well and both can include more than 2 PRIs. When configuring multiple T1's as ISDN-PRI's, it's possible to use NFAS (non-facility associated signaling) to enable one or two D-channels to support additional B-channels on separate T1 circuits.\nApplication.\nThe Primary Rate Interface channels are typically used by medium to large enterprises with digital private branch exchange (PBX) telephone systems to provide digital access to the public switched telephone network (PSTN). The B-channels may be used flexibly and reassigned when necessary to meet special needs such as video conferences.\nPRI channels and direct inward dialing are also common as a means of delivering inbound calls to voice over IP gateways from the PSTN.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41579", "revid": "6133955", "url": "https://en.wikipedia.org/wiki?curid=41579", "title": "Primary station", "text": "In a data communication network, the primary station is the station responsible for unbalanced control of a data link. \nThe primary station generates commands and interprets responses, and is responsible for initialization of data and control information interchange, organization and control of data flow, retransmission control, and all recovery functions at the link level.\nReferences.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41580", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41580", "title": "Primary time standard", "text": "In telecommunications, a primary time standard is a time standard that does not require calibration against another time standard.\nExamples of primary time, (\"i.e.\", frequency standards) are caesium standards and hydrogen masers.\nThe international second is based on the microwave frequency (9,192,631,770\u00a0Hz) associated with the atomic resonance of the hyperfine ground state levels of the caesium-133 atom in a magnetically neutral environment. Realizable caesium frequency standards use a strong electromagnet to deliberately introduce a magnetic field which overwhelms that of the Earth. The presence of this strong magnetic field introduces a slight, but known, increase in the atomic resonance frequency. However, very small variations in the calibration of the electric current in the electromagnet introduce minuscule frequency variations among different caesium oscillators."}
{"id": "41582", "revid": "44062", "url": "https://en.wikipedia.org/wiki?curid=41582", "title": "Priority", "text": "Prioritization is an action that arranges items or activities in order of importance.\nPriority may refer specifically to:\n&lt;templatestyles src=\"Template:TOC_right/styles.css\" /&gt;\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "41583", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41583", "title": "Priority level", "text": ""}
{"id": "41584", "revid": "1314202124", "url": "https://en.wikipedia.org/wiki?curid=41584", "title": "Private line", "text": "In telecommunications, a private line is typically a telephone company service that uses a dedicated, usually unswitched point-to-point circuit, but it may involve private switching arrangements, or predefined transmission physical or virtual paths. Most private lines connect only two locations, but some have multiple drop points. If the circuit is used for interconnecting switching systems, including manual switchboards, it is often called a tie line.\nAmong subscribers to the public switched telephone network, the term \"private line\" is often erroneously used to describe an \"individual\" telephone line for service for only one subscriber, as opposed to a party line with multiple stations connected.\nIn radio or wireless telephony, \"Private Line\" is a term trademarked by Motorola to describe an implementation of a Continuous Tone-Coded Squelch System (CTCSS), a method of using low-frequency subaudible tones to share a single radio channel among multiple users. Each user group would use a different low frequency tone. Motorola's trade name, especially the abbreviation \"PL\", has become a genericized trademark for the method. General Electric used the term \"Channel Guard\" to describe the same system and other manufacturers used other terms.\nA later digital version of a private line is called \"digital private line\" (DPL).\nAdvantages of private telephone lines are:"}
{"id": "41585", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41585", "title": "Proceed-to-select", "text": "Stage in establishing a telecommunication\nIn telecommunications, proceed-to-select is a signal or event in the call-access phase of a data call, which signal or event confirms the reception of a call-request signal and advises the calling data terminal equipment to proceed with the transmission of the selection signals. \nExamples of proceed-to-select pertain to a dial tone in a telephone system.\nReferences.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41586", "revid": "7903804", "url": "https://en.wikipedia.org/wiki?curid=41586", "title": "Propagation constant", "text": "Measure of change in amplitude and phase of a wave\nThe propagation constant of a sinusoidal electromagnetic wave is a measure of the change undergone by the amplitude and phase of the wave as it propagates in a given direction. The quantity being measured can be the voltage, the current in a circuit, or a field vector such as electric field strength or flux density. The propagation constant itself measures the dimensionless change in magnitude or phase per unit length. In the context of two-port networks and their cascades, propagation constant measures the change undergone by the source quantity as it propagates from one port to the next.\nThe propagation constant's value is expressed logarithmically, almost universally to the base \"e\", rather than base 10 that is used in telecommunications in other situations. The quantity measured, such as voltage, is expressed as a sinusoidal phasor. The phase of the sinusoid varies with distance which results in the propagation constant being a complex number, the imaginary part being caused by the phase change.\nAlternative names.\nThe term \"propagation constant\" is somewhat of a misnomer as it usually varies strongly with \"\u03c9\". It is probably the most widely used term but there are a large variety of alternative names used by various authors for this quantity. These include transmission parameter, transmission function, propagation parameter, propagation coefficient and transmission constant. If the plural is used, it suggests that \"\u03b1\" and \"\u03b2\" are being referenced separately but collectively as in transmission parameters, propagation parameters, etc. In transmission line theory, \"\u03b1\" and \"\u03b2\" are counted among the \"secondary coefficients\", the term \"secondary\" being used to contrast to the \"primary line coefficients\". The primary coefficients are the physical properties of the line, namely \"R\", \"C\", \"L\" and \"G\", from which the secondary coefficients may be derived using the telegrapher's equation. In the field of transmission lines, the term transmission coefficient has a different meaning despite the similarity of name: it is the companion of the reflection coefficient.\nDefinition.\nThe propagation constant, symbol \u03b3, for a given system is defined by the ratio of the complex amplitude at the source of the wave to the complex amplitude at some distance x, such that\n formula_1\nInverting the above equation and isolating \u03b3 results in the quotient of the complex amplitude ratio's natural logarithm and the distance x traveled:\n formula_2\nSince the propagation constant is a complex quantity we can write:\n formula_3\nwhere\nThat \u03b2 does indeed represent phase can be seen from Euler's formula:\n formula_5\nwhich is a sinusoid which varies in phase as \u03b8 varies but does not vary in amplitude because\n formula_6\nThe reason for the use of base e is also now made clear. The imaginary phase constant, \"i\u03b2\", can be added directly to the attenuation constant, \u03b1, to form a single complex number that can be handled in one mathematical operation provided they are to the same base. To arrive at radians requires the base e, and likewise to arrive at nepers for attenuation requires the base e.\nThe propagation constant for conducting lines can be calculated from the primary line coefficients by means of the relationship\n formula_7\nwhere\n formula_8 is the series impedance of the line per unit length and,\n formula_9 is the shunt admittance of the line per unit length.\nPlane wave.\nThe propagation factor of a plane wave traveling in a linear media in the x direction is given by\nformula_10\nwhere\nThe sign convention is chosen for consistency with propagation in lossy media. If the attenuation constant is positive, then the wave amplitude decreases as the wave propagates in the x direction.\nWavelength, phase velocity, and skin depth have simple relationships to the components of the propagation constant:\nformula_20\nAttenuation constant.\nIn telecommunications, the term attenuation constant, also called attenuation parameter or attenuation coefficient, is the attenuation of an electromagnetic wave propagating through a medium per unit distance from the source. It is the real part of the propagation constant and is measured using the unit neper per metre. A neper is approximately 8.7\u00a0dB. Attenuation constant can be defined by the amplitude ratio\n formula_21\nThe propagation constant per unit length is defined as the natural logarithm of the ratio of the sending end current or voltage to the receiving end current or voltage, divided by the distance \"x\" involved:\n formula_22\nConductive lines.\nThe attenuation constant for conductive lines can be calculated from the primary line coefficients as shown above. For a line meeting the distortionless condition, with a conductance \"G\" in the insulator, the attenuation constant is given by\n formula_23\nHowever, a real line is unlikely to meet this condition without the addition of loading coils and, furthermore, there are some frequency dependent effects operating on the primary \"constants\" which cause a frequency dependence of the loss. There are two main components to these losses, the metal loss and the dielectric loss.\nThe loss of most transmission lines are dominated by the metal loss, which causes a frequency dependency due to finite conductivity of metals, and the skin effect inside a conductor. The skin effect causes R along the conductor to be approximately dependent on frequency according to\n formula_24\nLosses in the dielectric depend on the loss tangent (tan\u00a0\"\u03b4\") of the material divided by the wavelength of the signal. Thus they are directly proportional to the frequency.\n formula_25\nOptical fibre.\nThe attenuation constant for a particular propagation mode in an optical fibre is the real part of the axial propagation constant.\nPhase constant.\nIn electromagnetic theory, the phase constant, also called phase change constant, parameter or coefficient is the imaginary component of the propagation constant for a plane wave. It represents the change in phase per unit length along the path traveled by the wave at any instant and is equal to the real part of the angular wavenumber of the wave. It is represented by the symbol \"\u03b2\" (SI unit: radians per metre).\nFrom the definition of (angular) wavenumber for transverse electromagnetic (TEM) waves in lossless media,\n formula_26\nFor a transmission line, the telegrapher's equations tells us that the wavenumber must be proportional to frequency for the transmission of the wave to be undistorted in the time domain. This includes, but is not limited to, the ideal case of a lossless line. The reason for this condition can be seen by considering that a useful signal is composed of many different wavelengths in the frequency domain. For there to be no distortion of the waveform, all these waves must travel at the same velocity so that they arrive at the far end of the line at the same time as a group. Since wave phase velocity is given by\n formula_27\nit is proved that \"\u03b2\" is required to be proportional to \"\u03c9\". In terms of primary coefficients of the line, this yields from the telegrapher's equation for a distortionless line the condition\n formula_28\nwhere \"L\" and \"C\" are, respectively, the inductance and capacitance per unit length of the line. However, practical lines can only be expected to approximately meet this condition over a limited frequency band.\nIn particular, the phase constant formula_14 is not always equivalent to the wavenumber formula_30. The relation\n formula_31\napplies to the TEM wave, which travels in free space or TEM-devices such as the coaxial cable and two parallel wires transmission lines. Nevertheless, it does not apply to the TE wave (transverse electric wave) and TM wave (transverse magnetic wave). For example, in a hollow waveguide where the TEM wave cannot exist but TE and TM waves can propagate,\n formula_32\n formula_33\nHere formula_34 is the cutoff frequency. In a rectangular waveguide, the cutoff frequency is\n formula_35\nwhere formula_36 are the mode numbers for the rectangle's sides of length formula_37 and formula_38 respectively. For TE modes, formula_39 (but formula_40 is not allowed), while for TM modes formula_41.\nThe phase velocity equals\n formula_42\nFilters and two-port networks.\nThe term propagation constant or propagation function is applied to filters and other two-port networks used for signal processing. In these cases, however, the attenuation and phase coefficients are expressed in terms of nepers and radians per network section rather than per unit length. Some authors make a distinction between per unit length measures (for which \"constant\" is used) and per section measures (for which \"function\" is used).\nThe propagation constant is a useful concept in filter design which invariably uses a cascaded section topology. In a cascaded topology, the propagation constant, attenuation constant and phase constant of individual sections may be simply added to find the total propagation constant etc.\nCascaded networks.\nThe ratio of output to input voltage for each network is given by\n formula_43\n formula_44\n formula_45\nThe terms formula_46 are impedance scaling terms and their use is explained in the image impedance article.\nThe overall voltage ratio is given by\n formula_47\nThus for \"n\" cascaded sections all having matching impedances facing each other, the overall propagation constant is given by\n formula_48\nSee also.\nThe concept of penetration depth is one of many ways to describe the absorption of electromagnetic waves. For the others, and their interrelationships, see the article \"Mathematical descriptions of opacity\".\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "41587", "revid": "37195583", "url": "https://en.wikipedia.org/wiki?curid=41587", "title": "Propagation path obstruction", "text": "In telecommunications, a propagation path obstruction is a man-made or natural physical feature that lies near enough to a radio path to cause a measurable effect on path loss, exclusive of reflection effects. An obstruction may lie to the side, above, or below the path. Ridges, bridges, cliffs, buildings, and trees are examples of obstructions. If the clearance from the nearest anticipated path position, over the expected range of Earth radius \"k\"-factor, exceeds 0.6 of the first Fresnel zone radius, the feature is not normally considered an obstruction."}
{"id": "41589", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41589", "title": "Protective distribution system", "text": "Telecommunication system by the US Government\nA protective distribution system (PDS), also called \"protected distribution system\", is a US government term for wireline or fiber-optic telecommunication system that includes terminals and adequate acoustical, electrical, electromagnetic, and physical safeguards to permit its use for the unencrypted transmission of classified information. At one time these systems were called \"approved circuits\".\nA complete protected distribution system includes the subscriber and terminal equipment and the interconnecting lines.\nDescription.\nThe purpose of a PDS is to deter, detect and/or make difficult physical access to the communication lines carrying national security information.\nA specification called the National Security Telecommunications and Information Systems Security Instruction (NSTISSI) 7003 was issued in December 1996 by the Committee on National Security Systems.\nApproval authority, standards, and guidance for the design, installation, and maintenance for PDS are provided by NSTISSI 7003 to U.S. government departments and agencies and their contractors and vendors. This instruction describes the requirements for all PDS installations within the U.S. and for low and medium threat locations outside the U.S.\nPDS is commonly used to protect SIPRNet and JWICS networks.\nThe document superseded one numbered NASCI 4009 on Protected Distribution Systems, dated December 30, 1981, and part of a document called NACSEM 5203, that covered guidelines for facility design, using the designations \"red\" and \"black\".\nThere are two types of PDS: hardened distribution systems and simple distribution systems.\nHardened distribution.\nHardened distribution PDSs provide significant physical protection and can be implemented in three forms: hardened carrier PDSs, alarmed carrier PDSs and continuously viewed carrier PDSs.\nHardened carrier.\nIn a hardened carrier PDS, the data cables are installed in a carrier constructed of electrical metallic tubing (EMT), ferrous conduit or pipe, or rigid sheet steel ducting. All of the connections in a Hardened Carrier System are permanently sealed completely around all surfaces with welds, epoxy or other such sealants. If the hardened carrier is buried under ground, to secure cables running between buildings for example, the carrier containing the cables is encased in concrete.\nWith a hardened carrier system, detection is accomplished via human inspections that are required to be performed periodically. Therefore, hardened carriers are installed below ceilings or above flooring so they can be visually inspected to ensure that no intrusions have occurred. These periodic visual inspections (PVIs) occur at a frequency dependent upon the level of threat to the environment, the security classification of the data, and the access control to the area.\nAlarmed carrier.\nAs an alternative to conducting human visual inspections, an alarmed carrier PDS may be constructed to automate the inspection process through electronic monitoring with an alarm system. In an Alarmed Carrier PDS, the carrier system is \u201calarmed\u201d with specialized optical fibers deployed within the conduit for the purpose of sensing acoustic vibrations that usually occur when an intrusion is being attempted on the conduit in order to gain access to the cables.\nAlarmed carrier PDS offers several advantages over hardened carrier PDS:\nLegacy alarmed carrier systems monitor the carrier containing the cables being protected. More advanced systems monitor the fibers within, or intrinsic to, the cables being protected to turn those cables into sensors, which detect intrusion attempts.\nDepending on the government organization, utilizing an alarmed carrier PDS in conjunction with interlocking armored cable may, in some cases, allow for the elimination of the carrier systems altogether. In these instances, the cables being protected can be installed in existing conveyance (wire basket, ladder rack) or suspended cabling (on D-rings, J-Hooks, etc.).\nContinuously viewed carrier.\nA Continuously Viewed Carrier PDS is one that is under continuous observation, 24 hours per day (including when operational). Such circuits may be grouped together, but should be separated from all non-continuously viewed circuits ensuring an open field of view. Standing orders should include the requirement to investigate any attempt to disturb the PDS. Appropriate security personnel should investigate the area of attempted penetration within 15 minutes of discovery. This type of hardened carrier is not used for Top Secret or special category information for non-U.S. UAA.\nUAA is an Uncontrolled Access Area (UAA). Like definitions include Controlled Access Area (CAA) and Restricted Access Area (RAA). A Secure Room (SR) offers the highest degree of protection.\nTherefore, from the least protected (least secure) to the most protected is as follows:\nUAA\nRAA\nCAA\nSR\nSimple distribution.\nSimple distribution PDSs are afforded a reduced level of physical security protection as compared to a hardened distribution PDS. They use a simple carrier system and the following means are acceptable under NSTISSI 7003: \nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41590", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41590", "title": "Protocol-control information", "text": "In telecommunications, the term protocol-control information (PCI) has the following meanings:\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41591", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41591", "title": "Protocol data unit", "text": "Unit of information transmitted over a computer network\nIn telecommunications, a protocol data unit (PDU) is a single unit of information transmitted among peer entities of a computer network. It is composed of protocol-specific control information and user data. In the layered architectures of communication protocol stacks, each layer implements protocols tailored to the specific type or mode of data exchange.\nFor example, the Transmission Control Protocol (TCP) implements a connection-oriented transfer mode, and the PDU of this protocol is called a \"segment\", while the User Datagram Protocol (UDP) uses datagrams as protocol data units for connectionless communication. A layer lower in the Internet protocol suite, at the Internet layer, the PDU is called a packet, irrespective of its payload type.\nPacket-switched data networks.\nIn the context of packet switching data networks, a protocol data unit (PDU) is best understood in relation to a service data unit (SDU).\nThe features or services of the network are implemented in distinct \"layers\". The physical layer sends ones and zeros across a wire or fiber. The data link layer then organizes these ones and zeros into chunks of data and gets them safely to the right place on the wire. The network layer transmits the organized data over multiple connected networks, and the transport layer delivers the data to the right software application at the destination.\nBetween the layers (and between the application and the top-most layer), the layers pass service data units (SDUs) across interfaces. The higher layer understands the structure of the data in the SDU, but the lower layer at the interface does not; moreover, the lower layer treats the SDU as the payload, undertaking to get it to the same interface at the destination. In order to do this, the \"protocol\" (lower) layer will add to the SDU certain data it needs to perform its function, which is called encapsulation. For example, it might add a port number to identify the application, a network address to help with routing, a code to identify the type of data in the packet and error-checking information. All this additional information, plus the original service data unit from the higher layer, constitutes the \"protocol data unit\" at this layer.\nThe SDU and metadata added by the lower layer can be larger than the maximum size of that layer's PDU (known as the maximum transmission unit; MTU). When this is the case, the PDU must be split into multiple payloads of a size suitable for transmission or processing by the lower layer, a process known as IP fragmentation.\nThe significance of this is that the PDU is the structured information that is passed to a matching protocol layer further along on the data's journey that allows the layer to deliver its intended function or service. The matching layer, or \"peer\", decodes the data to extract the original service data unit, decide if it is error-free and where to send it next, etc. Unless we have already arrived at the lowest (physical) layer, the PDU is passed to the peer using services of the next lower layer in the protocol \"stack\". When the PDU passes over the interface from the layer that constructed it to the layer that merely delivers it (and therefore does not understand its internal structure), it becomes a service data unit to that layer. The addition of addressing and control information (encapsulation) to an SDU to form a PDU and the passing of that PDU to the next lower layer as an SDU repeats until the lowest layer is reached and the data passes over some medium as a physical signal.\nThe above process can be likened to the mail system in which a letter (SDU) is placed in an envelope on which an address (addressing and control information) is written, making it a PDU. The sending post office might look only at the postcode and place the letter in a mailbag so that the address on the envelope can no longer be seen, making it now an SDU. The mailbag is labeled with the destination postcode and so becomes a PDU until it is combined with other bags in a crate when it is now an SDU, and the crate is labeled with the region to which all the bags are to be sent, making the crate a PDU. When the crate reaches the destination matching its label, it is opened and the bags (SDUs) removed only to become PDUs when someone reads the code of the destination post office. The letters themselves are SDUs when the bags are opened but become PDUs when the address is read for final delivery. When the addressee finally opens the envelope, the top-level SDU, the letter itself, emerges.\nExamples.\nOSI model.\nProtocol data units of the OSI model are:\nGiven a context pertaining to a specific OSI layer, \"PDU\" is sometimes used as a synonym for its representation at that layer.\nInternet protocol suite.\nProtocol data units for the Internet protocol suite are:\nOn TCP/IP over Ethernet, the data on the physical layer is carried in Ethernet frames.\nATM.\nThe data link layer PDU in Asynchronous Transfer Mode (ATM) networks is called a \"cell\".\nMedia access control protocol data unit.\nA media access control protocol data unit (MAC PDU or MPDU) is a message that is exchanged between media access control (MAC) entities in a communication system based on the layered OSI model.\nIn systems where the MPDU may be larger than the MAC service data unit (MSDU), the MPDU may \"include\" multiple MSDUs as a result of packet aggregation. In systems where the MPDU is smaller than the MSDU, then one MSDU may \"generate\" multiple MPDUs as a result of packet segmentation.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41592", "revid": "18054835", "url": "https://en.wikipedia.org/wiki?curid=41592", "title": "Provisioning (technology)", "text": "Process of preparing and equipping a network\nIn telecommunications, provisioning involves the process of preparing and equipping a network to allow it to provide new services to its users. In National Security/Emergency Preparedness telecommunications services, \"provisioning\" equates to \"initiation\" and includes altering the state of an existing priority service or capability.\nThe concept of network provisioning or service mediation, mostly used in the telecommunication industry, refers to the provisioning of the customer's services to the network elements, which are various equipment connected in that network communication system. Generally in telephony provisioning this is accomplished with network management database table mappings. It requires the existence of networking equipment and depends on network planning and design.\nIn a modern signal infrastructure employing information technology (IT) at all levels, there is no possible distinction between telecommunications services and \"higher level\" infrastructure. Accordingly, provisioning configures any required systems, provides users with access to data and technology resources, and refers to all enterprise-level information-resource management involved.\nOrganizationally, a CIO typically manages provisioning, necessarily involving human resources and IT departments cooperating to:\nAs its core, the provisioning process monitors access rights and privileges to ensure the security of an enterprise's resources and user privacy. As a secondary responsibility, it ensures compliance and minimizes the vulnerability of systems to penetration and abuse. As a tertiary responsibility, it tries to reduce the amount of custom configuration using boot image control and other methods that radically reduce the number of different configurations involved.\nDiscussion of provisioning often appears in the context of virtualization, orchestration, utility computing, cloud computing, and open-configuration concepts and projects. For instance, the OASIS Provisioning Services Technical Committee (PSTC) defines an XML-based framework for exchanging user, resource, and service-provisioning information - SPML (Service Provisioning Markup Language) for \"managing the provisioning and allocation of identity information and system resources within and between organizations\".\nOnce provisioning has taken place, the process of SysOpping ensures the maintenance of services to the expected standards. Provisioning thus refers only to the setup or startup part of the service operation, and SysOpping to the ongoing support.\nNetwork provisioning.\nOne type of provisioning.\nThe services which are assigned to the customer in the customer relationship management (CRM) have to be provisioned on the network element which is enabling the service and allows the customer to actually use the service. The relation between a service configured in the CRM and a service on the network elements is not necessarily a one-to-one relationship; for example, services like Microsoft Media Server (mms://) can be enabled by more than one network element.\nDuring the provisioning, the service mediation device translates the service and the corresponding parameters of the service to one or more services/parameters on the network elements involved. The algorithm used to translate a system service into network services is called provisioning logic.\nElectronic invoice feeds from your carriers can be automatically downloaded directly into the core of the telecom expense management (TEM) software and it will immediately conduct an audit of each single line item charge all the way down to the User Support and Operations Center (USOC) level. The provisioning software will capture each circuit number provided by all of your carriers and if billing occurs outside of the contracted rate an exception rule will trigger a red flag and notify the pre-established staff member to review the billing error.\nServer provisioning.\nServer provisioning is a set of actions to prepare a server with appropriate systems, data and software, and make it ready for network operation. Typical tasks when provisioning a server are: select a server from a pool of available servers, load the appropriate software (operating system, device drivers, middleware, and applications), appropriately customize and configure the system and the software to create or change a boot image for this server, and then change its parameters, such as IP address, IP Gateway to find associated network and storage resources (sometimes separated as \"resource provisioning\") to audit the system. By auditing the system, you ensure OVAL compliance with limit vulnerability, ensure compliance, or install patches. After these actions, you restart the system and load the new software. This makes the system ready for operation. Typically an internet service provider (ISP) or network operations center will perform these tasks to a well-defined set of parameters, for example, a boot image that the organization has approved and which uses software it has license to. Many instances of such a boot image create a virtual dedicated host.\nThere are many software products available to automate the provisioning of servers, services and end-user devices. Examples: BMC Bladelogic Server Automation, HP Server Automation, IBM Tivoli Provisioning Manager, Redhat Kickstart, xCAT, HP Insight CMU, etc. Middleware and applications can be installed either when the operating system is installed or afterwards by using an Application Service Automation tool. Further questions are addressed in academia such as when provisioning should be issued and how many servers are needed in multi-tier, or multi-service applications.\nIn cloud computing, servers may be provisioned via a web user interface or an application programming interface (API). One of the unique things about cloud computing is how rapidly and easily this can be done. Monitoring software can be used to trigger automatic provisioning when existing resources become too heavily stressed.\nIn short, server provisioning configures servers based on resource requirements. The use of a hardware or software component (e.g. single/dual processor, RAM, HDD, RAID Controller, a number of LAN cards, applications, OS, etc.) depends on the functionality of the server, such as ISP, virtualization, NOS, or voice processing. Server redundancy depends on the availability of servers in the organization. Critical applications have less downtime when using cluster servers, RAID, or a mirroring system.\nService used by most larger-scale centers in part to avoid this. Additional resource provisioning may be done per service.\nThere are several software on the market for server provisioning such as Cobbler or HP Intelligent Provisioning.\nUser provisioning.\nUser provisioning refers to the creation, maintenance and deactivation of user objects and user attributes, as they exist in one or more systems, directories or applications, in response to automated or interactive business processes. User provisioning software may include one or more of the following processes: change propagation, self-service workflow, consolidated user administration, delegated user administration, and federated change control. User objects may represent employees, contractors, vendors, partners, customers or other recipients of a service. Services may include electronic mail, inclusion in a published user directory, access to a database, access to a network or mainframe, etc. User provisioning is a type of identity management software, particularly useful within organizations, where users may be represented by multiple objects on multiple systems and multiple instances.\nSelf-service provisioning for cloud computing services.\nOn-demand self-service is described by the National Institute of Standards and Technology (NIST) as an essential characteristic of cloud computing. The self-service nature of cloud computing lets end users obtain and remove cloud services\u2015including applications, the infrastructure supporting the applications, and configuration\u2015 themselves without requiring the assistance of an IT staff member. The automatic self-servicing may target different application goals and constraints (e.g. deadlines and cost), as well as handling different application architectures (e.g., bags-of-tasks and workflows). Cloud users can obtain cloud services through a cloud service catalog or a self-service portal. Because business users can obtain and configure cloud services themselves, this means IT staff can be more productive and gives them more time to manage cloud infrastructures.\nOne downside of cloud service provisioning is that it is not instantaneous. A cloud virtual machine (VM) can be acquired at any time by the user, but it may take up to several minutes for the acquired VM to be ready to use. The VM startup time is dependent on factors, such as image size, VM type, data center location, and number of VMs. Cloud providers have different VM startup performance.\nMobile subscriber provisioning.\nMobile subscriber provisioning refers to the setting up of new services, such as GPRS, MMS and Instant Messaging for an existing subscriber of a mobile phone network, and any gateways to standard Internet chat or mail services. The network operator typically sends these settings to the subscriber's handset using SMS text services or HTML, and less commonly WAP, depending on what the mobile operating systems can accept.\nA general example of provisioning is with data services. A mobile user who is using his or her device for voice calling may wish to switch to data services in order to read emails or browse the Internet. The mobile device's services are \"provisioned\" and thus the user is able to stay connected through push emails and other features of smartphone services.\nDevice management systems can benefit end-users by incorporating plug-and-play data services, supporting whatever device the end-user is using.. Such a platform can automatically detect devices in the network, sending them settings for immediate and continued usability. The process is fully automated, keeping the history of used devices and sending settings only to subscriber devices which were not previously set. One method of managing mobile updates is to filter IMEI/IMSI pairs. Some operators report activity of 50 over-the-air settings update files per second.\nMobile content provisioning.\nThis refers to delivering mobile content, such as mobile internet to a mobile phone, agnostic of the features of said device. These may include operating system type and versions, Java version, browser version, screen form factors, audio capabilities, language settings and many other characteristics. As of April 2006, an estimated 5,000 permutations were relevant. Mobile content provisioning facilitates a common user experience, though delivered on widely different handsets.\nMobile device provisioning.\nProvisioning devices involves delivering configuration data and policy settings to the mobile devices from a central point \u2013 Mobile device management system tools.\nInternet access provisioning.\nWhen getting a customer online, the client system must be configured. Depending on the connection technology (e.g., DSL, Cable, Fibre), the client system configuration may include:\nThere are four approaches to provisioning internet access:\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41593", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41593", "title": "Pseudorandom noise", "text": "Pseudo-random signal with characteristics similar to noise\nIn cryptography, pseudorandom noise (PRN) is a signal similar to noise which satisfies one or more of the standard tests for statistical randomness. Although it seems to lack any definite pattern, pseudorandom noise consists of a deterministic sequence of pulses that will repeat itself after its period.\nIn cryptographic devices, the pseudorandom noise pattern is determined by a key and the repetition period can be very long, even millions of digits.\nPseudorandom noise is used in some electronic musical instruments, either by itself or as an input to subtractive synthesis, and in many white noise machines.\nIn spread-spectrum systems, the receiver correlates a locally generated signal with the received signal. Such spread-spectrum systems require a set of one or more \"codes\" or \"sequences\" such that\nIn a direct-sequence spread spectrum system, each bit in the pseudorandom binary sequence is known as a \"chip\" and the \"inverse\" of its period as \"chip rate\"; \"compare bit rate and symbol rate.\"\nIn a frequency-hopping spread spectrum sequence, each value in the pseudorandom sequence is known as a \"channel number\" and the \"inverse\" of its period as the \"hop rate\". FCC Part 15 mandates at least 50 different channels and at least a 2.5\u00a0Hz hop rate for narrow band frequency-hopping systems.\nGPS satellites broadcast data at a rate of 50 data bits per second \u2013 each satellite modulates its data with one PN bit stream at 1.023 million chips per second and the same data with another PN bit stream at 10.23 million chips per second.\nGPS receivers correlate the received PN bit stream with a local reference to measure distance. GPS is a receive-only system that uses relative timing measurements from several satellites (and the known positions of the satellites) to determine receiver position.\nOther range-finding applications involve two-way transmissions. A local station generates a pseudorandom bit sequence and transmits it to the remote location (using any modulation technique). Some object at the remote location echoes this PN signal back to the location station \u2013 either passively, as in some kinds of radar and sonar systems, or using an active transponder at the remote location, as in the Apollo Unified S-band system. By correlating a (delayed version of) the transmitted signal with the received signal, a precise round trip time to the remote location can be determined and thus the distance.\nPN code.\nA pseudo-noise code (PN code) or pseudo-random-noise code (PRN code) is one that has a spectrum similar to a random sequence of bits but is deterministically generated. The most commonly used sequences in direct-sequence spread spectrum systems are maximal length sequences, Gold codes, Kasami codes, and Barker codes.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41594", "revid": "547762", "url": "https://en.wikipedia.org/wiki?curid=41594", "title": "Pseudorandom number sequence", "text": ""}
{"id": "41595", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41595", "title": "Psophometer", "text": "In telecommunications, a psophometer is an instrument that measures the perceptible noise of a telephone circuit.\nThe core of the meter is based on a true RMS voltmeter, which measures the level of the noise signal. This was used for the first psophometers, in the 1930s. As the human-perceived level of noise is more important for telephony than their raw voltage, a modern psophometer incorporates a weighting network to represent this perception. The characteristics of the weighting network depend on the type of circuit under investigation, such as whether the circuit is used to normal speech standards (300\u00a0Hz\u00a0\u2013\u00a03.3\u00a0kHz), or for high-fidelity broadcast-quality sound (50\u00a0Hz\u00a0\u2013\u00a015\u00a0kHz).\nEtymology.\nThe name was coined in the 1930s, on a basis from , itself derived from . It is unrelated to .\nThe '-meter' suffix was already widely used in English, but also derives originally from Greek.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41596", "revid": "14984434", "url": "https://en.wikipedia.org/wiki?curid=41596", "title": "Psophometric voltage", "text": "Psophometric voltage is a circuit noise voltage measured with a psophometer that includes a CCIF-1951 weighting network.\n\"Psophometric voltage\" should not be confused with \"\"psophometric emf,\" i.e.\", the emf in a generator or line with 600 \u03a9 internal resistance. For practical purposes, the psophometric emf is twice the corresponding psophometric voltage.\nPsophometric voltage readings, \"V\", in millivolts, are commonly converted to dBm(psoph) by dBm(psoph) = 20 log10\"V\"\u00a0\u2013\u00a057.78.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41597", "revid": "11677590", "url": "https://en.wikipedia.org/wiki?curid=41597", "title": "Public data transmission service", "text": ""}
{"id": "41598", "revid": "994059", "url": "https://en.wikipedia.org/wiki?curid=41598", "title": "Public land mobile network", "text": "Combination of wireless communication services\nIn telecommunication, a public land mobile network (PLMN) is a combination of wireless communication services offered by a specific operator in a specific country. A PLMN typically consists of several cellular technologies like GSM/2G, UMTS/3G, LTE/4G, NR/5G, offered by a single operator within a given country, often referred to as a cellular network.\nPLMN code.\nA PLMN is identified by a globally unique PLMN code, which consists of a MCC (Mobile Country Code) and MNC (Mobile Network Code). Hence, it is a five- to six-digit number identifying a country, and a mobile network operator in that country, usually represented in the form 001-01 or 001\u2013001.\nA PLMN is part of a:\nLeading zeros in PLMN codes.\nNote that an MNC can be of two-digit form and three-digit form with leading zeros. It is administered by the respective national numbering plan administrator. From PLMN assignments, it is apparent that such dualities of two-digit and three-digit MNCs with the same number value are avoided (see the list of mobile country codes and mobile network codes). An example for an actual three-digit/two-digit MNC with leading zeros is in Bermuda MCC, 350-007 and 350-00, 350-01.\nPLMN code and IMSI.\nThe IMSI, which identifies a SIM or USIM for one subscriber, typically starts with the PLMN code. For example, an IMSI belonging to the PLMN 262-33 would look like 262330000000001. Mobile phones use this to detect roaming, so that a mobile phone subscribed on a network with a PLMN code that mismatches the start of the USIM's IMSI will typically display an \"R\" on the icon that indicates connection strength.\nPLMN services.\nA PLMN typically offers the following services to a mobile subscriber:\nThe availability, quality and bandwidth of these services strongly depends on the particular technology used to implement a PLMN.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41599", "revid": "11677590", "url": "https://en.wikipedia.org/wiki?curid=41599", "title": "Pulsating direct current", "text": ""}
{"id": "41600", "revid": "50014652", "url": "https://en.wikipedia.org/wiki?curid=41600", "title": "Pulse", "text": "Tactile arterial palpation of the heartbeat by fingertips\nIn medicine, pulse is the rhythmic expansion and contraction of an artery in response to the cardiac cycle (heartbeat). The pulse may be felt (palpated) in any place that allows an artery to be compressed near the surface of the body, such as at the neck (carotid artery), wrist (radial artery or ulnar artery), at the groin (femoral artery), behind the knee (popliteal artery), near the ankle joint (posterior tibial artery), and on foot (dorsalis pedis artery). The pulse is most commonly measured at the wrist or neck for adults and at the brachial artery (inner upper arm between the shoulder and elbow) for infants and very young children. A sphygmograph is an instrument for measuring the pulse.\nPhysiology.\nClaudius Galen was perhaps the first physiologist to describe the pulse. The pulse is an expedient tactile method of determination of systolic blood pressure to a trained observer. Diastolic blood pressure is non-palpable and unobservable by tactile methods, occurring between heartbeats.\nPressure waves generated by the heart in systole move the arterial walls. Forward movement of blood occurs when the boundaries are pliable and compliant. These properties form enough to create a palpable pressure wave.\nPulse velocity, pulse deficits and much more physiologic data are readily and simplistically visualized by the use of one or more arterial catheters connected to a transducer and oscilloscope. This invasive technique has been commonly used in intensive care since the 1970s.\nThe pulse may be further indirectly observed under light absorbances of varying wavelengths with assigned and inexpensively reproduced mathematical ratios. Applied capture of variances of light signal from the blood component hemoglobin under oxygenated vs. deoxygenated conditions allows the technology of pulse oximetry.\nCharacteristics.\nRate.\nThe rate of the pulse can be observed and measured on the outside of an artery by tactile or visual means. It is recorded as arterial beats per minute or BPM. Although the pulse and heart beat are related, they are not the same. For example, there is a delay between the onset of the heart beat and the onset of the pulse, known as the pulse transit time, which varies by site. Similarly measurements of heart rate variability and pulse rate variability differ. \nIn healthy people, the pulse rate is close to the heart rate, as measured by ECG. Measuring the pulse rate is therefore a convenient way to estimate the heart rate. Pulse deficit is a condition in which a person has a difference between their pulse rate and heart rate. It can be observed by simultaneous palpation at the radial artery and auscultation using a stethoscope at the PMI, near the heart apex, for example. Typically, in people with pulse deficit, heart beats do not result in pulsations at the periphery, meaning the pulse rate is lower than the heart rate. Pulse deficit has been found to be significant in the context of premature ventricular contraction and atrial fibrillation.\nRhythm.\nA normal pulse is regular in rhythm and force. An irregular pulse may be due to sinus arrhythmia, ectopic beats, atrial fibrillation, paroxysmal atrial tachycardia, atrial flutter, partial heart block etc. Intermittent dropping out of beats at pulse is called \"intermittent pulse\". Examples of \"regular\" intermittent (regularly irregular) pulse include pulsus bigeminus, second-degree atrioventricular block. An example of \"irregular\" intermittent (irregularly irregular) pulse is atrial fibrillation.\nVolume.\nThe degree of expansion displayed by artery during diastolic and systolic state is called volume. It is also known as amplitude, expansion or size of pulse.\nHypokinetic pulse.\nA weak pulse signifies narrow pulse pressure. It may be due to low cardiac output (as seen in shock, congestive cardiac failure), hypovolemia, valvular heart disease (such as aortic outflow tract obstruction, mitral stenosis, aortic arch syndrome) etc.\nHyperkinetic pulse.\nA bounding pulse signifies high pulse pressure. It may be due to low peripheral resistance (as seen in fever, anemia, thyrotoxicosis, hyperkinetic heart syndrome, A-V fistula, Paget's disease, beriberi, liver cirrhosis), increased cardiac output, increased stroke volume (as seen in anxiety, exercise, complete heart block, aortic regurgitation), decreased distensibility of arterial system (as seen in atherosclerosis, hypertension and coarctation of aorta).\nThe strength of the pulse can also be reported:\nForce.\nAlso known as compressibility of pulse. It is a rough indication of systolic blood pressure.\nTension.\nDetermined mainly by mean arterial blood pressure; corresponds to diastolic blood pressure. A low tension pulse (pulsus mollis), the vessel is soft or impalpable between beats. In high tension pulse (pulsus durus), vessels feel rigid even between pulse beats.\nForm.\nA form or contour of a pulse is palpatory estimation of arteriogram. A quickly rising and quickly falling pulse (pulsus celer) is seen in aortic regurgitation. A slow rising and slowly falling pulse (pulsus tardus) is seen in aortic stenosis.\nEquality.\nComparing pulses and different places gives valuable clinical information.\nA discrepant or unequal pulse between left and right radial artery is observed in anomalous or aberrant course of artery, coarctation of aorta, aortitis, dissecting aneurysm, peripheral embolism etc. An unequal pulse between upper and lower extremities is seen in coarctation to aorta, aortitis, block at bifurcation of aorta, dissection of aorta, iatrogenic trauma and arteriosclerotic obstruction.\nCondition of arterial wall.\nA normal artery is not palpable after flattening by digital pressure. A thick radial artery which is palpable 7.5\u201310\u00a0cm up the forearm is suggestive of arteriosclerosis.\nRadio-femoral delay.\nIn coarctation of aorta, femoral pulse may be significantly delayed as compared to radial pulse (unless there is coexisting aortic regurgitation). The delay can also be observed in supravalvar aortic stenosis.\nPatterns.\nSeveral pulse patterns can be of clinical significance. These include:\nCommon palpable sites.\nSites can be divided into peripheral pulses and central pulses. Central pulses include the carotid and femoral pulses.\nHead and neck.\nAlthough the pulse can be felt in multiple places in the head, people should not normally hear their heartbeats within the head. This is called pulsatile tinnitus, and it can indicate several medical disorders.\nHistory.\nPulse rate was first measured by ancient Greek physicians and scientists. The first person to measure the heart beat was Herophilus of Alexandria, Egypt (c. 335\u2013280 BC) who designed a water clock to time the pulse. Rumi has mentioned in a poem that \"The wise physician measured the patient's pulse and became aware of his condition.\" It shows the practice was common during Rumi's era and geography. \nThe first person to accurately measure the pulse rate was Santorio Santorii who invented the \"pulsilogium\", a form of pendulum which was later studied by Galileo Galilei. A century later another physician, de Lacroix, used the pulsilogium to test cardiac function.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41601", "revid": "1951353", "url": "https://en.wikipedia.org/wiki?curid=41601", "title": "Pulse-address multiple access", "text": "In telecommunications, pulse-address multiple access (PAMA) is a channel access method that enables the ability of a communication satellite to receive signals from several Earth terminals simultaneously and to amplify, translate, and relay the signals back to Earth, based on the addressing of each station by an assignment of a unique combination of time and frequency slots.\nThis ability may be restricted by allowing only some of the terminals access to the satellite at any given time.\nReferences.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41602", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=41602", "title": "Synchronous digital hierarchy", "text": ""}
{"id": "41603", "revid": "668752", "url": "https://en.wikipedia.org/wiki?curid=41603", "title": "Pulse amplitude", "text": ""}
{"id": "41605", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41605", "title": "Pulse duration", "text": "In signal processing and telecommunications, pulse duration is the interval between the time, during the first transition, that the amplitude of the pulse reaches a specified fraction (level) of its final amplitude, and the time the pulse amplitude drops, on the last transition, to the same level.\nThe interval between the 50% points of the final amplitude is usually used to determine or define pulse duration, and this is understood to be the case unless otherwise specified. Other fractions of the final amplitude, e.g., 90% or 1/e, may also be used, as may the root mean square (rms) value of the pulse amplitude.\nIn radar, the pulse duration is the time the radar's transmitter is energized during each cycle."}
{"id": "41606", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41606", "title": "Pulse link repeater", "text": "Telecommunications device\nIn telecommunications, a pulse link repeater (PLR) is a device that interfaces concatenated E and M signaling paths. \nA PLR converts a ground, received from the E lead of one signal path, to \u221248 VDC, which is applied to the M lead of the concatenated signal path. \nIn many commercial carrier systems, the channel bank cards or modules have a \"PLR\" option that permits the direct concatenation of E&amp;M signaling paths, without the need for separate PLR equipment.\nReferences.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41607", "revid": "14383484", "url": "https://en.wikipedia.org/wiki?curid=41607", "title": "Pulsing", "text": "Pulsing may refer to:\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "41608", "revid": "13467261", "url": "https://en.wikipedia.org/wiki?curid=41608", "title": "Pumping", "text": "Pumping may refer to:\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "41609", "revid": "8066546", "url": "https://en.wikipedia.org/wiki?curid=41609", "title": "Push-to-talk operation", "text": ""}
{"id": "41610", "revid": "355698", "url": "https://en.wikipedia.org/wiki?curid=41610", "title": "Push-to-type operation", "text": "Telegraphy\nPush-to-type operation: In telegraph or data transmission systems, that method of communication in which the operator at a station must keep a switch operated in order to send messages. \nPush-to-type operation is used in radio systems where the same frequency is employed for transmission and reception. \nPush-to-type operation is a derivative form of transmission and may be used in simplex, half-duplex, or duplex operation. \"Synonym\" press-to-type operation.\nThis is similar to Push-to-talk operation for radio phone communications.\nReferences.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41611", "revid": "169421", "url": "https://en.wikipedia.org/wiki?curid=41611", "title": "Quadrature", "text": "Quadrature may refer to:\nOther uses.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "41612", "revid": "11677590", "url": "https://en.wikipedia.org/wiki?curid=41612", "title": "Quadruply clad fiber", "text": ""}
{"id": "41613", "revid": "157530", "url": "https://en.wikipedia.org/wiki?curid=41613", "title": "Quality control", "text": "Processes that maintain quality at a constant level\nQuality control (QC) is a process by which entities review the quality of all factors involved in production. ISO 9000 defines quality control as \"a part of quality management focused on fulfilling quality requirements\".\nThis approach places emphasis on three aspects (enshrined in standards such as ISO 9001):\nInspection is a major component of quality control, where physical product is examined visually (or the end results of a service are analyzed). Product inspectors will be provided with lists and descriptions of unacceptable product defects such as cracks or surface blemishes for example.\nHistory and introduction.\nEarly stone tools such as anvils had no holes and were not designed as interchangeable parts. Mass production established processes for the creation of parts and system with identical dimensions and design, but these processes are not uniform and hence some customers were unsatisfied with the result. Quality control separates the act of testing products to uncover defects from the decision to allow or deny product release, which may be determined by fiscal constraints. For contract work, particularly work awarded by government agencies, quality control issues are among the top reasons for not renewing a contract.\nThe simplest form of quality control was a sketch of the desired item. If the item did not match the sketch, the item was rejected, in a simple Go/no go procedure. However, manufacturers soon found it was difficult and costly to make parts be exactly like their depiction; hence around 1840 tolerance limits were introduced, wherein a design would function if its parts were measured to be within the limits. Quality was thus precisely defined using devices such as plug gauges and ring gauges. However, this did not address the problem of defective items; recycling or disposing of the waste adds to the cost of production, as does trying to reduce the defect rate. Various methods have been proposed to prioritize quality control issues and determine whether to leave them unaddressed or use quality assurance techniques to improve and stabilize production.\nNotable approaches.\nThere is a tendency for individual consultants and organizations to name their own unique approaches to quality control\u2014a few of these have ended up in widespread use:\nIn project management.\nIn project management, quality control requires the project manager and/or the project team to inspect the accomplished work to ensure its alignment with the project scope. In practice, projects typically have a dedicated quality control team which focuses on this area.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41615", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41615", "title": "Quasi-analog signal", "text": "In telecommunications, a quasi-analog signal is a digital signal that has been converted to a form suitable for transmission over a specified analog channel. \nThe specification of the analog channel should include frequency range, bandwidth, signal-to-noise ratio, and envelope delay distortion. When quasi-analog form of signaling is used to convey message traffic over dial-up telephone systems, it is often referred to as voice-data. A modem may be used for the conversion process.\nReferences.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41616", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41616", "title": "Queuing delay", "text": "The time a job waits in a queue until it can be executed\nIn telecommunications and computer engineering, the queuing delay is the time a job waits in a queue until it can be executed. It is a key component of network delay. In a switched network, queuing delay is the time between the completion of signaling by the call originator and the arrival of a ringing signal at the call receiver. Queuing delay may be caused by delays at the originating switch, intermediate switches, or the call receiver servicing switch. In a data network, queuing delay is the sum of the delays between the request for service and the establishment of a circuit to the called data terminal equipment (DTE). In a packet-switched network, queuing delay is the sum of the delays encountered by a packet between the time of insertion into the network and the time of delivery to the address. \nRouter processing.\nThis term is most often used in reference to routers. When packets arrive at a router, they have to be processed and transmitted. A router can only process one packet at a time. If packets arrive faster than the router can process them (such as in a burst transmission) the router puts them into the queue (also called the buffer) until it can get around to transmitting them. Delay can also vary from packet to packet so averages and statistics are usually generated when measuring and evaluating queuing delay. \nAs a queue begins to fill up due to traffic arriving faster than it can be processed, the amount of delay a packet experiences going through the queue increases. The speed at which the contents of a queue can be processed is a function of the transmission rate of the facility. This leads to the classic delay curve. The average delay any given packet is likely to experience is given by the formula 1/(\u03bc-\u03bb) where \u03bc is the number of packets per second the facility can sustain and \u03bb is the average rate at which packets are arriving to be serviced. This formula can be used when no packets are dropped from the queue. \nThe maximum queuing delay is proportional to buffer size. The longer the line of packets waiting to be transmitted, the longer the average waiting time is. The router queue of packets waiting to be sent also introduces a potential cause of packet loss. Since the router has a finite amount of buffer memory to hold the queue, a router that receives packets at too high a rate may experience a full queue. In this case, the router has no other option than to simply discard excess packets.\nWhen the transmission protocol uses the dropped-packets symptom of filled buffers to regulate its transmit rate, as the Internet's TCP does, bandwidth is fairly shared at near theoretical capacity with minimal network congestion delays. Absent this feedback mechanism the delays become both unpredictable and rise sharply, a symptom also seen as freeways approach capacity; metered onramps are the most effective solution there, just as TCP's self-regulation is the most effective solution when the traffic is packets instead of cars). This result is both hard to model mathematically and quite counterintuitive to people who lack experience with mathematics or real networks. Failing to drop packets, choosing instead to buffer an ever-increasing number of them, produces bufferbloat.\nNotation.\nIn Kendall's notation, the M/M/1/K queuing model, where K is the size of the buffer, may be used to analyze the queuing delay in a specific system. Kendall's notation should be used to calculate the queuing delay when packets are dropped from the queue. The M/M/1/K queuing model is the most basic and important queuing model for network analysis.\nReferences.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41617", "revid": "1375", "url": "https://en.wikipedia.org/wiki?curid=41617", "title": "Queuing theory", "text": ""}
{"id": "41618", "revid": "30607451", "url": "https://en.wikipedia.org/wiki?curid=41618", "title": "Radiation angle", "text": "Half the vertex angle of the cone of light emitted at the exit face of an optical fiber\nIn fiber optics, the radiation angle is half the vertex angle of the cone of light emitted at the exit face of an optical fiber.\nThe cone boundary is usually defined (a) by the angle at which the far-field irradiance has decreased to a specified fraction of its maximum value or (b) as the cone within which there is a specified fraction of the total radiated power at any point in the far field.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41619", "revid": "7679161", "url": "https://en.wikipedia.org/wiki?curid=41619", "title": "Radiation mode", "text": "For an optical fiber or waveguide, a radiation mode or unbound mode is a mode which is not confined by the fiber core. Such a mode has fields that are transversely oscillatory everywhere external to the waveguide, and exists even at the limit of zero wavelength.\nSpecifically, a radiation mode is one for which \nformula_1\nwhere \"\u03b2\" is the imaginary part of the axial propagation constant, integer \"l\" is the azimuthal index of the mode, \"n\"(\"r\") is the refractive index at radius \"r\", \"a\" is the core radius, and \"k\" is the free-space wave number, \"k\" = 2\u03c0/\"\u03bb\", where \"\u03bb\" is the wavelength. Radiation modes correspond to refracted rays in the terminology of geometric optics."}
{"id": "41620", "revid": "6727347", "url": "https://en.wikipedia.org/wiki?curid=41620", "title": "Radiation pattern", "text": "Directional variation in strength of radio waves\nAn antenna radiation pattern (or antenna pattern or far-field pattern) is the \"directional\" (angular) dependence of the field strength (sometimes also the phase) of the radio waves from the antenna or other source.\nParticularly in the fields of fiber optics, lasers, and integrated optics, the term radiation pattern may also be used as a synonym for the near-field pattern or Fresnel pattern. This refers to the \"positional\" dependence of the electromagnetic field in the near field, or Fresnel region of the source. The near-field pattern is most commonly defined over a plane placed in front of the source, or over a cylindrical or spherical surface enclosing it.\nThe far-field pattern of an antenna may be determined experimentally at an antenna range, or alternatively, the near-field pattern may be found using a \"near-field scanner\", and the radiation pattern deduced from it by computation. The far-field radiation pattern can also be calculated from the antenna shape by computer programs such as NEC. Other software, like HFSS can also compute the near field.\nThe far field radiation pattern may be represented graphically as a plot of one of a number of related variables, like the strength at a constant (large) radius (an amplitude pattern or field pattern), the power per unit solid angle (power pattern), and the directive gain (gain pattern). Very often, only the relative amplitude is plotted, normalized either to the amplitude on the antenna boresight, or to the total radiated power. The plotted quantity may be shown on a linear scale, or in dB. The plot is typically represented as a three-dimensional graph (as at right), or as separate graphs in the vertical plane and horizontal plane. This is often known as a polar diagram.\nReciprocity.\nIt is a fundamental property of antennas that the receiving pattern (sensitivity as a function of direction) of an antenna when used for receiving is identical to the far-field radiation pattern of the antenna when used for transmitting. This is a consequence of the reciprocity theorem of electromagnetics and is proved below. Therefore, in discussions of radiation patterns the antenna can be viewed as either transmitting or receiving, whichever is more convenient.\nThere are limits to reciprocity: It applies only to \"passive\" antenna elements \u2013 \"active\" antennas that incorporate amplifiers or other individually powered components are \"not\" reciprocal. And even when the antenna is made of exclusively of passive elements, reciprocity only applies to the waves emitted and intercepted by the antenna. Reciprocity does \"not\" apply to the distribution of current in the various parts of the antenna generated by the intercepted waves nor currents that create emitted waves: Antenna current profiles typically differ for receiving and transmitting, despite the waves in the far field radiating inward and outward along the same path, with the same overall pattern, just with reversed direction.\nTypical patterns.\nSince electromagnetic radiation is dipole radiation, it is not possible to build an antenna that radiates coherently equally in all directions, although such a hypothetical isotropic antenna is used as a reference to calculate antenna gain.\nThe simplest antennas, monopole and dipole antennas, consist of one or two straight metal rods along a common axis. These axially symmetric antennas have radiation patterns with a similar symmetry, called omnidirectional patterns; they radiate equal power in all directions perpendicular to the antenna, with the power varying only with the angle to the axis, dropping off to zero on the antenna's axis. This illustrates the general principle that if the shape of an antenna is symmetrical, its radiation pattern will have the same symmetry.\nIn most antennas, the radiation from the different parts of the antenna interferes at some angles; the radiation pattern of the antenna can be considered an interference pattern. This results in minimum or zero radiation at certain angles where the radio waves from the different parts arrive out of phase, and local maxima of radiation at other angles where the radio waves arrive in phase. Therefore, the radiation plot of most antennas shows a pattern of maxima called \"\"lobes\" at various angles, separated by \"nulls\" at which the radiation goes to zero. The larger the antenna is compared to a wavelength, the more lobes there will be.\nIn a directional antenna in which the objective is to emit the radio waves in one particular direction, the antenna is designed to radiate most of its power in the lobe directed in the desired direction. Therefore, in the radiation plot this lobe appears larger than the others; it is called the \"main lobe\". The axis of maximum radiation, passing through the center of the main lobe, is called the \"beam axis\"\" or \"boresight axis\". In some antennas, such as split-beam antennas, there may exist more than one major lobe. The other lobes beside the main lobe, representing unwanted radiation in other directions, are called minor lobes. The minor lobes oriented at an angle to the main lobe are called \"side lobes\". The minor lobe in the opposite direction (180\u00b0) from the main lobe is called the \"back lobe\"\".\nMinor lobes usually represent radiation in undesired directions, so in directional antennas a design goal is usually to reduce the minor lobes. Side lobes are normally the largest of the minor lobes. The level of minor lobes is usually expressed as a ratio of the power density in the lobe in question to that of the major lobe. This ratio is often termed the side lobe ratio or side lobe level. Side lobe levels of \u221220\u00a0dB or greater are usually not desirable in many applications. Attainment of a side lobe level smaller than \u221230\u00a0dB usually requires very careful design and construction. In most radar systems, for example, low side lobe ratios are very important to minimize false target indications through the side lobes.\nProof of reciprocity.\nFor a complete proof, see the reciprocity (electromagnetism) article. Here, we present a common simple proof limited to the approximation of two antennas separated by a large distance compared to the size of the antenna, in a homogeneous medium. The first antenna is the test antenna whose patterns are to be investigated; this antenna is free to point in any direction. The second antenna is a reference antenna, which points rigidly at the first antenna.\nEach antenna is alternately connected to a transmitter having a particular source impedance, and a receiver having the same input impedance (the impedance may differ between the two antennas).\nIt is assumed that the two antennas are sufficiently far apart that the properties of the transmitting antenna are not affected by the load placed upon it by the receiving antenna. Consequently, the amount of power transferred from the transmitter to the receiver can be expressed as the product of two independent factors; one depending on the directional properties of the transmitting antenna, and the other depending on the directional properties of the receiving antenna.\nFor the transmitting antenna, by the definition of gain, formula_1, the radiation power density at a distance formula_2 from the antenna (i.e. the power passing through unit area) is\nformula_3.\nHere, the angles formula_4 and formula_5 indicate a dependence on direction from the antenna, and formula_6 stands for the power the transmitter would deliver into a matched load. The gain formula_1 may be broken down into three factors; the antenna gain (the directional redistribution of the power), the radiation efficiency (accounting for ohmic losses in the antenna), and lastly the loss due to mismatch between the antenna and transmitter. Strictly, to include the mismatch, it should be called the realized gain, but this is not common usage.\nFor the receiving antenna, the power delivered to the receiver is\nformula_8.\nHere formula_9 is the power density of the incident radiation, and formula_10 is the antenna aperture or effective area of the antenna (the area the antenna would need to occupy in order to intercept the observed captured power). The directional arguments are now relative to the receiving antenna, and again formula_10 is taken to include ohmic and mismatch losses.\nPutting these expressions together, the power transferred from transmitter to receiver is\nformula_12,\nwhere formula_1 and formula_10 are directionally dependent properties of the transmitting and receiving antennas respectively. For transmission from the reference\nantenna (2), to the test antenna (1), that is\nformula_15,\nand for transmission in the opposite direction\nformula_16.\nHere, the gain formula_17 and effective area formula_18 of antenna 2 are fixed, because the orientation of this antenna is fixed with respect to the first.\nNow for a given disposition of the antennas, the reciprocity theorem requires that the power transfer is equally effective in each direction, i.e.\nformula_19,\nwhence\nformula_20.\nBut the right hand side of this equation is fixed (because the orientation of antenna 2 is fixed), and so\nformula_21,\ni.e. the directional dependence of the (receiving) effective aperture and the (transmitting) gain are identical (QED). Furthermore, the constant of proportionality is the same irrespective of the nature of the antenna, and so must be the same for all antennas. Analysis of a particular antenna (such as a Hertzian dipole), shows that this constant is formula_22, where formula_23 is the free-space wavelength. Hence, for any antenna the gain and the effective aperture are related by\nformula_24.\nEven for a receiving antenna, it is more usual to state the gain than to specify the effective aperture. The power delivered to the receiver is therefore more usually written as\nformula_25\n(see link budget). The effective aperture is however of interest for comparison with the actual physical size of the antenna.\nReferences.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41621", "revid": "16163846", "url": "https://en.wikipedia.org/wiki?curid=41621", "title": "Radiation scattering", "text": ""}
{"id": "41622", "revid": "130326", "url": "https://en.wikipedia.org/wiki?curid=41622", "title": "Radio equipment", "text": ""}
{"id": "41623", "revid": "221", "url": "https://en.wikipedia.org/wiki?curid=41623", "title": "Radio fix", "text": "In telecommunications and position fixing, the term radio fix has the following meanings:\nCompare triangulation.\nObtaining a radio fix.\nA single transmitter can be used to give a line of position (LOP) of the craft. The (true) bearing to the station from the craft, TB or QUJ, is composed of the true heading, TH, plus the relative bearing, RB, of the station. The bearing from the station (QTE) is found by adding 180\u00b0 to the QUJ figure.\nThe line of position is then the line of bearing QUJ (i.e. from the station to the receiver) passing through the station.\nformula_1\nFor the diagram on the right, we have:\nformula_2\nformula_3\nA radio fix on two stations can be found in exactly the same way. The intersection of the two position lines gives the position of the receiver. For the diagram on the right, the LOPs are found as before:\nformula_4\nformula_5\nformula_6\nformula_7\nRemembering that the LOPs pass through their respective stations, it is now simple to find the location of the craft.\nRemember too, that bearings and direction are given/recorded with respect to True North and to Magnetic North. Values used by mobile stations usually need to be converted from Magnetic to True. (Fixed stations are expected to use True)."}
{"id": "41624", "revid": "6727347", "url": "https://en.wikipedia.org/wiki?curid=41624", "title": "Radio horizon", "text": ""}
{"id": "41625", "revid": "8273231", "url": "https://en.wikipedia.org/wiki?curid=41625", "title": "Radiometry", "text": "Techniques for measuring electromagnetic radiation\nRadiometry is a set of techniques for measuring electromagnetic radiation, including visible light. Radiometric techniques in optics characterize the distribution of the radiation's power in space, as opposed to photometric techniques, which characterize the light's interaction with the human eye. The fundamental difference between radiometry and photometry is that radiometry gives the entire optical radiation spectrum, while photometry is limited to the visible spectrum. Radiometry is distinct from quantum techniques such as photon counting.\nThe use of radiometers to determine the temperature of objects and gasses by measuring radiation flux is called pyrometry. Handheld pyrometer devices are often marketed as infrared thermometers.\nRadiometry is important in astronomy, especially radio astronomy, and plays a significant role in Earth remote sensing. The measurement techniques categorized as \"radiometry\" in optics are called \"photometry\" in some astronomical applications, contrary to the optics usage of the term.\nSpectroradiometry is the measurement of absolute radiometric quantities in narrow bands of wavelength.\nRadiometric quantities.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nIntegral and spectral radiometric quantities.\nIntegral quantities (like radiant flux) describe the total effect of radiation of all wavelengths or frequencies, while spectral quantities (like spectral power) describe the effect of radiation of a single wavelength \u03bb or frequency \u03bd. To each integral quantity there are corresponding spectral quantities, defined as the quotient of the integrated quantity by the range of frequency or wavelength considered. For example, the radiant flux \u03a6e corresponds to the spectral power \u03a6e,\u03bb and \u03a6e,\u03bd.\nGetting an integral quantity's spectral counterpart requires a limit transition. This comes from the idea that the precisely requested wavelength photon existence probability is zero. Let us show the relation between them using the radiant flux as an example:\nIntegral flux, whose unit is W:\nformula_1\nSpectral flux by wavelength, whose unit is W/m:\nformula_2\nwhere formula_3 is the radiant flux of the radiation in a small wavelength interval formula_4.\nThe area under a plot with wavelength horizontal axis equals to the total radiant flux.\nSpectral flux by frequency, whose unit is W/Hz:\nformula_5\nwhere formula_3 is the radiant flux of the radiation in a small frequency interval formula_7.\nThe area under a plot with frequency horizontal axis equals to the total radiant flux.\nThe spectral quantities by wavelength \u03bb and frequency \u03bd are related to each other, since the product of the two variables is the speed of light (formula_8):\nformula_9 or formula_10 or formula_11\nThe integral quantity can be obtained by the spectral quantity's integration:\nformula_12\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41626", "revid": "47023628", "url": "https://en.wikipedia.org/wiki?curid=41626", "title": "Randomizer", "text": "Randomizer may refer to:\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "41627", "revid": "49117425", "url": "https://en.wikipedia.org/wiki?curid=41627", "title": "Random number", "text": "Number generated by a random process\nA random number is generated by a random (stochastic) process such as throwing dice. Individual numbers cannot be predicted, but the likely result of generating a large quantity of numbers can be predicted by specific mathematical series and statistics.\nAlgorithms and implementations.\nRandom numbers are frequently used in algorithms such as Knuth's 1964-developed algorithm for shuffling lists. (popularly known as \"the Knuth shuffle\" or \"the Fisher\u2013Yates shuffle\", based on work they did in 1938).\nIn 1999, a new feature was added to the Pentium III: a hardware-based random number generator. It has been described as \"several oscillators combine their outputs and that odd waveform is sampled asynchronously.\" These numbers, however, were only 32 bit, at a time when export controls were on 56 bits and higher, so they were not state of the art.\nCommon understanding.\nIn common understanding, \"1 2 3 4 5\" is not as random as \"3 5 2 1 4\" and certainly not as random as \"47 88 1 32 41\" but \"we can't say authoritavely that the first sequence is not random ... it could have been generated by chance.\"\nWhen a police officer claims to have done a \"random .. door-to-door\" search, there is a certain expectation that members of a jury will have.\nReal world consequences.\nFlaws in \"randomness\" have real-world consequences.\nA 99.8% randomness was shown by researchers to negatively affect an estimated 27,000 customers of a large service and that the problem was not limited to just that situation.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41628", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41628", "title": "Receive-after-transmit time delay", "text": "In telecommunications, receive-after-transmit time delay is the time interval between (a) the instant of keying off the local transmitter to stop transmitting and (b) the instant the local receiver output has increased to 90% of its steady-state value in response to an RF signal from another transmitter. \nThe RF signal from the distant transmitter must exist at the local receiver input prior to, or at the time of, keying off the local transmitter. \nReceive-after-transmit time delay applies only to half-duplex operation.\nReferences.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41629", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41629", "title": "Received noise power", "text": "In telecommunications, received noise power is a measure of noise in a receiver. For example, the received noise power might be:\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41630", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41630", "title": "Attack-time delay", "text": "Time for a receiver or transmitter to respond to a signal\nIn telecommunications, attack-time delay is the time needed for a receiver or transmitter to respond to an incoming signal.\nFor a receiver, the attack-time delay is defined as the time interval from the instant a step radio-frequency signal, at a level equal to the receiver's threshold of sensitivity, is applied to the receiver input, to the instant when the receiver's output amplitude reaches 90% of its steady-state value. If a squelch circuit is operating, the receiver attack-time delay includes the time for the receiver to break squelch. \nFor a transmitter, the attack-time delay is defined as the interval from the instant the transmitter is keyed-on to the instant the transmitted radio-frequency signal amplitude has increased to a specified level, usually 90% of its key-on steady-state value. The transmitter attack-time delay excludes the time required for automatic antenna tuning.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41633", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41633", "title": "Recorder warning tone", "text": "Type of call-progress tone\nA recorder warning tone is a tone transmitted over a telephone line to indicate to the called party that the calling party is recording the conversation.\nIn the United States, the recorder warning tone is a half-second burst of 1400 Hz applied every 15 seconds. The recorder warning tone is required by law to be generated as an integral part of any recording device used for the purpose of recording telephone calls and is required to be not under the control of the calling party. The tone is recorded together with the conversation."}
{"id": "41634", "revid": "1256358", "url": "https://en.wikipedia.org/wiki?curid=41634", "title": "Record medium", "text": ""}
{"id": "41635", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41635", "title": "Recovery procedure", "text": "Process that attempts to bring a system back to a normal operating state\nIn telecommunications, a recovery procedure is a process that attempts to bring a system back to a normal operating state. Examples:"}
{"id": "41636", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41636", "title": "Reference circuit", "text": "Hypothetical electric circuit\nA reference circuit is a hypothetical electric circuit of specified equivalent length and configuration, and having a defined transmission characteristic or characteristics, used primarily as a reference for measuring the performance of other, \"i.e.,\" real, circuits or as a guide for planning and engineering of circuits and networks. \nNormally, several types of reference circuits are defined, with different configurations, because communications are required over a wide range of distances. Another type of reference circuit shows how to configure integrated circuits into function blocks, which Analog Devices provides for electrical design engineers. Analog Devices' Circuits from the Lab reference circuits are fully tested and come with the schematics, evaluation boards, and device drivers necessary for system integration. A group of related reference circuits is also called a \"reference system\"."}
{"id": "41637", "revid": "22883165", "url": "https://en.wikipedia.org/wiki?curid=41637", "title": "Reference clock", "text": "A reference clock may refer to the following:\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "41638", "revid": "35839912", "url": "https://en.wikipedia.org/wiki?curid=41638", "title": "Cycloid", "text": "Curve traced by a point on a rolling circle\nIn geometry, a cycloid is the curve traced by a point on a circle as it rolls along a straight line without slipping. A cycloid is a specific form of trochoid and is an example of a roulette, a curve generated by a curve rolling on another curve.\nThe cycloid, with the cusps pointing upward, is the curve of fastest descent under uniform gravity (the brachistochrone curve). It is also the form of a curve for which the period of an object in simple harmonic motion (rolling up and down repetitively) along the curve does not depend on the object's starting position (the tautochrone curve). In physics, when a charged particle at rest is put under a uniform electric and magnetic field perpendicular to one another, the particle\u2019s trajectory draws out a cycloid.\nHistory.\n&lt;templatestyles src=\"Template:Quote_box/styles.css\" /&gt;\nIt was in the left hand try-pot of the Pequod, with the soapstone diligently circling round me, that I was first indirectly struck by the remarkable fact, that in geometry all bodies gliding along the cycloid, my soapstone for example, will descend from any point in precisely the same time.\n\"Moby Dick\" by Herman Melville, 1851\nThe cycloid has been called \"The Helen of Geometers\" as, like Helen of Troy, it caused frequent quarrels among 17th-century mathematicians, while Sarah Hart sees it named as such \"because the properties of this curve are so beautiful\".\nHistorians of mathematics have proposed several candidates for the discoverer of the cycloid. Mathematical historian Paul Tannery speculated that such a simple curve must have been known to the ancients, citing similar work by Carpus of Antioch described by Iamblichus. English mathematician John Wallis writing in 1679 attributed the discovery to Nicholas of Cusa, but subsequent scholarship indicates that either Wallis was mistaken or the evidence he used is now lost. Galileo Galilei's name was put forward at the end of the 19th century and at least one author reports credit being given to Marin Mersenne. Beginning with the work of Moritz Cantor and Siegmund G\u00fcnther, scholars now assign priority to French mathematician Charles de Bovelles based on his description of the cycloid in his \"Introductio in geometriam\", published in 1503. In this work, Bovelles mistakes the arch traced by a rolling wheel as part of a larger circle with a radius 120% larger than the smaller wheel.\nGalileo originated the term \"cycloid\" and was the first to make a serious study of the curve. According to his student Evangelista Torricelli, in 1599 Galileo attempted the quadrature of the cycloid (determining the area under the cycloid) with an unusually empirical approach that involved tracing both the generating circle and the resulting cycloid on sheet metal, cutting them out and weighing them. He discovered the ratio was roughly 3:1, which is the true value, but he incorrectly concluded the ratio was an irrational fraction, which would have made quadrature impossible. Around 1628, Gilles Persone de Roberval likely learned of the quadrature problem from P\u00e8re Marin Mersenne and effected the quadrature in 1634 by using Cavalieri's Theorem. However, this work was not published until 1693 (in his \"Trait\u00e9 des Indivisibles\").\nConstructing the tangent of the cycloid dates to August 1638 when Mersenne received unique methods from Roberval, Pierre de Fermat and Ren\u00e9 Descartes. Mersenne passed these results along to Galileo, who gave them to his students Torricelli and Viviani, who were able to produce a quadrature. This result and others were published by Torricelli in 1644, which is also the first printed work on the cycloid. This led to Roberval charging Torricelli with plagiarism, with the controversy cut short by Torricelli's early death in 1647.\nIn 1658, Blaise Pascal had given up mathematics for theology but, while suffering from a toothache, began considering several problems concerning the cycloid. His toothache disappeared, and he took this as a heavenly sign to proceed with his research. Eight days later he had completed his essay and, to publicize the results, proposed a contest. Pascal proposed three questions relating to the center of gravity, area and volume of the cycloid, with the winner or winners to receive prizes of 20 and 40 Spanish doubloons. Pascal, Roberval and Senator Carcavy were the judges, and neither of the two submissions (by John Wallis and Antoine de Lalouv\u00e8re) was judged to be adequate. While the contest was ongoing, Christopher Wren sent Pascal a proposal for a proof of the rectification of the cycloid; Roberval claimed promptly that he had known of the proof for years. Wallis published Wren's proof (crediting Wren) in Wallis's \"Tractatus Duo\", giving Wren priority for the first published proof.\nFifteen years later, Christiaan Huygens had deployed the cycloidal pendulum to improve chronometers and had discovered that a particle would traverse a segment of an inverted cycloidal arch in the same amount of time, regardless of its starting point. In 1686, Gottfried Wilhelm Leibniz used analytic geometry to describe the curve with a single equation. In 1696, Johann Bernoulli posed the brachistochrone problem, the solution of which is a cycloid.\nEquations.\nThe cycloid through the origin, generated by a circle of radius r rolling over the \"x-\"axis on the positive side (\"y\" \u2265 0), consists of the points (\"x\", \"y\"), with\nformula_1\nwhere t is a real parameter corresponding to the angle through which the rolling circle has rotated. For given t, the circle's centre lies at (\"x\", \"y\") = (\"rt\", \"r\").\nThe Cartesian equation is obtained by solving the \"y\"-equation for t and substituting into the \"x-\"equation:formula_2or, eliminating the multiple-valued inverse cosine:formula_3When y is viewed as a function of x, the cycloid is differentiable everywhere except at the cusps on the x-axis, with the derivative tending toward formula_4 or formula_5 near a cusp (where y=0). The map from t to (\"x\", \"y\") is differentiable, in fact of class C\u221e, with derivative 0 at the cusps.\nThe slope of the tangent to the cycloid at the point formula_6 is given by formula_7.\nA cycloid segment from one cusp to the next is called an arch of the cycloid, for example the points with formula_8 and formula_9.\nConsidering the cycloid as the graph of a function formula_10, it satisfies the differential equation:\nformula_11\nIf we define formula_12 as the height difference from the cycloid's vertex (the point with a horizontal tangent and formula_13), then we have:\nformula_14\nInvolute.\nThe involute of the cycloid has exactly the same shape as the cycloid it originates from. This can be visualized as the path traced by the tip of a wire initially lying on a half arch of the cycloid: as it unrolls while remaining tangent to the original cycloid, it describes a new cycloid (see also cycloidal pendulum and arc length).\nDemonstration.\nThis demonstration uses the rolling-wheel definition of cycloid, as well as the instantaneous velocity vector of a moving point, tangent to its trajectory. In the adjacent picture, formula_15 and formula_16 are two points belonging to two rolling circles, with the base of the first just above the top of the second. Initially, formula_15 and formula_16 coincide at the intersection point of the two circles. When the circles roll horizontally with the same speed, formula_15 and formula_16 traverse two cycloid curves. Considering the red line connecting formula_15 and formula_16 at a given time, one proves \"the line is always\" \"tangent to the lower arc at formula_16 and orthogonal to the upper arc at formula_15\". Let formula_25 be the point in common between the upper and lower circles at the given time. Then:\nMeasurement.\nArea.\nUsing the above parameterization formula_82 and formula_83, the area under one arch, formula_84 is given by:\nformula_85\nThis is three times the area of the rolling circle.\nArc length.\nThe arc length S of one arch is given by\nformula_86\nAnother geometric way to calculate the length of the cycloid is to notice that when a wire describing an involute has been completely unwrapped from half an arch, it extends itself along two diameters, a length of 4\"r\". This is thus equal to half the length of arch, and that of a complete arch is 8\"r\".\nFrom the cycloid's vertex (the point with a horizontal tangent and formula_13) to any point within the same arch, the arc length squared is formula_88, which is proportional to the height difference formula_89; this property is the basis for the cycloid's isochronism. In fact, the arc length squared is equal to the height difference multiplied by the full arch length 8\"r\".\nCycloidal pendulum.\nIf a simple pendulum is suspended from the cusp of an inverted cycloid, such that the string is constrained to be tangent to one of its arches, and the pendulum's length \"L\" is equal to that of half the arc length of the cycloid (i.e., twice the diameter of the generating circle, \"L = 4r\"), the bob of the pendulum also traces a cycloid path. Such a pendulum is isochronous, with equal-time swings regardless of amplitude. Introducing a coordinate system centred in the position of the cusp, the equation of motion is given by:\nformula_90\nwhere formula_91 is the angle that the straight part of the string makes with the vertical axis, and is given by\nformula_92\nwhere \"A\" &lt; 1 is the \"amplitude\", formula_93 is the radian frequency of the pendulum and \"g\" the gravitational acceleration.\nThe 17th-century Dutch mathematician Christiaan Huygens discovered and proved these properties of the cycloid while searching for more accurate pendulum clock designs to be used in navigation.\nRelated curves.\nSeveral curves are related to the cycloid.\nAll these curves are roulettes with a circle rolled along another curve of uniform curvature. The cycloid, epicycloids, and hypocycloids have the property that each is similar to its evolute. If \"q\" is the product of that curvature with the circle's radius, signed positive for epi- and negative for hypo-, then the similitude ratio of curve to evolute is 1\u00a0+\u00a02\"q\".\nThe classic Spirograph toy traces out hypotrochoid and epitrochoid curves.\nOther uses.\nThe cycloidal arch was used by architect Louis Kahn in his design for the Kimbell Art Museum in Fort Worth, Texas. It was also used by Wallace K. Harrison in the design of the Hopkins Center at Dartmouth College in Hanover, New Hampshire.\nEarly research indicated that some transverse arching curves of the plates of golden age violins are closely modeled by curtate cycloid curves. Later work indicates that curtate cycloids do not serve as general models for these curves, which vary considerably.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41639", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41639", "title": "Reference noise", "text": "In telecommunications, reference noise is the magnitude of circuit noise chosen as a reference for measurement. \nMany different levels with a number of different weightings are in current use, and care must be taken to ensure that the proper parameters are stated. \nSpecific ones include: dBa, dBa(F1A), dBa(HA1), dBa0, dBm, dBm(psoph), dBm0, dBrn, dBrnC, dBrnC0, dBrn(f1-f2), dBrn(144-line), dBx."}
{"id": "41640", "revid": "6727347", "url": "https://en.wikipedia.org/wiki?curid=41640", "title": "Reference surface", "text": "In fiber optic technology, a reference surface is that surface of an optical fiber that is used to contact the transverse-alignment elements of a component such as a connector or mechanical splice. For telecommunications-grade fibers, the reference surface is the outer surface of the cladding. For plastic-clad silica (PCS) fibers, which have a strippable polymer cladding (not to be confused with the polymer overcoat of an all-silica fiber), the reference surface may be the core."}
{"id": "41641", "revid": "3125232", "url": "https://en.wikipedia.org/wiki?curid=41641", "title": "Reflection coefficient", "text": "Measure of wave reflectivity\nIn physics and electrical engineering the reflection coefficient is a parameter that describes how much of a wave is reflected by an impedance discontinuity in the transmission medium. It is equal to the ratio of the amplitude of the reflected wave to the incident wave, with each expressed as phasors. For example, it is used in optics to calculate the amount of light that is reflected from a surface with a different index of refraction, such as a glass surface, or in an electrical transmission line to calculate how much of the electromagnetic wave is reflected by an impedance discontinuity. The reflection coefficient is closely related to the \"transmission coefficient\". The reflectance of a system is also sometimes called a reflection coefficient.\nDifferent disciplines have different applications for the term.\nTransmission lines.\nIn telecommunications and transmission line theory, the reflection coefficient is the ratio of the complex amplitude of the reflected wave to that of the incident wave. The voltage and current at any point along a transmission line can always be resolved into forward and reflected traveling waves given a specified reference impedance \"Z0\". The reference impedance used is typically the characteristic impedance of a transmission line that's involved, but one can speak of reflection coefficient without any actual transmission line being present. In terms of the forward and reflected waves determined by the voltage and current, the reflection coefficient is defined as the complex ratio of the voltage of the reflected wave (formula_1) to that of the incident wave (formula_2). This is typically represented with a formula_3 (capital gamma) and can be written as:\nformula_4\nIt can also be defined using the \"currents\" associated with the reflected and forward waves, but introducing a minus sign to account for the opposite orientations of the two currents:\nformula_5\nThe reflection coefficient may also be established using other field or circuit pairs of quantities whose product defines power resolvable into a forward and reverse wave. With electromagnetic plane waves, one uses the ratio of the electric fields of the reflected to that of the incident wave (or magnetic fields, again with a minus sign); the ratio of each wave's electric field \"E\" to its magnetic field \"H\" is the medium's characteristic impedance, formula_6, (equal to the impedance of free space if the medium is a vacuum).\nIn the accompanying figure, a signal source with internal impedance formula_7 possibly followed by a transmission line of characteristic impedance formula_7 is represented by its Th\u00e9venin equivalent, driving the load formula_9. For a real (resistive) source impedance formula_7, if we define formula_3 using the reference impedance formula_12 then the source's maximum power is delivered to a load formula_13, in which case formula_14 implying no reflected power. More generally, the squared-magnitude of the reflection coefficient formula_15 denotes the proportion of that power that is reflected back to the source, with the power actually delivered toward the load being formula_16.\nAnywhere along an intervening (lossless) transmission line of characteristic impedance formula_6, the magnitude of the reflection coefficient formula_18 will remain the same (the powers of the forward and reflected waves stay the same) but with a different phase. In the case of a short circuited load (formula_19), one finds formula_20 at the load. This implies the reflected wave having a 180\u00b0 phase shift (phase reversal) with the voltages of the two waves being opposite at that point and adding to zero (as a short circuit demands).\nRelation to load impedance.\nThe reflection coefficient is determined by the load impedance at the end of the transmission line, as well as the characteristic impedance of the line. A load impedance of formula_9 terminating a line with a characteristic impedance of formula_22 will have a reflection coefficient of\nformula_23\nThis is the coefficient at the load. The reflection coefficient can also be measured at other points on the line. The \"magnitude\" of the reflection coefficient in a lossless transmission line is constant along the line (as are the powers in the forward and reflected waves). However its \"phase\" will be shifted by an amount dependent on the electrical distance formula_24 from the load. If the coefficient is measured at a point formula_25 meters from the load, so the electrical distance from the load is formula_26 radians, the coefficient formula_27 at that point will be \n formula_28\nNote that the phase of the reflection coefficient is changed by \"twice\" the phase length of the attached transmission line. That is to take into account not only the phase delay of the reflected wave, but the phase shift that had first been applied to the forward wave, with the reflection coefficient being the quotient of these. The reflection coefficient so measured, formula_27, corresponds to an impedance which is generally dissimilar to formula_9 present at the far side of the transmission line.\nThe complex reflection coefficient (in the region formula_31, corresponding to passive loads) may be displayed graphically using a Smith chart. The Smith chart is a polar plot of formula_3, therefore the magnitude of formula_3 is given directly by the distance of a point to the center (with the edge of the Smith chart corresponding to formula_34). Its evolution along a transmission line is likewise described by a rotation of formula_35 around the chart's center. Using the scales on a Smith chart, the resulting impedance (normalized to formula_6) can directly be read. Before the advent of modern electronic computers, the Smith chart was of particular use as a sort of analog computer for this purpose.\nThe reflected power in terms of the reflection coefficient is:\nformula_37 .\nStanding wave ratio.\nThe standing wave ratio (SWR) is determined solely by the \"magnitude\" of the reflection coefficient:\nformula_38\nAlong a lossless transmission line of characteristic impedance \"Z\"0, the SWR signifies the ratio of the voltage (or current) maxima to minima (or what it would be if the transmission line were long enough to produce them). The above calculation assumes that formula_3 has been calculated using \"Z\"0 as the reference impedance. Since it uses only the \"magnitude\" of formula_3, the SWR intentionally ignores the specific value of the load impedance \"ZL\" responsible for it, but only the magnitude of the resulting impedance mismatch. That SWR remains the same wherever measured along a transmission line (looking towards the load) since the addition of a transmission line length to a load formula_9 only changes the phase, not magnitude of formula_3. While having a one-to-one correspondence with reflection coefficient, SWR is the most commonly used figure of merit in describing the mismatch affecting a radio antenna or antenna system. It is most often measured at the transmitter side of a transmission line, but having, as explained, the same value as would be measured at the antenna (load) itself.\nElectrical networks.\nA transmission line is an example of a 2-port electrical network, but reflection coefficients are useful in the analysis of any electrical networks. A reflection coefficient for each port in the same way as for the boundary of a transmission line. It will, however, also depend on the properties of connections at other ports and so is not a property intrinsic to the network itself. For a 2-port network with the 2x2 scattering matrix \"S\", and with a source and load connected to its input and output, where the reflections off the source back into the input are formula_43 and the reflections off the load back into the output are formula_44, then the reflection coefficients at the input and output are given by:\nformula_45 and formula_46\nSeismology.\nReflection coefficient is used in feeder testing for reliability of medium.\nOptics and microwaves.\nIn optics and electromagnetics in general, \"reflection coefficient\" can refer to either the amplitude reflection coefficient described here, or the reflectance, depending on context. Typically, the reflectance is represented by a capital \"R\", while the amplitude reflection coefficient is represented by a lower-case \"r\". These related concepts are covered by Fresnel equations in classical optics.\nAcoustics.\nAcousticians use reflection coefficients to understand the effect of different materials on their acoustic environments. The field properties used to define the reflection coefficient are typically the acoustic pressure and velocity in the incident and reflected acoustic waves.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41642", "revid": "3727527", "url": "https://en.wikipedia.org/wiki?curid=41642", "title": "Reflection loss", "text": ""}
{"id": "41643", "revid": "44487672", "url": "https://en.wikipedia.org/wiki?curid=41643", "title": "Reflective array antenna", "text": "Class of directive antennas\nIn telecommunications and radar, a reflective array antenna is a class of directive antennas in which multiple driven elements are mounted in front of a flat surface designed to reflect the radio waves in a desired direction. They are a type of array antenna. They are often used in the VHF and UHF frequency bands. VHF examples are generally large and resemble a highway billboard, so they are sometimes called billboard antennas. Other names are bedspring array and bowtie array depending on the type of elements making up the antenna. The curtain array is a larger version used by shortwave radio broadcasting stations.\nReflective array antennas usually have a number of identical driven elements, fed in phase, in front of a flat, electrically large reflecting surface to produce a unidirectional beam of radio waves, increasing antenna gain and reducing radiation in unwanted directions. The larger the number of elements used, the higher the gain; the narrower the beam is and the smaller the sidelobes are. The individual elements are most commonly half wave dipoles, although they sometimes contain parasitic elements as well as driven elements. The reflector may be a metal sheet or more commonly a wire screen. A metal screen reflects radio waves as well as a solid metal sheet as long as the holes in the screen are smaller than about one-tenth of a wavelength, so screens are often used to reduce weight and wind loads on the antenna. They usually consist of a grill of parallel wires or rods, oriented parallel to the axis of the dipole elements.\nThe driven elements are fed by a network of transmission lines, which divide the power from the RF source equally between the elements. This often has the circuit geometry of a tree structure.\nBasic concepts.\nRadio signals.\nWhen a radio signal passes a conductor, it induces an electrical current in it. Since the radio signal fills space, and the conductor has a finite size, the induced currents add up or cancel out as they move along the conductor. A basic goal of antenna design is to make the currents add up to a maximum at the point where the energy is tapped off. To do this, the antenna elements are sized in relation to the wavelength of the radio signal, with the aim of setting up standing waves of current that are maximized at the feed point.\nThis means that an antenna designed to receive a particular wavelength has a natural size. To improve reception, one cannot simply make the antenna larger; this will improve the amount of signal intercepted by the antenna, which is largely a function of area, but will lower the efficiency of the reception (at a given wavelength). Thus, in order to improve reception, antenna designers often use multiple elements, combining them together so their signals add up. These are known as \"antenna arrays\".\nArray phasing.\nIn order for the signals to add together, they need to arrive in-phase. Consider two dipole antennas placed in a line end-to-end, or \"collinear\". If the resulting array is pointed directly at the source signal, both dipoles will see the same instantaneous signal, and thus their reception will be in-phase. However, if one were to rotate the antenna so it was at an angle to the signal, the extra path from the signal to the more distant dipole means it receives the signal slightly out of phase. When the two signals are then added up, they no longer strictly reinforce each other, and the output drops. This makes the array more sensitive horizontally, while stacking the dipoles in parallel narrows the pattern vertically. This allows the designer to tailor the reception pattern, and thus the gain, by moving the elements about.\nIf the antenna is properly aligned with the signal, at any given instant in time, all of the elements in an array will receive the same signal and be in-phase. However, the output from each element has to be gathered up at a single feed point, and as the signals travel across the antenna to that point, their phase is changing. In a two-element array this is not a problem because the feed point can be placed between them; any phase shift taking place in the transmission lines is equal for both elements. However, if one extends this to a four-element array, this approach no longer works, as the signal from the outer pair has to travel further and will thus be at a different phase than the inner pair when it reaches the center. To ensure that they all arrive with the same phase, it is common to see additional transmission wire inserted in the signal path, or for the transmission line to be crossed over to reverse the phase if the difference is greater than &lt;templatestyles src=\"Fraction/styles.css\" /&gt;1\u20442 a wavelength.\nReflectors.\nThe gain can be further improved through the addition of a \"reflector\". Generally any conductor in a flat sheet will act in a mirror-like fashion for radio signals, but this also holds true for non-continuous surfaces as long as the gaps between the conductors are less than about &lt;templatestyles src=\"Fraction/styles.css\" /&gt;1\u204410 of the target wavelength. This means that wire mesh or even parallel wires or metal bars can be used, which is especially useful both for reducing the total amount of material as well as reducing wind loads.\nDue to the change in signal propagation direction on reflection, the signal undergoes a reversal of phase. In order for the reflector to add to the output signal, it has to reach the elements in-phase. Generally this would require the reflector to be placed at &lt;templatestyles src=\"Fraction/styles.css\" /&gt;1\u20444 of a wavelength behind the elements, and this can be seen in many common reflector arrays like television antennas. However, there are a number of factors that can change this distance, and actual reflector positioning varies.\nReflectors also have the advantage of reducing the signal received from the back of the antenna. Signals received from the rear and re-broadcast from the reflector have not undergone a change of phase, and do not add to the signal from the front. This greatly improves the front-to-back ratio of the antenna, making it more directional. This can be useful when a more directional signal is desired, or unwanted signals are present. There are cases when this is not desirable, and although reflectors are commonly seen in array antennas, they are not universal. For instance, while UHF television antennas often use an array of bowtie antennas with a reflector, a bowtie array without a reflector is a relatively common design in the microwave region.\nGain limits.\nAs more elements are added to an array, the beamwidth of the antenna's main lobe decreases, leading to an increase in gain. In theory there is no limit to this process. However, as the number of elements increases, the complexity of the required feed network that keeps the signals in-phase increases. Ultimately, the rising inherent losses in the feed network become greater than the additional gain achieved with more elements, limiting the maximum gain that can be achieved.\n The gain of practical array antennas is limited to about 25\u201330\u00a0dB. Two half wave elements spaced a half wave apart and a quarter wave from a reflecting screen have been used as a standard gain antenna with about 9.8 dBi at its design frequency. Common 4-bay television antennas have gains around 10 to 12\u00a0dB, and 8-bay designs might increase this to 12 to 16\u00a0dB. The 32-element SCR-270 had a gain around 19.8\u00a0dB. Some very large reflective arrays have been constructed, notably the Soviet Duga radars which are hundreds of meters (yards) across and contain hundreds of elements. \"Active\" array antennas, in which groups of elements are driven by separate RF amplifiers, can have much higher gain, but are prohibitively expensive.\nSince the 1980s, versions for use at microwave frequencies have been made with patch antenna elements mounted in front of a metal surface.\nRadiation pattern and beam steering.\nWhen driven in phase, the radiation pattern of the reflective array is a single main lobe perpendicular to the plane of the antenna, plus several sidelobes at equal angles to either side. The more elements used, the narrower the main lobe and the less power is radiated in the sidelobes.\nThe main lobe of the antenna can be steered electronically within a limited angle by phase shifting the drive signals applied to the individual elements. Each antenna element is fed through a phase shifter which can be controlled digitally, delaying each signal by a successive amount. This causes the wavefronts created by the superposition of the individual elements to be at an angle to the plane of the antenna. Antennas that use this technique are called phased arrays and often used in modern radar systems. \nAnother option for steering the beam is mounting the entire antenna structure on a pivoting platform and rotating it mechanically.\nReferences.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41644", "revid": "9755426", "url": "https://en.wikipedia.org/wiki?curid=41644", "title": "Reflectance", "text": "Capacity of an object to reflect light\nThe reflectance of the surface of a material is its effectiveness in reflecting radiant energy. It is the fraction of incident electromagnetic power that is reflected at the boundary. Reflectance is a component of the response of the electronic structure of the material to the electromagnetic field of light, and is in general a function of the frequency, or wavelength, of the light, its polarization, and the angle of incidence. The dependence of reflectance on the wavelength is called a \"reflectance spectrum\" or \"spectral reflectance curve\".\nMathematical definitions.\nHemispherical reflectance.\nThe \"hemispherical reflectance\" of a surface, denoted R, is defined as\nformula_1\nwhere \u03a6er is the radiant flux \"reflected\" by that surface and \u03a6ei is the radiant flux \"received\" by that surface.\nSpectral hemispherical reflectance.\nThe \"spectral hemispherical reflectance in frequency\" and \"spectral hemispherical reflectance in wavelength\" of a surface, denoted \"R\"\"\u03bd\" and \"R\"\"\u03bb\" respectively, are defined as\nformula_2\nformula_3\nwhere\nDirectional reflectance.\nThe \"directional reflectance\" of a surface, denoted \"R\"\u03a9, is defined as\nformula_4\nwhere\nThis depends on both the reflected direction and the incoming direction. In other words, it has a value for every combination of incoming and outgoing directions. It is related to the bidirectional reflectance distribution function and its upper limit is 1. Another measure of reflectance, depending only on the outgoing direction, is \"I\"/\"F\", where \"I\" is the radiance reflected in a given direction and \"F\" is the incoming radiance averaged over all directions, in other words, the total flux of radiation hitting the surface per unit area, divided by \u03c0. This can be greater than 1 for a glossy surface illuminated by a source such as the sun, with the reflectance measured in the direction of maximum radiance (see also Seeliger effect).\nSpectral directional reflectance.\nThe \"spectral directional reflectance in frequency\" and \"spectral directional reflectance in wavelength\" of a surface, denoted \"R\"\u03a9,\"\u03bd\" and \"R\"\u03a9,\"\u03bb\" respectively, are defined as\nformula_5\nformula_6\nwhere\nAgain, one can also define a value of \"I\"/\"F\" (see above) for a given wavelength.\nReflectivity.\nFor homogeneous and semi-infinite (see halfspace) materials, reflectivity is the same as reflectance. \nReflectivity is the square of the magnitude of the Fresnel reflection coefficient,\nwhich is the ratio of the reflected to incident electric field; \nas such the reflection coefficient can be expressed as a complex number as determined by the Fresnel equations for a single layer, whereas the reflectance is always a positive real number.\nFor layered and finite media, according to the CIE, reflectivity is distinguished from \"reflectance\" by the fact that reflectivity is a value that applies to \"thick\" reflecting objects. When reflection occurs from thin layers of material, internal reflection effects can cause the reflectance to vary with surface thickness. Reflectivity is the limit value of reflectance as the sample becomes thick; it is the intrinsic reflectance of the surface, hence irrespective of other parameters such as the reflectance of the rear surface. Another way to interpret this is that the reflectance is the fraction of electromagnetic power reflected from a specific sample, while reflectivity is a property of the material itself, which would be measured on a perfect machine if the material filled half of all space.\nSurface type.\nGiven that reflectance is a directional property, most surfaces can be divided into those that give specular reflection and those that give diffuse reflection.\nFor specular surfaces, such as glass or polished metal, reflectance is nearly zero at all angles except at the appropriate reflected angle; that is the same angle with respect to the surface normal in the plane of incidence, but on the opposing side. When the radiation is incident normal to the surface, it is reflected back into the same direction.\nFor diffuse surfaces, such as matte white paint, reflectance is uniform; radiation is reflected in all angles equally or near-equally. Such surfaces are said to be Lambertian.\nMost practical objects exhibit a combination of diffuse and specular reflective properties.\nLiquid reflectance.\nReflection of light occurs at a boundary at which the index of refraction changes. Specular reflection is calculated by the Fresnel equations. Fresnel reflection is directional and therefore does not contribute significantly to albedo which primarily diffuses reflection.\nA liquid surface may be wavy. Reflectance may be adjusted to account for waviness.\nGrating efficiency.\nThe generalization of reflectance to a diffraction grating, which disperses light by wavelength, is called \"diffraction efficiency\".\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41645", "revid": "252195", "url": "https://en.wikipedia.org/wiki?curid=41645", "title": "Refracted ray", "text": ""}
{"id": "41646", "revid": "7903804", "url": "https://en.wikipedia.org/wiki?curid=41646", "title": "Refractive index contrast", "text": "Refractive index contrast, in an optical waveguide, such as an optical fiber, is a measure of the relative difference in refractive index of the core and cladding. The refractive index contrast, \u0394, is often given by formula_1, where \"n\"1 is the maximum refractive index in the core (or simply the core index for a step-index profile) and \"n\"2 is the refractive index of the cladding. The criterion \"n\"2 &lt; \"n\"1 must be satisfied in order to sustain a guided mode by total internal reflection. Alternative formulations include formula_2 and formula_3. Normal optical fibers, constructed of different glasses, have very low refractive index contrast (\u0394\u00ab1) and hence are weakly-guiding. The weak guiding will cause a greater portion of the cross-sectional Electric field profile to reside within the cladding (as evanescent tails of the guided mode) as compared to strongly-guided waveguides. Integrated optics can make use of higher core index to obtain \u0394&gt;1 allowing light to be efficiently guided around corners on the micro-scale, where popular high-\u0394 material platform is silicon-on-insulator. High-\u0394 allows sub-wavelength core dimensions and so greater control over the size of the evanescent tails. The most efficient low-loss optical fibers require low \u0394 to minimise losses to light scattered outwards.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41647", "revid": "12109628", "url": "https://en.wikipedia.org/wiki?curid=41647", "title": "Reframing time", "text": ""}
{"id": "41648", "revid": "16673431", "url": "https://en.wikipedia.org/wiki?curid=41648", "title": "Regeneration", "text": "Regeneration may refer to:\n&lt;templatestyles src=\"Template:TOC_right/styles.css\" /&gt;\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "41649", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41649", "title": "Relative transmission level", "text": "In telecommunications, relative transmission level is the ratio of the signal power, at a given point in a transmission system, to a reference signal power. \nThe ratio is usually determined by applying a standard test tone at zero transmission level point (or applying adjusted test tone power at any other point) and measuring the gain or loss to the location of interest. A distinction should be made between the standard test tone power and the expected median power of the actual signal required as the basis for the design of transmission systems.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41650", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41650", "title": "Release time (telecommunications)", "text": "Time interval in telecommunication theory\nIn telecommunications, release time is the time interval for a circuit to respond when an enabling signal is discontinued, for example:"}
{"id": "41651", "revid": "6032993", "url": "https://en.wikipedia.org/wiki?curid=41651", "title": "Reliability", "text": "Reliability, reliable, or unreliable may refer to:\n&lt;templatestyles src=\"Template:TOC_right/styles.css\" /&gt;\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "41652", "revid": "7098284", "url": "https://en.wikipedia.org/wiki?curid=41652", "title": "Remote access", "text": "Remote access may refer to:\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "41653", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41653", "title": "Remote call forwarding", "text": "In telecommunications, remote call forwarding is a service feature that allows incoming calls to be forwarded to a remote call forwarding number, such as a cell phone or another office location, and is designated by the call receiver.\nCustomers may have a remote-forwarding telephone number in a central switching office without having any other local telephone service in that office.\nOne common purpose for this service is to enable customers to retain their telephone number when they move to a location serviced by a different telephone exchange. The service is useful for business customers with widely advertised numbers which appear on headed paper, vehicles and various marketing literature. When customers ring, their calls are forwarded to the new location.\nRemote call forwarding is also a means for a suburban business to obtain a city-centre local number (with its full large-city coverage area) for inbound calls; while cheaper than a foreign exchange line, this can reduce long-distance telephony costs in markets where local calls are flat-rated but trunk calls are expensive.\nOne alternative to RCF is Caller Redirect whereby callers simply hear an intercept message notifying them that the number has changed. Another alternative is to port the existing number to a voice over IP carrier, which is not tied to a single physical location as the subscriber may be anywhere on broadband Internet. However, not all phone numbers can be ported.\nRemote Access to Call Forwarding.\nRemote Call Forwarding (RCF) requires neither a physical telephone set nor physical input by customer to get calls forwarded.\nIn this respect, it differs from the (similarly named) Remote Access to Call Forwarding in that the number is attached to a physical line where it rings normally until a call is made to a remote number to enable redirection.\nTo activate Remote Access to Call Forwarding, a subscriber calls a provider-supplied Remote Access Directory Number, enters the telephone number of the line to be redirected along with a personal identification number (PIN), a vertical service code (such as 72# or *73) and the number to which the calls are to be forwarded.\nRemote Access to Call Forwarding allows incoming calls to be diverted and answered elsewhere if a subscriber cannot use their telephone normally (for instance, the number is assigned to a lost or stolen wireless handset or to a landline in need of repair service). In some cases, a business which subscribes to standard call forwarding (*72) may be able to request temporary redirection of inbound calls when calling a telco repair service number (such as 6-1-1) to report a line outage.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41654", "revid": "757572", "url": "https://en.wikipedia.org/wiki?curid=41654", "title": "Remote Operations Service Element protocol", "text": "Communication protocol\nThe Remote Operations Service Element (ROSE) is the OSI service interface, specified in http://, that (a) provides remote operation capabilities, (b) allows interaction between entities in a distributed application, and (c) upon receiving a remote operations service request, allows the receiving entity to attempt the operation and report the results of the attempt to the requesting entity. \nOSI application protocols such as X.400 and X.500 use the services provided by ROSE. The ROSE protocol itself is defined using the notation of ASN.1.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41655", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41655", "title": "Repeater", "text": "Relay station\nIn telecommunications, a repeater is an electronic device that receives a signal and retransmits it. Repeaters are used to extend transmissions so that the signal can cover longer distances or be received on the other side of an obstruction. Some types of repeaters broadcast an identical signal, but alter its method of transmission, for example, on another frequency or baud rate.\nThere are several different types of repeaters; a telephone repeater is an amplifier in a telephone line, an optical repeater is an optoelectronic circuit that amplifies the light beam in an optical fiber cable; and a radio repeater is a radio receiver and transmitter that retransmits a radio signal. \nA broadcast relay station is a repeater used in broadcast radio and television.\nOverview.\nWhen an information-bearing signal passes through a communication channel, it is progressively degraded due to loss of power. For example, when a telephone call passes through a wire telephone line, some of the power in the electric current which represents the audio signal is dissipated as heat in the resistance of the copper wire. The longer the wire, the more power is lost, and the smaller the amplitude of the signal at the far end. So with a long enough wire the call will not be audible at the other end. Similarly, the greater the distance between a radio station and a receiver, the weaker the radio signal, and the poorer the reception. A repeater is an electronic device in a communication channel that increases the power of a signal and retransmits it, allowing it to travel further. Since it amplifies the signal, it requires a source of electric power.\nThe term \"repeater\" originated with telegraphy in the 19th century, and referred to an electromechanical device (a relay) used to regenerate telegraph signals. \nUse of the term has continued in telephony and data communications.\nIn computer networking, because repeaters work with the actual physical signal, and do not attempt to interpret the data being transmitted, they operate on the physical layer, the first layer of the OSI model; a multiport Ethernet repeater is usually called a hub.\nTypes.\nTelephone repeater.\nThis is used to increase the range of telephone signals in a telephone line.\n* Land line repeater\nThey are most frequently used in trunklines that carry long distance calls. In an analog telephone line consisting of a pair of wires, it consists of an amplifier circuit made of transistors which use power from a DC current source to increase the power of the alternating current audio signal on the line. Since the telephone is a duplex (bidirectional) communication system, the wire pair carries two audio signals, one going in each direction. So telephone repeaters have to be bilateral, amplifying the signal in both directions without causing feedback, which complicates their design considerably. Telephone repeaters were the first type of repeater and were some of the first applications of amplification. The development of telephone repeaters between 1900 and 1915 made long-distance phone service possible. Now, most telecommunications cables are fiber-optic cables which use optical repeaters (below).\nBefore the invention of electronic amplifiers, mechanically coupled carbon microphones were used as amplifiers in telephone repeaters. After the turn of the 20th century it was found that negative resistance mercury lamps could amplify, and they were used. The invention of audion tube repeaters around 1916 made transcontinental telephony practical. In the 1930s vacuum tube repeaters using hybrid coils became commonplace, allowing the use of thinner wires. In the 1950s negative impedance gain devices were more popular, and a transistorized version called the E6 repeater was the final major type used in the Bell System before the low cost of digital transmission made all voiceband repeaters obsolete. Frequency frogging repeaters were commonplace in frequency-division multiplexing systems from the middle to late 20th century.\n* Submarine cable repeater\nThis is a type of telephone repeater used in underwater submarine telecommunications cables.\nOptical communications repeater.\nThis is used to increase the range of signals in a fiber-optic cable. Digital information travels through a fiber-optic cable in the form of short pulses of light. The light is made up of particles called photons, which can be absorbed or scattered in the fiber. An optical communications repeater usually consists of a phototransistor which converts the light pulses to an electrical signal, an amplifier to increase the power of the signal, an electronic filter which reshapes the pulses, and a laser which converts the electrical signal to light again and sends it out the other fiber. However, optical amplifiers are being developed for repeaters to amplify the light itself without the need of converting it to an electric signal first.\nRadio repeater.\nThis is used to extend the range of coverage of a radio signal. The history of radio relay repeaters began in 1898 from the publication by Johann Mattausch in Austrian Journal Zeitschrift f\u00fcr Electrotechnik (v. 16,\n35 - 36). But his proposal \"Translator\" was primitive and not suitable for use. The first relay system with radio repeaters, which really functioned, was that invented in 1899 by Emile Guarini-Foresio.\nA radio repeater usually consists of a radio receiver connected to a radio transmitter. The received signal is amplified and retransmitted, often on another frequency, to provide coverage beyond the obstruction. Usage of a duplexer can allow the repeater to use one antenna for both receive and transmit at the same time.\n* Broadcast relay station, rebroadcastor or translator: This is a repeater used to extend the coverage of a radio or television broadcasting station. It consists of a secondary radio or television transmitter. The signal from the main transmitter often comes over leased telephone lines or by microwave relay.\n* Microwave relay: This is a specialized point-to-point telecommunications link, consisting of a microwave receiver that receives information over a beam of microwaves from another relay station in line-of-sight distance, and a microwave transmitter which passes the information on to the next station over another beam of microwaves. Networks of microwave relay stations transmit telephone calls, television programs, and computer data from one city to another over continent-wide areas.\n* Passive repeater: This is a microwave relay that simply consists of a flat metal surface to reflect the microwave beam in another direction. It is used to get microwave relay signals over hills and mountains when it is not necessary to amplify the signal.\n* Cellular repeater: This is a radio repeater for boosting cell phone reception in a limited area. The device functions like a small cellular base station, with a directional antenna to receive the signal from the nearest cell tower, an amplifier, and a local antenna to rebroadcast the signal to nearby cell phones. It is often used in downtown office buildings.\n* Digipeater: A repeater node in a packet radio network. It performs a store and forward function, passing on packets of information from one node to another.\n* Amateur radio repeater: Used by amateur radio operators to enable two-way communication across an area which would otherwise be difficult by point-to-point on VHF and UHF. These repeaters are set up and maintained by individual operators or clubs, and are generally available for any licensed amateur to use. A hill or mountaintop location is a preferable location to construct a repeater, as it will maximize the usability across a large area.\nRadio repeaters improve communication coverage in systems using frequencies that typically have line-of-sight propagation. Without a repeater, these systems are limited in range by the curvature of the Earth and the blocking effect of terrain or high buildings. A repeater on a hilltop or tall building can allow stations that are out of each other's line-of-sight range to communicate reliably.\nRadio repeaters may also allow translation from one set of radio frequencies to another, for example to allow two different public service agencies to interoperate (say, police and fire services of a city, or neighboring police departments). They may provide links to the public switched telephone network as well, or satellite network (BGAN, INMARSAT, MSAT) as an alternative path from source to the destination.\nTypically a repeater station listens on one frequency, A, and transmits on a second, B. All mobile stations listen for signals on channel B and transmit on channel A. The difference between the two frequencies may be relatively small compared to the frequency of operation, say 1%. Often the repeater station will use the same antenna for transmission and reception; highly selective filters called \"duplexers\" separate the faint incoming received signal from the billions of times more powerful outbound transmitted signal. Sometimes separate transmitting and receiving locations are used, connected by a wire line or a radio link. While the repeater station is designed for simultaneous reception and transmission, mobile units need not be equipped with the bulky and costly duplexers, as they only transmit or receive at any time.\nMobile units in a repeater system may be provided with a \"talkaround\" channel that allows direct mobile-to-mobile operation on a single channel. This may be used if out of reach of the repeater system, or for communications not requiring the attention of all mobiles. The \"talkaround\" channel may be the repeater output frequency; the repeater will not retransmit any signals on its output frequency.\nAn engineered radio communication system designer will analyze the coverage area desired and select repeater locations, elevations, antennas, operating frequencies and power levels to permit a predictable level of reliable communication over the designed coverage area.\nData handling.\nRepeaters can be divided into two types depending on the type of data they handle:\nAnalog repeater.\nThis type is used in channels that transmit data in the form of an analog signal in which the voltage or current is proportional to the amplitude of the signal, as in an audio signal. They are also used in trunklines that transmit multiple signals using frequency division multiplexing (FDM). Analog repeaters are composed of a linear amplifier, and may include electronic filters to compensate for frequency and phase distortion in the line.\nDigital repeater.\nThe digital repeater is used in channels that transmit data by binary digital signals, in which the data is in the form of pulses with only two possible values, representing the binary digits 1 and 0. A digital repeater amplifies the signal, and it also may retime, resynchronize, and reshape the pulses. A repeater that performs the retiming or resynchronizing functions may be called a regenerator.\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41656", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41656", "title": "Repeating coil", "text": "Voice-frequency transformer\nIn telecommunications, a repeating coil is a voice-frequency transformer characterized by a closed magnetic core, a pair of identical balanced primary (line) windings, a pair of identical but not necessarily balanced secondary (drop) windings, and low transmission loss at voice frequencies. It permits transfer of voice currents from one winding to another by magnetic induction, matches line and drop impedances, and prevents direct conduction between the line and the drop.\nIt is a special application of an isolation transformer, and is often used to prevent ground loops or earth loops, which cause humming or buzzing in audio circuits. It also prevents low direct current voltages from passing.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;"}
{"id": "41658", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=41658", "title": "Reradiation", "text": "In telecommunications, the term reradiation has the following meanings:\nNear-field effects of an AM antenna may extend out or more. Cellular and microwave towers within this radius can reflect the Medium Wave AM signal out at a phase which cancels the main Medium Wave AM signal. This process results in an interfering signal called reradiation.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41659", "revid": "304806", "url": "https://en.wikipedia.org/wiki?curid=41659", "title": "Resolution", "text": "Resolution(s) may refer to:\n&lt;templatestyles src=\"Template:TOC_right/styles.css\" /&gt;\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "41660", "revid": "49473253", "url": "https://en.wikipedia.org/wiki?curid=41660", "title": "Resonance", "text": "Physical characteristic of oscillating systems\nResonance is a phenomenon that occurs when an object or system is subjected to an external force or vibration whose frequency matches a resonant frequency (or resonance frequency) of the system, defined as a frequency that generates a maximum amplitude response in the system. When this happens, the object or system absorbs energy from the external force and starts vibrating with a larger amplitude. Resonance can occur in various systems, such as mechanical, electrical, or acoustic systems, and it is often desirable in certain applications, such as musical instruments or radio receivers. However, resonance can also be detrimental, leading to excessive vibrations or even structural failure in some cases. \nAll systems, including molecular systems and particles, tend to vibrate at a natural frequency depending upon their structure; when there is very little damping this frequency is approximately equal to, but slightly above, the resonant frequency. When an oscillating force, an external vibration, is applied at a resonant frequency of a dynamic system, object, or particle, the outside vibration will cause the system to oscillate at a higher amplitude (with more force) than when the same force is applied at other, non-resonant frequencies.\nThe resonant frequencies of a system can be identified when the response to an external vibration creates an amplitude that is a relative maximum within the system. Small periodic forces that are near a resonant frequency of the system have the ability to produce large amplitude oscillations in the system due to the storage of vibrational energy.\nResonance phenomena occur with all types of vibrations or waves: there is mechanical resonance, orbital resonance, acoustic resonance, electromagnetic resonance, nuclear magnetic resonance (NMR), electron spin resonance (ESR) and resonance of quantum wave functions. Resonant systems can be used to generate vibrations of a specific frequency (e.g., musical instruments), or pick out specific frequencies from a complex vibration containing many frequencies (e.g., filters).\nThe term \"resonance\" (from Latin \"resonantia\", 'echo', from \"resonare\", 'resound') originated from the field of acoustics, particularly the sympathetic resonance observed in musical instruments, e.g., when one string starts to vibrate and produce sound after a different one is struck.\nOverview.\nResonance occurs when a system is able to store and easily transfer energy between two or more different storage modes (such as kinetic energy and potential energy in the case of a simple pendulum). However, there are some losses from cycle to cycle, called damping. When damping is small, the resonant frequency is approximately equal to the natural frequency of the system, which is a frequency of unforced vibrations. Some systems have multiple and distinct resonant frequencies.\nExamples.\nA familiar example is a playground swing, which acts as a pendulum. Pushing a person in a swing in time with the natural interval of the swing (its resonant frequency) makes the swing go higher and higher (maximum amplitude), while attempts to push the swing at a faster or slower tempo produce smaller arcs. This is because the energy the swing absorbs is maximized when the pushes match the swing's natural oscillations.\nResonance occurs widely in nature, and is exploited in many devices. It is the mechanism by which virtually all sinusoidal waves and vibrations are generated. For example, when hard objects like metal, glass, or wood are struck, there are brief resonant vibrations in the object. Light and other short wavelength electromagnetic radiation is produced by resonance on an atomic scale, such as electrons in atoms. Other examples of resonance include:\nLinear systems.\nResonance manifests itself in many linear and nonlinear systems as oscillations around an equilibrium point. When the system is driven by a sinusoidal external input, a measured output of the system may oscillate in response. The ratio of the amplitude of the output's steady-state oscillations to the input's oscillations is called the gain, and the gain can be a function of the frequency of the sinusoidal external input. Peaks in the gain at certain frequencies correspond to resonances, where the amplitude of the measured output's oscillations are disproportionately large.\nSince many linear and nonlinear systems that oscillate are modeled as harmonic oscillators near their equilibria, a derivation of the resonant frequency for a driven, damped harmonic oscillator is shown. An RLC circuit is used to illustrate connections between resonance and a system's transfer function, frequency response, poles, and zeroes. Building off the RLC circuit example, these connections for higher-order linear systems with multiple inputs and outputs are generalized.\nThe driven, damped harmonic oscillator.\nConsider a damped mass on a spring driven by a sinusoidal, externally applied force. Newton's second law takes the form\n&lt;templatestyles src=\"Numbered block/styles.css\" /&gt;\nwhere \"m\" is the mass, \"x\" is the displacement of the mass from the equilibrium point, \"F\"0 is the driving amplitude, \"\u03c9\" is the driving angular frequency, \"k\" is the spring constant, and \"c\" is the viscous damping coefficient. This can be rewritten in the form\n&lt;templatestyles src=\"Numbered block/styles.css\" /&gt;\nwhere\nMany sources also refer to \"\u03c9\"0 as the \"resonant frequency\". However, as shown below, when analyzing oscillations of the displacement \"x\"(\"t\"), the resonant frequency is close to but not the same as \"\u03c9\"0. In general the resonant frequency is close to but not necessarily the same as the natural frequency. The RLC circuit example in the next section gives examples of different resonant frequencies for the same system.\nThe general solution of Equation (2) is the sum of a transient solution that depends on initial conditions and a steady state solution that is independent of initial conditions and depends only on the driving amplitude \"F\"0, driving frequency \"\u03c9\", undamped angular frequency \"\u03c9\"0, and the damping ratio \"\u03b6\". The transient solution decays in a relatively short amount of time, so to study resonance it is sufficient to consider the steady state solution.\nIt is possible to write the steady-state solution for \"x\"(\"t\") as a function proportional to the driving force with an induced phase change, \"\u03c6\".\nwhere formula_3\nThe phase value is usually taken to be between \u2212180\u00b0 and 0 so it represents a phase lag for both positive and negative values of the arctan argument.\nResonance occurs when, at certain driving frequencies, the steady-state amplitude of \"x\"(\"t\") is large compared to its amplitude at other driving frequencies. For the mass on a spring, resonance corresponds physically to the mass's oscillations having large displacements from the spring's equilibrium position at certain driving frequencies. Looking at the amplitude of \"x\"(\"t\") as a function of the driving frequency \"\u03c9\", the amplitude is maximal at the driving frequency\nformula_4\n\"\u03c9\"\"r\" is the resonant frequency for this system. Again, the resonant frequency does not equal the undamped angular frequency \"\u03c9\"0 of the oscillator. They are proportional, and if the damping ratio goes to zero they are the same, but for non-zero damping they are not the same frequency. As shown in the figure, resonance may also occur at other frequencies near the resonant frequency, including \"\u03c9\"0, but the maximum response is at the resonant frequency.\nAlso, \"\u03c9\"\"r\" is only real and non-zero if formula_5, so this system can only resonate when the harmonic oscillator is significantly underdamped. For systems with a very small damping ratio and a driving frequency near the resonant frequency, the steady state oscillations can become very large.\nThe pendulum.\nFor other driven, damped harmonic oscillators whose equations of motion do not look exactly like the mass on a spring example, the resonant frequency remains\nformula_6\nbut the definitions of \"\u03c9\"0 and \"\u03b6\" change based on the physics of the system. For a pendulum of length \"\u2113\" and small displacement angle \"\u03b8\", Equation (1) becomes\nformula_7\nand therefore\nRLC series circuits.\nConsider a circuit consisting of a resistor with resistance \"R\", an inductor with inductance \"L\", and a capacitor with capacitance \"C\" connected in series with current \"i\"(\"t\") and driven by a voltage source with voltage \"v\"\"in\"(\"t\"). The voltage drop around the circuit is\n&lt;templatestyles src=\"Numbered block/styles.css\" /&gt;\nRather than analyzing a candidate solution to this equation like in the mass on a spring example above, this section will analyze the frequency response of this circuit. Taking the Laplace transform of Equation (4),\nformula_10\nwhere \"I\"(\"s\") and \"V\"\"in\"(\"s\") are the Laplace transform of the current and input voltage, respectively, and \"s\" is a complex frequency parameter in the Laplace domain. Rearranging terms,\nformula_11\nVoltage across the capacitor.\nAn RLC circuit in series presents several options for where to measure an output voltage. Suppose the output voltage of interest is the voltage drop across the capacitor. As shown above, in the Laplace domain this voltage is\nformula_12\nor\nformula_13\nDefine for this circuit a natural frequency and a damping ratio,\nformula_14\nformula_15\nThe ratio of the output voltage to the input voltage becomes\nformula_16\n\"H\"(\"s\") is the transfer function between the input voltage and the output voltage. This transfer function has two poles\u2013roots of the polynomial in the transfer function's denominator\u2013at\n&lt;templatestyles src=\"Numbered block/styles.css\" /&gt;\nand no zeros\u2013roots of the polynomial in the transfer function's numerator. Moreover, for \"\u03b6\" \u2264 1, the magnitude of these poles is the natural frequency \"\u03c9\"0 and that for \"\u03b6\" &lt; 1/formula_17, our condition for resonance in the harmonic oscillator example, the poles are closer to the imaginary axis than to the real axis.\nEvaluating \"H\"(\"s\") along the imaginary axis \"s\" \n \"i\u03c9\", the transfer function describes the frequency response of this circuit. Equivalently, the frequency response can be analyzed by taking the Fourier transform of Equation (4) instead of the Laplace transform. The transfer function, which is also complex, can be written as a gain and phase,\nformula_18\nA sinusoidal input voltage at frequency \"\u03c9\" results in an output voltage at the same frequency that has been scaled by \"G\"(\"\u03c9\") and has a phase shift \"\u03a6\"(\"\u03c9\"). The gain and phase can be plotted versus frequency on a Bode plot. For the RLC circuit's capacitor voltage, the gain of the transfer function \"H\"(\"i\u03c9\") is\nNote the similarity between the gain here and the amplitude in Equation (3). Once again, the gain is maximized at the resonant frequency\nformula_4\nHere, the resonance corresponds physically to having a relatively large amplitude for the steady state oscillations of the voltage across the capacitor compared to its amplitude at other driving frequencies.\nVoltage across the inductor.\nThe resonant frequency need not always take the form given in the examples above. For the RLC circuit, suppose instead that the output voltage of interest is the voltage across the inductor. As shown above, in the Laplace domain the voltage across the inductor is\nformula_20\nformula_21\nformula_22\nusing the same definitions for \"\u03c9\"0 and \"\u03b6\" as in the previous example. The transfer function between \"V\"in(\"s\") and this new \"V\"out(\"s\") across the inductor is\nformula_23\nThis transfer function has the same poles as the transfer function in the previous example, but it also has two zeroes in the numerator at \"s\" \n 0. Evaluating \"H\"(\"s\") along the imaginary axis, its gain becomes\nformula_24\nCompared to the gain in Equation (6) using the capacitor voltage as the output, this gain has a factor of \"\u03c9\"2 in the numerator and will therefore have a different resonant frequency that maximizes the gain. That frequency is\nformula_25\nSo for the same RLC circuit but with the voltage across the inductor as the output, the resonant frequency is now \"larger\" than the natural frequency, though it still tends towards the natural frequency as the damping ratio goes to zero. That the same circuit can have different resonant frequencies for different choices of output is not contradictory. As shown in Equation (4), the voltage drop across the circuit is divided among the three circuit elements, and each element has different dynamics. The capacitor's voltage grows slowly by integrating the current over time and is therefore more sensitive to lower frequencies, whereas the inductor's voltage grows when the current changes rapidly and is therefore more sensitive to higher frequencies. While the circuit as a whole has a natural frequency where it tends to oscillate, the different dynamics of each circuit element make each element resonate at a slightly different frequency.\nVoltage across the resistor.\nSuppose that the output voltage of interest is the voltage across the resistor. In the Laplace domain the voltage across the resistor is\nformula_26\nformula_27\nand using the same natural frequency and damping ratio as in the capacitor example the transfer function is\nformula_28\nThis transfer function also has the same poles as the previous RLC circuit examples, but it only has one zero in the numerator at \"s\" = 0. For this transfer function, its gain is\nformula_29\nThe resonant frequency that maximizes this gain is\nformula_30\nand the gain is one at this frequency, so the voltage across the resistor resonates \"at\" the circuit's natural frequency and at this frequency the amplitude of the voltage across the resistor equals the input voltage's amplitude.\nAntiresonance.\nSome systems exhibit antiresonance that can be analyzed in the same way as resonance. For antiresonance, the amplitude of the response of the system at certain frequencies is disproportionately \"small\" rather than being disproportionately large. In the RLC circuit example, this phenomenon can be observed by analyzing both the inductor and the capacitor combined.\nSuppose that the output voltage of interest in the RLC circuit is the voltage across the inductor \"and\" the capacitor combined in series. Equation (4) showed that the sum of the voltages across the three circuit elements sums to the input voltage, so measuring the output voltage as the sum of the inductor and capacitor voltages combined is the same as \"v\"\"in\" minus the voltage drop across the resistor. The previous example showed that at the natural frequency of the system, the amplitude of the voltage drop across the resistor \"equals\" the amplitude of \"v\"\"in\", and therefore the voltage across the inductor and capacitor combined has zero amplitude. We can show this with the transfer function.\nThe sum of the inductor and capacitor voltages is\nformula_31\nformula_32\nUsing the same natural frequency and damping ratios as the previous examples, the transfer function is\nformula_33\nThis transfer has the same poles as the previous examples but has zeroes at\n&lt;templatestyles src=\"Numbered block/styles.css\" /&gt;\nEvaluating the transfer function along the imaginary axis, its gain is\nformula_34\nRather than look for resonance, i.e., peaks of the gain, notice that the gain goes to zero at \"\u03c9\" = \"\u03c9\"0, which complements our analysis of the resistor's voltage. This is called antiresonance, which has the opposite effect of resonance. Rather than result in outputs that are disproportionately large at this frequency, this circuit with this choice of output has no response at all at this frequency. The frequency that is filtered out corresponds exactly to the zeroes of the transfer function, which were shown in Equation (7) and were on the imaginary axis.\nRelationships between resonance and frequency response in the RLC series circuit example.\nThese RLC circuit examples illustrate how resonance is related to the frequency response of the system. Specifically, these examples illustrate:\nThe next section extends these concepts to resonance in a general linear system.\nGeneralizing resonance and antiresonance for linear systems.\nNext consider an arbitrary linear system with multiple inputs and outputs. For example, in state-space representation a third order linear time-invariant system with three inputs and two outputs might be written as\nformula_35\nformula_36\nwhere \"u\"\"i\"(\"t\") are the inputs, \"x\"\"i\"(t) are the state variables, \"y\"\"i\"(\"t\") are the outputs, and \"A\", \"B\", \"C\", and \"D\" are matrices describing the dynamics between the variables.\nThis system has a transfer function matrix whose elements are the transfer functions between the various inputs and outputs. For example,\nformula_37\nEach \"H\"\"ij\"(\"s\") is a scalar transfer function linking one of the inputs to one of the outputs. The RLC circuit examples above had one input voltage and showed four possible output voltages\u2013across the capacitor, across the inductor, across the resistor, and across the capacitor and inductor combined in series\u2013each with its own transfer function. If the RLC circuit were set up to measure all four of these output voltages, that system would have a 4\u00d71 transfer function matrix linking the single input to each of the four outputs.\nEvaluated along the imaginary axis, each \"H\"\"ij\"(\"i\u03c9\") can be written as a gain and phase shift,\nformula_38\nPeaks in the gain at certain frequencies correspond to resonances between that transfer function's input and output, assuming the system is stable.\nEach transfer function \"H\"\"ij\"(\"s\") can also be written as a fraction whose numerator and denominator are polynomials of \"s\".\nformula_39\nThe complex roots of the numerator are called zeroes, and the complex roots of the denominator are called poles. For a stable system, the positions of these poles and zeroes on the complex plane give some indication of whether the system can resonate or antiresonate and at which frequencies. In particular, any stable or marginally stable, complex conjugate pair of poles with imaginary components can be written in terms of a natural frequency and a damping ratio as\nformula_40\nas in Equation (5). The natural frequency \"\u03c9\"0 of that pole is the magnitude of the position of the pole on the complex plane and the damping ratio of that pole determines how quickly that oscillation decays. In general,\nIn the RLC circuit example, the first generalization relating poles to resonance is observed in Equation (5). The second generalization relating zeroes to antiresonance is observed in Equation (7). In the examples of the harmonic oscillator, the RLC circuit capacitor voltage, and the RLC circuit inductor voltage, \"poles near the imaginary axis\" corresponds to the significantly underdamped condition \u03b6 &lt; 1/formula_17.\nStanding waves.\nA physical system can have as many natural frequencies as it has degrees of freedom and can resonate near each of those natural frequencies. A mass on a spring, which has one degree of freedom, has one natural frequency. A double pendulum, which has two degrees of freedom, can have two natural frequencies. As the number of coupled harmonic oscillators increases, the time it takes to transfer energy from one to the next becomes significant. Systems with very large numbers of degrees of freedom can be thought of as continuous rather than as having discrete oscillators.\nEnergy transfers from one oscillator to the next in the form of waves. For example, the string of a guitar or the surface of water in a bowl can be modeled as a continuum of small coupled oscillators and waves can travel along them. In many cases these systems have the potential to resonate at certain frequencies, forming standing waves with large-amplitude oscillations at fixed positions. Resonance in the form of standing waves underlies many familiar phenomena, such as the sound produced by musical instruments, electromagnetic cavities used in lasers and microwave ovens, and energy levels of atoms.\nStanding waves on a string.\nWhen a string of fixed length is driven at a particular frequency, a wave propagates along the string at the same frequency. The waves reflect off the ends of the string, and eventually a steady state is reached with waves traveling in both directions. The waveform is the superposition of the waves.\nAt certain frequencies, the steady state waveform does not appear to travel along the string. At fixed positions called nodes, the string is never displaced. Between the nodes the string oscillates and exactly halfway between the nodes, at positions called anti-nodes, where the oscillations have their largest amplitude.\nFor a string of length formula_42 with fixed ends, the displacement formula_43 of the string perpendicular to the formula_44-axis at time formula_45 is\nformula_46\nwhere\nThe frequencies that resonate and form standing waves relate to the length of the string as\nformula_50\nformula_51\nwhere formula_52 is the speed of the wave and the integer formula_53 denotes different modes or harmonics. The standing wave with \"n\" = 1 oscillates at the fundamental frequency and has a wavelength that is twice the length of the string. The possible modes of oscillation form a harmonic series.\nResonance in complex networks.\nA generalization to complex networks of coupled harmonic oscillators shows that such systems have a finite number of natural resonant frequencies, related to the topological structure of the network itself. In particular, such frequencies result related to the eigenvalues of the network's Laplacian matrix. Let formula_54 be the adjacency matrix describing the topological structure of the network and formula_55 the corresponding Laplacian matrix, where formula_56 is the diagonal matrix of the degrees of the network's nodes. Then, for a network of classical and identical harmonic oscillators, when a sinusoidal driving force formula_57 is applied to a specific node, the global resonant frequencies of the network are given by formula_58 where formula_59 are the eigenvalues of the Laplacian formula_60.\nTypes.\nMechanical.\nMechanical resonance is the tendency of a mechanical system to absorb more energy when the frequency of its oscillations matches the system's natural frequency of vibration than it does at other frequencies. It may cause violent swaying motions and even catastrophic failure in improperly constructed structures including bridges, buildings, trains, and aircraft. When designing objects, engineers must ensure the mechanical resonance frequencies of the component parts do not match driving vibrational frequencies of motors or other oscillating parts, a phenomenon known as resonance disaster.\nAvoiding resonance disasters is a major concern in every building, tower, and bridge construction project. As a countermeasure, shock mounts can be installed to absorb resonant frequencies and thus dissipate the absorbed energy. The Taipei 101 building relies on a \u2014a tuned mass damper\u2014to cancel resonance. Furthermore, the structure is designed to resonate at a frequency that does not typically occur. Buildings in seismic zones are often constructed to take into account the oscillating frequencies of expected ground motion. In addition, engineers designing objects having engines must ensure that the mechanical resonant frequencies of the component parts do not match driving vibrational frequencies of the motors or other strongly oscillating parts.\nClocks keep time by mechanical resonance in a balance wheel, pendulum, or quartz crystal.\nThe cadence of runners has been hypothesized to be energetically favorable due to resonance between the elastic energy stored in the lower limb and the mass of the runner.\nInternational Space Station.\nThe rocket engines for the International Space Station (ISS) are controlled by an autopilot. Ordinarily, uploaded parameters for controlling the engine control system for the Zvezda module make the rocket engines boost the International Space Station to a higher orbit. The rocket engines are hinge-mounted, and ordinarily the crew does not notice the operation. On January 14, 2009, however, the uploaded parameters made the autopilot swing the rocket engines in larger and larger oscillations, at a frequency of 0.5\u00a0Hz. These oscillations were captured on video, and lasted for 142 seconds.\nAcoustic.\nAcoustic resonance is a branch of mechanical resonance that is concerned with the mechanical vibrations across the frequency range of human hearing, in other words sound. For humans, hearing is normally limited to frequencies between about 20\u00a0Hz and 20,000\u00a0Hz (20\u00a0kHz), Many objects and materials act as resonators with resonant frequencies within this range, and when struck vibrate mechanically, pushing on the surrounding air to create sound waves. This is the source of many percussive sounds we hear.\nAcoustic resonance is an important consideration for instrument builders, as most acoustic instruments use resonators, such as the strings and body of a violin, the length of tube in a flute, and the shape of, and tension on, a drum membrane.\nLike mechanical resonance, acoustic resonance can result in catastrophic failure of the object at resonance. The classic example of this is breaking a wine glass with sound at the precise resonant frequency of the glass, although this is difficult in practice.\nElectrical.\nElectrical resonance occurs in an electric circuit at a particular \"resonant frequency\" when the impedance of the circuit is at a minimum in a series circuit or at maximum in a parallel circuit (usually when the transfer function peaks in absolute value). Resonance in circuits are used for both transmitting and receiving wireless communications such as television, cell phones and radio.\nOptical.\nAn optical cavity, also called an \"optical resonator\", is an arrangement of mirrors that forms a standing wave cavity resonator for light waves. Optical cavities are a major component of lasers, surrounding the gain medium and providing feedback of the laser light. They are also used in optical parametric oscillators and some interferometers. Light confined in the cavity reflects multiple times producing standing waves for certain resonant frequencies. The standing wave patterns produced are called \"modes\". Longitudinal modes differ only in frequency while transverse modes differ for different frequencies and have different intensity patterns across the cross-section of the beam. Ring resonators and whispering galleries are examples of optical resonators that do not form standing waves.\nDifferent resonator types are distinguished by the focal lengths of the two mirrors and the distance between them; flat mirrors are not often used because of the difficulty of aligning them precisely. The geometry (resonator type) must be chosen so the beam remains stable, i.e., the beam size does not continue to grow with each reflection. Resonator types are also designed to meet other criteria such as minimum beam waist or having no focal point (and therefore intense light at that point) inside the cavity.\nOptical cavities are designed to have a very large \"Q\" factor. A beam reflects a large number of times with little attenuation\u2014therefore the frequency line width of the beam is small compared to the frequency of the laser.\nAdditional optical resonances are guided-mode resonances and surface plasmon resonance, which result in anomalous reflection and high evanescent fields at resonance. In this case, the resonant modes are guided modes of a waveguide or surface plasmon modes of a dielectric-metallic interface. These modes are usually excited by a subwavelength grating.\nOrbital.\nIn celestial mechanics, an orbital resonance occurs when two orbiting bodies exert a regular, periodic gravitational influence on each other, usually due to their orbital periods being related by a ratio of two small integers. Orbital resonances greatly enhance the mutual gravitational influence of the bodies. In most cases, this results in an \"unstable\" interaction, in which the bodies exchange momentum and shift orbits until the resonance no longer exists. Under some circumstances, a resonant system can be stable and self-correcting, so that the bodies remain in resonance. Examples are the 1:2:4 resonance of Jupiter's moons Ganymede, Europa, and Io, and the 2:3 resonance between Pluto and Neptune. Unstable resonances with Saturn's inner moons give rise to gaps in the rings of Saturn. The special case of 1:1 resonance (between bodies with similar orbital radii) causes large Solar System bodies to clear the neighborhood around their orbits by ejecting nearly everything else around them; this effect is used in the current definition of a planet.\nAtomic, particle, and molecular.\nNuclear magnetic resonance (NMR) is the name given to a physical resonance phenomenon involving the observation of specific quantum mechanical magnetic properties of an atomic nucleus in the presence of an applied, external magnetic field. Many scientific techniques exploit NMR phenomena to study molecular physics, crystals, and non-crystalline materials through NMR spectroscopy. NMR is also routinely used in advanced medical imaging techniques, such as in magnetic resonance imaging (MRI).\nAll nuclei containing odd numbers of nucleons have an intrinsic angular momentum and magnetic moment. A key feature of NMR is that the resonant frequency of a particular substance is directly proportional to the strength of the applied magnetic field. It is this feature that is exploited in imaging techniques; if a sample is placed in a non-uniform magnetic field then the resonant frequencies of the sample's nuclei depend on where in the field they are located. Therefore, the particle can be located quite precisely by its resonant frequency.\nElectron paramagnetic resonance, otherwise known as \"electron spin resonance\" (ESR), is a spectroscopic technique similar to NMR, but uses unpaired electrons instead. Materials for which this can be applied are much more limited since the material needs to both have an unpaired spin and be paramagnetic.\nThe M\u00f6ssbauer effect is the resonant and recoil-free emission and absorption of gamma ray photons by atoms bound in a solid form.\nResonance in particle physics appears in similar circumstances to classical physics at the level of quantum mechanics and quantum field theory. Resonances can also be thought of as unstable particles, with the formula in the Universal resonance curve section of this article applying if \"\u0393\" is the particle's decay rate and formula_61 is the particle's mass \"M\". In that case, the formula comes from the particle's propagator, with its mass replaced by the complex number \"M\"\u00a0+\u00a0\"i\u0393\". The formula is further related to the particle's decay rate by the optical theorem.\nDisadvantages.\nA column of soldiers marching in regular step on a narrow and structurally flexible bridge can set it into dangerously large amplitude oscillations. On April 12, 1831, the Broughton Suspension Bridge near Salford, England collapsed while a group of British soldiers were marching across. Since then, the British Army has had a standing order for soldiers to break stride when marching across bridges, to avoid resonance from their regular marching pattern affecting the bridge.\nVibrations of a motor or engine can induce resonant vibration in its supporting structures if their natural frequency is close to that of the vibrations of the engine. A common example is the rattling sound of a bus body when the engine is left idling.\nStructural resonance of a suspension bridge induced by winds can lead to its catastrophic collapse. Several early suspension bridges in Europe and United States were destroyed by structural resonance induced by modest winds. The collapse of the Tacoma Narrows Bridge on 7 November 1940 is characterized in physics as a classic example of resonance. It has been argued by Robert H. Scanlan and others that the destruction was instead caused by aeroelastic flutter, a complicated interaction between the bridge and the winds passing through it\u2014an example of a self oscillation, or a kind of \"self-sustaining vibration\" as referred to in the nonlinear theory of vibrations.\nQ factor.\nThe \"Q\" factor or \"quality factor\" is a dimensionless parameter that describes how under-damped an oscillator or resonator is, and characterizes the bandwidth of a resonator relative to its center frequency.\nA high value for \"Q\" indicates a lower rate of energy loss relative to the stored energy, i.e., the system is lightly damped. The parameter is defined by the equation:\nformula_62.\nThe higher the Q factor, the greater the amplitude at the resonant frequency, and the smaller the \"bandwidth\", or range of frequencies around resonance occurs. In electrical resonance, a high-\"Q\" circuit in a radio receiver is more difficult to tune, but has greater selectivity, and so would be better at filtering out signals from other stations. High Q oscillators are more stable.\nExamples that normally have a low Q factor include door closers (Q=0.5). Systems with high Q factors include tuning forks (Q=1000), atomic clocks and lasers (Q\u22481011).\nUniversal resonance curve.\nThe exact response of a resonance, especially for frequencies far from the resonant frequency, depends on the details of the physical system, and is usually not exactly symmetric about the resonant frequency, as illustrated for the simple harmonic oscillator above.\nFor a lightly damped linear oscillator with a resonance frequency formula_61, the \"intensity\" of oscillations formula_64 when the system is driven with a driving frequency \"formula_65\" is typically approximated by the following formula that is symmetric about the resonance frequency:formula_66\nWhere the susceptibility formula_67 links the amplitude of the oscillator to the driving force in frequency space:formula_68\nThe intensity is defined as the square of the amplitude of the oscillations. This is a Lorentzian function, or Cauchy distribution, and this response is found in many physical situations involving resonant systems. \u0393 is a parameter dependent on the damping of the oscillator, and is known as the \"linewidth\" of the resonance. Heavily damped oscillators tend to have broad linewidths, and respond to a wider range of driving frequencies around the resonant frequency. The linewidth is inversely proportional to the \"Q\" factor, which is a measure of the sharpness of the resonance.\nIn radio engineering and electronics engineering, this approximate symmetric response is known as the \"universal resonance curve\", a concept introduced by Frederick E. Terman in 1932 to simplify the approximate analysis of radio circuits with a range of center frequencies and \"Q\" values.\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "41661", "revid": "48649018", "url": "https://en.wikipedia.org/wiki?curid=41661", "title": "Response", "text": "Response may refer to:\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "41662", "revid": "7903804", "url": "https://en.wikipedia.org/wiki?curid=41662", "title": "Response time (technology)", "text": "Time a given technological system takes to respond to an input\nIn technology, response time is the time a system or functional unit takes to react to a given input.\nComputing.\nIn computing, the responsiveness of a service, how long a system takes to respond to a request for service, is measured through the response time. That service can be anything from a memory fetch, to a disk IO, to a complex database query, or loading a full web page. Ignoring transmission time for a moment, the response time is the sum of the service time and wait time. The service time is the time it takes to do the work you requested. For a given request the service time varies little as the workload increases \u2013 to do X amount of work it always takes X amount of time. The wait time is how long the request had to wait in a queue before being serviced and it varies from zero, when no waiting is required, to a large multiple of the service time, as many requests are already in the queue and have to be serviced first.\nWith basic queueing theory math you can calculate how the average wait time increases as the device providing the service goes from 0-100% busy. As the device becomes busier, the average wait time increases in a non-linear fashion. The busier the device is, the more dramatic the response time increases will seem as you approach 100% busy; all of that increase is caused by increases in wait time, which is the result of all the requests waiting in queue that have to run first.\nTransmission time gets added to response time when your request and the resulting response has to travel over a network and it can be very significant. Transmission time can include propagation delays due to distance (the speed of light is finite), delays due to transmission errors, and data communication bandwidth limits (especially at the last mile) slowing the transmission speed of the request or the reply.\nDevelopers can reduce the response time of a system (for end users or not) using program optimization techniques.\nReal-time systems.\nIn real-time systems the response time of a task or thread is defined as the time elapsed between the dispatch (time when task is ready to execute) to the time when it finishes its job (one dispatch). Response time is different from WCET which is the maximum time the task would take if it were to execute without interference. It is also different from deadline which is the length of time during which the task's output would be valid in the context of the specific system. And it has a relation to the TTFB, which is the time between the dispatch and the time when the response starts.\nDisplay technologies.\nResponse time is the amount of time a pixel in a display takes to change. It is measured in milliseconds (ms). Lower numbers mean faster transitions and therefore fewer visible image artifacts. Display monitors with long response times would create display motion blur around moving objects, making them unacceptable for rapidly moving images. Response times are usually measured from grey-to-grey transitions, based on a VESA industry standard from the 10% to the 90% points in the pixel response curve.\nIn fast paced competitive games such as Counter-Strike, the response time of a display is crucial for optimal performance. Displays that have a lower response time are more responsive to player input and produce less visual errors when displaying a rapidly changing image, making low response time important for competitive gaming. Most modern monitors that are marketed for gaming have a response time of 1ms, although it is not uncommon to see &lt;1ms response time in high end monitors, and &gt;1ms response time on less expensive monitors or monitors that have a higher resolution.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
